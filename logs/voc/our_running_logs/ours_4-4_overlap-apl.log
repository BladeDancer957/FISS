nohup: ignoring input
25
kvoc_4-4_OURS-APL On GPUs 0\Writing in results/seed_2023-ov/2023-03-16_voc_4-4_OURS-APL.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[6, 0, 7, 2]
Current Client Index:  6
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[8, 5, 3, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[5, 2, 0, 3]
Current Client Index:  5
Current Client Index:  2
Current Client Index:  0
Current Client Index:  3
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[3, 6, 1, 7]
Current Client Index:  3
Current Client Index:  6
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.09879439324140549, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.957759
Mean Acc: 0.761733
FreqW Acc: 0.923270
Mean IoU: 0.703965
Class IoU:
	class 0: 0.9539515191877496
	class 1: 0.7615770129403676
	class 2: 0.2054173196242969
	class 3: 0.8920301311931925
	class 4: 0.7068509839220561
Class Acc:
	class 0: 0.9848764620456163
	class 1: 0.7738026584549321
	class 2: 0.27733822617675347
	class 3: 0.9519182023560161
	class 4: 0.8207287142557349

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[0, 12, 6, 10]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/52, Loss=9.001189076900483
Loss made of: CE 1.24921715259552, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.27430534362793 EntMin 0.0
Epoch 1, Batch 20/52, Loss=7.2563887417316435
Loss made of: CE 0.9454250335693359, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.077418804168701 EntMin 0.0
Epoch 1, Batch 30/52, Loss=6.769150650501251
Loss made of: CE 0.8886802196502686, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.446710109710693 EntMin 0.0
Epoch 1, Batch 40/52, Loss=6.1686204195022585
Loss made of: CE 0.6387081146240234, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.109854698181152 EntMin 0.0
Epoch 1, Batch 50/52, Loss=6.048906081914902
Loss made of: CE 0.6356409192085266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.829582214355469 EntMin 0.0
Epoch 1, Class Loss=0.9707386493682861, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.9707386493682861, Class Loss=0.9707386493682861, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/52, Loss=5.663356333971024
Loss made of: CE 0.6443127393722534, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.845562934875488 EntMin 0.0
Epoch 2, Batch 20/52, Loss=5.358342713117599
Loss made of: CE 0.6432214379310608, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7210540771484375 EntMin 0.0
Epoch 2, Batch 30/52, Loss=5.510810589790344
Loss made of: CE 0.5210590362548828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.699591636657715 EntMin 0.0
Epoch 2, Batch 40/52, Loss=5.246954208612442
Loss made of: CE 0.4836757183074951, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.126607894897461 EntMin 0.0
Epoch 2, Batch 50/52, Loss=5.091253644227981
Loss made of: CE 0.6033235788345337, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.050069808959961 EntMin 0.0
Epoch 2, Class Loss=0.5897321701049805, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.5897321701049805, Class Loss=0.5897321701049805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/52, Loss=5.099390241503715
Loss made of: CE 0.39682865142822266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.753942966461182 EntMin 0.0
Epoch 3, Batch 20/52, Loss=4.884333795309066
Loss made of: CE 0.44149690866470337, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.231730937957764 EntMin 0.0
Epoch 3, Batch 30/52, Loss=4.897306302189827
Loss made of: CE 0.4943375587463379, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.828338623046875 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.7935982048511505
Loss made of: CE 0.38932380080223083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.024192810058594 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.781122905015946
Loss made of: CE 0.4149942398071289, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.284709930419922 EntMin 0.0
Epoch 3, Class Loss=0.45617392659187317, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.45617392659187317, Class Loss=0.45617392659187317, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/52, Loss=4.854489752650261
Loss made of: CE 0.5434566736221313, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.200282096862793 EntMin 0.0
Epoch 4, Batch 20/52, Loss=4.587738084793091
Loss made of: CE 0.38801318407058716, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2506022453308105 EntMin 0.0
Epoch 4, Batch 30/52, Loss=4.54920668900013
Loss made of: CE 0.2793853282928467, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7842729091644287 EntMin 0.0
Epoch 4, Batch 40/52, Loss=4.547630584239959
Loss made of: CE 0.3057797849178314, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.977755308151245 EntMin 0.0
Epoch 4, Batch 50/52, Loss=4.338889914751053
Loss made of: CE 0.3763134181499481, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.751840114593506 EntMin 0.0
Epoch 4, Class Loss=0.3855103850364685, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3855103850364685, Class Loss=0.3855103850364685, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/52, Loss=4.412589013576508
Loss made of: CE 0.3322647213935852, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.679664134979248 EntMin 0.0
Epoch 5, Batch 20/52, Loss=4.416228607296944
Loss made of: CE 0.31752049922943115, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.924197196960449 EntMin 0.0
Epoch 5, Batch 30/52, Loss=4.391994699835777
Loss made of: CE 0.37857723236083984, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8719546794891357 EntMin 0.0
Epoch 5, Batch 40/52, Loss=4.369655564427376
Loss made of: CE 0.38003796339035034, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.381709098815918 EntMin 0.0
Epoch 5, Batch 50/52, Loss=4.34231927394867
Loss made of: CE 0.35309505462646484, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8627867698669434 EntMin 0.0
Epoch 5, Class Loss=0.3536079227924347, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.3536079227924347, Class Loss=0.3536079227924347, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/52, Loss=4.17059640288353
Loss made of: CE 0.30266493558883667, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.65376353263855 EntMin 0.0
Epoch 6, Batch 20/52, Loss=4.38711368739605
Loss made of: CE 0.28959769010543823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5083155632019043 EntMin 0.0
Epoch 6, Batch 30/52, Loss=4.468885278701782
Loss made of: CE 0.42787766456604004, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.916645050048828 EntMin 0.0
Epoch 6, Batch 40/52, Loss=4.100532905757428
Loss made of: CE 0.3504312038421631, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.544325590133667 EntMin 0.0
Epoch 6, Batch 50/52, Loss=4.27006222307682
Loss made of: CE 0.43143796920776367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1931867599487305 EntMin 0.0
Epoch 6, Class Loss=0.32960423827171326, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.32960423827171326, Class Loss=0.32960423827171326, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=9.85644143819809
Loss made of: CE 1.1348228454589844, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.04586410522461 EntMin 0.0
Epoch 1, Batch 20/40, Loss=8.651649385690689
Loss made of: CE 0.7980396747589111, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.40626335144043 EntMin 0.0
Epoch 1, Batch 30/40, Loss=7.732328730821609
Loss made of: CE 0.7655273675918579, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.362523555755615 EntMin 0.0
Epoch 1, Batch 40/40, Loss=7.220924031734467
Loss made of: CE 0.6649703979492188, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.44716215133667 EntMin 0.0
Epoch 1, Class Loss=0.9984206557273865, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.9984206557273865, Class Loss=0.9984206557273865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/40, Loss=6.916527679562568
Loss made of: CE 0.35223501920700073, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.587371826171875 EntMin 0.0
Epoch 2, Batch 20/40, Loss=6.4342104375362394
Loss made of: CE 0.5491711497306824, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.211660385131836 EntMin 0.0
Epoch 2, Batch 30/40, Loss=6.240195548534393
Loss made of: CE 0.5088114738464355, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.52558708190918 EntMin 0.0
Epoch 2, Batch 40/40, Loss=6.3739174723625185
Loss made of: CE 0.5690517425537109, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1859235763549805 EntMin 0.0
Epoch 2, Class Loss=0.5476015210151672, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.5476015210151672, Class Loss=0.5476015210151672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/40, Loss=5.975152060389519
Loss made of: CE 0.3731839656829834, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.340671539306641 EntMin 0.0
Epoch 3, Batch 20/40, Loss=5.8705443173646925
Loss made of: CE 0.6186316013336182, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.550012111663818 EntMin 0.0
Epoch 3, Batch 30/40, Loss=5.790890470147133
Loss made of: CE 0.3506248891353607, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.440740585327148 EntMin 0.0
Epoch 3, Batch 40/40, Loss=5.618624037504196
Loss made of: CE 0.301196813583374, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0639495849609375 EntMin 0.0
Epoch 3, Class Loss=0.44880586862564087, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.44880586862564087, Class Loss=0.44880586862564087, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/40, Loss=5.474397519230843
Loss made of: CE 0.45501160621643066, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.535435199737549 EntMin 0.0
Epoch 4, Batch 20/40, Loss=5.479773327708244
Loss made of: CE 0.24627748131752014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679203510284424 EntMin 0.0
Epoch 4, Batch 30/40, Loss=5.46835435628891
Loss made of: CE 0.35808318853378296, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.130572319030762 EntMin 0.0
Epoch 4, Batch 40/40, Loss=5.560010024905205
Loss made of: CE 0.384683221578598, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.285562992095947 EntMin 0.0
Epoch 4, Class Loss=0.3968556523323059, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.3968556523323059, Class Loss=0.3968556523323059, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/40, Loss=5.393304604291916
Loss made of: CE 0.37750673294067383, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.922956466674805 EntMin 0.0
Epoch 5, Batch 20/40, Loss=5.228309863805771
Loss made of: CE 0.5081216096878052, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.229989528656006 EntMin 0.0
Epoch 5, Batch 30/40, Loss=5.304078224301338
Loss made of: CE 0.2961623966693878, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.532161712646484 EntMin 0.0
Epoch 5, Batch 40/40, Loss=5.331560093164444
Loss made of: CE 0.35929247736930847, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.739461898803711 EntMin 0.0
Epoch 5, Class Loss=0.35834163427352905, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.35834163427352905, Class Loss=0.35834163427352905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/40, Loss=5.1385566085577015
Loss made of: CE 0.29334932565689087, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645312309265137 EntMin 0.0
Epoch 6, Batch 20/40, Loss=5.270792882144451
Loss made of: CE 0.3165559470653534, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3446364402771 EntMin 0.0
Epoch 6, Batch 30/40, Loss=5.314678519964218
Loss made of: CE 0.31559664011001587, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.274138450622559 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.995176853239537
Loss made of: CE 0.218326136469841, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.529787063598633 EntMin 0.0
Epoch 6, Class Loss=0.33639654517173767, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.33639654517173767, Class Loss=0.33639654517173767, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=10.244042670726776
Loss made of: CE 1.3524141311645508, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.17823600769043 EntMin 0.0
Epoch 1, Batch 20/34, Loss=8.653929591178894
Loss made of: CE 0.892602801322937, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4140238761901855 EntMin 0.0
Epoch 1, Batch 30/34, Loss=8.222034800052644
Loss made of: CE 0.8794087171554565, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.813726425170898 EntMin 0.0
Epoch 1, Class Loss=1.162253737449646, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=1.162253737449646, Class Loss=1.162253737449646, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/34, Loss=7.285090214014053
Loss made of: CE 0.7235245704650879, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.422754287719727 EntMin 0.0
Epoch 2, Batch 20/34, Loss=6.698100405931473
Loss made of: CE 0.7086960673332214, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.657628059387207 EntMin 0.0
Epoch 2, Batch 30/34, Loss=6.471710586547852
Loss made of: CE 0.5554133653640747, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.022841930389404 EntMin 0.0
Epoch 2, Class Loss=0.6666619777679443, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6666619777679443, Class Loss=0.6666619777679443, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/34, Loss=6.00768872499466
Loss made of: CE 0.5781423449516296, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.252689361572266 EntMin 0.0
Epoch 3, Batch 20/34, Loss=5.939742621779442
Loss made of: CE 0.4960450530052185, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.185317039489746 EntMin 0.0
Epoch 3, Batch 30/34, Loss=6.06026014983654
Loss made of: CE 0.44947606325149536, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.281060695648193 EntMin 0.0
Epoch 3, Class Loss=0.5079867839813232, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.5079867839813232, Class Loss=0.5079867839813232, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/34, Loss=5.683417144417763
Loss made of: CE 0.43792447447776794, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.229825973510742 EntMin 0.0
Epoch 4, Batch 20/34, Loss=5.60400738120079
Loss made of: CE 0.39518821239471436, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.719002723693848 EntMin 0.0
Epoch 4, Batch 30/34, Loss=5.665150135755539
Loss made of: CE 0.5312561988830566, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.774117946624756 EntMin 0.0
Epoch 4, Class Loss=0.42451563477516174, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.42451563477516174, Class Loss=0.42451563477516174, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/34, Loss=5.376880764961243
Loss made of: CE 0.3370473086833954, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5428361892700195 EntMin 0.0
Epoch 5, Batch 20/34, Loss=5.496059972047806
Loss made of: CE 0.4304831027984619, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.569209098815918 EntMin 0.0
Epoch 5, Batch 30/34, Loss=5.163279992341995
Loss made of: CE 0.3187149465084076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.762958526611328 EntMin 0.0
Epoch 5, Class Loss=0.3661777675151825, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.3661777675151825, Class Loss=0.3661777675151825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/34, Loss=5.230144721269608
Loss made of: CE 0.3760558068752289, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.981128692626953 EntMin 0.0
Epoch 6, Batch 20/34, Loss=5.226093792915345
Loss made of: CE 0.38065844774246216, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.648639678955078 EntMin 0.0
Epoch 6, Batch 30/34, Loss=5.314436215162277
Loss made of: CE 0.29896610975265503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.602117538452148 EntMin 0.0
Epoch 6, Class Loss=0.3419884145259857, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.3419884145259857, Class Loss=0.3419884145259857, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=9.498633182048797
Loss made of: CE 1.3829927444458008, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.513803005218506 EntMin 0.0
Epoch 1, Batch 20/33, Loss=8.109618389606476
Loss made of: CE 1.117474913597107, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.206389904022217 EntMin 0.0
Epoch 1, Batch 30/33, Loss=6.878308874368668
Loss made of: CE 0.8129900693893433, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.029707908630371 EntMin 0.0
Epoch 1, Class Loss=1.1274784803390503, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=1.1274784803390503, Class Loss=1.1274784803390503, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/33, Loss=6.333123975992203
Loss made of: CE 0.7633745074272156, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.220457077026367 EntMin 0.0
Epoch 2, Batch 20/33, Loss=6.01528599858284
Loss made of: CE 0.8833722472190857, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.615083694458008 EntMin 0.0
Epoch 2, Batch 30/33, Loss=5.5948160767555235
Loss made of: CE 0.6152379512786865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9437737464904785 EntMin 0.0
Epoch 2, Class Loss=0.7240759134292603, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.7240759134292603, Class Loss=0.7240759134292603, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/33, Loss=5.287321096658706
Loss made of: CE 0.5904002785682678, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.504417419433594 EntMin 0.0
Epoch 3, Batch 20/33, Loss=5.251337206363678
Loss made of: CE 0.5001189708709717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.426597595214844 EntMin 0.0
Epoch 3, Batch 30/33, Loss=5.0857835680246355
Loss made of: CE 0.5046147704124451, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.311256408691406 EntMin 0.0
Epoch 3, Class Loss=0.5560528039932251, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5560528039932251, Class Loss=0.5560528039932251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/33, Loss=4.808700168132782
Loss made of: CE 0.45580893754959106, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9781339168548584 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.893833419680595
Loss made of: CE 0.43019434809684753, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3413262367248535 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.724832147359848
Loss made of: CE 0.347632497549057, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.969674825668335 EntMin 0.0
Epoch 4, Class Loss=0.44774681329727173, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.44774681329727173, Class Loss=0.44774681329727173, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/33, Loss=4.617630419135094
Loss made of: CE 0.46819251775741577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.255963325500488 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.53097111582756
Loss made of: CE 0.3044699430465698, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8682830333709717 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.712802016735077
Loss made of: CE 0.41560348868370056, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.020503044128418 EntMin 0.0
Epoch 5, Class Loss=0.3840525448322296, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.3840525448322296, Class Loss=0.3840525448322296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/33, Loss=4.465538784861565
Loss made of: CE 0.43896484375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.227167129516602 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.505757987499237
Loss made of: CE 0.33159029483795166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9518260955810547 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.372374022006989
Loss made of: CE 0.31363677978515625, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.293989181518555 EntMin 0.0
Epoch 6, Class Loss=0.3428409993648529, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.3428409993648529, Class Loss=0.3428409993648529, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.40679287910461426, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.842394
Mean Acc: 0.507990
FreqW Acc: 0.718930
Mean IoU: 0.387227
Class IoU:
	class 0: 0.84424365
	class 1: 0.77514404
	class 2: 0.32210296
	class 3: 0.57515156
	class 4: 0.56643105
	class 5: 0.0
	class 6: 0.0033610642
	class 7: 0.0001135333
	class 8: 0.39849785
Class Acc:
	class 0: 0.98309547
	class 1: 0.83291817
	class 2: 0.59275967
	class 3: 0.95736563
	class 4: 0.7989745
	class 5: 0.0
	class 6: 0.003361099
	class 7: 0.00011353359
	class 8: 0.40332052

federated global round: 6, step: 1
select part of clients to conduct local training
[11, 5, 8, 12]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/43, Loss=5.340145322680473
Loss made of: CE 0.49067896604537964, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.774170875549316 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.879119482636452
Loss made of: CE 0.6733226180076599, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.951113700866699 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.68306627869606
Loss made of: CE 0.4444058835506439, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.708489418029785 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.602436193823815
Loss made of: CE 0.521564245223999, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.807060956954956 EntMin 0.0
Epoch 1, Class Loss=0.532844603061676, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.532844603061676, Class Loss=0.532844603061676, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/43, Loss=4.4002142429351805
Loss made of: CE 0.39993709325790405, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9053447246551514 EntMin 0.0
Epoch 2, Batch 20/43, Loss=4.241740936040879
Loss made of: CE 0.3749206066131592, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8585593700408936 EntMin 0.0
Epoch 2, Batch 30/43, Loss=4.285044798254967
Loss made of: CE 0.48243802785873413, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.29987096786499 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.372844180464744
Loss made of: CE 0.48426780104637146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.142977714538574 EntMin 0.0
Epoch 2, Class Loss=0.4024299681186676, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.4024299681186676, Class Loss=0.4024299681186676, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/43, Loss=4.080241677165032
Loss made of: CE 0.4255588948726654, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6227564811706543 EntMin 0.0
Epoch 3, Batch 20/43, Loss=4.148954175412655
Loss made of: CE 0.3439274728298187, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.024752616882324 EntMin 0.0
Epoch 3, Batch 30/43, Loss=4.146629965305328
Loss made of: CE 0.40178608894348145, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.695458173751831 EntMin 0.0
Epoch 3, Batch 40/43, Loss=4.088437759876252
Loss made of: CE 0.25841695070266724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8725476264953613 EntMin 0.0
Epoch 3, Class Loss=0.3590949773788452, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.3590949773788452, Class Loss=0.3590949773788452, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/43, Loss=3.9716684833168983
Loss made of: CE 0.4151841998100281, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.097816467285156 EntMin 0.0
Epoch 4, Batch 20/43, Loss=4.030903221666813
Loss made of: CE 0.35003313422203064, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6242246627807617 EntMin 0.0
Epoch 4, Batch 30/43, Loss=4.044825583696365
Loss made of: CE 0.39180636405944824, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.549746513366699 EntMin 0.0
Epoch 4, Batch 40/43, Loss=4.01599805355072
Loss made of: CE 0.3104461133480072, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4068007469177246 EntMin 0.0
Epoch 4, Class Loss=0.3401787281036377, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.3401787281036377, Class Loss=0.3401787281036377, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/43, Loss=3.975541368126869
Loss made of: CE 0.30646812915802, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.923880100250244 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.924847288429737
Loss made of: CE 0.34767913818359375, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.643730640411377 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.9868045061826707
Loss made of: CE 0.3113405406475067, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.661581039428711 EntMin 0.0
Epoch 5, Batch 40/43, Loss=4.006202434003353
Loss made of: CE 0.3305389881134033, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.308727741241455 EntMin 0.0
Epoch 5, Class Loss=0.3139578700065613, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.3139578700065613, Class Loss=0.3139578700065613, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/43, Loss=3.8709306925535203
Loss made of: CE 0.3470328748226166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.423787832260132 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.939559119939804
Loss made of: CE 0.30107447504997253, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.530320167541504 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.8732978612184525
Loss made of: CE 0.22414368391036987, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.275947332382202 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.742787757515907
Loss made of: CE 0.31710925698280334, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4357213973999023 EntMin 0.0
Epoch 6, Class Loss=0.2841638922691345, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.2841638922691345, Class Loss=0.2841638922691345, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=5.768350118398667
Loss made of: CE 0.6527757048606873, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.083290100097656 EntMin 0.0
Epoch 1, Batch 20/34, Loss=5.5983761459589
Loss made of: CE 0.44613027572631836, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.219301700592041 EntMin 0.0
Epoch 1, Batch 30/34, Loss=5.43532187640667
Loss made of: CE 0.381062388420105, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.034197807312012 EntMin 0.0
Epoch 1, Class Loss=0.5079039335250854, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.5079039335250854, Class Loss=0.5079039335250854, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/34, Loss=5.270119994878769
Loss made of: CE 0.3877127170562744, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.833755970001221 EntMin 0.0
Epoch 2, Batch 20/34, Loss=5.046575772762298
Loss made of: CE 0.34282106161117554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.820894718170166 EntMin 0.0
Epoch 2, Batch 30/34, Loss=5.353181859850883
Loss made of: CE 0.43112802505493164, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.625690460205078 EntMin 0.0
Epoch 2, Class Loss=0.3746298849582672, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3746298849582672, Class Loss=0.3746298849582672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/34, Loss=4.84906379878521
Loss made of: CE 0.31631216406822205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.520406246185303 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.997835624217987
Loss made of: CE 0.3226630687713623, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.602071762084961 EntMin 0.0
Epoch 3, Batch 30/34, Loss=5.0962729275226595
Loss made of: CE 0.4323436915874481, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.420463562011719 EntMin 0.0
Epoch 3, Class Loss=0.32384535670280457, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.32384535670280457, Class Loss=0.32384535670280457, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/34, Loss=4.977335911989212
Loss made of: CE 0.3043885827064514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581005573272705 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.993129536509514
Loss made of: CE 0.31937193870544434, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.721349716186523 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.841440683603286
Loss made of: CE 0.308775395154953, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.037623405456543 EntMin 0.0
Epoch 4, Class Loss=0.3135901093482971, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.3135901093482971, Class Loss=0.3135901093482971, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/34, Loss=4.693945881724358
Loss made of: CE 0.2759853005409241, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.663384437561035 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.837473882734775
Loss made of: CE 0.34059152007102966, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.252769470214844 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.748712532222271
Loss made of: CE 0.35025709867477417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9516520500183105 EntMin 0.0
Epoch 5, Class Loss=0.291114866733551, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.291114866733551, Class Loss=0.291114866733551, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/34, Loss=4.8916540294885635
Loss made of: CE 0.35949569940567017, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.241641998291016 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.666762733459473
Loss made of: CE 0.24810129404067993, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.608926296234131 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.6085666254162785
Loss made of: CE 0.4873178005218506, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.146717071533203 EntMin 0.0
Epoch 6, Class Loss=0.2994951903820038, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.2994951903820038, Class Loss=0.2994951903820038, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=5.864819955825806
Loss made of: CE 0.5571581721305847, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.399775981903076 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.711065873503685
Loss made of: CE 0.376575231552124, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.366870403289795 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.676472592353821
Loss made of: CE 0.4819730222225189, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.850737571716309 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.354060563445091
Loss made of: CE 0.30685073137283325, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.356212615966797 EntMin 0.0
Epoch 1, Class Loss=0.4773131012916565, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.4773131012916565, Class Loss=0.4773131012916565, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/40, Loss=5.354949036240578
Loss made of: CE 0.4248201251029968, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.605226516723633 EntMin 0.0
Epoch 2, Batch 20/40, Loss=4.9973388522863385
Loss made of: CE 0.1914551556110382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.491147994995117 EntMin 0.0
Epoch 2, Batch 30/40, Loss=5.3243603348732
Loss made of: CE 0.3992486596107483, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.899901390075684 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.2204119801521305
Loss made of: CE 0.3629167973995209, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.027263164520264 EntMin 0.0
Epoch 2, Class Loss=0.37580588459968567, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.37580588459968567, Class Loss=0.37580588459968567, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/40, Loss=5.055550882220269
Loss made of: CE 0.3011643588542938, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.708555698394775 EntMin 0.0
Epoch 3, Batch 20/40, Loss=4.898490609228611
Loss made of: CE 0.31645190715789795, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4927873611450195 EntMin 0.0
Epoch 3, Batch 30/40, Loss=5.044501999020577
Loss made of: CE 0.45858320593833923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.321374416351318 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.991241610050201
Loss made of: CE 0.30383986234664917, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.094334125518799 EntMin 0.0
Epoch 3, Class Loss=0.33307182788848877, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.33307182788848877, Class Loss=0.33307182788848877, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/40, Loss=4.9150164186954495
Loss made of: CE 0.4097836911678314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.76535701751709 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.798571401834488
Loss made of: CE 0.3174733519554138, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.534695625305176 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.879083463549614
Loss made of: CE 0.327605664730072, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0385541915893555 EntMin 0.0
Epoch 4, Batch 40/40, Loss=4.833706098794937
Loss made of: CE 0.3026922345161438, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2796311378479 EntMin 0.0
Epoch 4, Class Loss=0.3121206760406494, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.3121206760406494, Class Loss=0.3121206760406494, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/40, Loss=4.780734468996525
Loss made of: CE 0.3233267664909363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1982316970825195 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.822671961784363
Loss made of: CE 0.3505154550075531, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.232222080230713 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.6122612491250035
Loss made of: CE 0.3092121183872223, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5741801261901855 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.7840446844697
Loss made of: CE 0.23832611739635468, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.498461723327637 EntMin 0.0
Epoch 5, Class Loss=0.3010755479335785, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.3010755479335785, Class Loss=0.3010755479335785, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/40, Loss=4.561389353871346
Loss made of: CE 0.23869574069976807, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.47801399230957 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.692256876826287
Loss made of: CE 0.26113373041152954, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.181196689605713 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.571609029173851
Loss made of: CE 0.26555272936820984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249438762664795 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.672834242880344
Loss made of: CE 0.24016094207763672, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.094975471496582 EntMin 0.0
Epoch 6, Class Loss=0.2682286202907562, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.2682286202907562, Class Loss=0.2682286202907562, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/40, Loss=5.77432770729065
Loss made of: CE 0.4605184495449066, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4062957763671875 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.673657914996147
Loss made of: CE 0.43963855504989624, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.110227584838867 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.512494233250618
Loss made of: CE 0.5571593046188354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.892266750335693 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.456542906165123
Loss made of: CE 0.3765786588191986, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.068624496459961 EntMin 0.0
Epoch 1, Class Loss=0.4837045669555664, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.4837045669555664, Class Loss=0.4837045669555664, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/40, Loss=5.40320281535387
Loss made of: CE 0.24171338975429535, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389135360717773 EntMin 0.0
Epoch 2, Batch 20/40, Loss=5.225641775131225
Loss made of: CE 0.36006593704223633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.343064308166504 EntMin 0.0
Epoch 2, Batch 30/40, Loss=5.1139612883329395
Loss made of: CE 0.47472071647644043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.503013610839844 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.329981815814972
Loss made of: CE 0.3787810802459717, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.306748390197754 EntMin 0.0
Epoch 2, Class Loss=0.3828139901161194, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.3828139901161194, Class Loss=0.3828139901161194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/40, Loss=5.159069159626961
Loss made of: CE 0.2798098027706146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.56568717956543 EntMin 0.0
Epoch 3, Batch 20/40, Loss=5.091790553927422
Loss made of: CE 0.5126457214355469, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.804097652435303 EntMin 0.0
Epoch 3, Batch 30/40, Loss=5.0564443409442905
Loss made of: CE 0.25963282585144043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.825764179229736 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.93496727347374
Loss made of: CE 0.23825055360794067, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.540229320526123 EntMin 0.0
Epoch 3, Class Loss=0.34745803475379944, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.34745803475379944, Class Loss=0.34745803475379944, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/40, Loss=4.805157482624054
Loss made of: CE 0.3952582776546478, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3909687995910645 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.965213495492935
Loss made of: CE 0.20878180861473083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.232090473175049 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.8738856837153435
Loss made of: CE 0.33867979049682617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.786588668823242 EntMin 0.0
Epoch 4, Batch 40/40, Loss=5.065504348278045
Loss made of: CE 0.3177601099014282, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.860471725463867 EntMin 0.0
Epoch 4, Class Loss=0.3307640254497528, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.3307640254497528, Class Loss=0.3307640254497528, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/40, Loss=4.887521916627884
Loss made of: CE 0.27663862705230713, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.440254211425781 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.728044894337654
Loss made of: CE 0.4008719325065613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.749661922454834 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.821078848838806
Loss made of: CE 0.25843343138694763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.233800411224365 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.774380102753639
Loss made of: CE 0.30195629596710205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.269832611083984 EntMin 0.0
Epoch 5, Class Loss=0.30435582995414734, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.30435582995414734, Class Loss=0.30435582995414734, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/40, Loss=4.626047442853451
Loss made of: CE 0.2679353952407837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.524908065795898 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.797440722584724
Loss made of: CE 0.27870050072669983, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760457992553711 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.868397812545299
Loss made of: CE 0.3227698504924774, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.094870567321777 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.5499377861619
Loss made of: CE 0.24157333374023438, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.224242687225342 EntMin 0.0
Epoch 6, Class Loss=0.2855740487575531, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.2855740487575531, Class Loss=0.2855740487575531, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2874366343021393, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.890227
Mean Acc: 0.602952
FreqW Acc: 0.799045
Mean IoU: 0.515529
Class IoU:
	class 0: 0.88327503
	class 1: 0.78716964
	class 2: 0.25620377
	class 3: 0.72374755
	class 4: 0.6206543
	class 5: 0.017314501
	class 6: 0.50266665
	class 7: 0.10082449
	class 8: 0.747906
Class Acc:
	class 0: 0.98155916
	class 1: 0.88748264
	class 2: 0.3772669
	class 3: 0.9451568
	class 4: 0.78881013
	class 5: 0.017353326
	class 6: 0.5077679
	class 7: 0.101084664
	class 8: 0.82008463

federated global round: 7, step: 1
select part of clients to conduct local training
[0, 6, 13, 5]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/52, Loss=4.483466836810112
Loss made of: CE 0.6281321048736572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.030307769775391 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.202956035733223
Loss made of: CE 0.34683167934417725, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5925204753875732 EntMin 0.0
Epoch 1, Batch 30/52, Loss=4.153839525580406
Loss made of: CE 0.34965047240257263, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6413326263427734 EntMin 0.0
Epoch 1, Batch 40/52, Loss=4.275634947419166
Loss made of: CE 0.43954774737358093, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2317376136779785 EntMin 0.0
Epoch 1, Batch 50/52, Loss=4.117193359136581
Loss made of: CE 0.2694231867790222, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.455040693283081 EntMin 0.0
Epoch 1, Class Loss=0.3974175453186035, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.3974175453186035, Class Loss=0.3974175453186035, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/52, Loss=4.14902698546648
Loss made of: CE 0.36437100172042847, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.684788465499878 EntMin 0.0
Epoch 2, Batch 20/52, Loss=4.036016096174717
Loss made of: CE 0.3402280807495117, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.525261640548706 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.229511013627052
Loss made of: CE 0.4395334720611572, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9562764167785645 EntMin 0.0
Epoch 2, Batch 40/52, Loss=4.162425670027733
Loss made of: CE 0.27421531081199646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4240217208862305 EntMin 0.0
Epoch 2, Batch 50/52, Loss=4.109543691575527
Loss made of: CE 0.358376145362854, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.337674140930176 EntMin 0.0
Epoch 2, Class Loss=0.3252674341201782, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.3252674341201782, Class Loss=0.3252674341201782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/52, Loss=4.08838215470314
Loss made of: CE 0.26715055108070374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.857903480529785 EntMin 0.0
Epoch 3, Batch 20/52, Loss=4.0552658408880236
Loss made of: CE 0.35857900977134705, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6370646953582764 EntMin 0.0
Epoch 3, Batch 30/52, Loss=4.158567804098129
Loss made of: CE 0.3392365574836731, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4515671730041504 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.070827847719192
Loss made of: CE 0.2925277054309845, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6980979442596436 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.079117928445339
Loss made of: CE 0.3269934356212616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5860936641693115 EntMin 0.0
Epoch 3, Class Loss=0.3091767430305481, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3091767430305481, Class Loss=0.3091767430305481, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/52, Loss=4.203084783256054
Loss made of: CE 0.3023698031902313, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6659255027770996 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.8673530519008636
Loss made of: CE 0.23933838307857513, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.676779270172119 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.9037801012396813
Loss made of: CE 0.19376018643379211, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2907958030700684 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.9751965552568436
Loss made of: CE 0.26077598333358765, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.351771354675293 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.778285051882267
Loss made of: CE 0.24201886355876923, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3511667251586914 EntMin 0.0
Epoch 4, Class Loss=0.2787682116031647, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.2787682116031647, Class Loss=0.2787682116031647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/52, Loss=3.9158937618136407
Loss made of: CE 0.23633304238319397, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4559085369110107 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.896826457977295
Loss made of: CE 0.22861348092556, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.413790464401245 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.938919171690941
Loss made of: CE 0.27638426423072815, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6539101600646973 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.8956137716770174
Loss made of: CE 0.29192858934402466, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8839223384857178 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.8521886602044106
Loss made of: CE 0.2681275010108948, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.490276336669922 EntMin 0.0
Epoch 5, Class Loss=0.271115779876709, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.271115779876709, Class Loss=0.271115779876709, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/52, Loss=3.794952630996704
Loss made of: CE 0.2464735209941864, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4048354625701904 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.914593268930912
Loss made of: CE 0.2389940619468689, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2732627391815186 EntMin 0.0
Epoch 6, Batch 30/52, Loss=4.101025445759296
Loss made of: CE 0.37239861488342285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.657411575317383 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.685000756382942
Loss made of: CE 0.2899533808231354, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.409106731414795 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.862489128112793
Loss made of: CE 0.28192266821861267, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.695072889328003 EntMin 0.0
Epoch 6, Class Loss=0.2728908956050873, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.2728908956050873, Class Loss=0.2728908956050873, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/34, Loss=5.364944872260094
Loss made of: CE 0.5653625726699829, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.790225982666016 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.8112840294837955
Loss made of: CE 0.29436981678009033, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.09188175201416 EntMin 0.0
Epoch 1, Batch 30/34, Loss=5.052170234918594
Loss made of: CE 0.3538782596588135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.521812438964844 EntMin 0.0
Epoch 1, Class Loss=0.42795056104660034, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.42795056104660034, Class Loss=0.42795056104660034, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/34, Loss=4.788809651136399
Loss made of: CE 0.3047487735748291, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.618569850921631 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.5877543777227405
Loss made of: CE 0.3146635890007019, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.190523147583008 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.553775408864022
Loss made of: CE 0.2434559464454651, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3840556144714355 EntMin 0.0
Epoch 2, Class Loss=0.3096439242362976, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3096439242362976, Class Loss=0.3096439242362976, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/34, Loss=4.446727742254734
Loss made of: CE 0.29625827074050903, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.049485206604004 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.510145393013954
Loss made of: CE 0.26098334789276123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18357515335083 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.6127600625157354
Loss made of: CE 0.296232134103775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1689863204956055 EntMin 0.0
Epoch 3, Class Loss=0.29496458172798157, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.29496458172798157, Class Loss=0.29496458172798157, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/34, Loss=4.54452945291996
Loss made of: CE 0.29608654975891113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143921852111816 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.54119293987751
Loss made of: CE 0.32266706228256226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.003334045410156 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.604353648424149
Loss made of: CE 0.397955983877182, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.692157745361328 EntMin 0.0
Epoch 4, Class Loss=0.3003270924091339, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3003270924091339, Class Loss=0.3003270924091339, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/34, Loss=4.366759200394154
Loss made of: CE 0.26035940647125244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.719538450241089 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.5393620654940605
Loss made of: CE 0.4888831079006195, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.350247859954834 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.2568552762269976
Loss made of: CE 0.28081560134887695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.036774635314941 EntMin 0.0
Epoch 5, Class Loss=0.29624709486961365, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.29624709486961365, Class Loss=0.29624709486961365, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/34, Loss=4.396293732523918
Loss made of: CE 0.3063809275627136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.222494125366211 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.350682154297829
Loss made of: CE 0.30122634768486023, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.151254177093506 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.391264748573303
Loss made of: CE 0.22487053275108337, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.70060396194458 EntMin 0.0
Epoch 6, Class Loss=0.2778976559638977, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.2778976559638977, Class Loss=0.2778976559638977, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=6.631205433607102
Loss made of: CE 0.6445655226707458, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.13686990737915 EntMin 0.0
Epoch 1, Batch 20/33, Loss=5.619429916143417
Loss made of: CE 0.7387949228286743, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1554388999938965 EntMin 0.0
Epoch 1, Batch 30/33, Loss=5.191190639138222
Loss made of: CE 0.43274515867233276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.50633430480957 EntMin 0.0
Epoch 1, Class Loss=0.6957059502601624, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.6957059502601624, Class Loss=0.6957059502601624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/33, Loss=4.848890632390976
Loss made of: CE 0.41627931594848633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.285490036010742 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.707122099399567
Loss made of: CE 0.29840585589408875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.277683734893799 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.486535549163818
Loss made of: CE 0.42773208022117615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249762058258057 EntMin 0.0
Epoch 2, Class Loss=0.36047014594078064, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.36047014594078064, Class Loss=0.36047014594078064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/33, Loss=4.47325484752655
Loss made of: CE 0.3784361779689789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.631014823913574 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.397613531351089
Loss made of: CE 0.26610469818115234, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.083065032958984 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.2418208360672
Loss made of: CE 0.2725617587566376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8468661308288574 EntMin 0.0
Epoch 3, Class Loss=0.3272254467010498, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.3272254467010498, Class Loss=0.3272254467010498, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/33, Loss=4.157057924568653
Loss made of: CE 0.23195502161979675, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7823214530944824 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.2423945143818855
Loss made of: CE 0.3095179796218872, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.138913154602051 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.137635403871537
Loss made of: CE 0.27686813473701477, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.647766590118408 EntMin 0.0
Epoch 4, Class Loss=0.284817099571228, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.284817099571228, Class Loss=0.284817099571228, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/33, Loss=4.231494411826134
Loss made of: CE 0.2876017689704895, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6350181102752686 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.090995900332928
Loss made of: CE 0.2749052047729492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.591597318649292 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.0090348571538925
Loss made of: CE 0.296190083026886, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.863903760910034 EntMin 0.0
Epoch 5, Class Loss=0.2775357663631439, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.2775357663631439, Class Loss=0.2775357663631439, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/33, Loss=4.067333099246025
Loss made of: CE 0.30492329597473145, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8546299934387207 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.019598081707954
Loss made of: CE 0.19417792558670044, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.571244955062866 EntMin 0.0
Epoch 6, Batch 30/33, Loss=3.8969283640384673
Loss made of: CE 0.290118008852005, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.954719305038452 EntMin 0.0
Epoch 6, Class Loss=0.27019256353378296, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.27019256353378296, Class Loss=0.27019256353378296, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/34, Loss=5.281033301353455
Loss made of: CE 0.5806882381439209, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.895986557006836 EntMin 0.0
Epoch 1, Batch 20/34, Loss=5.086413928866387
Loss made of: CE 0.3834182918071747, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.72875452041626 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.88881955742836
Loss made of: CE 0.3718690574169159, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.472026348114014 EntMin 0.0
Epoch 1, Class Loss=0.4214342534542084, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.4214342534542084, Class Loss=0.4214342534542084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/34, Loss=4.823815166950226
Loss made of: CE 0.301540732383728, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.36000394821167 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.594928821921348
Loss made of: CE 0.32008087635040283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.626622200012207 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.919775316119194
Loss made of: CE 0.3704908788204193, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.21818208694458 EntMin 0.0
Epoch 2, Class Loss=0.32515814900398254, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.32515814900398254, Class Loss=0.32515814900398254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/34, Loss=4.4404885321855545
Loss made of: CE 0.2750021815299988, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.101095199584961 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.562428447604179
Loss made of: CE 0.29318100214004517, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.109261989593506 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.671086552739143
Loss made of: CE 0.33138108253479004, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.102313995361328 EntMin 0.0
Epoch 3, Class Loss=0.2922344207763672, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.2922344207763672, Class Loss=0.2922344207763672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/34, Loss=4.526487290859222
Loss made of: CE 0.2788202166557312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.093784332275391 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.594943375885487
Loss made of: CE 0.31189778447151184, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.377474784851074 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.5316385015845295
Loss made of: CE 0.28594180941581726, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8817343711853027 EntMin 0.0
Epoch 4, Class Loss=0.28571754693984985, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.28571754693984985, Class Loss=0.28571754693984985, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/34, Loss=4.4435616612434385
Loss made of: CE 0.2521495819091797, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.148388862609863 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.529496096074581
Loss made of: CE 0.2817398011684418, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.146581649780273 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.459154200553894
Loss made of: CE 0.29350727796554565, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481431007385254 EntMin 0.0
Epoch 5, Class Loss=0.2784578502178192, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.2784578502178192, Class Loss=0.2784578502178192, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/34, Loss=4.690404644608497
Loss made of: CE 0.30055999755859375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.022241592407227 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.447592981159687
Loss made of: CE 0.1997593641281128, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.237608909606934 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.371133869886398
Loss made of: CE 0.37532883882522583, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6058669090271 EntMin 0.0
Epoch 6, Class Loss=0.28352832794189453, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.28352832794189453, Class Loss=0.28352832794189453, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.24458667635917664, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.925332
Mean Acc: 0.718097
FreqW Acc: 0.863546
Mean IoU: 0.627329
Class IoU:
	class 0: 0.91990083
	class 1: 0.8234643
	class 2: 0.28844205
	class 3: 0.75701994
	class 4: 0.64414436
	class 5: 0.0
	class 6: 0.8408838
	class 7: 0.6052097
	class 8: 0.76689315
Class Acc:
	class 0: 0.9773964
	class 1: 0.86497235
	class 2: 0.46779862
	class 3: 0.9287227
	class 4: 0.83743167
	class 5: 0.0
	class 6: 0.92068565
	class 7: 0.6370015
	class 8: 0.8288606

federated global round: 8, step: 1
select part of clients to conduct local training
[4, 11, 9, 6]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=5.623192217946053
Loss made of: CE 0.5521090030670166, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.651856422424316 EntMin 0.0
Epoch 1, Batch 20/25, Loss=5.1922928392887115
Loss made of: CE 0.4024185538291931, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3874030113220215 EntMin 0.0
Epoch 1, Class Loss=0.5188208818435669, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.5188208818435669, Class Loss=0.5188208818435669, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/25, Loss=4.773737385869026
Loss made of: CE 0.3673297166824341, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.021611213684082 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.659713450074196
Loss made of: CE 0.3963751196861267, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.520207405090332 EntMin 0.0
Epoch 2, Class Loss=0.35878294706344604, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.35878294706344604, Class Loss=0.35878294706344604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/25, Loss=4.546388851106167
Loss made of: CE 0.3254166841506958, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1394805908203125 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.363580119609833
Loss made of: CE 0.34403762221336365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.116369247436523 EntMin 0.0
Epoch 3, Class Loss=0.33184584975242615, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.33184584975242615, Class Loss=0.33184584975242615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/25, Loss=4.443482086062431
Loss made of: CE 0.267170250415802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2252044677734375 EntMin 0.0
Epoch 4, Batch 20/25, Loss=4.256936538219452
Loss made of: CE 0.22090421617031097, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.117218017578125 EntMin 0.0
Epoch 4, Class Loss=0.2831537425518036, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.2831537425518036, Class Loss=0.2831537425518036, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/25, Loss=4.256063398718834
Loss made of: CE 0.2268078774213791, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8146731853485107 EntMin 0.0
Epoch 5, Batch 20/25, Loss=4.424818201363086
Loss made of: CE 0.5064939260482788, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.741781234741211 EntMin 0.0
Epoch 5, Class Loss=0.2954196333885193, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.2954196333885193, Class Loss=0.2954196333885193, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/25, Loss=4.1194511413574215
Loss made of: CE 0.22957584261894226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.014491081237793 EntMin 0.0
Epoch 6, Batch 20/25, Loss=4.187397193908692
Loss made of: CE 0.28821122646331787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.293115615844727 EntMin 0.0
Epoch 6, Class Loss=0.24238547682762146, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.24238547682762146, Class Loss=0.24238547682762146, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/43, Loss=5.130391845107079
Loss made of: CE 0.4747161865234375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.626934051513672 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.593155819177627
Loss made of: CE 0.5201334953308105, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.900669813156128 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.3938086241483685
Loss made of: CE 0.31375715136528015, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.403467178344727 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.229839536547661
Loss made of: CE 0.3920762240886688, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.571197271347046 EntMin 0.0
Epoch 1, Class Loss=0.4377404451370239, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.4377404451370239, Class Loss=0.4377404451370239, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/43, Loss=4.125790724158287
Loss made of: CE 0.32185831665992737, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.613628387451172 EntMin 0.0
Epoch 2, Batch 20/43, Loss=4.028879287838936
Loss made of: CE 0.30790263414382935, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9009623527526855 EntMin 0.0
Epoch 2, Batch 30/43, Loss=3.9971154645085334
Loss made of: CE 0.42560479044914246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.133876323699951 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.172700643539429
Loss made of: CE 0.36948829889297485, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9281821250915527 EntMin 0.0
Epoch 2, Class Loss=0.3277059495449066, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.3277059495449066, Class Loss=0.3277059495449066, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/43, Loss=3.840203155577183
Loss made of: CE 0.3213613033294678, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.353377103805542 EntMin 0.0
Epoch 3, Batch 20/43, Loss=3.9885678976774215
Loss made of: CE 0.2823130786418915, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.933109760284424 EntMin 0.0
Epoch 3, Batch 30/43, Loss=3.9737326741218566
Loss made of: CE 0.3171091675758362, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.588202476501465 EntMin 0.0
Epoch 3, Batch 40/43, Loss=3.903454791009426
Loss made of: CE 0.21323049068450928, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7134313583374023 EntMin 0.0
Epoch 3, Class Loss=0.29867643117904663, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.29867643117904663, Class Loss=0.29867643117904663, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/43, Loss=3.7883019551634787
Loss made of: CE 0.25923478603363037, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9122769832611084 EntMin 0.0
Epoch 4, Batch 20/43, Loss=3.838368962705135
Loss made of: CE 0.28645414113998413, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3716650009155273 EntMin 0.0
Epoch 4, Batch 30/43, Loss=3.836227697134018
Loss made of: CE 0.3389663100242615, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5064239501953125 EntMin 0.0
Epoch 4, Batch 40/43, Loss=3.8035706385970114
Loss made of: CE 0.27465352416038513, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2768399715423584 EntMin 0.0
Epoch 4, Class Loss=0.27631866931915283, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.27631866931915283, Class Loss=0.27631866931915283, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/43, Loss=3.7818178206682207
Loss made of: CE 0.2507014274597168, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.841028928756714 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.7603549346327783
Loss made of: CE 0.29689568281173706, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.707638740539551 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.8167267382144927
Loss made of: CE 0.23957835137844086, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5591416358947754 EntMin 0.0
Epoch 5, Batch 40/43, Loss=3.818218283355236
Loss made of: CE 0.2639494240283966, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1405551433563232 EntMin 0.0
Epoch 5, Class Loss=0.2678087055683136, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.2678087055683136, Class Loss=0.2678087055683136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/43, Loss=3.7572558164596557
Loss made of: CE 0.27563607692718506, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.248936176300049 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.7819340780377386
Loss made of: CE 0.23334407806396484, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6030712127685547 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.7250789001584055
Loss made of: CE 0.2081364095211029, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.246544599533081 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.5499288737773895
Loss made of: CE 0.2508091330528259, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.291614532470703 EntMin 0.0
Epoch 6, Class Loss=0.2513926327228546, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.2513926327228546, Class Loss=0.2513926327228546, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/52, Loss=3.9163137644529344
Loss made of: CE 0.4093427360057831, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6873490810394287 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.240357887744904
Loss made of: CE 0.35910290479660034, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.827303409576416 EntMin 0.0
Epoch 1, Batch 30/52, Loss=4.079719014465809
Loss made of: CE 0.34629958868026733, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.828922748565674 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.864442214369774
Loss made of: CE 0.3286367952823639, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4655842781066895 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.945697323977947
Loss made of: CE 0.2942377030849457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.986077308654785 EntMin 0.0
Epoch 1, Class Loss=0.3221939206123352, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.3221939206123352, Class Loss=0.3221939206123352, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/52, Loss=3.954752431809902
Loss made of: CE 0.29346969723701477, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6267812252044678 EntMin 0.0
Epoch 2, Batch 20/52, Loss=4.031370580196381
Loss made of: CE 0.30160677433013916, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6242024898529053 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.143510614335537
Loss made of: CE 0.24736562371253967, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.466606855392456 EntMin 0.0
Epoch 2, Batch 40/52, Loss=4.0179302304983135
Loss made of: CE 0.24606585502624512, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5276026725769043 EntMin 0.0
Epoch 2, Batch 50/52, Loss=4.017067196965217
Loss made of: CE 0.2480764240026474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.13151741027832 EntMin 0.0
Epoch 2, Class Loss=0.29580962657928467, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.29580962657928467, Class Loss=0.29580962657928467, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/52, Loss=3.9959731847047806
Loss made of: CE 0.27132949233055115, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7892494201660156 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.953286683559418
Loss made of: CE 0.37402427196502686, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.456308364868164 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.947452922165394
Loss made of: CE 0.313019335269928, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.520284652709961 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.073243905603886
Loss made of: CE 0.2750055491924286, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8881824016571045 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.110228499770164
Loss made of: CE 0.24776574969291687, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8381316661834717 EntMin 0.0
Epoch 3, Class Loss=0.2825060784816742, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.2825060784816742, Class Loss=0.2825060784816742, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/52, Loss=4.001510141789913
Loss made of: CE 0.26442694664001465, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.602820873260498 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.919215078651905
Loss made of: CE 0.3108886480331421, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6540591716766357 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.844694918394089
Loss made of: CE 0.257779985666275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.493462085723877 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.9991868853569033
Loss made of: CE 0.28911125659942627, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.711697578430176 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.846845918893814
Loss made of: CE 0.19696533679962158, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.971423864364624 EntMin 0.0
Epoch 4, Class Loss=0.2739585041999817, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.2739585041999817, Class Loss=0.2739585041999817, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/52, Loss=3.6471291363239287
Loss made of: CE 0.25100430846214294, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.504101276397705 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.9347181648015974
Loss made of: CE 0.2528875470161438, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7247369289398193 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.7139225259423254
Loss made of: CE 0.2606763243675232, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6800363063812256 EntMin 0.0
Epoch 5, Batch 40/52, Loss=4.023327645659447
Loss made of: CE 0.24295590817928314, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6436119079589844 EntMin 0.0
Epoch 5, Batch 50/52, Loss=4.043491657078266
Loss made of: CE 0.2993132770061493, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.19851016998291 EntMin 0.0
Epoch 5, Class Loss=0.26553529500961304, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.26553529500961304, Class Loss=0.26553529500961304, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/52, Loss=3.8503233700990678
Loss made of: CE 0.174931138753891, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.439729690551758 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.847661578655243
Loss made of: CE 0.2609633803367615, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9143362045288086 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.834554924070835
Loss made of: CE 0.21002429723739624, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5194334983825684 EntMin 0.0
Epoch 6, Batch 40/52, Loss=4.006969784200192
Loss made of: CE 0.3265025019645691, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.673447608947754 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.8965762734413145
Loss made of: CE 0.22248709201812744, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7815282344818115 EntMin 0.0
Epoch 6, Class Loss=0.2619786858558655, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.2619786858558655, Class Loss=0.2619786858558655, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/34, Loss=4.865418304502964
Loss made of: CE 0.3908504843711853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.11241340637207 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.404836073517799
Loss made of: CE 0.2561078667640686, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.593090057373047 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.740150693058967
Loss made of: CE 0.30477213859558105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.031262397766113 EntMin 0.0
Epoch 1, Class Loss=0.3528601825237274, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.3528601825237274, Class Loss=0.3528601825237274, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/34, Loss=4.511201649904251
Loss made of: CE 0.30346015095710754, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.312554359436035 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.306896813213825
Loss made of: CE 0.28367990255355835, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.978680372238159 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.405692917108536
Loss made of: CE 0.3189557194709778, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.642907619476318 EntMin 0.0
Epoch 2, Class Loss=0.29581430554389954, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.29581430554389954, Class Loss=0.29581430554389954, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/34, Loss=4.248491735756398
Loss made of: CE 0.2724047303199768, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.684030294418335 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.23926665186882
Loss made of: CE 0.22961369156837463, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.784444570541382 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.367657513916493
Loss made of: CE 0.23439686000347137, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.765117645263672 EntMin 0.0
Epoch 3, Class Loss=0.28215160965919495, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.28215160965919495, Class Loss=0.28215160965919495, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/34, Loss=4.266807718575
Loss made of: CE 0.27376070618629456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9249846935272217 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.284117288887501
Loss made of: CE 0.2948457598686218, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8209331035614014 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.363040919601917
Loss made of: CE 0.3255137503147125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.536526679992676 EntMin 0.0
Epoch 4, Class Loss=0.27971822023391724, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.27971822023391724, Class Loss=0.27971822023391724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/34, Loss=4.165333119034767
Loss made of: CE 0.23642639815807343, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4650943279266357 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.3758603692054745
Loss made of: CE 0.422789990901947, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.336543560028076 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.016751585900783
Loss made of: CE 0.25065839290618896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7825584411621094 EntMin 0.0
Epoch 5, Class Loss=0.2746730446815491, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.2746730446815491, Class Loss=0.2746730446815491, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/34, Loss=4.189562644064426
Loss made of: CE 0.3083558976650238, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9588775634765625 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.124245563149453
Loss made of: CE 0.31057730317115784, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.103878021240234 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.27696118503809
Loss made of: CE 0.21942530572414398, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.516045093536377 EntMin 0.0
Epoch 6, Class Loss=0.2647499442100525, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.2647499442100525, Class Loss=0.2647499442100525, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2158980816602707, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.931790
Mean Acc: 0.761911
FreqW Acc: 0.877256
Mean IoU: 0.653931
Class IoU:
	class 0: 0.9272231
	class 1: 0.81457746
	class 2: 0.32012585
	class 3: 0.68693215
	class 4: 0.6493201
	class 5: 0.07634668
	class 6: 0.8812635
	class 7: 0.7396726
	class 8: 0.7899137
Class Acc:
	class 0: 0.97277963
	class 1: 0.8607798
	class 2: 0.5669454
	class 3: 0.96103644
	class 4: 0.8408559
	class 5: 0.07712497
	class 6: 0.9181559
	class 7: 0.82396555
	class 8: 0.8355595

federated global round: 9, step: 1
select part of clients to conduct local training
[5, 0, 9, 2]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/34, Loss=5.075008744001389
Loss made of: CE 0.38685375452041626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.396231174468994 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.882026156783104
Loss made of: CE 0.4067695140838623, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.574530601501465 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.754995372891426
Loss made of: CE 0.410936176776886, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.349300384521484 EntMin 0.0
Epoch 1, Class Loss=0.39971354603767395, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.39971354603767395, Class Loss=0.39971354603767395, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/34, Loss=4.629430410265923
Loss made of: CE 0.30379804968833923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.137434959411621 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.245843771100044
Loss made of: CE 0.295421838760376, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.308648109436035 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.703833426535129
Loss made of: CE 0.3986750543117523, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.065154075622559 EntMin 0.0
Epoch 2, Class Loss=0.3073709309101105, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3073709309101105, Class Loss=0.3073709309101105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/34, Loss=4.200171299278736
Loss made of: CE 0.2510378360748291, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.909383535385132 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.253996229171753
Loss made of: CE 0.32527273893356323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.881467819213867 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.491524034738541
Loss made of: CE 0.330231249332428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9004693031311035 EntMin 0.0
Epoch 3, Class Loss=0.2738298773765564, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.2738298773765564, Class Loss=0.2738298773765564, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/34, Loss=4.34410543590784
Loss made of: CE 0.29339203238487244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8804900646209717 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.389103165268898
Loss made of: CE 0.3157433569431305, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.340536594390869 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.273379893600941
Loss made of: CE 0.26732492446899414, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4860668182373047 EntMin 0.0
Epoch 4, Class Loss=0.277621328830719, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.277621328830719, Class Loss=0.277621328830719, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/34, Loss=4.178769959509372
Loss made of: CE 0.2275286614894867, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0604939460754395 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.292694790661335
Loss made of: CE 0.2621162533760071, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8687169551849365 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.269085739552975
Loss made of: CE 0.27887243032455444, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.394572734832764 EntMin 0.0
Epoch 5, Class Loss=0.26701679825782776, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.26701679825782776, Class Loss=0.26701679825782776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/34, Loss=4.44557137042284
Loss made of: CE 0.29162564873695374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6382946968078613 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.108415479958057
Loss made of: CE 0.21053777635097504, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.097953796386719 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.084180200099945
Loss made of: CE 0.33934420347213745, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.429403305053711 EntMin 0.0
Epoch 6, Class Loss=0.26570430397987366, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.26570430397987366, Class Loss=0.26570430397987366, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/52, Loss=4.133332431316376
Loss made of: CE 0.43084150552749634, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5031142234802246 EntMin 0.0
Epoch 1, Batch 20/52, Loss=3.630717448890209
Loss made of: CE 0.27303773164749146, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2438645362854004 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.743003112077713
Loss made of: CE 0.30269333720207214, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.269026756286621 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.8454697102308275
Loss made of: CE 0.2705339789390564, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.838341474533081 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.8192245468497275
Loss made of: CE 0.21572080254554749, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2259931564331055 EntMin 0.0
Epoch 1, Class Loss=0.2988777756690979, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.2988777756690979, Class Loss=0.2988777756690979, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/52, Loss=3.8398487895727156
Loss made of: CE 0.35746270418167114, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4690892696380615 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.6785032898187637
Loss made of: CE 0.24707815051078796, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2229390144348145 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.9400089755654335
Loss made of: CE 0.3077356815338135, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8278112411499023 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.7771012395620347
Loss made of: CE 0.26406049728393555, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1760830879211426 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.7620017781853674
Loss made of: CE 0.2861520051956177, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.013235092163086 EntMin 0.0
Epoch 2, Class Loss=0.27741366624832153, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.27741366624832153, Class Loss=0.27741366624832153, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/52, Loss=3.760362060368061
Loss made of: CE 0.215835303068161, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.577030897140503 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.731986719369888
Loss made of: CE 0.2213788479566574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.406034231185913 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.816906712949276
Loss made of: CE 0.2869982123374939, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.223741054534912 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.7765356063842774
Loss made of: CE 0.296418696641922, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4171013832092285 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.804364803433418
Loss made of: CE 0.27949172258377075, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.364442825317383 EntMin 0.0
Epoch 3, Class Loss=0.2704383432865143, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.2704383432865143, Class Loss=0.2704383432865143, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/52, Loss=3.9610349029302596
Loss made of: CE 0.2729528248310089, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3675670623779297 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.6069453611969946
Loss made of: CE 0.21987943351268768, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3868813514709473 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.664054608345032
Loss made of: CE 0.1607370674610138, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.099792003631592 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.720712475478649
Loss made of: CE 0.2728724181652069, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.211261749267578 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.5437862455844877
Loss made of: CE 0.25147300958633423, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3083009719848633 EntMin 0.0
Epoch 4, Class Loss=0.26321083307266235, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.26321083307266235, Class Loss=0.26321083307266235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/52, Loss=3.705311764776707
Loss made of: CE 0.21821936964988708, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.393488883972168 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.6025004237890244
Loss made of: CE 0.2031700313091278, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.383936882019043 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.6071822956204413
Loss made of: CE 0.26612886786460876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.345074415206909 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.607891894876957
Loss made of: CE 0.267875075340271, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.693324565887451 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.538647969067097
Loss made of: CE 0.2813607156276703, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1088900566101074 EntMin 0.0
Epoch 5, Class Loss=0.249259814620018, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.249259814620018, Class Loss=0.249259814620018, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/52, Loss=3.4967036083340646
Loss made of: CE 0.23579105734825134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.076605796813965 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.6403465047478676
Loss made of: CE 0.22743980586528778, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2192931175231934 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.7349939361214637
Loss made of: CE 0.31093648076057434, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2712514400482178 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.4280648574233057
Loss made of: CE 0.2920389771461487, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.176281452178955 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.5883909434080126
Loss made of: CE 0.29270410537719727, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3917527198791504 EntMin 0.0
Epoch 6, Class Loss=0.25539445877075195, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.25539445877075195, Class Loss=0.25539445877075195, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/52, Loss=3.8499889016151427
Loss made of: CE 0.4022699296474457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.544210433959961 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.218410438299179
Loss made of: CE 0.32596224546432495, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8354721069335938 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.9523900598287582
Loss made of: CE 0.3049912452697754, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6455323696136475 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.69394773542881
Loss made of: CE 0.3111661672592163, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3553225994110107 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.8327041178941728
Loss made of: CE 0.24321269989013672, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.90621280670166 EntMin 0.0
Epoch 1, Class Loss=0.3118153512477875, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.3118153512477875, Class Loss=0.3118153512477875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/52, Loss=3.8517280519008636
Loss made of: CE 0.29044198989868164, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.504967212677002 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.8933706283569336
Loss made of: CE 0.28100234270095825, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3498263359069824 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.9672585904598234
Loss made of: CE 0.25037717819213867, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2473933696746826 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.7659454166889192
Loss made of: CE 0.22183643281459808, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.329859733581543 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.7619360774755477
Loss made of: CE 0.2175520956516266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.053300857543945 EntMin 0.0
Epoch 2, Class Loss=0.2834930121898651, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.2834930121898651, Class Loss=0.2834930121898651, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/52, Loss=3.775011320412159
Loss made of: CE 0.2646719515323639, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5147504806518555 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 20/52, Loss=3.8924335315823555
Loss made of: CE 0.33857035636901855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.64479923248291 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.7023666575551033
Loss made of: CE 0.26173925399780273, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3040874004364014 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.8757105588912966
Loss made of: CE 0.36140963435173035, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.772067070007324 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.9249393314123155
Loss made of: CE 0.23541581630706787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.69537615776062 EntMin 0.0
Epoch 3, Class Loss=0.27877771854400635, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.27877771854400635, Class Loss=0.27877771854400635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/52, Loss=3.8435432747006417
Loss made of: CE 0.26153892278671265, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4245834350585938 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.701560679078102
Loss made of: CE 0.3487675189971924, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.364788770675659 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.7494759619235993
Loss made of: CE 0.265554279088974, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5024871826171875 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.8864378318190576
Loss made of: CE 0.265658974647522, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5244641304016113 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.5742313623428346
Loss made of: CE 0.2095767706632614, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6113901138305664 EntMin 0.0
Epoch 4, Class Loss=0.27411097288131714, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.27411097288131714, Class Loss=0.27411097288131714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/52, Loss=3.5138443380594255
Loss made of: CE 0.2304474413394928, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3750510215759277 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.8117484152317047
Loss made of: CE 0.23045673966407776, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6448373794555664 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.483688709139824
Loss made of: CE 0.257752001285553, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.257251262664795 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.956275525689125
Loss made of: CE 0.2568970322608948, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7217519283294678 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.835652719438076
Loss made of: CE 0.30216965079307556, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.220370292663574 EntMin 0.0
Epoch 5, Class Loss=0.2623361647129059, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.2623361647129059, Class Loss=0.2623361647129059, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/52, Loss=3.658404666185379
Loss made of: CE 0.2396131008863449, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3125991821289062 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.6386360108852385
Loss made of: CE 0.22635528445243835, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.754749298095703 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.61262241601944
Loss made of: CE 0.19653752446174622, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.141702175140381 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.7632986277341844
Loss made of: CE 0.30750572681427, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.505171298980713 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.6533623710274696
Loss made of: CE 0.20010074973106384, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4611566066741943 EntMin 0.0
Epoch 6, Class Loss=0.2622441351413727, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.2622441351413727, Class Loss=0.2622441351413727, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=5.002323618531227
Loss made of: CE 0.42197173833847046, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.262183666229248 EntMin 0.0
Epoch 1, Batch 20/25, Loss=4.522783495485783
Loss made of: CE 0.4840414524078369, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.345958709716797 EntMin 0.0
Epoch 1, Class Loss=0.3892931342124939, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.3892931342124939, Class Loss=0.3892931342124939, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/25, Loss=4.3310852378606794
Loss made of: CE 0.3044692873954773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7335410118103027 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.392250582575798
Loss made of: CE 0.22738337516784668, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.072572708129883 EntMin 0.0
Epoch 2, Class Loss=0.30684012174606323, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.30684012174606323, Class Loss=0.30684012174606323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/25, Loss=4.1574774727225305
Loss made of: CE 0.2664872407913208, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8584556579589844 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.379693101346493
Loss made of: CE 0.23138479888439178, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7360095977783203 EntMin 0.0
Epoch 3, Class Loss=0.27750808000564575, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.27750808000564575, Class Loss=0.27750808000564575, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/25, Loss=4.173511286079884
Loss made of: CE 0.25773078203201294, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.618089437484741 EntMin 0.0
Epoch 4, Batch 20/25, Loss=4.156327760219574
Loss made of: CE 0.22176143527030945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4977309703826904 EntMin 0.0
Epoch 4, Class Loss=0.26446181535720825, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.26446181535720825, Class Loss=0.26446181535720825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/25, Loss=4.046722589433193
Loss made of: CE 0.16103805601596832, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6000633239746094 EntMin 0.0
Epoch 5, Batch 20/25, Loss=4.06388519257307
Loss made of: CE 0.24337951838970184, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6225852966308594 EntMin 0.0
Epoch 5, Class Loss=0.23521846532821655, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.23521846532821655, Class Loss=0.23521846532821655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/25, Loss=3.9953330412507055
Loss made of: CE 0.29468315839767456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7622828483581543 EntMin 0.0
Epoch 6, Batch 20/25, Loss=4.047332617640495
Loss made of: CE 0.2601203918457031, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.815427303314209 EntMin 0.0
Epoch 6, Class Loss=0.2495259791612625, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.2495259791612625, Class Loss=0.2495259791612625, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.20599903166294098, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.935786
Mean Acc: 0.787190
FreqW Acc: 0.884558
Mean IoU: 0.675886
Class IoU:
	class 0: 0.93019485
	class 1: 0.8289419
	class 2: 0.32728097
	class 3: 0.72464246
	class 4: 0.6484389
	class 5: 0.17585477
	class 6: 0.8870308
	class 7: 0.7487494
	class 8: 0.8118395
Class Acc:
	class 0: 0.9690114
	class 1: 0.8838832
	class 2: 0.5772454
	class 3: 0.9474998
	class 4: 0.85676795
	class 5: 0.17892197
	class 6: 0.92376673
	class 7: 0.8641697
	class 8: 0.8834434

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[0, 15, 1, 4]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=8.357540786266327
Loss made of: CE 1.328794002532959, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.589146614074707 EntMin 0.0
Epoch 1, Class Loss=1.4251551628112793, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=1.4251551628112793, Class Loss=1.4251551628112793, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.330094712972641
Loss made of: CE 0.939868152141571, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.237425804138184 EntMin 0.0
Epoch 2, Class Loss=0.9518466591835022, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.9518466591835022, Class Loss=0.9518466591835022, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.8581562519073485
Loss made of: CE 0.9499963521957397, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.597265243530273 EntMin 0.0
Epoch 3, Class Loss=0.8649149537086487, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.8649149537086487, Class Loss=0.8649149537086487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.616820454597473
Loss made of: CE 0.8643196821212769, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3390631675720215 EntMin 0.0
Epoch 4, Class Loss=0.8101423978805542, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.8101423978805542, Class Loss=0.8101423978805542, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.2414894044399265
Loss made of: CE 0.7979859113693237, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.567013263702393 EntMin 0.0
Epoch 5, Class Loss=0.7478911876678467, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.7478911876678467, Class Loss=0.7478911876678467, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.311052513122559
Loss made of: CE 0.655437171459198, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.431088447570801 EntMin 0.0
Epoch 6, Class Loss=0.6936729550361633, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.6936729550361633, Class Loss=0.6936729550361633, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=8.213455510139465
Loss made of: CE 1.3002091646194458, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.515147686004639 EntMin 0.0
Epoch 1, Class Loss=1.427857756614685, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=1.427857756614685, Class Loss=1.427857756614685, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.542719745635987
Loss made of: CE 0.755560040473938, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.751430511474609 EntMin 0.0
Epoch 2, Class Loss=0.9586188793182373, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.9586188793182373, Class Loss=0.9586188793182373, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.87725476026535
Loss made of: CE 1.0346801280975342, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.85816764831543 EntMin 0.0
Epoch 3, Class Loss=0.8751800060272217, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.8751800060272217, Class Loss=0.8751800060272217, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.585524600744248
Loss made of: CE 0.8598145246505737, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.761396408081055 EntMin 0.0
Epoch 4, Class Loss=0.8140870928764343, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.8140870928764343, Class Loss=0.8140870928764343, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.336657619476318
Loss made of: CE 0.6634711027145386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8719682693481445 EntMin 0.0
Epoch 5, Class Loss=0.7666208744049072, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.7666208744049072, Class Loss=0.7666208744049072, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.252343374490738
Loss made of: CE 0.664922297000885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.011031150817871 EntMin 0.0
Epoch 6, Class Loss=0.7036498785018921, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.7036498785018921, Class Loss=0.7036498785018921, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=9.70761696100235
Loss made of: CE 1.3903716802597046, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.627924919128418 EntMin 0.0
Epoch 1, Batch 20/35, Loss=7.841426908969879
Loss made of: CE 0.9448075294494629, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.182229518890381 EntMin 0.0
Epoch 1, Batch 30/35, Loss=7.242151391506195
Loss made of: CE 0.8931500315666199, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.615991115570068 EntMin 0.0
Epoch 1, Class Loss=1.2087386846542358, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=1.2087386846542358, Class Loss=1.2087386846542358, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/35, Loss=6.567467957735062
Loss made of: CE 0.6752964854240417, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.870458126068115 EntMin 0.0
Epoch 2, Batch 20/35, Loss=6.594292271137237
Loss made of: CE 0.8541034460067749, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.275367736816406 EntMin 0.0
Epoch 2, Batch 30/35, Loss=6.21776322722435
Loss made of: CE 0.5365522503852844, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.935810089111328 EntMin 0.0
Epoch 2, Class Loss=0.6929751038551331, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.6929751038551331, Class Loss=0.6929751038551331, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/35, Loss=5.892835587263107
Loss made of: CE 0.566734254360199, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.154191017150879 EntMin 0.0
Epoch 3, Batch 20/35, Loss=6.107766819000244
Loss made of: CE 0.44905492663383484, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.650862693786621 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.869207280874252
Loss made of: CE 0.5341864824295044, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.728460311889648 EntMin 0.0
Epoch 3, Class Loss=0.55791836977005, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.55791836977005, Class Loss=0.55791836977005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/35, Loss=5.705677780508995
Loss made of: CE 0.4073711335659027, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.459031581878662 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.5896416991949085
Loss made of: CE 0.38323089480400085, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.960507869720459 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.7602653950452805
Loss made of: CE 0.4720652997493744, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.22088098526001 EntMin 0.0
Epoch 4, Class Loss=0.48201072216033936, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.48201072216033936, Class Loss=0.48201072216033936, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/35, Loss=5.527712491154671
Loss made of: CE 0.3831635117530823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9214768409729 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.679260551929474
Loss made of: CE 0.457223504781723, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.347140312194824 EntMin 0.0
Epoch 5, Batch 30/35, Loss=5.1739938169717785
Loss made of: CE 0.3241578936576843, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.531379699707031 EntMin 0.0
Epoch 5, Class Loss=0.4275001883506775, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4275001883506775, Class Loss=0.4275001883506775, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/35, Loss=5.275730937719345
Loss made of: CE 0.29561758041381836, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.893477916717529 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.394991001486778
Loss made of: CE 0.39001700282096863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.933784484863281 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.616334468126297
Loss made of: CE 0.46127328276634216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.703493118286133 EntMin 0.0
Epoch 6, Class Loss=0.4171810448169708, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.4171810448169708, Class Loss=0.4171810448169708, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=8.270172345638276
Loss made of: CE 1.4034160375595093, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.240603446960449 EntMin 0.0
Epoch 1, Class Loss=1.407086968421936, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=1.407086968421936, Class Loss=1.407086968421936, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.472014945745468
Loss made of: CE 0.8668352961540222, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.195153713226318 EntMin 0.0
Epoch 2, Class Loss=0.9447822570800781, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.9447822570800781, Class Loss=0.9447822570800781, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.995348310470581
Loss made of: CE 0.7836165428161621, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.458209037780762 EntMin 0.0
Epoch 3, Class Loss=0.8848444223403931, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.8848444223403931, Class Loss=0.8848444223403931, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.5583627104759215
Loss made of: CE 0.8126184940338135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.438492298126221 EntMin 0.0
Epoch 4, Class Loss=0.8041795492172241, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.8041795492172241, Class Loss=0.8041795492172241, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.399771744012833
Loss made of: CE 0.8188117742538452, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.486976623535156 EntMin 0.0
Epoch 5, Class Loss=0.7542008757591248, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.7542008757591248, Class Loss=0.7542008757591248, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.292751449346542
Loss made of: CE 0.6292129755020142, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.519360542297363 EntMin 0.0
Epoch 6, Class Loss=0.7105072736740112, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.7105072736740112, Class Loss=0.7105072736740112, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5449151992797852, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.857364
Mean Acc: 0.471083
FreqW Acc: 0.746483
Mean IoU: 0.388427
Class IoU:
	class 0: 0.8634576
	class 1: 0.54617304
	class 2: 0.21836527
	class 3: 0.45288062
	class 4: 0.60854185
	class 5: 0.1658944
	class 6: 0.7628389
	class 7: 0.64456993
	class 8: 0.6376026
	class 9: 0.0
	class 10: 0.14922898
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.9849018
	class 1: 0.5494575
	class 2: 0.2981763
	class 3: 0.94465375
	class 4: 0.737416
	class 5: 0.16778407
	class 6: 0.77889043
	class 7: 0.66258925
	class 8: 0.8504606
	class 9: 0.0
	class 10: 0.14975251
	class 11: 0.0
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[1, 13, 11, 12]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/35, Loss=7.605131340026856
Loss made of: CE 1.0285396575927734, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.664754390716553 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.5901425123214725
Loss made of: CE 0.7395541071891785, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1184000968933105 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.301491886377335
Loss made of: CE 0.7560415267944336, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.669097900390625 EntMin 0.0
Epoch 1, Class Loss=0.8939781785011292, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.8939781785011292, Class Loss=0.8939781785011292, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/35, Loss=5.895135143399239
Loss made of: CE 0.5450812578201294, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.108489513397217 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.986016702651978
Loss made of: CE 0.6391217708587646, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.818839073181152 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.718836453557015
Loss made of: CE 0.43448352813720703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.390921592712402 EntMin 0.0
Epoch 2, Class Loss=0.5433451533317566, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.5433451533317566, Class Loss=0.5433451533317566, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/35, Loss=5.392516583204269
Loss made of: CE 0.45906123518943787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.976846694946289 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.650089383125305
Loss made of: CE 0.3871699273586273, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.397322654724121 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.488131985068321
Loss made of: CE 0.4955763816833496, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3865180015563965 EntMin 0.0
Epoch 3, Class Loss=0.43591126799583435, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.43591126799583435, Class Loss=0.43591126799583435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/35, Loss=5.432658219337464
Loss made of: CE 0.3927299380302429, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.098419666290283 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.2324493616819385
Loss made of: CE 0.3402716815471649, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.773009777069092 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.416961663961411
Loss made of: CE 0.34995341300964355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.848705768585205 EntMin 0.0
Epoch 4, Class Loss=0.3908978998661041, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.3908978998661041, Class Loss=0.3908978998661041, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/35, Loss=5.3469956666231155
Loss made of: CE 0.33249929547309875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.72053337097168 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.414594441652298
Loss made of: CE 0.3868538737297058, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.031440734863281 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.9317154586315155
Loss made of: CE 0.29123982787132263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5442609786987305 EntMin 0.0
Epoch 5, Class Loss=0.36514729261398315, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.36514729261398315, Class Loss=0.36514729261398315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/35, Loss=4.990748745203018
Loss made of: CE 0.2612227201461792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.816920280456543 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.2463963508605955
Loss made of: CE 0.30718594789505005, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3934407234191895 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.383737456798554
Loss made of: CE 0.4521549344062805, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.411418914794922 EntMin 0.0
Epoch 6, Class Loss=0.36505165696144104, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.36505165696144104, Class Loss=0.36505165696144104, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=5.091885733604431
Loss made of: CE 0.6795822978019714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500596523284912 EntMin 0.0
Epoch 1, Class Loss=0.7208597660064697, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.7208597660064697, Class Loss=0.7208597660064697, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/19, Loss=5.175424432754516
Loss made of: CE 0.6586487889289856, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.902960777282715 EntMin 0.0
Epoch 2, Class Loss=0.6723374128341675, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.6723374128341675, Class Loss=0.6723374128341675, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/19, Loss=4.980715328454972
Loss made of: CE 0.5182623863220215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.408493995666504 EntMin 0.0
Epoch 3, Class Loss=0.5969375371932983, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.5969375371932983, Class Loss=0.5969375371932983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/19, Loss=4.983889222145081
Loss made of: CE 0.641907274723053, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.218850612640381 EntMin 0.0
Epoch 4, Class Loss=0.5783467292785645, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5783467292785645, Class Loss=0.5783467292785645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/19, Loss=4.9528001844882965
Loss made of: CE 0.47584018111228943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.659486770629883 EntMin 0.0
Epoch 5, Class Loss=0.5421460270881653, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5421460270881653, Class Loss=0.5421460270881653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/19, Loss=4.774207997322082
Loss made of: CE 0.5290728807449341, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.340002059936523 EntMin 0.0
Epoch 6, Class Loss=0.5246673822402954, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5246673822402954, Class Loss=0.5246673822402954, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=5.456124114990234
Loss made of: CE 0.7729678153991699, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.912578105926514 EntMin 0.0
Epoch 1, Batch 20/33, Loss=5.323121255636215
Loss made of: CE 0.7461522817611694, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.557436943054199 EntMin 0.0
Epoch 1, Batch 30/33, Loss=5.294667732715607
Loss made of: CE 0.6359544992446899, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.443168640136719 EntMin 0.0
Epoch 1, Class Loss=0.7522634863853455, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.7522634863853455, Class Loss=0.7522634863853455, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=5.412958997488022
Loss made of: CE 0.7771464586257935, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.350295543670654 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.986709415912628
Loss made of: CE 0.6416418552398682, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.885277271270752 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.9729371011257175
Loss made of: CE 0.6145192980766296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.491405010223389 EntMin 0.0
Epoch 2, Class Loss=0.6536543965339661, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.6536543965339661, Class Loss=0.6536543965339661, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.934588295221329
Loss made of: CE 0.5301297903060913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.492455005645752 EntMin 0.0
Epoch 3, Batch 20/33, Loss=5.057188868522644
Loss made of: CE 0.6486562490463257, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.346160411834717 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.923450055718422
Loss made of: CE 0.6749008893966675, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4743475914001465 EntMin 0.0
Epoch 3, Class Loss=0.6052049994468689, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.6052049994468689, Class Loss=0.6052049994468689, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.856786414980888
Loss made of: CE 0.5470446348190308, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0204315185546875 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.973354399204254
Loss made of: CE 0.5173057913780212, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3278608322143555 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.838478818535805
Loss made of: CE 0.5667982697486877, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8204283714294434 EntMin 0.0
Epoch 4, Class Loss=0.5678466558456421, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5678466558456421, Class Loss=0.5678466558456421, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.753596168756485
Loss made of: CE 0.4965321719646454, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.950669050216675 EntMin 0.0
Epoch 5, Batch 20/33, Loss=5.013902923464775
Loss made of: CE 0.4830421805381775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.013432502746582 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.7104811549186705
Loss made of: CE 0.5761993527412415, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.027675151824951 EntMin 0.0
Epoch 5, Class Loss=0.5489341616630554, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5489341616630554, Class Loss=0.5489341616630554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.601346051692962
Loss made of: CE 0.5521696209907532, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.04239559173584 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.623723331093788
Loss made of: CE 0.4768293797969818, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.512784481048584 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.623317697644234
Loss made of: CE 0.4246196746826172, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.441764831542969 EntMin 0.0
Epoch 6, Class Loss=0.5327209830284119, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5327209830284119, Class Loss=0.5327209830284119, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=5.4901301801204685
Loss made of: CE 0.8109349012374878, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.235567092895508 EntMin 0.0
Epoch 1, Batch 20/33, Loss=5.244726836681366
Loss made of: CE 0.8475048542022705, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.578243255615234 EntMin 0.0
Epoch 1, Batch 30/33, Loss=5.125804013013839
Loss made of: CE 0.5892133712768555, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.471342086791992 EntMin 0.0
Epoch 1, Class Loss=0.745872974395752, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.745872974395752, Class Loss=0.745872974395752, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=5.039311477541924
Loss made of: CE 0.4802221953868866, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.521154403686523 EntMin 0.0
Epoch 2, Batch 20/33, Loss=5.1698270499706265
Loss made of: CE 0.6829099655151367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.565141201019287 EntMin 0.0
Epoch 2, Batch 30/33, Loss=5.0059770226478575
Loss made of: CE 0.6130635738372803, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.009950637817383 EntMin 0.0
Epoch 2, Class Loss=0.6446370482444763, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.6446370482444763, Class Loss=0.6446370482444763, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.90463599562645
Loss made of: CE 0.6752716898918152, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9175453186035156 EntMin 0.0
Epoch 3, Batch 20/33, Loss=5.016045853495598
Loss made of: CE 0.6048023104667664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.336417198181152 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.7871204107999805
Loss made of: CE 0.6464918851852417, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9114232063293457 EntMin 0.0
Epoch 3, Class Loss=0.5985581874847412, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.5985581874847412, Class Loss=0.5985581874847412, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.830113381147385
Loss made of: CE 0.5234529972076416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2868266105651855 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.74953011572361
Loss made of: CE 0.557052731513977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.00266695022583 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.6757787555456165
Loss made of: CE 0.6335083246231079, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.937160491943359 EntMin 0.0
Epoch 4, Class Loss=0.5651029944419861, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.5651029944419861, Class Loss=0.5651029944419861, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.775490605831147
Loss made of: CE 0.5042217373847961, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7276456356048584 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.568638283014297
Loss made of: CE 0.7230770587921143, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6653785705566406 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.545629727840423
Loss made of: CE 0.6117126941680908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.058919906616211 EntMin 0.0
Epoch 5, Class Loss=0.5462636351585388, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5462636351585388, Class Loss=0.5462636351585388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.5670799106359485
Loss made of: CE 0.5997518301010132, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.038023471832275 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.725163689255714
Loss made of: CE 0.5323781967163086, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6920790672302246 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.779335156083107
Loss made of: CE 0.4777786135673523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5538034439086914 EntMin 0.0
Epoch 6, Class Loss=0.537249743938446, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.537249743938446, Class Loss=0.537249743938446, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.438304603099823, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.869115
Mean Acc: 0.560911
FreqW Acc: 0.770054
Mean IoU: 0.442501
Class IoU:
	class 0: 0.8807459
	class 1: 0.7253572
	class 2: 0.33624366
	class 3: 0.45540342
	class 4: 0.6216677
	class 5: 0.1702656
	class 6: 0.63406986
	class 7: 0.70119375
	class 8: 0.6575685
	class 9: 0.0
	class 10: 0.5699107
	class 11: 8.435842e-05
	class 12: 0.0
Class Acc:
	class 0: 0.9785898
	class 1: 0.7447513
	class 2: 0.60857636
	class 3: 0.95326966
	class 4: 0.8154763
	class 5: 0.17212418
	class 6: 0.6390948
	class 7: 0.7283505
	class 8: 0.85811454
	class 9: 0.0
	class 10: 0.79340756
	class 11: 8.435842e-05
	class 12: 0.0

federated global round: 12, step: 2
select part of clients to conduct local training
[0, 16, 7, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.933634811639786
Loss made of: CE 0.7135226130485535, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7573018074035645 EntMin 0.0
Epoch 1, Class Loss=0.6093922257423401, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.6093922257423401, Class Loss=0.6093922257423401, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.696073758602142
Loss made of: CE 0.5363451242446899, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.133820056915283 EntMin 0.0
Epoch 2, Class Loss=0.5606392025947571, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.5606392025947571, Class Loss=0.5606392025947571, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.707247412204742
Loss made of: CE 0.5813857316970825, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8628575801849365 EntMin 0.0
Epoch 3, Class Loss=0.5257249474525452, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.5257249474525452, Class Loss=0.5257249474525452, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.648793628811836
Loss made of: CE 0.6024456024169922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734585762023926 EntMin 0.0
Epoch 4, Class Loss=0.5076940655708313, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.5076940655708313, Class Loss=0.5076940655708313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.50532982647419
Loss made of: CE 0.5194052457809448, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0027265548706055 EntMin 0.0
Epoch 5, Class Loss=0.4818923771381378, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.4818923771381378, Class Loss=0.4818923771381378, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.554551312327385
Loss made of: CE 0.47010135650634766, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8511548042297363 EntMin 0.0
Epoch 6, Class Loss=0.46376127004623413, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.46376127004623413, Class Loss=0.46376127004623413, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/42, Loss=5.8960417866706845
Loss made of: CE 0.8142563104629517, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.170969486236572 EntMin 0.0
Epoch 1, Batch 20/42, Loss=5.485791689157486
Loss made of: CE 0.6040870547294617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.971096515655518 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.557018065452576
Loss made of: CE 0.6508584022521973, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698356628417969 EntMin 0.0
Epoch 1, Batch 40/42, Loss=5.070645517110824
Loss made of: CE 0.40729543566703796, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.037612438201904 EntMin 0.0
Epoch 1, Class Loss=0.718860387802124, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.718860387802124, Class Loss=0.718860387802124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/42, Loss=5.24648599922657
Loss made of: CE 0.38861581683158875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.583712577819824 EntMin 0.0
Epoch 2, Batch 20/42, Loss=5.092824524641037
Loss made of: CE 0.3836611211299896, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.010769367218018 EntMin 0.0
Epoch 2, Batch 30/42, Loss=5.0073547005653385
Loss made of: CE 0.3562389612197876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.692898273468018 EntMin 0.0
Epoch 2, Batch 40/42, Loss=5.104614675045013
Loss made of: CE 0.4877776801586151, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.300340175628662 EntMin 0.0
Epoch 2, Class Loss=0.47587090730667114, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.47587090730667114, Class Loss=0.47587090730667114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/42, Loss=4.859397572278977
Loss made of: CE 0.420482873916626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.177772045135498 EntMin 0.0
Epoch 3, Batch 20/42, Loss=5.147837686538696
Loss made of: CE 0.39144548773765564, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.976931571960449 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.827863937616348
Loss made of: CE 0.37050747871398926, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.642953395843506 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.7917748361825945
Loss made of: CE 0.32594814896583557, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.66258430480957 EntMin 0.0
Epoch 3, Class Loss=0.42058172821998596, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.42058172821998596, Class Loss=0.42058172821998596, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/42, Loss=4.996647614240646
Loss made of: CE 0.39780193567276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.330071926116943 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.826091954112053
Loss made of: CE 0.4554768204689026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.325510501861572 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.83376462161541
Loss made of: CE 0.3380061089992523, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.015300750732422 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.833179184794426
Loss made of: CE 0.4193776547908783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3256025314331055 EntMin 0.0
Epoch 4, Class Loss=0.4189583957195282, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.4189583957195282, Class Loss=0.4189583957195282, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/42, Loss=4.641316011548042
Loss made of: CE 0.4083584249019623, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1117472648620605 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.781191119551659
Loss made of: CE 0.3823609948158264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.256222724914551 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.822291633486747
Loss made of: CE 0.455111026763916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664035320281982 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.7941886484622955
Loss made of: CE 0.38084396719932556, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1582818031311035 EntMin 0.0
Epoch 5, Class Loss=0.39406946301460266, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.39406946301460266, Class Loss=0.39406946301460266, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/42, Loss=4.8479621559381485
Loss made of: CE 0.3543086051940918, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8975188732147217 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.613098001480102
Loss made of: CE 0.406604140996933, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.198873996734619 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.798019134998322
Loss made of: CE 0.41433316469192505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.438788414001465 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.922854509949684
Loss made of: CE 0.39233025908470154, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.854922294616699 EntMin 0.0
Epoch 6, Class Loss=0.40085211396217346, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.40085211396217346, Class Loss=0.40085211396217346, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/31, Loss=5.7729753196239475
Loss made of: CE 0.7634053230285645, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.882064342498779 EntMin 0.0
Epoch 1, Batch 20/31, Loss=5.5328223526477815
Loss made of: CE 0.6204843521118164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.936346054077148 EntMin 0.0
Epoch 1, Batch 30/31, Loss=5.362072849273682
Loss made of: CE 0.6207900047302246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.469476699829102 EntMin 0.0
Epoch 1, Class Loss=0.6815974712371826, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.6815974712371826, Class Loss=0.6815974712371826, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/31, Loss=5.114658945798874
Loss made of: CE 0.5774915814399719, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.395661354064941 EntMin 0.0
Epoch 2, Batch 20/31, Loss=4.905696913599968
Loss made of: CE 0.6214611530303955, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.16980504989624 EntMin 0.0
Epoch 2, Batch 30/31, Loss=5.3173295825719835
Loss made of: CE 0.4844178557395935, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.942460775375366 EntMin 0.0
Epoch 2, Class Loss=0.5916211009025574, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.5916211009025574, Class Loss=0.5916211009025574, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/31, Loss=5.043624091148376
Loss made of: CE 0.5978857278823853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.419565200805664 EntMin 0.0
Epoch 3, Batch 20/31, Loss=4.76074715256691
Loss made of: CE 0.6147493720054626, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9420080184936523 EntMin 0.0
Epoch 3, Batch 30/31, Loss=4.879607856273651
Loss made of: CE 0.4441201388835907, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.71689772605896 EntMin 0.0
Epoch 3, Class Loss=0.5432765483856201, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.5432765483856201, Class Loss=0.5432765483856201, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/31, Loss=4.720831111073494
Loss made of: CE 0.6630859375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.689150810241699 EntMin 0.0
Epoch 4, Batch 20/31, Loss=4.7619773000478745
Loss made of: CE 0.5559244155883789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0658745765686035 EntMin 0.0
Epoch 4, Batch 30/31, Loss=4.885678565502166
Loss made of: CE 0.7560257911682129, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.605022430419922 EntMin 0.0
Epoch 4, Class Loss=0.5345360040664673, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.5345360040664673, Class Loss=0.5345360040664673, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/31, Loss=4.623440608382225
Loss made of: CE 0.4932168126106262, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128077030181885 EntMin 0.0
Epoch 5, Batch 20/31, Loss=4.688521578907967
Loss made of: CE 0.48213571310043335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.871208667755127 EntMin 0.0
Epoch 5, Batch 30/31, Loss=4.915762981772422
Loss made of: CE 0.38689714670181274, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.031213283538818 EntMin 0.0
Epoch 5, Class Loss=0.5003202557563782, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.5003202557563782, Class Loss=0.5003202557563782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/31, Loss=4.622249564528465
Loss made of: CE 0.598572313785553, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.04186487197876 EntMin 0.0
Epoch 6, Batch 20/31, Loss=4.794428795576096
Loss made of: CE 0.45919427275657654, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.383731365203857 EntMin 0.0
Epoch 6, Batch 30/31, Loss=4.5963315814733505
Loss made of: CE 0.564407467842102, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.784733772277832 EntMin 0.0
Epoch 6, Class Loss=0.4939831793308258, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.4939831793308258, Class Loss=0.4939831793308258, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.870935881137848
Loss made of: CE 0.47215038537979126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363154888153076 EntMin 0.0
Epoch 1, Class Loss=0.6195359230041504, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.6195359230041504, Class Loss=0.6195359230041504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.874979841709137
Loss made of: CE 0.5203295946121216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.283745288848877 EntMin 0.0
Epoch 2, Class Loss=0.5647406578063965, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.5647406578063965, Class Loss=0.5647406578063965, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.744478443264962
Loss made of: CE 0.4451694190502167, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.630936622619629 EntMin 0.0
Epoch 3, Class Loss=0.5314807295799255, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.5314807295799255, Class Loss=0.5314807295799255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.56272015273571
Loss made of: CE 0.4534096121788025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7320775985717773 EntMin 0.0
Epoch 4, Class Loss=0.4921150207519531, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.4921150207519531, Class Loss=0.4921150207519531, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.54858982861042
Loss made of: CE 0.4708430767059326, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.001797199249268 EntMin 0.0
Epoch 5, Class Loss=0.48721349239349365, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.48721349239349365, Class Loss=0.48721349239349365, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.576240879297257
Loss made of: CE 0.4283120036125183, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.921113967895508 EntMin 0.0
Epoch 6, Class Loss=0.4800807535648346, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.4800807535648346, Class Loss=0.4800807535648346, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.39860913157463074, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.871581
Mean Acc: 0.591638
FreqW Acc: 0.778806
Mean IoU: 0.461272
Class IoU:
	class 0: 0.88718385
	class 1: 0.7297748
	class 2: 0.34183
	class 3: 0.43214715
	class 4: 0.573683
	class 5: 0.20691334
	class 6: 0.656344
	class 7: 0.72644085
	class 8: 0.6279311
	class 9: 0.002141701
	class 10: 0.47104204
	class 11: 0.33587882
	class 12: 0.005226479
Class Acc:
	class 0: 0.9739524
	class 1: 0.75428754
	class 2: 0.67428
	class 3: 0.9556944
	class 4: 0.86708915
	class 5: 0.2102538
	class 6: 0.6627749
	class 7: 0.7727388
	class 8: 0.8823506
	class 9: 0.0021440878
	class 10: 0.5018691
	class 11: 0.42863664
	class 12: 0.005226761

federated global round: 13, step: 2
select part of clients to conduct local training
[14, 13, 0, 1]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.6331651479005815
Loss made of: CE 0.4971209764480591, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.872631549835205 EntMin 0.0
Epoch 1, Class Loss=0.5218462347984314, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.5218462347984314, Class Loss=0.5218462347984314, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=4.560979974269867
Loss made of: CE 0.5205539464950562, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.74991512298584 EntMin 0.0
Epoch 2, Class Loss=0.4748315215110779, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.4748315215110779, Class Loss=0.4748315215110779, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=4.623385411500931
Loss made of: CE 0.40463826060295105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.55491304397583 EntMin 0.0
Epoch 3, Class Loss=0.45822203159332275, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.45822203159332275, Class Loss=0.45822203159332275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=4.500098937749863
Loss made of: CE 0.4239470362663269, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.896730422973633 EntMin 0.0
Epoch 4, Class Loss=0.45407843589782715, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.45407843589782715, Class Loss=0.45407843589782715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=4.389930218458176
Loss made of: CE 0.3738536834716797, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5963492393493652 EntMin 0.0
Epoch 5, Class Loss=0.43181198835372925, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.43181198835372925, Class Loss=0.43181198835372925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=4.408289098739624
Loss made of: CE 0.3831728398799896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7705373764038086 EntMin 0.0
Epoch 6, Class Loss=0.4308975636959076, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.4308975636959076, Class Loss=0.4308975636959076, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/19, Loss=4.479942661523819
Loss made of: CE 0.47864699363708496, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9535036087036133 EntMin 0.0
Epoch 1, Class Loss=0.5079120397567749, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5079120397567749, Class Loss=0.5079120397567749, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/19, Loss=4.50422990322113
Loss made of: CE 0.5239855051040649, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.373200416564941 EntMin 0.0
Epoch 2, Class Loss=0.4655875563621521, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.4655875563621521, Class Loss=0.4655875563621521, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/19, Loss=4.491841360926628
Loss made of: CE 0.36592400074005127, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.064444541931152 EntMin 0.0
Epoch 3, Class Loss=0.4558311700820923, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.4558311700820923, Class Loss=0.4558311700820923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/19, Loss=4.55793761909008
Loss made of: CE 0.4808585047721863, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.948673725128174 EntMin 0.0
Epoch 4, Class Loss=0.4553077220916748, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.4553077220916748, Class Loss=0.4553077220916748, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/19, Loss=4.589023506641388
Loss made of: CE 0.35913628339767456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3927392959594727 EntMin 0.0
Epoch 5, Class Loss=0.43790772557258606, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.43790772557258606, Class Loss=0.43790772557258606, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/19, Loss=4.309464028477668
Loss made of: CE 0.4108768403530121, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.786625862121582 EntMin 0.0
Epoch 6, Class Loss=0.4390992224216461, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.4390992224216461, Class Loss=0.4390992224216461, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.53138966858387
Loss made of: CE 0.4996964633464813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3413004875183105 EntMin 0.0
Epoch 1, Class Loss=0.5197635293006897, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.5197635293006897, Class Loss=0.5197635293006897, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/19, Loss=4.365222206711769
Loss made of: CE 0.5542211532592773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.994943141937256 EntMin 0.0
Epoch 2, Class Loss=0.4845614731311798, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.4845614731311798, Class Loss=0.4845614731311798, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/19, Loss=4.4152898579835895
Loss made of: CE 0.5080462694168091, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.711320638656616 EntMin 0.0
Epoch 3, Class Loss=0.4792618453502655, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.4792618453502655, Class Loss=0.4792618453502655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/19, Loss=4.4696354299783705
Loss made of: CE 0.5865709781646729, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.680835247039795 EntMin 0.0
Epoch 4, Class Loss=0.4682309627532959, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.4682309627532959, Class Loss=0.4682309627532959, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/19, Loss=4.315700760483741
Loss made of: CE 0.4694722592830658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.923961639404297 EntMin 0.0
Epoch 5, Class Loss=0.44826677441596985, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.44826677441596985, Class Loss=0.44826677441596985, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/19, Loss=4.437243884801864
Loss made of: CE 0.4798971116542816, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.767174482345581 EntMin 0.0
Epoch 6, Class Loss=0.44939371943473816, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.44939371943473816, Class Loss=0.44939371943473816, Reg Loss=0.0
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/35, Loss=7.670588618516922
Loss made of: CE 1.0423414707183838, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0568037033081055 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.578906470537186
Loss made of: CE 0.6624406576156616, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.391229629516602 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.181586509943008
Loss made of: CE 0.5474089980125427, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.636729717254639 EntMin 0.0
Epoch 1, Class Loss=0.7737246751785278, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.7737246751785278, Class Loss=0.7737246751785278, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000584
Epoch 2, Batch 10/35, Loss=5.80337278842926
Loss made of: CE 0.38618820905685425, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.307374954223633 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.854762476682663
Loss made of: CE 0.4772222936153412, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.760601043701172 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.553855669498444
Loss made of: CE 0.38157495856285095, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.536456108093262 EntMin 0.0
Epoch 2, Class Loss=0.4484120011329651, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.4484120011329651, Class Loss=0.4484120011329651, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/35, Loss=5.21959920823574
Loss made of: CE 0.4327808618545532, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.816615104675293 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.562743520736694
Loss made of: CE 0.38057827949523926, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.281647682189941 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.35406704545021
Loss made of: CE 0.3815997242927551, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.189196586608887 EntMin 0.0
Epoch 3, Class Loss=0.38023921847343445, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.38023921847343445, Class Loss=0.38023921847343445, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000487
Epoch 4, Batch 10/35, Loss=5.30890933573246
Loss made of: CE 0.293960303068161, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.107743263244629 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.186327278614044
Loss made of: CE 0.33540093898773193, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.693140983581543 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.294448214769363
Loss made of: CE 0.3070690333843231, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.012685298919678 EntMin 0.0
Epoch 4, Class Loss=0.35021501779556274, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.35021501779556274, Class Loss=0.35021501779556274, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000438
Epoch 5, Batch 10/35, Loss=5.300765484571457
Loss made of: CE 0.33304092288017273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.560073375701904 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.324207058548927
Loss made of: CE 0.3332187831401825, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.091556549072266 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.889564681053161
Loss made of: CE 0.2895875573158264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.335184097290039 EntMin 0.0
Epoch 5, Class Loss=0.3407219648361206, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.3407219648361206, Class Loss=0.3407219648361206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000389
Epoch 6, Batch 10/35, Loss=4.993837088346481
Loss made of: CE 0.26371896266937256, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.527505874633789 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.226037159562111
Loss made of: CE 0.2843320965766907, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3990936279296875 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.268503513932228
Loss made of: CE 0.35817962884902954, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.487285137176514 EntMin 0.0
Epoch 6, Class Loss=0.35491225123405457, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.35491225123405457, Class Loss=0.35491225123405457, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.36988329887390137, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.877288
Mean Acc: 0.599561
FreqW Acc: 0.786416
Mean IoU: 0.472192
Class IoU:
	class 0: 0.8919879
	class 1: 0.6940103
	class 2: 0.31838045
	class 3: 0.48622683
	class 4: 0.60308826
	class 5: 0.21328522
	class 6: 0.6118036
	class 7: 0.70862246
	class 8: 0.66589016
	class 9: 0.010386005
	class 10: 0.5537975
	class 11: 0.3143726
	class 12: 0.06665027
Class Acc:
	class 0: 0.9762405
	class 1: 0.7147388
	class 2: 0.6002996
	class 3: 0.94851476
	class 4: 0.8335938
	class 5: 0.21642551
	class 6: 0.61676365
	class 7: 0.7518058
	class 8: 0.8628616
	class 9: 0.010425667
	class 10: 0.82288235
	class 11: 0.37296644
	class 12: 0.06677176

federated global round: 14, step: 2
select part of clients to conduct local training
[16, 9, 4, 0]
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/42, Loss=5.490769010782242
Loss made of: CE 0.7335718870162964, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.258159637451172 EntMin 0.0
Epoch 1, Batch 20/42, Loss=5.196179673075676
Loss made of: CE 0.4888133704662323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5596537590026855 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.321889394521714
Loss made of: CE 0.5231911540031433, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.593945026397705 EntMin 0.0
Epoch 1, Batch 40/42, Loss=4.851551955938339
Loss made of: CE 0.35979753732681274, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.886645793914795 EntMin 0.0
Epoch 1, Class Loss=0.5930824279785156, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.5930824279785156, Class Loss=0.5930824279785156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/42, Loss=4.957910385727883
Loss made of: CE 0.3718380331993103, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.310788154602051 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.762445345520973
Loss made of: CE 0.3773361146450043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.924104690551758 EntMin 0.0
Epoch 2, Batch 30/42, Loss=4.684620946645737
Loss made of: CE 0.3395192623138428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.442808628082275 EntMin 0.0
Epoch 2, Batch 40/42, Loss=4.8342846751213076
Loss made of: CE 0.461364209651947, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8870482444763184 EntMin 0.0
Epoch 2, Class Loss=0.42722535133361816, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.42722535133361816, Class Loss=0.42722535133361816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/42, Loss=4.6623700082302095
Loss made of: CE 0.39836782217025757, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9589734077453613 EntMin 0.0
Epoch 3, Batch 20/42, Loss=4.964092546701432
Loss made of: CE 0.40299925208091736, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.898576736450195 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.60554072856903
Loss made of: CE 0.37131041288375854, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.497012615203857 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.577451059222222
Loss made of: CE 0.35803693532943726, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.725303649902344 EntMin 0.0
Epoch 3, Class Loss=0.3897152543067932, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.3897152543067932, Class Loss=0.3897152543067932, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/42, Loss=4.716781058907509
Loss made of: CE 0.34014010429382324, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.965407133102417 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.5195998579263685
Loss made of: CE 0.3799700140953064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.139121055603027 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.504388257861137
Loss made of: CE 0.3192056119441986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.585289716720581 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.628658017516136
Loss made of: CE 0.3609013557434082, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.525376796722412 EntMin 0.0
Epoch 4, Class Loss=0.3868597149848938, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.3868597149848938, Class Loss=0.3868597149848938, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/42, Loss=4.360831105709076
Loss made of: CE 0.39626431465148926, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8996429443359375 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.657463073730469
Loss made of: CE 0.4370751976966858, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9629266262054443 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.6946233123540875
Loss made of: CE 0.43706458806991577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.712261199951172 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.533578234910965
Loss made of: CE 0.4301663339138031, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9939165115356445 EntMin 0.0
Epoch 5, Class Loss=0.37889382243156433, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.37889382243156433, Class Loss=0.37889382243156433, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/42, Loss=4.653799837827682
Loss made of: CE 0.3186836242675781, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7162482738494873 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.382389989495278
Loss made of: CE 0.38522058725357056, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8723666667938232 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.625978431105613
Loss made of: CE 0.44061848521232605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1369781494140625 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.715407800674439
Loss made of: CE 0.41552576422691345, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.651613235473633 EntMin 0.0
Epoch 6, Class Loss=0.38833755254745483, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.38833755254745483, Class Loss=0.38833755254745483, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=6.824136489629746
Loss made of: CE 0.8672046661376953, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.296878337860107 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.167732119560242
Loss made of: CE 0.44954872131347656, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.204094886779785 EntMin 0.0
Epoch 1, Batch 30/35, Loss=5.702606928348541
Loss made of: CE 0.4594508111476898, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3009843826293945 EntMin 0.0
Epoch 1, Class Loss=0.6052878499031067, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.6052878499031067, Class Loss=0.6052878499031067, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/35, Loss=5.779428869485855
Loss made of: CE 0.467354953289032, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.623055934906006 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.51829246878624
Loss made of: CE 0.3102141320705414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.84074068069458 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.38106141090393
Loss made of: CE 0.31161707639694214, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758001327514648 EntMin 0.0
Epoch 2, Class Loss=0.38607656955718994, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.38607656955718994, Class Loss=0.38607656955718994, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/35, Loss=5.434166076779365
Loss made of: CE 0.3423546254634857, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.914520740509033 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.398457196354866
Loss made of: CE 0.35721278190612793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.791607856750488 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.232965290546417
Loss made of: CE 0.27711743116378784, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4654316902160645 EntMin 0.0
Epoch 3, Class Loss=0.3580669164657593, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.3580669164657593, Class Loss=0.3580669164657593, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/35, Loss=5.319398719072342
Loss made of: CE 0.2705409824848175, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1247663497924805 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.324790444970131
Loss made of: CE 0.3062875270843506, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.32180118560791 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.198751205205918
Loss made of: CE 0.32196691632270813, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.328865051269531 EntMin 0.0
Epoch 4, Class Loss=0.3446054756641388, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.3446054756641388, Class Loss=0.3446054756641388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/35, Loss=4.873198938369751
Loss made of: CE 0.262943834066391, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.851141929626465 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.197278663516045
Loss made of: CE 0.2589409053325653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.485115051269531 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.972430591285229
Loss made of: CE 0.44326910376548767, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.433915615081787 EntMin 0.0
Epoch 5, Class Loss=0.3263319134712219, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.3263319134712219, Class Loss=0.3263319134712219, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/35, Loss=5.139845758676529
Loss made of: CE 0.3194999694824219, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.065737724304199 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.802116379141808
Loss made of: CE 0.5269675850868225, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7737507820129395 EntMin 0.0
Epoch 6, Batch 30/35, Loss=4.961647117137909
Loss made of: CE 0.22341148555278778, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.647956848144531 EntMin 0.0
Epoch 6, Class Loss=0.3175641894340515, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3175641894340515, Class Loss=0.3175641894340515, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.37114804983139
Loss made of: CE 0.3772056996822357, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8392586708068848 EntMin 0.0
Epoch 1, Class Loss=0.4767428934574127, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.4767428934574127, Class Loss=0.4767428934574127, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/19, Loss=4.544248792529106
Loss made of: CE 0.42580899596214294, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.930589199066162 EntMin 0.0
Epoch 2, Class Loss=0.44584134221076965, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.44584134221076965, Class Loss=0.44584134221076965, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/19, Loss=4.38757404088974
Loss made of: CE 0.37265485525131226, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3128108978271484 EntMin 0.0
Epoch 3, Class Loss=0.44022253155708313, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.44022253155708313, Class Loss=0.44022253155708313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/19, Loss=4.266903272271156
Loss made of: CE 0.43004268407821655, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4861743450164795 EntMin 0.0
Epoch 4, Class Loss=0.4285593032836914, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.4285593032836914, Class Loss=0.4285593032836914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/19, Loss=4.243691372871399
Loss made of: CE 0.49849367141723633, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.606351613998413 EntMin 0.0
Epoch 5, Class Loss=0.43110185861587524, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.43110185861587524, Class Loss=0.43110185861587524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/19, Loss=4.306614148616791
Loss made of: CE 0.39279264211654663, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7557225227355957 EntMin 0.0
Epoch 6, Class Loss=0.4288032054901123, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.4288032054901123, Class Loss=0.4288032054901123, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000304
Epoch 1, Batch 10/19, Loss=4.4477806955575945
Loss made of: CE 0.4870077967643738, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.360919952392578 EntMin 0.0
Epoch 1, Class Loss=0.46677228808403015, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.46677228808403015, Class Loss=0.46677228808403015, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000258
Epoch 2, Batch 10/19, Loss=4.264407405257225
Loss made of: CE 0.5033621191978455, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8884360790252686 EntMin 0.0
Epoch 2, Class Loss=0.4600541591644287, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.4600541591644287, Class Loss=0.4600541591644287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000211
Epoch 3, Batch 10/19, Loss=4.304722830653191
Loss made of: CE 0.4833337068557739, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4669156074523926 EntMin 0.0
Epoch 3, Class Loss=0.4542584717273712, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.4542584717273712, Class Loss=0.4542584717273712, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000163
Epoch 4, Batch 10/19, Loss=4.33025316298008
Loss made of: CE 0.567940354347229, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.437741279602051 EntMin 0.0
Epoch 4, Class Loss=0.44659027457237244, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.44659027457237244, Class Loss=0.44659027457237244, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000113
Epoch 5, Batch 10/19, Loss=4.2046295434236525
Loss made of: CE 0.4397760331630707, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.757452964782715 EntMin 0.0
Epoch 5, Class Loss=0.43908461928367615, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.43908461928367615, Class Loss=0.43908461928367615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000061
Epoch 6, Batch 10/19, Loss=4.37238464653492
Loss made of: CE 0.4565550684928894, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5774435997009277 EntMin 0.0
Epoch 6, Class Loss=0.44754117727279663, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.44754117727279663, Class Loss=0.44754117727279663, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3414284884929657, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.896216
Mean Acc: 0.639315
FreqW Acc: 0.818221
Mean IoU: 0.532709
Class IoU:
	class 0: 0.9001377
	class 1: 0.71930474
	class 2: 0.3107886
	class 3: 0.51642066
	class 4: 0.59043175
	class 5: 0.2183017
	class 6: 0.64663225
	class 7: 0.7300364
	class 8: 0.7004064
	class 9: 0.014623765
	class 10: 0.6909134
	class 11: 0.30389607
	class 12: 0.5833272
Class Acc:
	class 0: 0.9744068
	class 1: 0.7478437
	class 2: 0.56298256
	class 3: 0.94494116
	class 4: 0.85012436
	class 5: 0.22172262
	class 6: 0.6529703
	class 7: 0.78799176
	class 8: 0.72606796
	class 9: 0.0147128785
	class 10: 0.7133666
	class 11: 0.35167265
	class 12: 0.7622921

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[11, 6, 10, 7]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=7.906419718265534
Loss made of: CE 1.3540856838226318, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.11247444152832 EntMin 0.0
Epoch 1, Batch 20/102, Loss=6.598805236816406
Loss made of: CE 0.983747661113739, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.182744026184082 EntMin 0.0
Epoch 1, Batch 30/102, Loss=6.5742366969585415
Loss made of: CE 0.9065203666687012, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.785514831542969 EntMin 0.0
Epoch 1, Batch 40/102, Loss=6.334336513280869
Loss made of: CE 0.7000597715377808, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.354421615600586 EntMin 0.0
Epoch 1, Batch 50/102, Loss=6.124805647134781
Loss made of: CE 0.705001950263977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.54688835144043 EntMin 0.0
Epoch 1, Batch 60/102, Loss=6.001120358705521
Loss made of: CE 0.7588502764701843, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.223310470581055 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.639939624071121
Loss made of: CE 0.7409112453460693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645206451416016 EntMin 0.0
Epoch 1, Batch 80/102, Loss=5.49613744020462
Loss made of: CE 0.7155753970146179, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.927118301391602 EntMin 0.0
Epoch 1, Batch 90/102, Loss=5.4268409550189975
Loss made of: CE 0.743865966796875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.295399188995361 EntMin 0.0
Epoch 1, Batch 100/102, Loss=5.452475541830063
Loss made of: CE 0.8807582855224609, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.667318344116211 EntMin 0.0
Epoch 1, Class Loss=0.878940224647522, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.878940224647522, Class Loss=0.878940224647522, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=5.2007484376430515
Loss made of: CE 0.6516563296318054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.075103759765625 EntMin 0.0
Epoch 2, Batch 20/102, Loss=5.136793053150177
Loss made of: CE 0.5551192760467529, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.467408180236816 EntMin 0.0
Epoch 2, Batch 30/102, Loss=5.35064577460289
Loss made of: CE 0.6775698661804199, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.761617660522461 EntMin 0.0
Epoch 2, Batch 40/102, Loss=5.245885688066482
Loss made of: CE 0.6465590000152588, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.055484771728516 EntMin 0.0
Epoch 2, Batch 50/102, Loss=5.151587161421776
Loss made of: CE 0.6059097051620483, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3523712158203125 EntMin 0.0
Epoch 2, Batch 60/102, Loss=5.352620521187783
Loss made of: CE 0.6885776519775391, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.233217239379883 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.787956646084785
Loss made of: CE 0.5019222497940063, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8408219814300537 EntMin 0.0
Epoch 2, Batch 80/102, Loss=5.032767987251281
Loss made of: CE 0.5231163501739502, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4637017250061035 EntMin 0.0
Epoch 2, Batch 90/102, Loss=5.207703647017479
Loss made of: CE 0.7366601228713989, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6755571365356445 EntMin 0.0
Epoch 2, Batch 100/102, Loss=5.205863523483276
Loss made of: CE 0.5119856595993042, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.01634407043457 EntMin 0.0
Epoch 2, Class Loss=0.5958763360977173, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.5958763360977173, Class Loss=0.5958763360977173, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=5.017538180947303
Loss made of: CE 0.5179675221443176, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.061805248260498 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.9997022420167925
Loss made of: CE 0.6284667253494263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.045363426208496 EntMin 0.0
Epoch 3, Batch 30/102, Loss=5.048238664865494
Loss made of: CE 0.46468132734298706, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.169731140136719 EntMin 0.0
Epoch 3, Batch 40/102, Loss=5.081061297655106
Loss made of: CE 0.46269088983535767, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363908290863037 EntMin 0.0
Epoch 3, Batch 50/102, Loss=5.097664937376976
Loss made of: CE 0.5334640741348267, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.235249996185303 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.8635019838809965
Loss made of: CE 0.5146133899688721, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.23649787902832 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.792623269557953
Loss made of: CE 0.39715343713760376, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.502792835235596 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 80/102, Loss=4.978405463695526
Loss made of: CE 0.3828877806663513, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.797275543212891 EntMin 0.0
Epoch 3, Batch 90/102, Loss=5.03604653775692
Loss made of: CE 0.4407634735107422, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8985793590545654 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.801218354701996
Loss made of: CE 0.37479090690612793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6865463256835938 EntMin 0.0
Epoch 3, Class Loss=0.5070233345031738, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.5070233345031738, Class Loss=0.5070233345031738, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.735272985696793
Loss made of: CE 0.49460071325302124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.171602249145508 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.933676525950432
Loss made of: CE 0.4068124294281006, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.981555938720703 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.780033114552498
Loss made of: CE 0.5548681020736694, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18621826171875 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.805627393722534
Loss made of: CE 0.41174131631851196, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7935447692871094 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.949288684129715
Loss made of: CE 0.4571002423763275, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.342160701751709 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.995476835966111
Loss made of: CE 0.46647870540618896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.555053234100342 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.869457975029945
Loss made of: CE 0.5386442542076111, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.388645172119141 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.9333280473947525
Loss made of: CE 0.6681810617446899, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.777032852172852 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.875633150339127
Loss made of: CE 0.41218432784080505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.346277236938477 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.7951735883951185
Loss made of: CE 0.4787868857383728, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.849729061126709 EntMin 0.0
Epoch 4, Class Loss=0.47358566522598267, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.47358566522598267, Class Loss=0.47358566522598267, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=5.083425831794739
Loss made of: CE 0.4911697506904602, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.224276065826416 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.91715390086174
Loss made of: CE 0.5039018392562866, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.108585834503174 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.895642945170403
Loss made of: CE 0.4164502024650574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.182994842529297 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.568420901894569
Loss made of: CE 0.4821525514125824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.040806770324707 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.925118499994278
Loss made of: CE 0.4947962462902069, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0279645919799805 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.439659500122071
Loss made of: CE 0.5238281488418579, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.100240707397461 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.655363139510155
Loss made of: CE 0.47915133833885193, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.732125759124756 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.463821429014206
Loss made of: CE 0.36141180992126465, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.630773544311523 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.837274062633514
Loss made of: CE 0.5640252828598022, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7988812923431396 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.819317927956581
Loss made of: CE 0.40814608335494995, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.71339750289917 EntMin 0.0
Epoch 5, Class Loss=0.455585241317749, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.455585241317749, Class Loss=0.455585241317749, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.852565434575081
Loss made of: CE 0.45364293456077576, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.392868518829346 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.560730773210525
Loss made of: CE 0.4120117425918579, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.112974643707275 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.395213526487351
Loss made of: CE 0.48527857661247253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.06814432144165 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.897217929363251
Loss made of: CE 0.5057684779167175, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.038386344909668 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.482218274474144
Loss made of: CE 0.44109559059143066, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9181923866271973 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.604252868890763
Loss made of: CE 0.5641685724258423, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.000146389007568 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.478494766354561
Loss made of: CE 0.4683775305747986, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3395514488220215 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.604542735219002
Loss made of: CE 0.4633048176765442, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8108959197998047 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.593632206320763
Loss made of: CE 0.4185241460800171, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.968068838119507 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.688258990645409
Loss made of: CE 0.4489430785179138, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7894177436828613 EntMin 0.0
Epoch 6, Class Loss=0.4469272196292877, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.4469272196292877, Class Loss=0.4469272196292877, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=7.815501034259796
Loss made of: CE 1.4064277410507202, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.612077236175537 EntMin 0.0
Epoch 1, Batch 20/23, Loss=6.619988262653351
Loss made of: CE 1.0065040588378906, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.14467716217041 EntMin 0.0
Epoch 1, Class Loss=1.2636091709136963, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=1.2636091709136963, Class Loss=1.2636091709136963, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=6.02362305521965
Loss made of: CE 1.0827845335006714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.983956336975098 EntMin 0.0
Epoch 2, Batch 20/23, Loss=5.569999527931214
Loss made of: CE 0.7271261215209961, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.550233840942383 EntMin 0.0
Epoch 2, Class Loss=0.9286593198776245, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.9286593198776245, Class Loss=0.9286593198776245, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=5.374226516485214
Loss made of: CE 0.7990626096725464, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.527135848999023 EntMin 0.0
Epoch 3, Batch 20/23, Loss=5.281005620956421
Loss made of: CE 0.8763002157211304, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.225324630737305 EntMin 0.0
Epoch 3, Class Loss=0.829383373260498, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.829383373260498, Class Loss=0.829383373260498, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.859700286388398
Loss made of: CE 0.8392178416252136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.394128322601318 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.903589659929276
Loss made of: CE 0.626266598701477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.631525039672852 EntMin 0.0
Epoch 4, Class Loss=0.7318081259727478, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.7318081259727478, Class Loss=0.7318081259727478, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=4.792386168241501
Loss made of: CE 0.7041789293289185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.17126989364624 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.755288285017014
Loss made of: CE 0.6542250514030457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.494522571563721 EntMin 0.0
Epoch 5, Class Loss=0.6798576712608337, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.6798576712608337, Class Loss=0.6798576712608337, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.653213149309158
Loss made of: CE 0.7292219996452332, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.206472396850586 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.655948105454445
Loss made of: CE 0.5758897066116333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.490709066390991 EntMin 0.0
Epoch 6, Class Loss=0.6271148324012756, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.6271148324012756, Class Loss=0.6271148324012756, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=8.03759982585907
Loss made of: CE 1.2865536212921143, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.016196250915527 EntMin 0.0
Epoch 1, Batch 20/23, Loss=6.683485209941864
Loss made of: CE 1.11954665184021, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.484682083129883 EntMin 0.0
Epoch 1, Class Loss=1.282747745513916, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=1.282747745513916, Class Loss=1.282747745513916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=5.927738392353058
Loss made of: CE 0.9999327659606934, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.516694068908691 EntMin 0.0
Epoch 2, Batch 20/23, Loss=5.434193402528763
Loss made of: CE 0.8991717100143433, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.753494739532471 EntMin 0.0
Epoch 2, Class Loss=0.927266001701355, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.927266001701355, Class Loss=0.927266001701355, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=5.186243939399719
Loss made of: CE 0.78225177526474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.790928363800049 EntMin 0.0
Epoch 3, Batch 20/23, Loss=5.294290870428085
Loss made of: CE 0.7632929682731628, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.642743110656738 EntMin 0.0
Epoch 3, Class Loss=0.8211387395858765, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.8211387395858765, Class Loss=0.8211387395858765, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.93130047917366
Loss made of: CE 0.6662454605102539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.134843826293945 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.9397240579128265
Loss made of: CE 0.7221983671188354, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.94049072265625 EntMin 0.0
Epoch 4, Class Loss=0.7374964356422424, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.7374964356422424, Class Loss=0.7374964356422424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=5.016913610696792
Loss made of: CE 0.6556615829467773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.932373046875 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.761955106258393
Loss made of: CE 0.7985223531723022, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.864295482635498 EntMin 0.0
Epoch 5, Class Loss=0.6858382821083069, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.6858382821083069, Class Loss=0.6858382821083069, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.661923593282699
Loss made of: CE 0.5438280701637268, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.082577705383301 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 20/23, Loss=4.650918006896973
Loss made of: CE 0.5929083824157715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.191381454467773 EntMin 0.0
Epoch 6, Class Loss=0.6252840161323547, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6252840161323547, Class Loss=0.6252840161323547, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=8.065831983089447
Loss made of: CE 1.3146474361419678, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.581613540649414 EntMin 0.0
Epoch 1, Batch 20/102, Loss=7.212009394168854
Loss made of: CE 0.9825748205184937, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.317573070526123 EntMin 0.0
Epoch 1, Batch 30/102, Loss=6.508327126502991
Loss made of: CE 0.8452130556106567, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.905526161193848 EntMin 0.0
Epoch 1, Batch 40/102, Loss=6.115690565109253
Loss made of: CE 0.8140389919281006, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6833391189575195 EntMin 0.0
Epoch 1, Batch 50/102, Loss=6.137860947847367
Loss made of: CE 0.8356591463088989, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.745242595672607 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.941275268793106
Loss made of: CE 0.8056938648223877, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.576035022735596 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.8448377549648285
Loss made of: CE 0.7727805972099304, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.57741641998291 EntMin 0.0
Epoch 1, Batch 80/102, Loss=5.624222445487976
Loss made of: CE 0.7644510269165039, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.284809589385986 EntMin 0.0
Epoch 1, Batch 90/102, Loss=5.01357934474945
Loss made of: CE 0.6118826866149902, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202833652496338 EntMin 0.0
Epoch 1, Batch 100/102, Loss=5.104604476690293
Loss made of: CE 0.7796233892440796, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.390270233154297 EntMin 0.0
Epoch 1, Class Loss=0.8971110582351685, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.8971110582351685, Class Loss=0.8971110582351685, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=5.615486943721772
Loss made of: CE 0.6090744733810425, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2450480461120605 EntMin 0.0
Epoch 2, Batch 20/102, Loss=5.336706435680389
Loss made of: CE 0.6541401147842407, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.160280227661133 EntMin 0.0
Epoch 2, Batch 30/102, Loss=5.434154736995697
Loss made of: CE 0.5993638038635254, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0524001121521 EntMin 0.0
Epoch 2, Batch 40/102, Loss=5.271969032287598
Loss made of: CE 0.5189792513847351, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.971859455108643 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.932451772689819
Loss made of: CE 0.4895971715450287, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.997333526611328 EntMin 0.0
Epoch 2, Batch 60/102, Loss=5.151821899414062
Loss made of: CE 0.5614291429519653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.678905487060547 EntMin 0.0
Epoch 2, Batch 70/102, Loss=5.565793061256409
Loss made of: CE 0.5945767164230347, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3426032066345215 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.9640370398759845
Loss made of: CE 0.4529592990875244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.873528480529785 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.920728051662445
Loss made of: CE 0.48142552375793457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.764767646789551 EntMin 0.0
Epoch 2, Batch 100/102, Loss=5.25070995092392
Loss made of: CE 0.45904964208602905, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.007309913635254 EntMin 0.0
Epoch 2, Class Loss=0.593090295791626, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.593090295791626, Class Loss=0.593090295791626, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=5.421473783254624
Loss made of: CE 0.6095999479293823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581993579864502 EntMin 0.0
Epoch 3, Batch 20/102, Loss=5.126979592442512
Loss made of: CE 0.6425060629844666, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.647310972213745 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.936822357773781
Loss made of: CE 0.5120210647583008, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.699234962463379 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.963693475723266
Loss made of: CE 0.3881310224533081, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4634785652160645 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.986041149497032
Loss made of: CE 0.5189977288246155, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9017720222473145 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.82707733809948
Loss made of: CE 0.5185635089874268, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.001506805419922 EntMin 0.0
Epoch 3, Batch 70/102, Loss=5.050254917144775
Loss made of: CE 0.48621582984924316, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.98774790763855 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.673424509167671
Loss made of: CE 0.5385951399803162, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9612271785736084 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.802942290902138
Loss made of: CE 0.5023165345191956, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.998945713043213 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.72405532002449
Loss made of: CE 0.40759479999542236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.136298179626465 EntMin 0.0
Epoch 3, Class Loss=0.5123100876808167, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.5123100876808167, Class Loss=0.5123100876808167, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=5.141244274377823
Loss made of: CE 0.45117244124412537, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4451422691345215 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.765390330553055
Loss made of: CE 0.4301517903804779, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.359813213348389 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.799905595183373
Loss made of: CE 0.4065588414669037, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7876908779144287 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.701506850123406
Loss made of: CE 0.4678212106227875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.118569374084473 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.874064418673515
Loss made of: CE 0.5531738996505737, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.315728187561035 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.923485514521599
Loss made of: CE 0.5123971700668335, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.477356433868408 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.664244690537453
Loss made of: CE 0.4314083456993103, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.145008087158203 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.8009939074516295
Loss made of: CE 0.47974252700805664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18123722076416 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.923758298158646
Loss made of: CE 0.5083597898483276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6991424560546875 EntMin 0.0
Epoch 4, Batch 100/102, Loss=5.097969099879265
Loss made of: CE 0.3992418050765991, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.135672092437744 EntMin 0.0
Epoch 4, Class Loss=0.48250117897987366, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.48250117897987366, Class Loss=0.48250117897987366, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/102, Loss=4.538034346699715
Loss made of: CE 0.48845481872558594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.908771514892578 EntMin 0.0
Epoch 5, Batch 20/102, Loss=5.0118121057748795
Loss made of: CE 0.4509737491607666, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5851335525512695 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.927284854650497
Loss made of: CE 0.5020896196365356, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.198525428771973 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.789270949363709
Loss made of: CE 0.4548800587654114, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.865035057067871 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.527823013067246
Loss made of: CE 0.5194401741027832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1570024490356445 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.66138131916523
Loss made of: CE 0.41324877738952637, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5496625900268555 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.822917088866234
Loss made of: CE 0.390697181224823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.504667282104492 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.820403972268105
Loss made of: CE 0.3855611979961395, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143423080444336 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.712619969248772
Loss made of: CE 0.5231396555900574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.107460021972656 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.530704697966575
Loss made of: CE 0.4618757665157318, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7839579582214355 EntMin 0.0
Epoch 5, Class Loss=0.45474451780319214, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.45474451780319214, Class Loss=0.45474451780319214, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.572469586133957
Loss made of: CE 0.5058085322380066, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.61285662651062 EntMin 0.0
Epoch 6, Batch 20/102, Loss=5.003585904836655
Loss made of: CE 0.43238744139671326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7016541957855225 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.2526918858289715
Loss made of: CE 0.4737734794616699, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6224541664123535 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.62722214460373
Loss made of: CE 0.42033320665359497, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.759042501449585 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.706088370084762
Loss made of: CE 0.4835790991783142, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.088391304016113 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.834450426697731
Loss made of: CE 0.38487839698791504, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8823556900024414 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.622225716710091
Loss made of: CE 0.4030420780181885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0144362449646 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.523714485764503
Loss made of: CE 0.4344594180583954, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.156576156616211 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.696897074580193
Loss made of: CE 0.43221205472946167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.290482044219971 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.659476011991501
Loss made of: CE 0.5006130933761597, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9962410926818848 EntMin 0.0
Epoch 6, Class Loss=0.45056140422821045, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.45056140422821045, Class Loss=0.45056140422821045, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5327720046043396, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.830488
Mean Acc: 0.532534
FreqW Acc: 0.703963
Mean IoU: 0.426610
Class IoU:
	class 0: 0.8298805
	class 1: 0.6393683
	class 2: 0.3171176
	class 3: 0.511761
	class 4: 0.5053247
	class 5: 0.22976846
	class 6: 0.6619515
	class 7: 0.7541265
	class 8: 0.68456966
	class 9: 0.0039557475
	class 10: 0.47472113
	class 11: 0.32149804
	class 12: 0.54935694
	class 13: 0.0
	class 14: 0.6266976
	class 15: 0.14227152
	class 16: 0.0
Class Acc:
	class 0: 0.9785336
	class 1: 0.6506937
	class 2: 0.592202
	class 3: 0.8398603
	class 4: 0.83230305
	class 5: 0.23234789
	class 6: 0.6702422
	class 7: 0.8031728
	class 8: 0.7149828
	class 9: 0.0039567584
	class 10: 0.6867487
	class 11: 0.37611097
	class 12: 0.73201334
	class 13: 0.0
	class 14: 0.79679996
	class 15: 0.14311746
	class 16: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[7, 11, 16, 18]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=5.033990719914437
Loss made of: CE 0.6403168439865112, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7267003059387207 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.026007178425789
Loss made of: CE 0.5791740417480469, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.034488677978516 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.790957343578339
Loss made of: CE 0.4823184609413147, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.368767738342285 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.821616137027741
Loss made of: CE 0.5177199840545654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084660530090332 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.038989859819412
Loss made of: CE 0.6013462543487549, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.703738689422607 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.9314799219369885
Loss made of: CE 0.4326145648956299, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7799296379089355 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.009272658824921
Loss made of: CE 0.49252232909202576, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.450760841369629 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.893495517969131
Loss made of: CE 0.5317995548248291, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.959242582321167 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.39475250840187
Loss made of: CE 0.42431169748306274, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7542338371276855 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.32876503765583
Loss made of: CE 0.5185074210166931, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0060133934021 EntMin 0.0
Epoch 1, Class Loss=0.5141872763633728, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.5141872763633728, Class Loss=0.5141872763633728, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.8804129421710964
Loss made of: CE 0.42733079195022583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.73856782913208 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.523597040772438
Loss made of: CE 0.47131937742233276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.266252040863037 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.715673094987869
Loss made of: CE 0.41253405809402466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.989663124084473 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.627144673466683
Loss made of: CE 0.43712323904037476, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.452771186828613 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.356512099504471
Loss made of: CE 0.3657822608947754, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.702263593673706 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.693097484111786
Loss made of: CE 0.45524701476097107, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.388632774353027 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.843206298351288
Loss made of: CE 0.49859780073165894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.413412094116211 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.5642602980136875
Loss made of: CE 0.4057755470275879, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.696920394897461 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 90/102, Loss=4.51343546807766
Loss made of: CE 0.4124990403652191, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.101296901702881 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.835017052292824
Loss made of: CE 0.45443424582481384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.620931625366211 EntMin 0.0
Epoch 2, Class Loss=0.46436893939971924, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.46436893939971924, Class Loss=0.46436893939971924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.8286051660776135
Loss made of: CE 0.47973379492759705, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0396199226379395 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.596859467029572
Loss made of: CE 0.49770116806030273, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.425560712814331 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.485259759426117
Loss made of: CE 0.48891109228134155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9652891159057617 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.663236787915229
Loss made of: CE 0.36674439907073975, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481441497802734 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.504799547791481
Loss made of: CE 0.445060670375824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.472049713134766 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.440601971745491
Loss made of: CE 0.4437045454978943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.699763774871826 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.741247162222862
Loss made of: CE 0.4055507183074951, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7453055381774902 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.48094671368599
Loss made of: CE 0.5904257297515869, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8465051651000977 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.45257485806942
Loss made of: CE 0.4630930423736572, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7370471954345703 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.408533051609993
Loss made of: CE 0.40255606174468994, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7897326946258545 EntMin 0.0
Epoch 3, Class Loss=0.45721980929374695, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.45721980929374695, Class Loss=0.45721980929374695, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.685087949037552
Loss made of: CE 0.4186999201774597, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.019879341125488 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.450782981514931
Loss made of: CE 0.3748754858970642, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.104648590087891 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.520923206210137
Loss made of: CE 0.4311106204986572, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5681328773498535 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.521076554059983
Loss made of: CE 0.3662211298942566, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.839154005050659 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.496855846047401
Loss made of: CE 0.5374990701675415, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.818263053894043 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.609079614281654
Loss made of: CE 0.4157952666282654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.188342571258545 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.408835202455521
Loss made of: CE 0.43635886907577515, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8965299129486084 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.616756230592728
Loss made of: CE 0.4894281327724457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9128642082214355 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.489707455039024
Loss made of: CE 0.4668458104133606, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.174391746520996 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.747903823852539
Loss made of: CE 0.4149675965309143, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.803077220916748 EntMin 0.0
Epoch 4, Class Loss=0.44978469610214233, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.44978469610214233, Class Loss=0.44978469610214233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.200104233622551
Loss made of: CE 0.4613703191280365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.267230033874512 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.647480973601342
Loss made of: CE 0.41522011160850525, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8782289028167725 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.745339891314506
Loss made of: CE 0.5190502405166626, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9797210693359375 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.440361070632934
Loss made of: CE 0.4224108159542084, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.563887357711792 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.370333394408226
Loss made of: CE 0.4657861590385437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.265778541564941 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.389560943841934
Loss made of: CE 0.40526363253593445, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4934823513031006 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.548362201452255
Loss made of: CE 0.41061052680015564, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.371041297912598 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.640570971369743
Loss made of: CE 0.38732337951660156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.267154693603516 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.438710525631905
Loss made of: CE 0.4755640923976898, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.805637836456299 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.460266986489296
Loss made of: CE 0.4383637309074402, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.805206775665283 EntMin 0.0
Epoch 5, Class Loss=0.4333353638648987, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4333353638648987, Class Loss=0.4333353638648987, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.363785350322724
Loss made of: CE 0.4567633271217346, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3064632415771484 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.680568543076515
Loss made of: CE 0.4016077518463135, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7584283351898193 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.1155318230390545
Loss made of: CE 0.4018405079841614, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3139243125915527 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.502292820811272
Loss made of: CE 0.4326428771018982, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8172659873962402 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.484516218304634
Loss made of: CE 0.48596200346946716, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7985196113586426 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.325257477164269
Loss made of: CE 0.4147047996520996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9048051834106445 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.396847310662269
Loss made of: CE 0.44689807295799255, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8151803016662598 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.260254102945328
Loss made of: CE 0.3752440810203552, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8533339500427246 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.3957590699195865
Loss made of: CE 0.4252655506134033, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.069573402404785 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.34743372797966
Loss made of: CE 0.43513354659080505, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.535885810852051 EntMin 0.0
Epoch 6, Class Loss=0.42350253462791443, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.42350253462791443, Class Loss=0.42350253462791443, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=5.061145561933517
Loss made of: CE 0.6766918897628784, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4636030197143555 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.694452679157257
Loss made of: CE 0.5523669719696045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.196843147277832 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 30/102, Loss=4.766403010487556
Loss made of: CE 0.45559000968933105, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.471681594848633 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.070413044095039
Loss made of: CE 0.4681575298309326, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.582974433898926 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.033478310704231
Loss made of: CE 0.4679659307003021, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.267643451690674 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.033796426653862
Loss made of: CE 0.46949681639671326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.791912078857422 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.808154565095902
Loss made of: CE 0.4809085726737976, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.019383907318115 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.789324834942818
Loss made of: CE 0.456472247838974, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.013823986053467 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.678588700294495
Loss made of: CE 0.5789107084274292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7424445152282715 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.66846017241478
Loss made of: CE 0.558208167552948, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9912593364715576 EntMin 0.0
Epoch 1, Class Loss=0.5184287428855896, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.5184287428855896, Class Loss=0.5184287428855896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.550119033455848
Loss made of: CE 0.44488853216171265, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7977848052978516 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.502373161911964
Loss made of: CE 0.42518675327301025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9572854042053223 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.579111725091934
Loss made of: CE 0.47773605585098267, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.154186248779297 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.7238410145044325
Loss made of: CE 0.49086201190948486, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035383224487305 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.7482835978269575
Loss made of: CE 0.4026082158088684, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.154566764831543 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.80955425798893
Loss made of: CE 0.6459312438964844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.05112886428833 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.454032558202743
Loss made of: CE 0.40704572200775146, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6414008140563965 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.5188572943210605
Loss made of: CE 0.4913899600505829, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.110844135284424 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.754771581292152
Loss made of: CE 0.5503883361816406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.766856670379639 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.87355198264122
Loss made of: CE 0.39936089515686035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.570316791534424 EntMin 0.0
Epoch 2, Class Loss=0.46345120668411255, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.46345120668411255, Class Loss=0.46345120668411255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.5519112855196
Loss made of: CE 0.48335298895835876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.085890769958496 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.5241872608661655
Loss made of: CE 0.521552562713623, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.735032558441162 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.802424141764641
Loss made of: CE 0.41031238436698914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.942616939544678 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.512789571285248
Loss made of: CE 0.3822605609893799, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9244747161865234 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.63308033645153
Loss made of: CE 0.4619649648666382, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.953042984008789 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.5211990743875505
Loss made of: CE 0.44880566000938416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.071397304534912 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.513308712840081
Loss made of: CE 0.36512649059295654, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.924069881439209 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.744107925891877
Loss made of: CE 0.3292430639266968, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5509562492370605 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.727387213706971
Loss made of: CE 0.3639414310455322, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7157058715820312 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.527543848752975
Loss made of: CE 0.3552742004394531, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.756833553314209 EntMin 0.0
Epoch 3, Class Loss=0.4402548372745514, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4402548372745514, Class Loss=0.4402548372745514, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.497628381848335
Loss made of: CE 0.45866698026657104, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.854747772216797 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.588310831785202
Loss made of: CE 0.3828681409358978, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.129356384277344 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.494418117403984
Loss made of: CE 0.5294058322906494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0219550132751465 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.576458790898323
Loss made of: CE 0.4392123818397522, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.783445358276367 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.714247804880142
Loss made of: CE 0.4358646273612976, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.432705879211426 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.68519834280014
Loss made of: CE 0.4295407235622406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.299659729003906 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.582385018467903
Loss made of: CE 0.5754682421684265, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.139890670776367 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.611450123786926
Loss made of: CE 0.5624427199363708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.407156467437744 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.580517408251763
Loss made of: CE 0.3672052025794983, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9375274181365967 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.676098221540451
Loss made of: CE 0.48716944456100464, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.059394836425781 EntMin 0.0
Epoch 4, Class Loss=0.4441188871860504, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4441188871860504, Class Loss=0.4441188871860504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.791597086191177
Loss made of: CE 0.4575406014919281, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.659855365753174 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.658365774154663
Loss made of: CE 0.4880211353302002, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9925670623779297 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.558974000811577
Loss made of: CE 0.35089024901390076, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6851978302001953 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.292750942707062
Loss made of: CE 0.4593384265899658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7763190269470215 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.521653258800507
Loss made of: CE 0.4549890160560608, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.270164966583252 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.236772665381432
Loss made of: CE 0.50032639503479, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7754428386688232 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.331050026416778
Loss made of: CE 0.49928098917007446, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6302201747894287 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.363141497969627
Loss made of: CE 0.35216331481933594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.11079740524292 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.362022423744202
Loss made of: CE 0.5241838693618774, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5201034545898438 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.489055019617081
Loss made of: CE 0.4229319095611572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.30595588684082 EntMin 0.0
Epoch 5, Class Loss=0.42754387855529785, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.42754387855529785, Class Loss=0.42754387855529785, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.722740593552589
Loss made of: CE 0.4018033444881439, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.088930130004883 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.262242874503135
Loss made of: CE 0.38947638869285583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.662337303161621 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.15175399184227
Loss made of: CE 0.4502204358577728, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8897957801818848 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.704633864760399
Loss made of: CE 0.4800400137901306, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.027524471282959 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.277158078551293
Loss made of: CE 0.3909587562084198, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7333390712738037 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.574250662326813
Loss made of: CE 0.5332093238830566, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.981598377227783 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.197332522273063
Loss made of: CE 0.45737147331237793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9841604232788086 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.39493710398674
Loss made of: CE 0.4230121076107025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7376232147216797 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.437226492166519
Loss made of: CE 0.4384591281414032, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.642643690109253 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.510651665925979
Loss made of: CE 0.4772801399230957, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8458409309387207 EntMin 0.0
Epoch 6, Class Loss=0.42192038893699646, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.42192038893699646, Class Loss=0.42192038893699646, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/105, Loss=5.430566942691803
Loss made of: CE 0.9443041682243347, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.377628326416016 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.901713705062866
Loss made of: CE 0.6127821803092957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.608175277709961 EntMin 0.0
Epoch 1, Batch 30/105, Loss=4.623651534318924
Loss made of: CE 0.5468722581863403, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2492475509643555 EntMin 0.0
Epoch 1, Batch 40/105, Loss=4.926200702786446
Loss made of: CE 0.471768319606781, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.75354266166687 EntMin 0.0
Epoch 1, Batch 50/105, Loss=4.761168572306633
Loss made of: CE 0.5652246475219727, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.014310836791992 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.754530254006386
Loss made of: CE 0.6390242576599121, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.156261444091797 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.7932344019413
Loss made of: CE 0.417047917842865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.415684223175049 EntMin 0.0
Epoch 1, Batch 80/105, Loss=5.0432906717062
Loss made of: CE 0.5217527747154236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.854674339294434 EntMin 0.0
Epoch 1, Batch 90/105, Loss=4.998242741823196
Loss made of: CE 0.4618481993675232, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.453397750854492 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.716336217522621
Loss made of: CE 0.3916945159435272, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.386680603027344 EntMin 0.0
Epoch 1, Class Loss=0.5401026606559753, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.5401026606559753, Class Loss=0.5401026606559753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/105, Loss=4.554009601473808
Loss made of: CE 0.4662451148033142, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.948908805847168 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.703129541873932
Loss made of: CE 0.5180448889732361, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.064633846282959 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.791007995605469
Loss made of: CE 0.5632026791572571, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.642024040222168 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.802725070714951
Loss made of: CE 0.43154990673065186, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.378561973571777 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.609485483169555
Loss made of: CE 0.4214445948600769, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.711238384246826 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 60/105, Loss=4.74873808324337
Loss made of: CE 0.37955331802368164, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8734326362609863 EntMin 0.0
Epoch 2, Batch 70/105, Loss=5.06124295592308
Loss made of: CE 0.46949025988578796, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.879213333129883 EntMin 0.0
Epoch 2, Batch 80/105, Loss=4.816695153713226
Loss made of: CE 0.48842453956604004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.197150707244873 EntMin 0.0
Epoch 2, Batch 90/105, Loss=4.9831801563501354
Loss made of: CE 0.46214765310287476, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.596196174621582 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.674718451499939
Loss made of: CE 0.4344794750213623, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.945659637451172 EntMin 0.0
Epoch 2, Class Loss=0.4797302484512329, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.4797302484512329, Class Loss=0.4797302484512329, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/105, Loss=4.645182871818543
Loss made of: CE 0.42374011874198914, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.945590019226074 EntMin 0.0
Epoch 3, Batch 20/105, Loss=4.869586506485939
Loss made of: CE 0.40998661518096924, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.435731887817383 EntMin 0.0
Epoch 3, Batch 30/105, Loss=4.576921528577804
Loss made of: CE 0.41978392004966736, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.224632263183594 EntMin 0.0
Epoch 3, Batch 40/105, Loss=4.890984648466111
Loss made of: CE 0.5304516553878784, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.12043571472168 EntMin 0.0
Epoch 3, Batch 50/105, Loss=4.600658094882965
Loss made of: CE 0.4961788058280945, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.002826690673828 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.734594663977623
Loss made of: CE 0.6130505800247192, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.935349225997925 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.565099394321441
Loss made of: CE 0.4967321753501892, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.789999008178711 EntMin 0.0
Epoch 3, Batch 80/105, Loss=4.725267785787582
Loss made of: CE 0.5775712728500366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195284366607666 EntMin 0.0
Epoch 3, Batch 90/105, Loss=4.621292087435722
Loss made of: CE 0.4816788136959076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.151364803314209 EntMin 0.0
Epoch 3, Batch 100/105, Loss=4.590674972534179
Loss made of: CE 0.3764120042324066, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.56398868560791 EntMin 0.0
Epoch 3, Class Loss=0.46610015630722046, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.46610015630722046, Class Loss=0.46610015630722046, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/105, Loss=4.426104918122292
Loss made of: CE 0.29899296164512634, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.883300542831421 EntMin 0.0
Epoch 4, Batch 20/105, Loss=4.539765769243241
Loss made of: CE 0.4276803731918335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.999660015106201 EntMin 0.0
Epoch 4, Batch 30/105, Loss=4.4631547898054125
Loss made of: CE 0.390598863363266, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8590011596679688 EntMin 0.0
Epoch 4, Batch 40/105, Loss=4.728801441192627
Loss made of: CE 0.33191150426864624, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.880828857421875 EntMin 0.0
Epoch 4, Batch 50/105, Loss=4.544240036606789
Loss made of: CE 0.5035603046417236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.070065498352051 EntMin 0.0
Epoch 4, Batch 60/105, Loss=4.586841189861298
Loss made of: CE 0.4953709840774536, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9204957485198975 EntMin 0.0
Epoch 4, Batch 70/105, Loss=4.653468319773674
Loss made of: CE 0.4134178161621094, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7524402141571045 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.4687028467655185
Loss made of: CE 0.3975154161453247, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.133510589599609 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.653621000051499
Loss made of: CE 0.4661165177822113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581804275512695 EntMin 0.0
Epoch 4, Batch 100/105, Loss=4.611551949381829
Loss made of: CE 0.48529863357543945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.98765230178833 EntMin 0.0
Epoch 4, Class Loss=0.45216992497444153, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.45216992497444153, Class Loss=0.45216992497444153, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/105, Loss=4.938291975855828
Loss made of: CE 0.5453850626945496, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.10232400894165 EntMin 0.0
Epoch 5, Batch 20/105, Loss=4.553566190600395
Loss made of: CE 0.42084676027297974, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084889888763428 EntMin 0.0
Epoch 5, Batch 30/105, Loss=4.47656978070736
Loss made of: CE 0.4254912734031677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7261598110198975 EntMin 0.0
Epoch 5, Batch 40/105, Loss=5.046634030342102
Loss made of: CE 0.5585366487503052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.893489837646484 EntMin 0.0
Epoch 5, Batch 50/105, Loss=4.454845571517945
Loss made of: CE 0.42006516456604004, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1580610275268555 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.641238400340081
Loss made of: CE 0.43807730078697205, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.939488410949707 EntMin 0.0
Epoch 5, Batch 70/105, Loss=4.416373854875564
Loss made of: CE 0.4843449592590332, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7969532012939453 EntMin 0.0
Epoch 5, Batch 80/105, Loss=4.734954094886779
Loss made of: CE 0.4519379138946533, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5822713375091553 EntMin 0.0
Epoch 5, Batch 90/105, Loss=4.627378046512604
Loss made of: CE 0.4869820475578308, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9780118465423584 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.509564551711082
Loss made of: CE 0.4798862338066101, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.535614967346191 EntMin 0.0
Epoch 5, Class Loss=0.4429944157600403, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.4429944157600403, Class Loss=0.4429944157600403, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/105, Loss=4.449295341968536
Loss made of: CE 0.4124428927898407, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7721409797668457 EntMin 0.0
Epoch 6, Batch 20/105, Loss=4.906910851597786
Loss made of: CE 0.6532629728317261, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3893585205078125 EntMin 0.0
Epoch 6, Batch 30/105, Loss=4.441752615571022
Loss made of: CE 0.43494415283203125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4637584686279297 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.285888999700546
Loss made of: CE 0.36651086807250977, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4470126628875732 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.384919434785843
Loss made of: CE 0.37360113859176636, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.671328067779541 EntMin 0.0
Epoch 6, Batch 60/105, Loss=4.542743030190468
Loss made of: CE 0.4132189452648163, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.116589546203613 EntMin 0.0
Epoch 6, Batch 70/105, Loss=4.31278403699398
Loss made of: CE 0.4295446574687958, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.097102165222168 EntMin 0.0
Epoch 6, Batch 80/105, Loss=4.733138433098793
Loss made of: CE 0.38279247283935547, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.522114276885986 EntMin 0.0
Epoch 6, Batch 90/105, Loss=4.36547603905201
Loss made of: CE 0.37908124923706055, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4902195930480957 EntMin 0.0
Epoch 6, Batch 100/105, Loss=4.393761694431305
Loss made of: CE 0.37235745787620544, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.816624641418457 EntMin 0.0
Epoch 6, Class Loss=0.429096519947052, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.429096519947052, Class Loss=0.429096519947052, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=4.696718949079513
Loss made of: CE 0.6100195050239563, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5120177268981934 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.401417922973633
Loss made of: CE 0.5205987691879272, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.86319637298584 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.8488494515419
Loss made of: CE 0.4271899461746216, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7238030433654785 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.678798338770866
Loss made of: CE 0.4755801558494568, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8429322242736816 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.50423167347908
Loss made of: CE 0.41640374064445496, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.79359769821167 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.317262983322143
Loss made of: CE 0.5157147645950317, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.12398624420166 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.964390012621879
Loss made of: CE 0.5844509601593018, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.675298690795898 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.677000489830971
Loss made of: CE 0.38925713300704956, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9410886764526367 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.838387793302536
Loss made of: CE 0.5159761905670166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6375584602355957 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.899124619364739
Loss made of: CE 0.5275028944015503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0482258796691895 EntMin 0.0
Epoch 1, Class Loss=0.508653998374939, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.508653998374939, Class Loss=0.508653998374939, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/102, Loss=4.555263814330101
Loss made of: CE 0.47079986333847046, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9216055870056152 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.699169600009919
Loss made of: CE 0.45765921473503113, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.99482798576355 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 30/102, Loss=4.753185987472534
Loss made of: CE 0.5496448278427124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.151997089385986 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.7857040733098986
Loss made of: CE 0.4334504008293152, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.778257846832275 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.505566340684891
Loss made of: CE 0.44299349188804626, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.567281723022461 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.9154104620218275
Loss made of: CE 0.49123892188072205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.614124774932861 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.968475511670112
Loss made of: CE 0.36229485273361206, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.826820135116577 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.715869334340096
Loss made of: CE 0.6885502338409424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.733787536621094 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.511296194791794
Loss made of: CE 0.4786381721496582, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.576743125915527 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.525842329859733
Loss made of: CE 0.41450726985931396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.081637859344482 EntMin 0.0
Epoch 2, Class Loss=0.45329248905181885, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.45329248905181885, Class Loss=0.45329248905181885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/102, Loss=4.783725309371948
Loss made of: CE 0.4355503022670746, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9547746181488037 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.992391809821129
Loss made of: CE 0.4363672435283661, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.767154693603516 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.511125123500824
Loss made of: CE 0.42680758237838745, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251507759094238 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.704169097542763
Loss made of: CE 0.3697361350059509, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9283804893493652 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.783318275213242
Loss made of: CE 0.4526824355125427, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.614546775817871 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.531939554214477
Loss made of: CE 0.43870624899864197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.791536331176758 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.66491566002369
Loss made of: CE 0.38700127601623535, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.110282897949219 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.617067661881447
Loss made of: CE 0.40140265226364136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.065503120422363 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.545773229002952
Loss made of: CE 0.41945964097976685, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.915341854095459 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.532960671186447
Loss made of: CE 0.3351231515407562, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.832385540008545 EntMin 0.0
Epoch 3, Class Loss=0.4451214075088501, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.4451214075088501, Class Loss=0.4451214075088501, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/102, Loss=4.768161213397979
Loss made of: CE 0.48794621229171753, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.057664394378662 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.67869893014431
Loss made of: CE 0.4240686297416687, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.200305938720703 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.724237123131752
Loss made of: CE 0.4040963649749756, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.817620038986206 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.288641503453254
Loss made of: CE 0.5356953144073486, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.072129249572754 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.70709410905838
Loss made of: CE 0.4034590721130371, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.192956924438477 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.5275371193885805
Loss made of: CE 0.4935776889324188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7699685096740723 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.425629729032517
Loss made of: CE 0.3612022399902344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.28917121887207 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.341066843271255
Loss made of: CE 0.37484389543533325, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.919732093811035 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.915844139456749
Loss made of: CE 0.4099307060241699, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.330849647521973 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.435485491156578
Loss made of: CE 0.397082656621933, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.845109701156616 EntMin 0.0
Epoch 4, Class Loss=0.43816739320755005, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.43816739320755005, Class Loss=0.43816739320755005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/102, Loss=4.343038693070412
Loss made of: CE 0.3956393599510193, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6740243434906006 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.755023637413979
Loss made of: CE 0.39769721031188965, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.343316078186035 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.496201121807099
Loss made of: CE 0.3911069333553314, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9474892616271973 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.432477238774299
Loss made of: CE 0.4086228311061859, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.409563064575195 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.4423580259084705
Loss made of: CE 0.4072342813014984, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.635812759399414 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.366206312179566
Loss made of: CE 0.406836599111557, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.141203880310059 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.247808742523193
Loss made of: CE 0.5101802349090576, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7066211700439453 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.313619384169579
Loss made of: CE 0.414959192276001, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.111186981201172 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.908827257156372
Loss made of: CE 0.48380833864212036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.197925090789795 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.304683899879455
Loss made of: CE 0.38274550437927246, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.65012264251709 EntMin 0.0
Epoch 5, Class Loss=0.42901894450187683, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.42901894450187683, Class Loss=0.42901894450187683, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/102, Loss=4.554863828420639
Loss made of: CE 0.4246346950531006, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9574687480926514 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.614661887288094
Loss made of: CE 0.39391669631004333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.032715797424316 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.629925611615181
Loss made of: CE 0.39962339401245117, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.307035446166992 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.402096825838089
Loss made of: CE 0.44829219579696655, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9565742015838623 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.885653302073479
Loss made of: CE 0.33855757117271423, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.995278835296631 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.449958246946335
Loss made of: CE 0.4248625934123993, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.101387023925781 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.611047384142876
Loss made of: CE 0.41701507568359375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.276659965515137 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.511154055595398
Loss made of: CE 0.4968125820159912, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7925028800964355 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.45472774207592
Loss made of: CE 0.4715878963470459, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.867509841918945 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.376858255267143
Loss made of: CE 0.4744389057159424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.375738143920898 EntMin 0.0
Epoch 6, Class Loss=0.4196941554546356, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.4196941554546356, Class Loss=0.4196941554546356, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4336376488208771, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.873659
Mean Acc: 0.596959
FreqW Acc: 0.783135
Mean IoU: 0.473227
Class IoU:
	class 0: 0.8834788
	class 1: 0.73403496
	class 2: 0.33773202
	class 3: 0.4681703
	class 4: 0.4709203
	class 5: 0.2547185
	class 6: 0.7112726
	class 7: 0.76596355
	class 8: 0.7139938
	class 9: 0.00038284986
	class 10: 0.46760178
	class 11: 0.30113775
	class 12: 0.5605972
	class 13: 5.1935663e-06
	class 14: 0.69289047
	class 15: 0.68196005
	class 16: 0.0
Class Acc:
	class 0: 0.96669513
	class 1: 0.7746042
	class 2: 0.70644444
	class 3: 0.93580437
	class 4: 0.87580764
	class 5: 0.2583114
	class 6: 0.72333604
	class 7: 0.82513905
	class 8: 0.754674
	class 9: 0.00038284986
	class 10: 0.65681803
	class 11: 0.35421303
	class 12: 0.7382579
	class 13: 5.1935663e-06
	class 14: 0.7806168
	class 15: 0.7971897
	class 16: 0.0

federated global round: 17, step: 3
select part of clients to conduct local training
[5, 3, 17, 11]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.05690701007843
Loss made of: CE 0.4322062134742737, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.286090612411499 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.417256337404251
Loss made of: CE 0.33797407150268555, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.624643087387085 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.105474191904068
Loss made of: CE 0.3448284864425659, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.375613212585449 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.1885168761014935
Loss made of: CE 0.368534654378891, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.099111557006836 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.192098742723465
Loss made of: CE 0.3968139588832855, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9133059978485107 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.617771258950233
Loss made of: CE 0.5795336961746216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400809288024902 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.680351966619492
Loss made of: CE 0.49245357513427734, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9227819442749023 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.528892359137535
Loss made of: CE 0.4168081283569336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.146734237670898 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.3742845326662065
Loss made of: CE 0.41106683015823364, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.954810857772827 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.161122235655784
Loss made of: CE 0.4110187888145447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.284648895263672 EntMin 0.0
Epoch 1, Class Loss=0.41337326169013977, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.41337326169013977, Class Loss=0.41337326169013977, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.530033254623413
Loss made of: CE 0.3845651149749756, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.913088321685791 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.424929690361023
Loss made of: CE 0.3528304994106293, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4538915157318115 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.352563646435738
Loss made of: CE 0.5301835536956787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.714658260345459 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.495163607597351
Loss made of: CE 0.4174792170524597, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.162490367889404 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.645670446753502
Loss made of: CE 0.3551306128501892, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18421745300293 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.5679835975170135
Loss made of: CE 0.42554041743278503, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.786987066268921 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.309976872801781
Loss made of: CE 0.42174220085144043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.011496067047119 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.662223809957505
Loss made of: CE 0.5413373708724976, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.912313461303711 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.301549538969994
Loss made of: CE 0.35966095328330994, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5157785415649414 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.747276538610459
Loss made of: CE 0.4309595227241516, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9160828590393066 EntMin 0.0
Epoch 2, Class Loss=0.4188544452190399, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.4188544452190399, Class Loss=0.4188544452190399, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.338450863957405
Loss made of: CE 0.3528890609741211, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8893496990203857 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.416769772768021
Loss made of: CE 0.3206462860107422, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.987030029296875 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.339715069532394
Loss made of: CE 0.47826260328292847, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4381043910980225 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.621492800116539
Loss made of: CE 0.37243133783340454, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.338921546936035 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.420104068517685
Loss made of: CE 0.44278037548065186, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6192526817321777 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.35074474811554
Loss made of: CE 0.5179765224456787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.321949005126953 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.317489302158355
Loss made of: CE 0.41695016622543335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8351352214813232 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.715798863768578
Loss made of: CE 0.42687010765075684, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5990519523620605 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 90/102, Loss=4.505264076590538
Loss made of: CE 0.31134775280952454, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.690089225769043 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.077712947130204
Loss made of: CE 0.3310587406158447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8537118434906006 EntMin 0.0
Epoch 3, Class Loss=0.4034043848514557, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.4034043848514557, Class Loss=0.4034043848514557, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.401404920220375
Loss made of: CE 0.3360242545604706, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.743086814880371 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.106532913446427
Loss made of: CE 0.43814706802368164, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8226068019866943 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.625272950530052
Loss made of: CE 0.3450619578361511, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.029991149902344 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.440651562809944
Loss made of: CE 0.31985291838645935, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.747952461242676 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.29741558432579
Loss made of: CE 0.366776704788208, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8866803646087646 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.472567912936211
Loss made of: CE 0.43221378326416016, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.730987310409546 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.356896591186524
Loss made of: CE 0.3373505771160126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.288715839385986 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.270524802803993
Loss made of: CE 0.3782389163970947, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.651361465454102 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.447124779224396
Loss made of: CE 0.45143789052963257, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.827155113220215 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.558313950896263
Loss made of: CE 0.5353206992149353, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9586615562438965 EntMin 0.0
Epoch 4, Class Loss=0.3948839604854584, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.3948839604854584, Class Loss=0.3948839604854584, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.384730577468872
Loss made of: CE 0.4542216658592224, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8792858123779297 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.25315021276474
Loss made of: CE 0.38173145055770874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.857028007507324 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.153294721245766
Loss made of: CE 0.3712906837463379, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.91656494140625 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.225537562370301
Loss made of: CE 0.5235764980316162, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6599559783935547 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.330155935883522
Loss made of: CE 0.3725799322128296, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.76637601852417 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.263360801339149
Loss made of: CE 0.38062745332717896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5165505409240723 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.337041118741036
Loss made of: CE 0.31794387102127075, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9410674571990967 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.7630285054445265
Loss made of: CE 0.3570493161678314, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.631108045578003 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.1677876770496365
Loss made of: CE 0.41168975830078125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.825211524963379 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.364708709716797
Loss made of: CE 0.32067328691482544, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8826491832733154 EntMin 0.0
Epoch 5, Class Loss=0.39009279012680054, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.39009279012680054, Class Loss=0.39009279012680054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.2370048493146895
Loss made of: CE 0.32284122705459595, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.800854444503784 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.1346842586994175
Loss made of: CE 0.3497692048549652, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2484822273254395 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.162126243114471
Loss made of: CE 0.3683025538921356, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6224136352539062 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.451740488409996
Loss made of: CE 0.4038934111595154, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.715339183807373 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.396388989686966
Loss made of: CE 0.5299084186553955, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.857946872711182 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.250808128714562
Loss made of: CE 0.30158424377441406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4296040534973145 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.15457906126976
Loss made of: CE 0.3393383026123047, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.565488338470459 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.044072946906089
Loss made of: CE 0.33430129289627075, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7980313301086426 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.56450502872467
Loss made of: CE 0.40566056966781616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.91733717918396 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.581866025924683
Loss made of: CE 0.4105953574180603, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.612196922302246 EntMin 0.0
Epoch 6, Class Loss=0.38686850666999817, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.38686850666999817, Class Loss=0.38686850666999817, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.219853702187538
Loss made of: CE 0.5229666829109192, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.456756591796875 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.1950667887926105
Loss made of: CE 0.4453568160533905, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2490813732147217 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.455515602231026
Loss made of: CE 0.4676138758659363, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.421434164047241 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.325511705875397
Loss made of: CE 0.5200248956680298, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.384827613830566 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.499377864599228
Loss made of: CE 0.5262936353683472, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.196466445922852 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.531630450487137
Loss made of: CE 0.4161624610424042, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7214555740356445 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.453019317984581
Loss made of: CE 0.4071677625179291, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.902253150939941 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.396667549014092
Loss made of: CE 0.45547449588775635, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3040947914123535 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.636180186271668
Loss made of: CE 0.4211784303188324, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.765315532684326 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.496767202019692
Loss made of: CE 0.3910709619522095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.632751941680908 EntMin 0.0
Epoch 1, Class Loss=0.4169657230377197, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.4169657230377197, Class Loss=0.4169657230377197, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.494808343052864
Loss made of: CE 0.4784531593322754, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.104693412780762 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.31692912876606
Loss made of: CE 0.46112996339797974, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.670163631439209 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.634390673041343
Loss made of: CE 0.387199342250824, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6954140663146973 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.379136300086975
Loss made of: CE 0.3530728816986084, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.386526584625244 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.47551905810833
Loss made of: CE 0.4066481590270996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4303946495056152 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.41811757683754
Loss made of: CE 0.353108286857605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.631411552429199 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.4563084751367565
Loss made of: CE 0.36159443855285645, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195786476135254 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.492941269278527
Loss made of: CE 0.361802339553833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.321719169616699 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.554835656285286
Loss made of: CE 0.40448492765426636, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6774404048919678 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.703476092219352
Loss made of: CE 0.3871326446533203, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7343311309814453 EntMin 0.0
Epoch 2, Class Loss=0.4094766080379486, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.4094766080379486, Class Loss=0.4094766080379486, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.60654533803463
Loss made of: CE 0.3167486786842346, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9221343994140625 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.3771012216806415
Loss made of: CE 0.3927229046821594, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4711198806762695 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.567549270391464
Loss made of: CE 0.4029526114463806, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.081395626068115 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.341017788648605
Loss made of: CE 0.3560067117214203, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.133943557739258 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.7882075846195224
Loss made of: CE 0.43545883893966675, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.16534948348999 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.2275400221347805
Loss made of: CE 0.4495254456996918, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4807443618774414 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.580378141999245
Loss made of: CE 0.275592565536499, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.088874816894531 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.590760806202889
Loss made of: CE 0.4045281410217285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9046149253845215 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.326289987564087
Loss made of: CE 0.3606402575969696, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5844006538391113 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.397980430722237
Loss made of: CE 0.4589628577232361, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.266447067260742 EntMin 0.0
Epoch 3, Class Loss=0.4060613811016083, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.4060613811016083, Class Loss=0.4060613811016083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.311633214354515
Loss made of: CE 0.3932593762874603, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7004647254943848 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.60764190852642
Loss made of: CE 0.36676108837127686, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.791933059692383 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.343182307481766
Loss made of: CE 0.3824481964111328, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6680245399475098 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.6479661673307415
Loss made of: CE 0.4188333749771118, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6382558345794678 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.4794725477695465
Loss made of: CE 0.36206719279289246, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.867948293685913 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.376153653860092
Loss made of: CE 0.5765656232833862, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.012411594390869 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 70/102, Loss=4.811301466822624
Loss made of: CE 0.4475066363811493, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0336151123046875 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.560181775689125
Loss made of: CE 0.28467288613319397, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.041740894317627 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.381783026456833
Loss made of: CE 0.3792782425880432, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.551870346069336 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.426727357506752
Loss made of: CE 0.4257752299308777, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8421549797058105 EntMin 0.0
Epoch 4, Class Loss=0.4051061272621155, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.4051061272621155, Class Loss=0.4051061272621155, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.275288039445877
Loss made of: CE 0.4884343147277832, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8209378719329834 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.436882337927818
Loss made of: CE 0.4554315209388733, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6747641563415527 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.270526197552681
Loss made of: CE 0.4170766770839691, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4632911682128906 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.404011708498001
Loss made of: CE 0.4444422721862793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5237789154052734 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.530167093873024
Loss made of: CE 0.2965852916240692, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.771103620529175 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.217149198055267
Loss made of: CE 0.3112116754055023, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.617389678955078 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.557839244604111
Loss made of: CE 0.39145755767822266, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.874967098236084 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.7357737809419636
Loss made of: CE 0.3796670436859131, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7265496253967285 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.271304097771645
Loss made of: CE 0.3558635413646698, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.978395462036133 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.268419066071511
Loss made of: CE 0.33866938948631287, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6213042736053467 EntMin 0.0
Epoch 5, Class Loss=0.3884815573692322, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.3884815573692322, Class Loss=0.3884815573692322, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.259581333398819
Loss made of: CE 0.36090144515037537, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.461141586303711 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.225745928287506
Loss made of: CE 0.3803499639034271, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5852973461151123 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.334440723061562
Loss made of: CE 0.3328438997268677, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.48914098739624 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.027946323156357
Loss made of: CE 0.3593997657299042, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4210915565490723 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.289035758376121
Loss made of: CE 0.4083020091056824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389217376708984 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.272403949499131
Loss made of: CE 0.35769209265708923, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5857315063476562 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.031039580702782
Loss made of: CE 0.4048076868057251, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6306376457214355 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.199156039953232
Loss made of: CE 0.41924262046813965, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.614917278289795 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.515260362625122
Loss made of: CE 0.40877556800842285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.446357488632202 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.370792907476425
Loss made of: CE 0.29720860719680786, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6918115615844727 EntMin 0.0
Epoch 6, Class Loss=0.3876612186431885, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.3876612186431885, Class Loss=0.3876612186431885, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=6.156507623195648
Loss made of: CE 0.8320683240890503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.751971244812012 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.4343487322330475
Loss made of: CE 0.7970864176750183, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.552260398864746 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.8813955187797546, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.8813955187797546, Class Loss=0.8813955187797546, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/23, Loss=5.059501737356186
Loss made of: CE 0.6390108466148376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.840449094772339 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.887299433350563
Loss made of: CE 0.44682401418685913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.236164093017578 EntMin 0.0
Epoch 2, Class Loss=0.6615572571754456, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.6615572571754456, Class Loss=0.6615572571754456, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/23, Loss=4.7865124344825745
Loss made of: CE 0.4383810758590698, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.088569641113281 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.7428654581308365
Loss made of: CE 0.5967526435852051, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.336796760559082 EntMin 0.0
Epoch 3, Class Loss=0.5344980359077454, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.5344980359077454, Class Loss=0.5344980359077454, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/23, Loss=4.796730872988701
Loss made of: CE 0.6029488444328308, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8797707557678223 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.515209448337555
Loss made of: CE 0.3817262053489685, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.184130668640137 EntMin 0.0
Epoch 4, Class Loss=0.4806451201438904, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.4806451201438904, Class Loss=0.4806451201438904, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/23, Loss=4.590896552801132
Loss made of: CE 0.44030237197875977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.26548957824707 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.337950918078422
Loss made of: CE 0.329985111951828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.123422145843506 EntMin 0.0
Epoch 5, Class Loss=0.436635822057724, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.436635822057724, Class Loss=0.436635822057724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/23, Loss=4.46215018928051
Loss made of: CE 0.4486209750175476, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8458361625671387 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.33984722495079
Loss made of: CE 0.4394876956939697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.83585786819458 EntMin 0.0
Epoch 6, Class Loss=0.4076082408428192, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.4076082408428192, Class Loss=0.4076082408428192, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.097690635919571
Loss made of: CE 0.46274682879447937, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8838677406311035 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.209384787082672
Loss made of: CE 0.43433690071105957, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.772152900695801 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.184279918670654
Loss made of: CE 0.3639647364616394, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.213714599609375 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.523855382204056
Loss made of: CE 0.3575488328933716, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.187814712524414 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.4704206645488735
Loss made of: CE 0.35447168350219727, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.891623020172119 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.435599738359452
Loss made of: CE 0.3618627190589905, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3951773643493652 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.243035954236984
Loss made of: CE 0.3478303849697113, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.824583053588867 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.373807513713837
Loss made of: CE 0.3904958665370941, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481733798980713 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.158408007025718
Loss made of: CE 0.488687127828598, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3389787673950195 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.236388108134269
Loss made of: CE 0.49786266684532166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7820229530334473 EntMin 0.0
Epoch 1, Class Loss=0.41243210434913635, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.41243210434913635, Class Loss=0.41243210434913635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000600
Epoch 2, Batch 10/102, Loss=4.187784472107888
Loss made of: CE 0.3783949315547943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4754538536071777 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.117732611298561
Loss made of: CE 0.39261874556541443, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6970133781433105 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.268267777562142
Loss made of: CE 0.4654778838157654, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.617222785949707 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.2447739988565445
Loss made of: CE 0.45109114050865173, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3626656532287598 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.246150487661362
Loss made of: CE 0.35441216826438904, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.778052806854248 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.299588936567306
Loss made of: CE 0.5757865905761719, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4705357551574707 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.9571373045444487
Loss made of: CE 0.3803481459617615, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3557775020599365 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.242160618305206
Loss made of: CE 0.4776679277420044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.269383430480957 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.245831748843193
Loss made of: CE 0.5114960670471191, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.617546081542969 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.405202159285546
Loss made of: CE 0.3844623565673828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.017056941986084 EntMin 0.0
Epoch 2, Class Loss=0.4101583659648895, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.4101583659648895, Class Loss=0.4101583659648895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/102, Loss=4.214853754639625
Loss made of: CE 0.4196934103965759, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.012167930603027 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.161175268888473
Loss made of: CE 0.4386786222457886, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2126245498657227 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.215032917261124
Loss made of: CE 0.3781876564025879, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.450003623962402 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.206250551342964
Loss made of: CE 0.3611764907836914, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7863681316375732 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.371382343769073
Loss made of: CE 0.4427090585231781, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4221043586730957 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.183214274048805
Loss made of: CE 0.4285409152507782, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.717895984649658 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.018737253546715
Loss made of: CE 0.31052860617637634, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8246545791625977 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.437220114469528
Loss made of: CE 0.33949780464172363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.33211612701416 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.365746015310288
Loss made of: CE 0.2897323966026306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.274704933166504 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.171693566441536
Loss made of: CE 0.34995901584625244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2050912380218506 EntMin 0.0
Epoch 3, Class Loss=0.40006017684936523, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.40006017684936523, Class Loss=0.40006017684936523, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/102, Loss=4.112649300694466
Loss made of: CE 0.4111664295196533, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.288994550704956 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.38889739215374
Loss made of: CE 0.3454614579677582, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358871936798096 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.185654240846634
Loss made of: CE 0.4084356725215912, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.924278736114502 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.097575590014458
Loss made of: CE 0.40546607971191406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2549147605895996 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.3713119447231295
Loss made of: CE 0.46081095933914185, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.880539894104004 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.3411570399999615
Loss made of: CE 0.3585233688354492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8893487453460693 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.289567396044731
Loss made of: CE 0.5364614725112915, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6702704429626465 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.373190855979919
Loss made of: CE 0.5094254016876221, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.752021789550781 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.311677938699722
Loss made of: CE 0.30417031049728394, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6340909004211426 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.235177320241928
Loss made of: CE 0.44688472151756287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.715721130371094 EntMin 0.0
Epoch 4, Class Loss=0.4056617319583893, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4056617319583893, Class Loss=0.4056617319583893, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000504
Epoch 5, Batch 10/102, Loss=4.477419599890709
Loss made of: CE 0.43954527378082275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5278804302215576 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.378903993964196
Loss made of: CE 0.47742050886154175, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.766681671142578 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.2348505586385725
Loss made of: CE 0.31118202209472656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.464059352874756 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.068905794620514
Loss made of: CE 0.37008771300315857, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8246800899505615 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.311854887008667
Loss made of: CE 0.4536247253417969, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.186525344848633 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.954714611172676
Loss made of: CE 0.4780570864677429, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7829060554504395 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.017923879623413
Loss made of: CE 0.46526989340782166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5778446197509766 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.168180775642395
Loss made of: CE 0.3030989170074463, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8090484142303467 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.193093675374985
Loss made of: CE 0.42382383346557617, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3196892738342285 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.251088851690293
Loss made of: CE 0.37709933519363403, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1226348876953125 EntMin 0.0
Epoch 5, Class Loss=0.39232027530670166, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.39232027530670166, Class Loss=0.39232027530670166, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000471
Epoch 6, Batch 10/102, Loss=4.433962053060531
Loss made of: CE 0.35632485151290894, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.554987907409668 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.046251702308655
Loss made of: CE 0.3585588335990906, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.562411069869995 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.8988916337490083
Loss made of: CE 0.36399006843566895, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.610145330429077 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.273082908987999
Loss made of: CE 0.47178372740745544, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.714698791503906 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.9853783369064333
Loss made of: CE 0.3547804355621338, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.316716194152832 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.192907285690308
Loss made of: CE 0.5562869906425476, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7235288619995117 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.900234803557396
Loss made of: CE 0.4454033076763153, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6148390769958496 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.112397330999374
Loss made of: CE 0.3800327777862549, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.579054832458496 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.218619674444199
Loss made of: CE 0.45735999941825867, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7129735946655273 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.229216080904007
Loss made of: CE 0.4776495695114136, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4573426246643066 EntMin 0.0
Epoch 6, Class Loss=0.3913928270339966, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.3913928270339966, Class Loss=0.3913928270339966, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4016226828098297, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.881969
Mean Acc: 0.626917
FreqW Acc: 0.796758
Mean IoU: 0.505355
Class IoU:
	class 0: 0.8883934
	class 1: 0.7309507
	class 2: 0.34662962
	class 3: 0.4881786
	class 4: 0.4704278
	class 5: 0.23894128
	class 6: 0.72285825
	class 7: 0.7695049
	class 8: 0.72721297
	class 9: 0.00058665074
	class 10: 0.5101683
	class 11: 0.30302936
	class 12: 0.5800941
	class 13: 0.40696788
	class 14: 0.70113134
	class 15: 0.7059586
	class 16: 0.0
Class Acc:
	class 0: 0.96661836
	class 1: 0.76972264
	class 2: 0.7514154
	class 3: 0.9131611
	class 4: 0.8774409
	class 5: 0.24253054
	class 6: 0.7352335
	class 7: 0.8312421
	class 8: 0.773922
	class 9: 0.00058665074
	class 10: 0.59638363
	class 11: 0.367982
	class 12: 0.7606423
	class 13: 0.4821649
	class 14: 0.7872333
	class 15: 0.8013077
	class 16: 0.0

federated global round: 18, step: 3
select part of clients to conduct local training
[9, 11, 13, 10]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.574845185875892
Loss made of: CE 0.36181125044822693, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3411717414855957 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.189894157648086
Loss made of: CE 0.3984419107437134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4304680824279785 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.343674293160438
Loss made of: CE 0.389666885137558, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.013177871704102 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.185875585675239
Loss made of: CE 0.3600681722164154, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4932751655578613 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.3437932908535
Loss made of: CE 0.44982680678367615, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8313920497894287 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.247492155432701
Loss made of: CE 0.4177553653717041, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.098204612731934 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.317148381471634
Loss made of: CE 0.35299667716026306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7449612617492676 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.40244924724102
Loss made of: CE 0.421586275100708, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8135008811950684 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.283053910732269
Loss made of: CE 0.32948189973831177, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5296058654785156 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.133323273062706
Loss made of: CE 0.3735155761241913, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.587751865386963 EntMin 0.0
Epoch 1, Class Loss=0.3946594297885895, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.3946594297885895, Class Loss=0.3946594297885895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/102, Loss=4.176346263289451
Loss made of: CE 0.35793933272361755, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4698963165283203 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.473277613520622
Loss made of: CE 0.3969384431838989, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9265098571777344 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.372542831301689
Loss made of: CE 0.49023813009262085, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.779618978500366 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.391045233607292
Loss made of: CE 0.40069979429244995, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6096725463867188 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.630090245604515
Loss made of: CE 0.3465085029602051, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.610808849334717 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.4225505828857425
Loss made of: CE 0.41647887229919434, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8735475540161133 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.317277836799621
Loss made of: CE 0.283904492855072, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8168087005615234 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.3203837931156155
Loss made of: CE 0.30683180689811707, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5649800300598145 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.303940844535828
Loss made of: CE 0.4165268540382385, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.809431791305542 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.075296676158905
Loss made of: CE 0.37153205275535583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6217784881591797 EntMin 0.0
Epoch 2, Class Loss=0.3893837034702301, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.3893837034702301, Class Loss=0.3893837034702301, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/102, Loss=4.382116270065308
Loss made of: CE 0.4746168255805969, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.451535701751709 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.5101375669240955
Loss made of: CE 0.39395254850387573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.240065097808838 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.243858799338341
Loss made of: CE 0.5809317231178284, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8555502891540527 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.395359486341476
Loss made of: CE 0.335909366607666, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6709840297698975 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.085063165426254
Loss made of: CE 0.38283252716064453, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7056641578674316 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.292046555876732
Loss made of: CE 0.44807490706443787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.911278486251831 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.602104622125625
Loss made of: CE 0.43184101581573486, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464239120483398 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.358822047710419
Loss made of: CE 0.3780639171600342, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4270830154418945 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.063205772638321
Loss made of: CE 0.31039148569107056, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.800250768661499 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.2360195994377134
Loss made of: CE 0.32871517539024353, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.726377010345459 EntMin 0.0
Epoch 3, Class Loss=0.38918256759643555, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.38918256759643555, Class Loss=0.38918256759643555, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/102, Loss=4.393019041419029
Loss made of: CE 0.3682224750518799, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.615145206451416 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.680767971277237
Loss made of: CE 0.39194807410240173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.192775726318359 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.200913155078888
Loss made of: CE 0.31658801436424255, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.813013792037964 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.497255757451057
Loss made of: CE 0.4240475296974182, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.052066326141357 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.280144557356834
Loss made of: CE 0.32531875371932983, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8902511596679688 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.370734238624573
Loss made of: CE 0.33160388469696045, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.342683792114258 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.459899419546128
Loss made of: CE 0.38861021399497986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.959059238433838 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.341022434830665
Loss made of: CE 0.43283742666244507, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.331892013549805 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.475148761272431
Loss made of: CE 0.31977781653404236, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7602498531341553 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.383993995189667
Loss made of: CE 0.35441726446151733, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.237139701843262 EntMin 0.0
Epoch 4, Class Loss=0.3944459855556488, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.3944459855556488, Class Loss=0.3944459855556488, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.213122960925102
Loss made of: CE 0.2978503406047821, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.230326175689697 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.097657504677772
Loss made of: CE 0.3551943004131317, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8048887252807617 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.123127835988998
Loss made of: CE 0.3119325041770935, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9701926708221436 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.457325568795204
Loss made of: CE 0.3342350125312805, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6981863975524902 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.114856916666031
Loss made of: CE 0.27006253600120544, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.513090133666992 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.265407198667527
Loss made of: CE 0.4125613570213318, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8546292781829834 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.325257739424705
Loss made of: CE 0.37868285179138184, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7986063957214355 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.278830718994141
Loss made of: CE 0.28287217020988464, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6917033195495605 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.230650368332863
Loss made of: CE 0.4085143804550171, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3007888793945312 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.254782861471176
Loss made of: CE 0.4289209544658661, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.368298292160034 EntMin 0.0
Epoch 5, Class Loss=0.3762804865837097, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.3762804865837097, Class Loss=0.3762804865837097, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/102, Loss=4.314998197555542
Loss made of: CE 0.4316045641899109, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.795624256134033 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.044942057132721
Loss made of: CE 0.39441239833831787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.69545841217041 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.119060805439949
Loss made of: CE 0.3774230480194092, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.405282735824585 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.205212426185608
Loss made of: CE 0.3282548487186432, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.328641414642334 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.165453234314919
Loss made of: CE 0.4729926586151123, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.767307996749878 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.264380976557732
Loss made of: CE 0.3803430497646332, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4011640548706055 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 70/102, Loss=4.347351118922234
Loss made of: CE 0.5627295970916748, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3527140617370605 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.1301725715398785
Loss made of: CE 0.3778265714645386, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7482447624206543 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.063855758309364
Loss made of: CE 0.33963966369628906, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.701066493988037 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.040004298090935
Loss made of: CE 0.3101404309272766, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3525807857513428 EntMin 0.0
Epoch 6, Class Loss=0.3775322735309601, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3775322735309601, Class Loss=0.3775322735309601, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000438
Epoch 1, Batch 10/102, Loss=4.164854699373246
Loss made of: CE 0.41624578833580017, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.461141109466553 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/102, Loss=3.9736219227313994
Loss made of: CE 0.3150347173213959, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.503358840942383 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.993190586566925
Loss made of: CE 0.34196388721466064, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.976409435272217 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.269632488489151
Loss made of: CE 0.34012335538864136, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7084951400756836 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.162926977872848
Loss made of: CE 0.3336781859397888, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.633647918701172 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.306688940525055
Loss made of: CE 0.3033122420310974, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3014612197875977 EntMin 0.0
Epoch 1, Batch 70/102, Loss=3.9457501649856566
Loss made of: CE 0.34642237424850464, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.695059299468994 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.090496164560318
Loss made of: CE 0.3927308917045593, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.822359561920166 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.8949684619903566
Loss made of: CE 0.42938798666000366, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2428154945373535 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.9075506776571274
Loss made of: CE 0.4230520725250244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.515613555908203 EntMin 0.0
Epoch 1, Class Loss=0.3868809640407562, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.3868809640407562, Class Loss=0.3868809640407562, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000405
Epoch 2, Batch 10/102, Loss=3.9434694826602934
Loss made of: CE 0.39121195673942566, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1746935844421387 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.86506764292717
Loss made of: CE 0.36590513586997986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.465843677520752 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.026902866363526
Loss made of: CE 0.4638513922691345, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4500389099121094 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.070354449748993
Loss made of: CE 0.4010046422481537, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.406182289123535 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.0467786461114885
Loss made of: CE 0.3553605079650879, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4994606971740723 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.1591465175151825
Loss made of: CE 0.5475364923477173, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.677290678024292 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.773389196395874
Loss made of: CE 0.3693785071372986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2210960388183594 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.091240864992142
Loss made of: CE 0.43261635303497314, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.821434497833252 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.0161672055721285
Loss made of: CE 0.5153787732124329, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400014400482178 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.088126987218857
Loss made of: CE 0.38407573103904724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7790420055389404 EntMin 0.0
Epoch 2, Class Loss=0.3889649510383606, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.3889649510383606, Class Loss=0.3889649510383606, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/102, Loss=3.997850704193115
Loss made of: CE 0.3896227777004242, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5577828884124756 EntMin 0.0
Epoch 3, Batch 20/102, Loss=3.94545618891716
Loss made of: CE 0.39997598528862, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2754950523376465 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.086223369836807
Loss made of: CE 0.4079671800136566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.468355178833008 EntMin 0.0
Epoch 3, Batch 40/102, Loss=3.9930188804864883
Loss made of: CE 0.33875763416290283, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.706092596054077 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.210907030105591
Loss made of: CE 0.41549962759017944, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3027830123901367 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.9958748549222944
Loss made of: CE 0.3596261739730835, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4295125007629395 EntMin 0.0
Epoch 3, Batch 70/102, Loss=3.959766873717308
Loss made of: CE 0.32284384965896606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.833676815032959 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.085763278603554
Loss made of: CE 0.3129868507385254, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8938233852386475 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.166119775176048
Loss made of: CE 0.3163626194000244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.293905735015869 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.974364402890205
Loss made of: CE 0.30218642950057983, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2251949310302734 EntMin 0.0
Epoch 3, Class Loss=0.3853156268596649, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.3853156268596649, Class Loss=0.3853156268596649, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=3.8984042525291445
Loss made of: CE 0.37001070380210876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.380457639694214 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.075905212759972
Loss made of: CE 0.31964820623397827, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.628660202026367 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.104610851407051
Loss made of: CE 0.43186235427856445, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.722360610961914 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.9644138276576997
Loss made of: CE 0.33352982997894287, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1062655448913574 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.090535032749176
Loss made of: CE 0.3996623456478119, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.408289909362793 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.25578233897686
Loss made of: CE 0.30820345878601074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.924109935760498 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.083347034454346
Loss made of: CE 0.502086341381073, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6548256874084473 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.030443367362023
Loss made of: CE 0.4281022250652313, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.359516620635986 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.091419813036919
Loss made of: CE 0.303081750869751, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4766645431518555 EntMin 0.0
Epoch 4, Batch 100/102, Loss=3.914581623673439
Loss made of: CE 0.39909762144088745, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084474086761475 EntMin 0.0
Epoch 4, Class Loss=0.3815443515777588, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.3815443515777588, Class Loss=0.3815443515777588, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/102, Loss=4.187050321698189
Loss made of: CE 0.4306187331676483, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.416572332382202 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.16054116487503
Loss made of: CE 0.43391865491867065, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.604618549346924 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.9917952090501787
Loss made of: CE 0.3144500255584717, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1514153480529785 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.8312914460897445
Loss made of: CE 0.36242032051086426, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.343204975128174 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.068116387724876
Loss made of: CE 0.41711580753326416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3640923500061035 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.7746663600206376
Loss made of: CE 0.43758413195610046, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5576865673065186 EntMin 0.0
Epoch 5, Batch 70/102, Loss=3.791445052623749
Loss made of: CE 0.41023945808410645, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2116200923919678 EntMin 0.0
Epoch 5, Batch 80/102, Loss=3.872647076845169
Loss made of: CE 0.25890037417411804, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7104668617248535 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.9524385571479796
Loss made of: CE 0.4197959899902344, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.097403049468994 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.953536206483841
Loss made of: CE 0.4004662036895752, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8301634788513184 EntMin 0.0
Epoch 5, Class Loss=0.3773469030857086, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.3773469030857086, Class Loss=0.3773469030857086, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000270
Epoch 6, Batch 10/102, Loss=4.11743845641613
Loss made of: CE 0.39082884788513184, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.29093599319458 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.810299924015999
Loss made of: CE 0.35606035590171814, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.427711009979248 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.757388335466385
Loss made of: CE 0.3816477656364441, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.789602041244507 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.137253832817078
Loss made of: CE 0.42859217524528503, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.830686330795288 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.83804467022419
Loss made of: CE 0.32237622141838074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.224644184112549 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.9770212441682817
Loss made of: CE 0.4838813543319702, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4700350761413574 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.749768874049187
Loss made of: CE 0.3998792767524719, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2670164108276367 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.8993747651576998
Loss made of: CE 0.3920994699001312, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3752989768981934 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.8849419087171553
Loss made of: CE 0.35709938406944275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2170228958129883 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 100/102, Loss=4.048299065232277
Loss made of: CE 0.43211621046066284, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.402526378631592 EntMin 0.0
Epoch 6, Class Loss=0.37591752409935, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.37591752409935, Class Loss=0.37591752409935, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=5.235017496347427
Loss made of: CE 0.6828012466430664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8568758964538574 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.009558847546577
Loss made of: CE 0.5397141575813293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.009219169616699 EntMin 0.0
Epoch 1, Class Loss=0.687104344367981, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.687104344367981, Class Loss=0.687104344367981, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/23, Loss=4.774592494964599
Loss made of: CE 0.5275206565856934, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.266792297363281 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.780753630399704
Loss made of: CE 0.5258448719978333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.096776008605957 EntMin 0.0
Epoch 2, Class Loss=0.5501750111579895, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.5501750111579895, Class Loss=0.5501750111579895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/23, Loss=4.463940989971161
Loss made of: CE 0.5655134916305542, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.493436813354492 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.511110073328018
Loss made of: CE 0.4599466323852539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.799622535705566 EntMin 0.0
Epoch 3, Class Loss=0.46387267112731934, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.46387267112731934, Class Loss=0.46387267112731934, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/23, Loss=4.2459679543972015
Loss made of: CE 0.4756842851638794, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.857635498046875 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.524610817432404
Loss made of: CE 0.344381183385849, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5573689937591553 EntMin 0.0
Epoch 4, Class Loss=0.4240705668926239, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.4240705668926239, Class Loss=0.4240705668926239, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/23, Loss=4.328846675157547
Loss made of: CE 0.37803763151168823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6935346126556396 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.224566248059273
Loss made of: CE 0.3458225131034851, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.465885877609253 EntMin 0.0
Epoch 5, Class Loss=0.409453809261322, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.409453809261322, Class Loss=0.409453809261322, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/23, Loss=4.220727476477623
Loss made of: CE 0.5108031034469604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.125544548034668 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.174738451838493
Loss made of: CE 0.3552016019821167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.027596473693848 EntMin 0.0
Epoch 6, Class Loss=0.3964150547981262, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.3964150547981262, Class Loss=0.3964150547981262, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/23, Loss=5.388864809274674
Loss made of: CE 0.7311620116233826, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1012043952941895 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.683651494979858
Loss made of: CE 0.7101127505302429, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.076427459716797 EntMin 0.0
Epoch 1, Class Loss=0.686047375202179, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.686047375202179, Class Loss=0.686047375202179, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/23, Loss=4.623098421096802
Loss made of: CE 0.697628378868103, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.387816905975342 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.21837964951992
Loss made of: CE 0.5418789982795715, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9620606899261475 EntMin 0.0
Epoch 2, Class Loss=0.5534993410110474, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.5534993410110474, Class Loss=0.5534993410110474, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/23, Loss=4.243848049640656
Loss made of: CE 0.45087695121765137, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.004520416259766 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.273456007242203
Loss made of: CE 0.3579217195510864, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.972761869430542 EntMin 0.0
Epoch 3, Class Loss=0.47927430272102356, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.47927430272102356, Class Loss=0.47927430272102356, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/23, Loss=4.059824940562248
Loss made of: CE 0.4146677851676941, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8541762828826904 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.1987267434597015
Loss made of: CE 0.501027524471283, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.633737325668335 EntMin 0.0
Epoch 4, Class Loss=0.43326154351234436, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.43326154351234436, Class Loss=0.43326154351234436, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/23, Loss=4.292921593785286
Loss made of: CE 0.3862036168575287, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.536224126815796 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.0612799495458605
Loss made of: CE 0.5551478266716003, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.085102081298828 EntMin 0.0
Epoch 5, Class Loss=0.42249923944473267, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.42249923944473267, Class Loss=0.42249923944473267, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/23, Loss=4.0724081158638
Loss made of: CE 0.35341477394104004, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7280850410461426 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.058970728516579
Loss made of: CE 0.39836376905441284, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7236061096191406 EntMin 0.0
Epoch 6, Class Loss=0.41633597016334534, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.41633597016334534, Class Loss=0.41633597016334534, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.394839882850647, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.881707
Mean Acc: 0.629606
FreqW Acc: 0.797951
Mean IoU: 0.503813
Class IoU:
	class 0: 0.8900485
	class 1: 0.7338288
	class 2: 0.34519583
	class 3: 0.45856205
	class 4: 0.4649008
	class 5: 0.279276
	class 6: 0.7244073
	class 7: 0.7608846
	class 8: 0.71101946
	class 9: 0.0006370667
	class 10: 0.4247592
	class 11: 0.32237437
	class 12: 0.5893891
	class 13: 0.44015563
	class 14: 0.69854814
	class 15: 0.7202309
	class 16: 0.0005967836
Class Acc:
	class 0: 0.9641493
	class 1: 0.77301264
	class 2: 0.6930424
	class 3: 0.92779005
	class 4: 0.88352066
	class 5: 0.28544968
	class 6: 0.7366786
	class 7: 0.80507517
	class 8: 0.74654216
	class 9: 0.00063711876
	class 10: 0.46023056
	class 11: 0.39500397
	class 12: 0.7778991
	class 13: 0.60913944
	class 14: 0.82676
	class 15: 0.8177759
	class 16: 0.0005968234

federated global round: 19, step: 3
select part of clients to conduct local training
[4, 7, 17, 15]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/105, Loss=4.597878786921501
Loss made of: CE 0.5643317699432373, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.899136066436768 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.341580507159233
Loss made of: CE 0.5105625987052917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8778302669525146 EntMin 0.0
Epoch 1, Batch 30/105, Loss=4.213661360740661
Loss made of: CE 0.5080868601799011, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303271293640137 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 40/105, Loss=4.108507618308067
Loss made of: CE 0.3995917737483978, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5099592208862305 EntMin 0.0
Epoch 1, Batch 50/105, Loss=3.959624782204628
Loss made of: CE 0.3339958190917969, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3213374614715576 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.422315686941147
Loss made of: CE 0.44051969051361084, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4289016723632812 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.336298355460167
Loss made of: CE 0.4590170383453369, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6455628871917725 EntMin 0.0
Epoch 1, Batch 80/105, Loss=4.190822982788086
Loss made of: CE 0.29901963472366333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5650060176849365 EntMin 0.0
Epoch 1, Batch 90/105, Loss=4.2327691406011585
Loss made of: CE 0.30943411588668823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4921770095825195 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.294067606329918
Loss made of: CE 0.3294541537761688, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.133062839508057 EntMin 0.0
Epoch 1, Class Loss=0.41299477219581604, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.41299477219581604, Class Loss=0.41299477219581604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/105, Loss=4.239481949806214
Loss made of: CE 0.37360823154449463, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4506053924560547 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.4510692954063416
Loss made of: CE 0.39612114429473877, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.017400741577148 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.40881969332695
Loss made of: CE 0.36845123767852783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.336552143096924 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.386934095621109
Loss made of: CE 0.5755681395530701, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8044204711914062 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.395595717430115
Loss made of: CE 0.39783960580825806, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8609471321105957 EntMin 0.0
Epoch 2, Batch 60/105, Loss=4.392988261580467
Loss made of: CE 0.35053715109825134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.997830867767334 EntMin 0.0
Epoch 2, Batch 70/105, Loss=4.292398396134376
Loss made of: CE 0.3394612669944763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0145416259765625 EntMin 0.0
Epoch 2, Batch 80/105, Loss=4.244167593121529
Loss made of: CE 0.3430872857570648, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6074066162109375 EntMin 0.0
Epoch 2, Batch 90/105, Loss=4.210719782114029
Loss made of: CE 0.3020838499069214, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.615957736968994 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.47375071644783
Loss made of: CE 0.39856159687042236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.047554016113281 EntMin 0.0
Epoch 2, Class Loss=0.4068249762058258, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.4068249762058258, Class Loss=0.4068249762058258, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/105, Loss=4.554697018861771
Loss made of: CE 0.4244531989097595, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6603026390075684 EntMin 0.0
Epoch 3, Batch 20/105, Loss=4.025648245215416
Loss made of: CE 0.3136621117591858, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5687193870544434 EntMin 0.0
Epoch 3, Batch 30/105, Loss=4.256934195756912
Loss made of: CE 0.3405625522136688, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6323487758636475 EntMin 0.0
Epoch 3, Batch 40/105, Loss=4.293227910995483
Loss made of: CE 0.44766196608543396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.068650245666504 EntMin 0.0
Epoch 3, Batch 50/105, Loss=4.12954158782959
Loss made of: CE 0.4496055543422699, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5991573333740234 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.366625636816025
Loss made of: CE 0.33420467376708984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.456178665161133 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.337602081894874
Loss made of: CE 0.440843790769577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.033094882965088 EntMin 0.0
Epoch 3, Batch 80/105, Loss=4.184102028608322
Loss made of: CE 0.2826398015022278, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1144652366638184 EntMin 0.0
Epoch 3, Batch 90/105, Loss=4.115669596195221
Loss made of: CE 0.3907015323638916, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7742533683776855 EntMin 0.0
Epoch 3, Batch 100/105, Loss=4.15879018008709
Loss made of: CE 0.3553328514099121, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.373253583908081 EntMin 0.0
Epoch 3, Class Loss=0.3939768373966217, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.3939768373966217, Class Loss=0.3939768373966217, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/105, Loss=4.203521552681923
Loss made of: CE 0.37733060121536255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2836151123046875 EntMin 0.0
Epoch 4, Batch 20/105, Loss=4.225674417614937
Loss made of: CE 0.42717278003692627, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6105265617370605 EntMin 0.0
Epoch 4, Batch 30/105, Loss=4.1982868820428845
Loss made of: CE 0.38627052307128906, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.769174098968506 EntMin 0.0
Epoch 4, Batch 40/105, Loss=4.100208860635758
Loss made of: CE 0.467077374458313, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.555095672607422 EntMin 0.0
Epoch 4, Batch 50/105, Loss=4.0847073823213575
Loss made of: CE 0.3696560859680176, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.426924228668213 EntMin 0.0
Epoch 4, Batch 60/105, Loss=4.0748925566673275
Loss made of: CE 0.4471147656440735, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9371094703674316 EntMin 0.0
Epoch 4, Batch 70/105, Loss=3.9633836567401888
Loss made of: CE 0.4402386546134949, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2483043670654297 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.267615529894829
Loss made of: CE 0.4244097173213959, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084056854248047 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.2755437850952145
Loss made of: CE 0.43413782119750977, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5257909297943115 EntMin 0.0
Epoch 4, Batch 100/105, Loss=3.9338733941316604
Loss made of: CE 0.330803781747818, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1841962337493896 EntMin 0.0
Epoch 4, Class Loss=0.38546743988990784, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.38546743988990784, Class Loss=0.38546743988990784, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/105, Loss=4.012622672319412
Loss made of: CE 0.3556193709373474, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5457522869110107 EntMin 0.0
Epoch 5, Batch 20/105, Loss=4.127340325713158
Loss made of: CE 0.5398672819137573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3529052734375 EntMin 0.0
Epoch 5, Batch 30/105, Loss=3.991411581635475
Loss made of: CE 0.4771844744682312, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.675048828125 EntMin 0.0
Epoch 5, Batch 40/105, Loss=4.174913436174393
Loss made of: CE 0.38164326548576355, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.492436170578003 EntMin 0.0
Epoch 5, Batch 50/105, Loss=3.8634139597415924
Loss made of: CE 0.3309253454208374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2346017360687256 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.3720189273357395
Loss made of: CE 0.4075784683227539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.28822135925293 EntMin 0.0
Epoch 5, Batch 70/105, Loss=4.048989102244377
Loss made of: CE 0.36965975165367126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.642987251281738 EntMin 0.0
Epoch 5, Batch 80/105, Loss=4.009936460852623
Loss made of: CE 0.3657500743865967, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6904590129852295 EntMin 0.0
Epoch 5, Batch 90/105, Loss=3.9971314549446104
Loss made of: CE 0.338082879781723, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1333770751953125 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.138740432262421
Loss made of: CE 0.4098796248435974, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5444536209106445 EntMin 0.0
Epoch 5, Class Loss=0.3840833604335785, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.3840833604335785, Class Loss=0.3840833604335785, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/105, Loss=4.280964267253876
Loss made of: CE 0.3320019543170929, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.346215009689331 EntMin 0.0
Epoch 6, Batch 20/105, Loss=3.918812835216522
Loss made of: CE 0.40847641229629517, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.363332748413086 EntMin 0.0
Epoch 6, Batch 30/105, Loss=3.8201078921556473
Loss made of: CE 0.3639358878135681, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.897881507873535 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.106986618041992
Loss made of: CE 0.3150201737880707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.020936012268066 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.163415879011154
Loss made of: CE 0.4314294755458832, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.157289028167725 EntMin 0.0
Epoch 6, Batch 60/105, Loss=3.935770758986473
Loss made of: CE 0.3570821285247803, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6794800758361816 EntMin 0.0
Epoch 6, Batch 70/105, Loss=3.853327524662018
Loss made of: CE 0.38231033086776733, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3170881271362305 EntMin 0.0
Epoch 6, Batch 80/105, Loss=4.040686783194542
Loss made of: CE 0.32307493686676025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4961342811584473 EntMin 0.0
Epoch 6, Batch 90/105, Loss=3.852928890287876
Loss made of: CE 0.3649327754974365, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5811920166015625 EntMin 0.0
Epoch 6, Batch 100/105, Loss=3.771416354179382
Loss made of: CE 0.3427524268627167, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2655370235443115 EntMin 0.0
Epoch 6, Class Loss=0.37633875012397766, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.37633875012397766, Class Loss=0.37633875012397766, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.120759871602059
Loss made of: CE 0.3793768286705017, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4077022075653076 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.193054154515266
Loss made of: CE 0.4661777913570404, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.03333044052124 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.18615668118
Loss made of: CE 0.34065675735473633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.076676845550537 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.14405822455883
Loss made of: CE 0.42792290449142456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.670726776123047 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.2303885370492935
Loss made of: CE 0.43109411001205444, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.11384916305542 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.194277757406235
Loss made of: CE 0.2822028398513794, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3971424102783203 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.212116953730583
Loss made of: CE 0.39950960874557495, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8767478466033936 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.191664010286331
Loss made of: CE 0.40499937534332275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.422205924987793 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/102, Loss=3.769223538041115
Loss made of: CE 0.3268835246562958, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0973424911499023 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.912114289402962
Loss made of: CE 0.44032829999923706, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.434420347213745 EntMin 0.0
Epoch 1, Class Loss=0.4015340507030487, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.4015340507030487, Class Loss=0.4015340507030487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/102, Loss=4.096410408616066
Loss made of: CE 0.33995211124420166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2720603942871094 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.948693773150444
Loss made of: CE 0.3874630331993103, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.873457431793213 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.199656441807747
Loss made of: CE 0.39913010597229004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.606379508972168 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.094896301627159
Loss made of: CE 0.3672066628932953, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128922462463379 EntMin 0.0
Epoch 2, Batch 50/102, Loss=3.869903290271759
Loss made of: CE 0.293144553899765, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.317065954208374 EntMin 0.0
Epoch 2, Batch 60/102, Loss=3.995822387933731
Loss made of: CE 0.32060176134109497, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6687850952148438 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.334061604738236
Loss made of: CE 0.44360995292663574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.906954526901245 EntMin 0.0
Epoch 2, Batch 80/102, Loss=3.946104234457016
Loss made of: CE 0.33123424649238586, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.232931613922119 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.9984390646219254
Loss made of: CE 0.35947853326797485, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.389080047607422 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.174915251135826
Loss made of: CE 0.39198458194732666, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.095298767089844 EntMin 0.0
Epoch 2, Class Loss=0.3900910019874573, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.3900910019874573, Class Loss=0.3900910019874573, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/102, Loss=4.364154264330864
Loss made of: CE 0.33872532844543457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3141095638275146 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.083938118815422
Loss made of: CE 0.42011308670043945, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.9386086463928223 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.9229559540748595
Loss made of: CE 0.3950746953487396, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.522676944732666 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.000202551484108
Loss made of: CE 0.34437131881713867, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.999373435974121 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.086370608210563
Loss made of: CE 0.45331090688705444, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.253998756408691 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.929961916804314
Loss made of: CE 0.36589235067367554, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.297775983810425 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.064986607432365
Loss made of: CE 0.45717495679855347, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5320520401000977 EntMin 0.0
Epoch 3, Batch 80/102, Loss=3.9127072364091875
Loss made of: CE 0.40833863615989685, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.466625213623047 EntMin 0.0
Epoch 3, Batch 90/102, Loss=3.973404476046562
Loss made of: CE 0.4154523015022278, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.246082305908203 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.8207385033369063
Loss made of: CE 0.3406217098236084, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3488576412200928 EntMin 0.0
Epoch 3, Class Loss=0.3859831392765045, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3859831392765045, Class Loss=0.3859831392765045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=4.0917203456163405
Loss made of: CE 0.358537882566452, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.60117244720459 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.8935434997081755
Loss made of: CE 0.4144849181175232, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.50201416015625 EntMin 0.0
Epoch 4, Batch 30/102, Loss=3.9638836592435838
Loss made of: CE 0.3189740777015686, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.108222007751465 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.9646244823932646
Loss made of: CE 0.3121921420097351, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4565703868865967 EntMin 0.0
Epoch 4, Batch 50/102, Loss=3.9174855202436447
Loss made of: CE 0.42912912368774414, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1290385723114014 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.063564500212669
Loss made of: CE 0.37008345127105713, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.512857437133789 EntMin 0.0
Epoch 4, Batch 70/102, Loss=3.957535555958748
Loss made of: CE 0.38578033447265625, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.483691930770874 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.03210881948471
Loss made of: CE 0.3783014714717865, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4844350814819336 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.032381772994995
Loss made of: CE 0.457149475812912, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.901012897491455 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.209408351778984
Loss made of: CE 0.3871323764324188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3196334838867188 EntMin 0.0
Epoch 4, Class Loss=0.3909974992275238, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.3909974992275238, Class Loss=0.3909974992275238, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/102, Loss=3.8008851230144503
Loss made of: CE 0.39988893270492554, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.535567045211792 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.041024363040924
Loss made of: CE 0.412167489528656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8013134002685547 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.120068171620369
Loss made of: CE 0.352486789226532, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6438348293304443 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.994670894742012
Loss made of: CE 0.3791181743144989, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.167450428009033 EntMin 0.0
Epoch 5, Batch 50/102, Loss=3.855749878287315
Loss made of: CE 0.42301997542381287, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.671238660812378 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.899756208062172
Loss made of: CE 0.32167768478393555, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0364623069763184 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.070323798060417
Loss made of: CE 0.37586307525634766, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.706871747970581 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.095378780364991
Loss made of: CE 0.33108219504356384, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4496030807495117 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.823192662000656
Loss made of: CE 0.4507499635219574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4669957160949707 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.897333499789238
Loss made of: CE 0.3578488826751709, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.252196311950684 EntMin 0.0
Epoch 5, Class Loss=0.3785746097564697, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.3785746097564697, Class Loss=0.3785746097564697, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/102, Loss=3.9120198279619216
Loss made of: CE 0.4047798812389374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1339797973632812 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.12789652645588
Loss made of: CE 0.39307713508605957, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.8567914962768555 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.660163000226021
Loss made of: CE 0.3594951033592224, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1815409660339355 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.8572046369314195
Loss made of: CE 0.35275155305862427, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.155632972717285 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.941559594869614
Loss made of: CE 0.42146238684654236, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5482797622680664 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.8540440082550047
Loss made of: CE 0.3658673167228699, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3759865760803223 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.929136189818382
Loss made of: CE 0.37863707542419434, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.617816686630249 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.7818486124277113
Loss made of: CE 0.38334986567497253, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.53391695022583 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.910713851451874
Loss made of: CE 0.4071378707885742, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2659518718719482 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.845990759134293
Loss made of: CE 0.3842479884624481, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.9317712783813477 EntMin 0.0
Epoch 6, Class Loss=0.37647372484207153, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.37647372484207153, Class Loss=0.37647372484207153, Reg Loss=0.0
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/23, Loss=5.112268587946891
Loss made of: CE 0.5745419859886169, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.287261009216309 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.6944553554058075
Loss made of: CE 0.5247063636779785, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.072499752044678 EntMin 0.0
Epoch 1, Class Loss=0.5637462735176086, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.5637462735176086, Class Loss=0.5637462735176086, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/23, Loss=4.611584711074829
Loss made of: CE 0.4980958104133606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.947399854660034 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.435624268651009
Loss made of: CE 0.3986760079860687, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.909200668334961 EntMin 0.0
Epoch 2, Class Loss=0.4831783175468445, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.4831783175468445, Class Loss=0.4831783175468445, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/23, Loss=4.375212597846985
Loss made of: CE 0.32832127809524536, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6540403366088867 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.444849118590355
Loss made of: CE 0.4903457760810852, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.432713985443115 EntMin 0.0
Epoch 3, Class Loss=0.43871888518333435, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.43871888518333435, Class Loss=0.43871888518333435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/23, Loss=4.490815952420235
Loss made of: CE 0.5754384994506836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.67324161529541 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.274467167258263
Loss made of: CE 0.3491429090499878, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8596229553222656 EntMin 0.0
Epoch 4, Class Loss=0.42356669902801514, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.42356669902801514, Class Loss=0.42356669902801514, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/23, Loss=4.385254213213921
Loss made of: CE 0.4278723895549774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.109061241149902 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.159468492865562
Loss made of: CE 0.32941657304763794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.034000396728516 EntMin 0.0
Epoch 5, Class Loss=0.41582435369491577, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.41582435369491577, Class Loss=0.41582435369491577, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/23, Loss=4.318556103110313
Loss made of: CE 0.39779770374298096, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8333792686462402 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.165049549937248
Loss made of: CE 0.40447765588760376, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.447960376739502 EntMin 0.0
Epoch 6, Class Loss=0.4050512909889221, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.4050512909889221, Class Loss=0.4050512909889221, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/103, Loss=3.8909820556640624
Loss made of: CE 0.3870158791542053, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.559452772140503 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/103, Loss=3.9740018516778948
Loss made of: CE 0.37464743852615356, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.485495090484619 EntMin 0.0
Epoch 1, Batch 30/103, Loss=4.081965419650078
Loss made of: CE 0.3063619136810303, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6530871391296387 EntMin 0.0
Epoch 1, Batch 40/103, Loss=4.3511961698532104
Loss made of: CE 0.34578531980514526, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.322007656097412 EntMin 0.0
Epoch 1, Batch 50/103, Loss=4.456920078396797
Loss made of: CE 0.5109013319015503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.757181167602539 EntMin 0.0
Epoch 1, Batch 60/103, Loss=4.18521457016468
Loss made of: CE 0.2991942763328552, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.892747640609741 EntMin 0.0
Epoch 1, Batch 70/103, Loss=4.211474698781967
Loss made of: CE 0.3159670829772949, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7684006690979004 EntMin 0.0
Epoch 1, Batch 80/103, Loss=4.4778032273054125
Loss made of: CE 0.3744144141674042, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.455588340759277 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/103, Loss=4.106157338619232
Loss made of: CE 0.31971704959869385, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.215874671936035 EntMin 0.0
Epoch 1, Batch 100/103, Loss=4.3207029581069945
Loss made of: CE 0.38185304403305054, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7453219890594482 EntMin 0.0
Epoch 1, Class Loss=0.3933567702770233, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.3933567702770233, Class Loss=0.3933567702770233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/103, Loss=4.169832530617714
Loss made of: CE 0.36412209272384644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5967888832092285 EntMin 0.0
Epoch 2, Batch 20/103, Loss=4.398501378297806
Loss made of: CE 0.35331499576568604, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6335649490356445 EntMin 0.0
Epoch 2, Batch 30/103, Loss=4.198127615451813
Loss made of: CE 0.43964096903800964, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.175537109375 EntMin 0.0
Epoch 2, Batch 40/103, Loss=4.351905944943428
Loss made of: CE 0.42640674114227295, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128916263580322 EntMin 0.0
Epoch 2, Batch 50/103, Loss=4.09947566986084
Loss made of: CE 0.36896616220474243, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.348482608795166 EntMin 0.0
Epoch 2, Batch 60/103, Loss=4.451256215572357
Loss made of: CE 0.30421239137649536, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195316314697266 EntMin 0.0
Epoch 2, Batch 70/103, Loss=4.229072737693786
Loss made of: CE 0.31813478469848633, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8393826484680176 EntMin 0.0
Epoch 2, Batch 80/103, Loss=4.156161588430405
Loss made of: CE 0.3577990233898163, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.292367935180664 EntMin 0.0
Epoch 2, Batch 90/103, Loss=4.374685379862785
Loss made of: CE 0.3058930039405823, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.184543132781982 EntMin 0.0
Epoch 2, Batch 100/103, Loss=4.637871405482292
Loss made of: CE 0.3664035201072693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.211711883544922 EntMin 0.0
Epoch 2, Class Loss=0.3872830271720886, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.3872830271720886, Class Loss=0.3872830271720886, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/103, Loss=4.3562961399555205
Loss made of: CE 0.3081730008125305, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6332054138183594 EntMin 0.0
Epoch 3, Batch 20/103, Loss=4.220881122350693
Loss made of: CE 0.43660029768943787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.815061569213867 EntMin 0.0
Epoch 3, Batch 30/103, Loss=4.219296219944954
Loss made of: CE 0.40010911226272583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4055304527282715 EntMin 0.0
Epoch 3, Batch 40/103, Loss=4.328454798460006
Loss made of: CE 0.39035582542419434, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6665351390838623 EntMin 0.0
Epoch 3, Batch 50/103, Loss=4.436309197545052
Loss made of: CE 0.3532986044883728, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.506455898284912 EntMin 0.0
Epoch 3, Batch 60/103, Loss=4.220736253261566
Loss made of: CE 0.4495421350002289, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6183879375457764 EntMin 0.0
Epoch 3, Batch 70/103, Loss=4.304856023192405
Loss made of: CE 0.38513392210006714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.542559623718262 EntMin 0.0
Epoch 3, Batch 80/103, Loss=4.227915716171265
Loss made of: CE 0.3259737193584442, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7383248805999756 EntMin 0.0
Epoch 3, Batch 90/103, Loss=4.214422476291657
Loss made of: CE 0.36339840292930603, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5103840827941895 EntMin 0.0
Epoch 3, Batch 100/103, Loss=4.202372291684151
Loss made of: CE 0.47711822390556335, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.246187686920166 EntMin 0.0
Epoch 3, Class Loss=0.3826572000980377, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.3826572000980377, Class Loss=0.3826572000980377, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/103, Loss=4.146965596079826
Loss made of: CE 0.406296968460083, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4003350734710693 EntMin 0.0
Epoch 4, Batch 20/103, Loss=4.235929402709007
Loss made of: CE 0.4807179868221283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.527693271636963 EntMin 0.0
Epoch 4, Batch 30/103, Loss=4.132167735695839
Loss made of: CE 0.46375036239624023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.255115509033203 EntMin 0.0
Epoch 4, Batch 40/103, Loss=3.90354029238224
Loss made of: CE 0.3730804920196533, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4688425064086914 EntMin 0.0
Epoch 4, Batch 50/103, Loss=4.130762270092964
Loss made of: CE 0.37330901622772217, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9841156005859375 EntMin 0.0
Epoch 4, Batch 60/103, Loss=4.127740177512169
Loss made of: CE 0.3668186366558075, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.684770345687866 EntMin 0.0
Epoch 4, Batch 70/103, Loss=4.028247934579849
Loss made of: CE 0.36493515968322754, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.812802791595459 EntMin 0.0
Epoch 4, Batch 80/103, Loss=4.155476135015488
Loss made of: CE 0.31420284509658813, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.329890727996826 EntMin 0.0
Epoch 4, Batch 90/103, Loss=3.9325274556875227
Loss made of: CE 0.35708221793174744, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.575718402862549 EntMin 0.0
Epoch 4, Batch 100/103, Loss=4.245312613248825
Loss made of: CE 0.3954177796840668, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.463486433029175 EntMin 0.0
Epoch 4, Class Loss=0.3795979619026184, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.3795979619026184, Class Loss=0.3795979619026184, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/103, Loss=4.130666574835777
Loss made of: CE 0.33128079771995544, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7125630378723145 EntMin 0.0
Epoch 5, Batch 20/103, Loss=4.283765757083893
Loss made of: CE 0.4144456088542938, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.907954216003418 EntMin 0.0
Epoch 5, Batch 30/103, Loss=3.966341894865036
Loss made of: CE 0.3949335515499115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.220682621002197 EntMin 0.0
Epoch 5, Batch 40/103, Loss=4.092528370022774
Loss made of: CE 0.3043372631072998, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.108524322509766 EntMin 0.0
Epoch 5, Batch 50/103, Loss=4.102471831440925
Loss made of: CE 0.30554714798927307, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3406052589416504 EntMin 0.0
Epoch 5, Batch 60/103, Loss=3.9420100152492523
Loss made of: CE 0.4315316081047058, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.517251491546631 EntMin 0.0
Epoch 5, Batch 70/103, Loss=3.881167230010033
Loss made of: CE 0.3854374289512634, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.533628225326538 EntMin 0.0
Epoch 5, Batch 80/103, Loss=4.2227074086666105
Loss made of: CE 0.3170098066329956, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.38761305809021 EntMin 0.0
Epoch 5, Batch 90/103, Loss=4.154529705643654
Loss made of: CE 0.4738592505455017, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.57138729095459 EntMin 0.0
Epoch 5, Batch 100/103, Loss=4.091038930416107
Loss made of: CE 0.3345416486263275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.571404218673706 EntMin 0.0
Epoch 5, Class Loss=0.37731337547302246, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.37731337547302246, Class Loss=0.37731337547302246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/103, Loss=3.9100747644901275
Loss made of: CE 0.3581765592098236, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.287567138671875 EntMin 0.0
Epoch 6, Batch 20/103, Loss=3.9646550178527833
Loss made of: CE 0.322760671377182, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0457141399383545 EntMin 0.0
Epoch 6, Batch 30/103, Loss=3.9048653930425643
Loss made of: CE 0.35836195945739746, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4918508529663086 EntMin 0.0
Epoch 6, Batch 40/103, Loss=3.8086292296648026
Loss made of: CE 0.3635803163051605, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.14639949798584 EntMin 0.0
Epoch 6, Batch 50/103, Loss=3.937315371632576
Loss made of: CE 0.29708951711654663, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.529784679412842 EntMin 0.0
Epoch 6, Batch 60/103, Loss=4.025324875116349
Loss made of: CE 0.31593355536460876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.524723052978516 EntMin 0.0
Epoch 6, Batch 70/103, Loss=4.2856784671545025
Loss made of: CE 0.3518122434616089, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7218844890594482 EntMin 0.0
Epoch 6, Batch 80/103, Loss=4.13440850675106
Loss made of: CE 0.40875157713890076, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5046653747558594 EntMin 0.0
Epoch 6, Batch 90/103, Loss=4.211642774939537
Loss made of: CE 0.38836783170700073, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.546414852142334 EntMin 0.0
Epoch 6, Batch 100/103, Loss=4.097985017299652
Loss made of: CE 0.35965245962142944, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7110202312469482 EntMin 0.0
Epoch 6, Class Loss=0.3728145956993103, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.3728145956993103, Class Loss=0.3728145956993103, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3920843005180359, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.880749
Mean Acc: 0.626941
FreqW Acc: 0.797057
Mean IoU: 0.498315
Class IoU:
	class 0: 0.8906586
	class 1: 0.7356743
	class 2: 0.33386824
	class 3: 0.47129285
	class 4: 0.46874702
	class 5: 0.26271644
	class 6: 0.71097493
	class 7: 0.76835096
	class 8: 0.72183496
	class 9: 0.0002738775
	class 10: 0.31541714
	class 11: 0.30913928
	class 12: 0.59809285
	class 13: 0.43029875
	class 14: 0.7093534
	class 15: 0.72135663
	class 16: 0.023310242
Class Acc:
	class 0: 0.9636055
	class 1: 0.77270037
	class 2: 0.69355845
	class 3: 0.9407104
	class 4: 0.88980114
	class 5: 0.26724246
	class 6: 0.7229911
	class 7: 0.822348
	class 8: 0.7604083
	class 9: 0.0002738775
	class 10: 0.330753
	class 11: 0.3886926
	class 12: 0.77137345
	class 13: 0.66990584
	class 14: 0.8163215
	class 15: 0.8237217
	class 16: 0.023594514

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[19, 23, 1, 8]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=9.76331785917282
Loss made of: CE 1.6021451950073242, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.617110252380371 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=8.403312468528748
Loss made of: CE 1.30271577835083, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.170355796813965 EntMin 0.0
Epoch 1, Class Loss=1.5055984258651733, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=1.5055984258651733, Class Loss=1.5055984258651733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=7.919954252243042
Loss made of: CE 0.9773415327072144, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.183884620666504 EntMin 0.0
Epoch 2, Batch 20/24, Loss=7.32180517911911
Loss made of: CE 0.9606376886367798, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.266112804412842 EntMin 0.0
Epoch 2, Class Loss=1.03304123878479, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=1.03304123878479, Class Loss=1.03304123878479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=6.7071366786956785
Loss made of: CE 0.8750038146972656, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.674587249755859 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.488909155130386
Loss made of: CE 0.8419272303581238, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.092741012573242 EntMin 0.0
Epoch 3, Class Loss=0.9050535559654236, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.9050535559654236, Class Loss=0.9050535559654236, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=6.1318907797336575
Loss made of: CE 0.777493953704834, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.176449775695801 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.4975470960140225
Loss made of: CE 0.7201751470565796, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.391284465789795 EntMin 0.0
Epoch 4, Class Loss=0.7759303450584412, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.7759303450584412, Class Loss=0.7759303450584412, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=5.739095878601074
Loss made of: CE 0.7493149638175964, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6857452392578125 EntMin 0.0
Epoch 5, Batch 20/24, Loss=6.222393208742142
Loss made of: CE 0.6467907428741455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.761930465698242 EntMin 0.0
Epoch 5, Class Loss=0.7044484615325928, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.7044484615325928, Class Loss=0.7044484615325928, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 10/24, Loss=6.044851559400558
Loss made of: CE 0.5966242551803589, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.137543201446533 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.597287517786026
Loss made of: CE 0.6387737989425659, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.641073226928711 EntMin 0.0
Epoch 6, Class Loss=0.6360496282577515, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.6360496282577515, Class Loss=0.6360496282577515, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=9.234544551372528
Loss made of: CE 1.4973738193511963, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.782168388366699 EntMin 0.0
Epoch 1, Batch 20/21, Loss=7.884141683578491
Loss made of: CE 1.1347771883010864, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.912432670593262 EntMin 0.0
Epoch 1, Class Loss=1.4631506204605103, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=1.4631506204605103, Class Loss=1.4631506204605103, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/21, Loss=6.970522624254227
Loss made of: CE 0.8999883532524109, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.513774394989014 EntMin 0.0
Epoch 2, Batch 20/21, Loss=6.526525175571441
Loss made of: CE 0.8434149622917175, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.011897087097168 EntMin 0.0
Epoch 2, Class Loss=0.9751546382904053, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.9751546382904053, Class Loss=0.9751546382904053, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/21, Loss=6.6950690150260925
Loss made of: CE 0.8500015735626221, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.073828220367432 EntMin 0.0
Epoch 3, Batch 20/21, Loss=6.3166581928730015
Loss made of: CE 0.9192380309104919, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.852559566497803 EntMin 0.0
Epoch 3, Class Loss=0.8794397115707397, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.8794397115707397, Class Loss=0.8794397115707397, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/21, Loss=6.351996642351151
Loss made of: CE 0.8334080576896667, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.713685035705566 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.597852277755737
Loss made of: CE 0.6210805773735046, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464308738708496 EntMin 0.0
Epoch 4, Class Loss=0.7857170104980469, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.7857170104980469, Class Loss=0.7857170104980469, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/21, Loss=5.908202689886093
Loss made of: CE 0.7340390086174011, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.184700012207031 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.839223504066467
Loss made of: CE 0.7807102203369141, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.981400489807129 EntMin 0.0
Epoch 5, Class Loss=0.7182408571243286, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.7182408571243286, Class Loss=0.7182408571243286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/21, Loss=5.735331273078918
Loss made of: CE 0.6423587799072266, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.965230464935303 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.711243373155594
Loss made of: CE 0.6963035464286804, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.433215141296387 EntMin 0.0
Epoch 6, Class Loss=0.6934115290641785, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.6934115290641785, Class Loss=0.6934115290641785, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=11.045065009593964
Loss made of: CE 1.6529979705810547, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.371240615844727 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.670555591583252, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=1.670555591583252, Class Loss=1.670555591583252, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=8.301536560058594
Loss made of: CE 0.9352132081985474, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.144523620605469 EntMin 0.0
Epoch 2, Class Loss=1.0637439489364624, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=1.0637439489364624, Class Loss=1.0637439489364624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=7.525963938236236
Loss made of: CE 0.9387871623039246, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.627727031707764 EntMin 0.0
Epoch 3, Class Loss=0.863835871219635, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.863835871219635, Class Loss=0.863835871219635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=6.87672905921936
Loss made of: CE 0.7197662591934204, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.508808135986328 EntMin 0.0
Epoch 4, Class Loss=0.7224406003952026, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.7224406003952026, Class Loss=0.7224406003952026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=6.58011149764061
Loss made of: CE 0.5973291397094727, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.156686782836914 EntMin 0.0
Epoch 5, Class Loss=0.6153546571731567, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.6153546571731567, Class Loss=0.6153546571731567, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=6.152490413188934
Loss made of: CE 0.5213861465454102, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.257784366607666 EntMin 0.0
Epoch 6, Class Loss=0.5357417464256287, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.5357417464256287, Class Loss=0.5357417464256287, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=9.960257971286774
Loss made of: CE 1.3211678266525269, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.048320770263672 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=8.207056558132171
Loss made of: CE 1.0666168928146362, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.406896114349365 EntMin 0.0
Epoch 1, Class Loss=1.3381508588790894, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.3381508588790894, Class Loss=1.3381508588790894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=7.5132724165916445
Loss made of: CE 0.9342667460441589, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.220977783203125 EntMin 0.0
Epoch 2, Batch 20/24, Loss=7.195340216159821
Loss made of: CE 0.8598765134811401, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.793473243713379 EntMin 0.0
Epoch 2, Class Loss=0.983873724937439, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.983873724937439, Class Loss=0.983873724937439, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=7.132236278057098
Loss made of: CE 0.9799765348434448, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.808880805969238 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.748494017124176
Loss made of: CE 0.9220865964889526, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.198729515075684 EntMin 0.0
Epoch 3, Class Loss=0.8993910551071167, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.8993910551071167, Class Loss=0.8993910551071167, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=6.378880435228348
Loss made of: CE 0.8774851560592651, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8438920974731445 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.516912752389908
Loss made of: CE 0.7934543490409851, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.308285713195801 EntMin 0.0
Epoch 4, Class Loss=0.8308815956115723, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.8308815956115723, Class Loss=0.8308815956115723, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=6.515879219770431
Loss made of: CE 0.9447106719017029, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.69356632232666 EntMin 0.0
Epoch 5, Batch 20/24, Loss=6.0249797880649565
Loss made of: CE 0.7706540822982788, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.01815128326416 EntMin 0.0
Epoch 5, Class Loss=0.7722604274749756, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.7722604274749756, Class Loss=0.7722604274749756, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=6.146554487943649
Loss made of: CE 0.787420928478241, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8917765617370605 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.981140851974487
Loss made of: CE 0.6775815486907959, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.405575752258301 EntMin 0.0
Epoch 6, Class Loss=0.7405058145523071, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.7405058145523071, Class Loss=0.7405058145523071, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6162708401679993, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.822366
Mean Acc: 0.408948
FreqW Acc: 0.691763
Mean IoU: 0.322970
Class IoU:
	class 0: 0.8230768
	class 1: 0.5139336
	class 2: 0.27794603
	class 3: 0.50950474
	class 4: 0.36026675
	class 5: 0.056942426
	class 6: 0.2780853
	class 7: 0.33936432
	class 8: 0.72289985
	class 9: 0.0003069871
	class 10: 0.28896585
	class 11: 0.295526
	class 12: 0.5963861
	class 13: 0.40927726
	class 14: 0.5902247
	class 15: 0.71843606
	class 16: 0.0012317055
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0
Class Acc:
	class 0: 0.9821318
	class 1: 0.52304095
	class 2: 0.44382775
	class 3: 0.86268616
	class 4: 0.8501499
	class 5: 0.05696324
	class 6: 0.2810475
	class 7: 0.34113732
	class 8: 0.7955947
	class 9: 0.0003069871
	class 10: 0.34037653
	class 11: 0.3325658
	class 12: 0.74253005
	class 13: 0.6294349
	class 14: 0.6386351
	class 15: 0.7662561
	class 16: 0.001231742
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[23, 14, 4, 11]
Current Client Index:  23
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=6.449595195055008
Loss made of: CE 0.8645462393760681, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.690747261047363 EntMin 0.0
Epoch 1, Batch 20/21, Loss=6.156205314397812
Loss made of: CE 0.8897713422775269, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.342652797698975 EntMin 0.0
Epoch 1, Class Loss=0.949634850025177, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.949634850025177, Class Loss=0.949634850025177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/21, Loss=5.823914784193039
Loss made of: CE 0.8130433559417725, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.511609077453613 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.630553466081619
Loss made of: CE 0.732176661491394, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6582746505737305 EntMin 0.0
Epoch 2, Class Loss=0.8138179779052734, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.8138179779052734, Class Loss=0.8138179779052734, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/21, Loss=6.103602916002274
Loss made of: CE 0.6854130625724792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.870811462402344 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.725696837902069
Loss made of: CE 0.7855541110038757, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.301513671875 EntMin 0.0
Epoch 3, Class Loss=0.7447474598884583, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.7447474598884583, Class Loss=0.7447474598884583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/21, Loss=5.724129903316498
Loss made of: CE 0.6966748237609863, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.016523361206055 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.130043286085129
Loss made of: CE 0.6019777059555054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.196129322052002 EntMin 0.0
Epoch 4, Class Loss=0.677615225315094, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.677615225315094, Class Loss=0.677615225315094, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=5.455524736642838
Loss made of: CE 0.5858689546585083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.808647632598877 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.458445134758949
Loss made of: CE 0.6858968138694763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.635205268859863 EntMin 0.0
Epoch 5, Class Loss=0.6324774622917175, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.6324774622917175, Class Loss=0.6324774622917175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/21, Loss=5.469247484207154
Loss made of: CE 0.5510419607162476, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.325247764587402 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.238196104764938
Loss made of: CE 0.5898422002792358, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.237501621246338 EntMin 0.0
Epoch 6, Class Loss=0.6078851222991943, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.6078851222991943, Class Loss=0.6078851222991943, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=7.4287093937397
Loss made of: CE 0.9048269987106323, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.602151870727539 EntMin 0.0
Epoch 1, Batch 20/26, Loss=6.662603789567948
Loss made of: CE 0.7683271169662476, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.824995994567871 EntMin 0.0
Epoch 1, Class Loss=0.9567056894302368, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.9567056894302368, Class Loss=0.9567056894302368, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=6.58387701511383
Loss made of: CE 0.8497838377952576, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.119555950164795 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/26, Loss=6.3376608967781065
Loss made of: CE 0.754985511302948, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.967015266418457 EntMin 0.0
Epoch 2, Class Loss=0.7750868201255798, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.7750868201255798, Class Loss=0.7750868201255798, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=5.980566155910492
Loss made of: CE 0.8711797595024109, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.322236061096191 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.9881107091903685
Loss made of: CE 0.7308847308158875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.764914512634277 EntMin 0.0
Epoch 3, Class Loss=0.6850417256355286, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.6850417256355286, Class Loss=0.6850417256355286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.579915773868561
Loss made of: CE 0.6835224032402039, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.10362434387207 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.598618239164352
Loss made of: CE 0.6977899074554443, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726997375488281 EntMin 0.0
Epoch 4, Class Loss=0.6304724812507629, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.6304724812507629, Class Loss=0.6304724812507629, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.37897167801857
Loss made of: CE 0.7851712107658386, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.000493049621582 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.49548564851284
Loss made of: CE 0.6018790602684021, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.942996025085449 EntMin 0.0
Epoch 5, Class Loss=0.5866956114768982, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5866956114768982, Class Loss=0.5866956114768982, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.3865030169487
Loss made of: CE 0.562313437461853, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.185598373413086 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.408554548025132
Loss made of: CE 0.4839456081390381, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.053820610046387 EntMin 0.0
Epoch 6, Class Loss=0.5767674446105957, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5767674446105957, Class Loss=0.5767674446105957, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=7.101808512210846
Loss made of: CE 1.1084684133529663, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.075684547424316 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/26, Loss=6.80124841928482
Loss made of: CE 0.8518010377883911, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.779109954833984 EntMin 0.0
Epoch 1, Class Loss=0.9629965424537659, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.9629965424537659, Class Loss=0.9629965424537659, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=6.151259648799896
Loss made of: CE 1.01088285446167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7955708503723145 EntMin 0.0
Epoch 2, Batch 20/26, Loss=6.319419652223587
Loss made of: CE 0.736284613609314, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.652525901794434 EntMin 0.0
Epoch 2, Class Loss=0.7736167907714844, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.7736167907714844, Class Loss=0.7736167907714844, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=6.134740215539932
Loss made of: CE 0.6588632464408875, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.049645900726318 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.748103314638138
Loss made of: CE 0.6920585036277771, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.041629791259766 EntMin 0.0
Epoch 3, Class Loss=0.6739786863327026, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.6739786863327026, Class Loss=0.6739786863327026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.619365364313126
Loss made of: CE 0.6201426982879639, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.672281265258789 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.4744785070419315
Loss made of: CE 0.5652918815612793, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6110053062438965 EntMin 0.0
Epoch 4, Class Loss=0.6198875904083252, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.6198875904083252, Class Loss=0.6198875904083252, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.6150465369224545
Loss made of: CE 0.511124849319458, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3409013748168945 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.625910604000092
Loss made of: CE 0.5929857492446899, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.657893657684326 EntMin 0.0
Epoch 5, Class Loss=0.5966979265213013, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5966979265213013, Class Loss=0.5966979265213013, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.751493000984192
Loss made of: CE 0.558459460735321, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611093997955322 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.201999697089195
Loss made of: CE 0.5812868475914001, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.270928859710693 EntMin 0.0
Epoch 6, Class Loss=0.565346896648407, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.565346896648407, Class Loss=0.565346896648407, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=7.023942911624909
Loss made of: CE 1.051121711730957, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.378357887268066 EntMin 0.0
Epoch 1, Batch 20/24, Loss=6.805402076244354
Loss made of: CE 1.08125638961792, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.812808036804199 EntMin 0.0
Epoch 1, Class Loss=0.9662670493125916, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.9662670493125916, Class Loss=0.9662670493125916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/24, Loss=6.477121716737747
Loss made of: CE 1.0586713552474976, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.373602867126465 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.448906254768372
Loss made of: CE 0.8948335647583008, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.985507011413574 EntMin 0.0
Epoch 2, Class Loss=0.8346136808395386, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.8346136808395386, Class Loss=0.8346136808395386, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/24, Loss=6.180243492126465
Loss made of: CE 0.8163886070251465, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.945839881896973 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 20/24, Loss=6.305202609300613
Loss made of: CE 0.7097966074943542, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.768314361572266 EntMin 0.0
Epoch 3, Class Loss=0.7503098249435425, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.7503098249435425, Class Loss=0.7503098249435425, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/24, Loss=5.665097403526306
Loss made of: CE 0.6636362075805664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.836004734039307 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.372419983148575
Loss made of: CE 0.6870282888412476, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.257051467895508 EntMin 0.0
Epoch 4, Class Loss=0.6970078349113464, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.6970078349113464, Class Loss=0.6970078349113464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/24, Loss=6.049353331327438
Loss made of: CE 0.6401702761650085, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.03549861907959 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.968302857875824
Loss made of: CE 0.5895801782608032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.33310604095459 EntMin 0.0
Epoch 5, Class Loss=0.6593654751777649, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.6593654751777649, Class Loss=0.6593654751777649, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/24, Loss=5.67314504981041
Loss made of: CE 0.7515963912010193, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.224953651428223 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.646225607395172
Loss made of: CE 0.6975721120834351, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.739768981933594 EntMin 0.0
Epoch 6, Class Loss=0.6391818523406982, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.6391818523406982, Class Loss=0.6391818523406982, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5592780709266663, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.833338
Mean Acc: 0.447295
FreqW Acc: 0.710844
Mean IoU: 0.358036
Class IoU:
	class 0: 0.83537906
	class 1: 0.5481259
	class 2: 0.283465
	class 3: 0.50411135
	class 4: 0.36491027
	class 5: 0.08647056
	class 6: 0.3762834
	class 7: 0.4267783
	class 8: 0.7360441
	class 9: 2.4108935e-05
	class 10: 0.36959404
	class 11: 0.29174316
	class 12: 0.6076932
	class 13: 0.43781435
	class 14: 0.5740858
	class 15: 0.7248619
	class 16: 0.00028378086
	class 17: 0.0
	class 18: 0.0
	class 19: 0.108692765
	class 20: 0.24239156
Class Acc:
	class 0: 0.9807883
	class 1: 0.5567332
	class 2: 0.4780743
	class 3: 0.8376508
	class 4: 0.8543717
	class 5: 0.086502776
	class 6: 0.38343292
	class 7: 0.43255207
	class 8: 0.8135146
	class 9: 2.4108935e-05
	class 10: 0.478281
	class 11: 0.3536935
	class 12: 0.7837971
	class 13: 0.6035038
	class 14: 0.6185438
	class 15: 0.7709663
	class 16: 0.00028406727
	class 17: 0.0
	class 18: 0.0
	class 19: 0.10942675
	class 20: 0.25106192

federated global round: 22, step: 4
select part of clients to conduct local training
[0, 8, 16, 14]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.826559513807297
Loss made of: CE 1.0418622493743896, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.142264366149902 EntMin 0.0
Epoch 1, Batch 20/24, Loss=6.294258779287338
Loss made of: CE 0.857315719127655, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.753974914550781 EntMin 0.0
Epoch 1, Class Loss=0.8473197221755981, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.8473197221755981, Class Loss=0.8473197221755981, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/24, Loss=6.1049684345722195
Loss made of: CE 0.632185161113739, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.099610805511475 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.71736096739769
Loss made of: CE 0.7050687670707703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.303296089172363 EntMin 0.0
Epoch 2, Class Loss=0.6948665380477905, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.6948665380477905, Class Loss=0.6948665380477905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/24, Loss=5.855655759572983
Loss made of: CE 0.7221630811691284, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.182782173156738 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.121031153202057
Loss made of: CE 0.5891109704971313, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.231141090393066 EntMin 0.0
Epoch 3, Class Loss=0.6443110704421997, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.6443110704421997, Class Loss=0.6443110704421997, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/24, Loss=5.549596339464188
Loss made of: CE 0.6343899965286255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.918494701385498 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.902725011110306
Loss made of: CE 0.5917553305625916, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.395109176635742 EntMin 0.0
Epoch 4, Class Loss=0.6050844192504883, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.6050844192504883, Class Loss=0.6050844192504883, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/24, Loss=5.851014804840088
Loss made of: CE 0.5787492990493774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.743457794189453 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.509206426143646
Loss made of: CE 0.7211431860923767, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.788816452026367 EntMin 0.0
Epoch 5, Class Loss=0.6061784029006958, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.6061784029006958, Class Loss=0.6061784029006958, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/24, Loss=5.516746568679809
Loss made of: CE 0.6522798538208008, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.71656608581543 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.608772125840187
Loss made of: CE 0.5341375470161438, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.039524555206299 EntMin 0.0
Epoch 6, Class Loss=0.5871979594230652, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.5871979594230652, Class Loss=0.5871979594230652, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.634581989049911
Loss made of: CE 0.9150893092155457, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.204758644104004 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.848603278398514
Loss made of: CE 0.7501922845840454, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4686431884765625 EntMin 0.0
Epoch 1, Class Loss=0.8640837073326111, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.8640837073326111, Class Loss=0.8640837073326111, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/24, Loss=6.0625239431858065
Loss made of: CE 0.6003080010414124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.745450973510742 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.781632071733474
Loss made of: CE 0.6756323575973511, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.079622268676758 EntMin 0.0
Epoch 2, Class Loss=0.7353076338768005, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.7353076338768005, Class Loss=0.7353076338768005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/24, Loss=5.7870226263999935
Loss made of: CE 0.6573073863983154, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.597076416015625 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.776214760541916
Loss made of: CE 0.8078050017356873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.913348197937012 EntMin 0.0
Epoch 3, Class Loss=0.6773888468742371, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.6773888468742371, Class Loss=0.6773888468742371, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/24, Loss=5.641668665409088
Loss made of: CE 0.6711674928665161, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1667656898498535 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.642896384000778
Loss made of: CE 0.6759648323059082, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.606420993804932 EntMin 0.0
Epoch 4, Class Loss=0.6440168619155884, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.6440168619155884, Class Loss=0.6440168619155884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/24, Loss=5.81687360405922
Loss made of: CE 0.7903995513916016, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9315900802612305 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.391116970777512
Loss made of: CE 0.646636962890625, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.557699680328369 EntMin 0.0
Epoch 5, Class Loss=0.6296960711479187, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6296960711479187, Class Loss=0.6296960711479187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/24, Loss=5.507381844520569
Loss made of: CE 0.6732730865478516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.625067234039307 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.337962675094604
Loss made of: CE 0.657927393913269, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.568351745605469 EntMin 0.0
Epoch 6, Class Loss=0.6236352920532227, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6236352920532227, Class Loss=0.6236352920532227, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=8.612367117404938
Loss made of: CE 0.9823951721191406, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.409932613372803 EntMin 0.0
Epoch 1, Class Loss=1.0002039670944214, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=1.0002039670944214, Class Loss=1.0002039670944214, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/19, Loss=6.899466168880463
Loss made of: CE 0.6974740028381348, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.145385265350342 EntMin 0.0
Epoch 2, Class Loss=0.6923650503158569, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.6923650503158569, Class Loss=0.6923650503158569, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/19, Loss=6.177467399835587
Loss made of: CE 0.5788156986236572, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.41004753112793 EntMin 0.0
Epoch 3, Class Loss=0.5492076277732849, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.5492076277732849, Class Loss=0.5492076277732849, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/19, Loss=5.949498274922371
Loss made of: CE 0.6182287335395813, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.659640789031982 EntMin 0.0
Epoch 4, Class Loss=0.4813022017478943, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.4813022017478943, Class Loss=0.4813022017478943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/19, Loss=5.790518724918366
Loss made of: CE 0.40891680121421814, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.057062149047852 EntMin 0.0
Epoch 5, Class Loss=0.4550430476665497, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.4550430476665497, Class Loss=0.4550430476665497, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/19, Loss=5.478137421607971
Loss made of: CE 0.3337729871273041, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.414714813232422 EntMin 0.0
Epoch 6, Class Loss=0.4272838532924652, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.4272838532924652, Class Loss=0.4272838532924652, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=6.334742230176926
Loss made of: CE 0.7010078430175781, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746821403503418 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.58876667022705
Loss made of: CE 0.6163164973258972, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.213809013366699 EntMin 0.0
Epoch 1, Class Loss=0.7274165749549866, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.7274165749549866, Class Loss=0.7274165749549866, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/26, Loss=5.855437362194062
Loss made of: CE 0.6816290616989136, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.431293487548828 EntMin 0.0
Epoch 2, Batch 20/26, Loss=5.729717963933945
Loss made of: CE 0.5374678373336792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.675310134887695 EntMin 0.0
Epoch 2, Class Loss=0.62189120054245, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.62189120054245, Class Loss=0.62189120054245, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/26, Loss=5.446656900644302
Loss made of: CE 0.7551390528678894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684774398803711 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.4721500933170315
Loss made of: CE 0.5842711329460144, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.450625896453857 EntMin 0.0
Epoch 3, Class Loss=0.5689517259597778, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.5689517259597778, Class Loss=0.5689517259597778, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/26, Loss=5.266629576683044
Loss made of: CE 0.5255946516990662, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.039694786071777 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.2108816504478455
Loss made of: CE 0.6121649742126465, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.205193519592285 EntMin 0.0
Epoch 4, Class Loss=0.5402495861053467, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.5402495861053467, Class Loss=0.5402495861053467, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/26, Loss=5.143629351258278
Loss made of: CE 0.6488735675811768, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.334896087646484 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.194298914074897
Loss made of: CE 0.5638922452926636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.987344264984131 EntMin 0.0
Epoch 5, Class Loss=0.5356112718582153, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5356112718582153, Class Loss=0.5356112718582153, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/26, Loss=5.02974870800972
Loss made of: CE 0.4711860120296478, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3703155517578125 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.108673492074013
Loss made of: CE 0.44865211844444275, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.136087894439697 EntMin 0.0
Epoch 6, Class Loss=0.5189907550811768, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5189907550811768, Class Loss=0.5189907550811768, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5360735654830933, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.836649
Mean Acc: 0.453194
FreqW Acc: 0.717165
Mean IoU: 0.365165
Class IoU:
	class 0: 0.8406879
	class 1: 0.5276153
	class 2: 0.2724891
	class 3: 0.49682
	class 4: 0.37777096
	class 5: 0.07892353
	class 6: 0.36774623
	class 7: 0.40380442
	class 8: 0.73388237
	class 9: 0.00015911896
	class 10: 0.34298757
	class 11: 0.25055212
	class 12: 0.6073247
	class 13: 0.4173596
	class 14: 0.59058136
	class 15: 0.7226671
	class 16: 0.00012412676
	class 17: 0.0
	class 18: 0.019490192
	class 19: 0.3067963
	class 20: 0.31068376
Class Acc:
	class 0: 0.98086095
	class 1: 0.5349034
	class 2: 0.43156314
	class 3: 0.8576575
	class 4: 0.84540397
	class 5: 0.078969866
	class 6: 0.3716315
	class 7: 0.407566
	class 8: 0.7991933
	class 9: 0.00015911896
	class 10: 0.4411035
	class 11: 0.28983015
	class 12: 0.8078291
	class 13: 0.5664245
	class 14: 0.64775705
	class 15: 0.77144754
	class 16: 0.00012416184
	class 17: 0.0
	class 18: 0.019760221
	class 19: 0.3425364
	class 20: 0.32234532

federated global round: 23, step: 4
select part of clients to conduct local training
[12, 10, 9, 6]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=5.586239445209503
Loss made of: CE 0.5666211843490601, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.105746269226074 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.3263514786958694
Loss made of: CE 0.48101672530174255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.092700004577637 EntMin 0.0
Epoch 1, Class Loss=0.6535656452178955, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.6535656452178955, Class Loss=0.6535656452178955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/26, Loss=5.207761538028717
Loss made of: CE 0.5666408538818359, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.004080772399902 EntMin 0.0
Epoch 2, Batch 20/26, Loss=5.262068819999695
Loss made of: CE 0.47422105073928833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611910343170166 EntMin 0.0
Epoch 2, Class Loss=0.5510927438735962, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.5510927438735962, Class Loss=0.5510927438735962, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/26, Loss=5.375557136535645
Loss made of: CE 0.5100950002670288, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.892148017883301 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.211465054750443
Loss made of: CE 0.565142810344696, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.269359111785889 EntMin 0.0
Epoch 3, Class Loss=0.5363550782203674, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.5363550782203674, Class Loss=0.5363550782203674, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/26, Loss=5.0114234536886215
Loss made of: CE 0.606757402420044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8805012702941895 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.013536947965622
Loss made of: CE 0.5433904528617859, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.122718334197998 EntMin 0.0
Epoch 4, Class Loss=0.5113804936408997, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.5113804936408997, Class Loss=0.5113804936408997, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/26, Loss=4.886672720313072
Loss made of: CE 0.46811699867248535, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.315886497497559 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.155802252888679
Loss made of: CE 0.5473600625991821, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.938064098358154 EntMin 0.0
Epoch 5, Class Loss=0.5058906674385071, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5058906674385071, Class Loss=0.5058906674385071, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/26, Loss=4.7791340231895445
Loss made of: CE 0.3931541442871094, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.752615451812744 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.6782545238733295
Loss made of: CE 0.5242348909378052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.028122901916504 EntMin 0.0
Epoch 6, Class Loss=0.47344499826431274, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.47344499826431274, Class Loss=0.47344499826431274, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=7.40291428565979
Loss made of: CE 1.1270594596862793, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.049678802490234 EntMin 0.0
Epoch 1, Class Loss=1.0932540893554688, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=1.0932540893554688, Class Loss=1.0932540893554688, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=6.310910040140152
Loss made of: CE 0.6590405106544495, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.622842788696289 EntMin 0.0
Epoch 2, Class Loss=0.8240878582000732, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.8240878582000732, Class Loss=0.8240878582000732, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=5.720502632856369
Loss made of: CE 0.7853288650512695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7477803230285645 EntMin 0.0
Epoch 3, Class Loss=0.7313783168792725, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.7313783168792725, Class Loss=0.7313783168792725, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=5.748791909217834
Loss made of: CE 0.6872249841690063, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.263630390167236 EntMin 0.0
Epoch 4, Class Loss=0.6458525657653809, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.6458525657653809, Class Loss=0.6458525657653809, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=5.624824488162995
Loss made of: CE 0.5475139021873474, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.608697414398193 EntMin 0.0
Epoch 5, Class Loss=0.6339930295944214, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.6339930295944214, Class Loss=0.6339930295944214, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=5.241627287864685
Loss made of: CE 0.6269358396530151, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.170801162719727 EntMin 0.0
Epoch 6, Class Loss=0.6111515164375305, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6111515164375305, Class Loss=0.6111515164375305, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=6.020271944999695
Loss made of: CE 0.8261489868164062, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.995862007141113 EntMin 0.0
Epoch 1, Batch 20/21, Loss=5.42830268740654
Loss made of: CE 0.5971006155014038, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611051559448242 EntMin 0.0
Epoch 1, Class Loss=0.772693932056427, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.772693932056427, Class Loss=0.772693932056427, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/21, Loss=5.1974510729312895
Loss made of: CE 0.684467077255249, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.675983905792236 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.454910558462143
Loss made of: CE 0.5275993347167969, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2316436767578125 EntMin 0.0
Epoch 2, Class Loss=0.6378410458564758, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.6378410458564758, Class Loss=0.6378410458564758, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/21, Loss=5.4909251689910885
Loss made of: CE 0.5043153762817383, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.159186363220215 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.338556039333343
Loss made of: CE 0.6147257685661316, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.809275150299072 EntMin 0.0
Epoch 3, Class Loss=0.5817189812660217, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5817189812660217, Class Loss=0.5817189812660217, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/21, Loss=4.900486469268799
Loss made of: CE 0.5138272047042847, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1732964515686035 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.156586053967476
Loss made of: CE 0.5280120372772217, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9513497352600098 EntMin 0.0
Epoch 4, Class Loss=0.5300114154815674, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.5300114154815674, Class Loss=0.5300114154815674, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=4.896363651752472
Loss made of: CE 0.5218337774276733, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.69737434387207 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.9514431923627855
Loss made of: CE 0.4751483201980591, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.752053737640381 EntMin 0.0
Epoch 5, Class Loss=0.5402724742889404, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5402724742889404, Class Loss=0.5402724742889404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/21, Loss=5.235336595773697
Loss made of: CE 0.5052472352981567, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.382002830505371 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.0639599621295925
Loss made of: CE 0.5179280638694763, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7809085845947266 EntMin 0.0
Epoch 6, Class Loss=0.5263952612876892, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.5263952612876892, Class Loss=0.5263952612876892, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.181423777341843
Loss made of: CE 0.708000123500824, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.237678527832031 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.988651555776596
Loss made of: CE 0.7112932205200195, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.754770755767822 EntMin 0.0
Epoch 1, Class Loss=0.7241622805595398, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.7241622805595398, Class Loss=0.7241622805595398, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/24, Loss=5.562970215082169
Loss made of: CE 0.6983575820922852, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.584185600280762 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.763741219043732
Loss made of: CE 0.6027123928070068, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.599883556365967 EntMin 0.0
Epoch 2, Class Loss=0.6368952393531799, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6368952393531799, Class Loss=0.6368952393531799, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/24, Loss=5.598403173685074
Loss made of: CE 0.5648335814476013, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.71627140045166 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.475200027227402
Loss made of: CE 0.6728010177612305, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.693515300750732 EntMin 0.0
Epoch 3, Class Loss=0.6104662418365479, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.6104662418365479, Class Loss=0.6104662418365479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/24, Loss=5.4259803771972654
Loss made of: CE 0.5605282783508301, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7349653244018555 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.360879820585251
Loss made of: CE 0.6157094240188599, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5753984451293945 EntMin 0.0
Epoch 4, Class Loss=0.6087740063667297, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.6087740063667297, Class Loss=0.6087740063667297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/24, Loss=5.37964499592781
Loss made of: CE 0.7825239896774292, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.547680854797363 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.685052841901779
Loss made of: CE 0.48830658197402954, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2721357345581055 EntMin 0.0
Epoch 5, Class Loss=0.6069446802139282, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.6069446802139282, Class Loss=0.6069446802139282, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/24, Loss=5.467605000734329
Loss made of: CE 0.5989860892295837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.651149749755859 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.258276095986366
Loss made of: CE 0.5913891792297363, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.662878036499023 EntMin 0.0
Epoch 6, Class Loss=0.5836755037307739, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5836755037307739, Class Loss=0.5836755037307739, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5216637849807739, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.840993
Mean Acc: 0.493167
FreqW Acc: 0.727610
Mean IoU: 0.392507
Class IoU:
	class 0: 0.84822315
	class 1: 0.5658125
	class 2: 0.3029595
	class 3: 0.4635416
	class 4: 0.403219
	class 5: 0.11035224
	class 6: 0.19575544
	class 7: 0.50726026
	class 8: 0.75317824
	class 9: 2.5073292e-05
	class 10: 0.31894284
	class 11: 0.25171652
	class 12: 0.62187785
	class 13: 0.42121106
	class 14: 0.63174546
	class 15: 0.7301837
	class 16: 3.9028888e-05
	class 17: 0.15252998
	class 18: 0.19311023
	class 19: 0.27650955
	class 20: 0.4944516
Class Acc:
	class 0: 0.9745543
	class 1: 0.5807468
	class 2: 0.5219321
	class 3: 0.8883905
	class 4: 0.8498889
	class 5: 0.110497884
	class 6: 0.19663765
	class 7: 0.51689374
	class 8: 0.8261637
	class 9: 2.5073292e-05
	class 10: 0.39376786
	class 11: 0.30164975
	class 12: 0.8254965
	class 13: 0.6174866
	class 14: 0.72128296
	class 15: 0.7868954
	class 16: 3.9035735e-05
	class 17: 0.1581036
	class 18: 0.22355095
	class 19: 0.312959
	class 20: 0.5495481

federated global round: 24, step: 4
select part of clients to conduct local training
[18, 24, 25, 10]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=6.734060740470886
Loss made of: CE 0.7636103630065918, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.143779277801514 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.8315420746803284, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.8315420746803284, Class Loss=0.8315420746803284, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/19, Loss=5.969775247573852
Loss made of: CE 0.8111027479171753, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.940901756286621 EntMin 0.0
Epoch 2, Class Loss=0.6868208646774292, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.6868208646774292, Class Loss=0.6868208646774292, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/19, Loss=5.743225121498108
Loss made of: CE 0.7533585429191589, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.610238075256348 EntMin 0.0
Epoch 3, Class Loss=0.6432433724403381, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.6432433724403381, Class Loss=0.6432433724403381, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/19, Loss=5.555842936038971
Loss made of: CE 0.6358412504196167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249630451202393 EntMin 0.0
Epoch 4, Class Loss=0.617430567741394, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.617430567741394, Class Loss=0.617430567741394, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/19, Loss=5.549795073270798
Loss made of: CE 0.47478747367858887, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.83919620513916 EntMin 0.0
Epoch 5, Class Loss=0.5917289853096008, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.5917289853096008, Class Loss=0.5917289853096008, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/19, Loss=5.197728925943375
Loss made of: CE 0.5728349089622498, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.536083698272705 EntMin 0.0
Epoch 6, Class Loss=0.5705899000167847, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.5705899000167847, Class Loss=0.5705899000167847, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/21, Loss=5.021752279996872
Loss made of: CE 0.4944462776184082, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.191195011138916 EntMin 0.0
Epoch 1, Batch 20/21, Loss=5.438184076547623
Loss made of: CE 0.7219288349151611, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.857735633850098 EntMin 0.0
Epoch 1, Class Loss=0.6435858607292175, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.6435858607292175, Class Loss=0.6435858607292175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/21, Loss=4.945097503066063
Loss made of: CE 0.5586127638816833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0777435302734375 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.561224865913391
Loss made of: CE 0.7333248853683472, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.008148193359375 EntMin 0.0
Epoch 2, Class Loss=0.5774451494216919, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.5774451494216919, Class Loss=0.5774451494216919, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/21, Loss=5.203106722235679
Loss made of: CE 0.5806090831756592, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.457777500152588 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.032504686713219
Loss made of: CE 0.5648415684700012, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.518252372741699 EntMin 0.0
Epoch 3, Class Loss=0.5456646680831909, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.5456646680831909, Class Loss=0.5456646680831909, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/21, Loss=4.734034591913224
Loss made of: CE 0.6141718626022339, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.580785274505615 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.8088576823472975
Loss made of: CE 0.474890798330307, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.274466037750244 EntMin 0.0
Epoch 4, Class Loss=0.5099719166755676, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.5099719166755676, Class Loss=0.5099719166755676, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/21, Loss=4.72807836830616
Loss made of: CE 0.4931403398513794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303140640258789 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.761106684803963
Loss made of: CE 0.5313187837600708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.107549667358398 EntMin 0.0
Epoch 5, Class Loss=0.5147466063499451, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.5147466063499451, Class Loss=0.5147466063499451, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/21, Loss=5.030090528726578
Loss made of: CE 0.6773097515106201, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.380623817443848 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.470108634233474
Loss made of: CE 0.5021585822105408, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.663750410079956 EntMin 0.0
Epoch 6, Class Loss=0.5169311165809631, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.5169311165809631, Class Loss=0.5169311165809631, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=5.611688715219498
Loss made of: CE 0.7052792310714722, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6793975830078125 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.5021839261054994
Loss made of: CE 0.6420928835868835, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.321566581726074 EntMin 0.0
Epoch 1, Class Loss=0.6721616983413696, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.6721616983413696, Class Loss=0.6721616983413696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/24, Loss=5.621876579523087
Loss made of: CE 0.7075168490409851, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.862066268920898 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.310043162107467
Loss made of: CE 0.6896824836730957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.811171054840088 EntMin 0.0
Epoch 2, Class Loss=0.630215048789978, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.630215048789978, Class Loss=0.630215048789978, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/24, Loss=5.532797652482986
Loss made of: CE 0.5831276178359985, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.902186870574951 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.130204647779465
Loss made of: CE 0.48578739166259766, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.250181674957275 EntMin 0.0
Epoch 3, Class Loss=0.6203258037567139, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.6203258037567139, Class Loss=0.6203258037567139, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/24, Loss=5.365446257591247
Loss made of: CE 0.665329098701477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.298985481262207 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.276268464326859
Loss made of: CE 0.6144067049026489, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.742827892303467 EntMin 0.0
Epoch 4, Class Loss=0.5918158292770386, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.5918158292770386, Class Loss=0.5918158292770386, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/24, Loss=5.2076017498970035
Loss made of: CE 0.6983911395072937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.227005481719971 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.17306666970253
Loss made of: CE 0.6192984580993652, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.366392135620117 EntMin 0.0
Epoch 5, Class Loss=0.5870221257209778, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.5870221257209778, Class Loss=0.5870221257209778, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/24, Loss=4.7844983577728275
Loss made of: CE 0.6392590403556824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.323416709899902 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.996997857093811
Loss made of: CE 0.6575524806976318, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.675106525421143 EntMin 0.0
Epoch 6, Class Loss=0.5709046125411987, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.5709046125411987, Class Loss=0.5709046125411987, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=6.525004321336747
Loss made of: CE 0.9065839648246765, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.47331428527832 EntMin 0.0
Epoch 1, Class Loss=0.8959938883781433, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.8959938883781433, Class Loss=0.8959938883781433, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/19, Loss=5.833320891857147
Loss made of: CE 0.5537147521972656, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.044609069824219 EntMin 0.0
Epoch 2, Class Loss=0.7710875272750854, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.7710875272750854, Class Loss=0.7710875272750854, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/19, Loss=5.475028002262116
Loss made of: CE 0.7077583074569702, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.800152778625488 EntMin 0.0
Epoch 3, Class Loss=0.7329488396644592, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.7329488396644592, Class Loss=0.7329488396644592, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/19, Loss=5.687997072935104
Loss made of: CE 0.745137631893158, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.184037208557129 EntMin 0.0
Epoch 4, Class Loss=0.6663287281990051, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.6663287281990051, Class Loss=0.6663287281990051, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/19, Loss=5.508670425415039
Loss made of: CE 0.5925230979919434, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.410111427307129 EntMin 0.0
Epoch 5, Class Loss=0.6570150256156921, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.6570150256156921, Class Loss=0.6570150256156921, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/19, Loss=5.167765456438064
Loss made of: CE 0.6423643827438354, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.187680244445801 EntMin 0.0
Epoch 6, Class Loss=0.6386594176292419, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6386594176292419, Class Loss=0.6386594176292419, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.5355138778686523, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.838409
Mean Acc: 0.508964
FreqW Acc: 0.725491
Mean IoU: 0.399739
Class IoU:
	class 0: 0.84436566
	class 1: 0.50122833
	class 2: 0.2980874
	class 3: 0.47453701
	class 4: 0.40738976
	class 5: 0.12304651
	class 6: 0.22231595
	class 7: 0.48806766
	class 8: 0.75178015
	class 9: 0.0
	class 10: 0.2573206
	class 11: 0.2646706
	class 12: 0.6397843
	class 13: 0.40547585
	class 14: 0.6206821
	class 15: 0.7226159
	class 16: 1.3638638e-05
	class 17: 0.40243155
	class 18: 0.25613976
	class 19: 0.19182284
	class 20: 0.5227413
Class Acc:
	class 0: 0.9693607
	class 1: 0.5187982
	class 2: 0.5090892
	class 3: 0.8605681
	class 4: 0.8352835
	class 5: 0.123382874
	class 6: 0.22349642
	class 7: 0.49457058
	class 8: 0.8084391
	class 9: 0.0
	class 10: 0.27603352
	class 11: 0.3248194
	class 12: 0.81108683
	class 13: 0.5767894
	class 14: 0.69664615
	class 15: 0.7754745
	class 16: 1.3638991e-05
	class 17: 0.6919975
	class 18: 0.38031766
	class 19: 0.19994482
	class 20: 0.6121378

voc_4-4_OURS-APL On GPUs 0
Run in 91978s
