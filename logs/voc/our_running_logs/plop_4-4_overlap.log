nohup: ignoring input
25
kvoc_4-4_PLOP On GPUs 0\Writing in results/seed_2023-ov/2023-03-11_voc_4-4_PLOP.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[6, 0, 7, 2]
Current Client Index:  6
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[8, 5, 3, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[5, 2, 0, 3]
Current Client Index:  5
Current Client Index:  2
Current Client Index:  0
Current Client Index:  3
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[3, 6, 1, 7]
Current Client Index:  3
Current Client Index:  6
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.09879439324140549, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.957759
Mean Acc: 0.761733
FreqW Acc: 0.923270
Mean IoU: 0.703965
Class IoU:
	class 0: 0.9539515191877496
	class 1: 0.7615770129403676
	class 2: 0.2054173196242969
	class 3: 0.8920301311931925
	class 4: 0.7068509839220561
Class Acc:
	class 0: 0.9848764620456163
	class 1: 0.7738026584549321
	class 2: 0.27733822617675347
	class 3: 0.9519182023560161
	class 4: 0.8207287142557349

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[0, 12, 6, 10]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/52, Loss=8.916943728923798
Loss made of: CE 1.144055962562561, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.277935981750488 EntMin 0.0
Epoch 1, Batch 20/52, Loss=7.139327800273895
Loss made of: CE 0.8405463099479675, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.039034843444824 EntMin 0.0
Epoch 1, Batch 30/52, Loss=6.650381958484649
Loss made of: CE 0.8004255294799805, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.440421104431152 EntMin 0.0
Epoch 1, Batch 40/52, Loss=6.052467530965805
Loss made of: CE 0.5496306419372559, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.073419570922852 EntMin 0.0
Epoch 1, Batch 50/52, Loss=5.936309415102005
Loss made of: CE 0.5514610409736633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.76016902923584 EntMin 0.0
Epoch 1, Class Loss=0.8768524527549744, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.8768524527549744, Class Loss=0.8768524527549744, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/52, Loss=5.544019278883934
Loss made of: CE 0.5497021675109863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.794495582580566 EntMin 0.0
Epoch 2, Batch 20/52, Loss=5.247793918848037
Loss made of: CE 0.5605798959732056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.680510997772217 EntMin 0.0
Epoch 2, Batch 30/52, Loss=5.372434329986572
Loss made of: CE 0.4607650637626648, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.656620025634766 EntMin 0.0
Epoch 2, Batch 40/52, Loss=5.10379051566124
Loss made of: CE 0.39646849036216736, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.078802108764648 EntMin 0.0
Epoch 2, Batch 50/52, Loss=4.95942140519619
Loss made of: CE 0.4640127420425415, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.028782367706299 EntMin 0.0
Epoch 2, Class Loss=0.49909695982933044, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.49909695982933044, Class Loss=0.49909695982933044, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/52, Loss=4.975616598129273
Loss made of: CE 0.3339861333370209, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.711911201477051 EntMin 0.0
Epoch 3, Batch 20/52, Loss=4.750648710131645
Loss made of: CE 0.3488680124282837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.164123058319092 EntMin 0.0
Epoch 3, Batch 30/52, Loss=4.771103629469872
Loss made of: CE 0.4204447269439697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7850098609924316 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.66303286254406
Loss made of: CE 0.28781697154045105, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.956982374191284 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.6441795319318775
Loss made of: CE 0.2991258502006531, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.250423431396484 EntMin 0.0
Epoch 3, Class Loss=0.37067291140556335, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.37067291140556335, Class Loss=0.37067291140556335, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/52, Loss=4.73347038179636
Loss made of: CE 0.470880389213562, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.166750907897949 EntMin 0.0
Epoch 4, Batch 20/52, Loss=4.459325712919235
Loss made of: CE 0.31047576665878296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.183160781860352 EntMin 0.0
Epoch 4, Batch 30/52, Loss=4.4100761890411375
Loss made of: CE 0.21121162176132202, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.716310501098633 EntMin 0.0
Epoch 4, Batch 40/52, Loss=4.381193137168884
Loss made of: CE 0.2108011692762375, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.903402805328369 EntMin 0.0
Epoch 4, Batch 50/52, Loss=4.179341480135918
Loss made of: CE 0.29925858974456787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6749744415283203 EntMin 0.0
Epoch 4, Class Loss=0.2930849492549896, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.2930849492549896, Class Loss=0.2930849492549896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/52, Loss=4.275964561104774
Loss made of: CE 0.24335268139839172, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.618748188018799 EntMin 0.0
Epoch 5, Batch 20/52, Loss=4.265208774805069
Loss made of: CE 0.205746591091156, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8865227699279785 EntMin 0.0
Epoch 5, Batch 30/52, Loss=4.232548925280571
Loss made of: CE 0.29234835505485535, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8153538703918457 EntMin 0.0
Epoch 5, Batch 40/52, Loss=4.202852496504784
Loss made of: CE 0.2474224865436554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3355793952941895 EntMin 0.0
Epoch 5, Batch 50/52, Loss=4.1788699477910995
Loss made of: CE 0.25252336263656616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.795623540878296 EntMin 0.0
Epoch 5, Class Loss=0.25560399889945984, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.25560399889945984, Class Loss=0.25560399889945984, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/52, Loss=3.981594605743885
Loss made of: CE 0.1858665943145752, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.575096845626831 EntMin 0.0
Epoch 6, Batch 20/52, Loss=4.17842036485672
Loss made of: CE 0.21141885221004486, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.452387571334839 EntMin 0.0
Epoch 6, Batch 30/52, Loss=4.273346531391144
Loss made of: CE 0.3217298686504364, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.839761734008789 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.924168063700199
Loss made of: CE 0.23883652687072754, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.439861297607422 EntMin 0.0
Epoch 6, Batch 50/52, Loss=4.083384421467781
Loss made of: CE 0.2675784230232239, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.139122009277344 EntMin 0.0
Epoch 6, Class Loss=0.22571909427642822, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.22571909427642822, Class Loss=0.22571909427642822, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=9.681536328792571
Loss made of: CE 1.046398401260376, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.049055099487305 EntMin 0.0
Epoch 1, Batch 20/40, Loss=8.512935835123063
Loss made of: CE 0.6700551509857178, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.421405792236328 EntMin 0.0
Epoch 1, Batch 30/40, Loss=7.584436571598053
Loss made of: CE 0.6299881935119629, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.304344177246094 EntMin 0.0
Epoch 1, Batch 40/40, Loss=7.033660367131233
Loss made of: CE 0.4780813157558441, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.403678894042969 EntMin 0.0
Epoch 1, Class Loss=0.8468101620674133, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.8468101620674133, Class Loss=0.8468101620674133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/40, Loss=6.758167919516564
Loss made of: CE 0.26669037342071533, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.506335258483887 EntMin 0.0
Epoch 2, Batch 20/40, Loss=6.249136945605278
Loss made of: CE 0.4564865529537201, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.123595237731934 EntMin 0.0
Epoch 2, Batch 30/40, Loss=6.07344928085804
Loss made of: CE 0.4424768388271332, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.444882869720459 EntMin 0.0
Epoch 2, Batch 40/40, Loss=6.22439369559288
Loss made of: CE 0.444084107875824, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.15018367767334 EntMin 0.0
Epoch 2, Class Loss=0.45332953333854675, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.45332953333854675, Class Loss=0.45332953333854675, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/40, Loss=5.816990673542023
Loss made of: CE 0.3070050776004791, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2702789306640625 EntMin 0.0
Epoch 3, Batch 20/40, Loss=5.691444531083107
Loss made of: CE 0.5073139667510986, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.489506721496582 EntMin 0.0
Epoch 3, Batch 30/40, Loss=5.619426722824573
Loss made of: CE 0.28957271575927734, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.312830924987793 EntMin 0.0
Epoch 3, Batch 40/40, Loss=5.432185423374176
Loss made of: CE 0.24159035086631775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.910724639892578 EntMin 0.0
Epoch 3, Class Loss=0.35976526141166687, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.35976526141166687, Class Loss=0.35976526141166687, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/40, Loss=5.269802424311638
Loss made of: CE 0.35613903403282166, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.435051918029785 EntMin 0.0
Epoch 4, Batch 20/40, Loss=5.307259276509285
Loss made of: CE 0.1815074384212494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.609533309936523 EntMin 0.0
Epoch 4, Batch 30/40, Loss=5.2848798632621765
Loss made of: CE 0.28264689445495605, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.06012487411499 EntMin 0.0
Epoch 4, Batch 40/40, Loss=5.35903859436512
Loss made of: CE 0.2739717662334442, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.208596706390381 EntMin 0.0
Epoch 4, Class Loss=0.2985711097717285, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.2985711097717285, Class Loss=0.2985711097717285, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/40, Loss=5.188312239944935
Loss made of: CE 0.2448217123746872, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.820905685424805 EntMin 0.0
Epoch 5, Batch 20/40, Loss=5.029287123680115
Loss made of: CE 0.33156538009643555, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.09414005279541 EntMin 0.0
Epoch 5, Batch 30/40, Loss=5.12556100487709
Loss made of: CE 0.1925632208585739, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.405207633972168 EntMin 0.0
Epoch 5, Batch 40/40, Loss=5.130915732681752
Loss made of: CE 0.24009166657924652, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.621359825134277 EntMin 0.0
Epoch 5, Class Loss=0.25525057315826416, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.25525057315826416, Class Loss=0.25525057315826416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/40, Loss=4.925073984265327
Loss made of: CE 0.1721981167793274, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.529222011566162 EntMin 0.0
Epoch 6, Batch 20/40, Loss=5.075343427062035
Loss made of: CE 0.1974312961101532, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.285387992858887 EntMin 0.0
Epoch 6, Batch 30/40, Loss=5.136253525316715
Loss made of: CE 0.22791516780853271, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.181086540222168 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.786123578250408
Loss made of: CE 0.15910020470619202, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.414368629455566 EntMin 0.0
Epoch 6, Class Loss=0.2337508201599121, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.2337508201599121, Class Loss=0.2337508201599121, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=10.13475170135498
Loss made of: CE 1.2475568056106567, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.164908409118652 EntMin 0.0
Epoch 1, Batch 20/34, Loss=8.50351436138153
Loss made of: CE 0.7863874435424805, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.409663200378418 EntMin 0.0
Epoch 1, Batch 30/34, Loss=8.094188737869263
Loss made of: CE 0.7888376116752625, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.789754867553711 EntMin 0.0
Epoch 1, Class Loss=1.0428200960159302, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=1.0428200960159302, Class Loss=1.0428200960159302, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/34, Loss=7.162141174077988
Loss made of: CE 0.6093363165855408, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.365395545959473 EntMin 0.0
Epoch 2, Batch 20/34, Loss=6.583373564481735
Loss made of: CE 0.6402149200439453, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.603118419647217 EntMin 0.0
Epoch 2, Batch 30/34, Loss=6.358539819717407
Loss made of: CE 0.46371912956237793, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.998612880706787 EntMin 0.0
Epoch 2, Class Loss=0.5875927209854126, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.5875927209854126, Class Loss=0.5875927209854126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/34, Loss=5.89638078212738
Loss made of: CE 0.5249934792518616, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.208651542663574 EntMin 0.0
Epoch 3, Batch 20/34, Loss=5.813348430395126
Loss made of: CE 0.42021772265434265, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.11618709564209 EntMin 0.0
Epoch 3, Batch 30/34, Loss=5.9410943508148195
Loss made of: CE 0.4120015501976013, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.22685432434082 EntMin 0.0
Epoch 3, Class Loss=0.4312399923801422, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4312399923801422, Class Loss=0.4312399923801422, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/34, Loss=5.549312448501587
Loss made of: CE 0.358731746673584, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.175655364990234 EntMin 0.0
Epoch 4, Batch 20/34, Loss=5.475526201725006
Loss made of: CE 0.3147844672203064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684080123901367 EntMin 0.0
Epoch 4, Batch 30/34, Loss=5.51930071413517
Loss made of: CE 0.43543142080307007, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.721901893615723 EntMin 0.0
Epoch 4, Class Loss=0.3442004919052124, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3442004919052124, Class Loss=0.3442004919052124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/34, Loss=5.237077946960926
Loss made of: CE 0.258337140083313, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.483327865600586 EntMin 0.0
Epoch 5, Batch 20/34, Loss=5.348791152238846
Loss made of: CE 0.3098854124546051, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5378217697143555 EntMin 0.0
Epoch 5, Batch 30/34, Loss=5.029879805445671
Loss made of: CE 0.24892662465572357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.714363098144531 EntMin 0.0
Epoch 5, Class Loss=0.28038209676742554, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.28038209676742554, Class Loss=0.28038209676742554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/34, Loss=5.082898361980915
Loss made of: CE 0.2997819781303406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.878925800323486 EntMin 0.0
Epoch 6, Batch 20/34, Loss=5.058558708429336
Loss made of: CE 0.2378237545490265, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.633840084075928 EntMin 0.0
Epoch 6, Batch 30/34, Loss=5.144860744476318
Loss made of: CE 0.21556243300437927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.523241996765137 EntMin 0.0
Epoch 6, Class Loss=0.24631449580192566, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.24631449580192566, Class Loss=0.24631449580192566, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=9.313686299324036
Loss made of: CE 1.1998546123504639, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.430824279785156 EntMin 0.0
Epoch 1, Batch 20/33, Loss=7.940800380706787
Loss made of: CE 0.9751437902450562, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.184532165527344 EntMin 0.0
Epoch 1, Batch 30/33, Loss=6.767064410448074
Loss made of: CE 0.737473726272583, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.020174026489258 EntMin 0.0
Epoch 1, Class Loss=1.0070005655288696, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=1.0070005655288696, Class Loss=1.0070005655288696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/33, Loss=6.239732176065445
Loss made of: CE 0.6839747428894043, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.20393705368042 EntMin 0.0
Epoch 2, Batch 20/33, Loss=5.906930327415466
Loss made of: CE 0.7904565334320068, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.59590482711792 EntMin 0.0
Epoch 2, Batch 30/33, Loss=5.485173404216766
Loss made of: CE 0.5477564334869385, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.882054328918457 EntMin 0.0
Epoch 2, Class Loss=0.6467435956001282, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.6467435956001282, Class Loss=0.6467435956001282, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/33, Loss=5.164880800247192
Loss made of: CE 0.5179587602615356, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.458521842956543 EntMin 0.0
Epoch 3, Batch 20/33, Loss=5.127336275577545
Loss made of: CE 0.42049694061279297, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.36994743347168 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.976014751195907
Loss made of: CE 0.44852226972579956, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.267429828643799 EntMin 0.0
Epoch 3, Class Loss=0.4861629605293274, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.4861629605293274, Class Loss=0.4861629605293274, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/33, Loss=4.672557127475739
Loss made of: CE 0.37746936082839966, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9285764694213867 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.785677126049995
Loss made of: CE 0.36517614126205444, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.289371013641357 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.599418750405311
Loss made of: CE 0.295088529586792, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.902280330657959 EntMin 0.0
Epoch 4, Class Loss=0.37881579995155334, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.37881579995155334, Class Loss=0.37881579995155334, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/33, Loss=4.478126394748688
Loss made of: CE 0.394098162651062, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.191672325134277 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.375670756399631
Loss made of: CE 0.24759630858898163, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.786618709564209 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.556366074085235
Loss made of: CE 0.3371220529079437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.951381206512451 EntMin 0.0
Epoch 5, Class Loss=0.3034782111644745, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.3034782111644745, Class Loss=0.3034782111644745, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/33, Loss=4.303759206831455
Loss made of: CE 0.34734994173049927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.145212650299072 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.358665961027145
Loss made of: CE 0.2201029509305954, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.863396406173706 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.199072568118572
Loss made of: CE 0.256416916847229, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.226755619049072 EntMin 0.0
Epoch 6, Class Loss=0.26120319962501526, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.26120319962501526, Class Loss=0.26120319962501526, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.39345839619636536, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.851708
Mean Acc: 0.479254
FreqW Acc: 0.733450
Mean IoU: 0.397394
Class IoU:
	class 0: 0.85089767
	class 1: 0.7146446
	class 2: 0.26893976
	class 3: 0.59392405
	class 4: 0.5518805
	class 5: 0.0
	class 6: 0.01529463
	class 7: 0.004298197
	class 8: 0.5766655
Class Acc:
	class 0: 0.9863464
	class 1: 0.7722504
	class 2: 0.41512504
	class 3: 0.69721216
	class 4: 0.749532
	class 5: 0.0
	class 6: 0.015295546
	class 7: 0.0042995233
	class 8: 0.6732209

federated global round: 6, step: 1
select part of clients to conduct local training
[11, 5, 8, 12]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/43, Loss=5.207554417848587
Loss made of: CE 0.40463489294052124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.755136489868164 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.762225511670112
Loss made of: CE 0.5935072898864746, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9139862060546875 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.570523282885551
Loss made of: CE 0.3869972825050354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.671149730682373 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.478960624337196
Loss made of: CE 0.4218427538871765, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7430570125579834 EntMin 0.0
Epoch 1, Class Loss=0.4449746012687683, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.4449746012687683, Class Loss=0.4449746012687683, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/43, Loss=4.265701432526112
Loss made of: CE 0.30346497893333435, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8643460273742676 EntMin 0.0
Epoch 2, Batch 20/43, Loss=4.122941811382771
Loss made of: CE 0.3049975037574768, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8293986320495605 EntMin 0.0
Epoch 2, Batch 30/43, Loss=4.15869782269001
Loss made of: CE 0.3774576187133789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.26912260055542 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.229975138604641
Loss made of: CE 0.3824465870857239, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084539890289307 EntMin 0.0
Epoch 2, Class Loss=0.3171941936016083, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.3171941936016083, Class Loss=0.3171941936016083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/43, Loss=3.937918695807457
Loss made of: CE 0.3646499514579773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.557835578918457 EntMin 0.0
Epoch 3, Batch 20/43, Loss=4.009175665676594
Loss made of: CE 0.24809858202934265, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9773757457733154 EntMin 0.0
Epoch 3, Batch 30/43, Loss=4.0145059987902645
Loss made of: CE 0.3081434965133667, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6395277976989746 EntMin 0.0
Epoch 3, Batch 40/43, Loss=3.9428153961896895
Loss made of: CE 0.1775425523519516, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.812910795211792 EntMin 0.0
Epoch 3, Class Loss=0.2749612033367157, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.2749612033367157, Class Loss=0.2749612033367157, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/43, Loss=3.822882926464081
Loss made of: CE 0.3163589835166931, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.042386531829834 EntMin 0.0
Epoch 4, Batch 20/43, Loss=3.8517955899238587
Loss made of: CE 0.26233822107315063, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.526797294616699 EntMin 0.0
Epoch 4, Batch 30/43, Loss=3.880831614136696
Loss made of: CE 0.2816261351108551, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.474217414855957 EntMin 0.0
Epoch 4, Batch 40/43, Loss=3.8510974898934363
Loss made of: CE 0.21088580787181854, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.36051344871521 EntMin 0.0
Epoch 4, Class Loss=0.24234656989574432, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.24234656989574432, Class Loss=0.24234656989574432, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/43, Loss=3.8253178000450134
Loss made of: CE 0.21143513917922974, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.868088722229004 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.7870012879371644
Loss made of: CE 0.27781519293785095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.587395191192627 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.817680795490742
Loss made of: CE 0.22814922034740448, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.608440399169922 EntMin 0.0
Epoch 5, Batch 40/43, Loss=3.8584597021341325
Loss made of: CE 0.25188180804252625, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.223144054412842 EntMin 0.0
Epoch 5, Class Loss=0.2301337867975235, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.2301337867975235, Class Loss=0.2301337867975235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/43, Loss=3.7187201544642448
Loss made of: CE 0.2545236647129059, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3203330039978027 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.790202984213829
Loss made of: CE 0.21846501529216766, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4578633308410645 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.709071455895901
Loss made of: CE 0.16207319498062134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.202604055404663 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.5862705960869787
Loss made of: CE 0.2472868412733078, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.352562665939331 EntMin 0.0
Epoch 6, Class Loss=0.2077493816614151, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.2077493816614151, Class Loss=0.2077493816614151, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=5.594190979003907
Loss made of: CE 0.5435419082641602, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0507402420043945 EntMin 0.0
Epoch 1, Batch 20/34, Loss=5.474654096364975
Loss made of: CE 0.3653687834739685, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.203097820281982 EntMin 0.0
Epoch 1, Batch 30/34, Loss=5.301686504483223
Loss made of: CE 0.2546764016151428, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.016609191894531 EntMin 0.0
Epoch 1, Class Loss=0.388030469417572, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.388030469417572, Class Loss=0.388030469417572, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/34, Loss=5.120233231782914
Loss made of: CE 0.27765515446662903, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.79306697845459 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.893248687684536
Loss made of: CE 0.25126731395721436, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.768538475036621 EntMin 0.0
Epoch 2, Batch 30/34, Loss=5.196486832201481
Loss made of: CE 0.2820202708244324, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.585305690765381 EntMin 0.0
Epoch 2, Class Loss=0.2624768912792206, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.2624768912792206, Class Loss=0.2624768912792206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/34, Loss=4.685661673545837
Loss made of: CE 0.2397623062133789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.45827579498291 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.839935587346554
Loss made of: CE 0.1979217529296875, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.545392990112305 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.924290975928306
Loss made of: CE 0.24524036049842834, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.347804546356201 EntMin 0.0
Epoch 3, Class Loss=0.21558374166488647, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.21558374166488647, Class Loss=0.21558374166488647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/34, Loss=4.793670183420181
Loss made of: CE 0.19084422290325165, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.516582489013672 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.809912018477917
Loss made of: CE 0.20953874289989471, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.657215118408203 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.658381003141403
Loss made of: CE 0.20899197459220886, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.983698844909668 EntMin 0.0
Epoch 4, Class Loss=0.1971052587032318, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.1971052587032318, Class Loss=0.1971052587032318, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/34, Loss=4.525914815068245
Loss made of: CE 0.18021909892559052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.580014228820801 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.637194384634495
Loss made of: CE 0.1957462728023529, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.172943115234375 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.561435678601265
Loss made of: CE 0.23048417270183563, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.874911308288574 EntMin 0.0
Epoch 5, Class Loss=0.18146158754825592, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.18146158754825592, Class Loss=0.18146158754825592, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/34, Loss=4.689774431288242
Loss made of: CE 0.22151708602905273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.130606651306152 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.459859342873097
Loss made of: CE 0.1569736897945404, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.492252349853516 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.40922757089138
Loss made of: CE 0.26801973581314087, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.101030349731445 EntMin 0.0
Epoch 6, Class Loss=0.17858661711215973, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.17858661711215973, Class Loss=0.17858661711215973, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=5.71489617228508
Loss made of: CE 0.4218948781490326, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.379354000091553 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.550979752838612
Loss made of: CE 0.25961288809776306, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.345553398132324 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.520337387919426
Loss made of: CE 0.34542223811149597, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.791927337646484 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.205230529606342
Loss made of: CE 0.2308407723903656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.309918403625488 EntMin 0.0
Epoch 1, Class Loss=0.3492985665798187, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.3492985665798187, Class Loss=0.3492985665798187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/40, Loss=5.203587311506271
Loss made of: CE 0.32593390345573425, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.559460163116455 EntMin 0.0
Epoch 2, Batch 20/40, Loss=4.862328064441681
Loss made of: CE 0.1459999531507492, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.434288024902344 EntMin 0.0
Epoch 2, Batch 30/40, Loss=5.148137095570564
Loss made of: CE 0.31004267930984497, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.842495441436768 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.0536349132657055
Loss made of: CE 0.24574849009513855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.982236385345459 EntMin 0.0
Epoch 2, Class Loss=0.2654423117637634, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.2654423117637634, Class Loss=0.2654423117637634, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/40, Loss=4.897644992172718
Loss made of: CE 0.1853857934474945, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.675559043884277 EntMin 0.0
Epoch 3, Batch 20/40, Loss=4.745965713262558
Loss made of: CE 0.20949769020080566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.450935363769531 EntMin 0.0
Epoch 3, Batch 30/40, Loss=4.871598033607006
Loss made of: CE 0.2757914662361145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.252840518951416 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.831794692575931
Loss made of: CE 0.19599437713623047, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.059146404266357 EntMin 0.0
Epoch 3, Class Loss=0.22682547569274902, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.22682547569274902, Class Loss=0.22682547569274902, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/40, Loss=4.752547858655452
Loss made of: CE 0.31043747067451477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.720507621765137 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.625928239524365
Loss made of: CE 0.19504539668560028, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500921726226807 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.716649186611176
Loss made of: CE 0.1979309320449829, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.992430686950684 EntMin 0.0
Epoch 4, Batch 40/40, Loss=4.670280802249908
Loss made of: CE 0.1848216950893402, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.214148044586182 EntMin 0.0
Epoch 4, Class Loss=0.20838192105293274, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.20838192105293274, Class Loss=0.20838192105293274, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/40, Loss=4.599683423340321
Loss made of: CE 0.2117602527141571, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1217851638793945 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.648170648515224
Loss made of: CE 0.22768883407115936, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.152978420257568 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.441399891674519
Loss made of: CE 0.21843504905700684, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.485199451446533 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.591910590231419
Loss made of: CE 0.1619688868522644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.415262222290039 EntMin 0.0
Epoch 5, Class Loss=0.19211582839488983, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.19211582839488983, Class Loss=0.19211582839488983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/40, Loss=4.370704248547554
Loss made of: CE 0.15447835624217987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.383028030395508 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.516335885226726
Loss made of: CE 0.16150885820388794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.121417045593262 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.389720921963454
Loss made of: CE 0.19088098406791687, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.163767337799072 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.4899767056107525
Loss made of: CE 0.14173656702041626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.014971733093262 EntMin 0.0
Epoch 6, Class Loss=0.1750122308731079, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.1750122308731079, Class Loss=0.1750122308731079, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/40, Loss=5.627629718184471
Loss made of: CE 0.33387407660484314, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.393911361694336 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.540128821134568
Loss made of: CE 0.2966461181640625, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.087953567504883 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.3620895713567736
Loss made of: CE 0.46596312522888184, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.849428653717041 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.299292317032814
Loss made of: CE 0.27053317427635193, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.027461528778076 EntMin 0.0
Epoch 1, Class Loss=0.3654840588569641, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.3654840588569641, Class Loss=0.3654840588569641, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/40, Loss=5.248617376387119
Loss made of: CE 0.15004853904247284, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.335818290710449 EntMin 0.0
Epoch 2, Batch 20/40, Loss=5.066188956797123
Loss made of: CE 0.25294387340545654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.286424160003662 EntMin 0.0
Epoch 2, Batch 30/40, Loss=4.949801896512509
Loss made of: CE 0.34489747881889343, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.449636459350586 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.172767478227615
Loss made of: CE 0.24424563348293304, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.288633346557617 EntMin 0.0
Epoch 2, Class Loss=0.27288511395454407, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.27288511395454407, Class Loss=0.27288511395454407, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/40, Loss=5.004464787244797
Loss made of: CE 0.18550646305084229, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.520545959472656 EntMin 0.0
Epoch 3, Batch 20/40, Loss=4.932710184156894
Loss made of: CE 0.38123345375061035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.765542984008789 EntMin 0.0
Epoch 3, Batch 30/40, Loss=4.907629457116127
Loss made of: CE 0.185212180018425, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758989334106445 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.773327049612999
Loss made of: CE 0.17027299106121063, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.482192039489746 EntMin 0.0
Epoch 3, Class Loss=0.24723930656909943, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.24723930656909943, Class Loss=0.24723930656909943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/40, Loss=4.634558099508285
Loss made of: CE 0.23152673244476318, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.314547538757324 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.792104759812355
Loss made of: CE 0.13469314575195312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.168558120727539 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.70670273900032
Loss made of: CE 0.2206004112958908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.735589027404785 EntMin 0.0
Epoch 4, Batch 40/40, Loss=4.89430350214243
Loss made of: CE 0.21479758620262146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.805300712585449 EntMin 0.0
Epoch 4, Class Loss=0.2236224263906479, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.2236224263906479, Class Loss=0.2236224263906479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/40, Loss=4.719869919121265
Loss made of: CE 0.17893072962760925, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.37733793258667 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.562066806852817
Loss made of: CE 0.27015888690948486, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.701366424560547 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.644748592376709
Loss made of: CE 0.1467278152704239, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.181792736053467 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.587281179428101
Loss made of: CE 0.1924261748790741, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.20293664932251 EntMin 0.0
Epoch 5, Class Loss=0.19739757478237152, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.19739757478237152, Class Loss=0.19739757478237152, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/40, Loss=4.4574712961912155
Loss made of: CE 0.14449632167816162, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4343109130859375 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.627866569161415
Loss made of: CE 0.1603565812110901, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.711879730224609 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.704579366743564
Loss made of: CE 0.20689205825328827, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.070817947387695 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.360181380808354
Loss made of: CE 0.13757912814617157, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.141348838806152 EntMin 0.0
Epoch 6, Class Loss=0.18700015544891357, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.18700015544891357, Class Loss=0.18700015544891357, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.320622980594635, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.874603
Mean Acc: 0.526864
FreqW Acc: 0.779002
Mean IoU: 0.433827
Class IoU:
	class 0: 0.8848591
	class 1: 0.7691359
	class 2: 0.30422902
	class 3: 0.034925275
	class 4: 0.6112766
	class 5: 0.008208016
	class 6: 0.55184287
	class 7: 0.16697174
	class 8: 0.5729969
Class Acc:
	class 0: 0.9763859
	class 1: 0.81923467
	class 2: 0.5178748
	class 3: 0.034947757
	class 4: 0.74061775
	class 5: 0.008210712
	class 6: 0.568154
	class 7: 0.16852121
	class 8: 0.90782636

federated global round: 7, step: 1
select part of clients to conduct local training
[0, 6, 13, 5]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/52, Loss=4.331293261051178
Loss made of: CE 0.4934249818325043, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.974940776824951 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.0515330761671065
Loss made of: CE 0.2668995261192322, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5474977493286133 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.9869542852044106
Loss made of: CE 0.21299926936626434, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5918588638305664 EntMin 0.0
Epoch 1, Batch 40/52, Loss=4.097086080908776
Loss made of: CE 0.2201293408870697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.190328121185303 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.960805830359459
Loss made of: CE 0.18067680299282074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4059648513793945 EntMin 0.0
Epoch 1, Class Loss=0.2821207642555237, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.2821207642555237, Class Loss=0.2821207642555237, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/52, Loss=4.00587998777628
Loss made of: CE 0.250368595123291, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.622950553894043 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.874869666993618
Loss made of: CE 0.21971046924591064, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.447993755340576 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.051956358551979
Loss made of: CE 0.3091233968734741, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.887061834335327 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.9712122276425363
Loss made of: CE 0.18041925132274628, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.342909574508667 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.9443052649497985
Loss made of: CE 0.22192782163619995, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291877746582031 EntMin 0.0
Epoch 2, Class Loss=0.2158127874135971, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.2158127874135971, Class Loss=0.2158127874135971, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/52, Loss=3.9115796014666557
Loss made of: CE 0.1580800712108612, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.779911994934082 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.855083632469177
Loss made of: CE 0.2041933536529541, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.541759967803955 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.9576862290501595
Loss made of: CE 0.22757181525230408, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.322380781173706 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.8873564898967743
Loss made of: CE 0.16351968050003052, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6185240745544434 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.890371583402157
Loss made of: CE 0.16775402426719666, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5253512859344482 EntMin 0.0
Epoch 3, Class Loss=0.1967839002609253, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.1967839002609253, Class Loss=0.1967839002609253, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/52, Loss=4.039014701545239
Loss made of: CE 0.2688671350479126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6254358291625977 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.689694081246853
Loss made of: CE 0.14864575862884521, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5696258544921875 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.7110024534165857
Loss made of: CE 0.12307151407003403, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2025442123413086 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.744137515127659
Loss made of: CE 0.1457499861717224, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2350759506225586 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.5520475283265114
Loss made of: CE 0.16690203547477722, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2010865211486816 EntMin 0.0
Epoch 4, Class Loss=0.17614422738552094, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.17614422738552094, Class Loss=0.17614422738552094, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/52, Loss=3.7117262482643127
Loss made of: CE 0.1520977020263672, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3512377738952637 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.68366924226284
Loss made of: CE 0.15069682896137238, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3270530700683594 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.7223535045981406
Loss made of: CE 0.19467508792877197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5204503536224365 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.6934508249163627
Loss made of: CE 0.17557677626609802, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8013579845428467 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.6336374685168265
Loss made of: CE 0.16156379878520966, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.34348726272583 EntMin 0.0
Epoch 5, Class Loss=0.17238613963127136, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.17238613963127136, Class Loss=0.17238613963127136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/52, Loss=3.583368328213692
Loss made of: CE 0.14699159562587738, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.29512095451355 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.684612084925175
Loss made of: CE 0.15256944298744202, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1347005367279053 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.862139266729355
Loss made of: CE 0.22765323519706726, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5198113918304443 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.4621698558330536
Loss made of: CE 0.1523997187614441, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3073832988739014 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.6439626701176167
Loss made of: CE 0.1859598010778427, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.600109577178955 EntMin 0.0
Epoch 6, Class Loss=0.16554680466651917, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.16554680466651917, Class Loss=0.16554680466651917, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/34, Loss=5.1916818708181385
Loss made of: CE 0.4405596852302551, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.771439552307129 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.650202828645706
Loss made of: CE 0.1810169667005539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035648345947266 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.898544466495514
Loss made of: CE 0.2623841464519501, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.482193946838379 EntMin 0.0
Epoch 1, Class Loss=0.29778462648391724, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.29778462648391724, Class Loss=0.29778462648391724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/34, Loss=4.646904802322387
Loss made of: CE 0.18734531104564667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.580930709838867 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.4318201914429665
Loss made of: CE 0.2310081422328949, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.141793727874756 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.3968716084957125
Loss made of: CE 0.1515655517578125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.371522903442383 EntMin 0.0
Epoch 2, Class Loss=0.19942940771579742, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.19942940771579742, Class Loss=0.19942940771579742, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/34, Loss=4.287839660048485
Loss made of: CE 0.211054727435112, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.009123802185059 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.356679984927178
Loss made of: CE 0.17482182383537292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1198506355285645 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.446392956376076
Loss made of: CE 0.20522813498973846, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.117276668548584 EntMin 0.0
Epoch 3, Class Loss=0.1872580349445343, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.1872580349445343, Class Loss=0.1872580349445343, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/34, Loss=4.357758098840714
Loss made of: CE 0.18523132801055908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.071642875671387 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.373860217630863
Loss made of: CE 0.18865738809108734, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.954958438873291 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.416223664581776
Loss made of: CE 0.2514931857585907, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.630490303039551 EntMin 0.0
Epoch 4, Class Loss=0.18084868788719177, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.18084868788719177, Class Loss=0.18084868788719177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/34, Loss=4.1893713369965555
Loss made of: CE 0.14949440956115723, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.680748701095581 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.333048811554908
Loss made of: CE 0.21371659636497498, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.308117866516113 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.076163391768932
Loss made of: CE 0.16046784818172455, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.972297191619873 EntMin 0.0
Epoch 5, Class Loss=0.1737269163131714, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.1737269163131714, Class Loss=0.1737269163131714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/34, Loss=4.21533240377903
Loss made of: CE 0.2147020697593689, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.118768215179443 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.163528919219971
Loss made of: CE 0.16534888744354248, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.101729393005371 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.224336516857147
Loss made of: CE 0.13536293804645538, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.632352352142334 EntMin 0.0
Epoch 6, Class Loss=0.16575171053409576, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.16575171053409576, Class Loss=0.16575171053409576, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=6.4125704109668735
Loss made of: CE 0.5103684663772583, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.127429008483887 EntMin 0.0
Epoch 1, Batch 20/33, Loss=5.472637650370598
Loss made of: CE 0.6264394521713257, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.135191440582275 EntMin 0.0
Epoch 1, Batch 30/33, Loss=5.022439616918564
Loss made of: CE 0.3265010118484497, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.431981563568115 EntMin 0.0
Epoch 1, Class Loss=0.549258291721344, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.549258291721344, Class Loss=0.549258291721344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/33, Loss=4.722044710814953
Loss made of: CE 0.306989461183548, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.231241226196289 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.566414174437523
Loss made of: CE 0.24075636267662048, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.213980674743652 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.324310848116875
Loss made of: CE 0.30489546060562134, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.222817897796631 EntMin 0.0
Epoch 2, Class Loss=0.26890724897384644, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.26890724897384644, Class Loss=0.26890724897384644, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/33, Loss=4.300539648532867
Loss made of: CE 0.2436833381652832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.580718040466309 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.2419918641448024
Loss made of: CE 0.1805761158466339, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0325188636779785 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.085886597633362
Loss made of: CE 0.20769202709197998, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7825379371643066 EntMin 0.0
Epoch 3, Class Loss=0.21943585574626923, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.21943585574626923, Class Loss=0.21943585574626923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/33, Loss=3.9923297673463822
Loss made of: CE 0.16846051812171936, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7173209190368652 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.073787100613117
Loss made of: CE 0.19240139424800873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.086636543273926 EntMin 0.0
Epoch 4, Batch 30/33, Loss=3.9807743549346926
Loss made of: CE 0.17821121215820312, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.573709011077881 EntMin 0.0
Epoch 4, Class Loss=0.18785375356674194, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.18785375356674194, Class Loss=0.18785375356674194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/33, Loss=4.0378025934100155
Loss made of: CE 0.17911414802074432, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4960360527038574 EntMin 0.0
Epoch 5, Batch 20/33, Loss=3.9197732642292977
Loss made of: CE 0.1869293451309204, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5020270347595215 EntMin 0.0
Epoch 5, Batch 30/33, Loss=3.8218331769108773
Loss made of: CE 0.19563496112823486, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.784590482711792 EntMin 0.0
Epoch 5, Class Loss=0.17744484543800354, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.17744484543800354, Class Loss=0.17744484543800354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/33, Loss=3.8882914587855337
Loss made of: CE 0.19951212406158447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.784575939178467 EntMin 0.0
Epoch 6, Batch 20/33, Loss=3.8390260189771652
Loss made of: CE 0.1280342936515808, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4703454971313477 EntMin 0.0
Epoch 6, Batch 30/33, Loss=3.714624971151352
Loss made of: CE 0.18236228823661804, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.853529691696167 EntMin 0.0
Epoch 6, Class Loss=0.17115198075771332, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.17115198075771332, Class Loss=0.17115198075771332, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/34, Loss=5.122804430127144
Loss made of: CE 0.4670514762401581, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.873770236968994 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.942709511518478
Loss made of: CE 0.2849287986755371, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.703150272369385 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.733880364894867
Loss made of: CE 0.195329949259758, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.474249839782715 EntMin 0.0
Epoch 1, Class Loss=0.29459697008132935, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.29459697008132935, Class Loss=0.29459697008132935, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/34, Loss=4.658248773217201
Loss made of: CE 0.19249841570854187, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.308093547821045 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.437174960970879
Loss made of: CE 0.21960994601249695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.588888168334961 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.763036972284317
Loss made of: CE 0.23933488130569458, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.162637233734131 EntMin 0.0
Epoch 2, Class Loss=0.21255646646022797, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.21255646646022797, Class Loss=0.21255646646022797, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/34, Loss=4.282463447749615
Loss made of: CE 0.1920563280582428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.033089637756348 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.406569404900074
Loss made of: CE 0.16661712527275085, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.081960678100586 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.505315214395523
Loss made of: CE 0.19663390517234802, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.042409896850586 EntMin 0.0
Epoch 3, Class Loss=0.18493065237998962, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.18493065237998962, Class Loss=0.18493065237998962, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/34, Loss=4.355860547721386
Loss made of: CE 0.16288889944553375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.014786720275879 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.423340472579002
Loss made of: CE 0.2005045861005783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.327643394470215 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.360567313432694
Loss made of: CE 0.18545660376548767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8107640743255615 EntMin 0.0
Epoch 4, Class Loss=0.17727108299732208, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.17727108299732208, Class Loss=0.17727108299732208, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/34, Loss=4.269690868258476
Loss made of: CE 0.15422648191452026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.087242603302002 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.368154481053352
Loss made of: CE 0.17812499403953552, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0931620597839355 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.288940306007862
Loss made of: CE 0.186322420835495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.416121482849121 EntMin 0.0
Epoch 5, Class Loss=0.16789470613002777, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.16789470613002777, Class Loss=0.16789470613002777, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/34, Loss=4.4962331131100655
Loss made of: CE 0.19231945276260376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9605770111083984 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.258117932081222
Loss made of: CE 0.12372952699661255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.120116233825684 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.176490981131792
Loss made of: CE 0.22112862765789032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.509142875671387 EntMin 0.0
Epoch 6, Class Loss=0.16723807156085968, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.16723807156085968, Class Loss=0.16723807156085968, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.30071568489074707, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.902800
Mean Acc: 0.632395
FreqW Acc: 0.833523
Mean IoU: 0.520733
Class IoU:
	class 0: 0.92058617
	class 1: 0.79397076
	class 2: 0.32330075
	class 3: 0.0020596094
	class 4: 0.6843544
	class 5: 0.0
	class 6: 0.79508805
	class 7: 0.60569674
	class 8: 0.5615427
Class Acc:
	class 0: 0.96551305
	class 1: 0.8166845
	class 2: 0.5712385
	class 3: 0.0020596972
	class 4: 0.7644314
	class 5: 0.0
	class 6: 0.9461269
	class 7: 0.7040518
	class 8: 0.92144585

federated global round: 8, step: 1
select part of clients to conduct local training
[4, 11, 9, 6]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=5.466891178488732
Loss made of: CE 0.44567662477493286, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.627948760986328 EntMin 0.0
Epoch 1, Batch 20/25, Loss=5.049995890259742
Loss made of: CE 0.3167960047721863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.343455791473389 EntMin 0.0
Epoch 1, Class Loss=0.39630892872810364, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.39630892872810364, Class Loss=0.39630892872810364, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/25, Loss=4.623455204069614
Loss made of: CE 0.26402604579925537, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9638776779174805 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.527562502026558
Loss made of: CE 0.2965308427810669, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478495121002197 EntMin 0.0
Epoch 2, Class Loss=0.2658020853996277, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.2658020853996277, Class Loss=0.2658020853996277, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/25, Loss=4.397182534635067
Loss made of: CE 0.22769367694854736, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.070056915283203 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.196683359146118
Loss made of: CE 0.25809982419013977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.031519889831543 EntMin 0.0
Epoch 3, Class Loss=0.23939964175224304, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.23939964175224304, Class Loss=0.23939964175224304, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/25, Loss=4.297239409387112
Loss made of: CE 0.18695327639579773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.158827781677246 EntMin 0.0
Epoch 4, Batch 20/25, Loss=4.0998722687363625
Loss made of: CE 0.1423584520816803, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035274982452393 EntMin 0.0
Epoch 4, Class Loss=0.1998043656349182, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.1998043656349182, Class Loss=0.1998043656349182, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/25, Loss=4.085190819203854
Loss made of: CE 0.13617216050624847, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.727620840072632 EntMin 0.0
Epoch 5, Batch 20/25, Loss=4.253652794659137
Loss made of: CE 0.35077422857284546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.691554069519043 EntMin 0.0
Epoch 5, Class Loss=0.19877685606479645, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.19877685606479645, Class Loss=0.19877685606479645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/25, Loss=3.9656545907258987
Loss made of: CE 0.15994960069656372, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9282612800598145 EntMin 0.0
Epoch 6, Batch 20/25, Loss=4.038070584833622
Loss made of: CE 0.21358653903007507, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2299957275390625 EntMin 0.0
Epoch 6, Class Loss=0.16725477576255798, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.16725477576255798, Class Loss=0.16725477576255798, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/43, Loss=4.94857012629509
Loss made of: CE 0.332878053188324, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.596213340759277 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.461058714985848
Loss made of: CE 0.4205598533153534, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.85492205619812 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.250180739164352
Loss made of: CE 0.23738405108451843, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.357907295227051 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.104718290269375
Loss made of: CE 0.27827978134155273, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5369107723236084 EntMin 0.0
Epoch 1, Class Loss=0.3272508680820465, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.3272508680820465, Class Loss=0.3272508680820465, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/43, Loss=3.972880019247532
Loss made of: CE 0.21466945111751556, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5633018016815186 EntMin 0.0
Epoch 2, Batch 20/43, Loss=3.8867569983005525
Loss made of: CE 0.22881756722927094, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.813765287399292 EntMin 0.0
Epoch 2, Batch 30/43, Loss=3.840173679590225
Loss made of: CE 0.3013504445552826, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.100978851318359 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.013163128495217
Loss made of: CE 0.2773330509662628, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8763844966888428 EntMin 0.0
Epoch 2, Class Loss=0.23206306993961334, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.23206306993961334, Class Loss=0.23206306993961334, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/43, Loss=3.6832661494612693
Loss made of: CE 0.26667773723602295, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2790417671203613 EntMin 0.0
Epoch 3, Batch 20/43, Loss=3.844029429554939
Loss made of: CE 0.2027013599872589, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.863156318664551 EntMin 0.0
Epoch 3, Batch 30/43, Loss=3.835324402153492
Loss made of: CE 0.2555707097053528, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.544034481048584 EntMin 0.0
Epoch 3, Batch 40/43, Loss=3.7456071361899377
Loss made of: CE 0.1299028843641281, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.637791156768799 EntMin 0.0
Epoch 3, Class Loss=0.21244102716445923, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.21244102716445923, Class Loss=0.21244102716445923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/43, Loss=3.647648490220308
Loss made of: CE 0.20583868026733398, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8616323471069336 EntMin 0.0
Epoch 4, Batch 20/43, Loss=3.675775969028473
Loss made of: CE 0.22374460101127625, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.273383855819702 EntMin 0.0
Epoch 4, Batch 30/43, Loss=3.6716284796595575
Loss made of: CE 0.23756882548332214, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.432586431503296 EntMin 0.0
Epoch 4, Batch 40/43, Loss=3.6425037927925588
Loss made of: CE 0.18289634585380554, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2181572914123535 EntMin 0.0
Epoch 4, Class Loss=0.1923632025718689, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.1923632025718689, Class Loss=0.1923632025718689, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/43, Loss=3.6226763755083082
Loss made of: CE 0.16079792380332947, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.766507863998413 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.6155050337314605
Loss made of: CE 0.2386435568332672, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.652609348297119 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.6701820969581602
Loss made of: CE 0.1784335821866989, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.502669095993042 EntMin 0.0
Epoch 5, Batch 40/43, Loss=3.677652435004711
Loss made of: CE 0.2085544317960739, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0501439571380615 EntMin 0.0
Epoch 5, Class Loss=0.18683987855911255, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.18683987855911255, Class Loss=0.18683987855911255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/43, Loss=3.5952322721481322
Loss made of: CE 0.22772334516048431, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.149264335632324 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.636487875878811
Loss made of: CE 0.15028290450572968, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5139336585998535 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.5799500793218613
Loss made of: CE 0.1503409743309021, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.174238681793213 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.398772504925728
Loss made of: CE 0.195043683052063, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.216203451156616 EntMin 0.0
Epoch 6, Class Loss=0.17756080627441406, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.17756080627441406, Class Loss=0.17756080627441406, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/52, Loss=3.7652085050940514
Loss made of: CE 0.30265647172927856, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.634277820587158 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.071088591217995
Loss made of: CE 0.18558815121650696, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7723026275634766 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.90000474601984
Loss made of: CE 0.14632855355739594, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7785701751708984 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.6833572000265122
Loss made of: CE 0.24653753638267517, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3870291709899902 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.7806547924876215
Loss made of: CE 0.1872740238904953, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9324862957000732 EntMin 0.0
Epoch 1, Class Loss=0.20436300337314606, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.20436300337314606, Class Loss=0.20436300337314606, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/52, Loss=3.752657559514046
Loss made of: CE 0.15388992428779602, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5666539669036865 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.8301367819309236
Loss made of: CE 0.20213565230369568, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.553128957748413 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.93445059210062
Loss made of: CE 0.14604906737804413, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3587942123413086 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.762263236939907
Loss made of: CE 0.15402033925056458, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.406081199645996 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.7958860456943513
Loss made of: CE 0.15726396441459656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.034154891967773 EntMin 0.0
Epoch 2, Class Loss=0.1746060699224472, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.1746060699224472, Class Loss=0.1746060699224472, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/52, Loss=3.754833909124136
Loss made of: CE 0.17866258323192596, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6848695278167725 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.7086115039885046
Loss made of: CE 0.22819626331329346, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.435678482055664 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.716780000925064
Loss made of: CE 0.1715414822101593, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3876590728759766 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.79048879891634
Loss made of: CE 0.13466906547546387, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6902894973754883 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.855655723810196
Loss made of: CE 0.16002699732780457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.668844223022461 EntMin 0.0
Epoch 3, Class Loss=0.16381904482841492, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.16381904482841492, Class Loss=0.16381904482841492, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/52, Loss=3.7483290039002894
Loss made of: CE 0.1432393491268158, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5070340633392334 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.6463366031646727
Loss made of: CE 0.15322577953338623, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4935054779052734 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.601076103746891
Loss made of: CE 0.16599266231060028, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.353698492050171 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.7269304946064947
Loss made of: CE 0.12443487346172333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5051097869873047 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.6076417095959186
Loss made of: CE 0.10746611654758453, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8267762660980225 EntMin 0.0
Epoch 4, Class Loss=0.1529521942138672, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.1529521942138672, Class Loss=0.1529521942138672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/52, Loss=3.377664542943239
Loss made of: CE 0.1581803858280182, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3291187286376953 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.6782827094197272
Loss made of: CE 0.1375400424003601, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5915088653564453 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.4371329858899116
Loss made of: CE 0.18260285258293152, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.488527774810791 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.7726026855409147
Loss made of: CE 0.1366223394870758, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.564075469970703 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.78421286419034
Loss made of: CE 0.17354127764701843, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.107341289520264 EntMin 0.0
Epoch 5, Class Loss=0.15157550573349, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.15157550573349, Class Loss=0.15157550573349, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/52, Loss=3.552199111878872
Loss made of: CE 0.08385172486305237, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.285341262817383 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.5712225466966627
Loss made of: CE 0.1292239874601364, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7904651165008545 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.5831522032618524
Loss made of: CE 0.12264664471149445, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.38144850730896 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.758539319038391
Loss made of: CE 0.16058842837810516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.583550453186035 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.6324913665652274
Loss made of: CE 0.13398979604244232, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.666327953338623 EntMin 0.0
Epoch 6, Class Loss=0.15057985484600067, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.15057985484600067, Class Loss=0.15057985484600067, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/34, Loss=4.68355391472578
Loss made of: CE 0.25443920493125916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.082324981689453 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.239062947034836
Loss made of: CE 0.1484704613685608, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5404181480407715 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.562314209342003
Loss made of: CE 0.18815700709819794, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9760243892669678 EntMin 0.0
Epoch 1, Class Loss=0.21669267117977142, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.21669267117977142, Class Loss=0.21669267117977142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/34, Loss=4.343737423419952
Loss made of: CE 0.16036748886108398, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.268001556396484 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.137807109951973
Loss made of: CE 0.18441510200500488, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9167439937591553 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.223227167129517
Loss made of: CE 0.1578400582075119, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.596948623657227 EntMin 0.0
Epoch 2, Class Loss=0.17245125770568848, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.17245125770568848, Class Loss=0.17245125770568848, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/34, Loss=4.0753112271428105
Loss made of: CE 0.17634102702140808, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6382901668548584 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.076146739721298
Loss made of: CE 0.14152944087982178, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7273261547088623 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.2106267139315605
Loss made of: CE 0.14955586194992065, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7131400108337402 EntMin 0.0
Epoch 3, Class Loss=0.16556641459465027, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.16556641459465027, Class Loss=0.16556641459465027, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/34, Loss=4.1003254935145375
Loss made of: CE 0.16639362275600433, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8561348915100098 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.115586751699448
Loss made of: CE 0.16117799282073975, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7486538887023926 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.188752751052379
Loss made of: CE 0.2030523270368576, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.496359825134277 EntMin 0.0
Epoch 4, Class Loss=0.16370870172977448, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.16370870172977448, Class Loss=0.16370870172977448, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/34, Loss=3.995357559621334
Loss made of: CE 0.12385980784893036, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.401228427886963 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.192231072485447
Loss made of: CE 0.2429715096950531, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.299919605255127 EntMin 0.0
Epoch 5, Batch 30/34, Loss=3.850767642259598
Loss made of: CE 0.15044152736663818, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7389283180236816 EntMin 0.0
Epoch 5, Class Loss=0.15789760649204254, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.15789760649204254, Class Loss=0.15789760649204254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/34, Loss=4.023209515213966
Loss made of: CE 0.19545213878154755, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.883866786956787 EntMin 0.0
Epoch 6, Batch 20/34, Loss=3.954605672508478
Loss made of: CE 0.18678812682628632, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.06708288192749 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.115087436139584
Loss made of: CE 0.1226855218410492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4653849601745605 EntMin 0.0
Epoch 6, Class Loss=0.1558385193347931, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.1558385193347931, Class Loss=0.1558385193347931, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2662130296230316, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.907846
Mean Acc: 0.649875
FreqW Acc: 0.842459
Mean IoU: 0.537660
Class IoU:
	class 0: 0.92523247
	class 1: 0.7915781
	class 2: 0.3304534
	class 3: 0.012242211
	class 4: 0.6852979
	class 5: 0.022365963
	class 6: 0.86064166
	class 7: 0.6421958
	class 8: 0.5689368
Class Acc:
	class 0: 0.96551174
	class 1: 0.8156021
	class 2: 0.5800865
	class 3: 0.012252011
	class 4: 0.75344974
	class 5: 0.022480551
	class 6: 0.94860727
	class 7: 0.83759326
	class 8: 0.91328853

federated global round: 9, step: 1
select part of clients to conduct local training
[5, 0, 9, 2]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/34, Loss=4.887869004905224
Loss made of: CE 0.2749437987804413, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.371908187866211 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.719518640637398
Loss made of: CE 0.2792508602142334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5517778396606445 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.594070081412792
Loss made of: CE 0.24015694856643677, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.350037574768066 EntMin 0.0
Epoch 1, Class Loss=0.25371867418289185, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.25371867418289185, Class Loss=0.25371867418289185, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/34, Loss=4.471177643537521
Loss made of: CE 0.19138285517692566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1123576164245605 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.0797686472535135
Loss made of: CE 0.17851126194000244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.250512599945068 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.536269503831863
Loss made of: CE 0.25704365968704224, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.048284530639648 EntMin 0.0
Epoch 2, Class Loss=0.18964248895645142, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.18964248895645142, Class Loss=0.18964248895645142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/34, Loss=4.0273807182908055
Loss made of: CE 0.1655990183353424, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8405978679656982 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.073274673521519
Loss made of: CE 0.17250892519950867, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.846722602844238 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.332141196727752
Loss made of: CE 0.2030879706144333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.871255397796631 EntMin 0.0
Epoch 3, Class Loss=0.159912571310997, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.159912571310997, Class Loss=0.159912571310997, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/34, Loss=4.157699643075466
Loss made of: CE 0.15107223391532898, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.817147731781006 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.225415843725204
Loss made of: CE 0.19109944999217987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303120136260986 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.110861144959927
Loss made of: CE 0.1717173159122467, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.423063278198242 EntMin 0.0
Epoch 4, Class Loss=0.16125158965587616, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.16125158965587616, Class Loss=0.16125158965587616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/34, Loss=4.015881554782391
Loss made of: CE 0.1346823275089264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.017778396606445 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.135807484388351
Loss made of: CE 0.18060551583766937, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8241467475891113 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.123644097149372
Loss made of: CE 0.1747431755065918, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.350481986999512 EntMin 0.0
Epoch 5, Class Loss=0.15180915594100952, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.15180915594100952, Class Loss=0.15180915594100952, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/34, Loss=4.273491760343314
Loss made of: CE 0.170425683259964, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5924558639526367 EntMin 0.0
Epoch 6, Batch 20/34, Loss=3.945894184708595
Loss made of: CE 0.12255913019180298, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.040489196777344 EntMin 0.0
Epoch 6, Batch 30/34, Loss=3.928860242664814
Loss made of: CE 0.18589411675930023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.407275199890137 EntMin 0.0
Epoch 6, Class Loss=0.15177835524082184, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.15177835524082184, Class Loss=0.15177835524082184, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/52, Loss=3.987298937141895
Loss made of: CE 0.29961246252059937, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.468369245529175 EntMin 0.0
Epoch 1, Batch 20/52, Loss=3.487880478799343
Loss made of: CE 0.18632715940475464, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.184077262878418 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.590026371181011
Loss made of: CE 0.1948273628950119, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.198254108428955 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.6784827664494513
Loss made of: CE 0.15686428546905518, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.781278610229492 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.6562381237745285
Loss made of: CE 0.11711424589157104, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1615500450134277 EntMin 0.0
Epoch 1, Class Loss=0.19474424421787262, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.19474424421787262, Class Loss=0.19474424421787262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/52, Loss=3.6664022877812386
Loss made of: CE 0.22323577105998993, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.400587558746338 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.5130280405282974
Loss made of: CE 0.155273899435997, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.13887882232666 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.78223410397768
Loss made of: CE 0.19106397032737732, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.772590160369873 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.614434026181698
Loss made of: CE 0.1608007401227951, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.116769313812256 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.605555368959904
Loss made of: CE 0.16151311993598938, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9838333129882812 EntMin 0.0
Epoch 2, Class Loss=0.16807468235492706, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.16807468235492706, Class Loss=0.16807468235492706, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/52, Loss=3.603700242936611
Loss made of: CE 0.14431241154670715, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5099411010742188 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.5757928043603897
Loss made of: CE 0.14902257919311523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3426263332366943 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.6516228556632995
Loss made of: CE 0.19055168330669403, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.160085678100586 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.6104206912219525
Loss made of: CE 0.1464998424053192, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3688151836395264 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.6432815730571746
Loss made of: CE 0.15116024017333984, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3048858642578125 EntMin 0.0
Epoch 3, Class Loss=0.16378861665725708, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.16378861665725708, Class Loss=0.16378861665725708, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/52, Loss=3.806072513759136
Loss made of: CE 0.210863396525383, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.316226005554199 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.445060148835182
Loss made of: CE 0.12878522276878357, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.327531337738037 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.506165413558483
Loss made of: CE 0.10365414619445801, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0344038009643555 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.552414555847645
Loss made of: CE 0.16671180725097656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.152672290802002 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.3700672544538977
Loss made of: CE 0.15168267488479614, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2531259059906006 EntMin 0.0
Epoch 4, Class Loss=0.15528158843517303, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.15528158843517303, Class Loss=0.15528158843517303, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/52, Loss=3.5535310320556164
Loss made of: CE 0.120189368724823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3366541862487793 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.4497433573007585
Loss made of: CE 0.12236045300960541, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.309847593307495 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.460818588733673
Loss made of: CE 0.17422011494636536, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.283064842224121 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.462162195146084
Loss made of: CE 0.15850697457790375, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6498284339904785 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.3823458805680273
Loss made of: CE 0.16052424907684326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0544471740722656 EntMin 0.0
Epoch 5, Class Loss=0.15152613818645477, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.15152613818645477, Class Loss=0.15152613818645477, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/52, Loss=3.3515029974281787
Loss made of: CE 0.13931354880332947, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0365371704101562 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.4783007867634295
Loss made of: CE 0.1380823850631714, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.155799627304077 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.57721663787961
Loss made of: CE 0.18491296470165253, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2219924926757812 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.2825497940182684
Loss made of: CE 0.16035352647304535, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1186025142669678 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.4310946211218836
Loss made of: CE 0.16481724381446838, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3317809104919434 EntMin 0.0
Epoch 6, Class Loss=0.1518906205892563, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.1518906205892563, Class Loss=0.1518906205892563, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/52, Loss=3.705391030013561
Loss made of: CE 0.2973879873752594, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4879536628723145 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.039686161279678
Loss made of: CE 0.1726074069738388, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7914276123046875 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.7845601469278334
Loss made of: CE 0.1423564851284027, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5825228691101074 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.534568537771702
Loss made of: CE 0.23138892650604248, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3023314476013184 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.6819036960601808
Loss made of: CE 0.1504260152578354, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8611152172088623 EntMin 0.0
Epoch 1, Class Loss=0.20313222706317902, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.20313222706317902, Class Loss=0.20313222706317902, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/52, Loss=3.6723188683390617
Loss made of: CE 0.14994481205940247, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.446744441986084 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.7246130257844925
Loss made of: CE 0.18405544757843018, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.29134464263916 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.798353397846222
Loss made of: CE 0.14373019337654114, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1618096828460693 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.589174085110426
Loss made of: CE 0.14246082305908203, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2553653717041016 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.597274290025234
Loss made of: CE 0.13758231699466705, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9993245601654053 EntMin 0.0
Epoch 2, Class Loss=0.1733885109424591, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.1733885109424591, Class Loss=0.1733885109424591, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/52, Loss=3.60332552716136
Loss made of: CE 0.16784200072288513, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.448620557785034 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.7130733862519265
Loss made of: CE 0.19502703845500946, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.596231460571289 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.5342661678791045
Loss made of: CE 0.15287575125694275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2153127193450928 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.679912405461073
Loss made of: CE 0.17528849840164185, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7273812294006348 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.756162793934345
Loss made of: CE 0.15830941498279572, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.627450704574585 EntMin 0.0
Epoch 3, Class Loss=0.1641891449689865, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.1641891449689865, Class Loss=0.1641891449689865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/52, Loss=3.666075752675533
Loss made of: CE 0.1402762532234192, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3657987117767334 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.5167932197451592
Loss made of: CE 0.16333413124084473, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2888107299804688 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.5769635424017907
Loss made of: CE 0.16855435073375702, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4444289207458496 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.7008281111717225
Loss made of: CE 0.13368986546993256, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4473540782928467 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.412645245343447
Loss made of: CE 0.12153615802526474, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.559762954711914 EntMin 0.0
Epoch 4, Class Loss=0.1614353209733963, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.1614353209733963, Class Loss=0.1614353209733963, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/52, Loss=3.364911910891533
Loss made of: CE 0.15408828854560852, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.291950225830078 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.6515957340598106
Loss made of: CE 0.13418862223625183, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.600315809249878 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.325528443604708
Loss made of: CE 0.185258150100708, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.197268486022949 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.7893080592155455
Loss made of: CE 0.16000036895275116, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6727118492126465 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.6788442716002465
Loss made of: CE 0.1729130744934082, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195526123046875 EntMin 0.0
Epoch 5, Class Loss=0.15708936750888824, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.15708936750888824, Class Loss=0.15708936750888824, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/52, Loss=3.484884078800678
Loss made of: CE 0.11322230100631714, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2667651176452637 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.4835795290768146
Loss made of: CE 0.11701878160238266, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7122249603271484 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.4385812029242517
Loss made of: CE 0.11426979303359985, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1068828105926514 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.6136897653341293
Loss made of: CE 0.15770143270492554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.489687919616699 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.492365271598101
Loss made of: CE 0.11850189417600632, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4166646003723145 EntMin 0.0
Epoch 6, Class Loss=0.15302644670009613, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.15302644670009613, Class Loss=0.15302644670009613, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=4.837025719881058
Loss made of: CE 0.32946550846099854, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.222012042999268 EntMin 0.0
Epoch 1, Batch 20/25, Loss=4.404074162244797
Loss made of: CE 0.36323556303977966, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.315008163452148 EntMin 0.0
Epoch 1, Class Loss=0.2911356985569, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.2911356985569, Class Loss=0.2911356985569, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/25, Loss=4.170563986897468
Loss made of: CE 0.2242436408996582, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6499249935150146 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.212665908038616
Loss made of: CE 0.12939277291297913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.002504348754883 EntMin 0.0
Epoch 2, Class Loss=0.21450874209403992, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.21450874209403992, Class Loss=0.21450874209403992, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/25, Loss=3.997449424862862
Loss made of: CE 0.1345347762107849, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.817230224609375 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.218271724879742
Loss made of: CE 0.16430482268333435, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.649801254272461 EntMin 0.0
Epoch 3, Class Loss=0.1893979161977768, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.1893979161977768, Class Loss=0.1893979161977768, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/25, Loss=4.011755263805389
Loss made of: CE 0.17242762446403503, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5313382148742676 EntMin 0.0
Epoch 4, Batch 20/25, Loss=3.981437557190657
Loss made of: CE 0.13057082891464233, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4239110946655273 EntMin 0.0
Epoch 4, Class Loss=0.17335990071296692, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.17335990071296692, Class Loss=0.17335990071296692, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/25, Loss=3.8848858416080474
Loss made of: CE 0.10864202678203583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5100317001342773 EntMin 0.0
Epoch 5, Batch 20/25, Loss=3.9241506800055506
Loss made of: CE 0.18889592587947845, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.545729398727417 EntMin 0.0
Epoch 5, Class Loss=0.16142970323562622, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.16142970323562622, Class Loss=0.16142970323562622, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/25, Loss=3.8470058262348177
Loss made of: CE 0.21883130073547363, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7053446769714355 EntMin 0.0
Epoch 6, Batch 20/25, Loss=3.8816190473735332
Loss made of: CE 0.1658419370651245, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7471463680267334 EntMin 0.0
Epoch 6, Class Loss=0.16621705889701843, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.16621705889701843, Class Loss=0.16621705889701843, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2724628150463104, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.907195
Mean Acc: 0.661963
FreqW Acc: 0.843104
Mean IoU: 0.541742
Class IoU:
	class 0: 0.92549133
	class 1: 0.78017694
	class 2: 0.33371183
	class 3: 0.001053177
	class 4: 0.69014835
	class 5: 0.0851908
	class 6: 0.8624451
	class 7: 0.6272912
	class 8: 0.57016724
Class Acc:
	class 0: 0.9597167
	class 1: 0.8002809
	class 2: 0.58092505
	class 3: 0.0010532426
	class 4: 0.7636568
	class 5: 0.08610242
	class 6: 0.9557906
	class 7: 0.8694931
	class 8: 0.94064635

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[0, 15, 1, 4]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=8.187963569164276
Loss made of: CE 1.0614222288131714, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.660182952880859 EntMin 0.0
Epoch 1, Class Loss=1.2039555311203003, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=1.2039555311203003, Class Loss=1.2039555311203003, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.149844586849213
Loss made of: CE 0.7877435684204102, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.243595123291016 EntMin 0.0
Epoch 2, Class Loss=0.8023603558540344, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.8023603558540344, Class Loss=0.8023603558540344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.673244720697403
Loss made of: CE 0.7955347895622253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.569317817687988 EntMin 0.0
Epoch 3, Class Loss=0.7291182279586792, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.7291182279586792, Class Loss=0.7291182279586792, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.461253833770752
Loss made of: CE 0.7416965961456299, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.299869537353516 EntMin 0.0
Epoch 4, Class Loss=0.6830130815505981, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.6830130815505981, Class Loss=0.6830130815505981, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.095018619298935
Loss made of: CE 0.683085560798645, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.549013137817383 EntMin 0.0
Epoch 5, Class Loss=0.6329009532928467, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.6329009532928467, Class Loss=0.6329009532928467, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.172999125719071
Loss made of: CE 0.5588637590408325, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421728134155273 EntMin 0.0
Epoch 6, Class Loss=0.5819932222366333, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.5819932222366333, Class Loss=0.5819932222366333, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=8.050446879863738
Loss made of: CE 1.0745501518249512, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.581681251525879 EntMin 0.0
Epoch 1, Class Loss=1.2058700323104858, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=1.2058700323104858, Class Loss=1.2058700323104858, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.378833681344986
Loss made of: CE 0.6575804948806763, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.754727363586426 EntMin 0.0
Epoch 2, Class Loss=0.8095892667770386, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.8095892667770386, Class Loss=0.8095892667770386, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.720600348711014
Loss made of: CE 0.9250038862228394, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8408002853393555 EntMin 0.0
Epoch 3, Class Loss=0.7438979744911194, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.7438979744911194, Class Loss=0.7438979744911194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.43723424077034
Loss made of: CE 0.7293208837509155, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.752788066864014 EntMin 0.0
Epoch 4, Class Loss=0.6867817640304565, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.6867817640304565, Class Loss=0.6867817640304565, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.183217388391495
Loss made of: CE 0.5584222674369812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.860454082489014 EntMin 0.0
Epoch 5, Class Loss=0.6443060040473938, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.6443060040473938, Class Loss=0.6443060040473938, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.125749593973159
Loss made of: CE 0.576164960861206, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.965989828109741 EntMin 0.0
Epoch 6, Class Loss=0.5940320491790771, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.5940320491790771, Class Loss=0.5940320491790771, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=9.57051774263382
Loss made of: CE 1.121158480644226, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.685431480407715 EntMin 0.0
Epoch 1, Batch 20/35, Loss=7.65575635433197
Loss made of: CE 0.748417854309082, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.214568138122559 EntMin 0.0
Epoch 1, Batch 30/35, Loss=7.101032602787018
Loss made of: CE 0.6939883232116699, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.613142013549805 EntMin 0.0
Epoch 1, Class Loss=1.0042712688446045, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=1.0042712688446045, Class Loss=1.0042712688446045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/35, Loss=6.434536492824554
Loss made of: CE 0.5588540434837341, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.86268424987793 EntMin 0.0
Epoch 2, Batch 20/35, Loss=6.462958332896233
Loss made of: CE 0.7123495936393738, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.263286113739014 EntMin 0.0
Epoch 2, Batch 30/35, Loss=6.083786743879318
Loss made of: CE 0.40082284808158875, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.905237197875977 EntMin 0.0
Epoch 2, Class Loss=0.5614752769470215, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.5614752769470215, Class Loss=0.5614752769470215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/35, Loss=5.764219728112221
Loss made of: CE 0.4260789752006531, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.142138481140137 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.966741901636124
Loss made of: CE 0.33373117446899414, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.568465232849121 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.710608270764351
Loss made of: CE 0.39848852157592773, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.673652172088623 EntMin 0.0
Epoch 3, Class Loss=0.44051605463027954, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.44051605463027954, Class Loss=0.44051605463027954, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/35, Loss=5.547434702515602
Loss made of: CE 0.3305402398109436, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.501906394958496 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.454235389828682
Loss made of: CE 0.31115907430648804, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.970447540283203 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.6308631807565686
Loss made of: CE 0.3714160919189453, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.185510635375977 EntMin 0.0
Epoch 4, Class Loss=0.36086785793304443, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.36086785793304443, Class Loss=0.36086785793304443, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/35, Loss=5.404955965280533
Loss made of: CE 0.27497923374176025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.922455787658691 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.520347160100937
Loss made of: CE 0.3208409547805786, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.304707050323486 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.994793684780598
Loss made of: CE 0.22082459926605225, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.492918014526367 EntMin 0.0
Epoch 5, Class Loss=0.30799105763435364, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.30799105763435364, Class Loss=0.30799105763435364, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/35, Loss=5.092485792934895
Loss made of: CE 0.20396125316619873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.814800262451172 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.1905608251690865
Loss made of: CE 0.26802682876586914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.854728698730469 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.398649145662785
Loss made of: CE 0.3535453677177429, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.653682231903076 EntMin 0.0
Epoch 6, Class Loss=0.278457909822464, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.278457909822464, Class Loss=0.278457909822464, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=8.09228574037552
Loss made of: CE 1.3153045177459717, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.283910274505615 EntMin 0.0
Epoch 1, Class Loss=1.1900835037231445, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=1.1900835037231445, Class Loss=1.1900835037231445, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.317665207386017
Loss made of: CE 0.7008285522460938, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.215816497802734 EntMin 0.0
Epoch 2, Class Loss=0.7968313097953796, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.7968313097953796, Class Loss=0.7968313097953796, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.82702197432518
Loss made of: CE 0.6466668248176575, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427022933959961 EntMin 0.0
Epoch 3, Class Loss=0.7447901368141174, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.7447901368141174, Class Loss=0.7447901368141174, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.40560530424118
Loss made of: CE 0.6869564056396484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.417387008666992 EntMin 0.0
Epoch 4, Class Loss=0.6804917454719543, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.6804917454719543, Class Loss=0.6804917454719543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.276984316110611
Loss made of: CE 0.708197832107544, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.485321998596191 EntMin 0.0
Epoch 5, Class Loss=0.6405695676803589, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.6405695676803589, Class Loss=0.6405695676803589, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.150895249843598
Loss made of: CE 0.5295742750167847, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.509411334991455 EntMin 0.0
Epoch 6, Class Loss=0.5866563320159912, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5866563320159912, Class Loss=0.5866563320159912, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6085777878761292, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.837372
Mean Acc: 0.336483
FreqW Acc: 0.717205
Mean IoU: 0.270491
Class IoU:
	class 0: 0.86019963
	class 1: 0.18349326
	class 2: 0.142954
	class 3: 0.00018146756
	class 4: 0.3333911
	class 5: 0.14560544
	class 6: 0.8019514
	class 7: 0.5826439
	class 8: 0.43026677
	class 9: 0.0
	class 10: 0.03569852
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.98641974
	class 1: 0.1835876
	class 2: 0.16803335
	class 3: 0.00018175312
	class 4: 0.34275728
	class 5: 0.14691144
	class 6: 0.92843395
	class 7: 0.73240006
	class 8: 0.849859
	class 9: 0.0
	class 10: 0.035699204
	class 11: 0.0
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[1, 13, 11, 12]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/35, Loss=7.408173161745071
Loss made of: CE 0.7541542053222656, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.694274425506592 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.363644516468048
Loss made of: CE 0.5804791450500488, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.140402793884277 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.162088656425476
Loss made of: CE 0.6064039468765259, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6836042404174805 EntMin 0.0
Epoch 1, Class Loss=0.6917368769645691, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.6917368769645691, Class Loss=0.6917368769645691, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/35, Loss=5.793218642473221
Loss made of: CE 0.4351009726524353, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.102880477905273 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.872718295454979
Loss made of: CE 0.5412565469741821, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.816234111785889 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.588001018762588
Loss made of: CE 0.3120228052139282, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.377188682556152 EntMin 0.0
Epoch 2, Class Loss=0.4229463040828705, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.4229463040828705, Class Loss=0.4229463040828705, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/35, Loss=5.281364104151725
Loss made of: CE 0.33456286787986755, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.977804183959961 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.5470274031162266
Loss made of: CE 0.28370001912117004, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.370273590087891 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.3507483035326
Loss made of: CE 0.31420108675956726, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.325629711151123 EntMin 0.0
Epoch 3, Class Loss=0.3224450945854187, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3224450945854187, Class Loss=0.3224450945854187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/35, Loss=5.299952499568462
Loss made of: CE 0.2711879014968872, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.06843376159668 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.081498758494854
Loss made of: CE 0.2517474293708801, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7701945304870605 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.274771815538406
Loss made of: CE 0.2698550820350647, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7928032875061035 EntMin 0.0
Epoch 4, Class Loss=0.28227391839027405, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.28227391839027405, Class Loss=0.28227391839027405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/35, Loss=5.198187567293644
Loss made of: CE 0.23189041018486023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.676336288452148 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.261607591807842
Loss made of: CE 0.26926666498184204, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.95921516418457 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.767940783500672
Loss made of: CE 0.19528284668922424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.459777355194092 EntMin 0.0
Epoch 5, Class Loss=0.2524392306804657, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.2524392306804657, Class Loss=0.2524392306804657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/35, Loss=4.842309533059597
Loss made of: CE 0.18470637500286102, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.770903587341309 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.0340538889169695
Loss made of: CE 0.20530448853969574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.305444240570068 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.193246264755726
Loss made of: CE 0.34172961115837097, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.351507663726807 EntMin 0.0
Epoch 6, Class Loss=0.2416430115699768, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.2416430115699768, Class Loss=0.2416430115699768, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.981569761037827
Loss made of: CE 0.5578514337539673, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.503068923950195 EntMin 0.0
Epoch 1, Class Loss=0.5953215956687927, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5953215956687927, Class Loss=0.5953215956687927, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/19, Loss=5.03650022149086
Loss made of: CE 0.5277698040008545, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.911150932312012 EntMin 0.0
Epoch 2, Class Loss=0.5534825921058655, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.5534825921058655, Class Loss=0.5534825921058655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/19, Loss=4.849303126335144
Loss made of: CE 0.4400263726711273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.414746284484863 EntMin 0.0
Epoch 3, Class Loss=0.48858141899108887, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.48858141899108887, Class Loss=0.48858141899108887, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/19, Loss=4.855228042602539
Loss made of: CE 0.5339968800544739, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.201787948608398 EntMin 0.0
Epoch 4, Class Loss=0.45916080474853516, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.45916080474853516, Class Loss=0.45916080474853516, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/19, Loss=4.82625567317009
Loss made of: CE 0.36143335700035095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.653449535369873 EntMin 0.0
Epoch 5, Class Loss=0.42775827646255493, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.42775827646255493, Class Loss=0.42775827646255493, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/19, Loss=4.6374013453722
Loss made of: CE 0.4241696000099182, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.290778160095215 EntMin 0.0
Epoch 6, Class Loss=0.41516536474227905, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.41516536474227905, Class Loss=0.41516536474227905, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=5.306494599580764
Loss made of: CE 0.6427524089813232, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.898528099060059 EntMin 0.0
Epoch 1, Batch 20/33, Loss=5.191661989688873
Loss made of: CE 0.5947777032852173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.552297115325928 EntMin 0.0
Epoch 1, Batch 30/33, Loss=5.167746949195862
Loss made of: CE 0.507344126701355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.441037178039551 EntMin 0.0
Epoch 1, Class Loss=0.6060812473297119, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.6060812473297119, Class Loss=0.6060812473297119, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=5.284453421831131
Loss made of: CE 0.6231259107589722, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.34979248046875 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.854458200931549
Loss made of: CE 0.4937194585800171, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8509929180145264 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.850917643308639
Loss made of: CE 0.5258180499076843, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.524290084838867 EntMin 0.0
Epoch 2, Class Loss=0.5283580422401428, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.5283580422401428, Class Loss=0.5283580422401428, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.788086003065109
Loss made of: CE 0.4118722379207611, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.462128162384033 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.9039504587650296
Loss made of: CE 0.540697455406189, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.36018705368042 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.772972452640533
Loss made of: CE 0.5612632036209106, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.472607612609863 EntMin 0.0
Epoch 3, Class Loss=0.4755937159061432, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4755937159061432, Class Loss=0.4755937159061432, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.713440635800362
Loss made of: CE 0.41782569885253906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0093584060668945 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.811032369732857
Loss made of: CE 0.39256078004837036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.294813632965088 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.682787624001503
Loss made of: CE 0.4537620544433594, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.765547275543213 EntMin 0.0
Epoch 4, Class Loss=0.4423612952232361, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4423612952232361, Class Loss=0.4423612952232361, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.574152505397796
Loss made of: CE 0.37065011262893677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8768155574798584 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.862895575165749
Loss made of: CE 0.3589165210723877, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.982281446456909 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.542197996377945
Loss made of: CE 0.47662919759750366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0072855949401855 EntMin 0.0
Epoch 5, Class Loss=0.4191367030143738, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.4191367030143738, Class Loss=0.4191367030143738, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.424812740087509
Loss made of: CE 0.4134700894355774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.000414848327637 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.4598695009946825
Loss made of: CE 0.3651975095272064, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.465352773666382 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.456295076012611
Loss made of: CE 0.3226205110549927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.402094841003418 EntMin 0.0
Epoch 6, Class Loss=0.40190425515174866, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.40190425515174866, Class Loss=0.40190425515174866, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=5.344032821059227
Loss made of: CE 0.6385691165924072, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.247574329376221 EntMin 0.0
Epoch 1, Batch 20/33, Loss=5.0930066227912905
Loss made of: CE 0.700356125831604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5661540031433105 EntMin 0.0
Epoch 1, Batch 30/33, Loss=4.994315710663796
Loss made of: CE 0.4569736421108246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.470458030700684 EntMin 0.0
Epoch 1, Class Loss=0.5992230176925659, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.5992230176925659, Class Loss=0.5992230176925659, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=4.897622895240784
Loss made of: CE 0.36948221921920776, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.531609535217285 EntMin 0.0
Epoch 2, Batch 20/33, Loss=5.0261922746896746
Loss made of: CE 0.5489851236343384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.547991752624512 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.8726415008306505
Loss made of: CE 0.5197979807853699, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.97886323928833 EntMin 0.0
Epoch 2, Class Loss=0.5260184407234192, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.5260184407234192, Class Loss=0.5260184407234192, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.766955217719078
Loss made of: CE 0.5759561061859131, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9155993461608887 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.874314150214195
Loss made of: CE 0.47313618659973145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.300748825073242 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.649076214432716
Loss made of: CE 0.5236149430274963, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.893345594406128 EntMin 0.0
Epoch 3, Class Loss=0.4727911651134491, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.4727911651134491, Class Loss=0.4727911651134491, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.680895981192589
Loss made of: CE 0.38018369674682617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.255710601806641 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.587222239375114
Loss made of: CE 0.43413639068603516, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.977954864501953 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.527711009979248
Loss made of: CE 0.48705825209617615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.937036037445068 EntMin 0.0
Epoch 4, Class Loss=0.4376232624053955, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.4376232624053955, Class Loss=0.4376232624053955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.6073618799448015
Loss made of: CE 0.37574315071105957, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.682058811187744 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.402963706851006
Loss made of: CE 0.5838803052902222, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.626950740814209 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.38989147245884
Loss made of: CE 0.47744521498680115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.003015518188477 EntMin 0.0
Epoch 5, Class Loss=0.4122627079486847, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.4122627079486847, Class Loss=0.4122627079486847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.402856647968292
Loss made of: CE 0.44564732909202576, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.996108055114746 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.561712405085563
Loss made of: CE 0.3657377362251282, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.649073600769043 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.61399844288826
Loss made of: CE 0.36563849449157715, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5077805519104004 EntMin 0.0
Epoch 6, Class Loss=0.39886027574539185, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.39886027574539185, Class Loss=0.39886027574539185, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5037776231765747, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.852720
Mean Acc: 0.415272
FreqW Acc: 0.744581
Mean IoU: 0.332537
Class IoU:
	class 0: 0.8780269
	class 1: 0.18751201
	class 2: 0.22869055
	class 3: 0.0018330251
	class 4: 0.45253885
	class 5: 0.1918473
	class 6: 0.7853955
	class 7: 0.57379603
	class 8: 0.4707401
	class 9: 0.00011443707
	class 10: 0.5501809
	class 11: 0.0022992115
	class 12: 0.0
Class Acc:
	class 0: 0.98445445
	class 1: 0.18760611
	class 2: 0.3130297
	class 3: 0.0018391256
	class 4: 0.47036257
	class 5: 0.19486563
	class 6: 0.842081
	class 7: 0.68346626
	class 8: 0.88753504
	class 9: 0.00011443707
	class 10: 0.83087975
	class 11: 0.0022992957
	class 12: 0.0

federated global round: 12, step: 2
select part of clients to conduct local training
[0, 16, 7, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.792825418710708
Loss made of: CE 0.5323061347007751, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.745083808898926 EntMin 0.0
Epoch 1, Class Loss=0.47069159150123596, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.47069159150123596, Class Loss=0.47069159150123596, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.523090153932571
Loss made of: CE 0.4119454324245453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.122331142425537 EntMin 0.0
Epoch 2, Class Loss=0.42207562923431396, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.42207562923431396, Class Loss=0.42207562923431396, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.548243895173073
Loss made of: CE 0.4199983477592468, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8287532329559326 EntMin 0.0
Epoch 3, Class Loss=0.3960285484790802, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3960285484790802, Class Loss=0.3960285484790802, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.4915973335504535
Loss made of: CE 0.43893617391586304, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.689856052398682 EntMin 0.0
Epoch 4, Class Loss=0.3733198046684265, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3733198046684265, Class Loss=0.3733198046684265, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.361312067508697
Loss made of: CE 0.3906794786453247, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.984586477279663 EntMin 0.0
Epoch 5, Class Loss=0.35238271951675415, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.35238271951675415, Class Loss=0.35238271951675415, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.37878525853157
Loss made of: CE 0.3593832850456238, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8256630897521973 EntMin 0.0
Epoch 6, Class Loss=0.3317306339740753, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.3317306339740753, Class Loss=0.3317306339740753, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/42, Loss=5.671825712919235
Loss made of: CE 0.6075719594955444, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.204165458679199 EntMin 0.0
Epoch 1, Batch 20/42, Loss=5.288056728243828
Loss made of: CE 0.4673040807247162, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.983461380004883 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.375370585918427
Loss made of: CE 0.4828227162361145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.700292110443115 EntMin 0.0
Epoch 1, Batch 40/42, Loss=4.914208966493606
Loss made of: CE 0.30133718252182007, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.023153781890869 EntMin 0.0
Epoch 1, Class Loss=0.533395528793335, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.533395528793335, Class Loss=0.533395528793335, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/42, Loss=5.092932707071304
Loss made of: CE 0.2726215124130249, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.565640926361084 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.935911080241203
Loss made of: CE 0.25973981618881226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.955666542053223 EntMin 0.0
Epoch 2, Batch 30/42, Loss=4.841420990228653
Loss made of: CE 0.2354399859905243, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698428153991699 EntMin 0.0
Epoch 2, Batch 40/42, Loss=4.927537140250206
Loss made of: CE 0.37538987398147583, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.280268669128418 EntMin 0.0
Epoch 2, Class Loss=0.3378552198410034, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.3378552198410034, Class Loss=0.3378552198410034, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/42, Loss=4.718230699002743
Loss made of: CE 0.3006975054740906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.145755767822266 EntMin 0.0
Epoch 3, Batch 20/42, Loss=4.983942312002182
Loss made of: CE 0.19396254420280457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.925569534301758 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.669309893250466
Loss made of: CE 0.18232613801956177, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645346641540527 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.610101330280304
Loss made of: CE 0.17763297259807587, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.620903968811035 EntMin 0.0
Epoch 3, Class Loss=0.2899130880832672, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.2899130880832672, Class Loss=0.2899130880832672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/42, Loss=4.797226765751839
Loss made of: CE 0.2739899158477783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.293331146240234 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.625343741476536
Loss made of: CE 0.3073486089706421, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2834296226501465 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.639563113451004
Loss made of: CE 0.21682602167129517, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.995242118835449 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.663223288953304
Loss made of: CE 0.25440770387649536, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.302677631378174 EntMin 0.0
Epoch 4, Class Loss=0.27248743176460266, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.27248743176460266, Class Loss=0.27248743176460266, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/42, Loss=4.452114325761795
Loss made of: CE 0.254487007856369, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.039273262023926 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.616515092551708
Loss made of: CE 0.2707599997520447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.231932163238525 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.640954028069973
Loss made of: CE 0.34647178649902344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.595255374908447 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.577587257325649
Loss made of: CE 0.2416372299194336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.081988334655762 EntMin 0.0
Epoch 5, Class Loss=0.2524872422218323, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.2524872422218323, Class Loss=0.2524872422218323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/42, Loss=4.636763598024845
Loss made of: CE 0.2268548309803009, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.81931734085083 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.416794887185096
Loss made of: CE 0.2672950029373169, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.145408630371094 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.590396173298359
Loss made of: CE 0.2970711886882782, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.407235145568848 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.717115764319897
Loss made of: CE 0.253059446811676, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.774227142333984 EntMin 0.0
Epoch 6, Class Loss=0.24714922904968262, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.24714922904968262, Class Loss=0.24714922904968262, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/31, Loss=5.590254184603691
Loss made of: CE 0.5771840810775757, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.873178482055664 EntMin 0.0
Epoch 1, Batch 20/31, Loss=5.365129768848419
Loss made of: CE 0.4407808184623718, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9526214599609375 EntMin 0.0
Epoch 1, Batch 30/31, Loss=5.185047382116318
Loss made of: CE 0.46072256565093994, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.484235763549805 EntMin 0.0
Epoch 1, Class Loss=0.5036965012550354, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.5036965012550354, Class Loss=0.5036965012550354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/31, Loss=4.945503962039948
Loss made of: CE 0.45210230350494385, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.350566864013672 EntMin 0.0
Epoch 2, Batch 20/31, Loss=4.733620953559876
Loss made of: CE 0.4656742513179779, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.114624977111816 EntMin 0.0
Epoch 2, Batch 30/31, Loss=5.149519473314285
Loss made of: CE 0.35816261172294617, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.907578945159912 EntMin 0.0
Epoch 2, Class Loss=0.44519710540771484, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.44519710540771484, Class Loss=0.44519710540771484, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/31, Loss=4.8860966056585315
Loss made of: CE 0.3946060538291931, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.366742134094238 EntMin 0.0
Epoch 3, Batch 20/31, Loss=4.596061167120934
Loss made of: CE 0.48022016882896423, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.929839849472046 EntMin 0.0
Epoch 3, Batch 30/31, Loss=4.712442454695702
Loss made of: CE 0.2713352143764496, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.674569606781006 EntMin 0.0
Epoch 3, Class Loss=0.3997366726398468, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3997366726398468, Class Loss=0.3997366726398468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/31, Loss=4.541707560420036
Loss made of: CE 0.5005476474761963, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.652665138244629 EntMin 0.0
Epoch 4, Batch 20/31, Loss=4.611874189972878
Loss made of: CE 0.3987812399864197, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.045088768005371 EntMin 0.0
Epoch 4, Batch 30/31, Loss=4.6820953488349915
Loss made of: CE 0.5051356554031372, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.565024375915527 EntMin 0.0
Epoch 4, Class Loss=0.38159409165382385, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.38159409165382385, Class Loss=0.38159409165382385, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/31, Loss=4.442874395847321
Loss made of: CE 0.3374330997467041, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1016716957092285 EntMin 0.0
Epoch 5, Batch 20/31, Loss=4.512438115477562
Loss made of: CE 0.32331931591033936, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.831770658493042 EntMin 0.0
Epoch 5, Batch 30/31, Loss=4.736832977831364
Loss made of: CE 0.26778632402420044, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9745664596557617 EntMin 0.0
Epoch 5, Class Loss=0.3584911525249481, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.3584911525249481, Class Loss=0.3584911525249481, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/31, Loss=4.436978191137314
Loss made of: CE 0.4418596029281616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9986486434936523 EntMin 0.0
Epoch 6, Batch 20/31, Loss=4.613500171899796
Loss made of: CE 0.32619208097457886, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.33517599105835 EntMin 0.0
Epoch 6, Batch 30/31, Loss=4.396381977200508
Loss made of: CE 0.393798828125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.714827299118042 EntMin 0.0
Epoch 6, Class Loss=0.35210421681404114, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.35210421681404114, Class Loss=0.35210421681404114, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.716169232130051
Loss made of: CE 0.36965712904930115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.356716156005859 EntMin 0.0
Epoch 1, Class Loss=0.4780605733394623, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.4780605733394623, Class Loss=0.4780605733394623, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.713530698418618
Loss made of: CE 0.4093519449234009, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.280003070831299 EntMin 0.0
Epoch 2, Class Loss=0.43016353249549866, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.43016353249549866, Class Loss=0.43016353249549866, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.572754916548729
Loss made of: CE 0.31630802154541016, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.597285747528076 EntMin 0.0
Epoch 3, Class Loss=0.3969559967517853, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.3969559967517853, Class Loss=0.3969559967517853, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.406384259462357
Loss made of: CE 0.3274146318435669, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.691135883331299 EntMin 0.0
Epoch 4, Class Loss=0.36423835158348083, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.36423835158348083, Class Loss=0.36423835158348083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.39288977086544
Loss made of: CE 0.3552699685096741, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.977238178253174 EntMin 0.0
Epoch 5, Class Loss=0.3515351712703705, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.3515351712703705, Class Loss=0.3515351712703705, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.4099814057350155
Loss made of: CE 0.3138637840747833, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9083504676818848 EntMin 0.0
Epoch 6, Class Loss=0.33781012892723083, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.33781012892723083, Class Loss=0.33781012892723083, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4794471859931946, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.851103
Mean Acc: 0.453632
FreqW Acc: 0.754355
Mean IoU: 0.354736
Class IoU:
	class 0: 0.88591874
	class 1: 0.19876806
	class 2: 0.27671692
	class 3: 0.0031858257
	class 4: 0.5015829
	class 5: 0.11039254
	class 6: 0.7509747
	class 7: 0.5656979
	class 8: 0.4477588
	class 9: 0.003571908
	class 10: 0.57938576
	class 11: 0.25756142
	class 12: 0.030055331
Class Acc:
	class 0: 0.9722417
	class 1: 0.19885716
	class 2: 0.4269116
	class 3: 0.0031982893
	class 4: 0.5241102
	class 5: 0.11099309
	class 6: 0.796917
	class 7: 0.69919
	class 8: 0.9177476
	class 9: 0.0035755157
	class 10: 0.6856532
	class 11: 0.5277282
	class 12: 0.030092621

federated global round: 13, step: 2
select part of clients to conduct local training
[14, 13, 0, 1]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.483635208010673
Loss made of: CE 0.36240220069885254, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8916213512420654 EntMin 0.0
Epoch 1, Class Loss=0.36917322874069214, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.36917322874069214, Class Loss=0.36917322874069214, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=4.38424023091793
Loss made of: CE 0.3769710958003998, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.721975803375244 EntMin 0.0
Epoch 2, Class Loss=0.33520862460136414, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.33520862460136414, Class Loss=0.33520862460136414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=4.43866203725338
Loss made of: CE 0.27274343371391296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.507843017578125 EntMin 0.0
Epoch 3, Class Loss=0.3123202919960022, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.3123202919960022, Class Loss=0.3123202919960022, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=4.2910304620862005
Loss made of: CE 0.2852250039577484, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.860372543334961 EntMin 0.0
Epoch 4, Class Loss=0.30316901206970215, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.30316901206970215, Class Loss=0.30316901206970215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=4.209864191710949
Loss made of: CE 0.24597756564617157, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.583773374557495 EntMin 0.0
Epoch 5, Class Loss=0.29391488432884216, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.29391488432884216, Class Loss=0.29391488432884216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=4.213545003533364
Loss made of: CE 0.261938214302063, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.733865261077881 EntMin 0.0
Epoch 6, Class Loss=0.28661975264549255, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.28661975264549255, Class Loss=0.28661975264549255, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/19, Loss=4.3320179879665375
Loss made of: CE 0.3389069736003876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.943479061126709 EntMin 0.0
Epoch 1, Class Loss=0.36435365676879883, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.36435365676879883, Class Loss=0.36435365676879883, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/19, Loss=4.3407774597406386
Loss made of: CE 0.37582796812057495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358724594116211 EntMin 0.0
Epoch 2, Class Loss=0.3335120379924774, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.3335120379924774, Class Loss=0.3335120379924774, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/19, Loss=4.319831462204457
Loss made of: CE 0.2651861906051636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.043212413787842 EntMin 0.0
Epoch 3, Class Loss=0.3171986937522888, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.3171986937522888, Class Loss=0.3171986937522888, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/19, Loss=4.3729332208633425
Loss made of: CE 0.3449386954307556, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9055087566375732 EntMin 0.0
Epoch 4, Class Loss=0.3066636025905609, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.3066636025905609, Class Loss=0.3066636025905609, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/19, Loss=4.418923641741276
Loss made of: CE 0.22881649434566498, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.357053279876709 EntMin 0.0
Epoch 5, Class Loss=0.2962426543235779, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.2962426543235779, Class Loss=0.2962426543235779, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/19, Loss=4.142203418910503
Loss made of: CE 0.2905188798904419, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.75276255607605 EntMin 0.0
Epoch 6, Class Loss=0.2952038049697876, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.2952038049697876, Class Loss=0.2952038049697876, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.360089272260666
Loss made of: CE 0.3473680913448334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.31939697265625 EntMin 0.0
Epoch 1, Class Loss=0.36568084359169006, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.36568084359169006, Class Loss=0.36568084359169006, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/19, Loss=4.203127041459084
Loss made of: CE 0.40298527479171753, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.976410388946533 EntMin 0.0
Epoch 2, Class Loss=0.34458670020103455, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.34458670020103455, Class Loss=0.34458670020103455, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/19, Loss=4.262326002120972
Loss made of: CE 0.3898468017578125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.681950569152832 EntMin 0.0
Epoch 3, Class Loss=0.3352520763874054, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3352520763874054, Class Loss=0.3352520763874054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/19, Loss=4.306631538271904
Loss made of: CE 0.4322372376918793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.644373416900635 EntMin 0.0
Epoch 4, Class Loss=0.3254736661911011, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3254736661911011, Class Loss=0.3254736661911011, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/19, Loss=4.16359560340643
Loss made of: CE 0.33032846450805664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.923102855682373 EntMin 0.0
Epoch 5, Class Loss=0.30808186531066895, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.30808186531066895, Class Loss=0.30808186531066895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/19, Loss=4.269494953751564
Loss made of: CE 0.3558586537837982, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.738359212875366 EntMin 0.0
Epoch 6, Class Loss=0.3092058002948761, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.3092058002948761, Class Loss=0.3092058002948761, Reg Loss=0.0
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/35, Loss=7.369552445411682
Loss made of: CE 0.542259931564331, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.100159645080566 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.2657349616289135
Loss made of: CE 0.42209118604660034, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3915205001831055 EntMin 0.0
Epoch 1, Batch 30/35, Loss=5.994677281379699
Loss made of: CE 0.33341237902641296, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.656102180480957 EntMin 0.0
Epoch 1, Class Loss=0.4978974461555481, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.4978974461555481, Class Loss=0.4978974461555481, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000584
Epoch 2, Batch 10/35, Loss=5.646414476633072
Loss made of: CE 0.28178054094314575, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.303238868713379 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.721008729934693
Loss made of: CE 0.3392680883407593, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.715879440307617 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.410565234720707
Loss made of: CE 0.23114018142223358, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.510462760925293 EntMin 0.0
Epoch 2, Class Loss=0.3070836067199707, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.3070836067199707, Class Loss=0.3070836067199707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/35, Loss=5.052351109683514
Loss made of: CE 0.23582035303115845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.817809581756592 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.403860281407833
Loss made of: CE 0.23397782444953918, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.251084804534912 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.189101293683052
Loss made of: CE 0.2558163106441498, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.123990058898926 EntMin 0.0
Epoch 3, Class Loss=0.2501150071620941, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.2501150071620941, Class Loss=0.2501150071620941, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000487
Epoch 4, Batch 10/35, Loss=5.138781817257405
Loss made of: CE 0.1920250654220581, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.067765235900879 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.003266279399395
Loss made of: CE 0.20437881350517273, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.662230491638184 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.137757903337478
Loss made of: CE 0.22582706809043884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9704694747924805 EntMin 0.0
Epoch 4, Class Loss=0.2284703552722931, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.2284703552722931, Class Loss=0.2284703552722931, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000438
Epoch 5, Batch 10/35, Loss=5.166806904971599
Loss made of: CE 0.2111673355102539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.534777641296387 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.146998804807663
Loss made of: CE 0.2275751829147339, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.037335395812988 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.711001987755298
Loss made of: CE 0.17726120352745056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.273434638977051 EntMin 0.0
Epoch 5, Class Loss=0.21496854722499847, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.21496854722499847, Class Loss=0.21496854722499847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000389
Epoch 6, Batch 10/35, Loss=4.8045300677418705
Loss made of: CE 0.15480148792266846, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.485898494720459 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.996100881695748
Loss made of: CE 0.16110864281654358, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.317425727844238 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.065201161801815
Loss made of: CE 0.24313323199748993, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.392847061157227 EntMin 0.0
Epoch 6, Class Loss=0.2071734070777893, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.2071734070777893, Class Loss=0.2071734070777893, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4700097143650055, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.853130
Mean Acc: 0.476329
FreqW Acc: 0.761304
Mean IoU: 0.357857
Class IoU:
	class 0: 0.88917804
	class 1: 0.19130783
	class 2: 0.27852032
	class 3: 0.006706688
	class 4: 0.5234927
	class 5: 0.081909046
	class 6: 0.7282184
	class 7: 0.5528855
	class 8: 0.5153247
	class 9: 0.0049100956
	class 10: 0.43891254
	class 11: 0.2461208
	class 12: 0.19465986
Class Acc:
	class 0: 0.96577096
	class 1: 0.19138424
	class 2: 0.43916973
	class 3: 0.0067423466
	class 4: 0.54843247
	class 5: 0.08205394
	class 6: 0.7682792
	class 7: 0.6713915
	class 8: 0.84413505
	class 9: 0.0049182223
	class 10: 0.9149729
	class 11: 0.55227435
	class 12: 0.20275532

federated global round: 14, step: 2
select part of clients to conduct local training
[16, 9, 4, 0]
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/42, Loss=5.261993116140365
Loss made of: CE 0.5048882961273193, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.269132137298584 EntMin 0.0
Epoch 1, Batch 20/42, Loss=4.993724238872528
Loss made of: CE 0.3147220015525818, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581637859344482 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.136560213565827
Loss made of: CE 0.3603019714355469, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590888023376465 EntMin 0.0
Epoch 1, Batch 40/42, Loss=4.67647855579853
Loss made of: CE 0.23899075388908386, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.88826847076416 EntMin 0.0
Epoch 1, Class Loss=0.3987131118774414, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.3987131118774414, Class Loss=0.3987131118774414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/42, Loss=4.804124681651592
Loss made of: CE 0.2279832810163498, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.271328926086426 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.597506386041641
Loss made of: CE 0.24662643671035767, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.868171691894531 EntMin 0.0
Epoch 2, Batch 30/42, Loss=4.524642698466778
Loss made of: CE 0.20798182487487793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.449308395385742 EntMin 0.0
Epoch 2, Batch 40/42, Loss=4.662786141037941
Loss made of: CE 0.2979987561702728, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.854050397872925 EntMin 0.0
Epoch 2, Class Loss=0.27944791316986084, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.27944791316986084, Class Loss=0.27944791316986084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/42, Loss=4.513571500778198
Loss made of: CE 0.2602475583553314, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.940551519393921 EntMin 0.0
Epoch 3, Batch 20/42, Loss=4.801569674909115
Loss made of: CE 0.1975264549255371, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.873775959014893 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.445231147110462
Loss made of: CE 0.1849144995212555, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4926652908325195 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.412381504476071
Loss made of: CE 0.19151026010513306, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7127790451049805 EntMin 0.0
Epoch 3, Class Loss=0.24981756508350372, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.24981756508350372, Class Loss=0.24981756508350372, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/42, Loss=4.540620192885399
Loss made of: CE 0.23857645690441132, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9597973823547363 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.359027095139027
Loss made of: CE 0.24171742796897888, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.131196022033691 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.341221942007541
Loss made of: CE 0.189253568649292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.550715208053589 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.477950255572796
Loss made of: CE 0.23860272765159607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.532160758972168 EntMin 0.0
Epoch 4, Class Loss=0.24179133772850037, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.24179133772850037, Class Loss=0.24179133772850037, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/42, Loss=4.193576715886593
Loss made of: CE 0.24079236388206482, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.850710391998291 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.505703353881836
Loss made of: CE 0.2750380039215088, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.947941780090332 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.540832382440567
Loss made of: CE 0.3210691809654236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.689020156860352 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.379988303780555
Loss made of: CE 0.25549930334091187, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.987060070037842 EntMin 0.0
Epoch 5, Class Loss=0.23819828033447266, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.23819828033447266, Class Loss=0.23819828033447266, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/42, Loss=4.4785937204957005
Loss made of: CE 0.1916031539440155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7040717601776123 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.222183361649513
Loss made of: CE 0.2633607089519501, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.854078769683838 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.4660650715231895
Loss made of: CE 0.3246878981590271, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.099367141723633 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.556618681550026
Loss made of: CE 0.2802959084510803, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.620800971984863 EntMin 0.0
Epoch 6, Class Loss=0.24242986738681793, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.24242986738681793, Class Loss=0.24242986738681793, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=6.539862233400345
Loss made of: CE 0.6000745296478271, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.378970623016357 EntMin 0.0
Epoch 1, Batch 20/35, Loss=5.954564613103867
Loss made of: CE 0.3248586654663086, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.182246208190918 EntMin 0.0
Epoch 1, Batch 30/35, Loss=5.5262190192937855
Loss made of: CE 0.28915584087371826, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.288444519042969 EntMin 0.0
Epoch 1, Class Loss=0.38948795199394226, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.38948795199394226, Class Loss=0.38948795199394226, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/35, Loss=5.58605924397707
Loss made of: CE 0.3238154947757721, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.652647018432617 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.338429237902164
Loss made of: CE 0.2188129872083664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.786779403686523 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.189440067112446
Loss made of: CE 0.2032059133052826, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.70687198638916 EntMin 0.0
Epoch 2, Class Loss=0.2399541288614273, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.2399541288614273, Class Loss=0.2399541288614273, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/35, Loss=5.251280272006989
Loss made of: CE 0.16805057227611542, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.851051330566406 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.188340362906456
Loss made of: CE 0.18785670399665833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.702971458435059 EntMin 0.0
Epoch 3, Batch 30/35, Loss=4.997943107783795
Loss made of: CE 0.19255486130714417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3895416259765625 EntMin 0.0
Epoch 3, Class Loss=0.2131957858800888, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.2131957858800888, Class Loss=0.2131957858800888, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/35, Loss=5.114850609004497
Loss made of: CE 0.1661052405834198, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.110360145568848 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.105303934216499
Loss made of: CE 0.20853810012340546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.264352798461914 EntMin 0.0
Epoch 4, Batch 30/35, Loss=4.98735450655222
Loss made of: CE 0.19965645670890808, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.310775279998779 EntMin 0.0
Epoch 4, Class Loss=0.20145851373672485, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.20145851373672485, Class Loss=0.20145851373672485, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/35, Loss=4.674804800748825
Loss made of: CE 0.15822818875312805, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8031110763549805 EntMin 0.0
Epoch 5, Batch 20/35, Loss=4.990334583818912
Loss made of: CE 0.16388583183288574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3770623207092285 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.758983358740807
Loss made of: CE 0.23498046398162842, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.400052547454834 EntMin 0.0
Epoch 5, Class Loss=0.19470295310020447, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.19470295310020447, Class Loss=0.19470295310020447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/35, Loss=4.949954921007157
Loss made of: CE 0.19612400233745575, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.026029586791992 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.5995771661400795
Loss made of: CE 0.25338631868362427, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.714491367340088 EntMin 0.0
Epoch 6, Batch 30/35, Loss=4.779454116523266
Loss made of: CE 0.14556363224983215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6081223487854 EntMin 0.0
Epoch 6, Class Loss=0.18532782793045044, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.18532782793045044, Class Loss=0.18532782793045044, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.213998085260391
Loss made of: CE 0.27367326617240906, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.835740089416504 EntMin 0.0
Epoch 1, Class Loss=0.3262856900691986, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.3262856900691986, Class Loss=0.3262856900691986, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/19, Loss=4.370880860090256
Loss made of: CE 0.28955546021461487, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.914710521697998 EntMin 0.0
Epoch 2, Class Loss=0.3069438338279724, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.3069438338279724, Class Loss=0.3069438338279724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/19, Loss=4.225252121686935
Loss made of: CE 0.24756529927253723, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2908120155334473 EntMin 0.0
Epoch 3, Class Loss=0.30099719762802124, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.30099719762802124, Class Loss=0.30099719762802124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/19, Loss=4.106506222486496
Loss made of: CE 0.26063621044158936, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.460040807723999 EntMin 0.0
Epoch 4, Class Loss=0.29151269793510437, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.29151269793510437, Class Loss=0.29151269793510437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/19, Loss=4.080992808938026
Loss made of: CE 0.3000248372554779, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5772719383239746 EntMin 0.0
Epoch 5, Class Loss=0.2894411087036133, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.2894411087036133, Class Loss=0.2894411087036133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/19, Loss=4.153640356659889
Loss made of: CE 0.26723888516426086, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7434966564178467 EntMin 0.0
Epoch 6, Class Loss=0.28519052267074585, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.28519052267074585, Class Loss=0.28519052267074585, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000304
Epoch 1, Batch 10/19, Loss=4.297374731302261
Loss made of: CE 0.3129720389842987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.35987663269043 EntMin 0.0
Epoch 1, Class Loss=0.3229188919067383, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.3229188919067383, Class Loss=0.3229188919067383, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000258
Epoch 2, Batch 10/19, Loss=4.1117936879396435
Loss made of: CE 0.3629812002182007, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8787155151367188 EntMin 0.0
Epoch 2, Class Loss=0.315960168838501, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.315960168838501, Class Loss=0.315960168838501, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000211
Epoch 3, Batch 10/19, Loss=4.151798638701439
Loss made of: CE 0.33968862891197205, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4593305587768555 EntMin 0.0
Epoch 3, Class Loss=0.3082475960254669, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3082475960254669, Class Loss=0.3082475960254669, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000163
Epoch 4, Batch 10/19, Loss=4.176662670075894
Loss made of: CE 0.3975091874599457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421073913574219 EntMin 0.0
Epoch 4, Class Loss=0.3034195601940155, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3034195601940155, Class Loss=0.3034195601940155, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000113
Epoch 5, Batch 10/19, Loss=4.064166724681854
Loss made of: CE 0.31218618154525757, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.750190258026123 EntMin 0.0
Epoch 5, Class Loss=0.30081552267074585, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.30081552267074585, Class Loss=0.30081552267074585, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000061
Epoch 6, Batch 10/19, Loss=4.221100457012653
Loss made of: CE 0.3225432336330414, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5622029304504395 EntMin 0.0
Epoch 6, Class Loss=0.3053286671638489, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.3053286671638489, Class Loss=0.3053286671638489, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4586127698421478, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.853566
Mean Acc: 0.484376
FreqW Acc: 0.768313
Mean IoU: 0.382656
Class IoU:
	class 0: 0.8914367
	class 1: 0.18750395
	class 2: 0.27799267
	class 3: 0.004050205
	class 4: 0.54831153
	class 5: 0.09007455
	class 6: 0.7387077
	class 7: 0.5503842
	class 8: 0.30511183
	class 9: 0.0072794775
	class 10: 0.72698075
	class 11: 0.26974404
	class 12: 0.3769504
Class Acc:
	class 0: 0.9601367
	class 1: 0.18758509
	class 2: 0.4343634
	class 3: 0.004061552
	class 4: 0.5802406
	class 5: 0.09028179
	class 6: 0.7849805
	class 7: 0.6858505
	class 8: 0.33601665
	class 9: 0.007305007
	class 10: 0.77626705
	class 11: 0.5146926
	class 12: 0.9351089

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[11, 6, 10, 7]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=7.432723248004914
Loss made of: CE 0.9723595380783081, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0310282707214355 EntMin 0.0
Epoch 1, Batch 20/102, Loss=6.170460188388825
Loss made of: CE 0.7707098722457886, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.123178958892822 EntMin 0.0
Epoch 1, Batch 30/102, Loss=6.223644030094147
Loss made of: CE 0.5725375413894653, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.709486961364746 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.989429068565369
Loss made of: CE 0.47192806005477905, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.287458419799805 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.794344446063041
Loss made of: CE 0.4300830066204071, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.526200771331787 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.69765265583992
Loss made of: CE 0.46654826402664185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.230840682983398 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.366697052121163
Loss made of: CE 0.5169284343719482, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.647241592407227 EntMin 0.0
Epoch 1, Batch 80/102, Loss=5.220805126428604
Loss made of: CE 0.4760424494743347, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.912774085998535 EntMin 0.0
Epoch 1, Batch 90/102, Loss=5.151978942751884
Loss made of: CE 0.38059037923812866, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.29927396774292 EntMin 0.0
Epoch 1, Batch 100/102, Loss=5.17921544611454
Loss made of: CE 0.5192031860351562, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.672289848327637 EntMin 0.0
Epoch 1, Class Loss=0.5665123462677002, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.5665123462677002, Class Loss=0.5665123462677002, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=4.942005747556687
Loss made of: CE 0.375032901763916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084556579589844 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.904569634795189
Loss made of: CE 0.3552352786064148, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.480752944946289 EntMin 0.0
Epoch 2, Batch 30/102, Loss=5.059606716036797
Loss made of: CE 0.41604432463645935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758611679077148 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.996313932538032
Loss made of: CE 0.35058292746543884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.072888374328613 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.916434243321419
Loss made of: CE 0.30728229880332947, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.314077377319336 EntMin 0.0
Epoch 2, Batch 60/102, Loss=5.0908106684684755
Loss made of: CE 0.3606913089752197, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.194903373718262 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.548400770127773
Loss made of: CE 0.27847492694854736, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.860532760620117 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.771992266178131
Loss made of: CE 0.22409513592720032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.473563194274902 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.966398152709007
Loss made of: CE 0.5094775557518005, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.690395355224609 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.937257952988148
Loss made of: CE 0.28692400455474854, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.999757766723633 EntMin 0.0
Epoch 2, Class Loss=0.34786301851272583, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.34786301851272583, Class Loss=0.34786301851272583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=4.779309010505676
Loss made of: CE 0.2964836359024048, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0415568351745605 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.763993367552757
Loss made of: CE 0.3939271569252014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.049375057220459 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.820180104672909
Loss made of: CE 0.2306199073791504, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.07008171081543 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.8129991218447685
Loss made of: CE 0.21005885303020477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.316298007965088 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.812025621533394
Loss made of: CE 0.26639533042907715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.177452087402344 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.605811977386475
Loss made of: CE 0.25241461396217346, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2858357429504395 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.53112316429615
Loss made of: CE 0.22396335005760193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.449049472808838 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 80/102, Loss=4.728494675457478
Loss made of: CE 0.22975830733776093, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.678457736968994 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.786070546507835
Loss made of: CE 0.3128187656402588, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8811004161834717 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.550094306468964
Loss made of: CE 0.19036555290222168, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6586883068084717 EntMin 0.0
Epoch 3, Class Loss=0.27717676758766174, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.27717676758766174, Class Loss=0.27717676758766174, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.428823557496071
Loss made of: CE 0.2669945955276489, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.178462505340576 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.688953413069248
Loss made of: CE 0.22018294036388397, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.97489595413208 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.501433536410332
Loss made of: CE 0.3067213296890259, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.155967712402344 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.560569728910923
Loss made of: CE 0.23419803380966187, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.792203903198242 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.686121094226837
Loss made of: CE 0.18638692796230316, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363193511962891 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.7487383171916004
Loss made of: CE 0.29754287004470825, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.534262657165527 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.594287028908729
Loss made of: CE 0.2579466700553894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.35176944732666 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.691114668548107
Loss made of: CE 0.30721229314804077, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.744142532348633 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.621799467504024
Loss made of: CE 0.22278742492198944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2363433837890625 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.53105404227972
Loss made of: CE 0.24160465598106384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807347297668457 EntMin 0.0
Epoch 4, Class Loss=0.23953382670879364, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.23953382670879364, Class Loss=0.23953382670879364, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=4.789897303283214
Loss made of: CE 0.16426020860671997, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.187932014465332 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.648853042721749
Loss made of: CE 0.15273967385292053, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.102303981781006 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.626360012590885
Loss made of: CE 0.22521589696407318, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.189719200134277 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.284510600566864
Loss made of: CE 0.2706221342086792, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9765758514404297 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.620015567541122
Loss made of: CE 0.2179197072982788, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.021295547485352 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.116912737488747
Loss made of: CE 0.23965458571910858, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.016633987426758 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.410567684471607
Loss made of: CE 0.20669341087341309, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.655219078063965 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.237584754824638
Loss made of: CE 0.19695237278938293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.612908363342285 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.544831384718418
Loss made of: CE 0.1535523533821106, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6916418075561523 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.477278584241867
Loss made of: CE 0.1936538815498352, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.605998992919922 EntMin 0.0
Epoch 5, Class Loss=0.20919568836688995, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.20919568836688995, Class Loss=0.20919568836688995, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.533889806270599
Loss made of: CE 0.2334832400083542, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.294581413269043 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.23985407948494
Loss made of: CE 0.15598341822624207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.059983253479004 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.100923438370228
Loss made of: CE 0.17760178446769714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.02895450592041 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.6340584173798565
Loss made of: CE 0.28958427906036377, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.994656562805176 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.18340772986412
Loss made of: CE 0.20340678095817566, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9218201637268066 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.3230054274201395
Loss made of: CE 0.2130267322063446, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.005781173706055 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.176153503358364
Loss made of: CE 0.14712145924568176, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1893463134765625 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.337940374016762
Loss made of: CE 0.1662922203540802, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.770451545715332 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.282472231984139
Loss made of: CE 0.15923112630844116, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.806358814239502 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.412439520657062
Loss made of: CE 0.16832193732261658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.77878475189209 EntMin 0.0
Epoch 6, Class Loss=0.19556297361850739, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.19556297361850739, Class Loss=0.19556297361850739, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=7.278215461969376
Loss made of: CE 0.979263186454773, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.577816009521484 EntMin 0.0
Epoch 1, Batch 20/23, Loss=6.155549484491348
Loss made of: CE 0.6196703314781189, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.069319725036621 EntMin 0.0
Epoch 1, Class Loss=0.8187918066978455, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.8187918066978455, Class Loss=0.8187918066978455, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=5.6537212312221525
Loss made of: CE 0.6516579389572144, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9697747230529785 EntMin 0.0
Epoch 2, Batch 20/23, Loss=5.231444501876831
Loss made of: CE 0.49882638454437256, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.565397262573242 EntMin 0.0
Epoch 2, Class Loss=0.6029694676399231, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6029694676399231, Class Loss=0.6029694676399231, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=5.0661465466022495
Loss made of: CE 0.48648473620414734, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.521701812744141 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.987049424648285
Loss made of: CE 0.6665263175964355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.211475372314453 EntMin 0.0
Epoch 3, Class Loss=0.5315885543823242, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.5315885543823242, Class Loss=0.5315885543823242, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.599935409426689
Loss made of: CE 0.5577710866928101, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.399707317352295 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.674353623390198
Loss made of: CE 0.40408968925476074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6350860595703125 EntMin 0.0
Epoch 4, Class Loss=0.4885771572589874, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4885771572589874, Class Loss=0.4885771572589874, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=4.551843053102493
Loss made of: CE 0.4839836359024048, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.127931594848633 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.549718728661537
Loss made of: CE 0.4797542989253998, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.493865966796875 EntMin 0.0
Epoch 5, Class Loss=0.45176962018013, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.45176962018013, Class Loss=0.45176962018013, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.429213079810142
Loss made of: CE 0.3767346739768982, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.205671310424805 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.46095250248909
Loss made of: CE 0.38384366035461426, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5074257850646973 EntMin 0.0
Epoch 6, Class Loss=0.40538883209228516, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.40538883209228516, Class Loss=0.40538883209228516, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=7.479927402734757
Loss made of: CE 0.8230327367782593, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9845404624938965 EntMin 0.0
Epoch 1, Batch 20/23, Loss=6.267299908399582
Loss made of: CE 0.8203878402709961, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.465945720672607 EntMin 0.0
Epoch 1, Class Loss=0.8435841798782349, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.8435841798782349, Class Loss=0.8435841798782349, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=5.517202425003052
Loss made of: CE 0.504607617855072, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.479418754577637 EntMin 0.0
Epoch 2, Batch 20/23, Loss=5.150359588861465
Loss made of: CE 0.5590390563011169, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.755427837371826 EntMin 0.0
Epoch 2, Class Loss=0.6076419949531555, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.6076419949531555, Class Loss=0.6076419949531555, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=4.909906113147736
Loss made of: CE 0.450642466545105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.793893337249756 EntMin 0.0
Epoch 3, Batch 20/23, Loss=5.029357016086578
Loss made of: CE 0.5025656223297119, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.648318290710449 EntMin 0.0
Epoch 3, Class Loss=0.5542533993721008, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5542533993721008, Class Loss=0.5542533993721008, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.6913968324661255
Loss made of: CE 0.5009293556213379, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.168623924255371 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.70084602534771
Loss made of: CE 0.5043222308158875, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9364352226257324 EntMin 0.0
Epoch 4, Class Loss=0.49217578768730164, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.49217578768730164, Class Loss=0.49217578768730164, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=4.783918035030365
Loss made of: CE 0.4173300266265869, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.915947914123535 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.517980918288231
Loss made of: CE 0.4787457585334778, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.853591442108154 EntMin 0.0
Epoch 5, Class Loss=0.4441947042942047, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.4441947042942047, Class Loss=0.4441947042942047, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.4464153200387955
Loss made of: CE 0.3749711513519287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.057722091674805 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 20/23, Loss=4.4656204879283905
Loss made of: CE 0.46104106307029724, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.205478668212891 EntMin 0.0
Epoch 6, Class Loss=0.4175415635108948, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.4175415635108948, Class Loss=0.4175415635108948, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=7.542142111063003
Loss made of: CE 0.7955588102340698, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5330400466918945 EntMin 0.0
Epoch 1, Batch 20/102, Loss=6.834435683488846
Loss made of: CE 0.613717257976532, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.270999908447266 EntMin 0.0
Epoch 1, Batch 30/102, Loss=6.169751942157745
Loss made of: CE 0.5848492980003357, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.855117321014404 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.752586004137993
Loss made of: CE 0.4566572606563568, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.696120738983154 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.772546133399009
Loss made of: CE 0.6110021471977234, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.693826675415039 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.637252303957939
Loss made of: CE 0.5529474020004272, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594365119934082 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.561106473207474
Loss made of: CE 0.4839678406715393, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.544830799102783 EntMin 0.0
Epoch 1, Batch 80/102, Loss=5.294607102870941
Loss made of: CE 0.44084736704826355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.240262985229492 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.712285867333412
Loss made of: CE 0.36473843455314636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.16161584854126 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.811668360233307
Loss made of: CE 0.4952227771282196, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358574867248535 EntMin 0.0
Epoch 1, Class Loss=0.574069082736969, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.574069082736969, Class Loss=0.574069082736969, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=5.3297772198915485
Loss made of: CE 0.3533051013946533, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.199813365936279 EntMin 0.0
Epoch 2, Batch 20/102, Loss=5.093067634105682
Loss made of: CE 0.3943331837654114, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.184317588806152 EntMin 0.0
Epoch 2, Batch 30/102, Loss=5.1690690368413925
Loss made of: CE 0.3432505130767822, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.987639904022217 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.999405807256698
Loss made of: CE 0.2966378629207611, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.959786415100098 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.6553422778844835
Loss made of: CE 0.26240894198417664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.994208812713623 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.929346451163292
Loss made of: CE 0.37814438343048096, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.670021057128906 EntMin 0.0
Epoch 2, Batch 70/102, Loss=5.2798118308186535
Loss made of: CE 0.3678823411464691, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.262938976287842 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.722455106675625
Loss made of: CE 0.2614893317222595, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8212385177612305 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.670423093438148
Loss made of: CE 0.335041880607605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.725096225738525 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.991626530885696
Loss made of: CE 0.26658761501312256, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.020329475402832 EntMin 0.0
Epoch 2, Class Loss=0.345669150352478, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.345669150352478, Class Loss=0.345669150352478, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=5.123982800543308
Loss made of: CE 0.32578545808792114, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.565042495727539 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.8980199754238125
Loss made of: CE 0.3132467269897461, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6454172134399414 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.667392963171006
Loss made of: CE 0.32078108191490173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.727197170257568 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.702063097059726
Loss made of: CE 0.19941620528697968, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.420754432678223 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.728387881815434
Loss made of: CE 0.2681390643119812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.862957000732422 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.550256034731865
Loss made of: CE 0.23212778568267822, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.00439453125 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.79644746184349
Loss made of: CE 0.211221382021904, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9166483879089355 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.398663865029812
Loss made of: CE 0.18025276064872742, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9588840007781982 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.518717586994171
Loss made of: CE 0.24843156337738037, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.973463773727417 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.50099990516901
Loss made of: CE 0.21792197227478027, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.141000270843506 EntMin 0.0
Epoch 3, Class Loss=0.27514663338661194, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.27514663338661194, Class Loss=0.27514663338661194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.86373790204525
Loss made of: CE 0.24618837237358093, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.35532808303833 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.524057585000992
Loss made of: CE 0.26077765226364136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3415374755859375 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.5007598713040355
Loss made of: CE 0.18308696150779724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7544212341308594 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.4479641139507295
Loss made of: CE 0.23658069968223572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0847344398498535 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.542912817001342
Loss made of: CE 0.28788506984710693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.269583225250244 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.636231723427772
Loss made of: CE 0.23768606781959534, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.479824066162109 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.383328539133072
Loss made of: CE 0.2454848289489746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.04714298248291 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.514933024346829
Loss made of: CE 0.2301330417394638, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1791911125183105 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.654543726146221
Loss made of: CE 0.24297837913036346, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.702767372131348 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.79339379221201
Loss made of: CE 0.17907147109508514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.118642807006836 EntMin 0.0
Epoch 4, Class Loss=0.23750168085098267, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.23750168085098267, Class Loss=0.23750168085098267, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/102, Loss=4.262918245792389
Loss made of: CE 0.2208995521068573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.886037349700928 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.735646593570709
Loss made of: CE 0.20710444450378418, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.466311931610107 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.650011368095875
Loss made of: CE 0.24339210987091064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1629109382629395 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.470595651865006
Loss made of: CE 0.16642418503761292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8475122451782227 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.259259632229805
Loss made of: CE 0.22375671565532684, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.116402626037598 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.399879139661789
Loss made of: CE 0.19190344214439392, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.495755672454834 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.517385709285736
Loss made of: CE 0.16274891793727875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.512116432189941 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.539183609187603
Loss made of: CE 0.16699063777923584, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.060309410095215 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.3809714883565904
Loss made of: CE 0.29327693581581116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.088033199310303 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.229447628557682
Loss made of: CE 0.24386247992515564, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.759310245513916 EntMin 0.0
Epoch 5, Class Loss=0.2075505554676056, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.2075505554676056, Class Loss=0.2075505554676056, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.2797494068741795
Loss made of: CE 0.21643070876598358, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5472638607025146 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.725969465076924
Loss made of: CE 0.16839584708213806, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.670238494873047 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.946138732135296
Loss made of: CE 0.16773459315299988, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5684802532196045 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.337667945027351
Loss made of: CE 0.21133670210838318, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7310032844543457 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.429152505099774
Loss made of: CE 0.2075577676296234, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.045229911804199 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.530304498970509
Loss made of: CE 0.15083307027816772, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9102628231048584 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.34843399822712
Loss made of: CE 0.1501406878232956, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.009397506713867 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.215087978541851
Loss made of: CE 0.156595841050148, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.177289009094238 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.385147447884083
Loss made of: CE 0.16191627085208893, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.250691890716553 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.366227670013904
Loss made of: CE 0.2681528925895691, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9197216033935547 EntMin 0.0
Epoch 6, Class Loss=0.19322647154331207, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.19322647154331207, Class Loss=0.19322647154331207, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.703177273273468, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.801416
Mean Acc: 0.374049
FreqW Acc: 0.674317
Mean IoU: 0.278165
Class IoU:
	class 0: 0.8321581
	class 1: 0.03299837
	class 2: 0.106795035
	class 3: 0.0
	class 4: 0.29535103
	class 5: 0.016965969
	class 6: 0.7625033
	class 7: 0.6146217
	class 8: 0.13054457
	class 9: 0.0031064125
	class 10: 0.47024617
	class 11: 0.26675484
	class 12: 0.33841127
	class 13: 0.0
	class 14: 0.5273585
	class 15: 0.33098617
	class 16: 0.0
Class Acc:
	class 0: 0.96923655
	class 1: 0.03299862
	class 2: 0.12699151
	class 3: 0.0
	class 4: 0.30230215
	class 5: 0.01697631
	class 6: 0.8337929
	class 7: 0.6478926
	class 8: 0.13085845
	class 9: 0.0031106954
	class 10: 0.73186904
	class 11: 0.5490817
	class 12: 0.82490534
	class 13: 0.0
	class 14: 0.8371516
	class 15: 0.35166833
	class 16: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[7, 11, 16, 18]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=4.734834387898445
Loss made of: CE 0.32894372940063477, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7334020137786865 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.780426850914955
Loss made of: CE 0.28474533557891846, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.017248153686523 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.56266703158617
Loss made of: CE 0.2122846096754074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.359163761138916 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.580280934274197
Loss made of: CE 0.2275562286376953, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.101611137390137 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.80019820779562
Loss made of: CE 0.3637891709804535, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.763896942138672 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 60/102, Loss=4.730498634278774
Loss made of: CE 0.300270140171051, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.762524366378784 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.707485561072827
Loss made of: CE 0.27254194021224976, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.38520622253418 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.603564658761025
Loss made of: CE 0.24317112565040588, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8798828125 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.115466746687889
Loss made of: CE 0.21314296126365662, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7190170288085938 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.080265198647976
Loss made of: CE 0.2710651457309723, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.973461151123047 EntMin 0.0
Epoch 1, Class Loss=0.26697802543640137, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.26697802543640137, Class Loss=0.26697802543640137, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.609577091038227
Loss made of: CE 0.18086159229278564, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6524455547332764 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.258058574795723
Loss made of: CE 0.2325039952993393, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.20693302154541 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.431942065060139
Loss made of: CE 0.18829920887947083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.979478359222412 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.376418401300907
Loss made of: CE 0.2217426598072052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427986145019531 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.094779953360558
Loss made of: CE 0.18611089885234833, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6828064918518066 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.458907660841942
Loss made of: CE 0.2698414921760559, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3582000732421875 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.602372850477695
Loss made of: CE 0.24440699815750122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.454993724822998 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.292321242392063
Loss made of: CE 0.18907728791236877, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6892428398132324 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.242605961859226
Loss made of: CE 0.21516220271587372, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.114056587219238 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.534874150156975
Loss made of: CE 0.18558695912361145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.584894180297852 EntMin 0.0
Epoch 2, Class Loss=0.2189863622188568, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.2189863622188568, Class Loss=0.2189863622188568, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.537047438323498
Loss made of: CE 0.2400633543729782, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.061103820800781 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.334923100471497
Loss made of: CE 0.19846419990062714, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3885583877563477 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.194825808703899
Loss made of: CE 0.18860316276550293, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9102301597595215 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.376304362714291
Loss made of: CE 0.1513119637966156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.398189544677734 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.23442809432745
Loss made of: CE 0.17805781960487366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.460826873779297 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.172832469642162
Loss made of: CE 0.19410920143127441, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.636859655380249 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.458415140211582
Loss made of: CE 0.14336395263671875, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.696164608001709 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.1595283843576905
Loss made of: CE 0.11490597575902939, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7776174545288086 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.16545929685235
Loss made of: CE 0.18314999341964722, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7068381309509277 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.136851164698601
Loss made of: CE 0.17572526633739471, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7886104583740234 EntMin 0.0
Epoch 3, Class Loss=0.19762158393859863, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.19762158393859863, Class Loss=0.19762158393859863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.409967981278896
Loss made of: CE 0.20025885105133057, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.997542381286621 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.210226806998253
Loss made of: CE 0.171659916639328, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0137715339660645 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.248119743168354
Loss made of: CE 0.1401309370994568, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5111308097839355 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.261387993395329
Loss made of: CE 0.1705106943845749, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8286561965942383 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.187348885834217
Loss made of: CE 0.22260122001171112, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.762256145477295 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.305238381028175
Loss made of: CE 0.1974739134311676, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.260739326477051 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.133605821430683
Loss made of: CE 0.20450395345687866, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.937927007675171 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.358353973925114
Loss made of: CE 0.20818059146404266, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.903675079345703 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.220401604473591
Loss made of: CE 0.18763095140457153, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.100907325744629 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.450625345110893
Loss made of: CE 0.1302136331796646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7890124320983887 EntMin 0.0
Epoch 4, Class Loss=0.1898055225610733, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.1898055225610733, Class Loss=0.1898055225610733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=3.9248080000281336
Loss made of: CE 0.20522265136241913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.179100036621094 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.35137647986412
Loss made of: CE 0.179122194647789, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8638949394226074 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.504535944759846
Loss made of: CE 0.2200297713279724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9572858810424805 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.149819573760032
Loss made of: CE 0.16094301640987396, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.529392957687378 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.109291182458401
Loss made of: CE 0.16181759536266327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.22557258605957 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.142311694473028
Loss made of: CE 0.1682167500257492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.442716360092163 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.233538361638784
Loss made of: CE 0.1543821096420288, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.330667495727539 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.3301818177103995
Loss made of: CE 0.15680450201034546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.126155853271484 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.096416902542114
Loss made of: CE 0.23353688418865204, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7497739791870117 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.147090607881546
Loss made of: CE 0.23102664947509766, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.718451023101807 EntMin 0.0
Epoch 5, Class Loss=0.17784219980239868, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.17784219980239868, Class Loss=0.17784219980239868, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.059822650253773
Loss made of: CE 0.18189847469329834, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2274255752563477 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.390108314901591
Loss made of: CE 0.1349514126777649, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.520620107650757 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.792892806977034
Loss made of: CE 0.15511231124401093, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.249715566635132 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.184541071951389
Loss made of: CE 0.19301214814186096, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6366498470306396 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.205047172307968
Loss made of: CE 0.17077526450157166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.877368927001953 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.05134002417326
Loss made of: CE 0.14079797267913818, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.898446559906006 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.126142322272062
Loss made of: CE 0.14936861395835876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8365304470062256 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.000251340866089
Loss made of: CE 0.1320837438106537, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.842759609222412 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.123991939425468
Loss made of: CE 0.15073755383491516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.012522220611572 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.089894872903824
Loss made of: CE 0.2104582041501999, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5027823448181152 EntMin 0.0
Epoch 6, Class Loss=0.16947434842586517, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.16947434842586517, Class Loss=0.16947434842586517, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=4.788258641958237
Loss made of: CE 0.34574827551841736, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.466558456420898 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.421774536371231
Loss made of: CE 0.3280058801174164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.211147308349609 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 30/102, Loss=4.54043699502945
Loss made of: CE 0.2553175091743469, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.459967613220215 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.798918110132218
Loss made of: CE 0.24717935919761658, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.56158447265625 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.771444353461265
Loss made of: CE 0.2299654483795166, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.246339797973633 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.756586894392967
Loss made of: CE 0.22511093318462372, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.753781795501709 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.557051359117031
Loss made of: CE 0.2799825072288513, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.01143741607666 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.5300268366932865
Loss made of: CE 0.24670755863189697, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.054081916809082 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.425170813500881
Loss made of: CE 0.196989044547081, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.736149549484253 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.425787501037121
Loss made of: CE 0.23273059725761414, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.973665714263916 EntMin 0.0
Epoch 1, Class Loss=0.2694193124771118, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.2694193124771118, Class Loss=0.2694193124771118, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.300581036508083
Loss made of: CE 0.22155940532684326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7644124031066895 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.26311336606741
Loss made of: CE 0.21691589057445526, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9827611446380615 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.316409161686897
Loss made of: CE 0.24026310443878174, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.169684410095215 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.433761721849441
Loss made of: CE 0.21747064590454102, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9386110305786133 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.515573884546757
Loss made of: CE 0.16029107570648193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.119532585144043 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.522659243643284
Loss made of: CE 0.23757526278495789, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9443700313568115 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.174108190834522
Loss made of: CE 0.1702190488576889, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5559425354003906 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.217538544535637
Loss made of: CE 0.14730314910411835, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1021552085876465 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.472056676447392
Loss made of: CE 0.2835484743118286, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.722290992736816 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.557118062674999
Loss made of: CE 0.16767045855522156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4949727058410645 EntMin 0.0
Epoch 2, Class Loss=0.21821445226669312, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.21821445226669312, Class Loss=0.21821445226669312, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.268330770730972
Loss made of: CE 0.22572866082191467, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.974356174468994 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.2380221575498584
Loss made of: CE 0.24798083305358887, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6775708198547363 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.508342450857162
Loss made of: CE 0.16567283868789673, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.828361511230469 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.277849170565605
Loss made of: CE 0.13273099064826965, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.899902820587158 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.352024756371975
Loss made of: CE 0.19212016463279724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9138612747192383 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.257260970771313
Loss made of: CE 0.19028884172439575, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.055649757385254 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.239771680533886
Loss made of: CE 0.1529192179441452, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.916964054107666 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.448481976985931
Loss made of: CE 0.174393892288208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5138750076293945 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.439478124678135
Loss made of: CE 0.19140836596488953, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.681191921234131 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.259034852683544
Loss made of: CE 0.16008546948432922, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7320775985717773 EntMin 0.0
Epoch 3, Class Loss=0.19925907254219055, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.19925907254219055, Class Loss=0.19925907254219055, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.2196161419153215
Loss made of: CE 0.21771317720413208, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8640589714050293 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.324272690713405
Loss made of: CE 0.18900518119335175, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.234514236450195 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.185338425636291
Loss made of: CE 0.2448771893978119, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9515700340270996 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.2881339259445665
Loss made of: CE 0.1979590654373169, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7069005966186523 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.426645785570145
Loss made of: CE 0.18373024463653564, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.409211158752441 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.417657251656055
Loss made of: CE 0.2326725721359253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.265476703643799 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.306511355936527
Loss made of: CE 0.22462114691734314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1497578620910645 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.342407248914242
Loss made of: CE 0.24293206632137299, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.377263069152832 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.300977748632431
Loss made of: CE 0.17826110124588013, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.869837522506714 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.374084557592869
Loss made of: CE 0.22592680156230927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.968382835388184 EntMin 0.0
Epoch 4, Class Loss=0.1886449009180069, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.1886449009180069, Class Loss=0.1886449009180069, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.523212359845639
Loss made of: CE 0.12173734605312347, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.646831512451172 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.365118789672851
Loss made of: CE 0.10839352011680603, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.985980987548828 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.281954583525658
Loss made of: CE 0.14957232773303986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.653280019760132 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.020357528328896
Loss made of: CE 0.2581559121608734, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7672832012176514 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.2066800147295
Loss made of: CE 0.16777312755584717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2918243408203125 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.9358640372753144
Loss made of: CE 0.1917484849691391, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7435755729675293 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.071300895512104
Loss made of: CE 0.2134300321340561, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.560636520385742 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.113216663897037
Loss made of: CE 0.1690424680709839, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.104131698608398 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.058884940296411
Loss made of: CE 0.13400305807590485, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.455841541290283 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.136949276924133
Loss made of: CE 0.15884487330913544, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.199040412902832 EntMin 0.0
Epoch 5, Class Loss=0.1762424111366272, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.1762424111366272, Class Loss=0.1762424111366272, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.444493982195854
Loss made of: CE 0.19214074313640594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.020391464233398 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.973779134452343
Loss made of: CE 0.1163896918296814, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6275148391723633 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.856932985782623
Loss made of: CE 0.15877312421798706, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8971595764160156 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.4018821805715564
Loss made of: CE 0.24791204929351807, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.918560028076172 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.9779886469244956
Loss made of: CE 0.16910889744758606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.649188756942749 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.247746085375548
Loss made of: CE 0.16109198331832886, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.945044994354248 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.876691982150078
Loss made of: CE 0.139360249042511, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9277751445770264 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.109317842870951
Loss made of: CE 0.15725761651992798, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.694869041442871 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.122345824539662
Loss made of: CE 0.15883323550224304, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5436220169067383 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.237201656401157
Loss made of: CE 0.17325256764888763, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.796205997467041 EntMin 0.0
Epoch 6, Class Loss=0.17049621045589447, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.17049621045589447, Class Loss=0.17049621045589447, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/105, Loss=5.104732874035835
Loss made of: CE 0.5764768123626709, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.342589378356934 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.6331277877092365
Loss made of: CE 0.2820708751678467, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.630393028259277 EntMin 0.0
Epoch 1, Batch 30/105, Loss=4.367290170490742
Loss made of: CE 0.2774861454963684, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.299386501312256 EntMin 0.0
Epoch 1, Batch 40/105, Loss=4.647156627476216
Loss made of: CE 0.21103578805923462, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7160604000091553 EntMin 0.0
Epoch 1, Batch 50/105, Loss=4.495354159176349
Loss made of: CE 0.32428497076034546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9678378105163574 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.528296747803688
Loss made of: CE 0.3524726629257202, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.162950038909912 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.513794089853763
Loss made of: CE 0.25363534688949585, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.374515533447266 EntMin 0.0
Epoch 1, Batch 80/105, Loss=4.769918850064277
Loss made of: CE 0.22805801033973694, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.879785060882568 EntMin 0.0
Epoch 1, Batch 90/105, Loss=4.695524817705154
Loss made of: CE 0.2525850832462311, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389206886291504 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.441981784999371
Loss made of: CE 0.2207743376493454, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.392096519470215 EntMin 0.0
Epoch 1, Class Loss=0.2810417711734772, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.2810417711734772, Class Loss=0.2810417711734772, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/105, Loss=4.300362561643124
Loss made of: CE 0.2670089602470398, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9973764419555664 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.448200598359108
Loss made of: CE 0.2657535672187805, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.055624961853027 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.5316450253129
Loss made of: CE 0.2189263552427292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.628920555114746 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.500159026682377
Loss made of: CE 0.2196379154920578, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.338347434997559 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.303835107386112
Loss made of: CE 0.17485889792442322, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6627516746520996 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 60/105, Loss=4.478498004376888
Loss made of: CE 0.19075427949428558, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.884182929992676 EntMin 0.0
Epoch 2, Batch 70/105, Loss=4.808766607940197
Loss made of: CE 0.21989208459854126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.884455680847168 EntMin 0.0
Epoch 2, Batch 80/105, Loss=4.522424100339412
Loss made of: CE 0.20561406016349792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.214915752410889 EntMin 0.0
Epoch 2, Batch 90/105, Loss=4.710588897764683
Loss made of: CE 0.2184755504131317, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.604033470153809 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.375149098038674
Loss made of: CE 0.17521610856056213, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9272141456604004 EntMin 0.0
Epoch 2, Class Loss=0.2340863049030304, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.2340863049030304, Class Loss=0.2340863049030304, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/105, Loss=4.360933598876
Loss made of: CE 0.15645524859428406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.934488296508789 EntMin 0.0
Epoch 3, Batch 20/105, Loss=4.59015431702137
Loss made of: CE 0.17212127149105072, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.393545150756836 EntMin 0.0
Epoch 3, Batch 30/105, Loss=4.286610625684261
Loss made of: CE 0.17051391303539276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2358479499816895 EntMin 0.0
Epoch 3, Batch 40/105, Loss=4.629861940443516
Loss made of: CE 0.23285675048828125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.103642463684082 EntMin 0.0
Epoch 3, Batch 50/105, Loss=4.333322435617447
Loss made of: CE 0.16512006521224976, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.961153030395508 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.486637400090695
Loss made of: CE 0.26012107729911804, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8843231201171875 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.293656550347805
Loss made of: CE 0.21390074491500854, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.732510566711426 EntMin 0.0
Epoch 3, Batch 80/105, Loss=4.4382288828492165
Loss made of: CE 0.2825719118118286, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.172171592712402 EntMin 0.0
Epoch 3, Batch 90/105, Loss=4.288428045809269
Loss made of: CE 0.20472955703735352, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.087306022644043 EntMin 0.0
Epoch 3, Batch 100/105, Loss=4.297268022596836
Loss made of: CE 0.16823622584342957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.569502830505371 EntMin 0.0
Epoch 3, Class Loss=0.21414224803447723, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.21414224803447723, Class Loss=0.21414224803447723, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/105, Loss=4.136324152350426
Loss made of: CE 0.14023390412330627, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.785472869873047 EntMin 0.0
Epoch 4, Batch 20/105, Loss=4.284796003997326
Loss made of: CE 0.1856151521205902, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8865649700164795 EntMin 0.0
Epoch 4, Batch 30/105, Loss=4.181443792581558
Loss made of: CE 0.18252983689308167, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.841233015060425 EntMin 0.0
Epoch 4, Batch 40/105, Loss=4.43925267457962
Loss made of: CE 0.1580725908279419, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.803037405014038 EntMin 0.0
Epoch 4, Batch 50/105, Loss=4.248276422917843
Loss made of: CE 0.16230729222297668, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.960998058319092 EntMin 0.0
Epoch 4, Batch 60/105, Loss=4.27510828524828
Loss made of: CE 0.16341373324394226, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9015398025512695 EntMin 0.0
Epoch 4, Batch 70/105, Loss=4.362178228795528
Loss made of: CE 0.21971473097801208, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7141833305358887 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.178430829942227
Loss made of: CE 0.2415962517261505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1027512550354 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.367959925532341
Loss made of: CE 0.2283603847026825, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.543933391571045 EntMin 0.0
Epoch 4, Batch 100/105, Loss=4.285122013092041
Loss made of: CE 0.16335439682006836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8642661571502686 EntMin 0.0
Epoch 4, Class Loss=0.1991240382194519, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.1991240382194519, Class Loss=0.1991240382194519, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/105, Loss=4.664287239313126
Loss made of: CE 0.19155395030975342, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.05839729309082 EntMin 0.0
Epoch 5, Batch 20/105, Loss=4.264190551638603
Loss made of: CE 0.17401453852653503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.054046154022217 EntMin 0.0
Epoch 5, Batch 30/105, Loss=4.195155788958073
Loss made of: CE 0.1975838989019394, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6596126556396484 EntMin 0.0
Epoch 5, Batch 40/105, Loss=4.682512883841992
Loss made of: CE 0.16913703083992004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.903716087341309 EntMin 0.0
Epoch 5, Batch 50/105, Loss=4.174051374197006
Loss made of: CE 0.20285271108150482, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.155548572540283 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.332106265425682
Loss made of: CE 0.2431047260761261, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.859405040740967 EntMin 0.0
Epoch 5, Batch 70/105, Loss=4.101022115349769
Loss made of: CE 0.14815710484981537, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.704911231994629 EntMin 0.0
Epoch 5, Batch 80/105, Loss=4.448299212753772
Loss made of: CE 0.18010078370571136, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5583338737487793 EntMin 0.0
Epoch 5, Batch 90/105, Loss=4.336581885814667
Loss made of: CE 0.2089943140745163, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9427828788757324 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.230513581633568
Loss made of: CE 0.24119798839092255, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.517782211303711 EntMin 0.0
Epoch 5, Class Loss=0.19491936266422272, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.19491936266422272, Class Loss=0.19491936266422272, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/105, Loss=4.1654472425580025
Loss made of: CE 0.18391141295433044, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7389516830444336 EntMin 0.0
Epoch 6, Batch 20/105, Loss=4.599191370606422
Loss made of: CE 0.32063019275665283, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.282199382781982 EntMin 0.0
Epoch 6, Batch 30/105, Loss=4.118869572132826
Loss made of: CE 0.22329208254814148, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3777594566345215 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.007006078958511
Loss made of: CE 0.16030308604240417, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4188780784606934 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.1077598050236706
Loss made of: CE 0.1436539888381958, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.625108480453491 EntMin 0.0
Epoch 6, Batch 60/105, Loss=4.283351151645183
Loss made of: CE 0.17234443128108978, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.142833709716797 EntMin 0.0
Epoch 6, Batch 70/105, Loss=4.0255324728786945
Loss made of: CE 0.11866386979818344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.141333103179932 EntMin 0.0
Epoch 6, Batch 80/105, Loss=4.433223447948694
Loss made of: CE 0.16095586121082306, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4794158935546875 EntMin 0.0
Epoch 6, Batch 90/105, Loss=4.076142704486847
Loss made of: CE 0.1190255880355835, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4296793937683105 EntMin 0.0
Epoch 6, Batch 100/105, Loss=4.155628749728203
Loss made of: CE 0.14427991211414337, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7597665786743164 EntMin 0.0
Epoch 6, Class Loss=0.18682590126991272, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.18682590126991272, Class Loss=0.18682590126991272, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=4.4209377124905584
Loss made of: CE 0.36885103583335876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5023810863494873 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.143085566163063
Loss made of: CE 0.29869604110717773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.894093990325928 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.603657822310924
Loss made of: CE 0.23309370875358582, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7154879570007324 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.446175807714463
Loss made of: CE 0.23788313567638397, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8388733863830566 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.268387314677239
Loss made of: CE 0.1941150426864624, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7868049144744873 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.064502577483654
Loss made of: CE 0.32126718759536743, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1054887771606445 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.750755318999291
Loss made of: CE 0.27942466735839844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.633495807647705 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.4299669444561
Loss made of: CE 0.21799616515636444, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9285857677459717 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.596157142519951
Loss made of: CE 0.29795536398887634, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6175050735473633 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.646839301288128
Loss made of: CE 0.2826748490333557, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.038825511932373 EntMin 0.0
Epoch 1, Class Loss=0.26260632276535034, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.26260632276535034, Class Loss=0.26260632276535034, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/102, Loss=4.287273105978966
Loss made of: CE 0.21124854683876038, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8965797424316406 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.402424351871014
Loss made of: CE 0.17534306645393372, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.947064161300659 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 30/102, Loss=4.448992709815502
Loss made of: CE 0.25351813435554504, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.064090251922607 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.505479842424393
Loss made of: CE 0.2430831789970398, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.745105743408203 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.239404973387718
Loss made of: CE 0.20805823802947998, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.522374153137207 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.6218763753771785
Loss made of: CE 0.15266448259353638, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5022687911987305 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.691241838037968
Loss made of: CE 0.18863801658153534, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7467117309570312 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.436167599260807
Loss made of: CE 0.24421989917755127, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.681807518005371 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.248605544865131
Loss made of: CE 0.2286320924758911, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.473324775695801 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.252690291404724
Loss made of: CE 0.1809212863445282, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9983298778533936 EntMin 0.0
Epoch 2, Class Loss=0.21251331269741058, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.21251331269741058, Class Loss=0.21251331269741058, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/102, Loss=4.489459496736527
Loss made of: CE 0.17228977382183075, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.914630889892578 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.716573630273342
Loss made of: CE 0.24440333247184753, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.739382743835449 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.222875055670738
Loss made of: CE 0.24092429876327515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.272514343261719 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.4113211125135425
Loss made of: CE 0.12457159161567688, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.851914882659912 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.490432347357273
Loss made of: CE 0.2338424175977707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.551304817199707 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.228272652626037
Loss made of: CE 0.21361351013183594, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7379751205444336 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.38724470436573
Loss made of: CE 0.15214458107948303, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.092642784118652 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.323728118091822
Loss made of: CE 0.19074329733848572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.068470001220703 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.226756227016449
Loss made of: CE 0.16059736907482147, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8446033000946045 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.238393974304199
Loss made of: CE 0.14109328389167786, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8067901134490967 EntMin 0.0
Epoch 3, Class Loss=0.19043883681297302, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.19043883681297302, Class Loss=0.19043883681297302, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/102, Loss=4.446824570000172
Loss made of: CE 0.19569429755210876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9903969764709473 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.367925624549389
Loss made of: CE 0.17704224586486816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.191231727600098 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.449362833797932
Loss made of: CE 0.18264751136302948, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8010144233703613 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.971883472800255
Loss made of: CE 0.16419151425361633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.033463478088379 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.410554125905037
Loss made of: CE 0.16513794660568237, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.111147880554199 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.1874781340360645
Loss made of: CE 0.16819939017295837, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6863276958465576 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.139716185629368
Loss made of: CE 0.14750492572784424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.289992332458496 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.049231569468975
Loss made of: CE 0.16330522298812866, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.895775556564331 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.6292140580713745
Loss made of: CE 0.20705312490463257, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2830352783203125 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.137399116903543
Loss made of: CE 0.14451375603675842, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7498011589050293 EntMin 0.0
Epoch 4, Class Loss=0.18017324805259705, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.18017324805259705, Class Loss=0.18017324805259705, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/102, Loss=4.0221041813492775
Loss made of: CE 0.18251436948776245, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.610443592071533 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.426067005097866
Loss made of: CE 0.17026405036449432, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.289717674255371 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.204520715773105
Loss made of: CE 0.10988008975982666, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.004058361053467 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.1287137299776075
Loss made of: CE 0.15161073207855225, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.320558547973633 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.1132841147482395
Loss made of: CE 0.1498308628797531, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5691821575164795 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.0818041324615475
Loss made of: CE 0.19200114905834198, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.100848197937012 EntMin 0.0
Epoch 5, Batch 70/102, Loss=3.9443212702870367
Loss made of: CE 0.1774083822965622, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6545467376708984 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.027957217395306
Loss made of: CE 0.13841602206230164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.03591251373291 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.632609678804874
Loss made of: CE 0.2337920367717743, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.181838035583496 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.026923203468323
Loss made of: CE 0.1416899561882019, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.603915214538574 EntMin 0.0
Epoch 5, Class Loss=0.1711447387933731, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.1711447387933731, Class Loss=0.1711447387933731, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/102, Loss=4.283775201439857
Loss made of: CE 0.1566483974456787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9206814765930176 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.2601032048463825
Loss made of: CE 0.19509343802928925, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9008922576904297 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.345010983943939
Loss made of: CE 0.14462833106517792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.243037700653076 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.101289160549641
Loss made of: CE 0.20333385467529297, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.913635730743408 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.5694497637450695
Loss made of: CE 0.14333418011665344, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9273955821990967 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.140760198235512
Loss made of: CE 0.13706618547439575, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.122163772583008 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.283777137100697
Loss made of: CE 0.19890058040618896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.257997035980225 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.199779029935598
Loss made of: CE 0.16554248332977295, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7764244079589844 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.14413450807333
Loss made of: CE 0.13533401489257812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.768151760101318 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.057836775481701
Loss made of: CE 0.19067732989788055, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.362457275390625 EntMin 0.0
Epoch 6, Class Loss=0.1635390669107437, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.1635390669107437, Class Loss=0.1635390669107437, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.642952561378479, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.802196
Mean Acc: 0.389946
FreqW Acc: 0.700748
Mean IoU: 0.287599
Class IoU:
	class 0: 0.8649751
	class 1: 0.04620501
	class 2: 0.08910371
	class 3: 0.0
	class 4: 0.454768
	class 5: 0.0021550616
	class 6: 0.77872604
	class 7: 0.68303156
	class 8: 0.05884367
	class 9: 0.0
	class 10: 0.45881903
	class 11: 0.29177752
	class 12: 0.23547186
	class 13: 0.0
	class 14: 0.50966334
	class 15: 0.41563755
	class 16: 0.0
Class Acc:
	class 0: 0.9280421
	class 1: 0.046209212
	class 2: 0.10427204
	class 3: 0.0
	class 4: 0.471614
	class 5: 0.002155722
	class 6: 0.90614384
	class 7: 0.74328756
	class 8: 0.05885638
	class 9: 0.0
	class 10: 0.6169896
	class 11: 0.46263027
	class 12: 0.49034426
	class 13: 0.0
	class 14: 0.8835683
	class 15: 0.91497195
	class 16: 0.0

federated global round: 17, step: 3
select part of clients to conduct local training
[5, 3, 17, 11]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=3.8074868127703665
Loss made of: CE 0.18089473247528076, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3233230113983154 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/102, Loss=4.218943209946156
Loss made of: CE 0.13568006455898285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.608710765838623 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.855401176214218
Loss made of: CE 0.16501754522323608, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3666138648986816 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.9230477660894394
Loss made of: CE 0.1407027542591095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.030866622924805 EntMin 0.0
Epoch 1, Batch 50/102, Loss=3.956925604492426
Loss made of: CE 0.15023931860923767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9291954040527344 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.301975257694721
Loss made of: CE 0.20067298412322998, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.388664245605469 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.411962702870369
Loss made of: CE 0.20343849062919617, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.940103530883789 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.2531858250498775
Loss made of: CE 0.1607363373041153, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.149421691894531 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.0878333412110806
Loss made of: CE 0.17935042083263397, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9111526012420654 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.8783231027424336
Loss made of: CE 0.11981718987226486, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2078943252563477 EntMin 0.0
Epoch 1, Class Loss=0.1629490852355957, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.1629490852355957, Class Loss=0.1629490852355957, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.226065572351217
Loss made of: CE 0.15897664427757263, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.864696979522705 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.1091402977705
Loss made of: CE 0.15708430111408234, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4354677200317383 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.06874742358923
Loss made of: CE 0.13413366675376892, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6132774353027344 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.209717817604542
Loss made of: CE 0.20653069019317627, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.188770771026611 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.412869499623776
Loss made of: CE 0.1407013237476349, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.157804489135742 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.229963874816894
Loss made of: CE 0.1786920726299286, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.79270601272583 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.038464877009392
Loss made of: CE 0.20090219378471375, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9126205444335938 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.327106533199549
Loss made of: CE 0.19626490771770477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684788703918457 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.0613415412604805
Loss made of: CE 0.13014206290245056, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.405364751815796 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.42476651519537
Loss made of: CE 0.14537857472896576, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9069392681121826 EntMin 0.0
Epoch 2, Class Loss=0.15915626287460327, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.15915626287460327, Class Loss=0.15915626287460327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.064648193120957
Loss made of: CE 0.1475619077682495, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.849595546722412 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.12548161521554
Loss made of: CE 0.11889074742794037, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9371554851531982 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.041219940036536
Loss made of: CE 0.1506737768650055, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.39882230758667 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.314103990793228
Loss made of: CE 0.15727642178535461, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.328826427459717 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.117921406775713
Loss made of: CE 0.22832340002059937, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6098170280456543 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.026907705515623
Loss made of: CE 0.18438461422920227, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.210879325866699 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.008053962886334
Loss made of: CE 0.11398595571517944, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.785548210144043 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.369301975518465
Loss made of: CE 0.10873385518789291, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.573701858520508 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 90/102, Loss=4.174394971132278
Loss made of: CE 0.11404653638601303, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6165921688079834 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.8084960907697676
Loss made of: CE 0.1365283578634262, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.778010845184326 EntMin 0.0
Epoch 3, Class Loss=0.151262104511261, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.151262104511261, Class Loss=0.151262104511261, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.084141771495342
Loss made of: CE 0.11491888016462326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.690594434738159 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.7835685446858407
Loss made of: CE 0.13701549172401428, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7765045166015625 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.33346116989851
Loss made of: CE 0.15953761339187622, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.949338436126709 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.127732776850462
Loss made of: CE 0.15182790160179138, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.709252119064331 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.023807264864445
Loss made of: CE 0.10704906284809113, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.859910011291504 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.183268140256405
Loss made of: CE 0.21695439517498016, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6547889709472656 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.07745224237442
Loss made of: CE 0.15982674062252045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.277398109436035 EntMin 0.0
Epoch 4, Batch 80/102, Loss=3.9856472574174404
Loss made of: CE 0.1527991145849228, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.687392234802246 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.089639557152987
Loss made of: CE 0.18806764483451843, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.747431755065918 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.226385650038719
Loss made of: CE 0.150778666138649, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8773648738861084 EntMin 0.0
Epoch 4, Class Loss=0.1456112563610077, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.1456112563610077, Class Loss=0.1456112563610077, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.031292362511158
Loss made of: CE 0.1366712898015976, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7819290161132812 EntMin 0.0
Epoch 5, Batch 20/102, Loss=3.908261726051569
Loss made of: CE 0.12940189242362976, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.751511335372925 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.8417436480522156
Loss made of: CE 0.12544941902160645, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.83280348777771 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.9358761951327326
Loss made of: CE 0.15202239155769348, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5747737884521484 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.0048119612038136
Loss made of: CE 0.10032536089420319, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6979589462280273 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.9433602184057235
Loss made of: CE 0.1634707897901535, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4468069076538086 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.036122255772352
Loss made of: CE 0.10542251914739609, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8123154640197754 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.472239737212658
Loss made of: CE 0.17757205665111542, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.546004056930542 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.8589490957558157
Loss made of: CE 0.16035176813602448, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.790238380432129 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.04350812137127
Loss made of: CE 0.13592609763145447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.757718563079834 EntMin 0.0
Epoch 5, Class Loss=0.14025560021400452, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.14025560021400452, Class Loss=0.14025560021400452, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=3.936653880774975
Loss made of: CE 0.12725433707237244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7902231216430664 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.8629418447613717
Loss made of: CE 0.11248414218425751, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.293271541595459 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.8531909815967085
Loss made of: CE 0.17081892490386963, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5928120613098145 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.185824736207723
Loss made of: CE 0.1185588389635086, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.727405071258545 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.116593214869499
Loss made of: CE 0.20852112770080566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.856457710266113 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.9255990855395795
Loss made of: CE 0.09088185429573059, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.382265090942383 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.8439026311039926
Loss made of: CE 0.14135774970054626, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4907588958740234 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.7292604401707647
Loss made of: CE 0.126939058303833, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.735440969467163 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.221597491204738
Loss made of: CE 0.14983993768692017, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.003005027770996 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.2421921521425245
Loss made of: CE 0.13282927870750427, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.545379161834717 EntMin 0.0
Epoch 6, Class Loss=0.13662125170230865, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.13662125170230865, Class Loss=0.13662125170230865, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=3.9537732891738413
Loss made of: CE 0.14620964229106903, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427602291107178 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.942334571480751
Loss made of: CE 0.15653058886528015, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2450523376464844 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.19918891415
Loss made of: CE 0.20382723212242126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.351215362548828 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.070144733786583
Loss made of: CE 0.21060752868652344, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.37939977645874 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.219686083495617
Loss made of: CE 0.15636542439460754, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1942009925842285 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.254973364621401
Loss made of: CE 0.15855224430561066, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6559455394744873 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.159037133306265
Loss made of: CE 0.1616024672985077, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9011030197143555 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.091380229592323
Loss made of: CE 0.1804410070180893, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.290565252304077 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.329527805000543
Loss made of: CE 0.15920047461986542, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.679940700531006 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.213956911861897
Loss made of: CE 0.1466280221939087, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.593045234680176 EntMin 0.0
Epoch 1, Class Loss=0.16231580078601837, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.16231580078601837, Class Loss=0.16231580078601837, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.222617021203041
Loss made of: CE 0.18623730540275574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1437177658081055 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.01605838611722
Loss made of: CE 0.21845506131649017, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6596951484680176 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.363022775948048
Loss made of: CE 0.15763643383979797, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6717300415039062 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.094942421466112
Loss made of: CE 0.12682273983955383, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.341691732406616 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.171325930953026
Loss made of: CE 0.16016100347042084, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3885209560394287 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.157063486427068
Loss made of: CE 0.15571454167366028, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.582756519317627 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.178483317792415
Loss made of: CE 0.15090122818946838, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.188697814941406 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.214635664224625
Loss made of: CE 0.14012044668197632, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.264941215515137 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.259005118906498
Loss made of: CE 0.11975368112325668, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6216180324554443 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.387720513343811
Loss made of: CE 0.179590106010437, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.694791793823242 EntMin 0.0
Epoch 2, Class Loss=0.15564291179180145, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.15564291179180145, Class Loss=0.15564291179180145, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.238409605622292
Loss made of: CE 0.11883049458265305, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.922870635986328 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.059862031787634
Loss made of: CE 0.16899752616882324, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.391713619232178 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.270190372318029
Loss made of: CE 0.18355797231197357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.024781703948975 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.063922344148159
Loss made of: CE 0.13821178674697876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.11892032623291 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.481696308404207
Loss made of: CE 0.1717212200164795, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.09210205078125 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.937151437997818
Loss made of: CE 0.14673040807247162, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4310460090637207 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.290928795188665
Loss made of: CE 0.10129411518573761, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.017177104949951 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.273558363318443
Loss made of: CE 0.14881187677383423, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.827387571334839 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.044470440596342
Loss made of: CE 0.11780174821615219, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.509556293487549 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.100630228221417
Loss made of: CE 0.1845528781414032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.275302886962891 EntMin 0.0
Epoch 3, Class Loss=0.14744745194911957, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.14744745194911957, Class Loss=0.14744745194911957, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=3.9646705694496633
Loss made of: CE 0.1392170637845993, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.647245168685913 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.2917571604251865
Loss made of: CE 0.16825136542320251, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7633399963378906 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.023822813481092
Loss made of: CE 0.13268202543258667, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5945940017700195 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.300202273577452
Loss made of: CE 0.15229544043540955, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.548326015472412 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.183573414385319
Loss made of: CE 0.1459062695503235, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8142013549804688 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 60/102, Loss=4.0712748765945435
Loss made of: CE 0.1635783612728119, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.99362850189209 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.529632899165153
Loss made of: CE 0.19756868481636047, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.029847145080566 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.3088715970516205
Loss made of: CE 0.09960669279098511, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.043705940246582 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.099910324811935
Loss made of: CE 0.12049858272075653, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.450282573699951 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.07522893473506
Loss made of: CE 0.1875379979610443, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8730921745300293 EntMin 0.0
Epoch 4, Class Loss=0.14477623999118805, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.14477623999118805, Class Loss=0.14477623999118805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.003159090876579
Loss made of: CE 0.16486404836177826, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.762305736541748 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.105119957029819
Loss made of: CE 0.17144666612148285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6482455730438232 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.9731207743287085
Loss made of: CE 0.14929604530334473, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.403351306915283 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.0743828728795055
Loss made of: CE 0.16138850152492523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.460775852203369 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.2147474989295
Loss made of: CE 0.0975707471370697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7090091705322266 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.934558320045471
Loss made of: CE 0.13528722524642944, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5697953701019287 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.260762328654527
Loss made of: CE 0.12618951499462128, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.89105486869812 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.4695558927953245
Loss made of: CE 0.14745956659317017, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.676049709320068 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.9632894530892373
Loss made of: CE 0.1255638301372528, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.903991222381592 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.968908299505711
Loss made of: CE 0.15395288169384003, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5945794582366943 EntMin 0.0
Epoch 5, Class Loss=0.14067287743091583, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.14067287743091583, Class Loss=0.14067287743091583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=3.9416137978434564
Loss made of: CE 0.14312711358070374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.396533489227295 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.92108960300684
Loss made of: CE 0.1306823492050171, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5291147232055664 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.031219740211964
Loss made of: CE 0.15858905017375946, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.482468605041504 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.729979772120714
Loss made of: CE 0.12079253047704697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3889997005462646 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.004552194476128
Loss made of: CE 0.11994913220405579, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.365514755249023 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.986227570474148
Loss made of: CE 0.15706835687160492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.569180965423584 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.6983266450464725
Loss made of: CE 0.13076740503311157, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.563667058944702 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.8622270710766315
Loss made of: CE 0.15974214673042297, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.542680501937866 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.193159092962742
Loss made of: CE 0.13350939750671387, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3531899452209473 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.05468762293458
Loss made of: CE 0.1107160672545433, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.661344051361084 EntMin 0.0
Epoch 6, Class Loss=0.13592679798603058, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.13592679798603058, Class Loss=0.13592679798603058, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=5.773798131942749
Loss made of: CE 0.6328179240226746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.729986190795898 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.178950002789497
Loss made of: CE 0.5352258682250977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.548355579376221 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.5744257569313049, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.5744257569313049, Class Loss=0.5744257569313049, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/23, Loss=4.83870250582695
Loss made of: CE 0.5376651287078857, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.818422794342041 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.658631429076195
Loss made of: CE 0.29635295271873474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.164259910583496 EntMin 0.0
Epoch 2, Class Loss=0.471466064453125, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.471466064453125, Class Loss=0.471466064453125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/23, Loss=4.548566463589668
Loss made of: CE 0.3644663095474243, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035843849182129 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.550027017295361
Loss made of: CE 0.4743212163448334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.302366256713867 EntMin 0.0
Epoch 3, Class Loss=0.3943718671798706, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.3943718671798706, Class Loss=0.3943718671798706, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/23, Loss=4.589386315643788
Loss made of: CE 0.40057504177093506, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.821291923522949 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.309407120943069
Loss made of: CE 0.2430708110332489, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.137921333312988 EntMin 0.0
Epoch 4, Class Loss=0.3258742392063141, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.3258742392063141, Class Loss=0.3258742392063141, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/23, Loss=4.394533906877041
Loss made of: CE 0.26350700855255127, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.243680953979492 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.116034482419491
Loss made of: CE 0.24948132038116455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035370826721191 EntMin 0.0
Epoch 5, Class Loss=0.2802329957485199, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.2802329957485199, Class Loss=0.2802329957485199, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/23, Loss=4.259515312314034
Loss made of: CE 0.29731446504592896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.789999485015869 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.164345873892307
Loss made of: CE 0.3026183545589447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.836179733276367 EntMin 0.0
Epoch 6, Class Loss=0.25824815034866333, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.25824815034866333, Class Loss=0.25824815034866333, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=3.863705449551344
Loss made of: CE 0.17818769812583923, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.859335422515869 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.9340747117996218
Loss made of: CE 0.2109907865524292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7574212551116943 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.9560360483825208
Loss made of: CE 0.1460423320531845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1785664558410645 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.253847770392895
Loss made of: CE 0.13863813877105713, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.155961990356445 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.2109097182750705
Loss made of: CE 0.12970678508281708, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.883540153503418 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.160091362893581
Loss made of: CE 0.1455410122871399, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.374925374984741 EntMin 0.0
Epoch 1, Batch 70/102, Loss=3.9879789993166925
Loss made of: CE 0.16918079555034637, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8326900005340576 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.105104274302721
Loss made of: CE 0.1843763291835785, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.465365409851074 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.8821072682738302
Loss made of: CE 0.13573268055915833, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2914962768554688 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.9743974819779395
Loss made of: CE 0.1554953157901764, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.77219295501709 EntMin 0.0
Epoch 1, Class Loss=0.1655701845884323, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.1655701845884323, Class Loss=0.1655701845884323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000600
Epoch 2, Batch 10/102, Loss=3.924219947308302
Loss made of: CE 0.1358286440372467, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4633967876434326 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.8613957837224007
Loss made of: CE 0.15418319404125214, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6814794540405273 EntMin 0.0
Epoch 2, Batch 30/102, Loss=3.9646356336772444
Loss made of: CE 0.20636382699012756, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.618227005004883 EntMin 0.0
Epoch 2, Batch 40/102, Loss=3.9699628338217736
Loss made of: CE 0.159499853849411, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.348891019821167 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.005532546341419
Loss made of: CE 0.10937806218862534, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7492804527282715 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.023571415245533
Loss made of: CE 0.15574921667575836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.453925609588623 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.716456086188555
Loss made of: CE 0.14401978254318237, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.331472158432007 EntMin 0.0
Epoch 2, Batch 80/102, Loss=3.963177111744881
Loss made of: CE 0.1266414225101471, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.280787944793701 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.981404311209917
Loss made of: CE 0.22901463508605957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.617900371551514 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.125208522379398
Loss made of: CE 0.1480666697025299, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.006606578826904 EntMin 0.0
Epoch 2, Class Loss=0.15956218540668488, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.15956218540668488, Class Loss=0.15956218540668488, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/102, Loss=3.9475500486791133
Loss made of: CE 0.1547553986310959, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.047364711761475 EntMin 0.0
Epoch 3, Batch 20/102, Loss=3.875305026769638
Loss made of: CE 0.18198814988136292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.218764066696167 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.9566428035497667
Loss made of: CE 0.14212535321712494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.395969390869141 EntMin 0.0
Epoch 3, Batch 40/102, Loss=3.932128871977329
Loss made of: CE 0.11995386332273483, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7549190521240234 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.0768922284245495
Loss made of: CE 0.1352119892835617, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.387871265411377 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.915468382835388
Loss made of: CE 0.14514514803886414, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7551727294921875 EntMin 0.0
Epoch 3, Batch 70/102, Loss=3.760882745683193
Loss made of: CE 0.12445995211601257, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.78767466545105 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.159958123415708
Loss made of: CE 0.1547926664352417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.239904403686523 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.082603840529918
Loss made of: CE 0.12727752327919006, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.266730308532715 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.911226577311754
Loss made of: CE 0.13649459183216095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1806411743164062 EntMin 0.0
Epoch 3, Class Loss=0.15284179151058197, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.15284179151058197, Class Loss=0.15284179151058197, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/102, Loss=3.8266074903309346
Loss made of: CE 0.16992664337158203, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2664103507995605 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.128941641747952
Loss made of: CE 0.13885712623596191, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.361622333526611 EntMin 0.0
Epoch 4, Batch 30/102, Loss=3.8995995983481406
Loss made of: CE 0.16128495335578918, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.874112606048584 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.8056827008724214
Loss made of: CE 0.13458208739757538, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2033708095550537 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.075304837524891
Loss made of: CE 0.15269456803798676, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9189374446868896 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.080773372203112
Loss made of: CE 0.17735129594802856, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.855001211166382 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.004375147819519
Loss made of: CE 0.17486679553985596, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6674036979675293 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.074232194572687
Loss made of: CE 0.1897447556257248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7835283279418945 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.042243067920208
Loss made of: CE 0.13247603178024292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5620479583740234 EntMin 0.0
Epoch 4, Batch 100/102, Loss=3.957558584958315
Loss made of: CE 0.21560078859329224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.693114280700684 EntMin 0.0
Epoch 4, Class Loss=0.1528535634279251, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.1528535634279251, Class Loss=0.1528535634279251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000504
Epoch 5, Batch 10/102, Loss=4.1861937560141085
Loss made of: CE 0.10843857377767563, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5039565563201904 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.09838307723403
Loss made of: CE 0.08924248069524765, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7526233196258545 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.973711561411619
Loss made of: CE 0.13074558973312378, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4635157585144043 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.7875603400170803
Loss made of: CE 0.19179630279541016, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7713019847869873 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.020713959634304
Loss made of: CE 0.16792476177215576, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.161370277404785 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.6713757179677486
Loss made of: CE 0.15033753216266632, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8003597259521484 EntMin 0.0
Epoch 5, Batch 70/102, Loss=3.801986836642027
Loss made of: CE 0.18349632620811462, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5749669075012207 EntMin 0.0
Epoch 5, Batch 80/102, Loss=3.939358924329281
Loss made of: CE 0.1429392397403717, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.806586980819702 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.9251916028559206
Loss made of: CE 0.13019666075706482, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.280264377593994 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.961387151479721
Loss made of: CE 0.1346035599708557, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.10216760635376 EntMin 0.0
Epoch 5, Class Loss=0.14620667695999146, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.14620667695999146, Class Loss=0.14620667695999146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000471
Epoch 6, Batch 10/102, Loss=4.215503713488578
Loss made of: CE 0.17618998885154724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5310142040252686 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.773112792521715
Loss made of: CE 0.0992063507437706, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.559507369995117 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.623836952447891
Loss made of: CE 0.13030749559402466, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.644326686859131 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.001995632052422
Loss made of: CE 0.20156657695770264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.662165641784668 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.7093096412718296
Loss made of: CE 0.1387152373790741, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.295201301574707 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.8981187932193277
Loss made of: CE 0.16368640959262848, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6837987899780273 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.610540063679218
Loss made of: CE 0.11864513158798218, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.488420248031616 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.861163217574358
Loss made of: CE 0.11730199307203293, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.563779830932617 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.927957518398762
Loss made of: CE 0.12786823511123657, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6425321102142334 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.977386105060577
Loss made of: CE 0.14159482717514038, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.384614944458008 EntMin 0.0
Epoch 6, Class Loss=0.14435383677482605, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.14435383677482605, Class Loss=0.14435383677482605, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6032492518424988, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.808128
Mean Acc: 0.410725
FreqW Acc: 0.708587
Mean IoU: 0.295630
Class IoU:
	class 0: 0.8694908
	class 1: 0.05190167
	class 2: 0.08164249
	class 3: 0.0
	class 4: 0.50361013
	class 5: 0.0018003653
	class 6: 0.76986206
	class 7: 0.68449795
	class 8: 0.05974736
	class 9: 0.0
	class 10: 0.4615041
	class 11: 0.2908972
	class 12: 0.27110076
	class 13: 0.054718867
	class 14: 0.46357816
	class 15: 0.4613494
	class 16: 0.0
Class Acc:
	class 0: 0.9263736
	class 1: 0.051908363
	class 2: 0.09565414
	class 3: 0.0
	class 4: 0.5270083
	class 5: 0.0018008669
	class 6: 0.9129629
	class 7: 0.75335515
	class 8: 0.059765268
	class 9: 0.0
	class 10: 0.60767823
	class 11: 0.51923597
	class 12: 0.635556
	class 13: 0.054812066
	class 14: 0.92124736
	class 15: 0.91496086
	class 16: 0.0

federated global round: 18, step: 3
select part of clients to conduct local training
[9, 11, 13, 10]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.302133850753307
Loss made of: CE 0.15114417672157288, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.370102643966675 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.953477147221565
Loss made of: CE 0.11437356472015381, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3998756408691406 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.065164874494076
Loss made of: CE 0.19969704747200012, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.998142957687378 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.912515077739954
Loss made of: CE 0.12469889968633652, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.468362808227539 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.017400574684143
Loss made of: CE 0.20380637049674988, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8297715187072754 EntMin 0.0
Epoch 1, Batch 60/102, Loss=3.9581496149301527
Loss made of: CE 0.16957256197929382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0548787117004395 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.007875514775515
Loss made of: CE 0.1350260078907013, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6250853538513184 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.092691256850958
Loss made of: CE 0.14609754085540771, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8115458488464355 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.9898263074457647
Loss made of: CE 0.11930549144744873, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5281455516815186 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.88031315729022
Loss made of: CE 0.15818417072296143, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.551021099090576 EntMin 0.0
Epoch 1, Class Loss=0.14139460027217865, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.14139460027217865, Class Loss=0.14139460027217865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/102, Loss=3.9407769821584226
Loss made of: CE 0.12647899985313416, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.491027593612671 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.202968661487103
Loss made of: CE 0.13975386321544647, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.953859329223633 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 30/102, Loss=4.071287049353122
Loss made of: CE 0.2114306539297104, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7280194759368896 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.1256760157644745
Loss made of: CE 0.13761238753795624, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5773940086364746 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.303999692201614
Loss made of: CE 0.10156378149986267, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.629073619842529 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.1008704349398615
Loss made of: CE 0.10306381434202194, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.862175464630127 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.994420386850834
Loss made of: CE 0.13268521428108215, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.79658579826355 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.0430598393082615
Loss made of: CE 0.1252504289150238, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.545797109603882 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.022255781292915
Loss made of: CE 0.12297897040843964, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.693221092224121 EntMin 0.0
Epoch 2, Batch 100/102, Loss=3.7729493744671343
Loss made of: CE 0.10786034911870956, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5907421112060547 EntMin 0.0
Epoch 2, Class Loss=0.1366667002439499, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.1366667002439499, Class Loss=0.1366667002439499, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/102, Loss=4.033249831944704
Loss made of: CE 0.12314670532941818, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.272952556610107 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.199127108603716
Loss made of: CE 0.1379835158586502, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.149008750915527 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.9333429135382176
Loss made of: CE 0.14514154195785522, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8268017768859863 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.1150498740375046
Loss made of: CE 0.10989992320537567, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6774890422821045 EntMin 0.0
Epoch 3, Batch 50/102, Loss=3.748412986844778
Loss made of: CE 0.1633228361606598, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.600400924682617 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.964877100288868
Loss made of: CE 0.18614010512828827, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.859133243560791 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.276860067993402
Loss made of: CE 0.16497215628623962, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.399133682250977 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.031216780096292
Loss made of: CE 0.14414820075035095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3322935104370117 EntMin 0.0
Epoch 3, Batch 90/102, Loss=3.7740810491144656
Loss made of: CE 0.13240045309066772, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7867345809936523 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.9198643043637276
Loss made of: CE 0.1424037516117096, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.681636333465576 EntMin 0.0
Epoch 3, Class Loss=0.13319213688373566, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.13319213688373566, Class Loss=0.13319213688373566, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/102, Loss=4.071231565624475
Loss made of: CE 0.17658749222755432, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.526812553405762 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.3895245380699635
Loss made of: CE 0.11235842108726501, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.065343856811523 EntMin 0.0
Epoch 4, Batch 30/102, Loss=3.9101843781769277
Loss made of: CE 0.125132218003273, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7691593170166016 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.135013815760613
Loss made of: CE 0.15930819511413574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9888086318969727 EntMin 0.0
Epoch 4, Batch 50/102, Loss=3.9915785431861877
Loss made of: CE 0.09412139654159546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.863971710205078 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.052202585339546
Loss made of: CE 0.08419041335582733, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.175443649291992 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.099823285639286
Loss made of: CE 0.14015142619609833, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.803830623626709 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.021134133636951
Loss made of: CE 0.1422397941350937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.29658317565918 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.146802730858326
Loss made of: CE 0.11978046596050262, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6357421875 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.062494151294231
Loss made of: CE 0.13958075642585754, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.246113300323486 EntMin 0.0
Epoch 4, Class Loss=0.13132190704345703, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.13132190704345703, Class Loss=0.13132190704345703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=3.896212237328291
Loss made of: CE 0.1451624035835266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.170111179351807 EntMin 0.0
Epoch 5, Batch 20/102, Loss=3.77829969599843
Loss made of: CE 0.11876964569091797, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.723801612854004 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.7854278407990933
Loss made of: CE 0.12126396596431732, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.920494556427002 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.139413566142321
Loss made of: CE 0.13379304111003876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6365976333618164 EntMin 0.0
Epoch 5, Batch 50/102, Loss=3.8181412011384963
Loss made of: CE 0.10062193870544434, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4255523681640625 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.950725843012333
Loss made of: CE 0.1561134159564972, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8257205486297607 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.005687741935253
Loss made of: CE 0.14138810336589813, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8303542137145996 EntMin 0.0
Epoch 5, Batch 80/102, Loss=3.9995691552758217
Loss made of: CE 0.09469332545995712, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7386536598205566 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.935453998297453
Loss made of: CE 0.12310698628425598, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2333288192749023 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.934933605790138
Loss made of: CE 0.11788907647132874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3012759685516357 EntMin 0.0
Epoch 5, Class Loss=0.12478350847959518, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.12478350847959518, Class Loss=0.12478350847959518, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/102, Loss=4.026970753073693
Loss made of: CE 0.0993913859128952, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.755955696105957 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.7344399362802507
Loss made of: CE 0.1295417845249176, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6479382514953613 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.7856553196907043
Loss made of: CE 0.13909350335597992, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.338256597518921 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.9326685570180415
Loss made of: CE 0.09988563507795334, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.259390354156494 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.839133092015982
Loss made of: CE 0.09935818612575531, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.754183769226074 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.9808123148977757
Loss made of: CE 0.1026599258184433, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.296802043914795 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.016552956402302
Loss made of: CE 0.13725802302360535, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.164565563201904 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.8042184576392173
Loss made of: CE 0.12346505373716354, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7792890071868896 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.7710978753864763
Loss made of: CE 0.13750898838043213, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6569228172302246 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.6718155778944492
Loss made of: CE 0.09620583057403564, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.209102153778076 EntMin 0.0
Epoch 6, Class Loss=0.1226426512002945, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.1226426512002945, Class Loss=0.1226426512002945, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000438
Epoch 1, Batch 10/102, Loss=3.925003547221422
Loss made of: CE 0.15797476470470428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.489402770996094 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/102, Loss=3.716436930000782
Loss made of: CE 0.16214686632156372, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.514758825302124 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.7675348684191703
Loss made of: CE 0.13079391419887543, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9414069652557373 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.9972472675144672
Loss made of: CE 0.1267080307006836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6739416122436523 EntMin 0.0
Epoch 1, Batch 50/102, Loss=3.8869381345808507
Loss made of: CE 0.10418996214866638, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6255531311035156 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.007398404926062
Loss made of: CE 0.11751578748226166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2538163661956787 EntMin 0.0
Epoch 1, Batch 70/102, Loss=3.7012165389955043
Loss made of: CE 0.13754847645759583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6709420680999756 EntMin 0.0
Epoch 1, Batch 80/102, Loss=3.8280507922172546
Loss made of: CE 0.14011675119400024, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7996158599853516 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.649365232884884
Loss made of: CE 0.10092215240001678, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.239349842071533 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.6743222802877424
Loss made of: CE 0.12936946749687195, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4967410564422607 EntMin 0.0
Epoch 1, Class Loss=0.14254650473594666, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.14254650473594666, Class Loss=0.14254650473594666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000405
Epoch 2, Batch 10/102, Loss=3.6736022837460043
Loss made of: CE 0.12010689824819565, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1636104583740234 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.6263717032968996
Loss made of: CE 0.13967707753181458, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.476425886154175 EntMin 0.0
Epoch 2, Batch 30/102, Loss=3.7404820397496223
Loss made of: CE 0.15052025020122528, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4671542644500732 EntMin 0.0
Epoch 2, Batch 40/102, Loss=3.8068111456930636
Loss made of: CE 0.13409316539764404, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3783700466156006 EntMin 0.0
Epoch 2, Batch 50/102, Loss=3.8062770552933216
Loss made of: CE 0.09053352475166321, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4890074729919434 EntMin 0.0
Epoch 2, Batch 60/102, Loss=3.887790483981371
Loss made of: CE 0.1301344633102417, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6439406871795654 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.5225448079407213
Loss made of: CE 0.11725427955389023, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2156968116760254 EntMin 0.0
Epoch 2, Batch 80/102, Loss=3.8076611414551733
Loss made of: CE 0.11075124144554138, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8277125358581543 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.758926721662283
Loss made of: CE 0.20629291236400604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389988899230957 EntMin 0.0
Epoch 2, Batch 100/102, Loss=3.811226283013821
Loss made of: CE 0.12948279082775116, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7835512161254883 EntMin 0.0
Epoch 2, Class Loss=0.13835807144641876, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.13835807144641876, Class Loss=0.13835807144641876, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/102, Loss=3.736882983148098
Loss made of: CE 0.13171164691448212, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5833547115325928 EntMin 0.0
Epoch 3, Batch 20/102, Loss=3.689388320595026
Loss made of: CE 0.14572308957576752, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2376036643981934 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.806405559182167
Loss made of: CE 0.13289767503738403, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.394464492797852 EntMin 0.0
Epoch 3, Batch 40/102, Loss=3.740325861424208
Loss made of: CE 0.09275224059820175, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6727826595306396 EntMin 0.0
Epoch 3, Batch 50/102, Loss=3.9293183490633963
Loss made of: CE 0.13563044369220734, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3204493522644043 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.7462399639189243
Loss made of: CE 0.11911609023809433, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4236295223236084 EntMin 0.0
Epoch 3, Batch 70/102, Loss=3.716811553388834
Loss made of: CE 0.12313922494649887, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7993874549865723 EntMin 0.0
Epoch 3, Batch 80/102, Loss=3.8398276694118976
Loss made of: CE 0.13673001527786255, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.877622127532959 EntMin 0.0
Epoch 3, Batch 90/102, Loss=3.8925718404352665
Loss made of: CE 0.1333085298538208, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2939069271087646 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.7124119460582734
Loss made of: CE 0.10485127568244934, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1869943141937256 EntMin 0.0
Epoch 3, Class Loss=0.13786832988262177, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.13786832988262177, Class Loss=0.13786832988262177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=3.621026074141264
Loss made of: CE 0.1472802460193634, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.340169668197632 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.824910843372345
Loss made of: CE 0.12317874282598495, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5913619995117188 EntMin 0.0
Epoch 4, Batch 30/102, Loss=3.826055956631899
Loss made of: CE 0.16975915431976318, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7167553901672363 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.7080790773034096
Loss made of: CE 0.11889469623565674, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0656723976135254 EntMin 0.0
Epoch 4, Batch 50/102, Loss=3.833951427787542
Loss made of: CE 0.14320817589759827, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.383946180343628 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.004964090883732
Loss made of: CE 0.14824669063091278, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9139304161071777 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 70/102, Loss=3.8029748998582362
Loss made of: CE 0.14748045802116394, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6050801277160645 EntMin 0.0
Epoch 4, Batch 80/102, Loss=3.8070551462471487
Loss made of: CE 0.1563880890607834, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.39744234085083 EntMin 0.0
Epoch 4, Batch 90/102, Loss=3.8673896215856076
Loss made of: CE 0.10904863476753235, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.448570966720581 EntMin 0.0
Epoch 4, Batch 100/102, Loss=3.661120367050171
Loss made of: CE 0.16489452123641968, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.040510177612305 EntMin 0.0
Epoch 4, Class Loss=0.1361570805311203, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.1361570805311203, Class Loss=0.1361570805311203, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/102, Loss=3.9195349462330342
Loss made of: CE 0.09336547553539276, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.421736240386963 EntMin 0.0
Epoch 5, Batch 20/102, Loss=3.885637443512678
Loss made of: CE 0.08981090784072876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.620797872543335 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.7534399405121803
Loss made of: CE 0.13473808765411377, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.147547721862793 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.572137492895126
Loss made of: CE 0.14906719326972961, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.316706418991089 EntMin 0.0
Epoch 5, Batch 50/102, Loss=3.7769622661173345
Loss made of: CE 0.13934610784053802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2641167640686035 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.506610179692507
Loss made of: CE 0.13315804302692413, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.549160957336426 EntMin 0.0
Epoch 5, Batch 70/102, Loss=3.5680727824568748
Loss made of: CE 0.1480262726545334, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.201442241668701 EntMin 0.0
Epoch 5, Batch 80/102, Loss=3.6576973222196103
Loss made of: CE 0.11522971093654633, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7128865718841553 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.690501930564642
Loss made of: CE 0.11351531744003296, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.079505681991577 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.681383688002825
Loss made of: CE 0.13276346027851105, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8627562522888184 EntMin 0.0
Epoch 5, Class Loss=0.13396404683589935, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.13396404683589935, Class Loss=0.13396404683589935, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000270
Epoch 6, Batch 10/102, Loss=3.8885799683630466
Loss made of: CE 0.14787033200263977, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2861125469207764 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.566260167956352
Loss made of: CE 0.10379678010940552, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4248294830322266 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.5030778750777243
Loss made of: CE 0.12096817791461945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8161978721618652 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.8675229460000993
Loss made of: CE 0.18230514228343964, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.813096761703491 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.5865516901016234
Loss made of: CE 0.13202115893363953, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.229537010192871 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.7140272513031958
Loss made of: CE 0.14660021662712097, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.451962947845459 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.470899010449648
Loss made of: CE 0.12054787576198578, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.239527463912964 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.6582364037632944
Loss made of: CE 0.12315531820058823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.395414113998413 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.6247952453792096
Loss made of: CE 0.11062901467084885, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2089040279388428 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.811099097132683
Loss made of: CE 0.14115744829177856, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.383561849594116 EntMin 0.0
Epoch 6, Class Loss=0.13189765810966492, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.13189765810966492, Class Loss=0.13189765810966492, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=4.947151210904122
Loss made of: CE 0.4345822334289551, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8495254516601562 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.776608633995056
Loss made of: CE 0.4084749221801758, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9744889736175537 EntMin 0.0
Epoch 1, Class Loss=0.44549810886383057, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.44549810886383057, Class Loss=0.44549810886383057, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/23, Loss=4.501474252343177
Loss made of: CE 0.4191625118255615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.200237274169922 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.569607234001159
Loss made of: CE 0.3492635190486908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0337629318237305 EntMin 0.0
Epoch 2, Class Loss=0.35961607098579407, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.35961607098579407, Class Loss=0.35961607098579407, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/23, Loss=4.195052398741245
Loss made of: CE 0.422255277633667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.428889751434326 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.267872643470764
Loss made of: CE 0.3293014466762543, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726810455322266 EntMin 0.0
Epoch 3, Class Loss=0.2991185486316681, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.2991185486316681, Class Loss=0.2991185486316681, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/23, Loss=3.9988523706793786
Loss made of: CE 0.2872990071773529, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.777207374572754 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.3290942564606665
Loss made of: CE 0.21121138334274292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.512515068054199 EntMin 0.0
Epoch 4, Class Loss=0.25740158557891846, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.25740158557891846, Class Loss=0.25740158557891846, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/23, Loss=4.1138874053955075
Loss made of: CE 0.22076216340065002, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6193013191223145 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.029220728576183
Loss made of: CE 0.1787222921848297, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3409030437469482 EntMin 0.0
Epoch 5, Class Loss=0.25203680992126465, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.25203680992126465, Class Loss=0.25203680992126465, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/23, Loss=4.004679603874683
Loss made of: CE 0.31968075037002563, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.087060928344727 EntMin 0.0
Epoch 6, Batch 20/23, Loss=3.954476797580719
Loss made of: CE 0.16289180517196655, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9956395626068115 EntMin 0.0
Epoch 6, Class Loss=0.2352197915315628, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.2352197915315628, Class Loss=0.2352197915315628, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/23, Loss=4.990268838405609
Loss made of: CE 0.3419880270957947, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.130845069885254 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.400592625141144
Loss made of: CE 0.535662055015564, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.085450172424316 EntMin 0.0
Epoch 1, Class Loss=0.3504348397254944, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.3504348397254944, Class Loss=0.3504348397254944, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/23, Loss=4.305638444423676
Loss made of: CE 0.23413655161857605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.392016410827637 EntMin 0.0
Epoch 2, Batch 20/23, Loss=3.9771035090088844
Loss made of: CE 0.2644197344779968, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9275319576263428 EntMin 0.0
Epoch 2, Class Loss=0.2988434135913849, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.2988434135913849, Class Loss=0.2988434135913849, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/23, Loss=4.008111084997654
Loss made of: CE 0.21212992072105408, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.975797653198242 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.045126558840275
Loss made of: CE 0.16132454574108124, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9534738063812256 EntMin 0.0
Epoch 3, Class Loss=0.27178341150283813, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.27178341150283813, Class Loss=0.27178341150283813, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/23, Loss=3.8418791204690934
Loss made of: CE 0.24508705735206604, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8677239418029785 EntMin 0.0
Epoch 4, Batch 20/23, Loss=3.9703874945640565
Loss made of: CE 0.37300437688827515, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5859761238098145 EntMin 0.0
Epoch 4, Class Loss=0.23753854632377625, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.23753854632377625, Class Loss=0.23753854632377625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/23, Loss=4.070987704396248
Loss made of: CE 0.23765620589256287, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5027198791503906 EntMin 0.0
Epoch 5, Batch 20/23, Loss=3.875411978363991
Loss made of: CE 0.34498995542526245, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.047804832458496 EntMin 0.0
Epoch 5, Class Loss=0.24519293010234833, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.24519293010234833, Class Loss=0.24519293010234833, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/23, Loss=3.8670271024107934
Loss made of: CE 0.19028513133525848, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6963603496551514 EntMin 0.0
Epoch 6, Batch 20/23, Loss=3.8724826231598852
Loss made of: CE 0.25259098410606384, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.691058874130249 EntMin 0.0
Epoch 6, Class Loss=0.23843272030353546, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.23843272030353546, Class Loss=0.23843272030353546, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5886496901512146, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.811502
Mean Acc: 0.428800
FreqW Acc: 0.714783
Mean IoU: 0.307114
Class IoU:
	class 0: 0.8715085
	class 1: 0.07144332
	class 2: 0.06884703
	class 3: 0.0
	class 4: 0.5287209
	class 5: 0.002063441
	class 6: 0.76925004
	class 7: 0.67502373
	class 8: 0.08484873
	class 9: 0.0
	class 10: 0.25900248
	class 11: 0.28751892
	class 12: 0.29437304
	class 13: 0.37570935
	class 14: 0.44504985
	class 15: 0.48758444
	class 16: 0.0
Class Acc:
	class 0: 0.9224343
	class 1: 0.07145708
	class 2: 0.079486966
	class 3: 0.0
	class 4: 0.55631864
	class 5: 0.002063832
	class 6: 0.91015786
	class 7: 0.7293189
	class 8: 0.084909365
	class 9: 0.0
	class 10: 0.26789048
	class 11: 0.53973836
	class 12: 0.7208568
	class 13: 0.55656815
	class 14: 0.9265728
	class 15: 0.92183024
	class 16: 0.0

federated global round: 19, step: 3
select part of clients to conduct local training
[4, 7, 17, 15]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/105, Loss=4.312282872945071
Loss made of: CE 0.23617121577262878, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.931426048278809 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.059229054301977
Loss made of: CE 0.22782957553863525, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.849726676940918 EntMin 0.0
Epoch 1, Batch 30/105, Loss=3.947592454403639
Loss made of: CE 0.20035070180892944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.272299289703369 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 40/105, Loss=3.8184687346220016
Loss made of: CE 0.15204192698001862, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4951181411743164 EntMin 0.0
Epoch 1, Batch 50/105, Loss=3.6618957474827765
Loss made of: CE 0.12208996713161469, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3048343658447266 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.167615331709385
Loss made of: CE 0.15460969507694244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4037673473358154 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.038649482280016
Loss made of: CE 0.09667154401540756, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.58423113822937 EntMin 0.0
Epoch 1, Batch 80/105, Loss=3.943077354878187
Loss made of: CE 0.11020883172750473, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.540879726409912 EntMin 0.0
Epoch 1, Batch 90/105, Loss=3.9822967782616616
Loss made of: CE 0.08367395401000977, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.480686664581299 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.034163853526115
Loss made of: CE 0.11524587124586105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.101659297943115 EntMin 0.0
Epoch 1, Class Loss=0.16597230732440948, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.16597230732440948, Class Loss=0.16597230732440948, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/105, Loss=4.00568234398961
Loss made of: CE 0.15521863102912903, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3918519020080566 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.16011858060956
Loss made of: CE 0.10332533717155457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.968759059906006 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.143767736107111
Loss made of: CE 0.11173202842473984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.295836448669434 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.038727241754532
Loss made of: CE 0.1626371443271637, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.763573169708252 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.10238840430975
Loss made of: CE 0.15060710906982422, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7925281524658203 EntMin 0.0
Epoch 2, Batch 60/105, Loss=4.06434613019228
Loss made of: CE 0.14141348004341125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.933530807495117 EntMin 0.0
Epoch 2, Batch 70/105, Loss=4.0179319024086
Loss made of: CE 0.12522560358047485, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.926353931427002 EntMin 0.0
Epoch 2, Batch 80/105, Loss=3.937372052669525
Loss made of: CE 0.1359667181968689, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5560522079467773 EntMin 0.0
Epoch 2, Batch 90/105, Loss=3.9111552871763706
Loss made of: CE 0.1193312406539917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5389018058776855 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.137888580560684
Loss made of: CE 0.147213876247406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.973102331161499 EntMin 0.0
Epoch 2, Class Loss=0.15629872679710388, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.15629872679710388, Class Loss=0.15629872679710388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/105, Loss=4.246255588531494
Loss made of: CE 0.1748048961162567, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.632458448410034 EntMin 0.0
Epoch 3, Batch 20/105, Loss=3.7183690957725046
Loss made of: CE 0.13814973831176758, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.505589246749878 EntMin 0.0
Epoch 3, Batch 30/105, Loss=3.9391433753073217
Loss made of: CE 0.12820187211036682, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5243372917175293 EntMin 0.0
Epoch 3, Batch 40/105, Loss=3.9867626667022704
Loss made of: CE 0.16583876311779022, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.047938346862793 EntMin 0.0
Epoch 3, Batch 50/105, Loss=3.8484913550317286
Loss made of: CE 0.17093098163604736, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5135698318481445 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.063945020735264
Loss made of: CE 0.10372087359428406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4272613525390625 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.080780366063118
Loss made of: CE 0.24332958459854126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9714620113372803 EntMin 0.0
Epoch 3, Batch 80/105, Loss=3.9115581274032594
Loss made of: CE 0.09593257308006287, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0515706539154053 EntMin 0.0
Epoch 3, Batch 90/105, Loss=3.8109298303723333
Loss made of: CE 0.13994525372982025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7188363075256348 EntMin 0.0
Epoch 3, Batch 100/105, Loss=3.9020812712609767
Loss made of: CE 0.11708535999059677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.329491138458252 EntMin 0.0
Epoch 3, Class Loss=0.1488324999809265, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.1488324999809265, Class Loss=0.1488324999809265, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/105, Loss=3.9328056678175924
Loss made of: CE 0.14834681153297424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.307928562164307 EntMin 0.0
Epoch 4, Batch 20/105, Loss=3.932410540431738
Loss made of: CE 0.16414090991020203, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.573068380355835 EntMin 0.0
Epoch 4, Batch 30/105, Loss=3.898385048657656
Loss made of: CE 0.11230602115392685, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7166662216186523 EntMin 0.0
Epoch 4, Batch 40/105, Loss=3.817195392400026
Loss made of: CE 0.18675833940505981, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.484711647033691 EntMin 0.0
Epoch 4, Batch 50/105, Loss=3.8078856840729713
Loss made of: CE 0.20513437688350677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3701999187469482 EntMin 0.0
Epoch 4, Batch 60/105, Loss=3.80111196115613
Loss made of: CE 0.18712186813354492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9200005531311035 EntMin 0.0
Epoch 4, Batch 70/105, Loss=3.666467010974884
Loss made of: CE 0.1455097496509552, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1694822311401367 EntMin 0.0
Epoch 4, Batch 80/105, Loss=3.9977309435606
Loss made of: CE 0.13336478173732758, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.047868728637695 EntMin 0.0
Epoch 4, Batch 90/105, Loss=3.9689534649252893
Loss made of: CE 0.13364118337631226, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.426795721054077 EntMin 0.0
Epoch 4, Batch 100/105, Loss=3.641050487756729
Loss made of: CE 0.14879778027534485, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0875039100646973 EntMin 0.0
Epoch 4, Class Loss=0.14604401588439941, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.14604401588439941, Class Loss=0.14604401588439941, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/105, Loss=3.7530584007501604
Loss made of: CE 0.15190637111663818, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.539421558380127 EntMin 0.0
Epoch 5, Batch 20/105, Loss=3.828384503722191
Loss made of: CE 0.1863098442554474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.403189659118652 EntMin 0.0
Epoch 5, Batch 30/105, Loss=3.7051216669380667
Loss made of: CE 0.14138701558113098, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.619089126586914 EntMin 0.0
Epoch 5, Batch 40/105, Loss=3.9149450682103635
Loss made of: CE 0.12924335896968842, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4629111289978027 EntMin 0.0
Epoch 5, Batch 50/105, Loss=3.617088333517313
Loss made of: CE 0.10996236652135849, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1791770458221436 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.116877067089081
Loss made of: CE 0.1490173190832138, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2721781730651855 EntMin 0.0
Epoch 5, Batch 70/105, Loss=3.785618583858013
Loss made of: CE 0.15915779769420624, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.626132965087891 EntMin 0.0
Epoch 5, Batch 80/105, Loss=3.745980780571699
Loss made of: CE 0.11552710831165314, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6564579010009766 EntMin 0.0
Epoch 5, Batch 90/105, Loss=3.7158454306423665
Loss made of: CE 0.10001190751791, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0914125442504883 EntMin 0.0
Epoch 5, Batch 100/105, Loss=3.9006877809762956
Loss made of: CE 0.15724772214889526, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.530870199203491 EntMin 0.0
Epoch 5, Class Loss=0.1447555273771286, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.1447555273771286, Class Loss=0.1447555273771286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/105, Loss=4.011336119472981
Loss made of: CE 0.14694654941558838, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.290454626083374 EntMin 0.0
Epoch 6, Batch 20/105, Loss=3.649773275107145
Loss made of: CE 0.11804120242595673, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.321560859680176 EntMin 0.0
Epoch 6, Batch 30/105, Loss=3.587504951655865
Loss made of: CE 0.10761559009552002, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.884209156036377 EntMin 0.0
Epoch 6, Batch 40/105, Loss=3.8490959502756596
Loss made of: CE 0.1228756308555603, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0199456214904785 EntMin 0.0
Epoch 6, Batch 50/105, Loss=3.8985018841922283
Loss made of: CE 0.14516869187355042, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.125595569610596 EntMin 0.0
Epoch 6, Batch 60/105, Loss=3.6652819491922854
Loss made of: CE 0.14518606662750244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.635948896408081 EntMin 0.0
Epoch 6, Batch 70/105, Loss=3.576626816391945
Loss made of: CE 0.1103416383266449, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2748000621795654 EntMin 0.0
Epoch 6, Batch 80/105, Loss=3.7771421179175375
Loss made of: CE 0.12226930260658264, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.464522361755371 EntMin 0.0
Epoch 6, Batch 90/105, Loss=3.5960959285497665
Loss made of: CE 0.16842977702617645, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.58162784576416 EntMin 0.0
Epoch 6, Batch 100/105, Loss=3.5095701359212397
Loss made of: CE 0.1249459981918335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.254889965057373 EntMin 0.0
Epoch 6, Class Loss=0.14205855131149292, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.14205855131149292, Class Loss=0.14205855131149292, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=3.849823770672083
Loss made of: CE 0.1338469535112381, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4611287117004395 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.9774032294750215
Loss made of: CE 0.1423601359128952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.033751487731934 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.9137945331633093
Loss made of: CE 0.11865034699440002, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.05769157409668 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.8688531130552293
Loss made of: CE 0.12152554094791412, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.696941375732422 EntMin 0.0
Epoch 1, Batch 50/102, Loss=3.936323606967926
Loss made of: CE 0.1730547845363617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.156758785247803 EntMin 0.0
Epoch 1, Batch 60/102, Loss=3.9380322486162185
Loss made of: CE 0.1391434222459793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.410369396209717 EntMin 0.0
Epoch 1, Batch 70/102, Loss=3.9576404497027395
Loss made of: CE 0.13293161988258362, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8687586784362793 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 80/102, Loss=3.906482153385878
Loss made of: CE 0.1343419849872589, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4434456825256348 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.4810113340616224
Loss made of: CE 0.1157383844256401, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1059188842773438 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.6640611119568347
Loss made of: CE 0.14979247748851776, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4263617992401123 EntMin 0.0
Epoch 1, Class Loss=0.13606083393096924, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.13606083393096924, Class Loss=0.13606083393096924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/102, Loss=3.861913011223078
Loss made of: CE 0.11668920516967773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.331153631210327 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.6711836978793144
Loss made of: CE 0.1501639187335968, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8338217735290527 EntMin 0.0
Epoch 2, Batch 30/102, Loss=3.9175945058465005
Loss made of: CE 0.1240597814321518, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481184959411621 EntMin 0.0
Epoch 2, Batch 40/102, Loss=3.8711693942546845
Loss made of: CE 0.15124213695526123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.107800483703613 EntMin 0.0
Epoch 2, Batch 50/102, Loss=3.6009754866361616
Loss made of: CE 0.1133369654417038, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3185250759124756 EntMin 0.0
Epoch 2, Batch 60/102, Loss=3.726243010908365
Loss made of: CE 0.13247522711753845, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7349600791931152 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.07101018205285
Loss made of: CE 0.14605550467967987, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9188618659973145 EntMin 0.0
Epoch 2, Batch 80/102, Loss=3.6616369113326073
Loss made of: CE 0.11190740764141083, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2162036895751953 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.7200195215642453
Loss made of: CE 0.13144466280937195, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3836257457733154 EntMin 0.0
Epoch 2, Batch 100/102, Loss=3.9005629748106
Loss made of: CE 0.12893179059028625, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.091104030609131 EntMin 0.0
Epoch 2, Class Loss=0.13050265610218048, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.13050265610218048, Class Loss=0.13050265610218048, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/102, Loss=4.031448747962713
Loss made of: CE 0.12479911744594574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.275388479232788 EntMin 0.0
Epoch 3, Batch 20/102, Loss=3.806140137463808
Loss made of: CE 0.12458895146846771, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.9222378730773926 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.669345097988844
Loss made of: CE 0.12300972640514374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.535343885421753 EntMin 0.0
Epoch 3, Batch 40/102, Loss=3.707735588401556
Loss made of: CE 0.105533167719841, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9515128135681152 EntMin 0.0
Epoch 3, Batch 50/102, Loss=3.8094197176396847
Loss made of: CE 0.14585238695144653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249431610107422 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.630375950783491
Loss made of: CE 0.13120141625404358, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2365469932556152 EntMin 0.0
Epoch 3, Batch 70/102, Loss=3.8108962081372737
Loss made of: CE 0.10539206117391586, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.511366844177246 EntMin 0.0
Epoch 3, Batch 80/102, Loss=3.6320360474288464
Loss made of: CE 0.08728860318660736, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.462088108062744 EntMin 0.0
Epoch 3, Batch 90/102, Loss=3.7121811904013158
Loss made of: CE 0.13593393564224243, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1919572353363037 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.555565758794546
Loss made of: CE 0.12633901834487915, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3290653228759766 EntMin 0.0
Epoch 3, Class Loss=0.12795546650886536, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.12795546650886536, Class Loss=0.12795546650886536, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=3.818779066950083
Loss made of: CE 0.13628119230270386, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5417487621307373 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.6368014298379423
Loss made of: CE 0.13340188562870026, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.514026641845703 EntMin 0.0
Epoch 4, Batch 30/102, Loss=3.671781037002802
Loss made of: CE 0.09206587076187134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0726122856140137 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.7092646040022372
Loss made of: CE 0.12011679261922836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.425377607345581 EntMin 0.0
Epoch 4, Batch 50/102, Loss=3.602864932268858
Loss made of: CE 0.11444948613643646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.059587001800537 EntMin 0.0
Epoch 4, Batch 60/102, Loss=3.793075369298458
Loss made of: CE 0.1266418993473053, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5022826194763184 EntMin 0.0
Epoch 4, Batch 70/102, Loss=3.658538907021284
Loss made of: CE 0.14476333558559418, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4867000579833984 EntMin 0.0
Epoch 4, Batch 80/102, Loss=3.739446214586496
Loss made of: CE 0.12299561500549316, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4842867851257324 EntMin 0.0
Epoch 4, Batch 90/102, Loss=3.7650765664875507
Loss made of: CE 0.14551332592964172, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9058923721313477 EntMin 0.0
Epoch 4, Batch 100/102, Loss=3.9022931665182115
Loss made of: CE 0.09416868537664413, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.303851842880249 EntMin 0.0
Epoch 4, Class Loss=0.12745212018489838, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.12745212018489838, Class Loss=0.12745212018489838, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/102, Loss=3.5407335884869098
Loss made of: CE 0.14276745915412903, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.474116086959839 EntMin 0.0
Epoch 5, Batch 20/102, Loss=3.7774739608168604
Loss made of: CE 0.14977344870567322, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7995667457580566 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.8869508035480975
Loss made of: CE 0.12603287398815155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.631533622741699 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.716813928633928
Loss made of: CE 0.09759369492530823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.14553165435791 EntMin 0.0
Epoch 5, Batch 50/102, Loss=3.602648974210024
Loss made of: CE 0.11615075916051865, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6620864868164062 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.6560948722064497
Loss made of: CE 0.1045520231127739, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.007668972015381 EntMin 0.0
Epoch 5, Batch 70/102, Loss=3.7739304415881634
Loss made of: CE 0.12046730518341064, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7066190242767334 EntMin 0.0
Epoch 5, Batch 80/102, Loss=3.797592366486788
Loss made of: CE 0.10909073799848557, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.425471305847168 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.5420751214027404
Loss made of: CE 0.1739327609539032, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.463984251022339 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.6358988247811794
Loss made of: CE 0.1524752825498581, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.271946907043457 EntMin 0.0
Epoch 5, Class Loss=0.12647037208080292, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.12647037208080292, Class Loss=0.12647037208080292, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/102, Loss=3.648459865152836
Loss made of: CE 0.12078376859426498, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0895748138427734 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.851930444687605
Loss made of: CE 0.11076664924621582, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.8091650009155273 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.3821958273649217
Loss made of: CE 0.11399227380752563, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.178879499435425 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.586271906644106
Loss made of: CE 0.12671534717082977, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1372179985046387 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.6855005003511905
Loss made of: CE 0.1420641392469406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.493830680847168 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.5981290407478808
Loss made of: CE 0.12591879069805145, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3208367824554443 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.6690256759524345
Loss made of: CE 0.11356300115585327, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.625084400177002 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.5211124554276467
Loss made of: CE 0.1095866858959198, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.546346664428711 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.6458892725408076
Loss made of: CE 0.10753849148750305, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.25114107131958 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.605788756161928
Loss made of: CE 0.16488057374954224, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.9009006023406982 EntMin 0.0
Epoch 6, Class Loss=0.12527181208133698, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.12527181208133698, Class Loss=0.12527181208133698, Reg Loss=0.0
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/23, Loss=4.871254052221775
Loss made of: CE 0.4339629113674164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.309640407562256 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.528082296252251
Loss made of: CE 0.31660765409469604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.065855979919434 EntMin 0.0
Epoch 1, Class Loss=0.35458213090896606, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.35458213090896606, Class Loss=0.35458213090896606, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/23, Loss=4.422414806485176
Loss made of: CE 0.4394253194332123, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.943957805633545 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.220755307376384
Loss made of: CE 0.1902059018611908, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8689115047454834 EntMin 0.0
Epoch 2, Class Loss=0.3059903383255005, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.3059903383255005, Class Loss=0.3059903383255005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/23, Loss=4.160461191833019
Loss made of: CE 0.1749468445777893, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6147000789642334 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.275894263386727
Loss made of: CE 0.38704079389572144, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.448702812194824 EntMin 0.0
Epoch 3, Class Loss=0.28517040610313416, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.28517040610313416, Class Loss=0.28517040610313416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/23, Loss=4.288100968301296
Loss made of: CE 0.3497099280357361, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6358344554901123 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.104583235085011
Loss made of: CE 0.19772927463054657, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.812854290008545 EntMin 0.0
Epoch 4, Class Loss=0.2689194679260254, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.2689194679260254, Class Loss=0.2689194679260254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/23, Loss=4.195679475367069
Loss made of: CE 0.22061030566692352, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.112274646759033 EntMin 0.0
Epoch 5, Batch 20/23, Loss=3.9935577124357224
Loss made of: CE 0.26137790083885193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.02144718170166 EntMin 0.0
Epoch 5, Class Loss=0.258588969707489, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.258588969707489, Class Loss=0.258588969707489, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/23, Loss=4.127055965363979
Loss made of: CE 0.2158108800649643, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.807908535003662 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.014545142650604
Loss made of: CE 0.2872821092605591, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.422101974487305 EntMin 0.0
Epoch 6, Class Loss=0.2566636800765991, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.2566636800765991, Class Loss=0.2566636800765991, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/103, Loss=3.6266774125397205
Loss made of: CE 0.16002845764160156, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5352535247802734 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/103, Loss=3.7045013904571533
Loss made of: CE 0.13882839679718018, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.469846725463867 EntMin 0.0
Epoch 1, Batch 30/103, Loss=3.8125075578689573
Loss made of: CE 0.13495877385139465, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6258456707000732 EntMin 0.0
Epoch 1, Batch 40/103, Loss=4.067529256641865
Loss made of: CE 0.129644513130188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3038082122802734 EntMin 0.0
Epoch 1, Batch 50/103, Loss=4.198653198778629
Loss made of: CE 0.21614836156368256, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.842797756195068 EntMin 0.0
Epoch 1, Batch 60/103, Loss=3.9323416501283646
Loss made of: CE 0.11922112107276917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8940324783325195 EntMin 0.0
Epoch 1, Batch 70/103, Loss=3.944409032911062
Loss made of: CE 0.10681270807981491, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.763634204864502 EntMin 0.0
Epoch 1, Batch 80/103, Loss=4.18914133682847
Loss made of: CE 0.16268083453178406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427356719970703 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/103, Loss=3.8286915495991707
Loss made of: CE 0.13423705101013184, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1807217597961426 EntMin 0.0
Epoch 1, Batch 100/103, Loss=4.020138303935528
Loss made of: CE 0.14555716514587402, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.633840322494507 EntMin 0.0
Epoch 1, Class Loss=0.14308813214302063, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.14308813214302063, Class Loss=0.14308813214302063, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/103, Loss=3.8761231690645217
Loss made of: CE 0.12270879745483398, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.510554313659668 EntMin 0.0
Epoch 2, Batch 20/103, Loss=4.10583783313632
Loss made of: CE 0.1463126540184021, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6062660217285156 EntMin 0.0
Epoch 2, Batch 30/103, Loss=3.9098193898797033
Loss made of: CE 0.12854866683483124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.170968055725098 EntMin 0.0
Epoch 2, Batch 40/103, Loss=4.081029916554689
Loss made of: CE 0.18981538712978363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.118758678436279 EntMin 0.0
Epoch 2, Batch 50/103, Loss=3.783193293213844
Loss made of: CE 0.09864532947540283, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3110227584838867 EntMin 0.0
Epoch 2, Batch 60/103, Loss=4.141281791776419
Loss made of: CE 0.08556487411260605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.184865474700928 EntMin 0.0
Epoch 2, Batch 70/103, Loss=3.9441679447889326
Loss made of: CE 0.15532803535461426, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.812558174133301 EntMin 0.0
Epoch 2, Batch 80/103, Loss=3.8968562938272955
Loss made of: CE 0.1481168568134308, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.266348361968994 EntMin 0.0
Epoch 2, Batch 90/103, Loss=4.106082773208618
Loss made of: CE 0.14930900931358337, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1684489250183105 EntMin 0.0
Epoch 2, Batch 100/103, Loss=4.294844676554203
Loss made of: CE 0.135991632938385, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.049316883087158 EntMin 0.0
Epoch 2, Class Loss=0.13271623849868774, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.13271623849868774, Class Loss=0.13271623849868774, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/103, Loss=3.981079459935427
Loss made of: CE 0.11403524875640869, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5383048057556152 EntMin 0.0
Epoch 3, Batch 20/103, Loss=3.9033235393464567
Loss made of: CE 0.1500244438648224, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7376389503479004 EntMin 0.0
Epoch 3, Batch 30/103, Loss=3.9243367038667203
Loss made of: CE 0.1864948272705078, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.347851276397705 EntMin 0.0
Epoch 3, Batch 40/103, Loss=3.9999478437006473
Loss made of: CE 0.13126975297927856, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.615334987640381 EntMin 0.0
Epoch 3, Batch 50/103, Loss=4.148538211733102
Loss made of: CE 0.15691283345222473, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.491552829742432 EntMin 0.0
Epoch 3, Batch 60/103, Loss=3.89976160004735
Loss made of: CE 0.11052238196134567, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5326735973358154 EntMin 0.0
Epoch 3, Batch 70/103, Loss=4.025309492647648
Loss made of: CE 0.14725974202156067, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.497313499450684 EntMin 0.0
Epoch 3, Batch 80/103, Loss=3.942204013466835
Loss made of: CE 0.11560869216918945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.671006202697754 EntMin 0.0
Epoch 3, Batch 90/103, Loss=3.917693307250738
Loss made of: CE 0.11070813983678818, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.412843704223633 EntMin 0.0
Epoch 3, Batch 100/103, Loss=3.8913996294140816
Loss made of: CE 0.13038744032382965, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.213781356811523 EntMin 0.0
Epoch 3, Class Loss=0.12932148575782776, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.12932148575782776, Class Loss=0.12932148575782776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/103, Loss=3.8510180480778216
Loss made of: CE 0.1374431848526001, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3384881019592285 EntMin 0.0
Epoch 4, Batch 20/103, Loss=3.932201011478901
Loss made of: CE 0.17740032076835632, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.54082727432251 EntMin 0.0
Epoch 4, Batch 30/103, Loss=3.8694930389523505
Loss made of: CE 0.12408660352230072, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.271771430969238 EntMin 0.0
Epoch 4, Batch 40/103, Loss=3.646639956533909
Loss made of: CE 0.11797942966222763, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4385480880737305 EntMin 0.0
Epoch 4, Batch 50/103, Loss=3.820643576234579
Loss made of: CE 0.12934711575508118, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.947889804840088 EntMin 0.0
Epoch 4, Batch 60/103, Loss=3.8278324358165263
Loss made of: CE 0.1180461049079895, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6192569732666016 EntMin 0.0
Epoch 4, Batch 70/103, Loss=3.7231851115822794
Loss made of: CE 0.13270333409309387, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7984237670898438 EntMin 0.0
Epoch 4, Batch 80/103, Loss=3.8625735312700273
Loss made of: CE 0.12183591723442078, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.290482997894287 EntMin 0.0
Epoch 4, Batch 90/103, Loss=3.6490772388875485
Loss made of: CE 0.13102000951766968, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5687148571014404 EntMin 0.0
Epoch 4, Batch 100/103, Loss=3.965099810808897
Loss made of: CE 0.12523052096366882, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4209001064300537 EntMin 0.0
Epoch 4, Class Loss=0.12691786885261536, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.12691786885261536, Class Loss=0.12691786885261536, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/103, Loss=3.856183236092329
Loss made of: CE 0.12034134566783905, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6710710525512695 EntMin 0.0
Epoch 5, Batch 20/103, Loss=4.004701532423496
Loss made of: CE 0.12716355919837952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.895703315734863 EntMin 0.0
Epoch 5, Batch 30/103, Loss=3.6928811818361282
Loss made of: CE 0.14423702657222748, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.248468399047852 EntMin 0.0
Epoch 5, Batch 40/103, Loss=3.8179893478751183
Loss made of: CE 0.11023496836423874, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.119898796081543 EntMin 0.0
Epoch 5, Batch 50/103, Loss=3.848889195173979
Loss made of: CE 0.09436051547527313, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.32562255859375 EntMin 0.0
Epoch 5, Batch 60/103, Loss=3.6691924676299097
Loss made of: CE 0.15365320444107056, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.483516216278076 EntMin 0.0
Epoch 5, Batch 70/103, Loss=3.6059779480099676
Loss made of: CE 0.12654487788677216, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.532149076461792 EntMin 0.0
Epoch 5, Batch 80/103, Loss=3.9300914600491526
Loss made of: CE 0.115745410323143, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.393308639526367 EntMin 0.0
Epoch 5, Batch 90/103, Loss=3.8434495933353903
Loss made of: CE 0.11954173445701599, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.523254156112671 EntMin 0.0
Epoch 5, Batch 100/103, Loss=3.8208586305379866
Loss made of: CE 0.1084766685962677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5383307933807373 EntMin 0.0
Epoch 5, Class Loss=0.12645620107650757, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.12645620107650757, Class Loss=0.12645620107650757, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/103, Loss=3.630616043508053
Loss made of: CE 0.11852826923131943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2312381267547607 EntMin 0.0
Epoch 6, Batch 20/103, Loss=3.6955378644168375
Loss made of: CE 0.10263869166374207, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.033661365509033 EntMin 0.0
Epoch 6, Batch 30/103, Loss=3.6358003400266172
Loss made of: CE 0.11594044417142868, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4868264198303223 EntMin 0.0
Epoch 6, Batch 40/103, Loss=3.560524445772171
Loss made of: CE 0.11368454992771149, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1146609783172607 EntMin 0.0
Epoch 6, Batch 50/103, Loss=3.698301964998245
Loss made of: CE 0.11273535341024399, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.515354871749878 EntMin 0.0
Epoch 6, Batch 60/103, Loss=3.7620629966259003
Loss made of: CE 0.11045634746551514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478976249694824 EntMin 0.0
Epoch 6, Batch 70/103, Loss=4.0096739158034325
Loss made of: CE 0.096006840467453, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6984291076660156 EntMin 0.0
Epoch 6, Batch 80/103, Loss=3.8602590397000314
Loss made of: CE 0.1754767894744873, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.483194351196289 EntMin 0.0
Epoch 6, Batch 90/103, Loss=3.952934064716101
Loss made of: CE 0.12258046865463257, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5220041275024414 EntMin 0.0
Epoch 6, Batch 100/103, Loss=3.81055506169796
Loss made of: CE 0.15138542652130127, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7347426414489746 EntMin 0.0
Epoch 6, Class Loss=0.1257421374320984, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.1257421374320984, Class Loss=0.1257421374320984, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.599112331867218, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.804934
Mean Acc: 0.429243
FreqW Acc: 0.708828
Mean IoU: 0.294307
Class IoU:
	class 0: 0.8703513
	class 1: 0.077645294
	class 2: 0.08506105
	class 3: 0.0
	class 4: 0.5495653
	class 5: 0.0026342594
	class 6: 0.7651572
	class 7: 0.6881363
	class 8: 0.07332966
	class 9: 0.0
	class 10: 0.028684683
	class 11: 0.28788957
	class 12: 0.28801194
	class 13: 0.37364012
	class 14: 0.4439083
	class 15: 0.46919817
	class 16: 0.0
Class Acc:
	class 0: 0.91386724
	class 1: 0.077658065
	class 2: 0.10141374
	class 3: 0.0
	class 4: 0.5821596
	class 5: 0.0026346727
	class 6: 0.92246157
	class 7: 0.7781057
	class 8: 0.073373444
	class 9: 0.0
	class 10: 0.028688936
	class 11: 0.5503769
	class 12: 0.67826414
	class 13: 0.73141533
	class 14: 0.9285578
	class 15: 0.928154
	class 16: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[19, 23, 1, 8]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=9.418911468982696
Loss made of: CE 1.3838510513305664, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.470458030700684 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=8.032162261009216
Loss made of: CE 1.010970115661621, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.008796215057373 EntMin 0.0
Epoch 1, Class Loss=1.261955976486206, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=1.261955976486206, Class Loss=1.261955976486206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=7.653165537118912
Loss made of: CE 0.8592073321342468, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.030989646911621 EntMin 0.0
Epoch 2, Batch 20/24, Loss=7.022411280870438
Loss made of: CE 0.7596699595451355, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.213778495788574 EntMin 0.0
Epoch 2, Class Loss=0.8388643264770508, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.8388643264770508, Class Loss=0.8388643264770508, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=6.41616535782814
Loss made of: CE 0.628240704536438, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5629472732543945 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.145773524045945
Loss made of: CE 0.5921406745910645, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.945531845092773 EntMin 0.0
Epoch 3, Class Loss=0.682360053062439, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.682360053062439, Class Loss=0.682360053062439, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=5.799882280826568
Loss made of: CE 0.5879778861999512, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.178894996643066 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.218191674351692
Loss made of: CE 0.5053674578666687, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.265602111816406 EntMin 0.0
Epoch 4, Class Loss=0.5713753700256348, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.5713753700256348, Class Loss=0.5713753700256348, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=5.475461930036545
Loss made of: CE 0.5258560180664062, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.82790470123291 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.936812594532967
Loss made of: CE 0.4317505359649658, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.681100368499756 EntMin 0.0
Epoch 5, Class Loss=0.49842917919158936, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.49842917919158936, Class Loss=0.49842917919158936, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=5.689346915483474
Loss made of: CE 0.4298804700374603, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.137362480163574 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.296204549074173
Loss made of: CE 0.5041978359222412, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.563509941101074 EntMin 0.0
Epoch 6, Class Loss=0.4405633807182312, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.4405633807182312, Class Loss=0.4405633807182312, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=8.53779103755951
Loss made of: CE 0.8750828504562378, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.611318111419678 EntMin 0.0
Epoch 1, Batch 20/21, Loss=7.3262713074684145
Loss made of: CE 0.8143150210380554, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.948878288269043 EntMin 0.0
Epoch 1, Class Loss=0.9830018281936646, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.9830018281936646, Class Loss=0.9830018281936646, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/21, Loss=6.498990941047668
Loss made of: CE 0.5520030856132507, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4136481285095215 EntMin 0.0
Epoch 2, Batch 20/21, Loss=6.109528535604477
Loss made of: CE 0.6065219640731812, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.951858043670654 EntMin 0.0
Epoch 2, Class Loss=0.6393935680389404, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.6393935680389404, Class Loss=0.6393935680389404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/21, Loss=6.301726099848747
Loss made of: CE 0.49679550528526306, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.955204486846924 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.9350275725126265
Loss made of: CE 0.5575215816497803, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.814949989318848 EntMin 0.0
Epoch 3, Class Loss=0.5522336363792419, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.5522336363792419, Class Loss=0.5522336363792419, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/21, Loss=6.053176072239876
Loss made of: CE 0.509258508682251, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.626031398773193 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.297316554188728
Loss made of: CE 0.4092137813568115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4944586753845215 EntMin 0.0
Epoch 4, Class Loss=0.4986031949520111, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.4986031949520111, Class Loss=0.4986031949520111, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/21, Loss=5.6089428693056105
Loss made of: CE 0.42174196243286133, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.185050010681152 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.591102632880211
Loss made of: CE 0.5117002129554749, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.978509902954102 EntMin 0.0
Epoch 5, Class Loss=0.44199275970458984, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.44199275970458984, Class Loss=0.44199275970458984, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/21, Loss=5.408727473020553
Loss made of: CE 0.39313921332359314, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.918644905090332 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.388516280055046
Loss made of: CE 0.47321876883506775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.373649597167969 EntMin 0.0
Epoch 6, Class Loss=0.4225357174873352, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.4225357174873352, Class Loss=0.4225357174873352, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=10.759539806842804
Loss made of: CE 1.4407145977020264, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.166112899780273 EntMin 0.0
Epoch 1, Class Loss=1.479765772819519, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=1.479765772819519, Class Loss=1.479765772819519, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/19, Loss=8.015438759326935
Loss made of: CE 0.8020111918449402, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.128349304199219 EntMin 0.0
Epoch 2, Class Loss=0.8728516101837158, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.8728516101837158, Class Loss=0.8728516101837158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=7.296952921152115
Loss made of: CE 0.7704471945762634, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.683222770690918 EntMin 0.0
Epoch 3, Class Loss=0.6952835917472839, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.6952835917472839, Class Loss=0.6952835917472839, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=6.718037211894989
Loss made of: CE 0.5404053926467896, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.488282680511475 EntMin 0.0
Epoch 4, Class Loss=0.5662883520126343, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.5662883520126343, Class Loss=0.5662883520126343, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=6.45852327644825
Loss made of: CE 0.4642338752746582, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.156145095825195 EntMin 0.0
Epoch 5, Class Loss=0.46856799721717834, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.46856799721717834, Class Loss=0.46856799721717834, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=6.045571917295456
Loss made of: CE 0.3709118962287903, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.291654109954834 EntMin 0.0
Epoch 6, Class Loss=0.4013229310512543, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.4013229310512543, Class Loss=0.4013229310512543, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=9.275532060861588
Loss made of: CE 0.6722993850708008, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.952784538269043 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=7.669868636131286
Loss made of: CE 0.6147092580795288, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.280484676361084 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.8518833518028259, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.8518833518028259, Class Loss=0.8518833518028259, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=7.168590331077576
Loss made of: CE 0.5427824854850769, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1618146896362305 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.783325630426407
Loss made of: CE 0.5388001799583435, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.712044715881348 EntMin 0.0
Epoch 2, Class Loss=0.6673876643180847, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.6673876643180847, Class Loss=0.6673876643180847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=6.614890506863594
Loss made of: CE 0.6281255483627319, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6251220703125 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.2470300763845446
Loss made of: CE 0.509055495262146, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.202146053314209 EntMin 0.0
Epoch 3, Class Loss=0.5637502670288086, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5637502670288086, Class Loss=0.5637502670288086, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=5.981804835796356
Loss made of: CE 0.5939404368400574, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.714078426361084 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.092227557301522
Loss made of: CE 0.35508283972740173, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.224205017089844 EntMin 0.0
Epoch 4, Class Loss=0.5105404257774353, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5105404257774353, Class Loss=0.5105404257774353, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=6.214837148785591
Loss made of: CE 0.6034120917320251, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.948189735412598 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.636673444509507
Loss made of: CE 0.3898131549358368, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.954046249389648 EntMin 0.0
Epoch 5, Class Loss=0.4723069667816162, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.4723069667816162, Class Loss=0.4723069667816162, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=5.803654757142067
Loss made of: CE 0.3842220902442932, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.819738388061523 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.597807759046555
Loss made of: CE 0.38045239448547363, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.310218811035156 EntMin 0.0
Epoch 6, Class Loss=0.4478428363800049, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.4478428363800049, Class Loss=0.4478428363800049, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8019391298294067, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.788382
Mean Acc: 0.305244
FreqW Acc: 0.652873
Mean IoU: 0.210670
Class IoU:
	class 0: 0.8218523
	class 1: 0.0027807313
	class 2: 0.004850456
	class 3: 0.0
	class 4: 0.28864604
	class 5: 6.913911e-05
	class 6: 0.65254986
	class 7: 0.5672403
	class 8: 0.003761399
	class 9: 0.0
	class 10: 0.016265074
	class 11: 0.2879073
	class 12: 0.32891712
	class 13: 0.31641212
	class 14: 0.47509658
	class 15: 0.6577267
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0
Class Acc:
	class 0: 0.95961046
	class 1: 0.002780732
	class 2: 0.004873731
	class 3: 0.0
	class 4: 0.29684186
	class 5: 6.913911e-05
	class 6: 0.7984912
	class 7: 0.5787468
	class 8: 0.0037614233
	class 9: 0.0
	class 10: 0.016302977
	class 11: 0.58030486
	class 12: 0.7776317
	class 13: 0.6047094
	class 14: 0.9168341
	class 15: 0.86916786
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[23, 14, 4, 11]
Current Client Index:  23
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=6.079633790254593
Loss made of: CE 0.4753432273864746, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7145795822143555 EntMin 0.0
Epoch 1, Batch 20/21, Loss=5.765180706977844
Loss made of: CE 0.594176173210144, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.497161865234375 EntMin 0.0
Epoch 1, Class Loss=0.5699828863143921, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.5699828863143921, Class Loss=0.5699828863143921, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/21, Loss=5.4808982640504835
Loss made of: CE 0.49960824847221375, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.463381767272949 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.359298259019852
Loss made of: CE 0.5040329694747925, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.730310440063477 EntMin 0.0
Epoch 2, Class Loss=0.5261498093605042, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.5261498093605042, Class Loss=0.5261498093605042, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/21, Loss=5.836129295825958
Loss made of: CE 0.4224018454551697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.832705020904541 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.437990137934685
Loss made of: CE 0.4773189425468445, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2886457443237305 EntMin 0.0
Epoch 3, Class Loss=0.47115272283554077, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.47115272283554077, Class Loss=0.47115272283554077, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/21, Loss=5.463798850774765
Loss made of: CE 0.4475677013397217, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.998838424682617 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.877104666829109
Loss made of: CE 0.3901376724243164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.182503700256348 EntMin 0.0
Epoch 4, Class Loss=0.42311277985572815, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.42311277985572815, Class Loss=0.42311277985572815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=5.199170234799385
Loss made of: CE 0.3341107964515686, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.777474403381348 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.199542024731636
Loss made of: CE 0.41717422008514404, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.556052207946777 EntMin 0.0
Epoch 5, Class Loss=0.38127803802490234, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.38127803802490234, Class Loss=0.38127803802490234, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/21, Loss=5.210085147619248
Loss made of: CE 0.3323797285556793, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.341939926147461 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.986426231265068
Loss made of: CE 0.3738807439804077, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.192507266998291 EntMin 0.0
Epoch 6, Class Loss=0.35986581444740295, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.35986581444740295, Class Loss=0.35986581444740295, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=7.135872483253479
Loss made of: CE 0.6018427610397339, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.659104824066162 EntMin 0.0
Epoch 1, Batch 20/26, Loss=6.389767414331436
Loss made of: CE 0.5241436958312988, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.795985221862793 EntMin 0.0
Epoch 1, Class Loss=0.6359507441520691, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.6359507441520691, Class Loss=0.6359507441520691, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=6.311726397275924
Loss made of: CE 0.5601606369018555, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.086820602416992 EntMin 0.0
Epoch 2, Batch 20/26, Loss=6.046932584047317
Loss made of: CE 0.61180180311203, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.935338973999023 EntMin 0.0
Epoch 2, Class Loss=0.5181910395622253, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.5181910395622253, Class Loss=0.5181910395622253, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/26, Loss=5.68640772998333
Loss made of: CE 0.45327049493789673, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2594099044799805 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.722666692733765
Loss made of: CE 0.3952706754207611, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.733194828033447 EntMin 0.0
Epoch 3, Class Loss=0.4473724961280823, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.4473724961280823, Class Loss=0.4473724961280823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.299801540374756
Loss made of: CE 0.367520272731781, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.085015296936035 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.281695353984833
Loss made of: CE 0.42804259061813354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645591735839844 EntMin 0.0
Epoch 4, Class Loss=0.3974541425704956, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.3974541425704956, Class Loss=0.3974541425704956, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.149736818671227
Loss made of: CE 0.5565537214279175, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0228986740112305 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.232414343953133
Loss made of: CE 0.38671359419822693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.908294200897217 EntMin 0.0
Epoch 5, Class Loss=0.36460962891578674, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.36460962891578674, Class Loss=0.36460962891578674, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.119349256157875
Loss made of: CE 0.33194029331207275, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.037870407104492 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.080314818024635
Loss made of: CE 0.30873116850852966, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9265947341918945 EntMin 0.0
Epoch 6, Class Loss=0.34192728996276855, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.34192728996276855, Class Loss=0.34192728996276855, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=6.757693526148796
Loss made of: CE 0.6968635320663452, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.089285373687744 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/26, Loss=6.513792759180069
Loss made of: CE 0.6015219688415527, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.807656764984131 EntMin 0.0
Epoch 1, Class Loss=0.6283913850784302, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.6283913850784302, Class Loss=0.6283913850784302, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=5.90391540825367
Loss made of: CE 0.6903778314590454, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.778775215148926 EntMin 0.0
Epoch 2, Batch 20/26, Loss=6.118648254871369
Loss made of: CE 0.5126440525054932, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6070051193237305 EntMin 0.0
Epoch 2, Class Loss=0.520575225353241, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.520575225353241, Class Loss=0.520575225353241, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=5.890699669718742
Loss made of: CE 0.4275190830230713, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.035035133361816 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.452944302558899
Loss made of: CE 0.4643983244895935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9783854484558105 EntMin 0.0
Epoch 3, Class Loss=0.43317490816116333, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.43317490816116333, Class Loss=0.43317490816116333, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.329088634252548
Loss made of: CE 0.33926528692245483, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.596108436584473 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.1759058147668835
Loss made of: CE 0.33751171827316284, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.640824794769287 EntMin 0.0
Epoch 4, Class Loss=0.39109236001968384, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.39109236001968384, Class Loss=0.39109236001968384, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.361888730525971
Loss made of: CE 0.33491194248199463, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.388431549072266 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.366664335131645
Loss made of: CE 0.3255687952041626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623044967651367 EntMin 0.0
Epoch 5, Class Loss=0.3632139563560486, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.3632139563560486, Class Loss=0.3632139563560486, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.48635818362236
Loss made of: CE 0.26242226362228394, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.567570686340332 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.92023795247078
Loss made of: CE 0.37265893816947937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.248241424560547 EntMin 0.0
Epoch 6, Class Loss=0.33349743485450745, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.33349743485450745, Class Loss=0.33349743485450745, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=6.712184730172157
Loss made of: CE 0.710050106048584, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.407961845397949 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=6.500316089391708
Loss made of: CE 0.696478545665741, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.853953838348389 EntMin 0.0
Epoch 1, Class Loss=0.6104047894477844, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.6104047894477844, Class Loss=0.6104047894477844, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/24, Loss=6.184453862905502
Loss made of: CE 0.6003701090812683, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.317614555358887 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.125240871310234
Loss made of: CE 0.5378990173339844, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.017157554626465 EntMin 0.0
Epoch 2, Class Loss=0.5068535208702087, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.5068535208702087, Class Loss=0.5068535208702087, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/24, Loss=5.838448306918144
Loss made of: CE 0.5209015607833862, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.887972831726074 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.024271556735039
Loss made of: CE 0.3419463038444519, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.746726989746094 EntMin 0.0
Epoch 3, Class Loss=0.4550839066505432, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4550839066505432, Class Loss=0.4550839066505432, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/24, Loss=5.324464792013169
Loss made of: CE 0.43244093656539917, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7595977783203125 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.07207261621952
Loss made of: CE 0.4408509135246277, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.256654262542725 EntMin 0.0
Epoch 4, Class Loss=0.4108431339263916, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4108431339263916, Class Loss=0.4108431339263916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/24, Loss=5.702490174770356
Loss made of: CE 0.3705434799194336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9760942459106445 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.6099650979042055
Loss made of: CE 0.3592126965522766, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291431903839111 EntMin 0.0
Epoch 5, Class Loss=0.38357147574424744, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.38357147574424744, Class Loss=0.38357147574424744, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/24, Loss=5.33685899078846
Loss made of: CE 0.395988792181015, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.086025238037109 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.264967665076256
Loss made of: CE 0.3876509666442871, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.758447647094727 EntMin 0.0
Epoch 6, Class Loss=0.3548009991645813, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.3548009991645813, Class Loss=0.3548009991645813, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.752276599407196, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.796760
Mean Acc: 0.339191
FreqW Acc: 0.672525
Mean IoU: 0.239960
Class IoU:
	class 0: 0.8352817
	class 1: 0.01884989
	class 2: 0.001957363
	class 3: 0.0
	class 4: 0.3592373
	class 5: 0.00014005101
	class 6: 0.68793714
	class 7: 0.57433355
	class 8: 0.0011966365
	class 9: 0.0
	class 10: 0.07320587
	class 11: 0.25925887
	class 12: 0.326071
	class 13: 0.32957956
	class 14: 0.46148688
	class 15: 0.6836838
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.31055737
	class 20: 0.11638278
Class Acc:
	class 0: 0.9549799
	class 1: 0.01885128
	class 2: 0.00196096
	class 3: 0.0
	class 4: 0.37258375
	class 5: 0.00014005101
	class 6: 0.7927092
	class 7: 0.58722055
	class 8: 0.0011966546
	class 9: 0.0
	class 10: 0.074816376
	class 11: 0.61721087
	class 12: 0.83492196
	class 13: 0.5966556
	class 14: 0.916427
	class 15: 0.87458074
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.35803863
	class 20: 0.12071313

federated global round: 22, step: 4
select part of clients to conduct local training
[0, 8, 16, 14]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.47085854113102
Loss made of: CE 0.6081734299659729, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.108448028564453 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.974480563402176
Loss made of: CE 0.579390287399292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.699180603027344 EntMin 0.0
Epoch 1, Class Loss=0.516013503074646, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.516013503074646, Class Loss=0.516013503074646, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/24, Loss=5.780954295396805
Loss made of: CE 0.2913232147693634, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.04063606262207 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.384249195456505
Loss made of: CE 0.42153415083885193, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.208652496337891 EntMin 0.0
Epoch 2, Class Loss=0.4161365032196045, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.4161365032196045, Class Loss=0.4161365032196045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/24, Loss=5.51023004502058
Loss made of: CE 0.41067981719970703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1389875411987305 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.830552852153778
Loss made of: CE 0.34816884994506836, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.150454044342041 EntMin 0.0
Epoch 3, Class Loss=0.3728748559951782, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3728748559951782, Class Loss=0.3728748559951782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/24, Loss=5.244874250888825
Loss made of: CE 0.373197078704834, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.817716598510742 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.605652475357056
Loss made of: CE 0.3336603045463562, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.279279708862305 EntMin 0.0
Epoch 4, Class Loss=0.34543129801750183, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.34543129801750183, Class Loss=0.34543129801750183, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/24, Loss=5.57573393881321
Loss made of: CE 0.35042500495910645, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.659841537475586 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.147414761781692
Loss made of: CE 0.31758254766464233, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.65809440612793 EntMin 0.0
Epoch 5, Class Loss=0.32285991311073303, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.32285991311073303, Class Loss=0.32285991311073303, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/24, Loss=5.196401506662369
Loss made of: CE 0.34956058859825134, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6783576011657715 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.24181469976902
Loss made of: CE 0.24733087420463562, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9483261108398438 EntMin 0.0
Epoch 6, Class Loss=0.3025292754173279, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.3025292754173279, Class Loss=0.3025292754173279, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.2745135724544525
Loss made of: CE 0.4674450159072876, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.227133750915527 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.487833300232888
Loss made of: CE 0.38198575377464294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427237033843994 EntMin 0.0
Epoch 1, Class Loss=0.5190078020095825, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.5190078020095825, Class Loss=0.5190078020095825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/24, Loss=5.69723234474659
Loss made of: CE 0.2974351644515991, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.639754295349121 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.380091789364815
Loss made of: CE 0.3502366244792938, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.985913276672363 EntMin 0.0
Epoch 2, Class Loss=0.4350804090499878, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.4350804090499878, Class Loss=0.4350804090499878, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/24, Loss=5.413939979672432
Loss made of: CE 0.41570791602134705, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.50921630859375 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.423906686902046
Loss made of: CE 0.42880576848983765, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.831632614135742 EntMin 0.0
Epoch 3, Class Loss=0.3876307010650635, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.3876307010650635, Class Loss=0.3876307010650635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/24, Loss=5.263831499218941
Loss made of: CE 0.40670832991600037, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.068912982940674 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.258477738499641
Loss made of: CE 0.2704651951789856, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4674201011657715 EntMin 0.0
Epoch 4, Class Loss=0.3597507178783417, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.3597507178783417, Class Loss=0.3597507178783417, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/24, Loss=5.442234969139099
Loss made of: CE 0.41488194465637207, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.832634925842285 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.959011137485504
Loss made of: CE 0.29897889494895935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.434050559997559 EntMin 0.0
Epoch 5, Class Loss=0.33659112453460693, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.33659112453460693, Class Loss=0.33659112453460693, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/24, Loss=5.153910318017006
Loss made of: CE 0.2825028598308563, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.527528762817383 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.932996681332588
Loss made of: CE 0.27982038259506226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4550065994262695 EntMin 0.0
Epoch 6, Class Loss=0.3181118965148926, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.3181118965148926, Class Loss=0.3181118965148926, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=8.286244881153106
Loss made of: CE 0.6915735602378845, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.452773094177246 EntMin 0.0
Epoch 1, Class Loss=0.6951027512550354, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.6951027512550354, Class Loss=0.6951027512550354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/19, Loss=6.712295925617218
Loss made of: CE 0.4876626431941986, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.154288291931152 EntMin 0.0
Epoch 2, Class Loss=0.5007823705673218, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.5007823705673218, Class Loss=0.5007823705673218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/19, Loss=6.021519297361374
Loss made of: CE 0.4557887017726898, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.44578742980957 EntMin 0.0
Epoch 3, Class Loss=0.3823898136615753, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.3823898136615753, Class Loss=0.3823898136615753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/19, Loss=5.797128704190254
Loss made of: CE 0.41486525535583496, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.658381462097168 EntMin 0.0
Epoch 4, Class Loss=0.32292667031288147, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.32292667031288147, Class Loss=0.32292667031288147, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/19, Loss=5.611845250427723
Loss made of: CE 0.2303064912557602, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.068900108337402 EntMin 0.0
Epoch 5, Class Loss=0.2824861407279968, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.2824861407279968, Class Loss=0.2824861407279968, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/19, Loss=5.304874460399151
Loss made of: CE 0.20685479044914246, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.421912670135498 EntMin 0.0
Epoch 6, Class Loss=0.2513934075832367, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.2513934075832367, Class Loss=0.2513934075832367, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=6.07025316953659
Loss made of: CE 0.48436784744262695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.732519149780273 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.276461228728294
Loss made of: CE 0.41792091727256775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1361470222473145 EntMin 0.0
Epoch 1, Class Loss=0.45161691308021545, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.45161691308021545, Class Loss=0.45161691308021545, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/26, Loss=5.566315227746964
Loss made of: CE 0.43570947647094727, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3641676902771 EntMin 0.0
Epoch 2, Batch 20/26, Loss=5.416688612103462
Loss made of: CE 0.4129279851913452, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.624855041503906 EntMin 0.0
Epoch 2, Class Loss=0.384028822183609, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.384028822183609, Class Loss=0.384028822183609, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/26, Loss=5.172962459921837
Loss made of: CE 0.36684998869895935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.597478866577148 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.189373849332332
Loss made of: CE 0.3040349781513214, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.379455089569092 EntMin 0.0
Epoch 3, Class Loss=0.3475007116794586, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.3475007116794586, Class Loss=0.3475007116794586, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/26, Loss=4.977690289914608
Loss made of: CE 0.2844671905040741, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.965245246887207 EntMin 0.0
Epoch 4, Batch 20/26, Loss=4.906935304403305
Loss made of: CE 0.3256896734237671, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143033504486084 EntMin 0.0
Epoch 4, Class Loss=0.3185964822769165, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.3185964822769165, Class Loss=0.3185964822769165, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/26, Loss=4.866026484966278
Loss made of: CE 0.4413950443267822, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.225046157836914 EntMin 0.0
Epoch 5, Batch 20/26, Loss=4.865652242302895
Loss made of: CE 0.3657833933830261, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9264655113220215 EntMin 0.0
Epoch 5, Class Loss=0.3066759705543518, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.3066759705543518, Class Loss=0.3066759705543518, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/26, Loss=4.800026977062226
Loss made of: CE 0.289907306432724, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.433530330657959 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.788159693777561
Loss made of: CE 0.267305463552475, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.104609489440918 EntMin 0.0
Epoch 6, Class Loss=0.29306110739707947, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.29306110739707947, Class Loss=0.29306110739707947, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.731109082698822, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.802079
Mean Acc: 0.368372
FreqW Acc: 0.690222
Mean IoU: 0.257805
Class IoU:
	class 0: 0.8539293
	class 1: 0.018771118
	class 2: 0.0025920558
	class 3: 0.0
	class 4: 0.2927638
	class 5: 0.0002375549
	class 6: 0.6617704
	class 7: 0.62744373
	class 8: 0.0014537612
	class 9: 0.0
	class 10: 0.114627
	class 11: 0.24875613
	class 12: 0.3159058
	class 13: 0.33551183
	class 14: 0.48467058
	class 15: 0.69066095
	class 16: 0.0
	class 17: 0.0
	class 18: 0.07029328
	class 19: 0.37575904
	class 20: 0.31875506
Class Acc:
	class 0: 0.9483019
	class 1: 0.018773697
	class 2: 0.0025974119
	class 3: 0.0
	class 4: 0.30483216
	class 5: 0.0002375549
	class 6: 0.70081323
	class 7: 0.6398578
	class 8: 0.0014537929
	class 9: 0.0
	class 10: 0.11896567
	class 11: 0.6041395
	class 12: 0.88678813
	class 13: 0.5758379
	class 14: 0.91708815
	class 15: 0.88067967
	class 16: 0.0
	class 17: 0.0
	class 18: 0.073519014
	class 19: 0.6783919
	class 20: 0.3835296

federated global round: 23, step: 4
select part of clients to conduct local training
[12, 10, 9, 6]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=5.302686655521393
Loss made of: CE 0.3452835977077484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.112338542938232 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.033557105064392
Loss made of: CE 0.25585755705833435, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.050668716430664 EntMin 0.0
Epoch 1, Class Loss=0.38527074456214905, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.38527074456214905, Class Loss=0.38527074456214905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/26, Loss=4.904706832766533
Loss made of: CE 0.3418238162994385, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9404826164245605 EntMin 0.0
Epoch 2, Batch 20/26, Loss=4.997594232857227
Loss made of: CE 0.2558649778366089, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5857110023498535 EntMin 0.0
Epoch 2, Class Loss=0.3124428391456604, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.3124428391456604, Class Loss=0.3124428391456604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/26, Loss=5.042170071601868
Loss made of: CE 0.28430768847465515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.829799652099609 EntMin 0.0
Epoch 3, Batch 20/26, Loss=4.899711230397225
Loss made of: CE 0.2639675736427307, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.133498191833496 EntMin 0.0
Epoch 3, Class Loss=0.29108238220214844, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.29108238220214844, Class Loss=0.29108238220214844, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/26, Loss=4.70499969869852
Loss made of: CE 0.2949654161930084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.771888732910156 EntMin 0.0
Epoch 4, Batch 20/26, Loss=4.662209288775921
Loss made of: CE 0.27026480436325073, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9944348335266113 EntMin 0.0
Epoch 4, Class Loss=0.26520806550979614, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.26520806550979614, Class Loss=0.26520806550979614, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/26, Loss=4.561375992000103
Loss made of: CE 0.2740940749645233, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1976447105407715 EntMin 0.0
Epoch 5, Batch 20/26, Loss=4.809338946640492
Loss made of: CE 0.34003812074661255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.844999313354492 EntMin 0.0
Epoch 5, Class Loss=0.2588511109352112, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.2588511109352112, Class Loss=0.2588511109352112, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/26, Loss=4.400101408362389
Loss made of: CE 0.27202126383781433, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.640303611755371 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.289200428128242
Loss made of: CE 0.23057782649993896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.977057456970215 EntMin 0.0
Epoch 6, Class Loss=0.24542655050754547, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.24542655050754547, Class Loss=0.24542655050754547, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=6.89451442360878
Loss made of: CE 0.604374885559082, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.115842819213867 EntMin 0.0
Epoch 1, Class Loss=0.6384503841400146, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.6384503841400146, Class Loss=0.6384503841400146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=5.985557931661606
Loss made of: CE 0.3734113872051239, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.544442176818848 EntMin 0.0
Epoch 2, Class Loss=0.5256554484367371, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.5256554484367371, Class Loss=0.5256554484367371, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=5.416159361600876
Loss made of: CE 0.4442877471446991, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.713410377502441 EntMin 0.0
Epoch 3, Class Loss=0.4390698969364166, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.4390698969364166, Class Loss=0.4390698969364166, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=5.463914200663567
Loss made of: CE 0.378431499004364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.245827674865723 EntMin 0.0
Epoch 4, Class Loss=0.3686041235923767, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.3686041235923767, Class Loss=0.3686041235923767, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=5.302908310294152
Loss made of: CE 0.3737465739250183, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.648288249969482 EntMin 0.0
Epoch 5, Class Loss=0.3477940261363983, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.3477940261363983, Class Loss=0.3477940261363983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=4.936156663298607
Loss made of: CE 0.3522786796092987, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.161221981048584 EntMin 0.0
Epoch 6, Class Loss=0.32304349541664124, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.32304349541664124, Class Loss=0.32304349541664124, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=5.689318287372589
Loss made of: CE 0.5214369297027588, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1639509201049805 EntMin 0.0
Epoch 1, Batch 20/21, Loss=5.0766732633113865
Loss made of: CE 0.37124213576316833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.584200859069824 EntMin 0.0
Epoch 1, Class Loss=0.43080076575279236, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.43080076575279236, Class Loss=0.43080076575279236, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/21, Loss=4.867952528595924
Loss made of: CE 0.31392475962638855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.633028030395508 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.171564382314682
Loss made of: CE 0.2670997381210327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1884355545043945 EntMin 0.0
Epoch 2, Class Loss=0.3578207194805145, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.3578207194805145, Class Loss=0.3578207194805145, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/21, Loss=5.188756588101387
Loss made of: CE 0.34407708048820496, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.161149024963379 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.024952054023743
Loss made of: CE 0.3481366038322449, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760854244232178 EntMin 0.0
Epoch 3, Class Loss=0.30899345874786377, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.30899345874786377, Class Loss=0.30899345874786377, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/21, Loss=4.634442093968391
Loss made of: CE 0.3072090446949005, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.137240409851074 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.837424238026142
Loss made of: CE 0.27112701535224915, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.833688735961914 EntMin 0.0
Epoch 4, Class Loss=0.2733752727508545, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.2733752727508545, Class Loss=0.2733752727508545, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=4.562773820757866
Loss made of: CE 0.21398189663887024, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734954357147217 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.654580172896385
Loss made of: CE 0.2268202304840088, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.701371669769287 EntMin 0.0
Epoch 5, Class Loss=0.2624540328979492, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.2624540328979492, Class Loss=0.2624540328979492, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/21, Loss=4.929663190245629
Loss made of: CE 0.22983309626579285, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.397485733032227 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.757773613929748
Loss made of: CE 0.24868285655975342, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.710573673248291 EntMin 0.0
Epoch 6, Class Loss=0.2534746825695038, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.2534746825695038, Class Loss=0.2534746825695038, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=5.869242683053017
Loss made of: CE 0.3891100287437439, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.25773811340332 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.730489966273308
Loss made of: CE 0.40377193689346313, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.742115020751953 EntMin 0.0
Epoch 1, Class Loss=0.42591592669487, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.42591592669487, Class Loss=0.42591592669487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/24, Loss=5.2589143961668015
Loss made of: CE 0.4126499593257904, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5309247970581055 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.463506802916527
Loss made of: CE 0.36327990889549255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53666877746582 EntMin 0.0
Epoch 2, Class Loss=0.354113906621933, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.354113906621933, Class Loss=0.354113906621933, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/24, Loss=5.282297372817993
Loss made of: CE 0.3027406334877014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.630901336669922 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.099954530596733
Loss made of: CE 0.32734403014183044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.58325719833374 EntMin 0.0
Epoch 3, Class Loss=0.32374101877212524, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.32374101877212524, Class Loss=0.32374101877212524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/24, Loss=5.078292825818062
Loss made of: CE 0.2767685651779175, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.702363967895508 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.947355565428734
Loss made of: CE 0.3730795383453369, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.482903957366943 EntMin 0.0
Epoch 4, Class Loss=0.305276095867157, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.305276095867157, Class Loss=0.305276095867157, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/24, Loss=4.9804348170757295
Loss made of: CE 0.4398375451564789, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.485108375549316 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.35415645390749
Loss made of: CE 0.24448677897453308, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1929545402526855 EntMin 0.0
Epoch 5, Class Loss=0.2879614233970642, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.2879614233970642, Class Loss=0.2879614233970642, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/24, Loss=5.089814020693302
Loss made of: CE 0.31889599561691284, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.529746055603027 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.93250705897808
Loss made of: CE 0.3251631259918213, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.687650203704834 EntMin 0.0
Epoch 6, Class Loss=0.27874451875686646, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.27874451875686646, Class Loss=0.27874451875686646, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7202560901641846, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.804435
Mean Acc: 0.411505
FreqW Acc: 0.699465
Mean IoU: 0.287177
Class IoU:
	class 0: 0.85818607
	class 1: 0.020205522
	class 2: 0.0023195732
	class 3: 0.0
	class 4: 0.3759418
	class 5: 0.00083794235
	class 6: 0.6545974
	class 7: 0.61806035
	class 8: 0.0029635609
	class 9: 3.2145246e-07
	class 10: 0.116622604
	class 11: 0.24855193
	class 12: 0.3136391
	class 13: 0.34874207
	class 14: 0.472656
	class 15: 0.69292396
	class 16: 0.0
	class 17: 0.2826532
	class 18: 0.24111483
	class 19: 0.37784556
	class 20: 0.40285996
Class Acc:
	class 0: 0.9379795
	class 1: 0.020208182
	class 2: 0.0023243397
	class 3: 0.0
	class 4: 0.39471647
	class 5: 0.00083794235
	class 6: 0.69988096
	class 7: 0.6305181
	class 8: 0.0029637553
	class 9: 3.2145246e-07
	class 10: 0.11911115
	class 11: 0.6312073
	class 12: 0.8900048
	class 13: 0.54984
	class 14: 0.9187261
	class 15: 0.8886543
	class 16: 0.0
	class 17: 0.32743394
	class 18: 0.35332662
	class 19: 0.63305885
	class 20: 0.6408073

federated global round: 24, step: 4
select part of clients to conduct local training
[18, 24, 25, 10]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=6.374864760041237
Loss made of: CE 0.4566183090209961, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.142744541168213 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4819500148296356, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.4819500148296356, Class Loss=0.4819500148296356, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/19, Loss=5.66953561604023
Loss made of: CE 0.45783931016921997, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.914731979370117 EntMin 0.0
Epoch 2, Class Loss=0.4009009599685669, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.4009009599685669, Class Loss=0.4009009599685669, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/19, Loss=5.378144323825836
Loss made of: CE 0.39585623145103455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.59600305557251 EntMin 0.0
Epoch 3, Class Loss=0.35462379455566406, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.35462379455566406, Class Loss=0.35462379455566406, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/19, Loss=5.224869737029076
Loss made of: CE 0.2627179026603699, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.151456832885742 EntMin 0.0
Epoch 4, Class Loss=0.3258340358734131, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.3258340358734131, Class Loss=0.3258340358734131, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/19, Loss=5.2463068008422855
Loss made of: CE 0.285836398601532, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.850561141967773 EntMin 0.0
Epoch 5, Class Loss=0.31257128715515137, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.31257128715515137, Class Loss=0.31257128715515137, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/19, Loss=4.869215658307075
Loss made of: CE 0.28021174669265747, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.496390342712402 EntMin 0.0
Epoch 6, Class Loss=0.29506024718284607, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.29506024718284607, Class Loss=0.29506024718284607, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=4.73082050383091
Loss made of: CE 0.268845796585083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.165929794311523 EntMin 0.0
Epoch 1, Batch 20/21, Loss=5.1551301151514055
Loss made of: CE 0.3584309220314026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.842620849609375 EntMin 0.0
Epoch 1, Class Loss=0.3496992588043213, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.3496992588043213, Class Loss=0.3496992588043213, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/21, Loss=4.685923001170158
Loss made of: CE 0.31573015451431274, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.044842720031738 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.263434407114983
Loss made of: CE 0.3826216459274292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.909361362457275 EntMin 0.0
Epoch 2, Class Loss=0.305978000164032, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.305978000164032, Class Loss=0.305978000164032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/21, Loss=4.942443288862705
Loss made of: CE 0.34213703870773315, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.370605945587158 EntMin 0.0
Epoch 3, Batch 20/21, Loss=4.674129901826381
Loss made of: CE 0.30424726009368896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478429317474365 EntMin 0.0
Epoch 3, Class Loss=0.27126771211624146, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.27126771211624146, Class Loss=0.27126771211624146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/21, Loss=4.423624981939793
Loss made of: CE 0.2791583240032196, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.507019996643066 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.550814765691757
Loss made of: CE 0.2932399809360504, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.211951732635498 EntMin 0.0
Epoch 4, Class Loss=0.25146472454071045, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.25146472454071045, Class Loss=0.25146472454071045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/21, Loss=4.4266383573412895
Loss made of: CE 0.2579362988471985, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.296760559082031 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.456389199197292
Loss made of: CE 0.23966746032238007, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.036382675170898 EntMin 0.0
Epoch 5, Class Loss=0.2469276785850525, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.2469276785850525, Class Loss=0.2469276785850525, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/21, Loss=4.710668233036995
Loss made of: CE 0.3248785734176636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.337930679321289 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.168215109407901
Loss made of: CE 0.2273527830839157, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.616192579269409 EntMin 0.0
Epoch 6, Class Loss=0.24460574984550476, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.24460574984550476, Class Loss=0.24460574984550476, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=5.2860039353370665
Loss made of: CE 0.40546584129333496, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.723840713500977 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=5.152732077240944
Loss made of: CE 0.36038798093795776, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.252656936645508 EntMin 0.0
Epoch 1, Class Loss=0.35671135783195496, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.35671135783195496, Class Loss=0.35671135783195496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/24, Loss=5.281300836801529
Loss made of: CE 0.39837968349456787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.821939468383789 EntMin 0.0
Epoch 2, Batch 20/24, Loss=4.926330250501633
Loss made of: CE 0.3234797716140747, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.754249095916748 EntMin 0.0
Epoch 2, Class Loss=0.3139420747756958, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.3139420747756958, Class Loss=0.3139420747756958, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/24, Loss=5.193418110907078
Loss made of: CE 0.26303356885910034, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.868656635284424 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.773817257583142
Loss made of: CE 0.22186653316020966, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.197160243988037 EntMin 0.0
Epoch 3, Class Loss=0.2984718680381775, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.2984718680381775, Class Loss=0.2984718680381775, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/24, Loss=4.994466948509216
Loss made of: CE 0.29070138931274414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.204310894012451 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.931632846593857
Loss made of: CE 0.27382370829582214, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684691429138184 EntMin 0.0
Epoch 4, Class Loss=0.2804318070411682, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.2804318070411682, Class Loss=0.2804318070411682, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/24, Loss=4.820238260924816
Loss made of: CE 0.26589515805244446, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.144308567047119 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.799947988986969
Loss made of: CE 0.3057953417301178, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.39587926864624 EntMin 0.0
Epoch 5, Class Loss=0.2793859839439392, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.2793859839439392, Class Loss=0.2793859839439392, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/24, Loss=4.406991982460022
Loss made of: CE 0.31914573907852173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.280982971191406 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.681171116232872
Loss made of: CE 0.33204156160354614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.639764785766602 EntMin 0.0
Epoch 6, Class Loss=0.2752606272697449, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.2752606272697449, Class Loss=0.2752606272697449, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=6.100966075062752
Loss made of: CE 0.4637996554374695, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.513969421386719 EntMin 0.0
Epoch 1, Class Loss=0.5101014971733093, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.5101014971733093, Class Loss=0.5101014971733093, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/19, Loss=5.519486710429192
Loss made of: CE 0.32143616676330566, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.036693572998047 EntMin 0.0
Epoch 2, Class Loss=0.4596239924430847, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.4596239924430847, Class Loss=0.4596239924430847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/19, Loss=5.192713502049446
Loss made of: CE 0.3733515739440918, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.744768142700195 EntMin 0.0
Epoch 3, Class Loss=0.4325653910636902, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.4325653910636902, Class Loss=0.4325653910636902, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/19, Loss=5.40325343310833
Loss made of: CE 0.40134739875793457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.150496959686279 EntMin 0.0
Epoch 4, Class Loss=0.38447293639183044, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.38447293639183044, Class Loss=0.38447293639183044, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/19, Loss=5.2385248959064485
Loss made of: CE 0.3828597962856293, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.456183433532715 EntMin 0.0
Epoch 5, Class Loss=0.3806752860546112, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.3806752860546112, Class Loss=0.3806752860546112, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/19, Loss=4.893944630026818
Loss made of: CE 0.38396725058555603, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.19027042388916 EntMin 0.0
Epoch 6, Class Loss=0.36733362078666687, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.36733362078666687, Class Loss=0.36733362078666687, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.7468591332435608, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.797593
Mean Acc: 0.429981
FreqW Acc: 0.695535
Mean IoU: 0.280760
Class IoU:
	class 0: 0.85460776
	class 1: 0.017330762
	class 2: 0.0028875973
	class 3: 0.0
	class 4: 0.4429434
	class 5: 0.0018236039
	class 6: 0.66101015
	class 7: 0.580953
	class 8: 0.006398222
	class 9: 0.0
	class 10: 0.019028902
	class 11: 0.2506126
	class 12: 0.3344165
	class 13: 0.30958647
	class 14: 0.460905
	class 15: 0.7028079
	class 16: 0.0
	class 17: 0.27510986
	class 18: 0.24992344
	class 19: 0.3606528
	class 20: 0.36496803
Class Acc:
	class 0: 0.9266286
	class 1: 0.017333476
	class 2: 0.0028955697
	class 3: 0.0
	class 4: 0.4711259
	class 5: 0.0018236178
	class 6: 0.7446573
	class 7: 0.59226847
	class 8: 0.0063990494
	class 9: 0.0
	class 10: 0.019072099
	class 11: 0.6595052
	class 12: 0.8465264
	class 13: 0.3659202
	class 14: 0.91866875
	class 15: 0.87640476
	class 16: 0.0
	class 17: 0.8255965
	class 18: 0.5570553
	class 19: 0.49941316
	class 20: 0.6983112

voc_4-4_PLOP On GPUs 0
Run in 84714s
