nohup: ignoring input
30
kvoc_15-1_RCIL On GPUs 2\Writing in results/seed_2023-ov/2023-03-20_voc_15-1_RCIL.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: None
Epoch 1, lr = 0.010000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/198, Loss=1.7355791211128235
Loss made of: CE 1.0191090106964111, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/198, Loss=1.069772183895111
Loss made of: CE 1.164245843887329, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/198, Loss=0.8389947772026062
Loss made of: CE 0.755530834197998, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/198, Loss=0.7435545027256012
Loss made of: CE 0.5956938862800598, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/198, Loss=0.5847085118293762
Loss made of: CE 0.5707622170448303, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/198, Loss=0.5783372282981872
Loss made of: CE 0.4428330659866333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/198, Loss=0.5684750348329544
Loss made of: CE 0.5380986332893372, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/198, Loss=0.5276553481817245
Loss made of: CE 0.4249206483364105, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/198, Loss=0.476958167552948
Loss made of: CE 0.37565475702285767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/198, Loss=0.4770857959985733
Loss made of: CE 0.5237047076225281, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/198, Loss=0.45655196309089663
Loss made of: CE 0.41719746589660645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/198, Loss=0.46397745311260224
Loss made of: CE 0.3913610875606537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/198, Loss=0.4373313903808594
Loss made of: CE 0.4580998420715332, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/198, Loss=0.421360120177269
Loss made of: CE 0.4941953122615814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/198, Loss=0.45468553304672243
Loss made of: CE 0.7410526275634766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/198, Loss=0.42430172860622406
Loss made of: CE 0.3551011085510254, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/198, Loss=0.43332583606243136
Loss made of: CE 0.42274928092956543, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/198, Loss=0.39601376950740813
Loss made of: CE 0.3166622519493103, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/198, Loss=0.3674729228019714
Loss made of: CE 0.3592160940170288, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5933257341384888, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.5933257341384888, Class Loss=0.5933257341384888, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/198, Loss=0.34122899919748306
Loss made of: CE 0.26670366525650024, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/198, Loss=0.33477979749441145
Loss made of: CE 0.24370424449443817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/198, Loss=0.3318731427192688
Loss made of: CE 0.25382140278816223, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/198, Loss=0.3153762608766556
Loss made of: CE 0.3103240132331848, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/198, Loss=0.32037593722343444
Loss made of: CE 0.30166423320770264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/198, Loss=0.34739944636821746
Loss made of: CE 0.35258516669273376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/198, Loss=0.3267309933900833
Loss made of: CE 0.2465859353542328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/198, Loss=0.29831434041261673
Loss made of: CE 0.25681501626968384, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/198, Loss=0.33466452062129975
Loss made of: CE 0.3839007318019867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/198, Loss=0.34749795794487
Loss made of: CE 0.294184148311615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/198, Loss=0.31752067655324934
Loss made of: CE 0.23008570075035095, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/198, Loss=0.3155656635761261
Loss made of: CE 0.3069445490837097, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/198, Loss=0.2977971374988556
Loss made of: CE 0.3509402871131897, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/198, Loss=0.3510606437921524
Loss made of: CE 0.4814945161342621, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/198, Loss=0.34815609753131865
Loss made of: CE 0.41693800687789917, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/198, Loss=0.3209858939051628
Loss made of: CE 0.25214695930480957, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/198, Loss=0.3385344982147217
Loss made of: CE 0.3367103040218353, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/198, Loss=0.32560030519962313
Loss made of: CE 0.3790047764778137, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/198, Loss=0.32797514498233793
Loss made of: CE 0.38828057050704956, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3268849849700928, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3268849849700928, Class Loss=0.3268849849700928, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/198, Loss=0.29075749069452284
Loss made of: CE 0.24361827969551086, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/198, Loss=0.24385430663824081
Loss made of: CE 0.25004521012306213, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/198, Loss=0.27280181646347046
Loss made of: CE 0.2414826601743698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/198, Loss=0.2734598100185394
Loss made of: CE 0.25904586911201477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/198, Loss=0.2520084291696548
Loss made of: CE 0.23449234664440155, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/198, Loss=0.25633981823921204
Loss made of: CE 0.3266783356666565, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/198, Loss=0.2557872638106346
Loss made of: CE 0.2906454801559448, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/198, Loss=0.278139565885067
Loss made of: CE 0.32806092500686646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/198, Loss=0.2743559181690216
Loss made of: CE 0.2767952084541321, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/198, Loss=0.2523022383451462
Loss made of: CE 0.20640167593955994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/198, Loss=0.24533960074186326
Loss made of: CE 0.1966191530227661, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/198, Loss=0.25184619426727295
Loss made of: CE 0.2475448101758957, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/198, Loss=0.25993219017982483
Loss made of: CE 0.2663988769054413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/198, Loss=0.2721342369914055
Loss made of: CE 0.1991042196750641, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/198, Loss=0.3044642791152
Loss made of: CE 0.38157913088798523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/198, Loss=0.26863843500614165
Loss made of: CE 0.33141928911209106, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/198, Loss=0.2630727082490921
Loss made of: CE 0.21653416752815247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/198, Loss=0.2583529204130173
Loss made of: CE 0.24255478382110596, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/198, Loss=0.2688929527997971
Loss made of: CE 0.27916043996810913, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2650115489959717, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.2650115489959717, Class Loss=0.2650115489959717, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/198, Loss=0.24921099841594696
Loss made of: CE 0.2025650143623352, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/198, Loss=0.24117620438337325
Loss made of: CE 0.3061366081237793, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/198, Loss=0.2642255321145058
Loss made of: CE 0.254888117313385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/198, Loss=0.22635626792907715
Loss made of: CE 0.24226921796798706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/198, Loss=0.24704549312591553
Loss made of: CE 0.2481600046157837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/198, Loss=0.20685176998376847
Loss made of: CE 0.22518372535705566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/198, Loss=0.24123903661966323
Loss made of: CE 0.33473706245422363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/198, Loss=0.21378002762794496
Loss made of: CE 0.20549575984477997, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/198, Loss=0.20279190689325333
Loss made of: CE 0.27463313937187195, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/198, Loss=0.22305311262607574
Loss made of: CE 0.24622556567192078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/198, Loss=0.2353777691721916
Loss made of: CE 0.2924337089061737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/198, Loss=0.2133642852306366
Loss made of: CE 0.2361585795879364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/198, Loss=0.23798096626996995
Loss made of: CE 0.20853400230407715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/198, Loss=0.24509991556406022
Loss made of: CE 0.2530706524848938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/198, Loss=0.22014996856451036
Loss made of: CE 0.33810538053512573, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/198, Loss=0.2374748170375824
Loss made of: CE 0.22162684798240662, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/198, Loss=0.22078267931938172
Loss made of: CE 0.24734362959861755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/198, Loss=0.2380785435438156
Loss made of: CE 0.2541336417198181, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/198, Loss=0.2216542035341263
Loss made of: CE 0.2500039041042328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.23060593008995056, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.23060593008995056, Class Loss=0.23060593008995056, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/198, Loss=0.2108044743537903
Loss made of: CE 0.2562764286994934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/198, Loss=0.2102047994732857
Loss made of: CE 0.2513916492462158, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/198, Loss=0.20056638717651368
Loss made of: CE 0.1711345762014389, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/198, Loss=0.18684395849704744
Loss made of: CE 0.192472442984581, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/198, Loss=0.20593269318342208
Loss made of: CE 0.28872132301330566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/198, Loss=0.20309579372406006
Loss made of: CE 0.16289374232292175, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/198, Loss=0.22276495397090912
Loss made of: CE 0.22212329506874084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/198, Loss=0.19567187428474425
Loss made of: CE 0.15042701363563538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/198, Loss=0.19236125349998473
Loss made of: CE 0.17665812373161316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/198, Loss=0.1964128464460373
Loss made of: CE 0.25862109661102295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/198, Loss=0.2269558772444725
Loss made of: CE 0.22427576780319214, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/198, Loss=0.221911783516407
Loss made of: CE 0.2995888590812683, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/198, Loss=0.19959088116884233
Loss made of: CE 0.19464236497879028, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/198, Loss=0.2254450336098671
Loss made of: CE 0.2549948990345001, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/198, Loss=0.1935956746339798
Loss made of: CE 0.19658885896205902, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/198, Loss=0.2051091253757477
Loss made of: CE 0.20396053791046143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/198, Loss=0.19164513796567917
Loss made of: CE 0.1528073400259018, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/198, Loss=0.2066174864768982
Loss made of: CE 0.22561821341514587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/198, Loss=0.19811166524887086
Loss made of: CE 0.15951386094093323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.20410694181919098, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.20410694181919098, Class Loss=0.20410694181919098, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/198, Loss=0.18126111030578612
Loss made of: CE 0.17441849410533905, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/198, Loss=0.1763029620051384
Loss made of: CE 0.16151174902915955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/198, Loss=0.18524960428476334
Loss made of: CE 0.21419081091880798, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/198, Loss=0.20274728983640672
Loss made of: CE 0.14326395094394684, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/198, Loss=0.18003223091363907
Loss made of: CE 0.18902802467346191, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/198, Loss=0.19032339751720428
Loss made of: CE 0.14363209903240204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/198, Loss=0.17423357367515563
Loss made of: CE 0.13725784420967102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/198, Loss=0.16655120104551316
Loss made of: CE 0.19295617938041687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/198, Loss=0.17312969714403154
Loss made of: CE 0.22573743760585785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/198, Loss=0.17145464569330215
Loss made of: CE 0.13861137628555298, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/198, Loss=0.1743720665574074
Loss made of: CE 0.20837008953094482, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/198, Loss=0.2061890110373497
Loss made of: CE 0.20823805034160614, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/198, Loss=0.2004631221294403
Loss made of: CE 0.16043037176132202, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/198, Loss=0.1978266641497612
Loss made of: CE 0.17696666717529297, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/198, Loss=0.1934978760778904
Loss made of: CE 0.24383752048015594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/198, Loss=0.17951853573322296
Loss made of: CE 0.15244224667549133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/198, Loss=0.19332319051027297
Loss made of: CE 0.16097886860370636, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/198, Loss=0.16686802878975868
Loss made of: CE 0.1477559208869934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/198, Loss=0.16293080300092697
Loss made of: CE 0.21364685893058777, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.18231847882270813, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.18231847882270813, Class Loss=0.18231847882270813, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/157, Loss=1.8196092128753663
Loss made of: CE 1.6433322429656982, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=0.975098067522049
Loss made of: CE 0.7562999725341797, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.7574777930974961
Loss made of: CE 0.8376163244247437, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.5831452071666717
Loss made of: CE 0.6022660136222839, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.5614298343658447
Loss made of: CE 0.4252398610115051, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.5683813095092773
Loss made of: CE 0.3972944915294647, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.5198921740055085
Loss made of: CE 0.5309237837791443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.4776526153087616
Loss made of: CE 0.43857523798942566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.39562322199344635
Loss made of: CE 0.5303339958190918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.38830370604991915
Loss made of: CE 0.35622549057006836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.434824538230896
Loss made of: CE 0.5358697772026062, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.43239494860172273
Loss made of: CE 0.35960501432418823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.400932714343071
Loss made of: CE 0.45266979932785034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.4029431760311127
Loss made of: CE 0.388508141040802, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.3464537501335144
Loss made of: CE 0.2923426926136017, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5941564440727234, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.5941564440727234, Class Loss=0.5941564440727234, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/157, Loss=0.3058342695236206
Loss made of: CE 0.28491055965423584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.3562949687242508
Loss made of: CE 0.2960260808467865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.35271663814783094
Loss made of: CE 0.34099239110946655, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.3403637930750847
Loss made of: CE 0.2526351809501648, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.3143364995718002
Loss made of: CE 0.3117039203643799, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.3240812212228775
Loss made of: CE 0.3092963695526123, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.2865922287106514
Loss made of: CE 0.25766998529434204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.2981027066707611
Loss made of: CE 0.3016068637371063, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.3001662135124207
Loss made of: CE 0.26701819896698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.3042437195777893
Loss made of: CE 0.25840693712234497, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.2719188302755356
Loss made of: CE 0.2977485656738281, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.2731583595275879
Loss made of: CE 0.2809351980686188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.30957400500774385
Loss made of: CE 0.33286014199256897, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.2773603186011314
Loss made of: CE 0.2817489802837372, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.3530970737338066
Loss made of: CE 0.32914498448371887, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3134582042694092, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.3134582042694092, Class Loss=0.3134582042694092, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/157, Loss=0.28769095093011854
Loss made of: CE 0.32310158014297485, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.24404371231794358
Loss made of: CE 0.20815560221672058, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.24597825706005097
Loss made of: CE 0.2529042661190033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.2747691571712494
Loss made of: CE 0.3216893672943115, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.284915517270565
Loss made of: CE 0.3598843216896057, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.2567477643489838
Loss made of: CE 0.24709486961364746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.2714290291070938
Loss made of: CE 0.1785680502653122, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.26207602620124815
Loss made of: CE 0.23673640191555023, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.2423288866877556
Loss made of: CE 0.26103898882865906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.25419121235609055
Loss made of: CE 0.22777590155601501, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.2954420119524002
Loss made of: CE 0.23448818922042847, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.22638767212629318
Loss made of: CE 0.28852972388267517, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.24648430049419404
Loss made of: CE 0.22607383131980896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.27729320228099824
Loss made of: CE 0.30104541778564453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.22393110245466233
Loss made of: CE 0.18342915177345276, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2580311894416809, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.2580311894416809, Class Loss=0.2580311894416809, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/157, Loss=0.19968612641096115
Loss made of: CE 0.23512738943099976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.2288879171013832
Loss made of: CE 0.15204867720603943, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.23509766906499863
Loss made of: CE 0.26280102133750916, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.20621522516012192
Loss made of: CE 0.21237409114837646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.2237423464655876
Loss made of: CE 0.30188459157943726, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.21120996326208114
Loss made of: CE 0.26917847990989685, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.19580773562192916
Loss made of: CE 0.21021120250225067, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.2142847388982773
Loss made of: CE 0.2050010859966278, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.20316398590803147
Loss made of: CE 0.16591838002204895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.22650836557149887
Loss made of: CE 0.21394595503807068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.20740755796432495
Loss made of: CE 0.1610044687986374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.1665805108845234
Loss made of: CE 0.1452978551387787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.21180783659219743
Loss made of: CE 0.16663795709609985, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.20966648161411286
Loss made of: CE 0.15374375879764557, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.20876661837100982
Loss made of: CE 0.2892911434173584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.20943450927734375, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.20943450927734375, Class Loss=0.20943450927734375, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/157, Loss=0.17269067466259003
Loss made of: CE 0.1977512091398239, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.19179254472255708
Loss made of: CE 0.2028866708278656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.20353631377220155
Loss made of: CE 0.38382869958877563, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.17631787359714507
Loss made of: CE 0.20510147511959076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.17800804674625398
Loss made of: CE 0.13976404070854187, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.18578552156686784
Loss made of: CE 0.26007193326950073, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.16830630004405975
Loss made of: CE 0.19718842208385468, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.1865585505962372
Loss made of: CE 0.15287743508815765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.17697471231222153
Loss made of: CE 0.20168247818946838, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.18467565774917602
Loss made of: CE 0.18295803666114807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.1799531437456608
Loss made of: CE 0.13544929027557373, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.19084492623806
Loss made of: CE 0.18890118598937988, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.18619093298912048
Loss made of: CE 0.19227516651153564, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.17515211552381516
Loss made of: CE 0.23288056254386902, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.19996717125177382
Loss made of: CE 0.2072683870792389, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1838103085756302, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.1838103085756302, Class Loss=0.1838103085756302, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/157, Loss=0.184989545494318
Loss made of: CE 0.16078656911849976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.15419521927833557
Loss made of: CE 0.14092542231082916, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.17932944297790526
Loss made of: CE 0.2440948337316513, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.15132535696029664
Loss made of: CE 0.12707771360874176, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.16516973078250885
Loss made of: CE 0.19379958510398865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.1631956048309803
Loss made of: CE 0.12157511711120605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.16261843144893645
Loss made of: CE 0.13934747874736786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.14104042425751687
Loss made of: CE 0.1154894158244133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.15687037259340286
Loss made of: CE 0.13867001235485077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.15727395936846733
Loss made of: CE 0.1308070570230484, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.1516810268163681
Loss made of: CE 0.1444907784461975, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.17559196725487708
Loss made of: CE 0.09599756449460983, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.17431596145033837
Loss made of: CE 0.24610698223114014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.1626797892153263
Loss made of: CE 0.18648946285247803, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.17199621647596358
Loss made of: CE 0.16710583865642548, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1629878580570221, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.1629878580570221, Class Loss=0.1629878580570221, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/205, Loss=1.6700754523277284
Loss made of: CE 1.225313663482666, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/205, Loss=0.984031593799591
Loss made of: CE 0.6613380312919617, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/205, Loss=0.8318838655948639
Loss made of: CE 0.6973625421524048, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/205, Loss=0.7252509534358978
Loss made of: CE 0.6744931936264038, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/205, Loss=0.6424180686473846
Loss made of: CE 0.6122674345970154, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/205, Loss=0.6413574874401092
Loss made of: CE 0.5620932579040527, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/205, Loss=0.5723116576671601
Loss made of: CE 0.5263242721557617, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/205, Loss=0.5896853893995285
Loss made of: CE 0.6284950971603394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/205, Loss=0.4687966823577881
Loss made of: CE 0.3988749384880066, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/205, Loss=0.4783115774393082
Loss made of: CE 0.46856194734573364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/205, Loss=0.47427466213703157
Loss made of: CE 0.4300221800804138, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/205, Loss=0.5111736536026001
Loss made of: CE 0.5426845550537109, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/205, Loss=0.5154643356800079
Loss made of: CE 0.4509841203689575, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/205, Loss=0.42301192581653596
Loss made of: CE 0.4176921844482422, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/205, Loss=0.44333501160144806
Loss made of: CE 0.3883121907711029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/205, Loss=0.4230767905712128
Loss made of: CE 0.376036137342453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/205, Loss=0.39091128706932066
Loss made of: CE 0.3297607898712158, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/205, Loss=0.39105079174041746
Loss made of: CE 0.3876163959503174, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/205, Loss=0.4092527389526367
Loss made of: CE 0.41017651557922363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/205, Loss=0.49038965702056886
Loss made of: CE 0.4548724591732025, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5992632508277893, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5992632508277893, Class Loss=0.5992632508277893, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/205, Loss=0.37937502562999725
Loss made of: CE 0.4885880947113037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/205, Loss=0.37030358910560607
Loss made of: CE 0.4088911712169647, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/205, Loss=0.35505262166261675
Loss made of: CE 0.34348639845848083, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/205, Loss=0.3218255788087845
Loss made of: CE 0.34164077043533325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/205, Loss=0.33139538019895554
Loss made of: CE 0.391864150762558, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/205, Loss=0.3689273089170456
Loss made of: CE 0.4109061658382416, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/205, Loss=0.3740221977233887
Loss made of: CE 0.4454067349433899, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/205, Loss=0.3038333058357239
Loss made of: CE 0.28200364112854004, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/205, Loss=0.325661239027977
Loss made of: CE 0.3997056484222412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/205, Loss=0.3767310708761215
Loss made of: CE 0.42257487773895264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/205, Loss=0.3748574465513229
Loss made of: CE 0.2827732563018799, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/205, Loss=0.3652059629559517
Loss made of: CE 0.5489587187767029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/205, Loss=0.3428302019834518
Loss made of: CE 0.3577413558959961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/205, Loss=0.30576248466968536
Loss made of: CE 0.2723552882671356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/205, Loss=0.3171550303697586
Loss made of: CE 0.34839123487472534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/205, Loss=0.3322107374668121
Loss made of: CE 0.2720573842525482, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/205, Loss=0.3151469439268112
Loss made of: CE 0.42910894751548767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/205, Loss=0.2848373308777809
Loss made of: CE 0.28274089097976685, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/205, Loss=0.32044010013341906
Loss made of: CE 0.36334937810897827, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/205, Loss=0.3137307524681091
Loss made of: CE 0.23763355612754822, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.33799460530281067, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.33799460530281067, Class Loss=0.33799460530281067, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/205, Loss=0.26144262999296186
Loss made of: CE 0.2559797167778015, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/205, Loss=0.25834146738052366
Loss made of: CE 0.18150657415390015, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/205, Loss=0.2764211267232895
Loss made of: CE 0.22938750684261322, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/205, Loss=0.2690586194396019
Loss made of: CE 0.23145300149917603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/205, Loss=0.2775949090719223
Loss made of: CE 0.27204856276512146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/205, Loss=0.27771752029657365
Loss made of: CE 0.22428521513938904, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/205, Loss=0.2655564218759537
Loss made of: CE 0.20565402507781982, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/205, Loss=0.2906826943159103
Loss made of: CE 0.29801440238952637, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/205, Loss=0.2702150374650955
Loss made of: CE 0.2918378710746765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/205, Loss=0.24315404444932937
Loss made of: CE 0.25764578580856323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/205, Loss=0.2798299342393875
Loss made of: CE 0.49070847034454346, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/205, Loss=0.26513559073209764
Loss made of: CE 0.33637475967407227, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/205, Loss=0.2840205803513527
Loss made of: CE 0.3177919387817383, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/205, Loss=0.2927288949489594
Loss made of: CE 0.2807973027229309, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/205, Loss=0.2752246081829071
Loss made of: CE 0.26592010259628296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/205, Loss=0.28416279554367063
Loss made of: CE 0.2625358998775482, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/205, Loss=0.2642719015479088
Loss made of: CE 0.2350163757801056, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/205, Loss=0.25267307460308075
Loss made of: CE 0.20561465620994568, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/205, Loss=0.298660409450531
Loss made of: CE 0.4063718914985657, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/205, Loss=0.27083434015512464
Loss made of: CE 0.27164968848228455, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.27091243863105774, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.27091243863105774, Class Loss=0.27091243863105774, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/205, Loss=0.23716357201337815
Loss made of: CE 0.19453440606594086, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/205, Loss=0.2367288202047348
Loss made of: CE 0.21457386016845703, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/205, Loss=0.2282358467578888
Loss made of: CE 0.18998172879219055, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/205, Loss=0.2188583567738533
Loss made of: CE 0.2704085409641266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/205, Loss=0.2328132778406143
Loss made of: CE 0.1639219969511032, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/205, Loss=0.23390494287014008
Loss made of: CE 0.268103688955307, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/205, Loss=0.2266630545258522
Loss made of: CE 0.2537601590156555, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/205, Loss=0.2210426762700081
Loss made of: CE 0.21987473964691162, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/205, Loss=0.24117317497730256
Loss made of: CE 0.2036348283290863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/205, Loss=0.22473839223384856
Loss made of: CE 0.1886412352323532, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/205, Loss=0.23220834583044053
Loss made of: CE 0.24040883779525757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/205, Loss=0.2236597716808319
Loss made of: CE 0.2727724313735962, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/205, Loss=0.25521170496940615
Loss made of: CE 0.24236080050468445, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/205, Loss=0.23179590553045273
Loss made of: CE 0.24938401579856873, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/205, Loss=0.23060232400894165
Loss made of: CE 0.24174760282039642, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/205, Loss=0.2483542189002037
Loss made of: CE 0.2361118495464325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/205, Loss=0.22157904356718064
Loss made of: CE 0.21630212664604187, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/205, Loss=0.2623086377978325
Loss made of: CE 0.2277550995349884, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/205, Loss=0.23806819170713425
Loss made of: CE 0.3166196048259735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/205, Loss=0.2152829125523567
Loss made of: CE 0.2991480529308319, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2330903708934784, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.2330903708934784, Class Loss=0.2330903708934784, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/205, Loss=0.24514750093221666
Loss made of: CE 0.3839894235134125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/205, Loss=0.22868083268404008
Loss made of: CE 0.23750273883342743, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/205, Loss=0.22236270159482957
Loss made of: CE 0.20256440341472626, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/205, Loss=0.21048893481492997
Loss made of: CE 0.17394936084747314, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/205, Loss=0.1985174909234047
Loss made of: CE 0.17663733661174774, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/205, Loss=0.21061646044254304
Loss made of: CE 0.19112633168697357, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/205, Loss=0.19961491376161575
Loss made of: CE 0.1792128086090088, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/205, Loss=0.17421276718378068
Loss made of: CE 0.17026159167289734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/205, Loss=0.20622284561395646
Loss made of: CE 0.22983533143997192, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/205, Loss=0.19911995977163316
Loss made of: CE 0.18208730220794678, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/205, Loss=0.189112588763237
Loss made of: CE 0.2486877143383026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/205, Loss=0.20862398743629457
Loss made of: CE 0.20666955411434174, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/205, Loss=0.19956071823835372
Loss made of: CE 0.21987596154212952, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/205, Loss=0.19777520298957824
Loss made of: CE 0.26058873534202576, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/205, Loss=0.2038743391633034
Loss made of: CE 0.15536756813526154, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/205, Loss=0.214759561419487
Loss made of: CE 0.16794578731060028, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/205, Loss=0.19779350906610488
Loss made of: CE 0.17786049842834473, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/205, Loss=0.21627815514802934
Loss made of: CE 0.1733298897743225, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/205, Loss=0.22147345989942552
Loss made of: CE 0.15548014640808105, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/205, Loss=0.21657234132289888
Loss made of: CE 0.1300237774848938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2079073041677475, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.2079073041677475, Class Loss=0.2079073041677475, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/205, Loss=0.1745869904756546
Loss made of: CE 0.1764429211616516, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/205, Loss=0.1986330434679985
Loss made of: CE 0.1847287267446518, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/205, Loss=0.20279796570539474
Loss made of: CE 0.19811227917671204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/205, Loss=0.19072047993540764
Loss made of: CE 0.2998875677585602, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/205, Loss=0.19580101519823073
Loss made of: CE 0.17533239722251892, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/205, Loss=0.20498175770044327
Loss made of: CE 0.14516761898994446, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/205, Loss=0.20752448439598084
Loss made of: CE 0.2313147783279419, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/205, Loss=0.18173860162496566
Loss made of: CE 0.20093612372875214, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/205, Loss=0.18019118458032607
Loss made of: CE 0.17502737045288086, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/205, Loss=0.19865196347236633
Loss made of: CE 0.182606503367424, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/205, Loss=0.1686008781194687
Loss made of: CE 0.14700675010681152, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/205, Loss=0.167409186065197
Loss made of: CE 0.14650656282901764, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/205, Loss=0.20270005464553834
Loss made of: CE 0.2063082456588745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/205, Loss=0.18305031806230546
Loss made of: CE 0.14443781971931458, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/205, Loss=0.1709151178598404
Loss made of: CE 0.14404545724391937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/205, Loss=0.15793743804097177
Loss made of: CE 0.12327470630407333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/205, Loss=0.18152943700551988
Loss made of: CE 0.13777635991573334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/205, Loss=0.18029668778181077
Loss made of: CE 0.1592845469713211, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/205, Loss=0.1929841712117195
Loss made of: CE 0.16290850937366486, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/205, Loss=0.18927888125181197
Loss made of: CE 0.1741715967655182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.18738359212875366, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.18738359212875366, Class Loss=0.18738359212875366, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/200, Loss=1.7569039344787598
Loss made of: CE 1.2918496131896973, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/200, Loss=0.9493064761161805
Loss made of: CE 0.7803009748458862, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/200, Loss=0.7688530147075653
Loss made of: CE 0.6265832781791687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/200, Loss=0.6604846805334091
Loss made of: CE 0.4511825740337372, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/200, Loss=0.6160827308893204
Loss made of: CE 0.61384117603302, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/200, Loss=0.5378163158893585
Loss made of: CE 0.4587249755859375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/200, Loss=0.5299517512321472
Loss made of: CE 0.32576704025268555, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/200, Loss=0.5249037027359009
Loss made of: CE 0.48491430282592773, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/200, Loss=0.5085634797811508
Loss made of: CE 0.5968735218048096, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/200, Loss=0.445849683880806
Loss made of: CE 0.39827412366867065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/200, Loss=0.4543002277612686
Loss made of: CE 0.40195780992507935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/200, Loss=0.4473587930202484
Loss made of: CE 0.36100614070892334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/200, Loss=0.44435475766658783
Loss made of: CE 0.44521838426589966, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/200, Loss=0.3885821014642715
Loss made of: CE 0.39917999505996704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/200, Loss=0.41026959866285323
Loss made of: CE 0.401539146900177, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/200, Loss=0.46780323684215547
Loss made of: CE 0.5612794756889343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/200, Loss=0.4440654903650284
Loss made of: CE 0.39621633291244507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/200, Loss=0.4519350826740265
Loss made of: CE 0.3728034198284149, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/200, Loss=0.379475924372673
Loss made of: CE 0.32203155755996704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/200, Loss=0.40069253742694855
Loss made of: CE 0.3942020833492279, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5793776512145996, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.5793776512145996, Class Loss=0.5793776512145996, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/200, Loss=0.37140684127807616
Loss made of: CE 0.2836505174636841, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/200, Loss=0.3256016582250595
Loss made of: CE 0.3216644525527954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/200, Loss=0.3239106684923172
Loss made of: CE 0.4371832013130188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/200, Loss=0.35763942897319795
Loss made of: CE 0.3460407257080078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/200, Loss=0.3358516126871109
Loss made of: CE 0.3724919557571411, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/200, Loss=0.35189240872859956
Loss made of: CE 0.2581494450569153, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/200, Loss=0.324028354883194
Loss made of: CE 0.2309524416923523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/200, Loss=0.34589048773050307
Loss made of: CE 0.45285505056381226, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/200, Loss=0.3079438105225563
Loss made of: CE 0.37663182616233826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/200, Loss=0.31067087054252623
Loss made of: CE 0.3435921370983124, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/200, Loss=0.3033578932285309
Loss made of: CE 0.26278460025787354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/200, Loss=0.32736872136592865
Loss made of: CE 0.3313165307044983, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/200, Loss=0.32149231135845185
Loss made of: CE 0.2509832978248596, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/200, Loss=0.3752442792057991
Loss made of: CE 0.3763361871242523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/200, Loss=0.340882708132267
Loss made of: CE 0.22888235747814178, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/200, Loss=0.3273694008588791
Loss made of: CE 0.3011012375354767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/200, Loss=0.32985428273677825
Loss made of: CE 0.36502930521965027, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/200, Loss=0.3102345436811447
Loss made of: CE 0.4368748068809509, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/200, Loss=0.3472626656293869
Loss made of: CE 0.30944082140922546, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/200, Loss=0.35606627762317655
Loss made of: CE 0.3011559844017029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3346984386444092, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.3346984386444092, Class Loss=0.3346984386444092, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/200, Loss=0.2688104122877121
Loss made of: CE 0.3333585262298584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/200, Loss=0.31272324323654177
Loss made of: CE 0.45298510789871216, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/200, Loss=0.2807248577475548
Loss made of: CE 0.25921735167503357, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/200, Loss=0.28758627623319627
Loss made of: CE 0.2617976665496826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/200, Loss=0.2640512228012085
Loss made of: CE 0.20533394813537598, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/200, Loss=0.2659457340836525
Loss made of: CE 0.2633380591869354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/200, Loss=0.25147895216941835
Loss made of: CE 0.26856598258018494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/200, Loss=0.25726477801799774
Loss made of: CE 0.2627463936805725, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/200, Loss=0.261118957400322
Loss made of: CE 0.1762661188840866, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/200, Loss=0.2716924652457237
Loss made of: CE 0.33216971158981323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/200, Loss=0.25155489444732665
Loss made of: CE 0.22906777262687683, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/200, Loss=0.24604929238557816
Loss made of: CE 0.2679058015346527, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/200, Loss=0.2543508350849152
Loss made of: CE 0.2237853854894638, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/200, Loss=0.27940637767314913
Loss made of: CE 0.2884511947631836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/200, Loss=0.25289326459169387
Loss made of: CE 0.2350950688123703, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/200, Loss=0.28793541342020035
Loss made of: CE 0.2755192518234253, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/200, Loss=0.24004069417715074
Loss made of: CE 0.21293556690216064, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/200, Loss=0.26258205473423
Loss made of: CE 0.2218717336654663, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/200, Loss=0.2510607048869133
Loss made of: CE 0.24740749597549438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/200, Loss=0.25784982889890673
Loss made of: CE 0.19039714336395264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.26525598764419556, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.26525598764419556, Class Loss=0.26525598764419556, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/200, Loss=0.24409960359334945
Loss made of: CE 0.20529699325561523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/200, Loss=0.2116282895207405
Loss made of: CE 0.2929462492465973, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/200, Loss=0.2233692854642868
Loss made of: CE 0.22980892658233643, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/200, Loss=0.23377409279346467
Loss made of: CE 0.20493905246257782, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/200, Loss=0.23833400309085845
Loss made of: CE 0.2420756071805954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/200, Loss=0.21209046840667725
Loss made of: CE 0.21016022562980652, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/200, Loss=0.2527243733406067
Loss made of: CE 0.27782928943634033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/200, Loss=0.22830928564071656
Loss made of: CE 0.20945143699645996, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/200, Loss=0.2270810514688492
Loss made of: CE 0.20156145095825195, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/200, Loss=0.2206311970949173
Loss made of: CE 0.2086639702320099, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/200, Loss=0.19950414896011354
Loss made of: CE 0.17476940155029297, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/200, Loss=0.22328920811414718
Loss made of: CE 0.23072002828121185, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/200, Loss=0.212412928044796
Loss made of: CE 0.17265145480632782, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/200, Loss=0.21655801236629485
Loss made of: CE 0.1590043157339096, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/200, Loss=0.23454593271017074
Loss made of: CE 0.20506781339645386, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/200, Loss=0.22873469293117524
Loss made of: CE 0.1912805289030075, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/200, Loss=0.20965660512447357
Loss made of: CE 0.2064911127090454, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/200, Loss=0.24224012941122056
Loss made of: CE 0.28765255212783813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/200, Loss=0.2195923864841461
Loss made of: CE 0.20406433939933777, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/200, Loss=0.19405684024095535
Loss made of: CE 0.24325230717658997, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2236316204071045, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.2236316204071045, Class Loss=0.2236316204071045, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/200, Loss=0.2011012077331543
Loss made of: CE 0.17137503623962402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/200, Loss=0.217116342484951
Loss made of: CE 0.24700669944286346, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/200, Loss=0.19391660988330842
Loss made of: CE 0.25502026081085205, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/200, Loss=0.18470777422189713
Loss made of: CE 0.18813681602478027, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/200, Loss=0.18758987188339232
Loss made of: CE 0.24601779878139496, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/200, Loss=0.19393850415945052
Loss made of: CE 0.18593451380729675, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/200, Loss=0.1904665693640709
Loss made of: CE 0.14811941981315613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/200, Loss=0.2163970112800598
Loss made of: CE 0.15327464044094086, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/200, Loss=0.1982448846101761
Loss made of: CE 0.16194869577884674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/200, Loss=0.2021072432398796
Loss made of: CE 0.1959574669599533, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/200, Loss=0.17381239235401152
Loss made of: CE 0.1476241946220398, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/200, Loss=0.17583100348711014
Loss made of: CE 0.20065543055534363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/200, Loss=0.1649618998169899
Loss made of: CE 0.166659414768219, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/200, Loss=0.2151554450392723
Loss made of: CE 0.23558762669563293, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/200, Loss=0.19962417036294938
Loss made of: CE 0.1929977536201477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/200, Loss=0.19854144901037216
Loss made of: CE 0.1697530448436737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/200, Loss=0.19826004803180694
Loss made of: CE 0.1646883487701416, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/200, Loss=0.1967620760202408
Loss made of: CE 0.18021532893180847, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/200, Loss=0.20148280560970305
Loss made of: CE 0.1713026463985443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/200, Loss=0.19374184161424637
Loss made of: CE 0.17014162242412567, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.19518794119358063, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.19518794119358063, Class Loss=0.19518794119358063, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/200, Loss=0.1877255156636238
Loss made of: CE 0.23804877698421478, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/200, Loss=0.18131490498781205
Loss made of: CE 0.16788671910762787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/200, Loss=0.1928250566124916
Loss made of: CE 0.2175959050655365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/200, Loss=0.17194510549306868
Loss made of: CE 0.19077205657958984, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/200, Loss=0.17332020550966262
Loss made of: CE 0.17204730212688446, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/200, Loss=0.17208922058343887
Loss made of: CE 0.13706663250923157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/200, Loss=0.15774414241313933
Loss made of: CE 0.14432229101657867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/200, Loss=0.22091955244541167
Loss made of: CE 0.2626492977142334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/200, Loss=0.17921723425388336
Loss made of: CE 0.1721145361661911, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/200, Loss=0.18406250476837158
Loss made of: CE 0.22333918511867523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/200, Loss=0.18019988983869553
Loss made of: CE 0.21928104758262634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/200, Loss=0.18181682378053665
Loss made of: CE 0.15145635604858398, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/200, Loss=0.19960642457008362
Loss made of: CE 0.17124652862548828, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/200, Loss=0.18138605505228042
Loss made of: CE 0.21283727884292603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/200, Loss=0.2017124190926552
Loss made of: CE 0.1707979440689087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/200, Loss=0.19517624378204346
Loss made of: CE 0.16130779683589935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/200, Loss=0.18330188021063804
Loss made of: CE 0.2207896113395691, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/200, Loss=0.16814144626259803
Loss made of: CE 0.2792670428752899, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/200, Loss=0.1910679057240486
Loss made of: CE 0.19610366225242615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/200, Loss=0.1897766634821892
Loss made of: CE 0.22301894426345825, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1846674531698227, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.1846674531698227, Class Loss=0.1846674531698227, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.22960375249385834, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.915788
Mean Acc: 0.729400
FreqW Acc: 0.852209
Mean IoU: 0.618141
Class IoU:
	class 0: 0.9219362
	class 1: 0.7790623
	class 2: 0.3972242
	class 3: 0.00075220596
	class 4: 0.65422064
	class 5: 0.6903479
	class 6: 0.92254144
	class 7: 0.88309115
	class 8: 0.81865096
	class 9: 0.3455224
	class 10: 0.023200769
	class 11: 0.5584104
	class 12: 0.76270074
	class 13: 0.45888853
	class 14: 0.82901514
	class 15: 0.8446915
Class Acc:
	class 0: 0.9726418
	class 1: 0.81677413
	class 2: 0.881015
	class 3: 0.00075220596
	class 4: 0.76373965
	class 5: 0.75750077
	class 6: 0.95879257
	class 7: 0.9244078
	class 8: 0.8702053
	class 9: 0.40320456
	class 10: 0.023201734
	class 11: 0.59114033
	class 12: 0.94163203
	class 13: 0.9059861
	class 14: 0.94288635
	class 15: 0.91652715

federated global round: 1, step: 0
select part of clients to conduct local training
[1, 7, 2, 9]
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/164, Loss=0.22818942219018937
Loss made of: CE 0.20237784087657928, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/164, Loss=0.23814672231674194
Loss made of: CE 0.1285015344619751, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/164, Loss=0.2560791507363319
Loss made of: CE 0.20991955697536469, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/164, Loss=0.280930195748806
Loss made of: CE 0.18013553321361542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/164, Loss=0.23943290561437608
Loss made of: CE 0.3984404504299164, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/164, Loss=0.22218684554100038
Loss made of: CE 0.20667994022369385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/164, Loss=0.2183004841208458
Loss made of: CE 0.18914519250392914, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/164, Loss=0.22117488980293273
Loss made of: CE 0.2435414046049118, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/164, Loss=0.23600777834653855
Loss made of: CE 0.2024482935667038, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/164, Loss=0.22454607039690017
Loss made of: CE 0.20410841703414917, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/164, Loss=0.22015727758407594
Loss made of: CE 0.2148790806531906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/164, Loss=0.27669664323329923
Loss made of: CE 0.29212212562561035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/164, Loss=0.2098128914833069
Loss made of: CE 0.18004441261291504, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/164, Loss=0.25181056559085846
Loss made of: CE 0.29321810603141785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/164, Loss=0.22403501123189926
Loss made of: CE 0.23317551612854004, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/164, Loss=0.2425357311964035
Loss made of: CE 0.23118290305137634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.23621700704097748, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.23621700704097748, Class Loss=0.23621700704097748, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/164, Loss=0.18011161535978318
Loss made of: CE 0.17552010715007782, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/164, Loss=0.21801458895206452
Loss made of: CE 0.31858038902282715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/164, Loss=0.22982524633407592
Loss made of: CE 0.22128580510616302, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/164, Loss=0.21431249529123306
Loss made of: CE 0.1860423982143402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/164, Loss=0.17529969960451125
Loss made of: CE 0.2090212106704712, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/164, Loss=0.20059518963098527
Loss made of: CE 0.13583695888519287, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/164, Loss=0.18696751892566682
Loss made of: CE 0.14341488480567932, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/164, Loss=0.20120912939310073
Loss made of: CE 0.13733145594596863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/164, Loss=0.18983936309814453
Loss made of: CE 0.18293601274490356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/164, Loss=0.18417909294366835
Loss made of: CE 0.16311150789260864, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/164, Loss=0.19299295097589492
Loss made of: CE 0.2010389268398285, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/164, Loss=0.20356551557779312
Loss made of: CE 0.17751280963420868, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/164, Loss=0.24481373131275178
Loss made of: CE 0.22536449134349823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/164, Loss=0.18255849182605743
Loss made of: CE 0.150638610124588, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/164, Loss=0.20803305506706238
Loss made of: CE 0.20542559027671814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/164, Loss=0.19727350026369095
Loss made of: CE 0.1561380922794342, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.20091670751571655, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.20091670751571655, Class Loss=0.20091670751571655, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/164, Loss=0.16790264025330542
Loss made of: CE 0.14375421404838562, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/164, Loss=0.17792508006095886
Loss made of: CE 0.16946183145046234, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/164, Loss=0.19205459877848624
Loss made of: CE 0.29143375158309937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/164, Loss=0.16740947887301444
Loss made of: CE 0.16050392389297485, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/164, Loss=0.18156176060438156
Loss made of: CE 0.19129714369773865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/164, Loss=0.17202992141246795
Loss made of: CE 0.17055122554302216, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/164, Loss=0.17877829521894456
Loss made of: CE 0.214150533080101, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/164, Loss=0.17728720158338546
Loss made of: CE 0.17411065101623535, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/164, Loss=0.17602540105581282
Loss made of: CE 0.1490260362625122, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/164, Loss=0.18141133785247804
Loss made of: CE 0.2260552942752838, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/164, Loss=0.16515820398926734
Loss made of: CE 0.14089219272136688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/164, Loss=0.16383609771728516
Loss made of: CE 0.21656692028045654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/164, Loss=0.18628039062023163
Loss made of: CE 0.18086914718151093, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/164, Loss=0.18235025703907012
Loss made of: CE 0.17586123943328857, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/164, Loss=0.15760354772210122
Loss made of: CE 0.14967437088489532, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/164, Loss=0.16719696968793868
Loss made of: CE 0.16622239351272583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1743246167898178, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.1743246167898178, Class Loss=0.1743246167898178, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/164, Loss=0.15959598496556282
Loss made of: CE 0.16896507143974304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/164, Loss=0.17453858628869057
Loss made of: CE 0.12893444299697876, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/164, Loss=0.14893979206681252
Loss made of: CE 0.15924206376075745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/164, Loss=0.16714578345417977
Loss made of: CE 0.1706845760345459, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/164, Loss=0.15457094162702562
Loss made of: CE 0.16517172753810883, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/164, Loss=0.15510656237602233
Loss made of: CE 0.12516385316848755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/164, Loss=0.17618227750062943
Loss made of: CE 0.16844996809959412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/164, Loss=0.16691121608018875
Loss made of: CE 0.194411039352417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/164, Loss=0.15292625576257707
Loss made of: CE 0.12203240394592285, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/164, Loss=0.14897265583276748
Loss made of: CE 0.14353521168231964, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/164, Loss=0.13804105296730995
Loss made of: CE 0.19269762933254242, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/164, Loss=0.14118120297789574
Loss made of: CE 0.1689189076423645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/164, Loss=0.1450135163962841
Loss made of: CE 0.1474715769290924, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/164, Loss=0.14887773618102074
Loss made of: CE 0.12255259603261948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/164, Loss=0.1504310518503189
Loss made of: CE 0.11947058141231537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/164, Loss=0.15593817606568336
Loss made of: CE 0.14786893129348755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.15589000284671783, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.15589000284671783, Class Loss=0.15589000284671783, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/164, Loss=0.13812501803040506
Loss made of: CE 0.16407319903373718, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/164, Loss=0.15776490271091462
Loss made of: CE 0.15753649175167084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/164, Loss=0.15569599717855453
Loss made of: CE 0.16840624809265137, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/164, Loss=0.13493724092841147
Loss made of: CE 0.15269026160240173, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/164, Loss=0.13672301396727563
Loss made of: CE 0.10537765920162201, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/164, Loss=0.150498603284359
Loss made of: CE 0.161533385515213, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/164, Loss=0.13849288672208787
Loss made of: CE 0.14786949753761292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/164, Loss=0.13783957362174987
Loss made of: CE 0.1349457949399948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/164, Loss=0.13034637570381163
Loss made of: CE 0.13942091166973114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/164, Loss=0.13200064674019812
Loss made of: CE 0.1115512102842331, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/164, Loss=0.1604844830930233
Loss made of: CE 0.1519635170698166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/164, Loss=0.1444486729800701
Loss made of: CE 0.15108108520507812, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/164, Loss=0.14362707063555719
Loss made of: CE 0.14019596576690674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/164, Loss=0.14639937803149222
Loss made of: CE 0.13026070594787598, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/164, Loss=0.14211707487702369
Loss made of: CE 0.15497231483459473, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/164, Loss=0.1252857431769371
Loss made of: CE 0.1321256011724472, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.14203956723213196, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.14203956723213196, Class Loss=0.14203956723213196, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/164, Loss=0.12063244879245758
Loss made of: CE 0.10929422825574875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/164, Loss=0.13395211473107338
Loss made of: CE 0.11076351255178452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/164, Loss=0.1365125596523285
Loss made of: CE 0.12621568143367767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/164, Loss=0.13044123947620392
Loss made of: CE 0.14802633225917816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/164, Loss=0.13395978063344954
Loss made of: CE 0.11832769960165024, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/164, Loss=0.12559528201818465
Loss made of: CE 0.136768639087677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/164, Loss=0.12686830312013625
Loss made of: CE 0.08387702703475952, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/164, Loss=0.15010715052485465
Loss made of: CE 0.1348935067653656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/164, Loss=0.12635428607463836
Loss made of: CE 0.11390554904937744, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/164, Loss=0.12022890821099282
Loss made of: CE 0.12200981378555298, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/164, Loss=0.1362024076282978
Loss made of: CE 0.1344335973262787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/164, Loss=0.12561988905072213
Loss made of: CE 0.1147458553314209, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/164, Loss=0.13459315448999404
Loss made of: CE 0.158100426197052, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/164, Loss=0.1276646852493286
Loss made of: CE 0.09548395872116089, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/164, Loss=0.12706401571631432
Loss made of: CE 0.112285315990448, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/164, Loss=0.1387564718723297
Loss made of: CE 0.13291794061660767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.13084979355335236, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.13084979355335236, Class Loss=0.13084979355335236, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/157, Loss=0.28619617521762847
Loss made of: CE 0.24898716807365417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=0.22506359666585923
Loss made of: CE 0.15983258187770844, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.21872414648532867
Loss made of: CE 0.24368081986904144, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.17563316375017166
Loss made of: CE 0.1738470494747162, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.19093686789274217
Loss made of: CE 0.13891492784023285, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.22232468277215958
Loss made of: CE 0.1848604381084442, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.21269748955965043
Loss made of: CE 0.21980270743370056, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.19927559792995453
Loss made of: CE 0.17629192769527435, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.1895059883594513
Loss made of: CE 0.2403702735900879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.19762792587280273
Loss made of: CE 0.15349817276000977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.18654495924711229
Loss made of: CE 0.15813928842544556, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.20436109453439713
Loss made of: CE 0.16326749324798584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.18324397504329681
Loss made of: CE 0.1789265125989914, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.20023123025894166
Loss made of: CE 0.13978929817676544, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.17786935865879058
Loss made of: CE 0.13370543718338013, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2042267769575119, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.2042267769575119, Class Loss=0.2042267769575119, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/157, Loss=0.1489999160170555
Loss made of: CE 0.12447759509086609, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.16590175479650499
Loss made of: CE 0.14156678318977356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.1793242983520031
Loss made of: CE 0.18178337812423706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.15787830352783203
Loss made of: CE 0.14593517780303955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.15052156671881675
Loss made of: CE 0.11555211991071701, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.17576094195246697
Loss made of: CE 0.17233967781066895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.15501192957162857
Loss made of: CE 0.13968029618263245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.16404429003596305
Loss made of: CE 0.14322635531425476, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.16493250653147698
Loss made of: CE 0.14405126869678497, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.1547486014664173
Loss made of: CE 0.14994414150714874, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.1394732877612114
Loss made of: CE 0.1625903844833374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.149475809186697
Loss made of: CE 0.1517963409423828, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.15512542501091958
Loss made of: CE 0.14684447646141052, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.13460204675793647
Loss made of: CE 0.12028171867132187, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.1814228266477585
Loss made of: CE 0.16450372338294983, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.15922798216342926, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.15922798216342926, Class Loss=0.15922798216342926, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/157, Loss=0.1418348267674446
Loss made of: CE 0.15021531283855438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.14570554792881013
Loss made of: CE 0.1064084991812706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.1410201109945774
Loss made of: CE 0.12735921144485474, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.14569058865308762
Loss made of: CE 0.11997073888778687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.14328932762145996
Loss made of: CE 0.17631064355373383, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.1415963351726532
Loss made of: CE 0.1485612988471985, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.1475658744573593
Loss made of: CE 0.10682022571563721, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.14513323381543158
Loss made of: CE 0.12367337942123413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.13095831647515296
Loss made of: CE 0.15514445304870605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.14232699275016786
Loss made of: CE 0.12132387608289719, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.15413309782743453
Loss made of: CE 0.16047096252441406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.1427954114973545
Loss made of: CE 0.15917597711086273, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.15883045345544816
Loss made of: CE 0.17023146152496338, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.16797467917203904
Loss made of: CE 0.1392376720905304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.14027147144079208
Loss made of: CE 0.14375071227550507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1458846628665924, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.1458846628665924, Class Loss=0.1458846628665924, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/157, Loss=0.13902414739131927
Loss made of: CE 0.14448752999305725, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.14484892636537552
Loss made of: CE 0.09502377361059189, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.15898289978504182
Loss made of: CE 0.1929386854171753, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.1295000933110714
Loss made of: CE 0.13323521614074707, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.1443190760910511
Loss made of: CE 0.1919025182723999, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.13739047795534134
Loss made of: CE 0.173801988363266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.13472682759165763
Loss made of: CE 0.11845900118350983, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.14133725613355635
Loss made of: CE 0.13417792320251465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.1349522814154625
Loss made of: CE 0.12063118815422058, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.1489170767366886
Loss made of: CE 0.14451467990875244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.1414479836821556
Loss made of: CE 0.11273805797100067, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.12084614560008049
Loss made of: CE 0.12028227746486664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.14039795696735383
Loss made of: CE 0.10845768451690674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.14271675124764444
Loss made of: CE 0.1088840514421463, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.13525973111391068
Loss made of: CE 0.15701858699321747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1394183337688446, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.1394183337688446, Class Loss=0.1394183337688446, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/157, Loss=0.1169341742992401
Loss made of: CE 0.12281069159507751, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.15025655627250672
Loss made of: CE 0.12285642325878143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.16229941621422767
Loss made of: CE 0.39986342191696167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.12432307451963424
Loss made of: CE 0.15144944190979004, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.12470393031835555
Loss made of: CE 0.09227313846349716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.13492560759186745
Loss made of: CE 0.1385834515094757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.12204226776957512
Loss made of: CE 0.10292317718267441, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.1344986580312252
Loss made of: CE 0.12186549603939056, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.13220925703644754
Loss made of: CE 0.15960778295993805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.1360119342803955
Loss made of: CE 0.14138111472129822, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.13472790345549585
Loss made of: CE 0.10103170573711395, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.13651814460754394
Loss made of: CE 0.15718145668506622, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.13263699859380723
Loss made of: CE 0.12979596853256226, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.12510569915175437
Loss made of: CE 0.12392294406890869, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.1464236669242382
Loss made of: CE 0.10826452821493149, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.13416801393032074, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.13416801393032074, Class Loss=0.13416801393032074, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/157, Loss=0.1283556580543518
Loss made of: CE 0.11036207526922226, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.11534457877278328
Loss made of: CE 0.11532183736562729, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.12207561507821083
Loss made of: CE 0.139491006731987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.11395549029111862
Loss made of: CE 0.09055501222610474, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.11863541081547738
Loss made of: CE 0.1200571283698082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.11702890023589134
Loss made of: CE 0.09864869713783264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.12124177813529968
Loss made of: CE 0.09501373767852783, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.10569330006837845
Loss made of: CE 0.0954260528087616, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.12329201325774193
Loss made of: CE 0.12069766223430634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.12206202149391174
Loss made of: CE 0.10966992378234863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.11148673743009567
Loss made of: CE 0.11329754441976547, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.12342933490872383
Loss made of: CE 0.07638054341077805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.13339271619915963
Loss made of: CE 0.16629454493522644, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.12696224823594093
Loss made of: CE 0.13121943175792694, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.12219254076480865
Loss made of: CE 0.11464842408895493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.12071970850229263, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.12071970850229263, Class Loss=0.12071970850229263, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/200, Loss=0.23830352425575257
Loss made of: CE 0.2907167673110962, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/200, Loss=0.19132803976535798
Loss made of: CE 0.12674082815647125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/200, Loss=0.20295414626598357
Loss made of: CE 0.1770285964012146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/200, Loss=0.2030397981405258
Loss made of: CE 0.15358561277389526, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/200, Loss=0.18898915350437165
Loss made of: CE 0.18867897987365723, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/200, Loss=0.19114185273647308
Loss made of: CE 0.23349595069885254, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/200, Loss=0.21794328838586807
Loss made of: CE 0.1514996588230133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/200, Loss=0.20755632519721984
Loss made of: CE 0.17977210879325867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/200, Loss=0.19935100376605988
Loss made of: CE 0.2319394052028656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/200, Loss=0.19117591828107833
Loss made of: CE 0.21665839850902557, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/200, Loss=0.2158605143427849
Loss made of: CE 0.20870301127433777, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/200, Loss=0.20318280905485153
Loss made of: CE 0.1814281940460205, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/200, Loss=0.19482845664024354
Loss made of: CE 0.24474474787712097, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/200, Loss=0.19097011685371398
Loss made of: CE 0.1600446105003357, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/200, Loss=0.21272280365228652
Loss made of: CE 0.22142383456230164, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/200, Loss=0.22398223727941513
Loss made of: CE 0.28154057264328003, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/200, Loss=0.2361797273159027
Loss made of: CE 0.19564400613307953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/200, Loss=0.22917782813310622
Loss made of: CE 0.16381435096263885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/200, Loss=0.1962719053030014
Loss made of: CE 0.16078099608421326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/200, Loss=0.20624081343412398
Loss made of: CE 0.20245526731014252, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.20706000924110413, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.20706000924110413, Class Loss=0.20706000924110413, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/200, Loss=0.19285391867160798
Loss made of: CE 0.18503855168819427, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/200, Loss=0.1693172499537468
Loss made of: CE 0.16542020440101624, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/200, Loss=0.16596412360668183
Loss made of: CE 0.24124829471111298, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/200, Loss=0.1856817126274109
Loss made of: CE 0.16220901906490326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/200, Loss=0.1878424160182476
Loss made of: CE 0.23143963515758514, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/200, Loss=0.19371359050273895
Loss made of: CE 0.1409815400838852, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/200, Loss=0.17445802986621856
Loss made of: CE 0.13327454030513763, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/200, Loss=0.19364240318536757
Loss made of: CE 0.22548280656337738, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/200, Loss=0.16419689804315568
Loss made of: CE 0.22275114059448242, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/200, Loss=0.16464580744504928
Loss made of: CE 0.20568475127220154, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/200, Loss=0.17564424276351928
Loss made of: CE 0.17263810336589813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/200, Loss=0.17903562486171723
Loss made of: CE 0.19464683532714844, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/200, Loss=0.18160630762577057
Loss made of: CE 0.16408467292785645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/200, Loss=0.20853196680545807
Loss made of: CE 0.21177423000335693, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/200, Loss=0.21189777702093124
Loss made of: CE 0.12562614679336548, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/200, Loss=0.19603327810764312
Loss made of: CE 0.19741767644882202, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/200, Loss=0.190154030919075
Loss made of: CE 0.1849442571401596, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/200, Loss=0.18723755925893784
Loss made of: CE 0.2588067948818207, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/200, Loss=0.21186359226703644
Loss made of: CE 0.212459534406662, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/200, Loss=0.19560347944498063
Loss made of: CE 0.22853267192840576, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.18649619817733765, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.18649619817733765, Class Loss=0.18649619817733765, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/200, Loss=0.16384864524006842
Loss made of: CE 0.16298654675483704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/200, Loss=0.1851125255227089
Loss made of: CE 0.26521337032318115, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/200, Loss=0.16656283885240555
Loss made of: CE 0.16576698422431946, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/200, Loss=0.1658429116010666
Loss made of: CE 0.14046895503997803, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/200, Loss=0.16103170290589333
Loss made of: CE 0.14618515968322754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/200, Loss=0.17748137712478637
Loss made of: CE 0.1898384541273117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/200, Loss=0.16230671107769012
Loss made of: CE 0.15495476126670837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/200, Loss=0.16479611694812774
Loss made of: CE 0.19165253639221191, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/200, Loss=0.16164899468421937
Loss made of: CE 0.1061299741268158, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/200, Loss=0.16930967345833778
Loss made of: CE 0.18003898859024048, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/200, Loss=0.15935418084263803
Loss made of: CE 0.17633819580078125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/200, Loss=0.16482872143387794
Loss made of: CE 0.1646895855665207, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/200, Loss=0.1667667582631111
Loss made of: CE 0.16351276636123657, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/200, Loss=0.1773625023663044
Loss made of: CE 0.21011409163475037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/200, Loss=0.16627555787563325
Loss made of: CE 0.14765067398548126, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/200, Loss=0.1792576566338539
Loss made of: CE 0.18306921422481537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/200, Loss=0.15117237642407416
Loss made of: CE 0.13227108120918274, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/200, Loss=0.16264017820358276
Loss made of: CE 0.14975927770137787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/200, Loss=0.1587730437517166
Loss made of: CE 0.1748979389667511, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/200, Loss=0.17069216221570968
Loss made of: CE 0.14810308814048767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.16675323247909546, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.16675323247909546, Class Loss=0.16675323247909546, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/200, Loss=0.1609109789133072
Loss made of: CE 0.15352264046669006, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/200, Loss=0.1333466462790966
Loss made of: CE 0.16043508052825928, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/200, Loss=0.1522241860628128
Loss made of: CE 0.15891432762145996, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/200, Loss=0.15454248189926148
Loss made of: CE 0.14759168028831482, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/200, Loss=0.17094243317842484
Loss made of: CE 0.20159173011779785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/200, Loss=0.1397014833986759
Loss made of: CE 0.14876984059810638, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/200, Loss=0.1724664881825447
Loss made of: CE 0.2011873424053192, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/200, Loss=0.15745011270046233
Loss made of: CE 0.14786148071289062, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/200, Loss=0.1547209270298481
Loss made of: CE 0.14754199981689453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/200, Loss=0.14957594573497773
Loss made of: CE 0.15308964252471924, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/200, Loss=0.140764082968235
Loss made of: CE 0.15150214731693268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/200, Loss=0.15958459228277205
Loss made of: CE 0.1537317931652069, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/200, Loss=0.1461457774043083
Loss made of: CE 0.13293889164924622, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/200, Loss=0.15477751642465593
Loss made of: CE 0.1345033496618271, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/200, Loss=0.16281161457300186
Loss made of: CE 0.13991308212280273, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/200, Loss=0.1601794734597206
Loss made of: CE 0.12124651670455933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/200, Loss=0.14796067476272584
Loss made of: CE 0.13926254212856293, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/200, Loss=0.15272980481386184
Loss made of: CE 0.16374891996383667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/200, Loss=0.15305940359830855
Loss made of: CE 0.1719444841146469, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/200, Loss=0.13645437508821487
Loss made of: CE 0.13578228652477264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.15301744639873505, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.15301744639873505, Class Loss=0.15301744639873505, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/200, Loss=0.14612312614917755
Loss made of: CE 0.1420915126800537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/200, Loss=0.1482405662536621
Loss made of: CE 0.16053424775600433, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/200, Loss=0.1388324722647667
Loss made of: CE 0.1641746163368225, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/200, Loss=0.133261127024889
Loss made of: CE 0.1331092119216919, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/200, Loss=0.14882603660225868
Loss made of: CE 0.1984885036945343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/200, Loss=0.13619741275906563
Loss made of: CE 0.13135862350463867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/200, Loss=0.1413319356739521
Loss made of: CE 0.11514520645141602, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/200, Loss=0.14454247057437897
Loss made of: CE 0.10053368657827377, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/200, Loss=0.14639024212956428
Loss made of: CE 0.11610930413007736, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/200, Loss=0.1555401861667633
Loss made of: CE 0.1315743625164032, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/200, Loss=0.12586142718791962
Loss made of: CE 0.12406474351882935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/200, Loss=0.13361753523349762
Loss made of: CE 0.1299183964729309, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/200, Loss=0.12515120282769204
Loss made of: CE 0.11654578149318695, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/200, Loss=0.16017356812953948
Loss made of: CE 0.1615726351737976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/200, Loss=0.1467017039656639
Loss made of: CE 0.1535266488790512, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/200, Loss=0.14843140468001365
Loss made of: CE 0.12876670062541962, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/200, Loss=0.14128197655081748
Loss made of: CE 0.13019075989723206, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/200, Loss=0.14113325402140617
Loss made of: CE 0.13113710284233093, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/200, Loss=0.14942226707935333
Loss made of: CE 0.13733887672424316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/200, Loss=0.13610817193984986
Loss made of: CE 0.12655934691429138, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.14235840737819672, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.14235840737819672, Class Loss=0.14235840737819672, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/200, Loss=0.13122781962156296
Loss made of: CE 0.12775646150112152, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/200, Loss=0.13478685542941093
Loss made of: CE 0.13946828246116638, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/200, Loss=0.14636456221342087
Loss made of: CE 0.1754366159439087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/200, Loss=0.12506065517663956
Loss made of: CE 0.11554934829473495, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/200, Loss=0.12474384233355522
Loss made of: CE 0.14072386920452118, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/200, Loss=0.1290168359875679
Loss made of: CE 0.131225124001503, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/200, Loss=0.12493257224559784
Loss made of: CE 0.10359808057546616, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/200, Loss=0.15467844232916833
Loss made of: CE 0.17658007144927979, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/200, Loss=0.13525860384106636
Loss made of: CE 0.1408807337284088, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/200, Loss=0.142247062176466
Loss made of: CE 0.14244858920574188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/200, Loss=0.13260388374328613
Loss made of: CE 0.1316792368888855, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/200, Loss=0.13685845136642455
Loss made of: CE 0.1116098016500473, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/200, Loss=0.15681135281920433
Loss made of: CE 0.13670597970485687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/200, Loss=0.13908587470650674
Loss made of: CE 0.15749183297157288, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/200, Loss=0.14503053277730943
Loss made of: CE 0.13347360491752625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/200, Loss=0.1516210876405239
Loss made of: CE 0.1276690661907196, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/200, Loss=0.13719704896211624
Loss made of: CE 0.16131430864334106, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/200, Loss=0.12729604467749595
Loss made of: CE 0.1439988613128662, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/200, Loss=0.13775971606373788
Loss made of: CE 0.1393260657787323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/200, Loss=0.14107789397239684
Loss made of: CE 0.13601408898830414, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1376829445362091, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.1376829445362091, Class Loss=0.1376829445362091, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/205, Loss=0.2199203848838806
Loss made of: CE 0.1637776792049408, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/205, Loss=0.21343682557344437
Loss made of: CE 0.14221623539924622, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/205, Loss=0.23688620179891587
Loss made of: CE 0.20967857539653778, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/205, Loss=0.19851551949977875
Loss made of: CE 0.23747777938842773, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/205, Loss=0.2469580739736557
Loss made of: CE 0.20419351756572723, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/205, Loss=0.24708294868469238
Loss made of: CE 0.2053878754377365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/205, Loss=0.2192470446228981
Loss made of: CE 0.15804332494735718, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/205, Loss=0.2459009751677513
Loss made of: CE 0.33382996916770935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/205, Loss=0.20305577665567398
Loss made of: CE 0.12983812391757965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/205, Loss=0.20524940341711045
Loss made of: CE 0.17960643768310547, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/205, Loss=0.2212963655591011
Loss made of: CE 0.16874943673610687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/205, Loss=0.22862691879272462
Loss made of: CE 0.19862711429595947, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/205, Loss=0.2220659524202347
Loss made of: CE 0.14995315670967102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/205, Loss=0.1936873272061348
Loss made of: CE 0.16523459553718567, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/205, Loss=0.2218330904841423
Loss made of: CE 0.20002707839012146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/205, Loss=0.2180955246090889
Loss made of: CE 0.23010365664958954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/205, Loss=0.2056176096200943
Loss made of: CE 0.20151716470718384, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/205, Loss=0.1940109133720398
Loss made of: CE 0.18705859780311584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/205, Loss=0.19786115884780883
Loss made of: CE 0.19994479417800903, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/205, Loss=0.21932434141635895
Loss made of: CE 0.2380123734474182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.217581644654274, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.217581644654274, Class Loss=0.217581644654274, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/205, Loss=0.20029010474681855
Loss made of: CE 0.3264409899711609, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/205, Loss=0.21580683141946794
Loss made of: CE 0.2526204288005829, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/205, Loss=0.18201864510774612
Loss made of: CE 0.18456688523292542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/205, Loss=0.1671218067407608
Loss made of: CE 0.16424928605556488, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/205, Loss=0.18639693185687065
Loss made of: CE 0.20090582966804504, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/205, Loss=0.21312223076820375
Loss made of: CE 0.21385113894939423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/205, Loss=0.20426866859197618
Loss made of: CE 0.23131024837493896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/205, Loss=0.16545481532812117
Loss made of: CE 0.15725359320640564, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/205, Loss=0.18065518289804458
Loss made of: CE 0.1891898214817047, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/205, Loss=0.22753136605024338
Loss made of: CE 0.1778528392314911, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/205, Loss=0.18945587426424026
Loss made of: CE 0.1572912037372589, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/205, Loss=0.18856600821018218
Loss made of: CE 0.3384883403778076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/205, Loss=0.18820436000823976
Loss made of: CE 0.15874429047107697, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/205, Loss=0.18514651656150818
Loss made of: CE 0.16298353672027588, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/205, Loss=0.17965305894613265
Loss made of: CE 0.19687172770500183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/205, Loss=0.19315903186798095
Loss made of: CE 0.15165814757347107, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/205, Loss=0.18312447816133498
Loss made of: CE 0.314004123210907, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/205, Loss=0.1698121577501297
Loss made of: CE 0.12834015488624573, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/205, Loss=0.17562207952141762
Loss made of: CE 0.1600245237350464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/205, Loss=0.18808287084102632
Loss made of: CE 0.15647733211517334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.18908308446407318, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.18908308446407318, Class Loss=0.18908308446407318, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/205, Loss=0.17140801548957824
Loss made of: CE 0.18544495105743408, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/205, Loss=0.15569634586572648
Loss made of: CE 0.1219853013753891, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/205, Loss=0.17409757971763612
Loss made of: CE 0.16519460082054138, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/205, Loss=0.16097031980752946
Loss made of: CE 0.14839856326580048, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/205, Loss=0.16418204009532927
Loss made of: CE 0.15447019040584564, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/205, Loss=0.16490371972322465
Loss made of: CE 0.13880431652069092, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/205, Loss=0.18131515979766846
Loss made of: CE 0.12900897860527039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/205, Loss=0.1707279697060585
Loss made of: CE 0.2141895294189453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/205, Loss=0.1681390181183815
Loss made of: CE 0.17600521445274353, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/205, Loss=0.15556526333093643
Loss made of: CE 0.14444802701473236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/205, Loss=0.16525088846683503
Loss made of: CE 0.22576892375946045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/205, Loss=0.1597311869263649
Loss made of: CE 0.1571464240550995, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/205, Loss=0.17691502571105958
Loss made of: CE 0.18760335445404053, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/205, Loss=0.17393674999475478
Loss made of: CE 0.16581544280052185, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/205, Loss=0.16600651293992996
Loss made of: CE 0.14231786131858826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/205, Loss=0.17912973761558532
Loss made of: CE 0.19350866973400116, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/205, Loss=0.1615196943283081
Loss made of: CE 0.1415618658065796, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/205, Loss=0.15753131210803986
Loss made of: CE 0.15336129069328308, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/205, Loss=0.18964723348617554
Loss made of: CE 0.2502613961696625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/205, Loss=0.16659298539161682
Loss made of: CE 0.1800777167081833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.16746526956558228, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.16746526956558228, Class Loss=0.16746526956558228, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/205, Loss=0.16766171157360077
Loss made of: CE 0.15019434690475464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/205, Loss=0.16145272701978683
Loss made of: CE 0.1444282829761505, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/205, Loss=0.15223750099539757
Loss made of: CE 0.12436532229185104, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/205, Loss=0.1538756899535656
Loss made of: CE 0.1471744030714035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/205, Loss=0.16756100878119468
Loss made of: CE 0.11882030218839645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/205, Loss=0.14925375506281852
Loss made of: CE 0.16751354932785034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/205, Loss=0.145704548060894
Loss made of: CE 0.13178563117980957, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/205, Loss=0.15738172456622124
Loss made of: CE 0.14976941049098969, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/205, Loss=0.16965484321117402
Loss made of: CE 0.13152866065502167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/205, Loss=0.1499822422862053
Loss made of: CE 0.1393660306930542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/205, Loss=0.16429944485425949
Loss made of: CE 0.1922333687543869, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/205, Loss=0.15487901791930198
Loss made of: CE 0.1839921474456787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/205, Loss=0.16097403317689896
Loss made of: CE 0.1669456660747528, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/205, Loss=0.1595224604010582
Loss made of: CE 0.16869613528251648, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/205, Loss=0.1593451738357544
Loss made of: CE 0.1553397923707962, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/205, Loss=0.1676613613963127
Loss made of: CE 0.14470116794109344, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/205, Loss=0.14959216639399528
Loss made of: CE 0.13693389296531677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/205, Loss=0.1664355367422104
Loss made of: CE 0.15273436903953552, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/205, Loss=0.155294793844223
Loss made of: CE 0.19223909080028534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/205, Loss=0.1541909322142601
Loss made of: CE 0.14785030484199524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1586417555809021, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.1586417555809021, Class Loss=0.1586417555809021, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/205, Loss=0.16262879967689514
Loss made of: CE 0.21129176020622253, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/205, Loss=0.162147219479084
Loss made of: CE 0.16014860570430756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/205, Loss=0.1578316032886505
Loss made of: CE 0.13858669996261597, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/205, Loss=0.13640127927064896
Loss made of: CE 0.13399773836135864, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/205, Loss=0.13663774058222772
Loss made of: CE 0.1169782280921936, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/205, Loss=0.1545309104025364
Loss made of: CE 0.1287873089313507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/205, Loss=0.15271803885698318
Loss made of: CE 0.13375215232372284, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/205, Loss=0.12964447364211082
Loss made of: CE 0.10759174078702927, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/205, Loss=0.15346593856811525
Loss made of: CE 0.13954803347587585, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/205, Loss=0.14700762629508973
Loss made of: CE 0.16240990161895752, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/205, Loss=0.14304441660642625
Loss made of: CE 0.17488250136375427, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/205, Loss=0.14939551278948784
Loss made of: CE 0.1370345652103424, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/205, Loss=0.14434501007199288
Loss made of: CE 0.12328615039587021, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/205, Loss=0.15237969160079956
Loss made of: CE 0.16997431218624115, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/205, Loss=0.1478392519056797
Loss made of: CE 0.12587524950504303, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/205, Loss=0.14626487866044044
Loss made of: CE 0.11351899802684784, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/205, Loss=0.14420821517705917
Loss made of: CE 0.12772683799266815, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/205, Loss=0.14875155165791512
Loss made of: CE 0.11812039464712143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/205, Loss=0.154164856672287
Loss made of: CE 0.12795525789260864, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/205, Loss=0.1524215266108513
Loss made of: CE 0.11995506286621094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.14857037365436554, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.14857037365436554, Class Loss=0.14857037365436554, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/205, Loss=0.1280404381453991
Loss made of: CE 0.134232297539711, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/205, Loss=0.14406588971614837
Loss made of: CE 0.13663914799690247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/205, Loss=0.1532328963279724
Loss made of: CE 0.15830056369304657, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/205, Loss=0.14120030924677848
Loss made of: CE 0.16063518822193146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/205, Loss=0.14307854548096657
Loss made of: CE 0.15831515192985535, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/205, Loss=0.14890846237540245
Loss made of: CE 0.10297908633947372, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/205, Loss=0.15593242049217224
Loss made of: CE 0.14521118998527527, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/205, Loss=0.14643117487430574
Loss made of: CE 0.1353156864643097, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/205, Loss=0.1382903940975666
Loss made of: CE 0.12258037179708481, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/205, Loss=0.13878816217184067
Loss made of: CE 0.10887601226568222, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/205, Loss=0.13120611011981964
Loss made of: CE 0.10887716710567474, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/205, Loss=0.1356144592165947
Loss made of: CE 0.1269325315952301, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/205, Loss=0.14427583217620848
Loss made of: CE 0.1718660295009613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/205, Loss=0.14189749509096145
Loss made of: CE 0.1127626821398735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/205, Loss=0.13021959960460663
Loss made of: CE 0.11854517459869385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/205, Loss=0.1163356825709343
Loss made of: CE 0.11635191738605499, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/205, Loss=0.14438603818416595
Loss made of: CE 0.12959079444408417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/205, Loss=0.13321729749441147
Loss made of: CE 0.10641336441040039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/205, Loss=0.1457919642329216
Loss made of: CE 0.13831070065498352, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/205, Loss=0.14139871001243592
Loss made of: CE 0.12323224544525146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.14092278480529785, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.14092278480529785, Class Loss=0.14092278480529785, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.17084191739559174, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.939085
Mean Acc: 0.856186
FreqW Acc: 0.892306
Mean IoU: 0.740392
Class IoU:
	class 0: 0.93670875
	class 1: 0.8718185
	class 2: 0.40986246
	class 3: 0.7868794
	class 4: 0.691482
	class 5: 0.77164096
	class 6: 0.93595636
	class 7: 0.90323156
	class 8: 0.8818331
	class 9: 0.4316064
	class 10: 0.5024154
	class 11: 0.5787596
	class 12: 0.8236395
	class 13: 0.6109286
	class 14: 0.8541087
	class 15: 0.8554082
Class Acc:
	class 0: 0.9646493
	class 1: 0.96123177
	class 2: 0.9224095
	class 3: 0.7958491
	class 4: 0.84271
	class 5: 0.8768157
	class 6: 0.9667998
	class 7: 0.9394913
	class 8: 0.94288844
	class 9: 0.59596837
	class 10: 0.5237616
	class 11: 0.6197724
	class 12: 0.96421283
	class 13: 0.911764
	class 14: 0.9481391
	class 15: 0.9225173

federated global round: 2, step: 0
select part of clients to conduct local training
[8, 6, 2, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/229, Loss=0.1705174393951893
Loss made of: CE 0.13525325059890747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/229, Loss=0.18355903029441833
Loss made of: CE 0.23356692492961884, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/229, Loss=0.19500752836465834
Loss made of: CE 0.19276902079582214, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/229, Loss=0.17465028762817383
Loss made of: CE 0.136302649974823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/229, Loss=0.18402164280414582
Loss made of: CE 0.16873769462108612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/229, Loss=0.17821196615695953
Loss made of: CE 0.19326269626617432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/229, Loss=0.17150865122675896
Loss made of: CE 0.18655893206596375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/229, Loss=0.15943168103694916
Loss made of: CE 0.1574944108724594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/229, Loss=0.21473824828863144
Loss made of: CE 0.1372004747390747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/229, Loss=0.19146063104271888
Loss made of: CE 0.2006639689207077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/229, Loss=0.1818462647497654
Loss made of: CE 0.14897674322128296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/229, Loss=0.1598133288323879
Loss made of: CE 0.1345413774251938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/229, Loss=0.18568056598305702
Loss made of: CE 0.20583848655223846, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/229, Loss=0.18323662057518958
Loss made of: CE 0.1786758303642273, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/229, Loss=0.16270836144685746
Loss made of: CE 0.15922917425632477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/229, Loss=0.19934851825237274
Loss made of: CE 0.13007736206054688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/229, Loss=0.18847463577985762
Loss made of: CE 0.21732065081596375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/229, Loss=0.19895947277545928
Loss made of: CE 0.164547398686409, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/229, Loss=0.19181666523218155
Loss made of: CE 0.18379676342010498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/229, Loss=0.18323950469493866
Loss made of: CE 0.22478072345256805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 210/229, Loss=0.17385243102908135
Loss made of: CE 0.1601911187171936, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 220/229, Loss=0.21270895302295684
Loss made of: CE 0.37922999262809753, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1843394637107849, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.1843394637107849, Class Loss=0.1843394637107849, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/229, Loss=0.1726404182612896
Loss made of: CE 0.23489123582839966, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/229, Loss=0.1658095419406891
Loss made of: CE 0.17535988986492157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/229, Loss=0.15172019898891448
Loss made of: CE 0.13147130608558655, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/229, Loss=0.1756386138498783
Loss made of: CE 0.16749036312103271, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/229, Loss=0.15955210030078887
Loss made of: CE 0.17736904323101044, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/229, Loss=0.15576461851596832
Loss made of: CE 0.15104636549949646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/229, Loss=0.16965446174144744
Loss made of: CE 0.15431267023086548, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/229, Loss=0.15213233307003976
Loss made of: CE 0.14017519354820251, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/229, Loss=0.16383188515901564
Loss made of: CE 0.1385326087474823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/229, Loss=0.14989806413650514
Loss made of: CE 0.1671600043773651, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/229, Loss=0.1509976178407669
Loss made of: CE 0.18775300681591034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/229, Loss=0.1489250347018242
Loss made of: CE 0.17313173413276672, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/229, Loss=0.1623748242855072
Loss made of: CE 0.14747777581214905, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/229, Loss=0.15140947848558425
Loss made of: CE 0.14785273373126984, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/229, Loss=0.14572105854749678
Loss made of: CE 0.11313768476247787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/229, Loss=0.17192775756120682
Loss made of: CE 0.1907370388507843, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/229, Loss=0.15017598643898963
Loss made of: CE 0.12540362775325775, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/229, Loss=0.15391157791018487
Loss made of: CE 0.13043299317359924, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/229, Loss=0.16295005455613137
Loss made of: CE 0.26803058385849, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/229, Loss=0.15523208528757096
Loss made of: CE 0.17728953063488007, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 210/229, Loss=0.16530245840549468
Loss made of: CE 0.23735594749450684, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 220/229, Loss=0.16063037440180777
Loss made of: CE 0.16037964820861816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.15939483046531677, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.15939483046531677, Class Loss=0.15939483046531677, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/229, Loss=0.1341308079659939
Loss made of: CE 0.12921419739723206, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/229, Loss=0.1398182787001133
Loss made of: CE 0.15288564562797546, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/229, Loss=0.14274047911167145
Loss made of: CE 0.1140630692243576, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/229, Loss=0.14551001638174058
Loss made of: CE 0.13354453444480896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/229, Loss=0.13770000264048576
Loss made of: CE 0.12459737062454224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/229, Loss=0.15809864848852156
Loss made of: CE 0.20898818969726562, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/229, Loss=0.1350265473127365
Loss made of: CE 0.12519904971122742, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/229, Loss=0.1426643520593643
Loss made of: CE 0.14329269528388977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/229, Loss=0.1349326714873314
Loss made of: CE 0.182168647646904, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/229, Loss=0.14666094109416009
Loss made of: CE 0.1381816864013672, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/229, Loss=0.15369353890419007
Loss made of: CE 0.15858615934848785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/229, Loss=0.1416086420416832
Loss made of: CE 0.14807559549808502, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/229, Loss=0.15703822895884514
Loss made of: CE 0.24803456664085388, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/229, Loss=0.13936568200588226
Loss made of: CE 0.10203679651021957, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/229, Loss=0.14364439472556115
Loss made of: CE 0.12372185289859772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/229, Loss=0.14129211381077766
Loss made of: CE 0.11945460736751556, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/229, Loss=0.13764625787734985
Loss made of: CE 0.1045374870300293, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/229, Loss=0.13281055092811583
Loss made of: CE 0.13303081691265106, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/229, Loss=0.13835965543985368
Loss made of: CE 0.08662334084510803, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/229, Loss=0.14750011935830115
Loss made of: CE 0.10825783014297485, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 210/229, Loss=0.12663722783327103
Loss made of: CE 0.09092991054058075, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 220/229, Loss=0.14035037010908127
Loss made of: CE 0.13240602612495422, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1420138031244278, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.1420138031244278, Class Loss=0.1420138031244278, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/229, Loss=0.1266429081559181
Loss made of: CE 0.14636412262916565, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/229, Loss=0.13625583201646804
Loss made of: CE 0.1780996173620224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/229, Loss=0.13000135943293573
Loss made of: CE 0.18891684710979462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/229, Loss=0.12396717593073844
Loss made of: CE 0.14047712087631226, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/229, Loss=0.14793700948357583
Loss made of: CE 0.11049392074346542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/229, Loss=0.12064834833145141
Loss made of: CE 0.12460659444332123, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/229, Loss=0.13154674991965293
Loss made of: CE 0.12225015461444855, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/229, Loss=0.13385491594672203
Loss made of: CE 0.11721760034561157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/229, Loss=0.13206081315875054
Loss made of: CE 0.10726368427276611, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/229, Loss=0.12705271318554878
Loss made of: CE 0.14799641072750092, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/229, Loss=0.13571337684988977
Loss made of: CE 0.12941446900367737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/229, Loss=0.1392999418079853
Loss made of: CE 0.15307199954986572, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/229, Loss=0.13351601138710975
Loss made of: CE 0.15885987877845764, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/229, Loss=0.11894099861383438
Loss made of: CE 0.09192296117544174, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/229, Loss=0.13206998705863954
Loss made of: CE 0.17726711928844452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/229, Loss=0.13862477391958236
Loss made of: CE 0.15341494977474213, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/229, Loss=0.12345262765884399
Loss made of: CE 0.13211601972579956, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/229, Loss=0.12985555380582808
Loss made of: CE 0.1315799206495285, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/229, Loss=0.12441035062074661
Loss made of: CE 0.13593801856040955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/229, Loss=0.14469041898846627
Loss made of: CE 0.14898651838302612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 210/229, Loss=0.13517041578888894
Loss made of: CE 0.14070764183998108, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 220/229, Loss=0.12397151961922645
Loss made of: CE 0.15176396071910858, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.13156723976135254, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.13156723976135254, Class Loss=0.13156723976135254, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/229, Loss=0.13915613740682603
Loss made of: CE 0.1418248414993286, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/229, Loss=0.12104393020272255
Loss made of: CE 0.10301067680120468, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/229, Loss=0.13123818188905717
Loss made of: CE 0.0782049223780632, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/229, Loss=0.12763746678829194
Loss made of: CE 0.1227973997592926, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/229, Loss=0.1207810178399086
Loss made of: CE 0.14906403422355652, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/229, Loss=0.1378280632197857
Loss made of: CE 0.1261034905910492, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/229, Loss=0.1148294523358345
Loss made of: CE 0.10748859494924545, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/229, Loss=0.11357736513018608
Loss made of: CE 0.11803768575191498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/229, Loss=0.11253588572144509
Loss made of: CE 0.1251733899116516, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/229, Loss=0.12233653590083123
Loss made of: CE 0.13612684607505798, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/229, Loss=0.11829625368118286
Loss made of: CE 0.09414534270763397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/229, Loss=0.12389331161975861
Loss made of: CE 0.18326464295387268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/229, Loss=0.1303551085293293
Loss made of: CE 0.1305122822523117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/229, Loss=0.11624768450856209
Loss made of: CE 0.14480382204055786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/229, Loss=0.11598999202251434
Loss made of: CE 0.11776363849639893, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/229, Loss=0.11779563874006271
Loss made of: CE 0.11156608164310455, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/229, Loss=0.11390211060643196
Loss made of: CE 0.09464022517204285, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/229, Loss=0.11795752272009849
Loss made of: CE 0.1309042125940323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/229, Loss=0.11533558145165443
Loss made of: CE 0.12764684855937958, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/229, Loss=0.12878394722938538
Loss made of: CE 0.18547068536281586, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 210/229, Loss=0.13019266203045846
Loss made of: CE 0.1545569896697998, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 220/229, Loss=0.11917918846011162
Loss made of: CE 0.1356503814458847, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.12245509773492813, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.12245509773492813, Class Loss=0.12245509773492813, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/229, Loss=0.10924820303916931
Loss made of: CE 0.13122692704200745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/229, Loss=0.1375445879995823
Loss made of: CE 0.14827704429626465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/229, Loss=0.1080206073820591
Loss made of: CE 0.10874917358160019, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/229, Loss=0.10589893460273743
Loss made of: CE 0.0902407094836235, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/229, Loss=0.1106978103518486
Loss made of: CE 0.09146972745656967, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/229, Loss=0.11190176457166671
Loss made of: CE 0.10301405191421509, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/229, Loss=0.12081091552972793
Loss made of: CE 0.12195100635290146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/229, Loss=0.1124955765902996
Loss made of: CE 0.10839413106441498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/229, Loss=0.1150523878633976
Loss made of: CE 0.13231539726257324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/229, Loss=0.12323100939393043
Loss made of: CE 0.12165877223014832, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/229, Loss=0.11333136409521102
Loss made of: CE 0.0965631753206253, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/229, Loss=0.11529166623950005
Loss made of: CE 0.0953405424952507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/229, Loss=0.12085823863744735
Loss made of: CE 0.12072671949863434, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/229, Loss=0.12464110776782036
Loss made of: CE 0.113650381565094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/229, Loss=0.11428899466991424
Loss made of: CE 0.09929114580154419, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/229, Loss=0.12770948857069014
Loss made of: CE 0.09118761867284775, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/229, Loss=0.10854848101735115
Loss made of: CE 0.08850866556167603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/229, Loss=0.11422960087656975
Loss made of: CE 0.11871423572301865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/229, Loss=0.11851966753602028
Loss made of: CE 0.15840882062911987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/229, Loss=0.11198274344205857
Loss made of: CE 0.14967426657676697, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 210/229, Loss=0.11375523954629899
Loss made of: CE 0.1163686215877533, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 220/229, Loss=0.1215920552611351
Loss made of: CE 0.11124597489833832, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11621640622615814, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.11621640622615814, Class Loss=0.11621640622615814, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/198, Loss=0.17299899309873581
Loss made of: CE 0.10281533002853394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/198, Loss=0.17767009288072586
Loss made of: CE 0.2068631500005722, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/198, Loss=0.19698581099510193
Loss made of: CE 0.14998556673526764, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/198, Loss=0.18253971487283707
Loss made of: CE 0.15920603275299072, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/198, Loss=0.1633451282978058
Loss made of: CE 0.21240517497062683, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/198, Loss=0.18635462671518327
Loss made of: CE 0.18685409426689148, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/198, Loss=0.17245123088359832
Loss made of: CE 0.15027931332588196, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/198, Loss=0.16066418290138246
Loss made of: CE 0.14246541261672974, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/198, Loss=0.17525189965963364
Loss made of: CE 0.12522578239440918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/198, Loss=0.17703866362571716
Loss made of: CE 0.20442819595336914, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/198, Loss=0.15488654896616935
Loss made of: CE 0.15873882174491882, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/198, Loss=0.1679808959364891
Loss made of: CE 0.14444562792778015, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/198, Loss=0.1717326506972313
Loss made of: CE 0.18480299413204193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/198, Loss=0.15975444614887238
Loss made of: CE 0.13049478828907013, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/198, Loss=0.16745266765356065
Loss made of: CE 0.22737377882003784, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/198, Loss=0.16219179332256317
Loss made of: CE 0.12849901616573334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/198, Loss=0.16796413362026213
Loss made of: CE 0.14477311074733734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/198, Loss=0.15358469560742377
Loss made of: CE 0.14169588685035706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/198, Loss=0.1507719226181507
Loss made of: CE 0.15843963623046875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.16913007199764252, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.16913007199764252, Class Loss=0.16913007199764252, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007770
Epoch 2, Batch 10/198, Loss=0.14407383948564528
Loss made of: CE 0.11995558440685272, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/198, Loss=0.13809911981225015
Loss made of: CE 0.1000748723745346, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/198, Loss=0.137517037242651
Loss made of: CE 0.11844587326049805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/198, Loss=0.1396934486925602
Loss made of: CE 0.13830144703388214, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/198, Loss=0.15439242869615555
Loss made of: CE 0.17075809836387634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/198, Loss=0.1552341178059578
Loss made of: CE 0.14182855188846588, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/198, Loss=0.1635640010237694
Loss made of: CE 0.1235618144273758, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/198, Loss=0.15436618998646737
Loss made of: CE 0.12781034409999847, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/198, Loss=0.16843041703104972
Loss made of: CE 0.17905697226524353, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/198, Loss=0.15996523201465607
Loss made of: CE 0.14395368099212646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/198, Loss=0.14260300770401954
Loss made of: CE 0.1177816241979599, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/198, Loss=0.14268463999032974
Loss made of: CE 0.1405295729637146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/198, Loss=0.14702025577425956
Loss made of: CE 0.18381249904632568, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/198, Loss=0.16221230626106262
Loss made of: CE 0.18273648619651794, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/198, Loss=0.16169424280524253
Loss made of: CE 0.13598355650901794, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/198, Loss=0.15303342640399933
Loss made of: CE 0.14636850357055664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/198, Loss=0.1504812017083168
Loss made of: CE 0.1585056632757187, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/198, Loss=0.16005801483988763
Loss made of: CE 0.19144181907176971, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/198, Loss=0.16001005917787553
Loss made of: CE 0.2082568109035492, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.15240447223186493, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.15240447223186493, Class Loss=0.15240447223186493, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.007358
Epoch 3, Batch 10/198, Loss=0.15265655890107155
Loss made of: CE 0.1437268853187561, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/198, Loss=0.13473186045885086
Loss made of: CE 0.15126416087150574, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/198, Loss=0.1346874825656414
Loss made of: CE 0.14307139813899994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/198, Loss=0.14686307385563852
Loss made of: CE 0.13852332532405853, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/198, Loss=0.13244911581277846
Loss made of: CE 0.12232881784439087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/198, Loss=0.13136761635541916
Loss made of: CE 0.1250433325767517, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/198, Loss=0.1478177413344383
Loss made of: CE 0.16442716121673584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/198, Loss=0.14408274069428445
Loss made of: CE 0.1749463528394699, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/198, Loss=0.13680182844400407
Loss made of: CE 0.13228332996368408, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/198, Loss=0.14115767627954484
Loss made of: CE 0.11792224645614624, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/198, Loss=0.14207608699798585
Loss made of: CE 0.1221025139093399, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/198, Loss=0.14582864195108414
Loss made of: CE 0.15294252336025238, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/198, Loss=0.1454389847815037
Loss made of: CE 0.15929999947547913, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/198, Loss=0.15579197108745574
Loss made of: CE 0.13487403094768524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/198, Loss=0.163310906291008
Loss made of: CE 0.20659413933753967, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/198, Loss=0.15081966891884804
Loss made of: CE 0.16848717629909515, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/198, Loss=0.14680879414081574
Loss made of: CE 0.13498245179653168, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/198, Loss=0.13775217905640602
Loss made of: CE 0.13106125593185425, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/198, Loss=0.14133639112114907
Loss made of: CE 0.1565488874912262, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.14366643130779266, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.14366643130779266, Class Loss=0.14366643130779266, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.006943
Epoch 4, Batch 10/198, Loss=0.1427254095673561
Loss made of: CE 0.12819688022136688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/198, Loss=0.1446657098829746
Loss made of: CE 0.18332654237747192, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/198, Loss=0.14932310283184053
Loss made of: CE 0.12698762118816376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/198, Loss=0.12200639620423318
Loss made of: CE 0.11962704360485077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/198, Loss=0.1423776164650917
Loss made of: CE 0.1637352705001831, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/198, Loss=0.12309113442897797
Loss made of: CE 0.12970766425132751, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/198, Loss=0.1335916019976139
Loss made of: CE 0.21263234317302704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/198, Loss=0.13266763538122178
Loss made of: CE 0.13679340481758118, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/198, Loss=0.11830495968461037
Loss made of: CE 0.14478003978729248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/198, Loss=0.13261066749691963
Loss made of: CE 0.1573278307914734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/198, Loss=0.1305118754506111
Loss made of: CE 0.14713934063911438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/198, Loss=0.12218724042177201
Loss made of: CE 0.15827485918998718, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/198, Loss=0.14826545640826225
Loss made of: CE 0.14377442002296448, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/198, Loss=0.14152223616838455
Loss made of: CE 0.15313228964805603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/198, Loss=0.1278145655989647
Loss made of: CE 0.16366995871067047, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/198, Loss=0.1401451550424099
Loss made of: CE 0.1599111258983612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/198, Loss=0.1251194231212139
Loss made of: CE 0.12975339591503143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/198, Loss=0.13306609466671943
Loss made of: CE 0.14763149619102478, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/198, Loss=0.12853429093956947
Loss made of: CE 0.1585952341556549, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.13296793401241302, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.13296793401241302, Class Loss=0.13296793401241302, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006525
Epoch 5, Batch 10/198, Loss=0.1257489539682865
Loss made of: CE 0.15016040205955505, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/198, Loss=0.12422923892736434
Loss made of: CE 0.15526220202445984, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/198, Loss=0.1188779391348362
Loss made of: CE 0.10984615981578827, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/198, Loss=0.11377871483564377
Loss made of: CE 0.11963998526334763, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/198, Loss=0.11935191154479981
Loss made of: CE 0.1401626318693161, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/198, Loss=0.12222381085157394
Loss made of: CE 0.0956849679350853, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/198, Loss=0.12535640448331833
Loss made of: CE 0.10682710260152817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/198, Loss=0.11990254521369934
Loss made of: CE 0.0928376168012619, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/198, Loss=0.11973157897591591
Loss made of: CE 0.10960531234741211, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/198, Loss=0.12492845803499222
Loss made of: CE 0.150943785905838, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/198, Loss=0.13405806124210357
Loss made of: CE 0.12074951827526093, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/198, Loss=0.12960951253771782
Loss made of: CE 0.15000417828559875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/198, Loss=0.1230980135500431
Loss made of: CE 0.12033158540725708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/198, Loss=0.12496151477098465
Loss made of: CE 0.13464227318763733, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/198, Loss=0.12344189584255219
Loss made of: CE 0.12511424720287323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/198, Loss=0.12368296459317207
Loss made of: CE 0.10124413669109344, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/198, Loss=0.12968208342790605
Loss made of: CE 0.0975518673658371, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/198, Loss=0.1300282098352909
Loss made of: CE 0.13339796662330627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/198, Loss=0.12529539316892624
Loss made of: CE 0.10431250184774399, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1239515095949173, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.1239515095949173, Class Loss=0.1239515095949173, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006104
Epoch 6, Batch 10/198, Loss=0.11150833740830421
Loss made of: CE 0.15387511253356934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/198, Loss=0.12106269970536232
Loss made of: CE 0.109779492020607, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/198, Loss=0.11616526320576667
Loss made of: CE 0.13038501143455505, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/198, Loss=0.13362199813127518
Loss made of: CE 0.12100861966609955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/198, Loss=0.12072189152240753
Loss made of: CE 0.10165166854858398, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/198, Loss=0.12862607315182686
Loss made of: CE 0.1069410890340805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/198, Loss=0.11775104627013207
Loss made of: CE 0.10621720552444458, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/198, Loss=0.11209807246923446
Loss made of: CE 0.11578933894634247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/198, Loss=0.11905727162957191
Loss made of: CE 0.1572198122739792, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/198, Loss=0.11210078224539757
Loss made of: CE 0.07362811267375946, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/198, Loss=0.12655111029744148
Loss made of: CE 0.1657286286354065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/198, Loss=0.13063830584287645
Loss made of: CE 0.14132298529148102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/198, Loss=0.13270631209015846
Loss made of: CE 0.13201309740543365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/198, Loss=0.13037503957748414
Loss made of: CE 0.1290103793144226, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/198, Loss=0.12988122105598449
Loss made of: CE 0.16368252038955688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/198, Loss=0.11802738755941392
Loss made of: CE 0.08556892722845078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/198, Loss=0.12476009503006935
Loss made of: CE 0.12546585500240326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/198, Loss=0.11231864094734192
Loss made of: CE 0.10175340622663498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/198, Loss=0.11188780814409256
Loss made of: CE 0.1071346253156662, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.12129082530736923, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.12129082530736923, Class Loss=0.12129082530736923, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/200, Loss=0.16278985962271691
Loss made of: CE 0.20523712038993835, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/200, Loss=0.1479479394853115
Loss made of: CE 0.12751555442810059, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/200, Loss=0.15809544622898103
Loss made of: CE 0.137751966714859, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/200, Loss=0.15436407998204232
Loss made of: CE 0.12610456347465515, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/200, Loss=0.14282117486000062
Loss made of: CE 0.16676995158195496, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/200, Loss=0.15000718981027603
Loss made of: CE 0.16940152645111084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/200, Loss=0.1569121204316616
Loss made of: CE 0.12239420413970947, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/200, Loss=0.14751096591353416
Loss made of: CE 0.13734081387519836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/200, Loss=0.13849182799458504
Loss made of: CE 0.14753709733486176, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/200, Loss=0.14287832230329514
Loss made of: CE 0.12767374515533447, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/200, Loss=0.1559277892112732
Loss made of: CE 0.1443987339735031, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/200, Loss=0.14867895171046258
Loss made of: CE 0.12832871079444885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/200, Loss=0.138225506991148
Loss made of: CE 0.15677541494369507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/200, Loss=0.14036269560456277
Loss made of: CE 0.1307155191898346, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/200, Loss=0.16803418844938278
Loss made of: CE 0.15734702348709106, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/200, Loss=0.15287403911352157
Loss made of: CE 0.19052299857139587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/200, Loss=0.16700780317187308
Loss made of: CE 0.13835382461547852, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/200, Loss=0.15571155324578284
Loss made of: CE 0.1236250028014183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/200, Loss=0.13424147814512252
Loss made of: CE 0.12611857056617737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/200, Loss=0.15176648050546646
Loss made of: CE 0.149912029504776, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.15073247253894806, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.15073247253894806, Class Loss=0.15073247253894806, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.005998
Epoch 2, Batch 10/200, Loss=0.1366230495274067
Loss made of: CE 0.12292676419019699, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/200, Loss=0.12468666806817055
Loss made of: CE 0.11199051141738892, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/200, Loss=0.13667440190911292
Loss made of: CE 0.18890057504177094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/200, Loss=0.14490807354450225
Loss made of: CE 0.12487979233264923, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/200, Loss=0.1493874780833721
Loss made of: CE 0.18700794875621796, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/200, Loss=0.14149039909243583
Loss made of: CE 0.11980559676885605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/200, Loss=0.13346575945615768
Loss made of: CE 0.09143271297216415, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/200, Loss=0.14738546311855316
Loss made of: CE 0.16164854168891907, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/200, Loss=0.11839501038193703
Loss made of: CE 0.15896163880825043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/200, Loss=0.13065663799643518
Loss made of: CE 0.1421697437763214, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/200, Loss=0.1403105564415455
Loss made of: CE 0.1326439380645752, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/200, Loss=0.14138896465301515
Loss made of: CE 0.15507720410823822, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/200, Loss=0.140805946290493
Loss made of: CE 0.1308416724205017, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/200, Loss=0.14950964078307152
Loss made of: CE 0.195440873503685, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/200, Loss=0.15623465180397034
Loss made of: CE 0.11034037917852402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/200, Loss=0.15390308126807212
Loss made of: CE 0.1770920604467392, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/200, Loss=0.14234368726611138
Loss made of: CE 0.1496429443359375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/200, Loss=0.1327909469604492
Loss made of: CE 0.15296262502670288, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/200, Loss=0.15975210145115853
Loss made of: CE 0.20157860219478607, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/200, Loss=0.14553492963314058
Loss made of: CE 0.1846143901348114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1413123607635498, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.1413123607635498, Class Loss=0.1413123607635498, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.005679
Epoch 3, Batch 10/200, Loss=0.12520683258771897
Loss made of: CE 0.13394588232040405, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/200, Loss=0.14643218964338303
Loss made of: CE 0.19078725576400757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/200, Loss=0.1322098672389984
Loss made of: CE 0.1333298683166504, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/200, Loss=0.13644574210047722
Loss made of: CE 0.13786965608596802, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/200, Loss=0.12832176610827445
Loss made of: CE 0.14423567056655884, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/200, Loss=0.14108893424272537
Loss made of: CE 0.14220377802848816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/200, Loss=0.12878276407718658
Loss made of: CE 0.11964207887649536, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/200, Loss=0.12964626178145408
Loss made of: CE 0.12405484169721603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/200, Loss=0.12426017671823501
Loss made of: CE 0.09298287332057953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/200, Loss=0.12699056118726731
Loss made of: CE 0.1344682276248932, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/200, Loss=0.11743642315268517
Loss made of: CE 0.10925382375717163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/200, Loss=0.12757982388138772
Loss made of: CE 0.09797035157680511, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/200, Loss=0.13556144312024115
Loss made of: CE 0.14747220277786255, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/200, Loss=0.1405417427420616
Loss made of: CE 0.16961877048015594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/200, Loss=0.12655514627695083
Loss made of: CE 0.10149405896663666, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/200, Loss=0.1498372472822666
Loss made of: CE 0.14880603551864624, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/200, Loss=0.12365795075893402
Loss made of: CE 0.12773741781711578, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/200, Loss=0.12684387415647508
Loss made of: CE 0.1112988218665123, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/200, Loss=0.130599058419466
Loss made of: CE 0.15698224306106567, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/200, Loss=0.13851427882909775
Loss made of: CE 0.1187940463423729, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.13182559609413147, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.13182559609413147, Class Loss=0.13182559609413147, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/200, Loss=0.14227453544735907
Loss made of: CE 0.1269320398569107, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/200, Loss=0.11501725167036056
Loss made of: CE 0.10940613597631454, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/200, Loss=0.125423364341259
Loss made of: CE 0.11280562728643417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/200, Loss=0.1296304002404213
Loss made of: CE 0.1200026273727417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/200, Loss=0.13522807955741883
Loss made of: CE 0.13319751620292664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/200, Loss=0.11548179909586906
Loss made of: CE 0.13255225121974945, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/200, Loss=0.13902998864650726
Loss made of: CE 0.1585521697998047, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/200, Loss=0.13235778063535691
Loss made of: CE 0.11770902574062347, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/200, Loss=0.12984827905893326
Loss made of: CE 0.12323008477687836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/200, Loss=0.12534108608961106
Loss made of: CE 0.1182461678981781, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/200, Loss=0.11882618069648743
Loss made of: CE 0.10579037666320801, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/200, Loss=0.13119413405656816
Loss made of: CE 0.12428143620491028, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/200, Loss=0.11766903772950173
Loss made of: CE 0.10913878679275513, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/200, Loss=0.1298522599041462
Loss made of: CE 0.10611965507268906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/200, Loss=0.14321249350905418
Loss made of: CE 0.12365859746932983, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/200, Loss=0.14189310744404793
Loss made of: CE 0.10560977458953857, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/200, Loss=0.13205581083893775
Loss made of: CE 0.1212727352976799, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/200, Loss=0.13177539110183717
Loss made of: CE 0.1375986933708191, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/200, Loss=0.12655107900500298
Loss made of: CE 0.12280010432004929, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/200, Loss=0.11928303986787796
Loss made of: CE 0.12773872911930084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.12909725308418274, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.12909725308418274, Class Loss=0.12909725308418274, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.005036
Epoch 5, Batch 10/200, Loss=0.12457817867398262
Loss made of: CE 0.12107889354228973, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/200, Loss=0.13571380376815795
Loss made of: CE 0.12575754523277283, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/200, Loss=0.11591024175286294
Loss made of: CE 0.13483792543411255, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/200, Loss=0.11622102931141853
Loss made of: CE 0.121527761220932, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/200, Loss=0.12648327574133872
Loss made of: CE 0.16599473357200623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/200, Loss=0.11387604549527168
Loss made of: CE 0.11743950843811035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/200, Loss=0.1268024355173111
Loss made of: CE 0.09776818752288818, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/200, Loss=0.1306866243481636
Loss made of: CE 0.09501905739307404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/200, Loss=0.13223069608211518
Loss made of: CE 0.09527426958084106, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/200, Loss=0.12666384726762772
Loss made of: CE 0.12114053219556808, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/200, Loss=0.11157689616084099
Loss made of: CE 0.1146344467997551, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/200, Loss=0.11741810142993928
Loss made of: CE 0.1209816187620163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/200, Loss=0.10569252893328666
Loss made of: CE 0.11324070394039154, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/200, Loss=0.12601982206106185
Loss made of: CE 0.12485840171575546, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/200, Loss=0.1295991823077202
Loss made of: CE 0.12314096093177795, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/200, Loss=0.13026059344410895
Loss made of: CE 0.10796096920967102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/200, Loss=0.1224471352994442
Loss made of: CE 0.12414127588272095, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/200, Loss=0.11882255822420121
Loss made of: CE 0.10552980750799179, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/200, Loss=0.12852905467152595
Loss made of: CE 0.12924261391162872, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/200, Loss=0.1199829824268818
Loss made of: CE 0.09594377130270004, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.12297574430704117, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.12297574430704117, Class Loss=0.12297574430704117, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.004711
Epoch 6, Batch 10/200, Loss=0.11682038977742196
Loss made of: CE 0.11709992587566376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/200, Loss=0.12099620252847672
Loss made of: CE 0.10405100882053375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/200, Loss=0.12239920869469642
Loss made of: CE 0.15900349617004395, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/200, Loss=0.10826965123414993
Loss made of: CE 0.1062467098236084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/200, Loss=0.11207146868109703
Loss made of: CE 0.10907678306102753, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/200, Loss=0.10490978807210923
Loss made of: CE 0.09001921117305756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/200, Loss=0.10510894879698754
Loss made of: CE 0.08872616291046143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/200, Loss=0.13865273743867873
Loss made of: CE 0.14645886421203613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/200, Loss=0.11732250973582267
Loss made of: CE 0.10224181413650513, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/200, Loss=0.12239235118031502
Loss made of: CE 0.13909634947776794, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/200, Loss=0.11543362289667129
Loss made of: CE 0.13558533787727356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/200, Loss=0.11919383555650712
Loss made of: CE 0.12075760960578918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/200, Loss=0.1285137802362442
Loss made of: CE 0.11481332778930664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/200, Loss=0.11894761398434639
Loss made of: CE 0.15040622651576996, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/200, Loss=0.12397425770759582
Loss made of: CE 0.10632389783859253, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/200, Loss=0.13001699447631837
Loss made of: CE 0.10373284667730331, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/200, Loss=0.11614306271076202
Loss made of: CE 0.147203728556633, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/200, Loss=0.11113460883498191
Loss made of: CE 0.12707313895225525, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/200, Loss=0.11957487091422081
Loss made of: CE 0.13177262246608734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/200, Loss=0.12266886308789253
Loss made of: CE 0.15568821132183075, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11872722953557968, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.11872722953557968, Class Loss=0.11872722953557968, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/157, Loss=0.15354318395256997
Loss made of: CE 0.16387459635734558, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=0.14382182732224463
Loss made of: CE 0.10391564667224884, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.13812024891376495
Loss made of: CE 0.1928374469280243, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.11871127560734748
Loss made of: CE 0.09210069477558136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.12880609482526778
Loss made of: CE 0.08517378568649292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.15128543227910995
Loss made of: CE 0.14051435887813568, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.1435731254518032
Loss made of: CE 0.1554359793663025, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.12392636090517044
Loss made of: CE 0.12928420305252075, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.12267254441976547
Loss made of: CE 0.15501050651073456, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.12214185148477555
Loss made of: CE 0.1186101958155632, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.11954126507043839
Loss made of: CE 0.1068328246474266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.13175470679998397
Loss made of: CE 0.13751697540283203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.1267694912850857
Loss made of: CE 0.16231273114681244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.13225740641355516
Loss made of: CE 0.11812959611415863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.12185085341334342
Loss made of: CE 0.11683395504951477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.13222946226596832, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.13222946226596832, Class Loss=0.13222946226596832, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.005998
Epoch 2, Batch 10/157, Loss=0.11200460344552994
Loss made of: CE 0.11600055545568466, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.1277257688343525
Loss made of: CE 0.09435240924358368, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.12544743493199348
Loss made of: CE 0.12861643731594086, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.12159659564495087
Loss made of: CE 0.1101120114326477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.11328226402401924
Loss made of: CE 0.08357049524784088, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.1293954722583294
Loss made of: CE 0.14409056305885315, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.12453623637557029
Loss made of: CE 0.10817933082580566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.11338617876172066
Loss made of: CE 0.09941651672124863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.12469313442707061
Loss made of: CE 0.10745376348495483, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.11707741469144821
Loss made of: CE 0.13066306710243225, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.10705358907580376
Loss made of: CE 0.12530925869941711, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.11212226748466492
Loss made of: CE 0.11612457036972046, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.12023335322737694
Loss made of: CE 0.11568734049797058, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.1086182177066803
Loss made of: CE 0.08804123848676682, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.13408327624201774
Loss made of: CE 0.12216868251562119, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.12018439918756485, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.12018439918756485, Class Loss=0.12018439918756485, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.005679
Epoch 3, Batch 10/157, Loss=0.11023953631520271
Loss made of: CE 0.12478413432836533, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.10591330826282501
Loss made of: CE 0.08967121690511703, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.10368827506899833
Loss made of: CE 0.10633385181427002, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.11793146878480912
Loss made of: CE 0.11231257766485214, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.11727346181869507
Loss made of: CE 0.11014877259731293, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.11592954099178314
Loss made of: CE 0.12713293731212616, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.11782674714922906
Loss made of: CE 0.09916779398918152, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.11246431469917298
Loss made of: CE 0.09950938075780869, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.10393802970647811
Loss made of: CE 0.12713512778282166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.11008423343300819
Loss made of: CE 0.10341991484165192, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.11925451681017876
Loss made of: CE 0.1261425018310547, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.10114063546061516
Loss made of: CE 0.12812380492687225, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.12117410600185394
Loss made of: CE 0.13851681351661682, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.13293917775154113
Loss made of: CE 0.12170036882162094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.10020011961460114
Loss made of: CE 0.0845850333571434, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.11282026767730713, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.11282026767730713, Class Loss=0.11282026767730713, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/157, Loss=0.10425742790102958
Loss made of: CE 0.11443492025136948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.11232511624693871
Loss made of: CE 0.07816668599843979, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.1270945630967617
Loss made of: CE 0.1456286907196045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.10010522902011872
Loss made of: CE 0.10651953518390656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.1097520224750042
Loss made of: CE 0.1079966351389885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.11099321767687798
Loss made of: CE 0.14740583300590515, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.1009682334959507
Loss made of: CE 0.1048964411020279, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.11610838547348976
Loss made of: CE 0.11192094534635544, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.11452815979719162
Loss made of: CE 0.10093340277671814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.11534729301929474
Loss made of: CE 0.1323474943637848, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.11458401530981063
Loss made of: CE 0.10465317964553833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.10022524297237397
Loss made of: CE 0.09480524063110352, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.10794632136821747
Loss made of: CE 0.09655822813510895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.11823276802897453
Loss made of: CE 0.08955824375152588, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.10460313335061074
Loss made of: CE 0.12152355909347534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10986058413982391, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.10986058413982391, Class Loss=0.10986058413982391, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.005036
Epoch 5, Batch 10/157, Loss=0.09578262120485306
Loss made of: CE 0.09239691495895386, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.10423805788159371
Loss made of: CE 0.10557147115468979, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.1158290833234787
Loss made of: CE 0.1936715543270111, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.10104372464120388
Loss made of: CE 0.11728306859731674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.09947507381439209
Loss made of: CE 0.08232265710830688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.10537782907485962
Loss made of: CE 0.11319981515407562, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.10874502658843994
Loss made of: CE 0.1504548341035843, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.10448395535349846
Loss made of: CE 0.09247326850891113, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.10655276626348495
Loss made of: CE 0.10286691039800644, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.12086221277713775
Loss made of: CE 0.11056163161993027, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.11412971317768097
Loss made of: CE 0.08714431524276733, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.11158438697457314
Loss made of: CE 0.13801178336143494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.11169515326619148
Loss made of: CE 0.11499543488025665, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.10482558012008666
Loss made of: CE 0.09529348462820053, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.1137065514922142
Loss made of: CE 0.10543590784072876, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10782939195632935, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.10782939195632935, Class Loss=0.10782939195632935, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.004711
Epoch 6, Batch 10/157, Loss=0.10777460485696792
Loss made of: CE 0.10122856497764587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.10318269208073616
Loss made of: CE 0.1021263524889946, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.10579641908407211
Loss made of: CE 0.12000036239624023, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.09676924496889114
Loss made of: CE 0.06829309463500977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.10340561047196388
Loss made of: CE 0.09586171805858612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.10022918358445168
Loss made of: CE 0.07642960548400879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.10543145462870598
Loss made of: CE 0.09374240040779114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.08970712274312972
Loss made of: CE 0.087262824177742, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.10354894995689393
Loss made of: CE 0.0953073501586914, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.10958202704787254
Loss made of: CE 0.09704320877790451, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.09886129349470138
Loss made of: CE 0.11123405396938324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.10087990760803223
Loss made of: CE 0.06046736240386963, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.10173003673553467
Loss made of: CE 0.12227221578359604, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.11126820668578148
Loss made of: CE 0.15266990661621094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.10150906518101692
Loss made of: CE 0.1053643524646759, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1027754545211792, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.1027754545211792, Class Loss=0.1027754545211792, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.1511213779449463, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.948030
Mean Acc: 0.884169
FreqW Acc: 0.906296
Mean IoU: 0.782274
Class IoU:
	class 0: 0.94029963
	class 1: 0.8851273
	class 2: 0.4104175
	class 3: 0.8746309
	class 4: 0.7143844
	class 5: 0.79910684
	class 6: 0.93967545
	class 7: 0.9107754
	class 8: 0.8987555
	class 9: 0.4484224
	class 10: 0.7523559
	class 11: 0.5777576
	class 12: 0.8589301
	class 13: 0.765111
	class 14: 0.8745067
	class 15: 0.866132
Class Acc:
	class 0: 0.9673365
	class 1: 0.9627121
	class 2: 0.9247202
	class 3: 0.9169265
	class 4: 0.8764112
	class 5: 0.89819
	class 6: 0.9687275
	class 7: 0.9406992
	class 8: 0.95967376
	class 9: 0.5893371
	class 10: 0.798881
	class 11: 0.61212045
	class 12: 0.96151716
	class 13: 0.89209014
	class 14: 0.9506444
	class 15: 0.9267238

federated global round: 3, step: 0
select part of clients to conduct local training
[9, 5, 0, 3]
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/205, Loss=0.1259291276335716
Loss made of: CE 0.09745292365550995, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/205, Loss=0.14715884104371071
Loss made of: CE 0.08876484632492065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/205, Loss=0.13493138551712036
Loss made of: CE 0.1328505277633667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/205, Loss=0.12260752767324448
Loss made of: CE 0.17049984633922577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/205, Loss=0.16011320799589157
Loss made of: CE 0.13489758968353271, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/205, Loss=0.15196118876338005
Loss made of: CE 0.12265528738498688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/205, Loss=0.137780574709177
Loss made of: CE 0.10282866656780243, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/205, Loss=0.15305518880486488
Loss made of: CE 0.21145641803741455, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/205, Loss=0.12495657354593277
Loss made of: CE 0.08899464458227158, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/205, Loss=0.13973231837153435
Loss made of: CE 0.1198975145816803, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/205, Loss=0.1349601097404957
Loss made of: CE 0.12356279790401459, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/205, Loss=0.14938174560666084
Loss made of: CE 0.15081673860549927, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/205, Loss=0.14111355170607567
Loss made of: CE 0.10309573262929916, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/205, Loss=0.1294522024691105
Loss made of: CE 0.10167837142944336, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/205, Loss=0.13542656898498534
Loss made of: CE 0.1273767650127411, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/205, Loss=0.1319746658205986
Loss made of: CE 0.15034526586532593, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/205, Loss=0.133978720754385
Loss made of: CE 0.1326816976070404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/205, Loss=0.12982705757021903
Loss made of: CE 0.12769030034542084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/205, Loss=0.13050376027822494
Loss made of: CE 0.11846810579299927, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/205, Loss=0.13485907316207885
Loss made of: CE 0.15047264099121094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1375265121459961, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.1375265121459961, Class Loss=0.1375265121459961, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.005839
Epoch 2, Batch 10/205, Loss=0.14401964843273163
Loss made of: CE 0.2905596196651459, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/205, Loss=0.13475639671087264
Loss made of: CE 0.12845323979854584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/205, Loss=0.12048503160476684
Loss made of: CE 0.12523500621318817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/205, Loss=0.11634609326720238
Loss made of: CE 0.10329870879650116, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/205, Loss=0.11969390064477921
Loss made of: CE 0.15157216787338257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/205, Loss=0.139198699593544
Loss made of: CE 0.1355503648519516, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/205, Loss=0.14321859776973725
Loss made of: CE 0.1958356350660324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/205, Loss=0.11680608540773392
Loss made of: CE 0.11474063247442245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/205, Loss=0.11796857491135597
Loss made of: CE 0.13218387961387634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/205, Loss=0.13807987794280052
Loss made of: CE 0.11864091455936432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/205, Loss=0.12795225605368615
Loss made of: CE 0.11222188174724579, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/205, Loss=0.12261468023061753
Loss made of: CE 0.1276673525571823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/205, Loss=0.13179372623562813
Loss made of: CE 0.11659207940101624, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/205, Loss=0.12894127815961837
Loss made of: CE 0.12726595997810364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/205, Loss=0.11987258717417718
Loss made of: CE 0.14421039819717407, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/205, Loss=0.13773970901966096
Loss made of: CE 0.11007608473300934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/205, Loss=0.12037889212369919
Loss made of: CE 0.17864614725112915, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/205, Loss=0.12096173763275146
Loss made of: CE 0.0943119153380394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/205, Loss=0.11481491848826408
Loss made of: CE 0.10293664783239365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/205, Loss=0.12412696182727814
Loss made of: CE 0.11326852440834045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.12671498954296112, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.12671498954296112, Class Loss=0.12671498954296112, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.005359
Epoch 3, Batch 10/205, Loss=0.10938712880015374
Loss made of: CE 0.13544678688049316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/205, Loss=0.10943892151117325
Loss made of: CE 0.0817142054438591, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/205, Loss=0.11443516016006469
Loss made of: CE 0.09920793771743774, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/205, Loss=0.11504036262631416
Loss made of: CE 0.09297051280736923, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/205, Loss=0.11138011515140533
Loss made of: CE 0.09223028272390366, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/205, Loss=0.11241505071520805
Loss made of: CE 0.10655330866575241, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/205, Loss=0.12383868247270584
Loss made of: CE 0.10671496391296387, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/205, Loss=0.1258652128279209
Loss made of: CE 0.17124919593334198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/205, Loss=0.11902804523706437
Loss made of: CE 0.13942568004131317, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/205, Loss=0.11891300901770592
Loss made of: CE 0.11315980553627014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/205, Loss=0.11554869487881661
Loss made of: CE 0.1466277539730072, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/205, Loss=0.11805384978652
Loss made of: CE 0.09693842381238937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/205, Loss=0.11874261647462844
Loss made of: CE 0.11917422711849213, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/205, Loss=0.12135274559259415
Loss made of: CE 0.10803684592247009, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/205, Loss=0.1147586576640606
Loss made of: CE 0.11442679166793823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/205, Loss=0.126041466742754
Loss made of: CE 0.14057521522045135, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/205, Loss=0.12013033330440522
Loss made of: CE 0.12847790122032166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/205, Loss=0.11406177431344985
Loss made of: CE 0.11118873208761215, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/205, Loss=0.1316085010766983
Loss made of: CE 0.14056560397148132, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/205, Loss=0.11575841084122658
Loss made of: CE 0.1242704913020134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.11741990596055984, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.11741990596055984, Class Loss=0.11741990596055984, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.004874
Epoch 4, Batch 10/205, Loss=0.1235742636024952
Loss made of: CE 0.11240055412054062, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/205, Loss=0.11626882404088974
Loss made of: CE 0.10675551742315292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/205, Loss=0.11755655184388161
Loss made of: CE 0.11027425527572632, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/205, Loss=0.11383876875042916
Loss made of: CE 0.09632907807826996, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/205, Loss=0.11803207024931908
Loss made of: CE 0.07757292687892914, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/205, Loss=0.11103698760271072
Loss made of: CE 0.14917117357254028, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/205, Loss=0.10763280019164086
Loss made of: CE 0.09330960363149643, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/205, Loss=0.11697333380579948
Loss made of: CE 0.12232993543148041, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/205, Loss=0.12724373117089272
Loss made of: CE 0.11507707834243774, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/205, Loss=0.1111472338438034
Loss made of: CE 0.10178782790899277, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/205, Loss=0.11836767196655273
Loss made of: CE 0.11313441395759583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/205, Loss=0.11094324216246605
Loss made of: CE 0.13104142248630524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/205, Loss=0.1234530232846737
Loss made of: CE 0.09991739690303802, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/205, Loss=0.12450813129544258
Loss made of: CE 0.1362358182668686, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/205, Loss=0.13373388200998307
Loss made of: CE 0.09680238366127014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/205, Loss=0.11872988194227219
Loss made of: CE 0.11194197833538055, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/205, Loss=0.12463748604059219
Loss made of: CE 0.11591526120901108, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/205, Loss=0.1298999160528183
Loss made of: CE 0.12056698650121689, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/205, Loss=0.11230172142386437
Loss made of: CE 0.12994587421417236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/205, Loss=0.10712543353438378
Loss made of: CE 0.11162982881069183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1183832660317421, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.1183832660317421, Class Loss=0.1183832660317421, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.004384
Epoch 5, Batch 10/205, Loss=0.11638702377676964
Loss made of: CE 0.15808792412281036, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/205, Loss=0.11597114577889442
Loss made of: CE 0.12068642675876617, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/205, Loss=0.12114612981677056
Loss made of: CE 0.10388287156820297, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/205, Loss=0.10674294531345367
Loss made of: CE 0.09800751507282257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/205, Loss=0.10797683149576187
Loss made of: CE 0.0947902649641037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/205, Loss=0.11408825740218162
Loss made of: CE 0.09864884614944458, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/205, Loss=0.11458610817790031
Loss made of: CE 0.10923105478286743, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/205, Loss=0.09760338440537453
Loss made of: CE 0.09093671292066574, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/205, Loss=0.11918431669473648
Loss made of: CE 0.11453290283679962, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/205, Loss=0.10861331596970558
Loss made of: CE 0.12337470054626465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/205, Loss=0.11443311795592308
Loss made of: CE 0.14197459816932678, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/205, Loss=0.11306879594922066
Loss made of: CE 0.11833456158638, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/205, Loss=0.10639297217130661
Loss made of: CE 0.09600953757762909, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/205, Loss=0.10881609693169594
Loss made of: CE 0.13383528590202332, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/205, Loss=0.10662018358707429
Loss made of: CE 0.11394383013248444, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/205, Loss=0.11077468395233155
Loss made of: CE 0.09706266969442368, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/205, Loss=0.11221708804368973
Loss made of: CE 0.08953826129436493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/205, Loss=0.11079495400190353
Loss made of: CE 0.09226156771183014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/205, Loss=0.12012768536806107
Loss made of: CE 0.10071456432342529, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/205, Loss=0.11253123432397842
Loss made of: CE 0.08604995906352997, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.11184055358171463, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.11184055358171463, Class Loss=0.11184055358171463, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.003887
Epoch 6, Batch 10/205, Loss=0.1010627843439579
Loss made of: CE 0.10792266577482224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/205, Loss=0.10892637595534324
Loss made of: CE 0.11223709583282471, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/205, Loss=0.1140975147485733
Loss made of: CE 0.11968573182821274, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/205, Loss=0.11659302189946175
Loss made of: CE 0.1430143117904663, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/205, Loss=0.10731934979557992
Loss made of: CE 0.11297743767499924, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/205, Loss=0.108684691041708
Loss made of: CE 0.08734240382909775, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/205, Loss=0.11921849697828293
Loss made of: CE 0.09653161466121674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/205, Loss=0.10678988918662072
Loss made of: CE 0.10220462083816528, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/205, Loss=0.10939614549279213
Loss made of: CE 0.10277575254440308, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/205, Loss=0.1134809635579586
Loss made of: CE 0.09628628194332123, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/205, Loss=0.09921910837292672
Loss made of: CE 0.08242672681808472, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/205, Loss=0.1035554476082325
Loss made of: CE 0.09542914479970932, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/205, Loss=0.11158186942338943
Loss made of: CE 0.11824586242437363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/205, Loss=0.10846310257911682
Loss made of: CE 0.08723962306976318, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/205, Loss=0.09700644463300705
Loss made of: CE 0.09884142875671387, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/205, Loss=0.09599108546972275
Loss made of: CE 0.09608402848243713, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/205, Loss=0.10796960070729256
Loss made of: CE 0.09287133812904358, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/205, Loss=0.10209545716643334
Loss made of: CE 0.08522766828536987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/205, Loss=0.10809061154723168
Loss made of: CE 0.09561576694250107, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/205, Loss=0.11175491958856583
Loss made of: CE 0.08911308646202087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10785564035177231, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.10785564035177231, Class Loss=0.10785564035177231, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/245, Loss=0.12053749114274978
Loss made of: CE 0.14615583419799805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/245, Loss=0.18244264274835587
Loss made of: CE 0.33618006110191345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/245, Loss=0.13749650940299035
Loss made of: CE 0.1257796436548233, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/245, Loss=0.14634812623262405
Loss made of: CE 0.14272305369377136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/245, Loss=0.17593089938163758
Loss made of: CE 0.21894073486328125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/245, Loss=0.17415670454502105
Loss made of: CE 0.15318675339221954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/245, Loss=0.19430605992674826
Loss made of: CE 0.1561853587627411, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/245, Loss=0.16736250817775727
Loss made of: CE 0.20515607297420502, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/245, Loss=0.19417705684900283
Loss made of: CE 0.16883228719234467, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/245, Loss=0.17369917035102844
Loss made of: CE 0.19490179419517517, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/245, Loss=0.17827985137701036
Loss made of: CE 0.185672327876091, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/245, Loss=0.16326395571231841
Loss made of: CE 0.17240667343139648, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/245, Loss=0.20467634126544
Loss made of: CE 0.14360082149505615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/245, Loss=0.20083374828100203
Loss made of: CE 0.2294212281703949, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/245, Loss=0.17645121812820436
Loss made of: CE 0.24983207881450653, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/245, Loss=0.1882374107837677
Loss made of: CE 0.20474772155284882, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/245, Loss=0.1878368690609932
Loss made of: CE 0.24457710981369019, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/245, Loss=0.16780916675925256
Loss made of: CE 0.12332652509212494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/245, Loss=0.18931845873594283
Loss made of: CE 0.16257384419441223, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/245, Loss=0.17658499777317047
Loss made of: CE 0.1843908131122589, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 210/245, Loss=0.17923250645399094
Loss made of: CE 0.25866633653640747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 220/245, Loss=0.17677631080150605
Loss made of: CE 0.2237916886806488, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 230/245, Loss=0.1683232381939888
Loss made of: CE 0.17048507928848267, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 240/245, Loss=0.1788230746984482
Loss made of: CE 0.16346800327301025, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.17488330602645874, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.17488330602645874, Class Loss=0.17488330602645874, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009247
Epoch 2, Batch 10/245, Loss=0.1384104534983635
Loss made of: CE 0.13953527808189392, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/245, Loss=0.15438701212406158
Loss made of: CE 0.16803976893424988, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/245, Loss=0.15540661215782164
Loss made of: CE 0.14185220003128052, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/245, Loss=0.15367827117443084
Loss made of: CE 0.125824436545372, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/245, Loss=0.13983506411314012
Loss made of: CE 0.15707996487617493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/245, Loss=0.1569635346531868
Loss made of: CE 0.11297719180583954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/245, Loss=0.15199942141771317
Loss made of: CE 0.20301882922649384, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/245, Loss=0.15279753655195236
Loss made of: CE 0.17622798681259155, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/245, Loss=0.15115207508206369
Loss made of: CE 0.10955467075109482, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/245, Loss=0.14459140971302986
Loss made of: CE 0.12598268687725067, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/245, Loss=0.1755141258239746
Loss made of: CE 0.12718471884727478, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/245, Loss=0.14975931495428085
Loss made of: CE 0.12465111911296844, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/245, Loss=0.16163796037435532
Loss made of: CE 0.15608981251716614, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/245, Loss=0.14314412474632263
Loss made of: CE 0.1122991070151329, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/245, Loss=0.1476867154240608
Loss made of: CE 0.15367361903190613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/245, Loss=0.15675999969244003
Loss made of: CE 0.1778954267501831, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/245, Loss=0.15493722259998322
Loss made of: CE 0.18110793828964233, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/245, Loss=0.15337966457009317
Loss made of: CE 0.16077114641666412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/245, Loss=0.1650252014398575
Loss made of: CE 0.12436067312955856, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/245, Loss=0.1492408573627472
Loss made of: CE 0.13915497064590454, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 210/245, Loss=0.15171703472733497
Loss made of: CE 0.1826516091823578, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 220/245, Loss=0.13968411087989807
Loss made of: CE 0.1372823566198349, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 230/245, Loss=0.14343369528651237
Loss made of: CE 0.20446491241455078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 240/245, Loss=0.14812810495495796
Loss made of: CE 0.18518976867198944, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1516277939081192, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.1516277939081192, Class Loss=0.1516277939081192, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.008487
Epoch 3, Batch 10/245, Loss=0.12558645457029344
Loss made of: CE 0.11024744808673859, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/245, Loss=0.12954443022608758
Loss made of: CE 0.17093858122825623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/245, Loss=0.13924479484558105
Loss made of: CE 0.15837253630161285, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/245, Loss=0.1326061524450779
Loss made of: CE 0.10528057813644409, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/245, Loss=0.1336642771959305
Loss made of: CE 0.09056173264980316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/245, Loss=0.13928986713290215
Loss made of: CE 0.10247521847486496, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/245, Loss=0.13958752229809762
Loss made of: CE 0.15510503947734833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/245, Loss=0.13259257674217223
Loss made of: CE 0.1244683489203453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/245, Loss=0.12813055366277695
Loss made of: CE 0.15878522396087646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/245, Loss=0.13215703815221785
Loss made of: CE 0.11820335686206818, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/245, Loss=0.1349860370159149
Loss made of: CE 0.16685132682323456, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/245, Loss=0.1359952509403229
Loss made of: CE 0.10822810977697372, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/245, Loss=0.14174773395061493
Loss made of: CE 0.10257577151060104, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/245, Loss=0.11631454676389694
Loss made of: CE 0.09671410173177719, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/245, Loss=0.13546520695090294
Loss made of: CE 0.12408462166786194, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/245, Loss=0.16335219293832778
Loss made of: CE 0.16934648156166077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/245, Loss=0.14469698444008827
Loss made of: CE 0.20978689193725586, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/245, Loss=0.13677724301815034
Loss made of: CE 0.13816876709461212, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/245, Loss=0.15124099254608153
Loss made of: CE 0.11374704539775848, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/245, Loss=0.1309778220951557
Loss made of: CE 0.12854519486427307, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 210/245, Loss=0.13204275891184808
Loss made of: CE 0.13771533966064453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 220/245, Loss=0.12466201782226563
Loss made of: CE 0.119015634059906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 230/245, Loss=0.14496248662471772
Loss made of: CE 0.11494700610637665, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 240/245, Loss=0.14496436193585396
Loss made of: CE 0.1310964822769165, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.13608624041080475, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.13608624041080475, Class Loss=0.13608624041080475, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007719
Epoch 4, Batch 10/245, Loss=0.11799705177545547
Loss made of: CE 0.10676705837249756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/245, Loss=0.125494771450758
Loss made of: CE 0.14822012186050415, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/245, Loss=0.12149929851293564
Loss made of: CE 0.09889368712902069, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/245, Loss=0.11494058221578599
Loss made of: CE 0.09597641229629517, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/245, Loss=0.12556913048028945
Loss made of: CE 0.1264559030532837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/245, Loss=0.13492425605654718
Loss made of: CE 0.12933862209320068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/245, Loss=0.12248528599739075
Loss made of: CE 0.1250227689743042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/245, Loss=0.12765865847468377
Loss made of: CE 0.08870788663625717, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/245, Loss=0.1264033317565918
Loss made of: CE 0.15623699128627777, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/245, Loss=0.12907454147934913
Loss made of: CE 0.17837059497833252, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/245, Loss=0.11919470876455307
Loss made of: CE 0.11092247813940048, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/245, Loss=0.134974867105484
Loss made of: CE 0.1494024395942688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/245, Loss=0.12343486696481705
Loss made of: CE 0.15354397892951965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/245, Loss=0.11473205238580704
Loss made of: CE 0.1402977555990219, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/245, Loss=0.12156279757618904
Loss made of: CE 0.14248144626617432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/245, Loss=0.1263165481388569
Loss made of: CE 0.117083840072155, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/245, Loss=0.11842556446790695
Loss made of: CE 0.09710954129695892, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/245, Loss=0.13009312748908997
Loss made of: CE 0.16132822632789612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/245, Loss=0.12378572225570679
Loss made of: CE 0.123585045337677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/245, Loss=0.12119250968098641
Loss made of: CE 0.1460580825805664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 210/245, Loss=0.11751352250576019
Loss made of: CE 0.07445695996284485, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 220/245, Loss=0.11304265409708023
Loss made of: CE 0.10304242372512817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 230/245, Loss=0.1265801429748535
Loss made of: CE 0.13639241456985474, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 240/245, Loss=0.11308088600635528
Loss made of: CE 0.11002375930547714, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1230183094739914, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.1230183094739914, Class Loss=0.1230183094739914, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/245, Loss=0.1158090963959694
Loss made of: CE 0.09739677608013153, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/245, Loss=0.10900323614478111
Loss made of: CE 0.08296351134777069, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/245, Loss=0.1113805741071701
Loss made of: CE 0.14360368251800537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/245, Loss=0.1057298168540001
Loss made of: CE 0.09511017799377441, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/245, Loss=0.11233340054750443
Loss made of: CE 0.13240014016628265, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/245, Loss=0.11458141952753068
Loss made of: CE 0.14380520582199097, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/245, Loss=0.09714878797531128
Loss made of: CE 0.10185425728559494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/245, Loss=0.10899846255779266
Loss made of: CE 0.09153728187084198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/245, Loss=0.11221409440040589
Loss made of: CE 0.07170891016721725, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/245, Loss=0.11113194599747658
Loss made of: CE 0.09232696890830994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/245, Loss=0.12146684303879737
Loss made of: CE 0.17061564326286316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/245, Loss=0.1097000628709793
Loss made of: CE 0.11480289697647095, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/245, Loss=0.10684378296136857
Loss made of: CE 0.10296380519866943, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/245, Loss=0.11473543643951416
Loss made of: CE 0.09607841074466705, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/245, Loss=0.1091824896633625
Loss made of: CE 0.09238408505916595, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/245, Loss=0.1027878314256668
Loss made of: CE 0.11550410836935043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/245, Loss=0.10626514181494713
Loss made of: CE 0.1356607973575592, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/245, Loss=0.10298138409852982
Loss made of: CE 0.09537824243307114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/245, Loss=0.11357336863875389
Loss made of: CE 0.10820160806179047, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/245, Loss=0.11398814246058464
Loss made of: CE 0.17187556624412537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 210/245, Loss=0.11914119943976402
Loss made of: CE 0.13289859890937805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 220/245, Loss=0.11002398803830146
Loss made of: CE 0.13684982061386108, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 230/245, Loss=0.11529623493552207
Loss made of: CE 0.11075838655233383, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 240/245, Loss=0.11299297660589218
Loss made of: CE 0.15909062325954437, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.11093901842832565, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.11093901842832565, Class Loss=0.11093901842832565, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006156
Epoch 6, Batch 10/245, Loss=0.1148152656853199
Loss made of: CE 0.12848328053951263, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/245, Loss=0.1039742387831211
Loss made of: CE 0.13517525792121887, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/245, Loss=0.10077795311808586
Loss made of: CE 0.11239111423492432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/245, Loss=0.1055673323571682
Loss made of: CE 0.11107659339904785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/245, Loss=0.10734338089823722
Loss made of: CE 0.12850546836853027, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/245, Loss=0.1181166335940361
Loss made of: CE 0.12187974900007248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/245, Loss=0.10758281126618385
Loss made of: CE 0.0711907297372818, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/245, Loss=0.10359817370772362
Loss made of: CE 0.08675462007522583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/245, Loss=0.10686763525009155
Loss made of: CE 0.125297412276268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/245, Loss=0.10186085253953933
Loss made of: CE 0.11232586205005646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/245, Loss=0.09849618449807167
Loss made of: CE 0.08632849156856537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/245, Loss=0.11325812935829163
Loss made of: CE 0.12135855853557587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/245, Loss=0.1183283157646656
Loss made of: CE 0.10618289560079575, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/245, Loss=0.10307489037513733
Loss made of: CE 0.13342265784740448, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/245, Loss=0.0982631079852581
Loss made of: CE 0.1233644112944603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/245, Loss=0.10835667327046394
Loss made of: CE 0.08036243170499802, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/245, Loss=0.1041256345808506
Loss made of: CE 0.13209104537963867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/245, Loss=0.09872590228915215
Loss made of: CE 0.09654249995946884, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/245, Loss=0.10662930458784103
Loss made of: CE 0.08584924787282944, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/245, Loss=0.10581090748310089
Loss made of: CE 0.10382484644651413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 210/245, Loss=0.10062993466854095
Loss made of: CE 0.09298040717840195, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 220/245, Loss=0.10725806355476379
Loss made of: CE 0.09911643713712692, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 230/245, Loss=0.1045692078769207
Loss made of: CE 0.10150595754384995, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 240/245, Loss=0.10646231546998024
Loss made of: CE 0.09354647248983383, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10585401207208633, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.10585401207208633, Class Loss=0.10585401207208633, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/157, Loss=0.15198842361569403
Loss made of: CE 0.12786802649497986, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=0.16295160353183746
Loss made of: CE 0.16169171035289764, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.16833453103899956
Loss made of: CE 0.22425806522369385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.13748070523142814
Loss made of: CE 0.09952615201473236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.14848547950387
Loss made of: CE 0.12356404960155487, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.14654993414878845
Loss made of: CE 0.18201641738414764, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.13532171323895453
Loss made of: CE 0.1771916002035141, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.14672011584043504
Loss made of: CE 0.14664492011070251, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.15484135448932648
Loss made of: CE 0.1946384161710739, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.15409271866083146
Loss made of: CE 0.1773737519979477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.15061167627573013
Loss made of: CE 0.1397121399641037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.15015393495559692
Loss made of: CE 0.13530999422073364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.1379763051867485
Loss made of: CE 0.13881681859493256, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.15688428208231925
Loss made of: CE 0.12224933505058289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.15537939667701722
Loss made of: CE 0.14543163776397705, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.15088121592998505, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.15088121592998505, Class Loss=0.15088121592998505, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009247
Epoch 2, Batch 10/157, Loss=0.16000877618789672
Loss made of: CE 0.16300545632839203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.1413178563117981
Loss made of: CE 0.15292662382125854, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.1521376445889473
Loss made of: CE 0.11738764494657516, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.1418429858982563
Loss made of: CE 0.1462765783071518, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.13090917840600014
Loss made of: CE 0.1302129030227661, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.1412642352283001
Loss made of: CE 0.12073254585266113, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.1370246708393097
Loss made of: CE 0.12121172994375229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.14709801003336906
Loss made of: CE 0.18966542184352875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.14691532105207444
Loss made of: CE 0.1626417189836502, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.13328858986496925
Loss made of: CE 0.13654562830924988, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.1363629564642906
Loss made of: CE 0.13011369109153748, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.12227860540151596
Loss made of: CE 0.10137467086315155, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.14221452325582504
Loss made of: CE 0.14324305951595306, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.11845982670783997
Loss made of: CE 0.10874389857053757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.1342893697321415
Loss made of: CE 0.14312782883644104, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1384534239768982, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.1384534239768982, Class Loss=0.1384534239768982, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.008487
Epoch 3, Batch 10/157, Loss=0.12761413380503656
Loss made of: CE 0.12175139039754868, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.125796590000391
Loss made of: CE 0.13749593496322632, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.11981411203742028
Loss made of: CE 0.1170518547296524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.13959433138370514
Loss made of: CE 0.09380679577589035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.12480254769325257
Loss made of: CE 0.12511149048805237, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.12377221807837487
Loss made of: CE 0.1366070806980133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.1419099897146225
Loss made of: CE 0.1337348222732544, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.12450380250811577
Loss made of: CE 0.10115166753530502, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.13344227373600007
Loss made of: CE 0.12290720641613007, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.11510116308927536
Loss made of: CE 0.11688816547393799, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.12985558733344077
Loss made of: CE 0.13206911087036133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.137652026116848
Loss made of: CE 0.1210821121931076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.11008118242025375
Loss made of: CE 0.12372877448797226, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.12516754046082496
Loss made of: CE 0.13560611009597778, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.10582801327109337
Loss made of: CE 0.09639059752225876, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.12541933357715607, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.12541933357715607, Class Loss=0.12541933357715607, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007719
Epoch 4, Batch 10/157, Loss=0.12182983756065369
Loss made of: CE 0.10527896881103516, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.12108030244708061
Loss made of: CE 0.13206517696380615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.11102962717413903
Loss made of: CE 0.11719145625829697, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.12169265449047088
Loss made of: CE 0.10237470269203186, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.11774629205465317
Loss made of: CE 0.1656980961561203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.10796742364764214
Loss made of: CE 0.12963935732841492, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.10351512655615806
Loss made of: CE 0.1178954467177391, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.11417174115777015
Loss made of: CE 0.11589424312114716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.10449541360139847
Loss made of: CE 0.1230078861117363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.12099078372120857
Loss made of: CE 0.11175043880939484, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.10942474529147148
Loss made of: CE 0.09787293523550034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.11949577406048775
Loss made of: CE 0.12295864522457123, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.11337281316518784
Loss made of: CE 0.14881306886672974, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.12016514241695404
Loss made of: CE 0.09088616818189621, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.118080273270607
Loss made of: CE 0.11122344434261322, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.11525041610002518, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.11525041610002518, Class Loss=0.11525041610002518, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/157, Loss=0.10509341806173325
Loss made of: CE 0.09760504961013794, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.10475370660424232
Loss made of: CE 0.10807487368583679, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.11213097646832466
Loss made of: CE 0.11972159147262573, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.11491281017661095
Loss made of: CE 0.1266651451587677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.10906807854771614
Loss made of: CE 0.10280753672122955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.11262949705123901
Loss made of: CE 0.15113608539104462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.12132140547037125
Loss made of: CE 0.11365543305873871, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.1005551666021347
Loss made of: CE 0.12494929134845734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.10239360108971596
Loss made of: CE 0.0897161215543747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.11161809712648392
Loss made of: CE 0.10452260076999664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.10792045220732689
Loss made of: CE 0.07719214260578156, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.10347690358757973
Loss made of: CE 0.10437493026256561, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.10568929314613343
Loss made of: CE 0.11390193551778793, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.09435865432024002
Loss made of: CE 0.13126209378242493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.11500738933682442
Loss made of: CE 0.11299008131027222, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10790824890136719, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.10790824890136719, Class Loss=0.10790824890136719, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006156
Epoch 6, Batch 10/157, Loss=0.10594203993678093
Loss made of: CE 0.09733246266841888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.1042538344860077
Loss made of: CE 0.10896988958120346, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.09601199924945832
Loss made of: CE 0.09405829757452011, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.10101528018712998
Loss made of: CE 0.07373884320259094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.10884696766734123
Loss made of: CE 0.10301221907138824, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.09913571178913116
Loss made of: CE 0.1002669557929039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.10390668585896493
Loss made of: CE 0.08831115067005157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.10593195781111717
Loss made of: CE 0.13325688242912292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.10098173096776009
Loss made of: CE 0.09358223527669907, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.11321752890944481
Loss made of: CE 0.08809270709753036, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.10780867263674736
Loss made of: CE 0.10826467722654343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.10616509020328521
Loss made of: CE 0.15858277678489685, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.1009140357375145
Loss made of: CE 0.07931607961654663, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.0999443307518959
Loss made of: CE 0.0803045928478241, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.11180512756109237
Loss made of: CE 0.09180959314107895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1038447692990303, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.1038447692990303, Class Loss=0.1038447692990303, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/244, Loss=0.13883211016654967
Loss made of: CE 0.15654811263084412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/244, Loss=0.14223925918340682
Loss made of: CE 0.1702055186033249, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/244, Loss=0.13776246905326844
Loss made of: CE 0.11067481338977814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/244, Loss=0.14692081063985823
Loss made of: CE 0.16875237226486206, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/244, Loss=0.12839934676885606
Loss made of: CE 0.16636395454406738, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/244, Loss=0.12202404290437699
Loss made of: CE 0.10081778466701508, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/244, Loss=0.15140354558825492
Loss made of: CE 0.1265239119529724, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/244, Loss=0.15282988250255586
Loss made of: CE 0.2336719036102295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/244, Loss=0.14623123034834862
Loss made of: CE 0.16204822063446045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/244, Loss=0.1407875455915928
Loss made of: CE 0.1643836498260498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/244, Loss=0.13495114594697952
Loss made of: CE 0.14565534889698029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/244, Loss=0.14104119017720224
Loss made of: CE 0.1401592195034027, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/244, Loss=0.19839398711919784
Loss made of: CE 0.15653151273727417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/244, Loss=0.14332438483834267
Loss made of: CE 0.14389148354530334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/244, Loss=0.15077917128801346
Loss made of: CE 0.13252630829811096, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/244, Loss=0.15084353238344192
Loss made of: CE 0.15279600024223328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/244, Loss=0.16297414600849153
Loss made of: CE 0.13453054428100586, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/244, Loss=0.11977163404226303
Loss made of: CE 0.10678456723690033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/244, Loss=0.14548440501093865
Loss made of: CE 0.13108672201633453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/244, Loss=0.1514660581946373
Loss made of: CE 0.11846759915351868, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 210/244, Loss=0.16822971478104592
Loss made of: CE 0.1298382580280304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 220/244, Loss=0.1403264470398426
Loss made of: CE 0.1049875020980835, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 230/244, Loss=0.16292252987623215
Loss made of: CE 0.1166127547621727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 240/244, Loss=0.1928173743188381
Loss made of: CE 0.12340667098760605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.14936132729053497, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.14936132729053497, Class Loss=0.14936132729053497, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009247
Epoch 2, Batch 10/244, Loss=0.14962481036782266
Loss made of: CE 0.14718958735466003, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/244, Loss=0.14851255863904952
Loss made of: CE 0.18078574538230896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/244, Loss=0.14913005456328393
Loss made of: CE 0.1730554699897766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/244, Loss=0.14306993931531906
Loss made of: CE 0.17249071598052979, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/244, Loss=0.154531629383564
Loss made of: CE 0.14672741293907166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/244, Loss=0.12545025646686553
Loss made of: CE 0.1372036337852478, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/244, Loss=0.13906685933470725
Loss made of: CE 0.1294059157371521, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/244, Loss=0.13530238494277
Loss made of: CE 0.11494310945272446, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/244, Loss=0.14327578395605087
Loss made of: CE 0.1839582920074463, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/244, Loss=0.15007618218660354
Loss made of: CE 0.16832542419433594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/244, Loss=0.1530917577445507
Loss made of: CE 0.13566811382770538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/244, Loss=0.14234839975833893
Loss made of: CE 0.16030605137348175, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/244, Loss=0.15061974078416823
Loss made of: CE 0.1249479353427887, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/244, Loss=0.14430780038237573
Loss made of: CE 0.16085076332092285, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/244, Loss=0.15698712766170503
Loss made of: CE 0.16776803135871887, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/244, Loss=0.13321011066436766
Loss made of: CE 0.111392542719841, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/244, Loss=0.14528568983078002
Loss made of: CE 0.1326543390750885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/244, Loss=0.15100031569600106
Loss made of: CE 0.23560211062431335, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/244, Loss=0.14623815938830376
Loss made of: CE 0.14899902045726776, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/244, Loss=0.11795860528945923
Loss made of: CE 0.13090166449546814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 210/244, Loss=0.12158911526203156
Loss made of: CE 0.12741778790950775, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 220/244, Loss=0.13764578253030776
Loss made of: CE 0.15699173510074615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 230/244, Loss=0.14058255329728125
Loss made of: CE 0.12854377925395966, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 240/244, Loss=0.14192617014050485
Loss made of: CE 0.1211787685751915, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.14248183369636536, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.14248183369636536, Class Loss=0.14248183369636536, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.008487
Epoch 3, Batch 10/244, Loss=0.1413085162639618
Loss made of: CE 0.12797275185585022, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/244, Loss=0.12869834899902344
Loss made of: CE 0.08787404000759125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/244, Loss=0.13546421900391578
Loss made of: CE 0.13055475056171417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/244, Loss=0.1258430890738964
Loss made of: CE 0.14597150683403015, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/244, Loss=0.12667324393987656
Loss made of: CE 0.11300414800643921, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/244, Loss=0.13343139365315437
Loss made of: CE 0.10323861241340637, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/244, Loss=0.12615204676985742
Loss made of: CE 0.10586945712566376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/244, Loss=0.1318791002035141
Loss made of: CE 0.1392354965209961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/244, Loss=0.12969110608100892
Loss made of: CE 0.11136271804571152, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/244, Loss=0.1136229045689106
Loss made of: CE 0.1311376392841339, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/244, Loss=0.12882242500782012
Loss made of: CE 0.13463276624679565, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/244, Loss=0.12746048346161842
Loss made of: CE 0.14978329837322235, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/244, Loss=0.10503089129924774
Loss made of: CE 0.13692280650138855, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/244, Loss=0.12258283793926239
Loss made of: CE 0.11238285154104233, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/244, Loss=0.11898521408438682
Loss made of: CE 0.12342818826436996, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/244, Loss=0.13605003878474237
Loss made of: CE 0.14191602170467377, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/244, Loss=0.11395052894949913
Loss made of: CE 0.13010618090629578, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/244, Loss=0.11833440437912941
Loss made of: CE 0.12151475995779037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/244, Loss=0.12257332801818847
Loss made of: CE 0.12301798164844513, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/244, Loss=0.11668243482708932
Loss made of: CE 0.12454231828451157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 210/244, Loss=0.11446943283081054
Loss made of: CE 0.10395395755767822, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 220/244, Loss=0.12836883962154388
Loss made of: CE 0.14913390576839447, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 230/244, Loss=0.11868679597973823
Loss made of: CE 0.1299472451210022, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 240/244, Loss=0.12334854006767274
Loss made of: CE 0.09096717089414597, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.12454503774642944, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.12454503774642944, Class Loss=0.12454503774642944, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007719
Epoch 4, Batch 10/244, Loss=0.11405658796429634
Loss made of: CE 0.0907943993806839, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/244, Loss=0.10891885235905648
Loss made of: CE 0.11442824453115463, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/244, Loss=0.12198901548981667
Loss made of: CE 0.09565543383359909, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/244, Loss=0.11391514837741852
Loss made of: CE 0.09625594317913055, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/244, Loss=0.11030637621879577
Loss made of: CE 0.08621591329574585, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/244, Loss=0.11243830919265747
Loss made of: CE 0.1108769029378891, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/244, Loss=0.1104079820215702
Loss made of: CE 0.09844796359539032, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/244, Loss=0.1141585275530815
Loss made of: CE 0.100934699177742, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/244, Loss=0.10651030018925667
Loss made of: CE 0.09659576416015625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/244, Loss=0.10205210372805595
Loss made of: CE 0.08391877263784409, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/244, Loss=0.11244630962610244
Loss made of: CE 0.08416648954153061, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/244, Loss=0.10949471890926361
Loss made of: CE 0.11499534547328949, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/244, Loss=0.11530080065131187
Loss made of: CE 0.08420680463314056, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/244, Loss=0.10712434649467469
Loss made of: CE 0.13656343519687653, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/244, Loss=0.11797053813934326
Loss made of: CE 0.09251531958580017, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/244, Loss=0.1090054489672184
Loss made of: CE 0.11556031554937363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/244, Loss=0.11910326778888702
Loss made of: CE 0.2351398915052414, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/244, Loss=0.10957299694418907
Loss made of: CE 0.11462736129760742, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/244, Loss=0.10472233891487122
Loss made of: CE 0.08234985172748566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/244, Loss=0.12289130687713623
Loss made of: CE 0.1430119127035141, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 210/244, Loss=0.11946729943156242
Loss made of: CE 0.0994558036327362, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 220/244, Loss=0.11348697245121002
Loss made of: CE 0.11792220920324326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 230/244, Loss=0.10724387243390084
Loss made of: CE 0.08089795708656311, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 240/244, Loss=0.10512301847338676
Loss made of: CE 0.10768808424472809, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.11222485452890396, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.11222485452890396, Class Loss=0.11222485452890396, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/244, Loss=0.10320218056440353
Loss made of: CE 0.11717149615287781, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/244, Loss=0.10647594332695007
Loss made of: CE 0.12254445999860764, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/244, Loss=0.11648984998464584
Loss made of: CE 0.14273276925086975, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/244, Loss=0.0965008020401001
Loss made of: CE 0.08998608589172363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/244, Loss=0.10778883397579193
Loss made of: CE 0.13133352994918823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/244, Loss=0.09518386498093605
Loss made of: CE 0.09000782668590546, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/244, Loss=0.10436402186751366
Loss made of: CE 0.1811068058013916, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/244, Loss=0.10045225024223328
Loss made of: CE 0.13090160489082336, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/244, Loss=0.12404909059405327
Loss made of: CE 0.13348077237606049, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/244, Loss=0.09845436289906502
Loss made of: CE 0.0976196750998497, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/244, Loss=0.10102680176496506
Loss made of: CE 0.10245567560195923, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/244, Loss=0.10947274714708329
Loss made of: CE 0.1285499632358551, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/244, Loss=0.11827182844281196
Loss made of: CE 0.10822820663452148, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/244, Loss=0.09371448382735252
Loss made of: CE 0.09523391723632812, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/244, Loss=0.10469119548797608
Loss made of: CE 0.11060084402561188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/244, Loss=0.10181141346693039
Loss made of: CE 0.12172167003154755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/244, Loss=0.10737458392977714
Loss made of: CE 0.09190773218870163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/244, Loss=0.10335807427763939
Loss made of: CE 0.12728741765022278, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/244, Loss=0.1147300697863102
Loss made of: CE 0.09219586104154587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/244, Loss=0.10997420027852059
Loss made of: CE 0.09520556032657623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 210/244, Loss=0.10251955762505531
Loss made of: CE 0.09613868594169617, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 220/244, Loss=0.1108744390308857
Loss made of: CE 0.11073052883148193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 230/244, Loss=0.11294522732496262
Loss made of: CE 0.09759312868118286, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 240/244, Loss=0.09680734053254128
Loss made of: CE 0.10277575254440308, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10563795268535614, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.10563795268535614, Class Loss=0.10563795268535614, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006156
Epoch 6, Batch 10/244, Loss=0.09905649051070213
Loss made of: CE 0.13479889929294586, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/244, Loss=0.09783604964613915
Loss made of: CE 0.08551055192947388, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/244, Loss=0.092660803347826
Loss made of: CE 0.08710767328739166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/244, Loss=0.1013997070491314
Loss made of: CE 0.10051727294921875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/244, Loss=0.09656389653682709
Loss made of: CE 0.0775943249464035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/244, Loss=0.10393615961074829
Loss made of: CE 0.10003714263439178, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/244, Loss=0.10115495696663857
Loss made of: CE 0.12073763459920883, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/244, Loss=0.09963518306612969
Loss made of: CE 0.10949985682964325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/244, Loss=0.09452864155173302
Loss made of: CE 0.12101229280233383, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/244, Loss=0.09522282965481281
Loss made of: CE 0.0922514945268631, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/244, Loss=0.09767845571041107
Loss made of: CE 0.14177793264389038, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/244, Loss=0.10821318551898003
Loss made of: CE 0.08780230581760406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/244, Loss=0.10472671091556549
Loss made of: CE 0.13040505349636078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/244, Loss=0.1015024408698082
Loss made of: CE 0.09582310914993286, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/244, Loss=0.10473961383104324
Loss made of: CE 0.09416674822568893, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/244, Loss=0.10833391770720482
Loss made of: CE 0.0873153805732727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/244, Loss=0.09584724083542824
Loss made of: CE 0.08514279127120972, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/244, Loss=0.09859791472554207
Loss made of: CE 0.08098933100700378, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/244, Loss=0.11162161156535148
Loss made of: CE 0.0934329628944397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/244, Loss=0.09949991777539254
Loss made of: CE 0.08967505395412445, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 210/244, Loss=0.09633081555366516
Loss made of: CE 0.08309517800807953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 220/244, Loss=0.09062358140945434
Loss made of: CE 0.10059072822332382, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 230/244, Loss=0.09680527076125145
Loss made of: CE 0.08831223100423813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 240/244, Loss=0.12676495909690857
Loss made of: CE 0.12918630242347717, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10154744237661362, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.10154744237661362, Class Loss=0.10154744237661362, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.14662277698516846, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.949648
Mean Acc: 0.891249
FreqW Acc: 0.909577
Mean IoU: 0.787834
Class IoU:
	class 0: 0.941672
	class 1: 0.8766075
	class 2: 0.4169297
	class 3: 0.87873524
	class 4: 0.70379597
	class 5: 0.79721844
	class 6: 0.94495636
	class 7: 0.9125045
	class 8: 0.9279822
	class 9: 0.44436455
	class 10: 0.7391097
	class 11: 0.63122094
	class 12: 0.88273036
	class 13: 0.76443434
	class 14: 0.8750511
	class 15: 0.8680241
Class Acc:
	class 0: 0.96742153
	class 1: 0.97268796
	class 2: 0.926011
	class 3: 0.9293723
	class 4: 0.85590464
	class 5: 0.901876
	class 6: 0.9691243
	class 7: 0.94457895
	class 8: 0.96965206
	class 9: 0.6181029
	class 10: 0.77582437
	class 11: 0.67763406
	class 12: 0.9653132
	class 13: 0.9216451
	class 14: 0.94219977
	class 15: 0.9226298

federated global round: 4, step: 0
select part of clients to conduct local training
[7, 1, 6, 8]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.004384
Epoch 1, Batch 10/157, Loss=0.11572798565030099
Loss made of: CE 0.10122805833816528, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=0.11390823349356652
Loss made of: CE 0.09239392727613449, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.11262261271476745
Loss made of: CE 0.18663573265075684, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.09090864732861519
Loss made of: CE 0.0614556223154068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.10641073510050773
Loss made of: CE 0.0618785098195076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.10642805024981498
Loss made of: CE 0.10496179759502411, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.11109119206666947
Loss made of: CE 0.13059070706367493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.10054670795798301
Loss made of: CE 0.10114970058202744, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.09698945954442025
Loss made of: CE 0.10080593824386597, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.0992182396352291
Loss made of: CE 0.0851876363158226, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.09257932603359223
Loss made of: CE 0.07911728322505951, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.10355122238397599
Loss made of: CE 0.10526449978351593, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.09761632159352303
Loss made of: CE 0.10101640224456787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.10159382820129395
Loss made of: CE 0.08963223546743393, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.096308184415102
Loss made of: CE 0.09305869042873383, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.10303476452827454, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.10303476452827454, Class Loss=0.10303476452827454, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.003720
Epoch 2, Batch 10/157, Loss=0.0915564902126789
Loss made of: CE 0.07815327495336533, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.09871118068695069
Loss made of: CE 0.09257446229457855, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.09998230710625648
Loss made of: CE 0.10301069915294647, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.09114396795630456
Loss made of: CE 0.07828782498836517, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.08988786339759827
Loss made of: CE 0.07564857602119446, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.10326404795050621
Loss made of: CE 0.11780999600887299, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.09598413407802582
Loss made of: CE 0.0779753029346466, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.09622813686728478
Loss made of: CE 0.07677540928125381, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.09843977242708206
Loss made of: CE 0.10182549804449081, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.09485027566552162
Loss made of: CE 0.09119032323360443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.08485807403922081
Loss made of: CE 0.10787149518728256, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.09079241342842578
Loss made of: CE 0.09405545890331268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.09437231793999672
Loss made of: CE 0.09352128952741623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.08520549237728119
Loss made of: CE 0.08157519996166229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.10685727074742317
Loss made of: CE 0.0977015495300293, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09530283510684967, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.09530283510684967, Class Loss=0.09530283510684967, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.003043
Epoch 3, Batch 10/157, Loss=0.09022799357771874
Loss made of: CE 0.09788079559803009, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.08794246390461921
Loss made of: CE 0.07000239938497543, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.08684261813759804
Loss made of: CE 0.08966591954231262, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.08941790014505387
Loss made of: CE 0.07992095500230789, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.09309940710663796
Loss made of: CE 0.09835450351238251, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.08865157067775727
Loss made of: CE 0.09972845017910004, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.09318028911948203
Loss made of: CE 0.07706646621227264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.08926339074969292
Loss made of: CE 0.08199870586395264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.08040703982114791
Loss made of: CE 0.08865326642990112, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.09032883942127228
Loss made of: CE 0.08528833836317062, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.09938809648156166
Loss made of: CE 0.10187336802482605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.08313242644071579
Loss made of: CE 0.10284695029258728, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.09919385984539986
Loss made of: CE 0.12626808881759644, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.11061514690518379
Loss made of: CE 0.12699627876281738, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.08071629703044891
Loss made of: CE 0.08092725276947021, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09114821255207062, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.09114821255207062, Class Loss=0.09114821255207062, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.002349
Epoch 4, Batch 10/157, Loss=0.08667080700397492
Loss made of: CE 0.0916065201163292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.0900888629257679
Loss made of: CE 0.06733802706003189, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.09661014005541801
Loss made of: CE 0.11927765607833862, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.08309955894947052
Loss made of: CE 0.0861365795135498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.08883153572678566
Loss made of: CE 0.09667257964611053, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.08748028427362442
Loss made of: CE 0.11705684661865234, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.08904902189970017
Loss made of: CE 0.10253602266311646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.09545293375849724
Loss made of: CE 0.08900277316570282, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.09385787099599838
Loss made of: CE 0.08985660970211029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.0935235194861889
Loss made of: CE 0.11431305855512619, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.09132481962442399
Loss made of: CE 0.07132156193256378, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.0829800046980381
Loss made of: CE 0.07891856133937836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.08950548321008682
Loss made of: CE 0.09199206531047821, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.0955683134496212
Loss made of: CE 0.07277019321918488, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.08603774309158325
Loss made of: CE 0.09727632254362106, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08947043120861053, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.08947043120861053, Class Loss=0.08947043120861053, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.001631
Epoch 5, Batch 10/157, Loss=0.07992678061127663
Loss made of: CE 0.08336302638053894, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.0898701786994934
Loss made of: CE 0.08865760266780853, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.10369132682681084
Loss made of: CE 0.1808769702911377, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.08066950291395188
Loss made of: CE 0.0913742184638977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.08162724077701569
Loss made of: CE 0.06707915663719177, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.08653147742152215
Loss made of: CE 0.09135693311691284, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.08016103245317936
Loss made of: CE 0.06408798694610596, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.08969237208366394
Loss made of: CE 0.08085812628269196, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.08528775721788406
Loss made of: CE 0.07432003319263458, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.09390844032168388
Loss made of: CE 0.09390883147716522, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.09041197374463081
Loss made of: CE 0.06452664732933044, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.08893420696258544
Loss made of: CE 0.10344907641410828, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.09390550777316094
Loss made of: CE 0.10847821086645126, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.08893749564886093
Loss made of: CE 0.07680823653936386, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.09225591570138932
Loss made of: CE 0.08001881092786789, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08841944485902786, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.08841944485902786, Class Loss=0.08841944485902786, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.000874
Epoch 6, Batch 10/157, Loss=0.08719978109002113
Loss made of: CE 0.08330745995044708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.084877347946167
Loss made of: CE 0.07692007720470428, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.08751615658402442
Loss made of: CE 0.09798663854598999, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.08076265230774879
Loss made of: CE 0.06787314265966415, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.08574132472276688
Loss made of: CE 0.08044898509979248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.08001525476574897
Loss made of: CE 0.06567975878715515, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.09248541593551636
Loss made of: CE 0.08467260003089905, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.07680774852633476
Loss made of: CE 0.08067208528518677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.09450440183281898
Loss made of: CE 0.10082978010177612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.08839626386761665
Loss made of: CE 0.06963518261909485, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.08295818343758583
Loss made of: CE 0.07847775518894196, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.09132136031985283
Loss made of: CE 0.05295126885175705, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.09137648940086365
Loss made of: CE 0.12041141092777252, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.0957171492278576
Loss made of: CE 0.10305210202932358, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.08454857990145684
Loss made of: CE 0.07963865995407104, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08717381209135056, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.08717381209135056, Class Loss=0.08717381209135056, Reg Loss=0.0
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/164, Loss=0.10134811624884606
Loss made of: CE 0.11665834486484528, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/164, Loss=0.1167068675160408
Loss made of: CE 0.0774194523692131, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/164, Loss=0.1221234567463398
Loss made of: CE 0.11781194061040878, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/164, Loss=0.13666459545493126
Loss made of: CE 0.08329890668392181, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/164, Loss=0.10815043076872825
Loss made of: CE 0.1673102080821991, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/164, Loss=0.10966782048344612
Loss made of: CE 0.10577978938817978, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/164, Loss=0.11129831299185752
Loss made of: CE 0.0821356326341629, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/164, Loss=0.10851007550954819
Loss made of: CE 0.11198239773511887, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/164, Loss=0.11724318861961365
Loss made of: CE 0.0986916571855545, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/164, Loss=0.10695664286613464
Loss made of: CE 0.0850096195936203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/164, Loss=0.11118258014321328
Loss made of: CE 0.11221657693386078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/164, Loss=0.12738707065582275
Loss made of: CE 0.1057310402393341, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/164, Loss=0.10448080152273179
Loss made of: CE 0.08959890902042389, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/164, Loss=0.11238567307591438
Loss made of: CE 0.12015555799007416, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/164, Loss=0.11328041553497314
Loss made of: CE 0.1371982991695404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/164, Loss=0.11528142839670182
Loss made of: CE 0.10895698517560959, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.11360973864793777, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.11360973864793777, Class Loss=0.11360973864793777, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.006551
Epoch 2, Batch 10/164, Loss=0.10151020213961601
Loss made of: CE 0.10537999123334885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/164, Loss=0.10394968315958977
Loss made of: CE 0.13346537947654724, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/164, Loss=0.11004623994231225
Loss made of: CE 0.10176710784435272, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/164, Loss=0.11345941200852394
Loss made of: CE 0.11559753119945526, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/164, Loss=0.104543137550354
Loss made of: CE 0.12376350164413452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/164, Loss=0.1033546969294548
Loss made of: CE 0.09884130954742432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/164, Loss=0.10282269790768624
Loss made of: CE 0.09249651432037354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/164, Loss=0.10799301639199257
Loss made of: CE 0.09216561168432236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/164, Loss=0.10090727508068084
Loss made of: CE 0.11465960741043091, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/164, Loss=0.09877455383539199
Loss made of: CE 0.09426574409008026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/164, Loss=0.09904958158731461
Loss made of: CE 0.08920028060674667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/164, Loss=0.10827290341258049
Loss made of: CE 0.11340749263763428, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/164, Loss=0.11061965450644493
Loss made of: CE 0.1259574592113495, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/164, Loss=0.09236959367990494
Loss made of: CE 0.08240561932325363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/164, Loss=0.10948814451694489
Loss made of: CE 0.09783422946929932, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/164, Loss=0.10731626152992249
Loss made of: CE 0.08818438649177551, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10489296913146973, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.10489296913146973, Class Loss=0.10489296913146973, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.005359
Epoch 3, Batch 10/164, Loss=0.0952224925160408
Loss made of: CE 0.08726614713668823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/164, Loss=0.1036677747964859
Loss made of: CE 0.09618830680847168, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/164, Loss=0.10209280028939247
Loss made of: CE 0.13714271783828735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/164, Loss=0.0923647329211235
Loss made of: CE 0.0916716679930687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/164, Loss=0.10656381845474243
Loss made of: CE 0.10014694929122925, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/164, Loss=0.10255385637283325
Loss made of: CE 0.10270553827285767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/164, Loss=0.09350434467196464
Loss made of: CE 0.10938149690628052, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/164, Loss=0.10812264084815978
Loss made of: CE 0.13336597383022308, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/164, Loss=0.10114833638072014
Loss made of: CE 0.07070410996675491, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/164, Loss=0.10357415229082108
Loss made of: CE 0.0992894396185875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/164, Loss=0.09688647687435151
Loss made of: CE 0.08143523335456848, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/164, Loss=0.08746149279177189
Loss made of: CE 0.10489858686923981, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/164, Loss=0.10702186524868011
Loss made of: CE 0.09726512432098389, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/164, Loss=0.10110603794455528
Loss made of: CE 0.0978635847568512, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/164, Loss=0.09611357301473618
Loss made of: CE 0.09879755228757858, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/164, Loss=0.09612192288041115
Loss made of: CE 0.09011896699666977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09928515553474426, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.09928515553474426, Class Loss=0.09928515553474426, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.004136
Epoch 4, Batch 10/164, Loss=0.09941778257489205
Loss made of: CE 0.1254899799823761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/164, Loss=0.10415956899523734
Loss made of: CE 0.08530567586421967, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/164, Loss=0.08942766711115838
Loss made of: CE 0.09826838970184326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/164, Loss=0.09606823399662971
Loss made of: CE 0.08526076376438141, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/164, Loss=0.09245973899960518
Loss made of: CE 0.09818129986524582, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/164, Loss=0.09011616855859757
Loss made of: CE 0.07536618411540985, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/164, Loss=0.10297125726938247
Loss made of: CE 0.10799892246723175, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/164, Loss=0.10032395496964455
Loss made of: CE 0.10909271240234375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/164, Loss=0.09377717599272728
Loss made of: CE 0.08234292268753052, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/164, Loss=0.09829026088118553
Loss made of: CE 0.10314540565013885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/164, Loss=0.08930962681770324
Loss made of: CE 0.11130430549383163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/164, Loss=0.09525578618049621
Loss made of: CE 0.11269016563892365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/164, Loss=0.0891648106276989
Loss made of: CE 0.08477085828781128, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/164, Loss=0.10132747888565063
Loss made of: CE 0.09009431302547455, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/164, Loss=0.09730106890201569
Loss made of: CE 0.07509328424930573, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/164, Loss=0.09750243425369262
Loss made of: CE 0.10391607135534286, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09632711112499237, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.09632711112499237, Class Loss=0.09632711112499237, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.002872
Epoch 5, Batch 10/164, Loss=0.09386435970664024
Loss made of: CE 0.10748034715652466, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/164, Loss=0.10148657858371735
Loss made of: CE 0.10572491586208344, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/164, Loss=0.10462438613176346
Loss made of: CE 0.12711836397647858, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/164, Loss=0.08550147488713264
Loss made of: CE 0.09096002578735352, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/164, Loss=0.09088843539357186
Loss made of: CE 0.0662815272808075, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/164, Loss=0.09793834015727043
Loss made of: CE 0.09128358215093613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/164, Loss=0.08999788388609886
Loss made of: CE 0.09927671402692795, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/164, Loss=0.0861224465072155
Loss made of: CE 0.08619701862335205, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/164, Loss=0.09105098433792591
Loss made of: CE 0.07865144312381744, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/164, Loss=0.08933698087930679
Loss made of: CE 0.08897344768047333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/164, Loss=0.10153868347406388
Loss made of: CE 0.1032487004995346, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/164, Loss=0.09730255901813507
Loss made of: CE 0.10925643891096115, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/164, Loss=0.09697423875331879
Loss made of: CE 0.0804777517914772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/164, Loss=0.09433524757623672
Loss made of: CE 0.09829434007406235, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/164, Loss=0.09431801363825798
Loss made of: CE 0.0979580283164978, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/164, Loss=0.08401482850313187
Loss made of: CE 0.08596079051494598, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09365662932395935, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.09365662932395935, Class Loss=0.09365662932395935, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.001539
Epoch 6, Batch 10/164, Loss=0.08359221741557121
Loss made of: CE 0.08021844923496246, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/164, Loss=0.09739085882902146
Loss made of: CE 0.07877074927091599, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/164, Loss=0.09419945031404495
Loss made of: CE 0.08668498694896698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/164, Loss=0.09533916115760803
Loss made of: CE 0.09569092839956284, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/164, Loss=0.09098651111125947
Loss made of: CE 0.09375672042369843, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/164, Loss=0.09040134251117707
Loss made of: CE 0.1042778342962265, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/164, Loss=0.0894251860678196
Loss made of: CE 0.06959901750087738, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/164, Loss=0.10355042889714242
Loss made of: CE 0.08977609872817993, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/164, Loss=0.08783557415008544
Loss made of: CE 0.0785558819770813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/164, Loss=0.08717694133520126
Loss made of: CE 0.08238300681114197, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/164, Loss=0.09614591673016548
Loss made of: CE 0.0901777371764183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/164, Loss=0.090700613707304
Loss made of: CE 0.07723981142044067, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/164, Loss=0.09196100383996964
Loss made of: CE 0.11053963750600815, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/164, Loss=0.09296607449650765
Loss made of: CE 0.08557727932929993, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/164, Loss=0.0900193728506565
Loss made of: CE 0.08428981155157089, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/164, Loss=0.09676912203431129
Loss made of: CE 0.09609352052211761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09237703680992126, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.09237703680992126, Class Loss=0.09237703680992126, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.005679
Epoch 1, Batch 10/198, Loss=0.11573783531785012
Loss made of: CE 0.09534507244825363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/198, Loss=0.1259026549756527
Loss made of: CE 0.1313449740409851, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/198, Loss=0.12296887710690499
Loss made of: CE 0.15625707805156708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/198, Loss=0.12973395511507987
Loss made of: CE 0.11662977933883667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/198, Loss=0.11587548553943634
Loss made of: CE 0.10612104833126068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/198, Loss=0.13096837028861047
Loss made of: CE 0.10817079246044159, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/198, Loss=0.11796272397041321
Loss made of: CE 0.09571665525436401, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/198, Loss=0.10939992219209671
Loss made of: CE 0.09026387333869934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/198, Loss=0.12064262330532075
Loss made of: CE 0.09768867492675781, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/198, Loss=0.11152883172035218
Loss made of: CE 0.13277791440486908, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/198, Loss=0.10797165334224701
Loss made of: CE 0.10001146793365479, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/198, Loss=0.1119874283671379
Loss made of: CE 0.11781489849090576, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/198, Loss=0.11581750959157944
Loss made of: CE 0.1297747790813446, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/198, Loss=0.10834785252809524
Loss made of: CE 0.09468825161457062, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/198, Loss=0.11449154168367386
Loss made of: CE 0.14584797620773315, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/198, Loss=0.1161405511200428
Loss made of: CE 0.09727740287780762, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/198, Loss=0.11336805820465087
Loss made of: CE 0.10288029909133911, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/198, Loss=0.10701268836855889
Loss made of: CE 0.11467760801315308, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/198, Loss=0.1041151039302349
Loss made of: CE 0.11258479207754135, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.11556464433670044, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.11556464433670044, Class Loss=0.11556464433670044, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.004820
Epoch 2, Batch 10/198, Loss=0.10483693853020667
Loss made of: CE 0.08482588827610016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/198, Loss=0.1017339363694191
Loss made of: CE 0.07988056540489197, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/198, Loss=0.10049190521240234
Loss made of: CE 0.07656018435955048, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/198, Loss=0.10435804575681687
Loss made of: CE 0.09380593150854111, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/198, Loss=0.1068465143442154
Loss made of: CE 0.1282133162021637, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/198, Loss=0.1078954815864563
Loss made of: CE 0.10122393071651459, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/198, Loss=0.11576227247714996
Loss made of: CE 0.10318534076213837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/198, Loss=0.10142405182123185
Loss made of: CE 0.09869831800460815, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/198, Loss=0.11714267954230309
Loss made of: CE 0.13581198453903198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/198, Loss=0.11429499387741089
Loss made of: CE 0.113670215010643, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/198, Loss=0.10149203389883041
Loss made of: CE 0.08450931310653687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/198, Loss=0.10857794731855393
Loss made of: CE 0.10797879099845886, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/198, Loss=0.10296742171049118
Loss made of: CE 0.13408517837524414, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/198, Loss=0.11586903557181358
Loss made of: CE 0.14000310003757477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/198, Loss=0.11384924054145813
Loss made of: CE 0.09637713432312012, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/198, Loss=0.10961304679512977
Loss made of: CE 0.10451219975948334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/198, Loss=0.11603255867958069
Loss made of: CE 0.11160728335380554, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/198, Loss=0.11814693212509156
Loss made of: CE 0.13240757584571838, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/198, Loss=0.12323412373661995
Loss made of: CE 0.12777185440063477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1094004213809967, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.1094004213809967, Class Loss=0.1094004213809967, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.003943
Epoch 3, Batch 10/198, Loss=0.10828421637415886
Loss made of: CE 0.10349994152784348, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/198, Loss=0.09915373399853707
Loss made of: CE 0.11426010727882385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/198, Loss=0.1018127977848053
Loss made of: CE 0.09665456414222717, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/198, Loss=0.1073514997959137
Loss made of: CE 0.10338714718818665, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/198, Loss=0.10114211961627007
Loss made of: CE 0.080506831407547, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/198, Loss=0.10208457335829735
Loss made of: CE 0.09021897614002228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/198, Loss=0.10735053792595864
Loss made of: CE 0.12523692846298218, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/198, Loss=0.10751685798168183
Loss made of: CE 0.1137615293264389, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/198, Loss=0.1060955822467804
Loss made of: CE 0.11417657136917114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/198, Loss=0.11185268387198448
Loss made of: CE 0.0945035070180893, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/198, Loss=0.10512554869055749
Loss made of: CE 0.08424711227416992, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/198, Loss=0.09408300369977951
Loss made of: CE 0.08317209780216217, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/198, Loss=0.10714691132307053
Loss made of: CE 0.13918305933475494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/198, Loss=0.10616492852568626
Loss made of: CE 0.0968145951628685, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/198, Loss=0.12353298142552376
Loss made of: CE 0.14968734979629517, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/198, Loss=0.114028699696064
Loss made of: CE 0.12239726632833481, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/198, Loss=0.1026675634086132
Loss made of: CE 0.0888708233833313, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/198, Loss=0.10068048238754272
Loss made of: CE 0.08334854245185852, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/198, Loss=0.10127211660146714
Loss made of: CE 0.11508253216743469, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10552725195884705, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.10552725195884705, Class Loss=0.10552725195884705, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.003043
Epoch 4, Batch 10/198, Loss=0.10315856337547302
Loss made of: CE 0.082953542470932, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/198, Loss=0.10485924929380416
Loss made of: CE 0.10832371562719345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/198, Loss=0.10181016251444816
Loss made of: CE 0.09200116991996765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/198, Loss=0.09335759058594703
Loss made of: CE 0.08909770846366882, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/198, Loss=0.10787214189767838
Loss made of: CE 0.11520835012197495, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/198, Loss=0.09990786910057067
Loss made of: CE 0.10512473434209824, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/198, Loss=0.10505291447043419
Loss made of: CE 0.15307971835136414, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/198, Loss=0.09680366888642311
Loss made of: CE 0.08976408839225769, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/198, Loss=0.08680979833006859
Loss made of: CE 0.09118255227804184, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/198, Loss=0.10774867460131646
Loss made of: CE 0.12858229875564575, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/198, Loss=0.10685112029314041
Loss made of: CE 0.12839895486831665, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/198, Loss=0.09780582338571549
Loss made of: CE 0.1251072734594345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/198, Loss=0.10627109929919243
Loss made of: CE 0.10268359631299973, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/198, Loss=0.10617681220173836
Loss made of: CE 0.10828235000371933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/198, Loss=0.10016735717654228
Loss made of: CE 0.11563777178525925, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/198, Loss=0.11280030831694603
Loss made of: CE 0.12505528330802917, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/198, Loss=0.09906810447573662
Loss made of: CE 0.10419326275587082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/198, Loss=0.10279927402734756
Loss made of: CE 0.10215490311384201, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/198, Loss=0.10547602623701095
Loss made of: CE 0.13219204545021057, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10195473581552505, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.10195473581552505, Class Loss=0.10195473581552505, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.002113
Epoch 5, Batch 10/198, Loss=0.10395990386605262
Loss made of: CE 0.11909888684749603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/198, Loss=0.09732994437217712
Loss made of: CE 0.12699870765209198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/198, Loss=0.09750065132975579
Loss made of: CE 0.10060849040746689, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/198, Loss=0.09514574259519577
Loss made of: CE 0.0984838530421257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/198, Loss=0.09282342717051506
Loss made of: CE 0.09569080919027328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/198, Loss=0.09689042940735818
Loss made of: CE 0.09205015748739243, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/198, Loss=0.103895915299654
Loss made of: CE 0.09086894243955612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/198, Loss=0.10011029243469238
Loss made of: CE 0.07723960280418396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/198, Loss=0.09860573858022689
Loss made of: CE 0.08160869777202606, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/198, Loss=0.10331997498869896
Loss made of: CE 0.12705670297145844, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/198, Loss=0.10742241516709328
Loss made of: CE 0.09462243318557739, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/198, Loss=0.10475516095757484
Loss made of: CE 0.12227971851825714, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/198, Loss=0.10767968371510506
Loss made of: CE 0.109946608543396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/198, Loss=0.10284722298383712
Loss made of: CE 0.1029481589794159, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/198, Loss=0.0976591944694519
Loss made of: CE 0.10220967233181, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/198, Loss=0.09714561253786087
Loss made of: CE 0.08417229354381561, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/198, Loss=0.10064771324396134
Loss made of: CE 0.08728890120983124, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/198, Loss=0.10126804709434509
Loss made of: CE 0.09196990728378296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/198, Loss=0.09679685756564141
Loss made of: CE 0.07077937573194504, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10008934885263443, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.10008934885263443, Class Loss=0.10008934885263443, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.001132
Epoch 6, Batch 10/198, Loss=0.0952757827937603
Loss made of: CE 0.10787612199783325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/198, Loss=0.10123149231076241
Loss made of: CE 0.09344235062599182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/198, Loss=0.099210574477911
Loss made of: CE 0.10430483520030975, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/198, Loss=0.10620267838239669
Loss made of: CE 0.09844186156988144, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/198, Loss=0.10162428244948388
Loss made of: CE 0.08369144797325134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/198, Loss=0.10085687860846519
Loss made of: CE 0.08702059835195541, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/198, Loss=0.09799643903970719
Loss made of: CE 0.09234711527824402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/198, Loss=0.09411205127835273
Loss made of: CE 0.12731409072875977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/198, Loss=0.09280933439731598
Loss made of: CE 0.12227576225996017, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/198, Loss=0.08941804096102715
Loss made of: CE 0.06874299049377441, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/198, Loss=0.09949244558811188
Loss made of: CE 0.11827307194471359, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/198, Loss=0.10842427164316178
Loss made of: CE 0.09296688437461853, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/198, Loss=0.10450824499130248
Loss made of: CE 0.10446401685476303, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/198, Loss=0.10636133477091789
Loss made of: CE 0.11097508668899536, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/198, Loss=0.10082849264144897
Loss made of: CE 0.11333396285772324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/198, Loss=0.09715682119131089
Loss made of: CE 0.07319304347038269, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/198, Loss=0.1013000413775444
Loss made of: CE 0.08904191106557846, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/198, Loss=0.09644859731197357
Loss made of: CE 0.0778951495885849, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/198, Loss=0.09565472081303597
Loss made of: CE 0.08814847469329834, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09933506697416306, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.09933506697416306, Class Loss=0.09933506697416306, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006943
Epoch 1, Batch 10/229, Loss=0.10815861001610756
Loss made of: CE 0.08874952793121338, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/229, Loss=0.11851655691862106
Loss made of: CE 0.15134365856647491, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/229, Loss=0.11486256420612335
Loss made of: CE 0.10279296338558197, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/229, Loss=0.11204687878489494
Loss made of: CE 0.08169970661401749, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/229, Loss=0.10493100509047508
Loss made of: CE 0.1198648139834404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/229, Loss=0.11197109445929528
Loss made of: CE 0.10873983800411224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/229, Loss=0.11487308666110038
Loss made of: CE 0.10730312764644623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/229, Loss=0.101347915828228
Loss made of: CE 0.0925951600074768, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/229, Loss=0.11890049502253533
Loss made of: CE 0.08567653596401215, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/229, Loss=0.1047194741666317
Loss made of: CE 0.0971325933933258, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/229, Loss=0.10879956036806107
Loss made of: CE 0.11771439015865326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/229, Loss=0.10629243031144142
Loss made of: CE 0.10056000202894211, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/229, Loss=0.11696081385016441
Loss made of: CE 0.21595320105552673, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/229, Loss=0.10803694799542427
Loss made of: CE 0.0845557451248169, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/229, Loss=0.10698690339922905
Loss made of: CE 0.11584272235631943, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/229, Loss=0.11162224560976028
Loss made of: CE 0.08483319729566574, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/229, Loss=0.11738603860139847
Loss made of: CE 0.1311156302690506, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/229, Loss=0.10798047482967377
Loss made of: CE 0.08661621809005737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/229, Loss=0.1137292318046093
Loss made of: CE 0.12022309750318527, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/229, Loss=0.1083314798772335
Loss made of: CE 0.09052745997905731, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 210/229, Loss=0.10086604952812195
Loss made of: CE 0.09718354791402817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 220/229, Loss=0.12751042619347572
Loss made of: CE 0.18068674206733704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1118616983294487, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.1118616983294487, Class Loss=0.1118616983294487, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.005892
Epoch 2, Batch 10/229, Loss=0.10474957004189492
Loss made of: CE 0.1063704714179039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/229, Loss=0.11077487096190453
Loss made of: CE 0.11350893974304199, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/229, Loss=0.09693182706832885
Loss made of: CE 0.0832013487815857, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/229, Loss=0.10570045411586762
Loss made of: CE 0.11455890536308289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/229, Loss=0.10039519965648651
Loss made of: CE 0.11555978655815125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/229, Loss=0.10544545203447342
Loss made of: CE 0.07598665356636047, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/229, Loss=0.10495057180523873
Loss made of: CE 0.08912404626607895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/229, Loss=0.09916369765996932
Loss made of: CE 0.07362864911556244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/229, Loss=0.10651634410023689
Loss made of: CE 0.08535917848348618, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/229, Loss=0.10166415423154831
Loss made of: CE 0.14371779561042786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/229, Loss=0.10351639315485954
Loss made of: CE 0.1288347691297531, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/229, Loss=0.1045456163585186
Loss made of: CE 0.1190047487616539, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/229, Loss=0.11026670783758163
Loss made of: CE 0.11616915464401245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/229, Loss=0.09441639184951782
Loss made of: CE 0.08707498013973236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/229, Loss=0.09676096364855766
Loss made of: CE 0.09521490335464478, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/229, Loss=0.10467469245195389
Loss made of: CE 0.10477173328399658, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/229, Loss=0.09445005133748055
Loss made of: CE 0.08698847144842148, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/229, Loss=0.10409794598817826
Loss made of: CE 0.10842359066009521, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/229, Loss=0.10071713030338288
Loss made of: CE 0.13431188464164734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/229, Loss=0.09849239885807037
Loss made of: CE 0.12168385833501816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 210/229, Loss=0.10876120999455452
Loss made of: CE 0.13742487132549286, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 220/229, Loss=0.10623084008693695
Loss made of: CE 0.1128813847899437, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1026214063167572, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.1026214063167572, Class Loss=0.1026214063167572, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.004820
Epoch 3, Batch 10/229, Loss=0.09058197066187859
Loss made of: CE 0.09284552186727524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/229, Loss=0.09735164493322372
Loss made of: CE 0.11017453670501709, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/229, Loss=0.09853103309869767
Loss made of: CE 0.0761110931634903, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/229, Loss=0.09801052063703537
Loss made of: CE 0.10810555517673492, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/229, Loss=0.0979910708963871
Loss made of: CE 0.09256362915039062, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/229, Loss=0.09999349936842919
Loss made of: CE 0.13309240341186523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/229, Loss=0.09730881676077843
Loss made of: CE 0.10422541946172714, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/229, Loss=0.0992157019674778
Loss made of: CE 0.08371023833751678, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/229, Loss=0.08888405859470368
Loss made of: CE 0.10869300365447998, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/229, Loss=0.10223545953631401
Loss made of: CE 0.11700765788555145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/229, Loss=0.1105801671743393
Loss made of: CE 0.11903788894414902, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/229, Loss=0.09429802596569062
Loss made of: CE 0.0969247817993164, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/229, Loss=0.1021934762597084
Loss made of: CE 0.13571281731128693, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/229, Loss=0.09994296878576278
Loss made of: CE 0.0820341408252716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/229, Loss=0.09803905934095383
Loss made of: CE 0.08289653062820435, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/229, Loss=0.10024957880377769
Loss made of: CE 0.08462391793727875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/229, Loss=0.10370024070143699
Loss made of: CE 0.07875524461269379, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/229, Loss=0.0953494943678379
Loss made of: CE 0.08331476897001266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/229, Loss=0.09879992976784706
Loss made of: CE 0.10384594649076462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/229, Loss=0.09826576486229896
Loss made of: CE 0.09216328710317612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 210/229, Loss=0.09004550278186799
Loss made of: CE 0.0784916803240776, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 220/229, Loss=0.10191846489906312
Loss made of: CE 0.11239492893218994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09833994507789612, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.09833994507789612, Class Loss=0.09833994507789612, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.003720
Epoch 4, Batch 10/229, Loss=0.09286959245800971
Loss made of: CE 0.10886558890342712, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/229, Loss=0.10214312225580216
Loss made of: CE 0.13986247777938843, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/229, Loss=0.09464693665504456
Loss made of: CE 0.11949457973241806, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/229, Loss=0.09191847890615464
Loss made of: CE 0.10031859576702118, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/229, Loss=0.10341393947601318
Loss made of: CE 0.08857369422912598, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/229, Loss=0.08855573832988739
Loss made of: CE 0.08316784352064133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/229, Loss=0.09751338586211204
Loss made of: CE 0.09919941425323486, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/229, Loss=0.09989524334669113
Loss made of: CE 0.0935586541891098, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/229, Loss=0.09560447856783867
Loss made of: CE 0.09297136962413788, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/229, Loss=0.08752252236008644
Loss made of: CE 0.09614183753728867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/229, Loss=0.09592563733458519
Loss made of: CE 0.08464711159467697, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/229, Loss=0.10268744379281998
Loss made of: CE 0.10555124282836914, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/229, Loss=0.0944278284907341
Loss made of: CE 0.09569817781448364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/229, Loss=0.08583201877772809
Loss made of: CE 0.05808435007929802, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/229, Loss=0.09772127643227577
Loss made of: CE 0.12001584470272064, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/229, Loss=0.09368418157100677
Loss made of: CE 0.10282546281814575, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/229, Loss=0.08910475820302963
Loss made of: CE 0.08298399299383163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/229, Loss=0.09754591062664986
Loss made of: CE 0.08487002551555634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/229, Loss=0.0830036997795105
Loss made of: CE 0.08980631828308105, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/229, Loss=0.10237837433815003
Loss made of: CE 0.118732750415802, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 210/229, Loss=0.10302365943789482
Loss made of: CE 0.1204054206609726, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 220/229, Loss=0.08771650567650795
Loss made of: CE 0.07773751020431519, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09482097625732422, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.09482097625732422, Class Loss=0.09482097625732422, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.002583
Epoch 5, Batch 10/229, Loss=0.0953736238181591
Loss made of: CE 0.11132331192493439, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/229, Loss=0.09515529349446297
Loss made of: CE 0.06747156381607056, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/229, Loss=0.09514946863055229
Loss made of: CE 0.06283526867628098, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/229, Loss=0.09748970791697502
Loss made of: CE 0.10648749768733978, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/229, Loss=0.08717684522271156
Loss made of: CE 0.09726034104824066, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/229, Loss=0.10550757050514221
Loss made of: CE 0.07921284437179565, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/229, Loss=0.08358627781271935
Loss made of: CE 0.05977633595466614, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/229, Loss=0.08629443421959877
Loss made of: CE 0.0908198356628418, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/229, Loss=0.0888922743499279
Loss made of: CE 0.11113320291042328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/229, Loss=0.09204218462109566
Loss made of: CE 0.09152562916278839, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/229, Loss=0.0863238476216793
Loss made of: CE 0.077781543135643, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/229, Loss=0.09198316894471645
Loss made of: CE 0.12671494483947754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/229, Loss=0.09544615522027015
Loss made of: CE 0.09749376773834229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/229, Loss=0.09217573031783104
Loss made of: CE 0.12217804789543152, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/229, Loss=0.09092672914266586
Loss made of: CE 0.10407032817602158, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/229, Loss=0.0878018893301487
Loss made of: CE 0.09267401695251465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/229, Loss=0.08709798231720925
Loss made of: CE 0.07676445692777634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/229, Loss=0.09056385084986687
Loss made of: CE 0.10176786035299301, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/229, Loss=0.09134722277522087
Loss made of: CE 0.09308688342571259, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/229, Loss=0.10347072705626488
Loss made of: CE 0.11454250663518906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 210/229, Loss=0.09815993010997773
Loss made of: CE 0.09585493057966232, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 220/229, Loss=0.09174197688698768
Loss made of: CE 0.09768297523260117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09276754409074783, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.09276754409074783, Class Loss=0.09276754409074783, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.001384
Epoch 6, Batch 10/229, Loss=0.08231303356587887
Loss made of: CE 0.09362127631902695, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/229, Loss=0.10173610374331474
Loss made of: CE 0.12928391993045807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/229, Loss=0.08863409236073494
Loss made of: CE 0.09454740583896637, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/229, Loss=0.0893214501440525
Loss made of: CE 0.08707007020711899, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/229, Loss=0.08445827476680279
Loss made of: CE 0.084628164768219, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/229, Loss=0.0888405866920948
Loss made of: CE 0.09528136253356934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/229, Loss=0.08941520527005195
Loss made of: CE 0.08709892630577087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/229, Loss=0.09369274526834488
Loss made of: CE 0.09189315885305405, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/229, Loss=0.09137799963355064
Loss made of: CE 0.08264723420143127, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/229, Loss=0.09258161410689354
Loss made of: CE 0.10384919494390488, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/229, Loss=0.08765048384666443
Loss made of: CE 0.08700531721115112, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/229, Loss=0.08793380260467529
Loss made of: CE 0.08325286209583282, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/229, Loss=0.09318088069558143
Loss made of: CE 0.10857986658811569, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/229, Loss=0.09669341295957565
Loss made of: CE 0.0839613676071167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/229, Loss=0.09038367196917534
Loss made of: CE 0.0858001559972763, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/229, Loss=0.09935989528894425
Loss made of: CE 0.06952263414859772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/229, Loss=0.0949040912091732
Loss made of: CE 0.08724092692136765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/229, Loss=0.08957483097910882
Loss made of: CE 0.09393177926540375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/229, Loss=0.09541732221841812
Loss made of: CE 0.12248574197292328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/229, Loss=0.0940167523920536
Loss made of: CE 0.1236773356795311, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 210/229, Loss=0.08904119804501534
Loss made of: CE 0.08394651114940643, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 220/229, Loss=0.08794844299554824
Loss made of: CE 0.06583346426486969, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09138159453868866, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.09138159453868866, Class Loss=0.09138159453868866, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.14494292438030243, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.950830
Mean Acc: 0.889262
FreqW Acc: 0.911034
Mean IoU: 0.792346
Class IoU:
	class 0: 0.9425439
	class 1: 0.8807598
	class 2: 0.41696778
	class 3: 0.8826386
	class 4: 0.7126709
	class 5: 0.8128916
	class 6: 0.94414604
	class 7: 0.9120399
	class 8: 0.9238311
	class 9: 0.44856864
	class 10: 0.77228075
	class 11: 0.6092903
	class 12: 0.8707074
	class 13: 0.79715073
	class 14: 0.88127756
	class 15: 0.869777
Class Acc:
	class 0: 0.9691667
	class 1: 0.96784496
	class 2: 0.92333335
	class 3: 0.92683524
	class 4: 0.8479465
	class 5: 0.91817296
	class 6: 0.97179073
	class 7: 0.9446795
	class 8: 0.9678692
	class 9: 0.5860737
	class 10: 0.81374025
	class 11: 0.64616156
	class 12: 0.9657914
	class 13: 0.9054395
	class 14: 0.9475769
	class 15: 0.92577076

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[3, 6, 12, 0]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.5625506043434143, Reg Loss=0.12367451936006546
Clinet index 3, End of Epoch 1/6, Average Loss=0.6862251162528992, Class Loss=0.5625506043434143, Reg Loss=0.12367451936006546
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.45200851559638977, Reg Loss=0.1303093582391739
Clinet index 3, End of Epoch 2/6, Average Loss=0.5823178887367249, Class Loss=0.45200851559638977, Reg Loss=0.1303093582391739
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.3953116536140442, Reg Loss=0.13400930166244507
Clinet index 3, End of Epoch 3/6, Average Loss=0.5293209552764893, Class Loss=0.3953116536140442, Reg Loss=0.13400930166244507
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.38294321298599243, Reg Loss=0.13077424466609955
Clinet index 3, End of Epoch 4/6, Average Loss=0.5137174725532532, Class Loss=0.38294321298599243, Reg Loss=0.13077424466609955
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.3901668190956116, Reg Loss=0.16439782083034515
Clinet index 3, End of Epoch 5/6, Average Loss=0.5545646548271179, Class Loss=0.3901668190956116, Reg Loss=0.16439782083034515
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.3409642279148102, Reg Loss=0.15596653521060944
Clinet index 3, End of Epoch 6/6, Average Loss=0.4969307780265808, Class Loss=0.3409642279148102, Reg Loss=0.15596653521060944
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.561456024646759, Reg Loss=0.12973888218402863
Clinet index 6, End of Epoch 1/6, Average Loss=0.6911948919296265, Class Loss=0.561456024646759, Reg Loss=0.12973888218402863
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.4279978275299072, Reg Loss=0.11987121403217316
Clinet index 6, End of Epoch 2/6, Average Loss=0.5478690266609192, Class Loss=0.4279978275299072, Reg Loss=0.11987121403217316
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.4074266254901886, Reg Loss=0.1247994676232338
Clinet index 6, End of Epoch 3/6, Average Loss=0.5322260856628418, Class Loss=0.4074266254901886, Reg Loss=0.1247994676232338
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.3750017583370209, Reg Loss=0.13513123989105225
Clinet index 6, End of Epoch 4/6, Average Loss=0.5101330280303955, Class Loss=0.3750017583370209, Reg Loss=0.13513123989105225
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.36878275871276855, Reg Loss=0.14654859900474548
Clinet index 6, End of Epoch 5/6, Average Loss=0.5153313875198364, Class Loss=0.36878275871276855, Reg Loss=0.14654859900474548
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.3419114053249359, Reg Loss=0.16734281182289124
Clinet index 6, End of Epoch 6/6, Average Loss=0.5092542171478271, Class Loss=0.3419114053249359, Reg Loss=0.16734281182289124
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.5699222087860107, Reg Loss=0.1228211373090744
Clinet index 12, End of Epoch 1/6, Average Loss=0.6927433609962463, Class Loss=0.5699222087860107, Reg Loss=0.1228211373090744
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.4407593607902527, Reg Loss=0.1322043389081955
Clinet index 12, End of Epoch 2/6, Average Loss=0.5729637145996094, Class Loss=0.4407593607902527, Reg Loss=0.1322043389081955
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.41280069947242737, Reg Loss=0.12213476002216339
Clinet index 12, End of Epoch 3/6, Average Loss=0.534935474395752, Class Loss=0.41280069947242737, Reg Loss=0.12213476002216339
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.3913825750350952, Reg Loss=0.13030602037906647
Clinet index 12, End of Epoch 4/6, Average Loss=0.5216885805130005, Class Loss=0.3913825750350952, Reg Loss=0.13030602037906647
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.3726930320262909, Reg Loss=0.1524633765220642
Clinet index 12, End of Epoch 5/6, Average Loss=0.5251563787460327, Class Loss=0.3726930320262909, Reg Loss=0.1524633765220642
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.36762896180152893, Reg Loss=0.17100439965724945
Clinet index 12, End of Epoch 6/6, Average Loss=0.5386333465576172, Class Loss=0.36762896180152893, Reg Loss=0.17100439965724945
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.5198287963867188, Reg Loss=0.11483107507228851
Clinet index 0, End of Epoch 1/6, Average Loss=0.6346598863601685, Class Loss=0.5198287963867188, Reg Loss=0.11483107507228851
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.4308251440525055, Reg Loss=0.11748947948217392
Clinet index 0, End of Epoch 2/6, Average Loss=0.54831463098526, Class Loss=0.4308251440525055, Reg Loss=0.11748947948217392
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.3745152950286865, Reg Loss=0.12057270109653473
Clinet index 0, End of Epoch 3/6, Average Loss=0.49508798122406006, Class Loss=0.3745152950286865, Reg Loss=0.12057270109653473
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.38142040371894836, Reg Loss=0.13560724258422852
Clinet index 0, End of Epoch 4/6, Average Loss=0.5170276165008545, Class Loss=0.38142040371894836, Reg Loss=0.13560724258422852
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.36496788263320923, Reg Loss=0.1483217477798462
Clinet index 0, End of Epoch 5/6, Average Loss=0.5132896304130554, Class Loss=0.36496788263320923, Reg Loss=0.1483217477798462
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.3465403616428375, Reg Loss=0.15670517086982727
Clinet index 0, End of Epoch 6/6, Average Loss=0.5032455325126648, Class Loss=0.3465403616428375, Reg Loss=0.15670517086982727
federated aggregation...
Validation, Class Loss=0.3970825672149658, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.901879
Mean Acc: 0.876714
FreqW Acc: 0.834499
Mean IoU: 0.660669
Class IoU:
	class 0: 0.8784292
	class 1: 0.6626835
	class 2: 0.3240041
	class 3: 0.81208575
	class 4: 0.5746312
	class 5: 0.7035018
	class 6: 0.8905387
	class 7: 0.85436076
	class 8: 0.831833
	class 9: 0.40948698
	class 10: 0.6678301
	class 11: 0.6154461
	class 12: 0.758086
	class 13: 0.6705342
	class 14: 0.7964049
	class 15: 0.7815011
	class 16: 1.128744e-05
Class Acc:
	class 0: 0.8918449
	class 1: 0.9958831
	class 2: 0.96526533
	class 3: 0.9688611
	class 4: 0.9333501
	class 5: 0.95946944
	class 6: 0.9928609
	class 7: 0.9697159
	class 8: 0.97538024
	class 9: 0.7727421
	class 10: 0.8436815
	class 11: 0.76918006
	class 12: 0.9858323
	class 13: 0.94334066
	class 14: 0.9766246
	class 15: 0.96009713
	class 16: 1.128744e-05

federated global round: 6, step: 1
select part of clients to conduct local training
[0, 2, 8, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.32276278734207153, Reg Loss=0.15592016279697418
Clinet index 0, End of Epoch 1/6, Average Loss=0.4786829352378845, Class Loss=0.32276278734207153, Reg Loss=0.15592016279697418
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.34297558665275574, Reg Loss=0.165325328707695
Clinet index 0, End of Epoch 2/6, Average Loss=0.5083009004592896, Class Loss=0.34297558665275574, Reg Loss=0.165325328707695
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Class Loss=0.3322380781173706, Reg Loss=0.1650206744670868
Clinet index 0, End of Epoch 3/6, Average Loss=0.4972587525844574, Class Loss=0.3322380781173706, Reg Loss=0.1650206744670868
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Class Loss=0.3208683729171753, Reg Loss=0.1679418534040451
Clinet index 0, End of Epoch 4/6, Average Loss=0.4888102412223816, Class Loss=0.3208683729171753, Reg Loss=0.1679418534040451
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.31313493847846985, Reg Loss=0.16624395549297333
Clinet index 0, End of Epoch 5/6, Average Loss=0.479378879070282, Class Loss=0.31313493847846985, Reg Loss=0.16624395549297333
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Class Loss=0.3016027808189392, Reg Loss=0.16294197738170624
Clinet index 0, End of Epoch 6/6, Average Loss=0.46454477310180664, Class Loss=0.3016027808189392, Reg Loss=0.16294197738170624
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.34614989161491394, Reg Loss=0.1803237497806549
Clinet index 2, End of Epoch 1/6, Average Loss=0.5264736413955688, Class Loss=0.34614989161491394, Reg Loss=0.1803237497806549
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.3627944588661194, Reg Loss=0.18065686523914337
Clinet index 2, End of Epoch 2/6, Average Loss=0.5434513092041016, Class Loss=0.3627944588661194, Reg Loss=0.18065686523914337
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.33188578486442566, Reg Loss=0.16506682336330414
Clinet index 2, End of Epoch 3/6, Average Loss=0.4969525933265686, Class Loss=0.33188578486442566, Reg Loss=0.16506682336330414
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.3227260112762451, Reg Loss=0.16551341116428375
Clinet index 2, End of Epoch 4/6, Average Loss=0.4882394075393677, Class Loss=0.3227260112762451, Reg Loss=0.16551341116428375
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.3087671995162964, Reg Loss=0.16421398520469666
Clinet index 2, End of Epoch 5/6, Average Loss=0.47298118472099304, Class Loss=0.3087671995162964, Reg Loss=0.16421398520469666
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.3191012442111969, Reg Loss=0.16376858949661255
Clinet index 2, End of Epoch 6/6, Average Loss=0.48286983370780945, Class Loss=0.3191012442111969, Reg Loss=0.16376858949661255
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.37294602394104004, Reg Loss=0.16633999347686768
Clinet index 8, End of Epoch 1/6, Average Loss=0.5392860174179077, Class Loss=0.37294602394104004, Reg Loss=0.16633999347686768
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.36873841285705566, Reg Loss=0.17050328850746155
Clinet index 8, End of Epoch 2/6, Average Loss=0.5392416715621948, Class Loss=0.36873841285705566, Reg Loss=0.17050328850746155
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.3414244055747986, Reg Loss=0.16204500198364258
Clinet index 8, End of Epoch 3/6, Average Loss=0.5034694075584412, Class Loss=0.3414244055747986, Reg Loss=0.16204500198364258
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.32968223094940186, Reg Loss=0.16309669613838196
Clinet index 8, End of Epoch 4/6, Average Loss=0.4927789270877838, Class Loss=0.32968223094940186, Reg Loss=0.16309669613838196
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.3227598965167999, Reg Loss=0.1619466245174408
Clinet index 8, End of Epoch 5/6, Average Loss=0.4847065210342407, Class Loss=0.3227598965167999, Reg Loss=0.1619466245174408
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.31894955039024353, Reg Loss=0.16301551461219788
Clinet index 8, End of Epoch 6/6, Average Loss=0.4819650650024414, Class Loss=0.31894955039024353, Reg Loss=0.16301551461219788
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3635508120059967, Reg Loss=0.16449415683746338
Clinet index 4, End of Epoch 1/6, Average Loss=0.5280449390411377, Class Loss=0.3635508120059967, Reg Loss=0.16449415683746338
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.34976598620414734, Reg Loss=0.17095713317394257
Clinet index 4, End of Epoch 2/6, Average Loss=0.5207231044769287, Class Loss=0.34976598620414734, Reg Loss=0.17095713317394257
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.3306092321872711, Reg Loss=0.1680799126625061
Clinet index 4, End of Epoch 3/6, Average Loss=0.4986891448497772, Class Loss=0.3306092321872711, Reg Loss=0.1680799126625061
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.33327922224998474, Reg Loss=0.16737301647663116
Clinet index 4, End of Epoch 4/6, Average Loss=0.5006522536277771, Class Loss=0.33327922224998474, Reg Loss=0.16737301647663116
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.33014941215515137, Reg Loss=0.1659473329782486
Clinet index 4, End of Epoch 5/6, Average Loss=0.49609673023223877, Class Loss=0.33014941215515137, Reg Loss=0.1659473329782486
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.3056052029132843, Reg Loss=0.15223875641822815
Clinet index 4, End of Epoch 6/6, Average Loss=0.45784395933151245, Class Loss=0.3056052029132843, Reg Loss=0.15223875641822815
federated aggregation...
Validation, Class Loss=0.42037105560302734, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.888920
Mean Acc: 0.889208
FreqW Acc: 0.818673
Mean IoU: 0.651966
Class IoU:
	class 0: 0.8607829
	class 1: 0.6446402
	class 2: 0.31833822
	class 3: 0.7821553
	class 4: 0.5585189
	class 5: 0.6812333
	class 6: 0.8814955
	class 7: 0.85460436
	class 8: 0.81831944
	class 9: 0.41940245
	class 10: 0.64523256
	class 11: 0.5910928
	class 12: 0.7311199
	class 13: 0.6405521
	class 14: 0.77430606
	class 15: 0.7761861
	class 16: 0.105438344
Class Acc:
	class 0: 0.87137467
	class 1: 0.99753153
	class 2: 0.9703706
	class 3: 0.9738085
	class 4: 0.9396728
	class 5: 0.96190256
	class 6: 0.99450105
	class 7: 0.9703977
	class 8: 0.97569114
	class 9: 0.7891638
	class 10: 0.810231
	class 11: 0.7742073
	class 12: 0.9871541
	class 13: 0.9371267
	class 14: 0.98067194
	class 15: 0.959634
	class 16: 0.22309485

federated global round: 7, step: 1
select part of clients to conduct local training
[9, 13, 1, 11]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.30552735924720764, Reg Loss=0.14959882199764252
Clinet index 9, End of Epoch 1/6, Average Loss=0.45512616634368896, Class Loss=0.30552735924720764, Reg Loss=0.14959882199764252
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.3036840856075287, Reg Loss=0.15434427559375763
Clinet index 9, End of Epoch 2/6, Average Loss=0.4580283761024475, Class Loss=0.3036840856075287, Reg Loss=0.15434427559375763
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 3, Class Loss=0.31158211827278137, Reg Loss=0.15388838946819305
Clinet index 9, End of Epoch 3/6, Average Loss=0.46547049283981323, Class Loss=0.31158211827278137, Reg Loss=0.15388838946819305
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.2853398323059082, Reg Loss=0.1441287249326706
Clinet index 9, End of Epoch 4/6, Average Loss=0.42946857213974, Class Loss=0.2853398323059082, Reg Loss=0.1441287249326706
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.2838556170463562, Reg Loss=0.14424915611743927
Clinet index 9, End of Epoch 5/6, Average Loss=0.4281047582626343, Class Loss=0.2838556170463562, Reg Loss=0.14424915611743927
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.27546507120132446, Reg Loss=0.1364458054304123
Clinet index 9, End of Epoch 6/6, Average Loss=0.41191089153289795, Class Loss=0.27546507120132446, Reg Loss=0.1364458054304123
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.31488168239593506, Reg Loss=0.15838393568992615
Clinet index 13, End of Epoch 1/6, Average Loss=0.4732656180858612, Class Loss=0.31488168239593506, Reg Loss=0.15838393568992615
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.3186740577220917, Reg Loss=0.1622866690158844
Clinet index 13, End of Epoch 2/6, Average Loss=0.4809607267379761, Class Loss=0.3186740577220917, Reg Loss=0.1622866690158844
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.30291905999183655, Reg Loss=0.15124812722206116
Clinet index 13, End of Epoch 3/6, Average Loss=0.4541671872138977, Class Loss=0.30291905999183655, Reg Loss=0.15124812722206116
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.3008570075035095, Reg Loss=0.15668408572673798
Clinet index 13, End of Epoch 4/6, Average Loss=0.4575411081314087, Class Loss=0.3008570075035095, Reg Loss=0.15668408572673798
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.3168727457523346, Reg Loss=0.14727075397968292
Clinet index 13, End of Epoch 5/6, Average Loss=0.4641435146331787, Class Loss=0.3168727457523346, Reg Loss=0.14727075397968292
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.2949078381061554, Reg Loss=0.14576688408851624
Clinet index 13, End of Epoch 6/6, Average Loss=0.44067472219467163, Class Loss=0.2949078381061554, Reg Loss=0.14576688408851624
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.28412163257598877, Reg Loss=0.15273138880729675
Clinet index 1, End of Epoch 1/6, Average Loss=0.4368530213832855, Class Loss=0.28412163257598877, Reg Loss=0.15273138880729675
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.28122758865356445, Reg Loss=0.1572858840227127
Clinet index 1, End of Epoch 2/6, Average Loss=0.43851345777511597, Class Loss=0.28122758865356445, Reg Loss=0.1572858840227127
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.2858278155326843, Reg Loss=0.16152578592300415
Clinet index 1, End of Epoch 3/6, Average Loss=0.4473536014556885, Class Loss=0.2858278155326843, Reg Loss=0.16152578592300415
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.2872787415981293, Reg Loss=0.1528950333595276
Clinet index 1, End of Epoch 4/6, Average Loss=0.44017377495765686, Class Loss=0.2872787415981293, Reg Loss=0.1528950333595276
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.2595049738883972, Reg Loss=0.14481623470783234
Clinet index 1, End of Epoch 5/6, Average Loss=0.40432119369506836, Class Loss=0.2595049738883972, Reg Loss=0.14481623470783234
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.26791617274284363, Reg Loss=0.1498957723379135
Clinet index 1, End of Epoch 6/6, Average Loss=0.41781193017959595, Class Loss=0.26791617274284363, Reg Loss=0.1498957723379135
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.28344789147377014, Reg Loss=0.14730121195316315
Clinet index 11, End of Epoch 1/6, Average Loss=0.4307491183280945, Class Loss=0.28344789147377014, Reg Loss=0.14730121195316315
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.28489673137664795, Reg Loss=0.1497136652469635
Clinet index 11, End of Epoch 2/6, Average Loss=0.43461039662361145, Class Loss=0.28489673137664795, Reg Loss=0.1497136652469635
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.2906714081764221, Reg Loss=0.1522524654865265
Clinet index 11, End of Epoch 3/6, Average Loss=0.4429238736629486, Class Loss=0.2906714081764221, Reg Loss=0.1522524654865265
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.2743408977985382, Reg Loss=0.15117135643959045
Clinet index 11, End of Epoch 4/6, Average Loss=0.42551225423812866, Class Loss=0.2743408977985382, Reg Loss=0.15117135643959045
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.27386462688446045, Reg Loss=0.14207135140895844
Clinet index 11, End of Epoch 5/6, Average Loss=0.4159359931945801, Class Loss=0.27386462688446045, Reg Loss=0.14207135140895844
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.2629490792751312, Reg Loss=0.14686256647109985
Clinet index 11, End of Epoch 6/6, Average Loss=0.4098116457462311, Class Loss=0.2629490792751312, Reg Loss=0.14686256647109985
federated aggregation...
Validation, Class Loss=0.38615790009498596, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.890025
Mean Acc: 0.894484
FreqW Acc: 0.824168
Mean IoU: 0.667551
Class IoU:
	class 0: 0.86194086
	class 1: 0.6701942
	class 2: 0.3324166
	class 3: 0.79400873
	class 4: 0.5798173
	class 5: 0.7149438
	class 6: 0.8943581
	class 7: 0.86887926
	class 8: 0.80821186
	class 9: 0.4461622
	class 10: 0.6773662
	class 11: 0.58129424
	class 12: 0.74748
	class 13: 0.68791467
	class 14: 0.7911192
	class 15: 0.7950823
	class 16: 0.097174674
Class Acc:
	class 0: 0.8723918
	class 1: 0.99596864
	class 2: 0.96775883
	class 3: 0.9738404
	class 4: 0.92420036
	class 5: 0.9637581
	class 6: 0.9911789
	class 7: 0.96515787
	class 8: 0.98435223
	class 9: 0.7646081
	class 10: 0.8176889
	class 11: 0.7997435
	class 12: 0.97866154
	class 13: 0.9346404
	class 14: 0.97747785
	class 15: 0.9561701
	class 16: 0.3386265

federated global round: 8, step: 1
select part of clients to conduct local training
[8, 5, 2, 9]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.2844008803367615, Reg Loss=0.1330004781484604
Clinet index 8, End of Epoch 1/6, Average Loss=0.41740137338638306, Class Loss=0.2844008803367615, Reg Loss=0.1330004781484604
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.28103142976760864, Reg Loss=0.14104768633842468
Clinet index 8, End of Epoch 2/6, Average Loss=0.4220791161060333, Class Loss=0.28103142976760864, Reg Loss=0.14104768633842468
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Class Loss=0.27229541540145874, Reg Loss=0.13829798996448517
Clinet index 8, End of Epoch 3/6, Average Loss=0.4105933904647827, Class Loss=0.27229541540145874, Reg Loss=0.13829798996448517
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Class Loss=0.2660198509693146, Reg Loss=0.14155496656894684
Clinet index 8, End of Epoch 4/6, Average Loss=0.4075748324394226, Class Loss=0.2660198509693146, Reg Loss=0.14155496656894684
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Class Loss=0.26842328906059265, Reg Loss=0.13910453021526337
Clinet index 8, End of Epoch 5/6, Average Loss=0.4075278043746948, Class Loss=0.26842328906059265, Reg Loss=0.13910453021526337
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Class Loss=0.2757208049297333, Reg Loss=0.1375175416469574
Clinet index 8, End of Epoch 6/6, Average Loss=0.4132383465766907, Class Loss=0.2757208049297333, Reg Loss=0.1375175416469574
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.26070061326026917, Reg Loss=0.13680683076381683
Clinet index 5, End of Epoch 1/6, Average Loss=0.3975074291229248, Class Loss=0.26070061326026917, Reg Loss=0.13680683076381683
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.26714977622032166, Reg Loss=0.14642028510570526
Clinet index 5, End of Epoch 2/6, Average Loss=0.4135700464248657, Class Loss=0.26714977622032166, Reg Loss=0.14642028510570526
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.27020546793937683, Reg Loss=0.1502714902162552
Clinet index 5, End of Epoch 3/6, Average Loss=0.4204769730567932, Class Loss=0.27020546793937683, Reg Loss=0.1502714902162552
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.2922728359699249, Reg Loss=0.15214574337005615
Clinet index 5, End of Epoch 4/6, Average Loss=0.4444185793399811, Class Loss=0.2922728359699249, Reg Loss=0.15214574337005615
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.26523301005363464, Reg Loss=0.13572262227535248
Clinet index 5, End of Epoch 5/6, Average Loss=0.4009556174278259, Class Loss=0.26523301005363464, Reg Loss=0.13572262227535248
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.2622438073158264, Reg Loss=0.1411411464214325
Clinet index 5, End of Epoch 6/6, Average Loss=0.4033849537372589, Class Loss=0.2622438073158264, Reg Loss=0.1411411464214325
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.27436667680740356, Reg Loss=0.14234673976898193
Clinet index 2, End of Epoch 1/6, Average Loss=0.4167134165763855, Class Loss=0.27436667680740356, Reg Loss=0.14234673976898193
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Epoch 2, Class Loss=0.2881644070148468, Reg Loss=0.15070581436157227
Clinet index 2, End of Epoch 2/6, Average Loss=0.43887022137641907, Class Loss=0.2881644070148468, Reg Loss=0.15070581436157227
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Class Loss=0.26482245326042175, Reg Loss=0.13469573855400085
Clinet index 2, End of Epoch 3/6, Average Loss=0.3995181918144226, Class Loss=0.26482245326042175, Reg Loss=0.13469573855400085
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Class Loss=0.2813021242618561, Reg Loss=0.1452960968017578
Clinet index 2, End of Epoch 4/6, Average Loss=0.4265982210636139, Class Loss=0.2813021242618561, Reg Loss=0.1452960968017578
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Class Loss=0.2772173583507538, Reg Loss=0.14872956275939941
Clinet index 2, End of Epoch 5/6, Average Loss=0.4259469211101532, Class Loss=0.2772173583507538, Reg Loss=0.14872956275939941
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Class Loss=0.2651144564151764, Reg Loss=0.13774007558822632
Clinet index 2, End of Epoch 6/6, Average Loss=0.4028545320034027, Class Loss=0.2651144564151764, Reg Loss=0.13774007558822632
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.2788969874382019, Reg Loss=0.13406069576740265
Clinet index 9, End of Epoch 1/6, Average Loss=0.41295766830444336, Class Loss=0.2788969874382019, Reg Loss=0.13406069576740265
Pseudo labeling is: None
Epoch 2, lr = 0.000642
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.2772892713546753, Reg Loss=0.1293768584728241
Clinet index 9, End of Epoch 2/6, Average Loss=0.4066661298274994, Class Loss=0.2772892713546753, Reg Loss=0.1293768584728241
Pseudo labeling is: None
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.2790289521217346, Reg Loss=0.13644619286060333
Clinet index 9, End of Epoch 3/6, Average Loss=0.41547513008117676, Class Loss=0.2790289521217346, Reg Loss=0.13644619286060333
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.2771638035774231, Reg Loss=0.1346837878227234
Clinet index 9, End of Epoch 4/6, Average Loss=0.4118475914001465, Class Loss=0.2771638035774231, Reg Loss=0.1346837878227234
Pseudo labeling is: None
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.2718392014503479, Reg Loss=0.1331426501274109
Clinet index 9, End of Epoch 5/6, Average Loss=0.4049818515777588, Class Loss=0.2718392014503479, Reg Loss=0.1331426501274109
Pseudo labeling is: None
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.273881733417511, Reg Loss=0.1280193328857422
Clinet index 9, End of Epoch 6/6, Average Loss=0.4019010663032532, Class Loss=0.273881733417511, Reg Loss=0.1280193328857422
federated aggregation...
Validation, Class Loss=0.37371039390563965, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.887405
Mean Acc: 0.897227
FreqW Acc: 0.823888
Mean IoU: 0.672639
Class IoU:
	class 0: 0.85804695
	class 1: 0.6870458
	class 2: 0.33447742
	class 3: 0.811526
	class 4: 0.58630395
	class 5: 0.71723896
	class 6: 0.8993996
	class 7: 0.87468576
	class 8: 0.8320786
	class 9: 0.43353838
	class 10: 0.6809781
	class 11: 0.5848024
	class 12: 0.7574693
	class 13: 0.70229316
	class 14: 0.7830113
	class 15: 0.80458105
	class 16: 0.087380275
Class Acc:
	class 0: 0.8681715
	class 1: 0.9945664
	class 2: 0.9679294
	class 3: 0.9711799
	class 4: 0.92559695
	class 5: 0.96504694
	class 6: 0.9924183
	class 7: 0.9658559
	class 8: 0.9804701
	class 9: 0.7675709
	class 10: 0.81493896
	class 11: 0.7883607
	class 12: 0.9890251
	class 13: 0.93523854
	class 14: 0.98021597
	class 15: 0.95479
	class 16: 0.39147714

federated global round: 9, step: 1
select part of clients to conduct local training
[8, 9, 12, 4]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000414
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.27734237909317017, Reg Loss=0.12946763634681702
Clinet index 8, End of Epoch 1/6, Average Loss=0.4068100154399872, Class Loss=0.27734237909317017, Reg Loss=0.12946763634681702
Pseudo labeling is: None
Epoch 2, lr = 0.000351
Epoch 2, Class Loss=0.27475371956825256, Reg Loss=0.1370408684015274
Clinet index 8, End of Epoch 2/6, Average Loss=0.41179460287094116, Class Loss=0.27475371956825256, Reg Loss=0.1370408684015274
Pseudo labeling is: None
Epoch 3, lr = 0.000287
Epoch 3, Class Loss=0.262839674949646, Reg Loss=0.13364212214946747
Clinet index 8, End of Epoch 3/6, Average Loss=0.39648181200027466, Class Loss=0.262839674949646, Reg Loss=0.13364212214946747
Pseudo labeling is: None
Epoch 4, lr = 0.000222
Epoch 4, Class Loss=0.2642616331577301, Reg Loss=0.138824462890625
Clinet index 8, End of Epoch 4/6, Average Loss=0.4030860960483551, Class Loss=0.2642616331577301, Reg Loss=0.138824462890625
Pseudo labeling is: None
Epoch 5, lr = 0.000154
Epoch 5, Class Loss=0.2583693861961365, Reg Loss=0.12681998312473297
Clinet index 8, End of Epoch 5/6, Average Loss=0.38518935441970825, Class Loss=0.2583693861961365, Reg Loss=0.12681998312473297
Pseudo labeling is: None
Epoch 6, lr = 0.000082
Epoch 6, Class Loss=0.2613884508609772, Reg Loss=0.13070476055145264
Clinet index 8, End of Epoch 6/6, Average Loss=0.3920932114124298, Class Loss=0.2613884508609772, Reg Loss=0.13070476055145264
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000372
Epoch 1, Class Loss=0.26401087641716003, Reg Loss=0.13202805817127228
Clinet index 9, End of Epoch 1/6, Average Loss=0.3960389494895935, Class Loss=0.26401087641716003, Reg Loss=0.13202805817127228
Pseudo labeling is: None
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.26445239782333374, Reg Loss=0.12786024808883667
Clinet index 9, End of Epoch 2/6, Average Loss=0.3923126459121704, Class Loss=0.26445239782333374, Reg Loss=0.12786024808883667
Pseudo labeling is: None
Epoch 3, lr = 0.000258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.26534485816955566, Reg Loss=0.12948675453662872
Clinet index 9, End of Epoch 3/6, Average Loss=0.3948315978050232, Class Loss=0.26534485816955566, Reg Loss=0.12948675453662872
Pseudo labeling is: None
Epoch 4, lr = 0.000199
Epoch 4, Class Loss=0.25930386781692505, Reg Loss=0.12653128802776337
Clinet index 9, End of Epoch 4/6, Average Loss=0.3858351707458496, Class Loss=0.25930386781692505, Reg Loss=0.12653128802776337
Pseudo labeling is: None
Epoch 5, lr = 0.000138
Epoch 5, Class Loss=0.2573210597038269, Reg Loss=0.12989313900470734
Clinet index 9, End of Epoch 5/6, Average Loss=0.38721418380737305, Class Loss=0.2573210597038269, Reg Loss=0.12989313900470734
Pseudo labeling is: None
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.2658427953720093, Reg Loss=0.12496530264616013
Clinet index 9, End of Epoch 6/6, Average Loss=0.39080810546875, Class Loss=0.2658427953720093, Reg Loss=0.12496530264616013
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.2744443118572235, Reg Loss=0.13881130516529083
Clinet index 12, End of Epoch 1/6, Average Loss=0.41325563192367554, Class Loss=0.2744443118572235, Reg Loss=0.13881130516529083
Pseudo labeling is: None
Epoch 2, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.27383849024772644, Reg Loss=0.14331196248531342
Clinet index 12, End of Epoch 2/6, Average Loss=0.41715043783187866, Class Loss=0.27383849024772644, Reg Loss=0.14331196248531342
Pseudo labeling is: None
Epoch 3, lr = 0.000568
Epoch 3, Class Loss=0.277680903673172, Reg Loss=0.13798968493938446
Clinet index 12, End of Epoch 3/6, Average Loss=0.41567057371139526, Class Loss=0.277680903673172, Reg Loss=0.13798968493938446
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Class Loss=0.2738739550113678, Reg Loss=0.14396339654922485
Clinet index 12, End of Epoch 4/6, Average Loss=0.41783735156059265, Class Loss=0.2738739550113678, Reg Loss=0.14396339654922485
Pseudo labeling is: None
Epoch 5, lr = 0.000304
Epoch 5, Class Loss=0.26591360569000244, Reg Loss=0.13889269530773163
Clinet index 12, End of Epoch 5/6, Average Loss=0.40480631589889526, Class Loss=0.26591360569000244, Reg Loss=0.13889269530773163
Pseudo labeling is: None
Epoch 6, lr = 0.000163
Epoch 6, Class Loss=0.25618359446525574, Reg Loss=0.13519452512264252
Clinet index 12, End of Epoch 6/6, Average Loss=0.39137810468673706, Class Loss=0.25618359446525574, Reg Loss=0.13519452512264252
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.2667858302593231, Reg Loss=0.1374722272157669
Clinet index 4, End of Epoch 1/6, Average Loss=0.4042580723762512, Class Loss=0.2667858302593231, Reg Loss=0.1374722272157669
Pseudo labeling is: None
Epoch 2, lr = 0.000655
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.26475125551223755, Reg Loss=0.14342133700847626
Clinet index 4, End of Epoch 2/6, Average Loss=0.408172607421875, Class Loss=0.26475125551223755, Reg Loss=0.14342133700847626
Pseudo labeling is: None
Epoch 3, lr = 0.000536
Epoch 3, Class Loss=0.2625176012516022, Reg Loss=0.14013008773326874
Clinet index 4, End of Epoch 3/6, Average Loss=0.4026476740837097, Class Loss=0.2625176012516022, Reg Loss=0.14013008773326874
Pseudo labeling is: None
Epoch 4, lr = 0.000414
Epoch 4, Class Loss=0.2625871002674103, Reg Loss=0.13666990399360657
Clinet index 4, End of Epoch 4/6, Average Loss=0.39925700426101685, Class Loss=0.2625871002674103, Reg Loss=0.13666990399360657
Pseudo labeling is: None
Epoch 5, lr = 0.000287
Epoch 5, Class Loss=0.27257633209228516, Reg Loss=0.13612471520900726
Clinet index 4, End of Epoch 5/6, Average Loss=0.4087010622024536, Class Loss=0.27257633209228516, Reg Loss=0.13612471520900726
Pseudo labeling is: None
Epoch 6, lr = 0.000154
Epoch 6, Class Loss=0.25912514328956604, Reg Loss=0.1314258575439453
Clinet index 4, End of Epoch 6/6, Average Loss=0.39055100083351135, Class Loss=0.25912514328956604, Reg Loss=0.1314258575439453
federated aggregation...
Validation, Class Loss=0.36099669337272644, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.890905
Mean Acc: 0.897204
FreqW Acc: 0.828174
Mean IoU: 0.676779
Class IoU:
	class 0: 0.8626901
	class 1: 0.690036
	class 2: 0.34234
	class 3: 0.80923986
	class 4: 0.5945982
	class 5: 0.7155488
	class 6: 0.9040734
	class 7: 0.88306266
	class 8: 0.8272034
	class 9: 0.4375192
	class 10: 0.6892146
	class 11: 0.5837352
	class 12: 0.76809835
	class 13: 0.7068201
	class 14: 0.7969548
	class 15: 0.80541986
	class 16: 0.08869073
Class Acc:
	class 0: 0.872917
	class 1: 0.9944842
	class 2: 0.9644203
	class 3: 0.9701735
	class 4: 0.92359143
	class 5: 0.9649648
	class 6: 0.9921521
	class 7: 0.9641231
	class 8: 0.9828396
	class 9: 0.7805023
	class 10: 0.8257895
	class 11: 0.78626233
	class 12: 0.9876839
	class 13: 0.9368197
	class 14: 0.97912204
	class 15: 0.9558833
	class 16: 0.3707313

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[14, 6, 0, 16]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1639652252197266, Reg Loss=0.6483625173568726
Clinet index 14, End of Epoch 1/6, Average Loss=1.8123277425765991, Class Loss=1.1639652252197266, Reg Loss=0.6483625173568726
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=1.0467230081558228, Reg Loss=0.6073391437530518
Clinet index 14, End of Epoch 2/6, Average Loss=1.6540621519088745, Class Loss=1.0467230081558228, Reg Loss=0.6073391437530518
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.945144534111023, Reg Loss=0.59654700756073
Clinet index 14, End of Epoch 3/6, Average Loss=1.541691541671753, Class Loss=0.945144534111023, Reg Loss=0.59654700756073
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.7961923480033875, Reg Loss=0.5421780347824097
Clinet index 14, End of Epoch 4/6, Average Loss=1.3383703231811523, Class Loss=0.7961923480033875, Reg Loss=0.5421780347824097
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.728594183921814, Reg Loss=0.6324930787086487
Clinet index 14, End of Epoch 5/6, Average Loss=1.3610873222351074, Class Loss=0.728594183921814, Reg Loss=0.6324930787086487
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6565468311309814, Reg Loss=0.5400993227958679
Clinet index 14, End of Epoch 6/6, Average Loss=1.1966462135314941, Class Loss=0.6565468311309814, Reg Loss=0.5400993227958679
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.139657974243164, Reg Loss=0.6965804100036621
Clinet index 6, End of Epoch 1/6, Average Loss=1.8362383842468262, Class Loss=1.139657974243164, Reg Loss=0.6965804100036621
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=1.0482182502746582, Reg Loss=0.6084001064300537
Clinet index 6, End of Epoch 2/6, Average Loss=1.656618356704712, Class Loss=1.0482182502746582, Reg Loss=0.6084001064300537
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.9068657159805298, Reg Loss=0.5815907716751099
Clinet index 6, End of Epoch 3/6, Average Loss=1.4884564876556396, Class Loss=0.9068657159805298, Reg Loss=0.5815907716751099
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.7913456559181213, Reg Loss=0.6226800680160522
Clinet index 6, End of Epoch 4/6, Average Loss=1.4140257835388184, Class Loss=0.7913456559181213, Reg Loss=0.6226800680160522
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.7024998068809509, Reg Loss=0.5722312927246094
Clinet index 6, End of Epoch 5/6, Average Loss=1.274731159210205, Class Loss=0.7024998068809509, Reg Loss=0.5722312927246094
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6542332172393799, Reg Loss=0.5212677717208862
Clinet index 6, End of Epoch 6/6, Average Loss=1.1755009889602661, Class Loss=0.6542332172393799, Reg Loss=0.5212677717208862
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2208164930343628, Reg Loss=0.6787640452384949
Clinet index 0, End of Epoch 1/6, Average Loss=1.899580478668213, Class Loss=1.2208164930343628, Reg Loss=0.6787640452384949
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=1.051423192024231, Reg Loss=0.5935706496238708
Clinet index 0, End of Epoch 2/6, Average Loss=1.644993782043457, Class Loss=1.051423192024231, Reg Loss=0.5935706496238708
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.8907992243766785, Reg Loss=0.5506827235221863
Clinet index 0, End of Epoch 3/6, Average Loss=1.4414819478988647, Class Loss=0.8907992243766785, Reg Loss=0.5506827235221863
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.7853492498397827, Reg Loss=0.5610654950141907
Clinet index 0, End of Epoch 4/6, Average Loss=1.3464148044586182, Class Loss=0.7853492498397827, Reg Loss=0.5610654950141907
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6824182271957397, Reg Loss=0.5345649719238281
Clinet index 0, End of Epoch 5/6, Average Loss=1.2169831991195679, Class Loss=0.6824182271957397, Reg Loss=0.5345649719238281
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6611125469207764, Reg Loss=0.5730185508728027
Clinet index 0, End of Epoch 6/6, Average Loss=1.234131097793579, Class Loss=0.6611125469207764, Reg Loss=0.5730185508728027
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1311683654785156, Reg Loss=0.617064893245697
Clinet index 16, End of Epoch 1/6, Average Loss=1.7482333183288574, Class Loss=1.1311683654785156, Reg Loss=0.617064893245697
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.924920916557312, Reg Loss=0.5023316144943237
Clinet index 16, End of Epoch 2/6, Average Loss=1.4272525310516357, Class Loss=0.924920916557312, Reg Loss=0.5023316144943237
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.9025932550430298, Reg Loss=0.5060912370681763
Clinet index 16, End of Epoch 3/6, Average Loss=1.408684492111206, Class Loss=0.9025932550430298, Reg Loss=0.5060912370681763
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.7623818516731262, Reg Loss=0.5324671268463135
Clinet index 16, End of Epoch 4/6, Average Loss=1.294848918914795, Class Loss=0.7623818516731262, Reg Loss=0.5324671268463135
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.7108370065689087, Reg Loss=0.5482891201972961
Clinet index 16, End of Epoch 5/6, Average Loss=1.2591261863708496, Class Loss=0.7108370065689087, Reg Loss=0.5482891201972961
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6376359462738037, Reg Loss=0.4799036979675293
Clinet index 16, End of Epoch 6/6, Average Loss=1.117539644241333, Class Loss=0.6376359462738037, Reg Loss=0.4799036979675293
federated aggregation...
Validation, Class Loss=0.7382532358169556, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.801009
Mean Acc: 0.866138
FreqW Acc: 0.724267
Mean IoU: 0.593915
Class IoU:
	class 0: 0.74912804
	class 1: 0.54168797
	class 2: 0.31461495
	class 3: 0.82336587
	class 4: 0.32152992
	class 5: 0.6978308
	class 6: 0.8329649
	class 7: 0.77236396
	class 8: 0.87114537
	class 9: 0.35271835
	class 10: 0.6264196
	class 11: 0.54288733
	class 12: 0.8041306
	class 13: 0.6702043
	class 14: 0.731798
	class 15: 0.64417183
	class 16: 0.058189627
	class 17: 0.3353199
Class Acc:
	class 0: 0.75534207
	class 1: 0.9884954
	class 2: 0.94609797
	class 3: 0.8752351
	class 4: 0.9171804
	class 5: 0.93439114
	class 6: 0.98733866
	class 7: 0.96621495
	class 8: 0.9676845
	class 9: 0.7373219
	class 10: 0.7655216
	class 11: 0.74205005
	class 12: 0.9595935
	class 13: 0.8926035
	class 14: 0.93487674
	class 15: 0.9686326
	class 16: 0.70176184
	class 17: 0.550149

federated global round: 11, step: 2
select part of clients to conduct local training
[0, 12, 10, 17]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.6468684077262878, Reg Loss=0.5388924479484558
Clinet index 0, End of Epoch 1/6, Average Loss=1.1857608556747437, Class Loss=0.6468684077262878, Reg Loss=0.5388924479484558
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Class Loss=0.6162876486778259, Reg Loss=0.5438873171806335
Clinet index 0, End of Epoch 2/6, Average Loss=1.1601749658584595, Class Loss=0.6162876486778259, Reg Loss=0.5438873171806335
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.6001437306404114, Reg Loss=0.5071914196014404
Clinet index 0, End of Epoch 3/6, Average Loss=1.107335090637207, Class Loss=0.6001437306404114, Reg Loss=0.5071914196014404
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Class Loss=0.5966126322746277, Reg Loss=0.5111106634140015
Clinet index 0, End of Epoch 4/6, Average Loss=1.1077232360839844, Class Loss=0.5966126322746277, Reg Loss=0.5111106634140015
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5283543467521667, Reg Loss=0.48564761877059937
Clinet index 0, End of Epoch 5/6, Average Loss=1.0140019655227661, Class Loss=0.5283543467521667, Reg Loss=0.48564761877059937
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Class Loss=0.5391886234283447, Reg Loss=0.5226664543151855
Clinet index 0, End of Epoch 6/6, Average Loss=1.0618550777435303, Class Loss=0.5391886234283447, Reg Loss=0.5226664543151855
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.6151330471038818, Reg Loss=0.5993776321411133
Clinet index 12, End of Epoch 1/6, Average Loss=1.2145106792449951, Class Loss=0.6151330471038818, Reg Loss=0.5993776321411133
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.6297609210014343, Reg Loss=0.6115434765815735
Clinet index 12, End of Epoch 2/6, Average Loss=1.2413043975830078, Class Loss=0.6297609210014343, Reg Loss=0.6115434765815735
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.6301997900009155, Reg Loss=0.5574454665184021
Clinet index 12, End of Epoch 3/6, Average Loss=1.1876451969146729, Class Loss=0.6301997900009155, Reg Loss=0.5574454665184021
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.547569990158081, Reg Loss=0.5244851112365723
Clinet index 12, End of Epoch 4/6, Average Loss=1.0720551013946533, Class Loss=0.547569990158081, Reg Loss=0.5244851112365723
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.4898722767829895, Reg Loss=0.5193271636962891
Clinet index 12, End of Epoch 5/6, Average Loss=1.0091993808746338, Class Loss=0.4898722767829895, Reg Loss=0.5193271636962891
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.4876565635204315, Reg Loss=0.5643134713172913
Clinet index 12, End of Epoch 6/6, Average Loss=1.0519700050354004, Class Loss=0.4876565635204315, Reg Loss=0.5643134713172913
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.6422930359840393, Reg Loss=0.5492960214614868
Clinet index 10, End of Epoch 1/6, Average Loss=1.191589117050171, Class Loss=0.6422930359840393, Reg Loss=0.5492960214614868
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.6433275938034058, Reg Loss=0.5619838237762451
Clinet index 10, End of Epoch 2/6, Average Loss=1.2053114175796509, Class Loss=0.6433275938034058, Reg Loss=0.5619838237762451
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.6050689220428467, Reg Loss=0.5085204839706421
Clinet index 10, End of Epoch 3/6, Average Loss=1.1135894060134888, Class Loss=0.6050689220428467, Reg Loss=0.5085204839706421
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.5830899477005005, Reg Loss=0.5121303200721741
Clinet index 10, End of Epoch 4/6, Average Loss=1.0952203273773193, Class Loss=0.5830899477005005, Reg Loss=0.5121303200721741
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.5363376140594482, Reg Loss=0.48795604705810547
Clinet index 10, End of Epoch 5/6, Average Loss=1.0242936611175537, Class Loss=0.5363376140594482, Reg Loss=0.48795604705810547
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.501803457736969, Reg Loss=0.5521770715713501
Clinet index 10, End of Epoch 6/6, Average Loss=1.0539805889129639, Class Loss=0.501803457736969, Reg Loss=0.5521770715713501
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.5711501240730286, Reg Loss=0.5155571699142456
Clinet index 17, End of Epoch 1/6, Average Loss=1.086707353591919, Class Loss=0.5711501240730286, Reg Loss=0.5155571699142456
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.6105304956436157, Reg Loss=0.5214638710021973
Clinet index 17, End of Epoch 2/6, Average Loss=1.131994366645813, Class Loss=0.6105304956436157, Reg Loss=0.5214638710021973
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.610276997089386, Reg Loss=0.48705559968948364
Clinet index 17, End of Epoch 3/6, Average Loss=1.0973325967788696, Class Loss=0.610276997089386, Reg Loss=0.48705559968948364
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.5634928345680237, Reg Loss=0.5108240246772766
Clinet index 17, End of Epoch 4/6, Average Loss=1.0743168592453003, Class Loss=0.5634928345680237, Reg Loss=0.5108240246772766
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.5067142248153687, Reg Loss=0.5019319653511047
Clinet index 17, End of Epoch 5/6, Average Loss=1.0086462497711182, Class Loss=0.5067142248153687, Reg Loss=0.5019319653511047
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.4778309762477875, Reg Loss=0.5012799501419067
Clinet index 17, End of Epoch 6/6, Average Loss=0.9791109561920166, Class Loss=0.4778309762477875, Reg Loss=0.5012799501419067
federated aggregation...
Validation, Class Loss=0.7032467722892761, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.800352
Mean Acc: 0.868790
FreqW Acc: 0.723416
Mean IoU: 0.585649
Class IoU:
	class 0: 0.7492428
	class 1: 0.5524784
	class 2: 0.31540895
	class 3: 0.8212484
	class 4: 0.29053503
	class 5: 0.6687417
	class 6: 0.82139874
	class 7: 0.7562098
	class 8: 0.8481145
	class 9: 0.34685686
	class 10: 0.61346674
	class 11: 0.53787667
	class 12: 0.790494
	class 13: 0.68496853
	class 14: 0.7423588
	class 15: 0.66996545
	class 16: 0.057752106
	class 17: 0.2745731
Class Acc:
	class 0: 0.75427073
	class 1: 0.96988976
	class 2: 0.95898324
	class 3: 0.8889499
	class 4: 0.9449332
	class 5: 0.9529464
	class 6: 0.99158245
	class 7: 0.96899474
	class 8: 0.9604134
	class 9: 0.8104546
	class 10: 0.67739123
	class 11: 0.789334
	class 12: 0.930623
	class 13: 0.9156743
	class 14: 0.9363515
	class 15: 0.97579527
	class 16: 0.64594686
	class 17: 0.5656837

federated global round: 12, step: 2
select part of clients to conduct local training
[14, 8, 16, 3]
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.48833370208740234, Reg Loss=0.5001562833786011
Clinet index 14, End of Epoch 1/6, Average Loss=0.9884899854660034, Class Loss=0.48833370208740234, Reg Loss=0.5001562833786011
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Class Loss=0.5135724544525146, Reg Loss=0.5183352828025818
Clinet index 14, End of Epoch 2/6, Average Loss=1.0319077968597412, Class Loss=0.5135724544525146, Reg Loss=0.5183352828025818
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Class Loss=0.48638054728507996, Reg Loss=0.5274104475975037
Clinet index 14, End of Epoch 3/6, Average Loss=1.0137909650802612, Class Loss=0.48638054728507996, Reg Loss=0.5274104475975037
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.4752783179283142, Reg Loss=0.5023616552352905
Clinet index 14, End of Epoch 4/6, Average Loss=0.9776399731636047, Class Loss=0.4752783179283142, Reg Loss=0.5023616552352905
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Class Loss=0.45818883180618286, Reg Loss=0.5366080403327942
Clinet index 14, End of Epoch 5/6, Average Loss=0.994796872138977, Class Loss=0.45818883180618286, Reg Loss=0.5366080403327942
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Class Loss=0.42749762535095215, Reg Loss=0.52672278881073
Clinet index 14, End of Epoch 6/6, Average Loss=0.9542204141616821, Class Loss=0.42749762535095215, Reg Loss=0.52672278881073
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.4489133656024933, Reg Loss=0.49768757820129395
Clinet index 8, End of Epoch 1/6, Average Loss=0.9466009140014648, Class Loss=0.4489133656024933, Reg Loss=0.49768757820129395
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.42543482780456543, Reg Loss=0.4752127528190613
Clinet index 8, End of Epoch 2/6, Average Loss=0.9006475806236267, Class Loss=0.42543482780456543, Reg Loss=0.4752127528190613
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.45422738790512085, Reg Loss=0.475238561630249
Clinet index 8, End of Epoch 3/6, Average Loss=0.9294659495353699, Class Loss=0.45422738790512085, Reg Loss=0.475238561630249
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.4199775457382202, Reg Loss=0.4770902693271637
Clinet index 8, End of Epoch 4/6, Average Loss=0.8970677852630615, Class Loss=0.4199775457382202, Reg Loss=0.4770902693271637
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.38654419779777527, Reg Loss=0.4776797890663147
Clinet index 8, End of Epoch 5/6, Average Loss=0.8642239570617676, Class Loss=0.38654419779777527, Reg Loss=0.4776797890663147
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.3965657949447632, Reg Loss=0.4661877155303955
Clinet index 8, End of Epoch 6/6, Average Loss=0.8627535104751587, Class Loss=0.3965657949447632, Reg Loss=0.4661877155303955
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.44069570302963257, Reg Loss=0.48166584968566895
Clinet index 16, End of Epoch 1/6, Average Loss=0.9223615527153015, Class Loss=0.44069570302963257, Reg Loss=0.48166584968566895
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Class Loss=0.417378306388855, Reg Loss=0.46082794666290283
Clinet index 16, End of Epoch 2/6, Average Loss=0.8782062530517578, Class Loss=0.417378306388855, Reg Loss=0.46082794666290283
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Class Loss=0.45003849267959595, Reg Loss=0.4905250668525696
Clinet index 16, End of Epoch 3/6, Average Loss=0.9405635595321655, Class Loss=0.45003849267959595, Reg Loss=0.4905250668525696
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Class Loss=0.44205403327941895, Reg Loss=0.4644915759563446
Clinet index 16, End of Epoch 4/6, Average Loss=0.9065456390380859, Class Loss=0.44205403327941895, Reg Loss=0.4644915759563446
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Class Loss=0.43131333589553833, Reg Loss=0.4983588755130768
Clinet index 16, End of Epoch 5/6, Average Loss=0.9296722412109375, Class Loss=0.43131333589553833, Reg Loss=0.4983588755130768
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Class Loss=0.40500110387802124, Reg Loss=0.4601381719112396
Clinet index 16, End of Epoch 6/6, Average Loss=0.8651392459869385, Class Loss=0.40500110387802124, Reg Loss=0.4601381719112396
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.4420885145664215, Reg Loss=0.4640107750892639
Clinet index 3, End of Epoch 1/6, Average Loss=0.9060993194580078, Class Loss=0.4420885145664215, Reg Loss=0.4640107750892639
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.45219436287879944, Reg Loss=0.4637414813041687
Clinet index 3, End of Epoch 2/6, Average Loss=0.9159358739852905, Class Loss=0.45219436287879944, Reg Loss=0.4637414813041687
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.4748166799545288, Reg Loss=0.4940263628959656
Clinet index 3, End of Epoch 3/6, Average Loss=0.9688430428504944, Class Loss=0.4748166799545288, Reg Loss=0.4940263628959656
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.4221916198730469, Reg Loss=0.47182124853134155
Clinet index 3, End of Epoch 4/6, Average Loss=0.8940128684043884, Class Loss=0.4221916198730469, Reg Loss=0.47182124853134155
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.395063579082489, Reg Loss=0.45235615968704224
Clinet index 3, End of Epoch 5/6, Average Loss=0.8474197387695312, Class Loss=0.395063579082489, Reg Loss=0.45235615968704224
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.39823484420776367, Reg Loss=0.4839867055416107
Clinet index 3, End of Epoch 6/6, Average Loss=0.8822215795516968, Class Loss=0.39823484420776367, Reg Loss=0.4839867055416107
federated aggregation...
Validation, Class Loss=0.6446787714958191, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.815203
Mean Acc: 0.878204
FreqW Acc: 0.742355
Mean IoU: 0.601528
Class IoU:
	class 0: 0.7673685
	class 1: 0.5916598
	class 2: 0.32665834
	class 3: 0.82631624
	class 4: 0.326336
	class 5: 0.67489505
	class 6: 0.8296876
	class 7: 0.7823078
	class 8: 0.8740778
	class 9: 0.34292313
	class 10: 0.6364033
	class 11: 0.5364018
	class 12: 0.8078844
	class 13: 0.6883746
	class 14: 0.73741895
	class 15: 0.7104278
	class 16: 0.05996245
	class 17: 0.30840343
Class Acc:
	class 0: 0.7727596
	class 1: 0.98051876
	class 2: 0.9604547
	class 3: 0.8899496
	class 4: 0.929623
	class 5: 0.95814514
	class 6: 0.9922541
	class 7: 0.96681726
	class 8: 0.9604177
	class 9: 0.8282242
	class 10: 0.7041118
	class 11: 0.7929513
	class 12: 0.94107074
	class 13: 0.9130792
	class 14: 0.95245993
	class 15: 0.97286177
	class 16: 0.63255334
	class 17: 0.65942603

federated global round: 13, step: 2
select part of clients to conduct local training
[17, 13, 11, 7]
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.3625948429107666, Reg Loss=0.4794839024543762
Clinet index 17, End of Epoch 1/6, Average Loss=0.8420787453651428, Class Loss=0.3625948429107666, Reg Loss=0.4794839024543762
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Epoch 2, Class Loss=0.40572208166122437, Reg Loss=0.4960581362247467
Clinet index 17, End of Epoch 2/6, Average Loss=0.9017802476882935, Class Loss=0.40572208166122437, Reg Loss=0.4960581362247467
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Class Loss=0.42402201890945435, Reg Loss=0.46730637550354004
Clinet index 17, End of Epoch 3/6, Average Loss=0.8913283944129944, Class Loss=0.42402201890945435, Reg Loss=0.46730637550354004
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.3940633535385132, Reg Loss=0.4679165184497833
Clinet index 17, End of Epoch 4/6, Average Loss=0.8619798421859741, Class Loss=0.3940633535385132, Reg Loss=0.4679165184497833
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Class Loss=0.35061490535736084, Reg Loss=0.5077425241470337
Clinet index 17, End of Epoch 5/6, Average Loss=0.8583574295043945, Class Loss=0.35061490535736084, Reg Loss=0.5077425241470337
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Class Loss=0.3519565165042877, Reg Loss=0.47322630882263184
Clinet index 17, End of Epoch 6/6, Average Loss=0.8251827955245972, Class Loss=0.3519565165042877, Reg Loss=0.47322630882263184
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3661459684371948, Reg Loss=0.44912490248680115
Clinet index 13, End of Epoch 1/6, Average Loss=0.8152709007263184, Class Loss=0.3661459684371948, Reg Loss=0.44912490248680115
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.3741433024406433, Reg Loss=0.452572226524353
Clinet index 13, End of Epoch 2/6, Average Loss=0.8267155289649963, Class Loss=0.3741433024406433, Reg Loss=0.452572226524353
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.36960408091545105, Reg Loss=0.4511287808418274
Clinet index 13, End of Epoch 3/6, Average Loss=0.820732831954956, Class Loss=0.36960408091545105, Reg Loss=0.4511287808418274
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.3602161705493927, Reg Loss=0.454671710729599
Clinet index 13, End of Epoch 4/6, Average Loss=0.8148878812789917, Class Loss=0.3602161705493927, Reg Loss=0.454671710729599
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.3517460823059082, Reg Loss=0.4408109486103058
Clinet index 13, End of Epoch 5/6, Average Loss=0.7925570011138916, Class Loss=0.3517460823059082, Reg Loss=0.4408109486103058
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Class Loss=0.34214216470718384, Reg Loss=0.4796530604362488
Clinet index 13, End of Epoch 6/6, Average Loss=0.8217952251434326, Class Loss=0.34214216470718384, Reg Loss=0.4796530604362488
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3652685880661011, Reg Loss=0.4544159770011902
Clinet index 11, End of Epoch 1/6, Average Loss=0.8196845650672913, Class Loss=0.3652685880661011, Reg Loss=0.4544159770011902
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.3912160396575928, Reg Loss=0.4802095293998718
Clinet index 11, End of Epoch 2/6, Average Loss=0.8714255690574646, Class Loss=0.3912160396575928, Reg Loss=0.4802095293998718
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.36766237020492554, Reg Loss=0.47555869817733765
Clinet index 11, End of Epoch 3/6, Average Loss=0.8432210683822632, Class Loss=0.36766237020492554, Reg Loss=0.47555869817733765
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.34811171889305115, Reg Loss=0.4627833962440491
Clinet index 11, End of Epoch 4/6, Average Loss=0.8108950853347778, Class Loss=0.34811171889305115, Reg Loss=0.4627833962440491
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.32934796810150146, Reg Loss=0.49177002906799316
Clinet index 11, End of Epoch 5/6, Average Loss=0.8211179971694946, Class Loss=0.32934796810150146, Reg Loss=0.49177002906799316
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.3373497724533081, Reg Loss=0.4931570887565613
Clinet index 11, End of Epoch 6/6, Average Loss=0.8305068612098694, Class Loss=0.3373497724533081, Reg Loss=0.4931570887565613
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.4291118383407593, Reg Loss=0.49441954493522644
Clinet index 7, End of Epoch 1/6, Average Loss=0.9235314130783081, Class Loss=0.4291118383407593, Reg Loss=0.49441954493522644
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.406934529542923, Reg Loss=0.4868152439594269
Clinet index 7, End of Epoch 2/6, Average Loss=0.8937497735023499, Class Loss=0.406934529542923, Reg Loss=0.4868152439594269
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.3682289123535156, Reg Loss=0.4767368733882904
Clinet index 7, End of Epoch 3/6, Average Loss=0.8449658155441284, Class Loss=0.3682289123535156, Reg Loss=0.4767368733882904
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.3747175931930542, Reg Loss=0.48840898275375366
Clinet index 7, End of Epoch 4/6, Average Loss=0.8631265759468079, Class Loss=0.3747175931930542, Reg Loss=0.48840898275375366
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.3847367465496063, Reg Loss=0.48490840196609497
Clinet index 7, End of Epoch 5/6, Average Loss=0.8696451187133789, Class Loss=0.3847367465496063, Reg Loss=0.48490840196609497
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.3716558516025543, Reg Loss=0.490332692861557
Clinet index 7, End of Epoch 6/6, Average Loss=0.8619885444641113, Class Loss=0.3716558516025543, Reg Loss=0.490332692861557
federated aggregation...
Validation, Class Loss=0.600753128528595, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.828466
Mean Acc: 0.879961
FreqW Acc: 0.757439
Mean IoU: 0.611078
Class IoU:
	class 0: 0.7854286
	class 1: 0.6117308
	class 2: 0.33476424
	class 3: 0.8306813
	class 4: 0.33259696
	class 5: 0.666922
	class 6: 0.8405245
	class 7: 0.79596967
	class 8: 0.88136065
	class 9: 0.35113287
	class 10: 0.6414162
	class 11: 0.5363325
	class 12: 0.8149751
	class 13: 0.70860183
	class 14: 0.7415306
	class 15: 0.71392316
	class 16: 0.06435639
	class 17: 0.34714985
Class Acc:
	class 0: 0.79121196
	class 1: 0.9795725
	class 2: 0.95772755
	class 3: 0.90052474
	class 4: 0.9414094
	class 5: 0.96091396
	class 6: 0.9917456
	class 7: 0.96713537
	class 8: 0.963286
	class 9: 0.8193408
	class 10: 0.71350837
	class 11: 0.7867365
	class 12: 0.93824416
	class 13: 0.9135053
	class 14: 0.9500367
	class 15: 0.97274613
	class 16: 0.6010092
	class 17: 0.69065315

federated global round: 14, step: 2
select part of clients to conduct local training
[17, 4, 5, 2]
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000414
Epoch 1, Class Loss=0.312832772731781, Reg Loss=0.4612019956111908
Clinet index 17, End of Epoch 1/6, Average Loss=0.7740347385406494, Class Loss=0.312832772731781, Reg Loss=0.4612019956111908
Pseudo labeling is: None
Epoch 2, lr = 0.000351
Epoch 2, Class Loss=0.3659557104110718, Reg Loss=0.4765940308570862
Clinet index 17, End of Epoch 2/6, Average Loss=0.842549741268158, Class Loss=0.3659557104110718, Reg Loss=0.4765940308570862
Pseudo labeling is: None
Epoch 3, lr = 0.000287
Epoch 3, Class Loss=0.3557382822036743, Reg Loss=0.47640639543533325
Clinet index 17, End of Epoch 3/6, Average Loss=0.8321446776390076, Class Loss=0.3557382822036743, Reg Loss=0.47640639543533325
Pseudo labeling is: None
Epoch 4, lr = 0.000222
Epoch 4, Class Loss=0.3484478294849396, Reg Loss=0.452815979719162
Clinet index 17, End of Epoch 4/6, Average Loss=0.8012638092041016, Class Loss=0.3484478294849396, Reg Loss=0.452815979719162
Pseudo labeling is: None
Epoch 5, lr = 0.000154
Epoch 5, Class Loss=0.3498174250125885, Reg Loss=0.5003830194473267
Clinet index 17, End of Epoch 5/6, Average Loss=0.8502004146575928, Class Loss=0.3498174250125885, Reg Loss=0.5003830194473267
Pseudo labeling is: None
Epoch 6, lr = 0.000082
Epoch 6, Class Loss=0.3301931917667389, Reg Loss=0.4697290062904358
Clinet index 17, End of Epoch 6/6, Average Loss=0.7999222278594971, Class Loss=0.3301931917667389, Reg Loss=0.4697290062904358
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.2991940379142761, Reg Loss=0.44527459144592285
Clinet index 4, End of Epoch 1/6, Average Loss=0.744468629360199, Class Loss=0.2991940379142761, Reg Loss=0.44527459144592285
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.32678771018981934, Reg Loss=0.4638863503932953
Clinet index 4, End of Epoch 2/6, Average Loss=0.790674090385437, Class Loss=0.32678771018981934, Reg Loss=0.4638863503932953
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.32046616077423096, Reg Loss=0.46720337867736816
Clinet index 4, End of Epoch 3/6, Average Loss=0.7876695394515991, Class Loss=0.32046616077423096, Reg Loss=0.46720337867736816
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.31651240587234497, Reg Loss=0.44022318720817566
Clinet index 4, End of Epoch 4/6, Average Loss=0.7567355632781982, Class Loss=0.31651240587234497, Reg Loss=0.44022318720817566
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.32999342679977417, Reg Loss=0.4669203758239746
Clinet index 4, End of Epoch 5/6, Average Loss=0.7969138026237488, Class Loss=0.32999342679977417, Reg Loss=0.4669203758239746
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.31215667724609375, Reg Loss=0.4538649916648865
Clinet index 4, End of Epoch 6/6, Average Loss=0.7660216689109802, Class Loss=0.31215667724609375, Reg Loss=0.4538649916648865
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3409305512905121, Reg Loss=0.48792409896850586
Clinet index 5, End of Epoch 1/6, Average Loss=0.8288546800613403, Class Loss=0.3409305512905121, Reg Loss=0.48792409896850586
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.3579253554344177, Reg Loss=0.492610365152359
Clinet index 5, End of Epoch 2/6, Average Loss=0.8505357503890991, Class Loss=0.3579253554344177, Reg Loss=0.492610365152359
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.36019885540008545, Reg Loss=0.49307000637054443
Clinet index 5, End of Epoch 3/6, Average Loss=0.8532688617706299, Class Loss=0.36019885540008545, Reg Loss=0.49307000637054443
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.31954526901245117, Reg Loss=0.4897841811180115
Clinet index 5, End of Epoch 4/6, Average Loss=0.8093294501304626, Class Loss=0.31954526901245117, Reg Loss=0.4897841811180115
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.33536651730537415, Reg Loss=0.48614490032196045
Clinet index 5, End of Epoch 5/6, Average Loss=0.8215113878250122, Class Loss=0.33536651730537415, Reg Loss=0.48614490032196045
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.33038756251335144, Reg Loss=0.46124494075775146
Clinet index 5, End of Epoch 6/6, Average Loss=0.7916325330734253, Class Loss=0.33038756251335144, Reg Loss=0.46124494075775146
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3500683903694153, Reg Loss=0.505050778388977
Clinet index 2, End of Epoch 1/6, Average Loss=0.8551191687583923, Class Loss=0.3500683903694153, Reg Loss=0.505050778388977
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.319471538066864, Reg Loss=0.4613342881202698
Clinet index 2, End of Epoch 2/6, Average Loss=0.7808058261871338, Class Loss=0.319471538066864, Reg Loss=0.4613342881202698
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.358608216047287, Reg Loss=0.5000194907188416
Clinet index 2, End of Epoch 3/6, Average Loss=0.8586276769638062, Class Loss=0.358608216047287, Reg Loss=0.5000194907188416
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.3419801592826843, Reg Loss=0.49562788009643555
Clinet index 2, End of Epoch 4/6, Average Loss=0.8376080393791199, Class Loss=0.3419801592826843, Reg Loss=0.49562788009643555
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.3300163745880127, Reg Loss=0.49083995819091797
Clinet index 2, End of Epoch 5/6, Average Loss=0.8208563327789307, Class Loss=0.3300163745880127, Reg Loss=0.49083995819091797
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.3253992199897766, Reg Loss=0.47866392135620117
Clinet index 2, End of Epoch 6/6, Average Loss=0.8040631413459778, Class Loss=0.3253992199897766, Reg Loss=0.47866392135620117
federated aggregation...
Validation, Class Loss=0.5717201232910156, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.839291
Mean Acc: 0.879421
FreqW Acc: 0.771308
Mean IoU: 0.622084
Class IoU:
	class 0: 0.80057865
	class 1: 0.64287436
	class 2: 0.33939788
	class 3: 0.83772236
	class 4: 0.36022037
	class 5: 0.67464393
	class 6: 0.8488259
	class 7: 0.8101854
	class 8: 0.8854571
	class 9: 0.36229125
	class 10: 0.644294
	class 11: 0.54536486
	class 12: 0.8132084
	class 13: 0.7122668
	class 14: 0.7617398
	class 15: 0.7314995
	class 16: 0.06657354
	class 17: 0.36036447
Class Acc:
	class 0: 0.8070324
	class 1: 0.98159564
	class 2: 0.95782214
	class 3: 0.90435547
	class 4: 0.9316716
	class 5: 0.9598107
	class 6: 0.99235827
	class 7: 0.9652528
	class 8: 0.9600418
	class 9: 0.8097876
	class 10: 0.7055689
	class 11: 0.77981395
	class 12: 0.93969864
	class 13: 0.9072722
	class 14: 0.9514924
	class 15: 0.97305787
	class 16: 0.59430164
	class 17: 0.7086521

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[17, 21, 10, 5]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.7939477562904358, Reg Loss=0.676029622554779
Clinet index 17, End of Epoch 1/6, Average Loss=1.4699773788452148, Class Loss=0.7939477562904358, Reg Loss=0.676029622554779
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.6794541478157043, Reg Loss=0.5831565856933594
Clinet index 17, End of Epoch 2/6, Average Loss=1.262610673904419, Class Loss=0.6794541478157043, Reg Loss=0.5831565856933594
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6060024499893188, Reg Loss=0.5766432285308838
Clinet index 17, End of Epoch 3/6, Average Loss=1.1826456785202026, Class Loss=0.6060024499893188, Reg Loss=0.5766432285308838
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5712937116622925, Reg Loss=0.5533195734024048
Clinet index 17, End of Epoch 4/6, Average Loss=1.1246132850646973, Class Loss=0.5712937116622925, Reg Loss=0.5533195734024048
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5570541620254517, Reg Loss=0.5494775176048279
Clinet index 17, End of Epoch 5/6, Average Loss=1.1065316200256348, Class Loss=0.5570541620254517, Reg Loss=0.5494775176048279
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.5205737948417664, Reg Loss=0.5407105684280396
Clinet index 17, End of Epoch 6/6, Average Loss=1.0612843036651611, Class Loss=0.5205737948417664, Reg Loss=0.5407105684280396
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.7715476155281067, Reg Loss=0.6669399738311768
Clinet index 21, End of Epoch 1/6, Average Loss=1.4384875297546387, Class Loss=0.7715476155281067, Reg Loss=0.6669399738311768
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.6723145842552185, Reg Loss=0.5882222056388855
Clinet index 21, End of Epoch 2/6, Average Loss=1.260536789894104, Class Loss=0.6723145842552185, Reg Loss=0.5882222056388855
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6131914258003235, Reg Loss=0.5628867745399475
Clinet index 21, End of Epoch 3/6, Average Loss=1.176078200340271, Class Loss=0.6131914258003235, Reg Loss=0.5628867745399475
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5746338367462158, Reg Loss=0.5665757656097412
Clinet index 21, End of Epoch 4/6, Average Loss=1.141209602355957, Class Loss=0.5746338367462158, Reg Loss=0.5665757656097412
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5271865725517273, Reg Loss=0.5460728406906128
Clinet index 21, End of Epoch 5/6, Average Loss=1.0732593536376953, Class Loss=0.5271865725517273, Reg Loss=0.5460728406906128
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.5206342339515686, Reg Loss=0.5547343492507935
Clinet index 21, End of Epoch 6/6, Average Loss=1.0753686428070068, Class Loss=0.5206342339515686, Reg Loss=0.5547343492507935
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.7805501818656921, Reg Loss=0.6391347050666809
Clinet index 10, End of Epoch 1/6, Average Loss=1.419684886932373, Class Loss=0.7805501818656921, Reg Loss=0.6391347050666809
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.6769493222236633, Reg Loss=0.5707292556762695
Clinet index 10, End of Epoch 2/6, Average Loss=1.247678518295288, Class Loss=0.6769493222236633, Reg Loss=0.5707292556762695
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.5820977091789246, Reg Loss=0.5596939325332642
Clinet index 10, End of Epoch 3/6, Average Loss=1.141791582107544, Class Loss=0.5820977091789246, Reg Loss=0.5596939325332642
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5565172433853149, Reg Loss=0.5428541302680969
Clinet index 10, End of Epoch 4/6, Average Loss=1.0993714332580566, Class Loss=0.5565172433853149, Reg Loss=0.5428541302680969
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5336437225341797, Reg Loss=0.5386244058609009
Clinet index 10, End of Epoch 5/6, Average Loss=1.0722681283950806, Class Loss=0.5336437225341797, Reg Loss=0.5386244058609009
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.5001991987228394, Reg Loss=0.5479878187179565
Clinet index 10, End of Epoch 6/6, Average Loss=1.048187017440796, Class Loss=0.5001991987228394, Reg Loss=0.5479878187179565
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.7368588447570801, Reg Loss=0.6612478494644165
Clinet index 5, End of Epoch 1/6, Average Loss=1.3981066942214966, Class Loss=0.7368588447570801, Reg Loss=0.6612478494644165
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.6626484394073486, Reg Loss=0.5755098462104797
Clinet index 5, End of Epoch 2/6, Average Loss=1.2381582260131836, Class Loss=0.6626484394073486, Reg Loss=0.5755098462104797
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6133307218551636, Reg Loss=0.552056610584259
Clinet index 5, End of Epoch 3/6, Average Loss=1.1653873920440674, Class Loss=0.6133307218551636, Reg Loss=0.552056610584259
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5380069017410278, Reg Loss=0.5553128719329834
Clinet index 5, End of Epoch 4/6, Average Loss=1.0933197736740112, Class Loss=0.5380069017410278, Reg Loss=0.5553128719329834
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5350728631019592, Reg Loss=0.5438972115516663
Clinet index 5, End of Epoch 5/6, Average Loss=1.0789700746536255, Class Loss=0.5350728631019592, Reg Loss=0.5438972115516663
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.524121105670929, Reg Loss=0.5522404909133911
Clinet index 5, End of Epoch 6/6, Average Loss=1.0763616561889648, Class Loss=0.524121105670929, Reg Loss=0.5522404909133911
federated aggregation...
Validation, Class Loss=1.3384642601013184, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.333722
Mean Acc: 0.830292
FreqW Acc: 0.247395
Mean IoU: 0.490948
Class IoU:
	class 0: 0.08328192
	class 1: 0.47582525
	class 2: 0.23513219
	class 3: 0.7800979
	class 4: 0.31629643
	class 5: 0.6785802
	class 6: 0.721698
	class 7: 0.7860098
	class 8: 0.8300987
	class 9: 0.37471202
	class 10: 0.6123283
	class 11: 0.45504907
	class 12: 0.75883925
	class 13: 0.64851034
	class 14: 0.6105807
	class 15: 0.75886315
	class 16: 0.031550523
	class 17: 0.14262548
	class 18: 0.027939213
Class Acc:
	class 0: 0.08329769
	class 1: 0.99150157
	class 2: 0.9803832
	class 3: 0.91237754
	class 4: 0.9453138
	class 5: 0.9469422
	class 6: 0.992506
	class 7: 0.9664949
	class 8: 0.9641088
	class 9: 0.7351724
	class 10: 0.75373536
	class 11: 0.8189046
	class 12: 0.94967866
	class 13: 0.89267725
	class 14: 0.96535486
	class 15: 0.9600522
	class 16: 0.8211303
	class 17: 0.4345321
	class 18: 0.66138464

federated global round: 16, step: 3
select part of clients to conduct local training
[19, 11, 9, 1]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.5102965831756592, Reg Loss=0.5569050908088684
Clinet index 19, End of Epoch 1/6, Average Loss=1.0672016143798828, Class Loss=0.5102965831756592, Reg Loss=0.5569050908088684
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.4898592233657837, Reg Loss=0.5519366264343262
Clinet index 19, End of Epoch 2/6, Average Loss=1.0417958498001099, Class Loss=0.4898592233657837, Reg Loss=0.5519366264343262
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.48001930117607117, Reg Loss=0.5554354786872864
Clinet index 19, End of Epoch 3/6, Average Loss=1.0354547500610352, Class Loss=0.48001930117607117, Reg Loss=0.5554354786872864
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.462786465883255, Reg Loss=0.5504741668701172
Clinet index 19, End of Epoch 4/6, Average Loss=1.0132606029510498, Class Loss=0.462786465883255, Reg Loss=0.5504741668701172
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.46074721217155457, Reg Loss=0.5578399896621704
Clinet index 19, End of Epoch 5/6, Average Loss=1.0185872316360474, Class Loss=0.46074721217155457, Reg Loss=0.5578399896621704
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.4432472586631775, Reg Loss=0.5608816742897034
Clinet index 19, End of Epoch 6/6, Average Loss=1.0041289329528809, Class Loss=0.4432472586631775, Reg Loss=0.5608816742897034
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.5222002863883972, Reg Loss=0.5586930513381958
Clinet index 11, End of Epoch 1/6, Average Loss=1.0808932781219482, Class Loss=0.5222002863883972, Reg Loss=0.5586930513381958
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.5125651359558105, Reg Loss=0.5517038106918335
Clinet index 11, End of Epoch 2/6, Average Loss=1.064268946647644, Class Loss=0.5125651359558105, Reg Loss=0.5517038106918335
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.4953584372997284, Reg Loss=0.5464769601821899
Clinet index 11, End of Epoch 3/6, Average Loss=1.0418354272842407, Class Loss=0.4953584372997284, Reg Loss=0.5464769601821899
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.48248055577278137, Reg Loss=0.5483340620994568
Clinet index 11, End of Epoch 4/6, Average Loss=1.0308146476745605, Class Loss=0.48248055577278137, Reg Loss=0.5483340620994568
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.4702177345752716, Reg Loss=0.5543918609619141
Clinet index 11, End of Epoch 5/6, Average Loss=1.0246095657348633, Class Loss=0.4702177345752716, Reg Loss=0.5543918609619141
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.4516196846961975, Reg Loss=0.5460731983184814
Clinet index 11, End of Epoch 6/6, Average Loss=0.997692883014679, Class Loss=0.4516196846961975, Reg Loss=0.5460731983184814
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.500456690788269, Reg Loss=0.5356056690216064
Clinet index 9, End of Epoch 1/6, Average Loss=1.0360623598098755, Class Loss=0.500456690788269, Reg Loss=0.5356056690216064
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.497525155544281, Reg Loss=0.540706992149353
Clinet index 9, End of Epoch 2/6, Average Loss=1.0382320880889893, Class Loss=0.497525155544281, Reg Loss=0.540706992149353
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.47349733114242554, Reg Loss=0.54367595911026
Clinet index 9, End of Epoch 3/6, Average Loss=1.0171732902526855, Class Loss=0.47349733114242554, Reg Loss=0.54367595911026
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.4657577872276306, Reg Loss=0.5352692008018494
Clinet index 9, End of Epoch 4/6, Average Loss=1.00102698802948, Class Loss=0.4657577872276306, Reg Loss=0.5352692008018494
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.45483776926994324, Reg Loss=0.5457611680030823
Clinet index 9, End of Epoch 5/6, Average Loss=1.0005989074707031, Class Loss=0.45483776926994324, Reg Loss=0.5457611680030823
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.4367329180240631, Reg Loss=0.535155177116394
Clinet index 9, End of Epoch 6/6, Average Loss=0.9718880653381348, Class Loss=0.4367329180240631, Reg Loss=0.535155177116394
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.5280997157096863, Reg Loss=0.5564781427383423
Clinet index 1, End of Epoch 1/6, Average Loss=1.0845777988433838, Class Loss=0.5280997157096863, Reg Loss=0.5564781427383423
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.5154448747634888, Reg Loss=0.5478231906890869
Clinet index 1, End of Epoch 2/6, Average Loss=1.0632680654525757, Class Loss=0.5154448747634888, Reg Loss=0.5478231906890869
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.4988076090812683, Reg Loss=0.5553536415100098
Clinet index 1, End of Epoch 3/6, Average Loss=1.0541613101959229, Class Loss=0.4988076090812683, Reg Loss=0.5553536415100098
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.49064669013023376, Reg Loss=0.5527812838554382
Clinet index 1, End of Epoch 4/6, Average Loss=1.0434279441833496, Class Loss=0.49064669013023376, Reg Loss=0.5527812838554382
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.46746712923049927, Reg Loss=0.5491155385971069
Clinet index 1, End of Epoch 5/6, Average Loss=1.016582727432251, Class Loss=0.46746712923049927, Reg Loss=0.5491155385971069
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.4651079475879669, Reg Loss=0.55370032787323
Clinet index 1, End of Epoch 6/6, Average Loss=1.0188082456588745, Class Loss=0.4651079475879669, Reg Loss=0.55370032787323
federated aggregation...
Validation, Class Loss=1.3610363006591797, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.447763
Mean Acc: 0.846865
FreqW Acc: 0.356825
Mean IoU: 0.488830
Class IoU:
	class 0: 0.24432714
	class 1: 0.4126501
	class 2: 0.2379572
	class 3: 0.7821547
	class 4: 0.2999597
	class 5: 0.6528763
	class 6: 0.739284
	class 7: 0.774708
	class 8: 0.8544384
	class 9: 0.37766287
	class 10: 0.60276914
	class 11: 0.43381995
	class 12: 0.77655935
	class 13: 0.61234146
	class 14: 0.55852234
	class 15: 0.7449718
	class 16: 0.026624048
	class 17: 0.10267737
	class 18: 0.05345885
Class Acc:
	class 0: 0.24444719
	class 1: 0.99156874
	class 2: 0.9786738
	class 3: 0.92663395
	class 4: 0.94534516
	class 5: 0.9500813
	class 6: 0.99075717
	class 7: 0.9659375
	class 8: 0.96832937
	class 9: 0.7305849
	class 10: 0.7812829
	class 11: 0.8258605
	class 12: 0.9628213
	class 13: 0.9113645
	class 14: 0.9594326
	class 15: 0.96452135
	class 16: 0.8415366
	class 17: 0.4788048
	class 18: 0.6724577

federated global round: 17, step: 3
select part of clients to conduct local training
[8, 5, 21, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.44106489419937134, Reg Loss=0.5460455417633057
Clinet index 8, End of Epoch 1/6, Average Loss=0.987110435962677, Class Loss=0.44106489419937134, Reg Loss=0.5460455417633057
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.44010093808174133, Reg Loss=0.5418579578399658
Clinet index 8, End of Epoch 2/6, Average Loss=0.9819588661193848, Class Loss=0.44010093808174133, Reg Loss=0.5418579578399658
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.44617730379104614, Reg Loss=0.5427243709564209
Clinet index 8, End of Epoch 3/6, Average Loss=0.988901674747467, Class Loss=0.44617730379104614, Reg Loss=0.5427243709564209
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.4302643835544586, Reg Loss=0.5461239814758301
Clinet index 8, End of Epoch 4/6, Average Loss=0.9763883352279663, Class Loss=0.4302643835544586, Reg Loss=0.5461239814758301
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.4280867874622345, Reg Loss=0.5569757223129272
Clinet index 8, End of Epoch 5/6, Average Loss=0.9850624799728394, Class Loss=0.4280867874622345, Reg Loss=0.5569757223129272
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.4184441864490509, Reg Loss=0.5431342720985413
Clinet index 8, End of Epoch 6/6, Average Loss=0.9615784883499146, Class Loss=0.4184441864490509, Reg Loss=0.5431342720985413
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.4463181495666504, Reg Loss=0.5431963801383972
Clinet index 5, End of Epoch 1/6, Average Loss=0.9895145297050476, Class Loss=0.4463181495666504, Reg Loss=0.5431963801383972
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.44341689348220825, Reg Loss=0.5467099547386169
Clinet index 5, End of Epoch 2/6, Average Loss=0.9901268482208252, Class Loss=0.44341689348220825, Reg Loss=0.5467099547386169
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Class Loss=0.4332112967967987, Reg Loss=0.5413125157356262
Clinet index 5, End of Epoch 3/6, Average Loss=0.9745237827301025, Class Loss=0.4332112967967987, Reg Loss=0.5413125157356262
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Class Loss=0.42482417821884155, Reg Loss=0.5502132773399353
Clinet index 5, End of Epoch 4/6, Average Loss=0.9750374555587769, Class Loss=0.42482417821884155, Reg Loss=0.5502132773399353
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Class Loss=0.40973734855651855, Reg Loss=0.5439573526382446
Clinet index 5, End of Epoch 5/6, Average Loss=0.9536947011947632, Class Loss=0.40973734855651855, Reg Loss=0.5439573526382446
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Class Loss=0.41481438279151917, Reg Loss=0.5494822859764099
Clinet index 5, End of Epoch 6/6, Average Loss=0.9642966985702515, Class Loss=0.41481438279151917, Reg Loss=0.5494822859764099
Current Client Index:  21
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.4513572156429291, Reg Loss=0.5517992377281189
Clinet index 21, End of Epoch 1/6, Average Loss=1.0031564235687256, Class Loss=0.4513572156429291, Reg Loss=0.5517992377281189
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Class Loss=0.4369560182094574, Reg Loss=0.5478991866111755
Clinet index 21, End of Epoch 2/6, Average Loss=0.9848551750183105, Class Loss=0.4369560182094574, Reg Loss=0.5478991866111755
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.4367361068725586, Reg Loss=0.5473154783248901
Clinet index 21, End of Epoch 3/6, Average Loss=0.9840515851974487, Class Loss=0.4367361068725586, Reg Loss=0.5473154783248901
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Class Loss=0.42864933609962463, Reg Loss=0.5484121441841125
Clinet index 21, End of Epoch 4/6, Average Loss=0.9770615100860596, Class Loss=0.42864933609962463, Reg Loss=0.5484121441841125
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Class Loss=0.4062371253967285, Reg Loss=0.5427283644676208
Clinet index 21, End of Epoch 5/6, Average Loss=0.9489654898643494, Class Loss=0.4062371253967285, Reg Loss=0.5427283644676208
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Class Loss=0.40139585733413696, Reg Loss=0.5400792956352234
Clinet index 21, End of Epoch 6/6, Average Loss=0.9414751529693604, Class Loss=0.40139585733413696, Reg Loss=0.5400792956352234
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.4488508105278015, Reg Loss=0.5489449501037598
Clinet index 7, End of Epoch 1/6, Average Loss=0.9977957606315613, Class Loss=0.4488508105278015, Reg Loss=0.5489449501037598
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.4346451461315155, Reg Loss=0.5571000576019287
Clinet index 7, End of Epoch 2/6, Average Loss=0.9917452335357666, Class Loss=0.4346451461315155, Reg Loss=0.5571000576019287
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.4362814724445343, Reg Loss=0.5533223748207092
Clinet index 7, End of Epoch 3/6, Average Loss=0.9896038770675659, Class Loss=0.4362814724445343, Reg Loss=0.5533223748207092
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.41913098096847534, Reg Loss=0.5449223518371582
Clinet index 7, End of Epoch 4/6, Average Loss=0.9640533328056335, Class Loss=0.41913098096847534, Reg Loss=0.5449223518371582
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.4091144800186157, Reg Loss=0.5485095381736755
Clinet index 7, End of Epoch 5/6, Average Loss=0.9576240181922913, Class Loss=0.4091144800186157, Reg Loss=0.5485095381736755
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.4004087448120117, Reg Loss=0.5402073264122009
Clinet index 7, End of Epoch 6/6, Average Loss=0.9406160712242126, Class Loss=0.4004087448120117, Reg Loss=0.5402073264122009
federated aggregation...
Validation, Class Loss=1.3125629425048828, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.502302
Mean Acc: 0.853755
FreqW Acc: 0.410564
Mean IoU: 0.494945
Class IoU:
	class 0: 0.32172534
	class 1: 0.4025482
	class 2: 0.24271175
	class 3: 0.78836113
	class 4: 0.31330276
	class 5: 0.6496689
	class 6: 0.7327189
	class 7: 0.7591365
	class 8: 0.84322333
	class 9: 0.37061504
	class 10: 0.612873
	class 11: 0.45863232
	class 12: 0.7814811
	class 13: 0.6268514
	class 14: 0.5466965
	class 15: 0.7427522
	class 16: 0.02783862
	class 17: 0.11314604
	class 18: 0.06967416
Class Acc:
	class 0: 0.32199496
	class 1: 0.9931182
	class 2: 0.97871614
	class 3: 0.9230125
	class 4: 0.9371424
	class 5: 0.95742154
	class 6: 0.9924584
	class 7: 0.9673243
	class 8: 0.97171277
	class 9: 0.7490009
	class 10: 0.78322804
	class 11: 0.8183345
	class 12: 0.9602469
	class 13: 0.902942
	class 14: 0.95735234
	class 15: 0.9644936
	class 16: 0.8220314
	class 17: 0.51951635
	class 18: 0.7013016

federated global round: 18, step: 3
select part of clients to conduct local training
[14, 4, 11, 21]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.39242005348205566, Reg Loss=0.5498747825622559
Clinet index 14, End of Epoch 1/6, Average Loss=0.9422948360443115, Class Loss=0.39242005348205566, Reg Loss=0.5498747825622559
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.4004071354866028, Reg Loss=0.5415559411048889
Clinet index 14, End of Epoch 2/6, Average Loss=0.9419630765914917, Class Loss=0.4004071354866028, Reg Loss=0.5415559411048889
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.38398563861846924, Reg Loss=0.5512557625770569
Clinet index 14, End of Epoch 3/6, Average Loss=0.9352414011955261, Class Loss=0.38398563861846924, Reg Loss=0.5512557625770569
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.3807710111141205, Reg Loss=0.5420966744422913
Clinet index 14, End of Epoch 4/6, Average Loss=0.9228676557540894, Class Loss=0.3807710111141205, Reg Loss=0.5420966744422913
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.3837171196937561, Reg Loss=0.5416499376296997
Clinet index 14, End of Epoch 5/6, Average Loss=0.9253670573234558, Class Loss=0.3837171196937561, Reg Loss=0.5416499376296997
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.3765810430049896, Reg Loss=0.5366368293762207
Clinet index 14, End of Epoch 6/6, Average Loss=0.9132179021835327, Class Loss=0.3765810430049896, Reg Loss=0.5366368293762207
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.4123691916465759, Reg Loss=0.5512397885322571
Clinet index 4, End of Epoch 1/6, Average Loss=0.963608980178833, Class Loss=0.4123691916465759, Reg Loss=0.5512397885322571
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.4079333543777466, Reg Loss=0.5481592416763306
Clinet index 4, End of Epoch 2/6, Average Loss=0.9560925960540771, Class Loss=0.4079333543777466, Reg Loss=0.5481592416763306
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.40567877888679504, Reg Loss=0.5496993660926819
Clinet index 4, End of Epoch 3/6, Average Loss=0.9553781747817993, Class Loss=0.40567877888679504, Reg Loss=0.5496993660926819
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.39954426884651184, Reg Loss=0.5496050715446472
Clinet index 4, End of Epoch 4/6, Average Loss=0.9491493701934814, Class Loss=0.39954426884651184, Reg Loss=0.5496050715446472
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.3960527777671814, Reg Loss=0.5477648377418518
Clinet index 4, End of Epoch 5/6, Average Loss=0.9438176155090332, Class Loss=0.3960527777671814, Reg Loss=0.5477648377418518
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.3837340176105499, Reg Loss=0.553246796131134
Clinet index 4, End of Epoch 6/6, Average Loss=0.9369808435440063, Class Loss=0.3837340176105499, Reg Loss=0.553246796131134
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.4215097427368164, Reg Loss=0.542402982711792
Clinet index 11, End of Epoch 1/6, Average Loss=0.9639127254486084, Class Loss=0.4215097427368164, Reg Loss=0.542402982711792
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.41592153906822205, Reg Loss=0.5431926846504211
Clinet index 11, End of Epoch 2/6, Average Loss=0.9591141939163208, Class Loss=0.41592153906822205, Reg Loss=0.5431926846504211
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Class Loss=0.4040355384349823, Reg Loss=0.543011486530304
Clinet index 11, End of Epoch 3/6, Average Loss=0.9470469951629639, Class Loss=0.4040355384349823, Reg Loss=0.543011486530304
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Class Loss=0.3956098258495331, Reg Loss=0.5420507192611694
Clinet index 11, End of Epoch 4/6, Average Loss=0.9376605749130249, Class Loss=0.3956098258495331, Reg Loss=0.5420507192611694
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Class Loss=0.3973690867424011, Reg Loss=0.5392298102378845
Clinet index 11, End of Epoch 5/6, Average Loss=0.9365988969802856, Class Loss=0.3973690867424011, Reg Loss=0.5392298102378845
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Class Loss=0.380331426858902, Reg Loss=0.5425038933753967
Clinet index 11, End of Epoch 6/6, Average Loss=0.9228353500366211, Class Loss=0.380331426858902, Reg Loss=0.5425038933753967
Current Client Index:  21
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Epoch 1, Class Loss=0.40788719058036804, Reg Loss=0.5456638932228088
Clinet index 21, End of Epoch 1/6, Average Loss=0.9535510540008545, Class Loss=0.40788719058036804, Reg Loss=0.5456638932228088
Pseudo labeling is: None
Epoch 2, lr = 0.000525
Epoch 2, Class Loss=0.3961522877216339, Reg Loss=0.5377830862998962
Clinet index 21, End of Epoch 2/6, Average Loss=0.9339354038238525, Class Loss=0.3961522877216339, Reg Loss=0.5377830862998962
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.39829808473587036, Reg Loss=0.5416793823242188
Clinet index 21, End of Epoch 3/6, Average Loss=0.9399774670600891, Class Loss=0.39829808473587036, Reg Loss=0.5416793823242188
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Class Loss=0.4007529020309448, Reg Loss=0.5472438335418701
Clinet index 21, End of Epoch 4/6, Average Loss=0.9479967355728149, Class Loss=0.4007529020309448, Reg Loss=0.5472438335418701
Pseudo labeling is: None
Epoch 5, lr = 0.000394
Epoch 5, Class Loss=0.3893040120601654, Reg Loss=0.5441931486129761
Clinet index 21, End of Epoch 5/6, Average Loss=0.9334971904754639, Class Loss=0.3893040120601654, Reg Loss=0.5441931486129761
Pseudo labeling is: None
Epoch 6, lr = 0.000350
Epoch 6, Class Loss=0.38307255506515503, Reg Loss=0.5469753742218018
Clinet index 21, End of Epoch 6/6, Average Loss=0.9300479292869568, Class Loss=0.38307255506515503, Reg Loss=0.5469753742218018
federated aggregation...
Validation, Class Loss=1.262282133102417, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.542474
Mean Acc: 0.857104
FreqW Acc: 0.453079
Mean IoU: 0.505280
Class IoU:
	class 0: 0.37950468
	class 1: 0.4311299
	class 2: 0.25011688
	class 3: 0.7909267
	class 4: 0.32245705
	class 5: 0.64726394
	class 6: 0.75978184
	class 7: 0.7622145
	class 8: 0.84660214
	class 9: 0.36932817
	class 10: 0.61371124
	class 11: 0.45715684
	class 12: 0.780526
	class 13: 0.63142
	class 14: 0.5749672
	class 15: 0.75069445
	class 16: 0.027970156
	class 17: 0.11573571
	class 18: 0.088805236
Class Acc:
	class 0: 0.37989601
	class 1: 0.99140704
	class 2: 0.9784058
	class 3: 0.9270494
	class 4: 0.9414855
	class 5: 0.95185435
	class 6: 0.9912606
	class 7: 0.96740526
	class 8: 0.971485
	class 9: 0.73786163
	class 10: 0.7739133
	class 11: 0.8171967
	class 12: 0.9597197
	class 13: 0.90592206
	class 14: 0.9569157
	class 15: 0.9630996
	class 16: 0.81670654
	class 17: 0.5347588
	class 18: 0.71863264

federated global round: 19, step: 3
select part of clients to conduct local training
[15, 12, 21, 16]
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3782724440097809, Reg Loss=0.5429080128669739
Clinet index 15, End of Epoch 1/6, Average Loss=0.9211804866790771, Class Loss=0.3782724440097809, Reg Loss=0.5429080128669739
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.361309289932251, Reg Loss=0.5390704870223999
Clinet index 15, End of Epoch 2/6, Average Loss=0.9003797769546509, Class Loss=0.361309289932251, Reg Loss=0.5390704870223999
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.3594702482223511, Reg Loss=0.5391426086425781
Clinet index 15, End of Epoch 3/6, Average Loss=0.8986128568649292, Class Loss=0.3594702482223511, Reg Loss=0.5391426086425781
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.36540836095809937, Reg Loss=0.5479268431663513
Clinet index 15, End of Epoch 4/6, Average Loss=0.9133352041244507, Class Loss=0.36540836095809937, Reg Loss=0.5479268431663513
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.36539024114608765, Reg Loss=0.5500626564025879
Clinet index 15, End of Epoch 5/6, Average Loss=0.9154528975486755, Class Loss=0.36539024114608765, Reg Loss=0.5500626564025879
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.3610806167125702, Reg Loss=0.5448980331420898
Clinet index 15, End of Epoch 6/6, Average Loss=0.9059786796569824, Class Loss=0.3610806167125702, Reg Loss=0.5448980331420898
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.38903820514678955, Reg Loss=0.5461104512214661
Clinet index 12, End of Epoch 1/6, Average Loss=0.9351486563682556, Class Loss=0.38903820514678955, Reg Loss=0.5461104512214661
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.39186424016952515, Reg Loss=0.5502874851226807
Clinet index 12, End of Epoch 2/6, Average Loss=0.9421517252922058, Class Loss=0.39186424016952515, Reg Loss=0.5502874851226807
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.3804706931114197, Reg Loss=0.545638918876648
Clinet index 12, End of Epoch 3/6, Average Loss=0.9261096119880676, Class Loss=0.3804706931114197, Reg Loss=0.545638918876648
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.3729502260684967, Reg Loss=0.5355351567268372
Clinet index 12, End of Epoch 4/6, Average Loss=0.9084854125976562, Class Loss=0.3729502260684967, Reg Loss=0.5355351567268372
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.3688051402568817, Reg Loss=0.5454196929931641
Clinet index 12, End of Epoch 5/6, Average Loss=0.9142248630523682, Class Loss=0.3688051402568817, Reg Loss=0.5454196929931641
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.36798369884490967, Reg Loss=0.5448410511016846
Clinet index 12, End of Epoch 6/6, Average Loss=0.9128247499465942, Class Loss=0.36798369884490967, Reg Loss=0.5448410511016846
Current Client Index:  21
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000304
Epoch 1, Class Loss=0.381285160779953, Reg Loss=0.5405409336090088
Clinet index 21, End of Epoch 1/6, Average Loss=0.9218261241912842, Class Loss=0.381285160779953, Reg Loss=0.5405409336090088
Pseudo labeling is: None
Epoch 2, lr = 0.000258
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.36943209171295166, Reg Loss=0.5357115268707275
Clinet index 21, End of Epoch 2/6, Average Loss=0.9051436185836792, Class Loss=0.36943209171295166, Reg Loss=0.5357115268707275
Pseudo labeling is: None
Epoch 3, lr = 0.000211
Epoch 3, Class Loss=0.38239404559135437, Reg Loss=0.5411205887794495
Clinet index 21, End of Epoch 3/6, Average Loss=0.9235146045684814, Class Loss=0.38239404559135437, Reg Loss=0.5411205887794495
Pseudo labeling is: None
Epoch 4, lr = 0.000163
Epoch 4, Class Loss=0.371049165725708, Reg Loss=0.5438734889030457
Clinet index 21, End of Epoch 4/6, Average Loss=0.9149226546287537, Class Loss=0.371049165725708, Reg Loss=0.5438734889030457
Pseudo labeling is: None
Epoch 5, lr = 0.000113
Epoch 5, Class Loss=0.36847954988479614, Reg Loss=0.5389928817749023
Clinet index 21, End of Epoch 5/6, Average Loss=0.9074724316596985, Class Loss=0.36847954988479614, Reg Loss=0.5389928817749023
Pseudo labeling is: None
Epoch 6, lr = 0.000061
Epoch 6, Class Loss=0.37484604120254517, Reg Loss=0.5398898720741272
Clinet index 21, End of Epoch 6/6, Average Loss=0.9147359132766724, Class Loss=0.37484604120254517, Reg Loss=0.5398898720741272
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3898758888244629, Reg Loss=0.5462968349456787
Clinet index 16, End of Epoch 1/6, Average Loss=0.9361727237701416, Class Loss=0.3898758888244629, Reg Loss=0.5462968349456787
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.39352598786354065, Reg Loss=0.542157769203186
Clinet index 16, End of Epoch 2/6, Average Loss=0.9356837272644043, Class Loss=0.39352598786354065, Reg Loss=0.542157769203186
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.3723156452178955, Reg Loss=0.5452675223350525
Clinet index 16, End of Epoch 3/6, Average Loss=0.917583167552948, Class Loss=0.3723156452178955, Reg Loss=0.5452675223350525
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.37210050225257874, Reg Loss=0.5373588800430298
Clinet index 16, End of Epoch 4/6, Average Loss=0.9094593524932861, Class Loss=0.37210050225257874, Reg Loss=0.5373588800430298
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.3815433382987976, Reg Loss=0.5415962338447571
Clinet index 16, End of Epoch 5/6, Average Loss=0.9231395721435547, Class Loss=0.3815433382987976, Reg Loss=0.5415962338447571
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.3737863004207611, Reg Loss=0.5393462181091309
Clinet index 16, End of Epoch 6/6, Average Loss=0.9131325483322144, Class Loss=0.3737863004207611, Reg Loss=0.5393462181091309
federated aggregation...
Validation, Class Loss=1.2671926021575928, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.546959
Mean Acc: 0.855829
FreqW Acc: 0.455646
Mean IoU: 0.501137
Class IoU:
	class 0: 0.38630015
	class 1: 0.4129126
	class 2: 0.2553103
	class 3: 0.7847366
	class 4: 0.30344963
	class 5: 0.63258356
	class 6: 0.75990546
	class 7: 0.7683392
	class 8: 0.8433492
	class 9: 0.36576635
	class 10: 0.6196571
	class 11: 0.45842874
	class 12: 0.76620954
	class 13: 0.6243191
	class 14: 0.5693564
	class 15: 0.731988
	class 16: 0.027736738
	class 17: 0.11620223
	class 18: 0.09504726
Class Acc:
	class 0: 0.38671702
	class 1: 0.9912576
	class 2: 0.97470677
	class 3: 0.9203973
	class 4: 0.94534725
	class 5: 0.9560391
	class 6: 0.9911602
	class 7: 0.96682256
	class 8: 0.9712719
	class 9: 0.7365858
	class 10: 0.7605156
	class 11: 0.81371313
	class 12: 0.96077526
	class 13: 0.9049868
	class 14: 0.9591212
	class 15: 0.9655754
	class 16: 0.80846155
	class 17: 0.5343524
	class 18: 0.71294147

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[9, 12, 24, 8]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2127267122268677, Reg Loss=1.0947428941726685
Clinet index 9, End of Epoch 1/6, Average Loss=2.307469606399536, Class Loss=1.2127267122268677, Reg Loss=1.0947428941726685
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.8352973461151123, Reg Loss=0.9895859956741333
Clinet index 9, End of Epoch 2/6, Average Loss=1.8248833417892456, Class Loss=0.8352973461151123, Reg Loss=0.9895859956741333
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.5911781787872314, Reg Loss=0.9232527613639832
Clinet index 9, End of Epoch 3/6, Average Loss=1.5144309997558594, Class Loss=0.5911781787872314, Reg Loss=0.9232527613639832
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.49483269453048706, Reg Loss=0.9008529782295227
Clinet index 9, End of Epoch 4/6, Average Loss=1.3956856727600098, Class Loss=0.49483269453048706, Reg Loss=0.9008529782295227
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.4806484878063202, Reg Loss=0.8900114297866821
Clinet index 9, End of Epoch 5/6, Average Loss=1.3706599473953247, Class Loss=0.4806484878063202, Reg Loss=0.8900114297866821
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4517214298248291, Reg Loss=0.8980263471603394
Clinet index 9, End of Epoch 6/6, Average Loss=1.3497477769851685, Class Loss=0.4517214298248291, Reg Loss=0.8980263471603394
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.191847562789917, Reg Loss=1.1183027029037476
Clinet index 12, End of Epoch 1/6, Average Loss=2.310150146484375, Class Loss=1.191847562789917, Reg Loss=1.1183027029037476
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.8090147972106934, Reg Loss=0.9760447144508362
Clinet index 12, End of Epoch 2/6, Average Loss=1.7850594520568848, Class Loss=0.8090147972106934, Reg Loss=0.9760447144508362
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.5651726722717285, Reg Loss=0.9221559762954712
Clinet index 12, End of Epoch 3/6, Average Loss=1.4873286485671997, Class Loss=0.5651726722717285, Reg Loss=0.9221559762954712
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.4758833944797516, Reg Loss=0.9021589159965515
Clinet index 12, End of Epoch 4/6, Average Loss=1.3780423402786255, Class Loss=0.4758833944797516, Reg Loss=0.9021589159965515
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.4583375155925751, Reg Loss=0.8872343301773071
Clinet index 12, End of Epoch 5/6, Average Loss=1.3455718755722046, Class Loss=0.4583375155925751, Reg Loss=0.8872343301773071
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4626254439353943, Reg Loss=0.8764848709106445
Clinet index 12, End of Epoch 6/6, Average Loss=1.3391103744506836, Class Loss=0.4626254439353943, Reg Loss=0.8764848709106445
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1603572368621826, Reg Loss=1.0836201906204224
Clinet index 24, End of Epoch 1/6, Average Loss=2.2439775466918945, Class Loss=1.1603572368621826, Reg Loss=1.0836201906204224
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.7791351675987244, Reg Loss=0.9816780686378479
Clinet index 24, End of Epoch 2/6, Average Loss=1.7608132362365723, Class Loss=0.7791351675987244, Reg Loss=0.9816780686378479
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.5624349117279053, Reg Loss=0.9126484990119934
Clinet index 24, End of Epoch 3/6, Average Loss=1.475083351135254, Class Loss=0.5624349117279053, Reg Loss=0.9126484990119934
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5043150186538696, Reg Loss=0.884357750415802
Clinet index 24, End of Epoch 4/6, Average Loss=1.3886728286743164, Class Loss=0.5043150186538696, Reg Loss=0.884357750415802
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.4781007170677185, Reg Loss=0.8816704154014587
Clinet index 24, End of Epoch 5/6, Average Loss=1.3597711324691772, Class Loss=0.4781007170677185, Reg Loss=0.8816704154014587
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4489119350910187, Reg Loss=0.873725950717926
Clinet index 24, End of Epoch 6/6, Average Loss=1.322637915611267, Class Loss=0.4489119350910187, Reg Loss=0.873725950717926
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2210208177566528, Reg Loss=1.115964651107788
Clinet index 8, End of Epoch 1/6, Average Loss=2.3369855880737305, Class Loss=1.2210208177566528, Reg Loss=1.115964651107788
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.8250916600227356, Reg Loss=0.9714698791503906
Clinet index 8, End of Epoch 2/6, Average Loss=1.7965614795684814, Class Loss=0.8250916600227356, Reg Loss=0.9714698791503906
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.5757577419281006, Reg Loss=0.9238367676734924
Clinet index 8, End of Epoch 3/6, Average Loss=1.4995944499969482, Class Loss=0.5757577419281006, Reg Loss=0.9238367676734924
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.48850589990615845, Reg Loss=0.9081990122795105
Clinet index 8, End of Epoch 4/6, Average Loss=1.396704912185669, Class Loss=0.48850589990615845, Reg Loss=0.9081990122795105
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.4781223237514496, Reg Loss=0.8926053643226624
Clinet index 8, End of Epoch 5/6, Average Loss=1.3707276582717896, Class Loss=0.4781223237514496, Reg Loss=0.8926053643226624
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4607996642589569, Reg Loss=0.8890019655227661
Clinet index 8, End of Epoch 6/6, Average Loss=1.3498016595840454, Class Loss=0.4607996642589569, Reg Loss=0.8890019655227661
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=2.3811919689178467, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.269854
Mean Acc: 0.793305
FreqW Acc: 0.176291
Mean IoU: 0.477340
Class IoU:
	class 0: 0.0
	class 1: 0.63665855
	class 2: 0.30840117
	class 3: 0.75825596
	class 4: 0.41534686
	class 5: 0.67322457
	class 6: 0.862932
	class 7: 0.8132883
	class 8: 0.8489458
	class 9: 0.30425355
	class 10: 0.55534583
	class 11: 0.4860666
	class 12: 0.7802352
	class 13: 0.5152438
	class 14: 0.70966715
	class 15: 0.5746925
	class 16: 0.020294571
	class 17: 0.14164646
	class 18: 0.058054075
	class 19: 0.08424848
Class Acc:
	class 0: 0.0
	class 1: 0.9193421
	class 2: 0.95947134
	class 3: 0.80393463
	class 4: 0.84584314
	class 5: 0.84643734
	class 6: 0.946351
	class 7: 0.933202
	class 8: 0.9209108
	class 9: 0.57814765
	class 10: 0.7461768
	class 11: 0.7433478
	class 12: 0.8933315
	class 13: 0.82930243
	class 14: 0.91726345
	class 15: 0.9676054
	class 16: 0.88846314
	class 17: 0.66214216
	class 18: 0.6058219
	class 19: 0.85901487

federated global round: 21, step: 4
select part of clients to conduct local training
[5, 8, 0, 6]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4863899052143097, Reg Loss=0.8853033781051636
Clinet index 5, End of Epoch 1/6, Average Loss=1.3716932535171509, Class Loss=0.4863899052143097, Reg Loss=0.8853033781051636
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.4744033217430115, Reg Loss=0.8819879293441772
Clinet index 5, End of Epoch 2/6, Average Loss=1.356391191482544, Class Loss=0.4744033217430115, Reg Loss=0.8819879293441772
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.4696475863456726, Reg Loss=0.8830870389938354
Clinet index 5, End of Epoch 3/6, Average Loss=1.3527345657348633, Class Loss=0.4696475863456726, Reg Loss=0.8830870389938354
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.43602412939071655, Reg Loss=0.8895980715751648
Clinet index 5, End of Epoch 4/6, Average Loss=1.3256222009658813, Class Loss=0.43602412939071655, Reg Loss=0.8895980715751648
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.4300634562969208, Reg Loss=0.8866053819656372
Clinet index 5, End of Epoch 5/6, Average Loss=1.3166688680648804, Class Loss=0.4300634562969208, Reg Loss=0.8866053819656372
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.4397672414779663, Reg Loss=0.8731437921524048
Clinet index 5, End of Epoch 6/6, Average Loss=1.312911033630371, Class Loss=0.4397672414779663, Reg Loss=0.8731437921524048
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4800485074520111, Reg Loss=0.8925173878669739
Clinet index 8, End of Epoch 1/6, Average Loss=1.3725658655166626, Class Loss=0.4800485074520111, Reg Loss=0.8925173878669739
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Class Loss=0.4604995548725128, Reg Loss=0.8929159045219421
Clinet index 8, End of Epoch 2/6, Average Loss=1.3534154891967773, Class Loss=0.4604995548725128, Reg Loss=0.8929159045219421
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Class Loss=0.45706647634506226, Reg Loss=0.8882122039794922
Clinet index 8, End of Epoch 3/6, Average Loss=1.3452787399291992, Class Loss=0.45706647634506226, Reg Loss=0.8882122039794922
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Class Loss=0.44679397344589233, Reg Loss=0.8874805569648743
Clinet index 8, End of Epoch 4/6, Average Loss=1.3342745304107666, Class Loss=0.44679397344589233, Reg Loss=0.8874805569648743
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.4296302795410156, Reg Loss=0.8903895020484924
Clinet index 8, End of Epoch 5/6, Average Loss=1.3200197219848633, Class Loss=0.4296302795410156, Reg Loss=0.8903895020484924
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Class Loss=0.4241641163825989, Reg Loss=0.8902417421340942
Clinet index 8, End of Epoch 6/6, Average Loss=1.314405918121338, Class Loss=0.4241641163825989, Reg Loss=0.8902417421340942
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.46117210388183594, Reg Loss=0.8933455944061279
Clinet index 0, End of Epoch 1/6, Average Loss=1.3545176982879639, Class Loss=0.46117210388183594, Reg Loss=0.8933455944061279
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.4487501084804535, Reg Loss=0.89759761095047
Clinet index 0, End of Epoch 2/6, Average Loss=1.346347689628601, Class Loss=0.4487501084804535, Reg Loss=0.89759761095047
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.44788262248039246, Reg Loss=0.8931899070739746
Clinet index 0, End of Epoch 3/6, Average Loss=1.3410725593566895, Class Loss=0.44788262248039246, Reg Loss=0.8931899070739746
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.4357752203941345, Reg Loss=0.8950054049491882
Clinet index 0, End of Epoch 4/6, Average Loss=1.3307806253433228, Class Loss=0.4357752203941345, Reg Loss=0.8950054049491882
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.44736963510513306, Reg Loss=0.8892306089401245
Clinet index 0, End of Epoch 5/6, Average Loss=1.3366003036499023, Class Loss=0.44736963510513306, Reg Loss=0.8892306089401245
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.43208199739456177, Reg Loss=0.8950557112693787
Clinet index 0, End of Epoch 6/6, Average Loss=1.3271377086639404, Class Loss=0.43208199739456177, Reg Loss=0.8950557112693787
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.4730003774166107, Reg Loss=0.8869743943214417
Clinet index 6, End of Epoch 1/6, Average Loss=1.35997474193573, Class Loss=0.4730003774166107, Reg Loss=0.8869743943214417
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.4729166626930237, Reg Loss=0.8828572034835815
Clinet index 6, End of Epoch 2/6, Average Loss=1.35577392578125, Class Loss=0.4729166626930237, Reg Loss=0.8828572034835815
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.45583614706993103, Reg Loss=0.8813768625259399
Clinet index 6, End of Epoch 3/6, Average Loss=1.3372130393981934, Class Loss=0.45583614706993103, Reg Loss=0.8813768625259399
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.4190303385257721, Reg Loss=0.8864062428474426
Clinet index 6, End of Epoch 4/6, Average Loss=1.305436611175537, Class Loss=0.4190303385257721, Reg Loss=0.8864062428474426
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.44478175044059753, Reg Loss=0.8859270215034485
Clinet index 6, End of Epoch 5/6, Average Loss=1.3307087421417236, Class Loss=0.44478175044059753, Reg Loss=0.8859270215034485
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.4328756630420685, Reg Loss=0.8874456286430359
Clinet index 6, End of Epoch 6/6, Average Loss=1.3203213214874268, Class Loss=0.4328756630420685, Reg Loss=0.8874456286430359
federated aggregation...
Validation, Class Loss=2.5812032222747803, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.272583
Mean Acc: 0.803606
FreqW Acc: 0.185488
Mean IoU: 0.500084
Class IoU:
	class 0: 0.0
	class 1: 0.6764714
	class 2: 0.33343086
	class 3: 0.76972467
	class 4: 0.45641106
	class 5: 0.6925447
	class 6: 0.88407874
	class 7: 0.83982724
	class 8: 0.86729854
	class 9: 0.3269969
	class 10: 0.58184
	class 11: 0.5392953
	class 12: 0.7996676
	class 13: 0.5475583
	class 14: 0.7501008
	class 15: 0.63302946
	class 16: 0.019055266
	class 17: 0.116334215
	class 18: 0.057179905
	class 19: 0.11082694
Class Acc:
	class 0: 0.0
	class 1: 0.9255587
	class 2: 0.9580816
	class 3: 0.80229145
	class 4: 0.86373854
	class 5: 0.8520441
	class 6: 0.9529182
	class 7: 0.9261422
	class 8: 0.9235741
	class 9: 0.5539606
	class 10: 0.72485405
	class 11: 0.73362386
	class 12: 0.8905375
	class 13: 0.8643455
	class 14: 0.91056776
	class 15: 0.9675862
	class 16: 0.8983077
	class 17: 0.73915887
	class 18: 0.69829905
	class 19: 0.8865366

federated global round: 22, step: 4
select part of clients to conduct local training
[8, 25, 22, 5]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4136441946029663, Reg Loss=0.8915496468544006
Clinet index 8, End of Epoch 1/6, Average Loss=1.3051939010620117, Class Loss=0.4136441946029663, Reg Loss=0.8915496468544006
Pseudo labeling is: None
Epoch 2, lr = 0.000600
Epoch 2, Class Loss=0.42504626512527466, Reg Loss=0.8969730138778687
Clinet index 8, End of Epoch 2/6, Average Loss=1.322019338607788, Class Loss=0.42504626512527466, Reg Loss=0.8969730138778687
Pseudo labeling is: None
Epoch 3, lr = 0.000568
Epoch 3, Class Loss=0.4176817834377289, Reg Loss=0.887663722038269
Clinet index 8, End of Epoch 3/6, Average Loss=1.3053455352783203, Class Loss=0.4176817834377289, Reg Loss=0.887663722038269
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.4016719460487366, Reg Loss=0.8925417065620422
Clinet index 8, End of Epoch 4/6, Average Loss=1.2942136526107788, Class Loss=0.4016719460487366, Reg Loss=0.8925417065620422
Pseudo labeling is: None
Epoch 5, lr = 0.000504
Epoch 5, Class Loss=0.4115166664123535, Reg Loss=0.8871247172355652
Clinet index 8, End of Epoch 5/6, Average Loss=1.2986414432525635, Class Loss=0.4115166664123535, Reg Loss=0.8871247172355652
Pseudo labeling is: None
Epoch 6, lr = 0.000471
Epoch 6, Class Loss=0.41052305698394775, Reg Loss=0.8822983503341675
Clinet index 8, End of Epoch 6/6, Average Loss=1.2928214073181152, Class Loss=0.41052305698394775, Reg Loss=0.8822983503341675
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.4029763638973236, Reg Loss=0.8769614100456238
Clinet index 25, End of Epoch 1/6, Average Loss=1.279937744140625, Class Loss=0.4029763638973236, Reg Loss=0.8769614100456238
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.40094491839408875, Reg Loss=0.8816832304000854
Clinet index 25, End of Epoch 2/6, Average Loss=1.2826281785964966, Class Loss=0.40094491839408875, Reg Loss=0.8816832304000854
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.4055257737636566, Reg Loss=0.8805896043777466
Clinet index 25, End of Epoch 3/6, Average Loss=1.2861154079437256, Class Loss=0.4055257737636566, Reg Loss=0.8805896043777466
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.38814765214920044, Reg Loss=0.8792902231216431
Clinet index 25, End of Epoch 4/6, Average Loss=1.2674379348754883, Class Loss=0.38814765214920044, Reg Loss=0.8792902231216431
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.40012311935424805, Reg Loss=0.8826857209205627
Clinet index 25, End of Epoch 5/6, Average Loss=1.282808780670166, Class Loss=0.40012311935424805, Reg Loss=0.8826857209205627
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.38559412956237793, Reg Loss=0.8809270262718201
Clinet index 25, End of Epoch 6/6, Average Loss=1.2665212154388428, Class Loss=0.38559412956237793, Reg Loss=0.8809270262718201
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.43341177701950073, Reg Loss=0.877205491065979
Clinet index 22, End of Epoch 1/6, Average Loss=1.310617208480835, Class Loss=0.43341177701950073, Reg Loss=0.877205491065979
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.42592620849609375, Reg Loss=0.8705086708068848
Clinet index 22, End of Epoch 2/6, Average Loss=1.2964348793029785, Class Loss=0.42592620849609375, Reg Loss=0.8705086708068848
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.41006872057914734, Reg Loss=0.8783146142959595
Clinet index 22, End of Epoch 3/6, Average Loss=1.2883833646774292, Class Loss=0.41006872057914734, Reg Loss=0.8783146142959595
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.4013495445251465, Reg Loss=0.8813860416412354
Clinet index 22, End of Epoch 4/6, Average Loss=1.2827355861663818, Class Loss=0.4013495445251465, Reg Loss=0.8813860416412354
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.4148373603820801, Reg Loss=0.881597638130188
Clinet index 22, End of Epoch 5/6, Average Loss=1.296434998512268, Class Loss=0.4148373603820801, Reg Loss=0.881597638130188
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.39690080285072327, Reg Loss=0.8866212964057922
Clinet index 22, End of Epoch 6/6, Average Loss=1.283522129058838, Class Loss=0.39690080285072327, Reg Loss=0.8866212964057922
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4278324842453003, Reg Loss=0.8829495906829834
Clinet index 5, End of Epoch 1/6, Average Loss=1.3107820749282837, Class Loss=0.4278324842453003, Reg Loss=0.8829495906829834
Pseudo labeling is: None
Epoch 2, lr = 0.000733
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.42316800355911255, Reg Loss=0.881470799446106
Clinet index 5, End of Epoch 2/6, Average Loss=1.3046388626098633, Class Loss=0.42316800355911255, Reg Loss=0.881470799446106
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.42713841795921326, Reg Loss=0.8831902742385864
Clinet index 5, End of Epoch 3/6, Average Loss=1.310328722000122, Class Loss=0.42713841795921326, Reg Loss=0.8831902742385864
Pseudo labeling is: None
Epoch 4, lr = 0.000655
Epoch 4, Class Loss=0.4069271385669708, Reg Loss=0.8825845718383789
Clinet index 5, End of Epoch 4/6, Average Loss=1.2895116806030273, Class Loss=0.4069271385669708, Reg Loss=0.8825845718383789
Pseudo labeling is: None
Epoch 5, lr = 0.000616
Epoch 5, Class Loss=0.4114198088645935, Reg Loss=0.8819493651390076
Clinet index 5, End of Epoch 5/6, Average Loss=1.293369174003601, Class Loss=0.4114198088645935, Reg Loss=0.8819493651390076
Pseudo labeling is: None
Epoch 6, lr = 0.000576
Epoch 6, Class Loss=0.4231870472431183, Reg Loss=0.8805651664733887
Clinet index 5, End of Epoch 6/6, Average Loss=1.3037521839141846, Class Loss=0.4231870472431183, Reg Loss=0.8805651664733887
federated aggregation...
Validation, Class Loss=2.6922874450683594, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.273766
Mean Acc: 0.809128
FreqW Acc: 0.190061
Mean IoU: 0.509365
Class IoU:
	class 0: 0.0
	class 1: 0.6838638
	class 2: 0.3446899
	class 3: 0.7754332
	class 4: 0.45767492
	class 5: 0.7055403
	class 6: 0.89826447
	class 7: 0.8450341
	class 8: 0.86885643
	class 9: 0.34056997
	class 10: 0.61035955
	class 11: 0.54299337
	class 12: 0.8065136
	class 13: 0.5656816
	class 14: 0.75881827
	class 15: 0.67099
	class 16: 0.01784984
	class 17: 0.10286986
	class 18: 0.06120907
	class 19: 0.13009366
Class Acc:
	class 0: 0.0
	class 1: 0.9218521
	class 2: 0.9562869
	class 3: 0.8055892
	class 4: 0.8880608
	class 5: 0.87174255
	class 6: 0.9510974
	class 7: 0.92423254
	class 8: 0.9190343
	class 9: 0.51156294
	class 10: 0.73100126
	class 11: 0.7403906
	class 12: 0.8817556
	class 13: 0.86926425
	class 14: 0.91325957
	class 15: 0.96729195
	class 16: 0.8995719
	class 17: 0.7915818
	class 18: 0.7400173
	class 19: 0.8989764

federated global round: 23, step: 4
select part of clients to conduct local training
[3, 19, 1, 11]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4018353521823883, Reg Loss=0.8795779347419739
Clinet index 3, End of Epoch 1/6, Average Loss=1.2814133167266846, Class Loss=0.4018353521823883, Reg Loss=0.8795779347419739
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.40061667561531067, Reg Loss=0.8878072500228882
Clinet index 3, End of Epoch 2/6, Average Loss=1.2884238958358765, Class Loss=0.40061667561531067, Reg Loss=0.8878072500228882
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.3890678882598877, Reg Loss=0.8898827433586121
Clinet index 3, End of Epoch 3/6, Average Loss=1.2789506912231445, Class Loss=0.3890678882598877, Reg Loss=0.8898827433586121
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.4008970260620117, Reg Loss=0.8926994800567627
Clinet index 3, End of Epoch 4/6, Average Loss=1.2935965061187744, Class Loss=0.4008970260620117, Reg Loss=0.8926994800567627
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.39665690064430237, Reg Loss=0.8862680196762085
Clinet index 3, End of Epoch 5/6, Average Loss=1.2829248905181885, Class Loss=0.39665690064430237, Reg Loss=0.8862680196762085
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.3905060887336731, Reg Loss=0.8914811015129089
Clinet index 3, End of Epoch 6/6, Average Loss=1.281987190246582, Class Loss=0.3905060887336731, Reg Loss=0.8914811015129089
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.391526460647583, Reg Loss=0.9015470743179321
Clinet index 19, End of Epoch 1/6, Average Loss=1.2930735349655151, Class Loss=0.391526460647583, Reg Loss=0.9015470743179321
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.39448148012161255, Reg Loss=0.884487509727478
Clinet index 19, End of Epoch 2/6, Average Loss=1.2789690494537354, Class Loss=0.39448148012161255, Reg Loss=0.884487509727478
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.3837570250034332, Reg Loss=0.8920540809631348
Clinet index 19, End of Epoch 3/6, Average Loss=1.2758110761642456, Class Loss=0.3837570250034332, Reg Loss=0.8920540809631348
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.38310906291007996, Reg Loss=0.8912873864173889
Clinet index 19, End of Epoch 4/6, Average Loss=1.2743964195251465, Class Loss=0.38310906291007996, Reg Loss=0.8912873864173889
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.38280153274536133, Reg Loss=0.8923925161361694
Clinet index 19, End of Epoch 5/6, Average Loss=1.2751940488815308, Class Loss=0.38280153274536133, Reg Loss=0.8923925161361694
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.3849709928035736, Reg Loss=0.8911385536193848
Clinet index 19, End of Epoch 6/6, Average Loss=1.2761095762252808, Class Loss=0.3849709928035736, Reg Loss=0.8911385536193848
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.39229366183280945, Reg Loss=0.8928141593933105
Clinet index 1, End of Epoch 1/6, Average Loss=1.2851078510284424, Class Loss=0.39229366183280945, Reg Loss=0.8928141593933105
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.3996026813983917, Reg Loss=0.8945091962814331
Clinet index 1, End of Epoch 2/6, Average Loss=1.2941118478775024, Class Loss=0.3996026813983917, Reg Loss=0.8945091962814331
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.39514076709747314, Reg Loss=0.9006720781326294
Clinet index 1, End of Epoch 3/6, Average Loss=1.2958128452301025, Class Loss=0.39514076709747314, Reg Loss=0.9006720781326294
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.38028568029403687, Reg Loss=0.8961586356163025
Clinet index 1, End of Epoch 4/6, Average Loss=1.2764443159103394, Class Loss=0.38028568029403687, Reg Loss=0.8961586356163025
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.38773345947265625, Reg Loss=0.8935883045196533
Clinet index 1, End of Epoch 5/6, Average Loss=1.2813217639923096, Class Loss=0.38773345947265625, Reg Loss=0.8935883045196533
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.39121896028518677, Reg Loss=0.8971440196037292
Clinet index 1, End of Epoch 6/6, Average Loss=1.288362979888916, Class Loss=0.39121896028518677, Reg Loss=0.8971440196037292
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.3949359059333801, Reg Loss=0.882678210735321
Clinet index 11, End of Epoch 1/6, Average Loss=1.2776141166687012, Class Loss=0.3949359059333801, Reg Loss=0.882678210735321
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.4069012403488159, Reg Loss=0.8795531988143921
Clinet index 11, End of Epoch 2/6, Average Loss=1.286454439163208, Class Loss=0.4069012403488159, Reg Loss=0.8795531988143921
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.4063785672187805, Reg Loss=0.8819217681884766
Clinet index 11, End of Epoch 3/6, Average Loss=1.2883002758026123, Class Loss=0.4063785672187805, Reg Loss=0.8819217681884766
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.39166656136512756, Reg Loss=0.8859107494354248
Clinet index 11, End of Epoch 4/6, Average Loss=1.27757728099823, Class Loss=0.39166656136512756, Reg Loss=0.8859107494354248
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.38973096013069153, Reg Loss=0.8895013928413391
Clinet index 11, End of Epoch 5/6, Average Loss=1.279232382774353, Class Loss=0.38973096013069153, Reg Loss=0.8895013928413391
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.39585328102111816, Reg Loss=0.889657735824585
Clinet index 11, End of Epoch 6/6, Average Loss=1.2855110168457031, Class Loss=0.39585328102111816, Reg Loss=0.889657735824585
federated aggregation...
Validation, Class Loss=2.7686691284179688, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.274602
Mean Acc: 0.811664
FreqW Acc: 0.189572
Mean IoU: 0.510125
Class IoU:
	class 0: 0.0
	class 1: 0.6850424
	class 2: 0.34800842
	class 3: 0.79388744
	class 4: 0.43571687
	class 5: 0.7000077
	class 6: 0.8965585
	class 7: 0.8425464
	class 8: 0.86789984
	class 9: 0.34490278
	class 10: 0.6163069
	class 11: 0.5483109
	class 12: 0.80448645
	class 13: 0.58653295
	class 14: 0.7628998
	class 15: 0.65473473
	class 16: 0.017066
	class 17: 0.0931858
	class 18: 0.0680869
	class 19: 0.13631311
Class Acc:
	class 0: 0.0
	class 1: 0.92976785
	class 2: 0.95674276
	class 3: 0.8301673
	class 4: 0.9069031
	class 5: 0.87803006
	class 6: 0.9597718
	class 7: 0.92062384
	class 8: 0.9227289
	class 9: 0.4794942
	class 10: 0.72526413
	class 11: 0.73937005
	class 12: 0.87969184
	class 13: 0.8790718
	class 14: 0.91946965
	class 15: 0.969616
	class 16: 0.8989346
	class 17: 0.79437774
	class 18: 0.734585
	class 19: 0.90867263

federated global round: 24, step: 4
select part of clients to conduct local training
[4, 13, 18, 9]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.369660347700119, Reg Loss=0.8944967985153198
Clinet index 4, End of Epoch 1/6, Average Loss=1.2641571760177612, Class Loss=0.369660347700119, Reg Loss=0.8944967985153198
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.37431102991104126, Reg Loss=0.8967404365539551
Clinet index 4, End of Epoch 2/6, Average Loss=1.2710514068603516, Class Loss=0.37431102991104126, Reg Loss=0.8967404365539551
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.37263041734695435, Reg Loss=0.8938490748405457
Clinet index 4, End of Epoch 3/6, Average Loss=1.2664794921875, Class Loss=0.37263041734695435, Reg Loss=0.8938490748405457
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.375045508146286, Reg Loss=0.8903602361679077
Clinet index 4, End of Epoch 4/6, Average Loss=1.2654057741165161, Class Loss=0.375045508146286, Reg Loss=0.8903602361679077
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.3854847252368927, Reg Loss=0.8880507349967957
Clinet index 4, End of Epoch 5/6, Average Loss=1.2735354900360107, Class Loss=0.3854847252368927, Reg Loss=0.8880507349967957
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.3639084994792938, Reg Loss=0.887437105178833
Clinet index 4, End of Epoch 6/6, Average Loss=1.2513456344604492, Class Loss=0.3639084994792938, Reg Loss=0.887437105178833
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.38613995909690857, Reg Loss=0.8948298692703247
Clinet index 13, End of Epoch 1/6, Average Loss=1.2809698581695557, Class Loss=0.38613995909690857, Reg Loss=0.8948298692703247
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.37251561880111694, Reg Loss=0.8907509446144104
Clinet index 13, End of Epoch 2/6, Average Loss=1.2632665634155273, Class Loss=0.37251561880111694, Reg Loss=0.8907509446144104
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.38068583607673645, Reg Loss=0.8966542482376099
Clinet index 13, End of Epoch 3/6, Average Loss=1.277340054512024, Class Loss=0.38068583607673645, Reg Loss=0.8966542482376099
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.3755970299243927, Reg Loss=0.8980170488357544
Clinet index 13, End of Epoch 4/6, Average Loss=1.2736140489578247, Class Loss=0.3755970299243927, Reg Loss=0.8980170488357544
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.3747853636741638, Reg Loss=0.8960636258125305
Clinet index 13, End of Epoch 5/6, Average Loss=1.2708489894866943, Class Loss=0.3747853636741638, Reg Loss=0.8960636258125305
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.3742544949054718, Reg Loss=0.892800509929657
Clinet index 13, End of Epoch 6/6, Average Loss=1.2670550346374512, Class Loss=0.3742544949054718, Reg Loss=0.892800509929657
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3781769573688507, Reg Loss=0.8984838128089905
Clinet index 18, End of Epoch 1/6, Average Loss=1.2766607999801636, Class Loss=0.3781769573688507, Reg Loss=0.8984838128089905
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.3683156371116638, Reg Loss=0.8988441824913025
Clinet index 18, End of Epoch 2/6, Average Loss=1.2671598196029663, Class Loss=0.3683156371116638, Reg Loss=0.8988441824913025
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.386074036359787, Reg Loss=0.89649897813797
Clinet index 18, End of Epoch 3/6, Average Loss=1.2825729846954346, Class Loss=0.386074036359787, Reg Loss=0.89649897813797
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.37892335653305054, Reg Loss=0.898267924785614
Clinet index 18, End of Epoch 4/6, Average Loss=1.2771912813186646, Class Loss=0.37892335653305054, Reg Loss=0.898267924785614
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.37016478180885315, Reg Loss=0.8977073431015015
Clinet index 18, End of Epoch 5/6, Average Loss=1.2678720951080322, Class Loss=0.37016478180885315, Reg Loss=0.8977073431015015
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.3677027225494385, Reg Loss=0.8919397592544556
Clinet index 18, End of Epoch 6/6, Average Loss=1.259642481803894, Class Loss=0.3677027225494385, Reg Loss=0.8919397592544556
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3754824697971344, Reg Loss=0.8936451077461243
Clinet index 9, End of Epoch 1/6, Average Loss=1.269127607345581, Class Loss=0.3754824697971344, Reg Loss=0.8936451077461243
Pseudo labeling is: None
Epoch 2, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.37282097339630127, Reg Loss=0.9063872694969177
Clinet index 9, End of Epoch 2/6, Average Loss=1.2792081832885742, Class Loss=0.37282097339630127, Reg Loss=0.9063872694969177
Pseudo labeling is: None
Epoch 3, lr = 0.000568
Epoch 3, Class Loss=0.373149037361145, Reg Loss=0.8979716897010803
Clinet index 9, End of Epoch 3/6, Average Loss=1.2711207866668701, Class Loss=0.373149037361145, Reg Loss=0.8979716897010803
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Class Loss=0.38348686695098877, Reg Loss=0.8943744897842407
Clinet index 9, End of Epoch 4/6, Average Loss=1.2778613567352295, Class Loss=0.38348686695098877, Reg Loss=0.8943744897842407
Pseudo labeling is: None
Epoch 5, lr = 0.000304
Epoch 5, Class Loss=0.37718063592910767, Reg Loss=0.8946115970611572
Clinet index 9, End of Epoch 5/6, Average Loss=1.2717921733856201, Class Loss=0.37718063592910767, Reg Loss=0.8946115970611572
Pseudo labeling is: None
Epoch 6, lr = 0.000163
Epoch 6, Class Loss=0.36748912930488586, Reg Loss=0.899309515953064
Clinet index 9, End of Epoch 6/6, Average Loss=1.2667986154556274, Class Loss=0.36748912930488586, Reg Loss=0.899309515953064
federated aggregation...
Validation, Class Loss=2.819328784942627, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.274712
Mean Acc: 0.812461
FreqW Acc: 0.189318
Mean IoU: 0.508513
Class IoU:
	class 0: 0.0
	class 1: 0.6780007
	class 2: 0.34880212
	class 3: 0.796087
	class 4: 0.41631627
	class 5: 0.71216357
	class 6: 0.88985246
	class 7: 0.8421671
	class 8: 0.8717194
	class 9: 0.34387425
	class 10: 0.6140596
	class 11: 0.55091125
	class 12: 0.80091935
	class 13: 0.57302743
	class 14: 0.7572418
	class 15: 0.65643567
	class 16: 0.017231356
	class 17: 0.09109461
	class 18: 0.0661289
	class 19: 0.14422767
Class Acc:
	class 0: 0.0
	class 1: 0.93678486
	class 2: 0.9558268
	class 3: 0.8343553
	class 4: 0.9137796
	class 5: 0.89418703
	class 6: 0.96195483
	class 7: 0.9169882
	class 8: 0.928072
	class 9: 0.46861532
	class 10: 0.7029614
	class 11: 0.73388046
	class 12: 0.8739983
	class 13: 0.8846482
	class 14: 0.92137873
	class 15: 0.97000694
	class 16: 0.89983666
	class 17: 0.80256623
	class 18: 0.7422069
	class 19: 0.9071688

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 25, step: 5
select part of clients to conduct local training
[29, 3, 8, 19]
Current Client Index:  29
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.9264906644821167, Reg Loss=0.8053126931190491
Clinet index 29, End of Epoch 1/6, Average Loss=1.7318034172058105, Class Loss=0.9264906644821167, Reg Loss=0.8053126931190491
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.8376424908638, Reg Loss=0.7832251191139221
Clinet index 29, End of Epoch 2/6, Average Loss=1.6208676099777222, Class Loss=0.8376424908638, Reg Loss=0.7832251191139221
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.7133859395980835, Reg Loss=0.7780781984329224
Clinet index 29, End of Epoch 3/6, Average Loss=1.4914641380310059, Class Loss=0.7133859395980835, Reg Loss=0.7780781984329224
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.6083874106407166, Reg Loss=0.7825823426246643
Clinet index 29, End of Epoch 4/6, Average Loss=1.3909697532653809, Class Loss=0.6083874106407166, Reg Loss=0.7825823426246643
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5082462430000305, Reg Loss=0.7767256498336792
Clinet index 29, End of Epoch 5/6, Average Loss=1.2849719524383545, Class Loss=0.5082462430000305, Reg Loss=0.7767256498336792
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4584847092628479, Reg Loss=0.7829101085662842
Clinet index 29, End of Epoch 6/6, Average Loss=1.2413947582244873, Class Loss=0.4584847092628479, Reg Loss=0.7829101085662842
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.9058759808540344, Reg Loss=0.8010101318359375
Clinet index 3, End of Epoch 1/6, Average Loss=1.7068860530853271, Class Loss=0.9058759808540344, Reg Loss=0.8010101318359375
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.8783086538314819, Reg Loss=0.776511013507843
Clinet index 3, End of Epoch 2/6, Average Loss=1.6548197269439697, Class Loss=0.8783086538314819, Reg Loss=0.776511013507843
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.7132619619369507, Reg Loss=0.7721378207206726
Clinet index 3, End of Epoch 3/6, Average Loss=1.4853997230529785, Class Loss=0.7132619619369507, Reg Loss=0.7721378207206726
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.596142053604126, Reg Loss=0.7744420766830444
Clinet index 3, End of Epoch 4/6, Average Loss=1.3705841302871704, Class Loss=0.596142053604126, Reg Loss=0.7744420766830444
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5268115401268005, Reg Loss=0.7762239575386047
Clinet index 3, End of Epoch 5/6, Average Loss=1.3030354976654053, Class Loss=0.5268115401268005, Reg Loss=0.7762239575386047
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4524499177932739, Reg Loss=0.7836039066314697
Clinet index 3, End of Epoch 6/6, Average Loss=1.2360538244247437, Class Loss=0.4524499177932739, Reg Loss=0.7836039066314697
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=1.0205734968185425, Reg Loss=0.8064943552017212
Clinet index 8, End of Epoch 1/6, Average Loss=1.8270678520202637, Class Loss=1.0205734968185425, Reg Loss=0.8064943552017212
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.8942801356315613, Reg Loss=0.7854974865913391
Clinet index 8, End of Epoch 2/6, Average Loss=1.6797776222229004, Class Loss=0.8942801356315613, Reg Loss=0.7854974865913391
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.7482628226280212, Reg Loss=0.7720272541046143
Clinet index 8, End of Epoch 3/6, Average Loss=1.5202901363372803, Class Loss=0.7482628226280212, Reg Loss=0.7720272541046143
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.6319010853767395, Reg Loss=0.7776910066604614
Clinet index 8, End of Epoch 4/6, Average Loss=1.4095921516418457, Class Loss=0.6319010853767395, Reg Loss=0.7776910066604614
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.535353422164917, Reg Loss=0.7856998443603516
Clinet index 8, End of Epoch 5/6, Average Loss=1.3210532665252686, Class Loss=0.535353422164917, Reg Loss=0.7856998443603516
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4673927128314972, Reg Loss=0.7963865399360657
Clinet index 8, End of Epoch 6/6, Average Loss=1.2637792825698853, Class Loss=0.4673927128314972, Reg Loss=0.7963865399360657
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=1.0506798028945923, Reg Loss=0.8007132411003113
Clinet index 19, End of Epoch 1/6, Average Loss=1.8513929843902588, Class Loss=1.0506798028945923, Reg Loss=0.8007132411003113
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.9484055638313293, Reg Loss=0.7790132164955139
Clinet index 19, End of Epoch 2/6, Average Loss=1.7274187803268433, Class Loss=0.9484055638313293, Reg Loss=0.7790132164955139
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.7799433469772339, Reg Loss=0.7767807841300964
Clinet index 19, End of Epoch 3/6, Average Loss=1.5567240715026855, Class Loss=0.7799433469772339, Reg Loss=0.7767807841300964
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.6184163689613342, Reg Loss=0.7838997840881348
Clinet index 19, End of Epoch 4/6, Average Loss=1.4023160934448242, Class Loss=0.6184163689613342, Reg Loss=0.7838997840881348
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5333102345466614, Reg Loss=0.7864977121353149
Clinet index 19, End of Epoch 5/6, Average Loss=1.319808006286621, Class Loss=0.5333102345466614, Reg Loss=0.7864977121353149
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.45795556902885437, Reg Loss=0.7954177260398865
Clinet index 19, End of Epoch 6/6, Average Loss=1.2533732652664185, Class Loss=0.45795556902885437, Reg Loss=0.7954177260398865
federated aggregation...
Validation, Class Loss=3.2314088344573975, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.265127
Mean Acc: 0.761613
FreqW Acc: 0.180152
Mean IoU: 0.465117
Class IoU:
	class 0: 0.0
	class 1: 0.55014277
	class 2: 0.36535397
	class 3: 0.7418401
	class 4: 0.42457682
	class 5: 0.68013185
	class 6: 0.8333749
	class 7: 0.79652923
	class 8: 0.8631488
	class 9: 0.35007972
	class 10: 0.57464087
	class 11: 0.5427865
	class 12: 0.787119
	class 13: 0.53874815
	class 14: 0.70193076
	class 15: 0.663379
	class 16: 0.01432498
	class 17: 0.09559984
	class 18: 0.07277782
	class 19: 0.16642436
	class 20: 0.004544677
Class Acc:
	class 0: 0.0
	class 1: 0.9648752
	class 2: 0.9448681
	class 3: 0.77570045
	class 4: 0.90456337
	class 5: 0.829859
	class 6: 0.93130517
	class 7: 0.9300242
	class 8: 0.92863727
	class 9: 0.5146544
	class 10: 0.6812937
	class 11: 0.7593642
	class 12: 0.8796558
	class 13: 0.8595909
	class 14: 0.91004884
	class 15: 0.9712571
	class 16: 0.9149938
	class 17: 0.7815652
	class 18: 0.7380268
	class 19: 0.7688049
	class 20: 0.0047748154

federated global round: 26, step: 5
select part of clients to conduct local training
[4, 0, 29, 20]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4500613212585449, Reg Loss=0.7931018471717834
Clinet index 4, End of Epoch 1/6, Average Loss=1.2431631088256836, Class Loss=0.4500613212585449, Reg Loss=0.7931018471717834
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.43485838174819946, Reg Loss=0.7956245541572571
Clinet index 4, End of Epoch 2/6, Average Loss=1.2304829359054565, Class Loss=0.43485838174819946, Reg Loss=0.7956245541572571
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.40925732254981995, Reg Loss=0.8044028878211975
Clinet index 4, End of Epoch 3/6, Average Loss=1.2136602401733398, Class Loss=0.40925732254981995, Reg Loss=0.8044028878211975
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.40788015723228455, Reg Loss=0.8034244775772095
Clinet index 4, End of Epoch 4/6, Average Loss=1.2113046646118164, Class Loss=0.40788015723228455, Reg Loss=0.8034244775772095
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.3892934322357178, Reg Loss=0.7984477281570435
Clinet index 4, End of Epoch 5/6, Average Loss=1.1877411603927612, Class Loss=0.3892934322357178, Reg Loss=0.7984477281570435
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.39300718903541565, Reg Loss=0.8016462922096252
Clinet index 4, End of Epoch 6/6, Average Loss=1.1946535110473633, Class Loss=0.39300718903541565, Reg Loss=0.8016462922096252
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.40730488300323486, Reg Loss=0.7989380955696106
Clinet index 0, End of Epoch 1/6, Average Loss=1.2062430381774902, Class Loss=0.40730488300323486, Reg Loss=0.7989380955696106
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.3896518051624298, Reg Loss=0.7960519790649414
Clinet index 0, End of Epoch 2/6, Average Loss=1.1857037544250488, Class Loss=0.3896518051624298, Reg Loss=0.7960519790649414
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.3823241591453552, Reg Loss=0.7990525364875793
Clinet index 0, End of Epoch 3/6, Average Loss=1.1813766956329346, Class Loss=0.3823241591453552, Reg Loss=0.7990525364875793
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.37543511390686035, Reg Loss=0.7932094931602478
Clinet index 0, End of Epoch 4/6, Average Loss=1.168644666671753, Class Loss=0.37543511390686035, Reg Loss=0.7932094931602478
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.3592703938484192, Reg Loss=0.7979894876480103
Clinet index 0, End of Epoch 5/6, Average Loss=1.1572599411010742, Class Loss=0.3592703938484192, Reg Loss=0.7979894876480103
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.3694041073322296, Reg Loss=0.794940710067749
Clinet index 0, End of Epoch 6/6, Average Loss=1.1643447875976562, Class Loss=0.3694041073322296, Reg Loss=0.794940710067749
Current Client Index:  29
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4213622808456421, Reg Loss=0.7949217557907104
Clinet index 29, End of Epoch 1/6, Average Loss=1.2162840366363525, Class Loss=0.4213622808456421, Reg Loss=0.7949217557907104
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Class Loss=0.42277905344963074, Reg Loss=0.7981926798820496
Clinet index 29, End of Epoch 2/6, Average Loss=1.220971703529358, Class Loss=0.42277905344963074, Reg Loss=0.7981926798820496
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Class Loss=0.42402490973472595, Reg Loss=0.7964419722557068
Clinet index 29, End of Epoch 3/6, Average Loss=1.2204668521881104, Class Loss=0.42402490973472595, Reg Loss=0.7964419722557068
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Class Loss=0.39007043838500977, Reg Loss=0.8029934763908386
Clinet index 29, End of Epoch 4/6, Average Loss=1.1930639743804932, Class Loss=0.39007043838500977, Reg Loss=0.8029934763908386
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.3890777826309204, Reg Loss=0.7958934307098389
Clinet index 29, End of Epoch 5/6, Average Loss=1.1849712133407593, Class Loss=0.3890777826309204, Reg Loss=0.7958934307098389
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Class Loss=0.3835110664367676, Reg Loss=0.7929825782775879
Clinet index 29, End of Epoch 6/6, Average Loss=1.1764936447143555, Class Loss=0.3835110664367676, Reg Loss=0.7929825782775879
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4328795373439789, Reg Loss=0.796500027179718
Clinet index 20, End of Epoch 1/6, Average Loss=1.2293795347213745, Class Loss=0.4328795373439789, Reg Loss=0.796500027179718
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.4030883014202118, Reg Loss=0.7980806231498718
Clinet index 20, End of Epoch 2/6, Average Loss=1.2011688947677612, Class Loss=0.4030883014202118, Reg Loss=0.7980806231498718
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.40241023898124695, Reg Loss=0.7939903140068054
Clinet index 20, End of Epoch 3/6, Average Loss=1.19640052318573, Class Loss=0.40241023898124695, Reg Loss=0.7939903140068054
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.38559773564338684, Reg Loss=0.7982638478279114
Clinet index 20, End of Epoch 4/6, Average Loss=1.1838616132736206, Class Loss=0.38559773564338684, Reg Loss=0.7982638478279114
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.3848114013671875, Reg Loss=0.7971329689025879
Clinet index 20, End of Epoch 5/6, Average Loss=1.1819443702697754, Class Loss=0.3848114013671875, Reg Loss=0.7971329689025879
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.370956152677536, Reg Loss=0.7973865270614624
Clinet index 20, End of Epoch 6/6, Average Loss=1.1683427095413208, Class Loss=0.370956152677536, Reg Loss=0.7973865270614624
federated aggregation...
Validation, Class Loss=3.477623224258423, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.265631
Mean Acc: 0.763945
FreqW Acc: 0.182865
Mean IoU: 0.476378
Class IoU:
	class 0: 0.0
	class 1: 0.578381
	class 2: 0.37314183
	class 3: 0.74160665
	class 4: 0.45359397
	class 5: 0.6778218
	class 6: 0.8510194
	class 7: 0.814479
	class 8: 0.8578232
	class 9: 0.3556097
	class 10: 0.5847733
	class 11: 0.5640152
	class 12: 0.77364415
	class 13: 0.57645476
	class 14: 0.7227736
	class 15: 0.6756712
	class 16: 0.015334081
	class 17: 0.099896565
	class 18: 0.073401414
	class 19: 0.13561621
	class 20: 0.07888734
Class Acc:
	class 0: 0.0
	class 1: 0.9543172
	class 2: 0.9455755
	class 3: 0.767508
	class 4: 0.9060637
	class 5: 0.8324588
	class 6: 0.93659496
	class 7: 0.91076165
	class 8: 0.9225554
	class 9: 0.47808498
	class 10: 0.6543065
	class 11: 0.7417615
	class 12: 0.85685337
	class 13: 0.8514185
	class 14: 0.8994292
	class 15: 0.9712262
	class 16: 0.9209169
	class 17: 0.7894307
	class 18: 0.7423058
	class 19: 0.8663095
	class 20: 0.09495726

federated global round: 27, step: 5
select part of clients to conduct local training
[13, 12, 20, 15]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.41937005519866943, Reg Loss=0.7906824946403503
Clinet index 13, End of Epoch 1/6, Average Loss=1.210052490234375, Class Loss=0.41937005519866943, Reg Loss=0.7906824946403503
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.41525548696517944, Reg Loss=0.7923455834388733
Clinet index 13, End of Epoch 2/6, Average Loss=1.2076010704040527, Class Loss=0.41525548696517944, Reg Loss=0.7923455834388733
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.3902009427547455, Reg Loss=0.7976024150848389
Clinet index 13, End of Epoch 3/6, Average Loss=1.1878033876419067, Class Loss=0.3902009427547455, Reg Loss=0.7976024150848389
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.3906721770763397, Reg Loss=0.8010466694831848
Clinet index 13, End of Epoch 4/6, Average Loss=1.1917188167572021, Class Loss=0.3906721770763397, Reg Loss=0.8010466694831848
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.38484829664230347, Reg Loss=0.802225649356842
Clinet index 13, End of Epoch 5/6, Average Loss=1.1870739459991455, Class Loss=0.38484829664230347, Reg Loss=0.802225649356842
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.3912331163883209, Reg Loss=0.7949185967445374
Clinet index 13, End of Epoch 6/6, Average Loss=1.1861517429351807, Class Loss=0.3912331163883209, Reg Loss=0.7949185967445374
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3969016671180725, Reg Loss=0.7858193516731262
Clinet index 12, End of Epoch 1/6, Average Loss=1.1827210187911987, Class Loss=0.3969016671180725, Reg Loss=0.7858193516731262
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.39999550580978394, Reg Loss=0.7886795401573181
Clinet index 12, End of Epoch 2/6, Average Loss=1.188675045967102, Class Loss=0.39999550580978394, Reg Loss=0.7886795401573181
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.3764514625072479, Reg Loss=0.7883006930351257
Clinet index 12, End of Epoch 3/6, Average Loss=1.1647521257400513, Class Loss=0.3764514625072479, Reg Loss=0.7883006930351257
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.3564755618572235, Reg Loss=0.7912282943725586
Clinet index 12, End of Epoch 4/6, Average Loss=1.1477038860321045, Class Loss=0.3564755618572235, Reg Loss=0.7912282943725586
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.36684805154800415, Reg Loss=0.7893351316452026
Clinet index 12, End of Epoch 5/6, Average Loss=1.1561832427978516, Class Loss=0.36684805154800415, Reg Loss=0.7893351316452026
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.3635093569755554, Reg Loss=0.7890270352363586
Clinet index 12, End of Epoch 6/6, Average Loss=1.152536392211914, Class Loss=0.3635093569755554, Reg Loss=0.7890270352363586
Current Client Index:  20
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3794388175010681, Reg Loss=0.7976105213165283
Clinet index 20, End of Epoch 1/6, Average Loss=1.1770493984222412, Class Loss=0.3794388175010681, Reg Loss=0.7976105213165283
Pseudo labeling is: None
Epoch 2, lr = 0.000733
Epoch 2, Class Loss=0.38017454743385315, Reg Loss=0.7974762320518494
Clinet index 20, End of Epoch 2/6, Average Loss=1.177650809288025, Class Loss=0.38017454743385315, Reg Loss=0.7974762320518494
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.3822111189365387, Reg Loss=0.7918316125869751
Clinet index 20, End of Epoch 3/6, Average Loss=1.1740427017211914, Class Loss=0.3822111189365387, Reg Loss=0.7918316125869751
Pseudo labeling is: None
Epoch 4, lr = 0.000655
Epoch 4, Class Loss=0.3529030978679657, Reg Loss=0.7962871789932251
Clinet index 20, End of Epoch 4/6, Average Loss=1.1491903066635132, Class Loss=0.3529030978679657, Reg Loss=0.7962871789932251
Pseudo labeling is: None
Epoch 5, lr = 0.000616
Epoch 5, Class Loss=0.3703840970993042, Reg Loss=0.7942429780960083
Clinet index 20, End of Epoch 5/6, Average Loss=1.1646270751953125, Class Loss=0.3703840970993042, Reg Loss=0.7942429780960083
Pseudo labeling is: None
Epoch 6, lr = 0.000576
Epoch 6, Class Loss=0.34895944595336914, Reg Loss=0.7926603555679321
Clinet index 20, End of Epoch 6/6, Average Loss=1.1416198015213013, Class Loss=0.34895944595336914, Reg Loss=0.7926603555679321
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3533232808113098, Reg Loss=0.7947077751159668
Clinet index 15, End of Epoch 1/6, Average Loss=1.1480309963226318, Class Loss=0.3533232808113098, Reg Loss=0.7947077751159668
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.3566727936267853, Reg Loss=0.7909896373748779
Clinet index 15, End of Epoch 2/6, Average Loss=1.1476624011993408, Class Loss=0.3566727936267853, Reg Loss=0.7909896373748779
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.36844712495803833, Reg Loss=0.7947591543197632
Clinet index 15, End of Epoch 3/6, Average Loss=1.1632063388824463, Class Loss=0.36844712495803833, Reg Loss=0.7947591543197632
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.34231165051460266, Reg Loss=0.8012712597846985
Clinet index 15, End of Epoch 4/6, Average Loss=1.1435829401016235, Class Loss=0.34231165051460266, Reg Loss=0.8012712597846985
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.35259923338890076, Reg Loss=0.7981260418891907
Clinet index 15, End of Epoch 5/6, Average Loss=1.150725245475769, Class Loss=0.35259923338890076, Reg Loss=0.7981260418891907
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.32859936356544495, Reg Loss=0.8039186596870422
Clinet index 15, End of Epoch 6/6, Average Loss=1.1325180530548096, Class Loss=0.32859936356544495, Reg Loss=0.8039186596870422
federated aggregation...
Validation, Class Loss=3.673125743865967, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.265299
Mean Acc: 0.766505
FreqW Acc: 0.184270
Mean IoU: 0.486648
Class IoU:
	class 0: 0.0
	class 1: 0.61675686
	class 2: 0.36977273
	class 3: 0.73488414
	class 4: 0.48427194
	class 5: 0.69116175
	class 6: 0.85442716
	class 7: 0.82445604
	class 8: 0.86257476
	class 9: 0.35526574
	class 10: 0.55267864
	class 11: 0.56586367
	class 12: 0.7627293
	class 13: 0.5968725
	class 14: 0.7456283
	class 15: 0.66712976
	class 16: 0.015204965
	class 17: 0.096945286
	class 18: 0.07583291
	class 19: 0.1342997
	class 20: 0.21285431
Class Acc:
	class 0: 0.0
	class 1: 0.9486536
	class 2: 0.9468921
	class 3: 0.75846964
	class 4: 0.89961004
	class 5: 0.8353198
	class 6: 0.932502
	class 7: 0.9013743
	class 8: 0.92397827
	class 9: 0.46875256
	class 10: 0.59009796
	class 11: 0.72738177
	class 12: 0.8406358
	class 13: 0.8288986
	class 14: 0.8912068
	class 15: 0.97309345
	class 16: 0.91874456
	class 17: 0.81322795
	class 18: 0.7361
	class 19: 0.88371915
	class 20: 0.27794218

federated global round: 28, step: 5
select part of clients to conduct local training
[6, 29, 13, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3656710088253021, Reg Loss=0.7978442311286926
Clinet index 6, End of Epoch 1/6, Average Loss=1.1635152101516724, Class Loss=0.3656710088253021, Reg Loss=0.7978442311286926
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.3678334057331085, Reg Loss=0.795426607131958
Clinet index 6, End of Epoch 2/6, Average Loss=1.1632599830627441, Class Loss=0.3678334057331085, Reg Loss=0.795426607131958
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.3686866760253906, Reg Loss=0.8007724285125732
Clinet index 6, End of Epoch 3/6, Average Loss=1.1694591045379639, Class Loss=0.3686866760253906, Reg Loss=0.8007724285125732
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.3540443181991577, Reg Loss=0.8023711442947388
Clinet index 6, End of Epoch 4/6, Average Loss=1.1564154624938965, Class Loss=0.3540443181991577, Reg Loss=0.8023711442947388
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Class Loss=0.35917535424232483, Reg Loss=0.8028672933578491
Clinet index 6, End of Epoch 5/6, Average Loss=1.1620426177978516, Class Loss=0.35917535424232483, Reg Loss=0.8028672933578491
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.355499267578125, Reg Loss=0.8037503957748413
Clinet index 6, End of Epoch 6/6, Average Loss=1.1592496633529663, Class Loss=0.355499267578125, Reg Loss=0.8037503957748413
Current Client Index:  29
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3522288501262665, Reg Loss=0.7979419827461243
Clinet index 29, End of Epoch 1/6, Average Loss=1.1501708030700684, Class Loss=0.3522288501262665, Reg Loss=0.7979419827461243
Pseudo labeling is: None
Epoch 2, lr = 0.000584
Epoch 2, Class Loss=0.3535899221897125, Reg Loss=0.7977616786956787
Clinet index 29, End of Epoch 2/6, Average Loss=1.1513515710830688, Class Loss=0.3535899221897125, Reg Loss=0.7977616786956787
Pseudo labeling is: None
Epoch 3, lr = 0.000536
Epoch 3, Class Loss=0.3572612404823303, Reg Loss=0.7991183400154114
Clinet index 29, End of Epoch 3/6, Average Loss=1.1563795804977417, Class Loss=0.3572612404823303, Reg Loss=0.7991183400154114
Pseudo labeling is: None
Epoch 4, lr = 0.000487
Epoch 4, Class Loss=0.34135138988494873, Reg Loss=0.7969353795051575
Clinet index 29, End of Epoch 4/6, Average Loss=1.138286828994751, Class Loss=0.34135138988494873, Reg Loss=0.7969353795051575
Pseudo labeling is: None
Epoch 5, lr = 0.000438
Epoch 5, Class Loss=0.339521199464798, Reg Loss=0.8000538349151611
Clinet index 29, End of Epoch 5/6, Average Loss=1.1395750045776367, Class Loss=0.339521199464798, Reg Loss=0.8000538349151611
Pseudo labeling is: None
Epoch 6, lr = 0.000389
Epoch 6, Class Loss=0.347638875246048, Reg Loss=0.7932880520820618
Clinet index 29, End of Epoch 6/6, Average Loss=1.1409269571304321, Class Loss=0.347638875246048, Reg Loss=0.7932880520820618
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.3881191313266754, Reg Loss=0.7938212752342224
Clinet index 13, End of Epoch 1/6, Average Loss=1.1819404363632202, Class Loss=0.3881191313266754, Reg Loss=0.7938212752342224
Pseudo labeling is: None
Epoch 2, lr = 0.000642
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.38890331983566284, Reg Loss=0.7983688116073608
Clinet index 13, End of Epoch 2/6, Average Loss=1.187272071838379, Class Loss=0.38890331983566284, Reg Loss=0.7983688116073608
Pseudo labeling is: None
Epoch 3, lr = 0.000589
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.37574583292007446, Reg Loss=0.8005626797676086
Clinet index 13, End of Epoch 3/6, Average Loss=1.176308512687683, Class Loss=0.37574583292007446, Reg Loss=0.8005626797676086
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.3665379583835602, Reg Loss=0.7980487942695618
Clinet index 13, End of Epoch 4/6, Average Loss=1.1645867824554443, Class Loss=0.3665379583835602, Reg Loss=0.7980487942695618
Pseudo labeling is: None
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.36737293004989624, Reg Loss=0.7980088591575623
Clinet index 13, End of Epoch 5/6, Average Loss=1.1653817892074585, Class Loss=0.36737293004989624, Reg Loss=0.7980088591575623
Pseudo labeling is: None
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.3632797598838806, Reg Loss=0.7946497201919556
Clinet index 13, End of Epoch 6/6, Average Loss=1.1579294204711914, Class Loss=0.3632797598838806, Reg Loss=0.7946497201919556
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3648318648338318, Reg Loss=0.8064225316047668
Clinet index 2, End of Epoch 1/6, Average Loss=1.1712543964385986, Class Loss=0.3648318648338318, Reg Loss=0.8064225316047668
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.35612744092941284, Reg Loss=0.8060325384140015
Clinet index 2, End of Epoch 2/6, Average Loss=1.1621599197387695, Class Loss=0.35612744092941284, Reg Loss=0.8060325384140015
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.3537627160549164, Reg Loss=0.809040904045105
Clinet index 2, End of Epoch 3/6, Average Loss=1.1628036499023438, Class Loss=0.3537627160549164, Reg Loss=0.809040904045105
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.35156863927841187, Reg Loss=0.8045465350151062
Clinet index 2, End of Epoch 4/6, Average Loss=1.156115174293518, Class Loss=0.35156863927841187, Reg Loss=0.8045465350151062
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.3574208617210388, Reg Loss=0.8000309467315674
Clinet index 2, End of Epoch 5/6, Average Loss=1.157451868057251, Class Loss=0.3574208617210388, Reg Loss=0.8000309467315674
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.33693215250968933, Reg Loss=0.8021892309188843
Clinet index 2, End of Epoch 6/6, Average Loss=1.139121413230896, Class Loss=0.33693215250968933, Reg Loss=0.8021892309188843
federated aggregation...
Validation, Class Loss=3.812063217163086, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.265493
Mean Acc: 0.768382
FreqW Acc: 0.184590
Mean IoU: 0.489261
Class IoU:
	class 0: 0.0
	class 1: 0.6343618
	class 2: 0.37051034
	class 3: 0.72626173
	class 4: 0.49324903
	class 5: 0.6889313
	class 6: 0.8562813
	class 7: 0.81969666
	class 8: 0.8572399
	class 9: 0.35653678
	class 10: 0.57874244
	class 11: 0.5620502
	class 12: 0.76045924
	class 13: 0.6028616
	class 14: 0.7408834
	class 15: 0.6657172
	class 16: 0.015122839
	class 17: 0.09678704
	class 18: 0.07834928
	class 19: 0.1297967
	class 20: 0.24064432
Class Acc:
	class 0: 0.0
	class 1: 0.94125277
	class 2: 0.9469523
	class 3: 0.74870586
	class 4: 0.89384943
	class 5: 0.8377488
	class 6: 0.9326413
	class 7: 0.90230227
	class 8: 0.9234549
	class 9: 0.47544906
	class 10: 0.6249646
	class 11: 0.7365477
	class 12: 0.82994455
	class 13: 0.82874215
	class 14: 0.8820154
	class 15: 0.9726247
	class 16: 0.91754997
	class 17: 0.8220781
	class 18: 0.72922075
	class 19: 0.87905973
	class 20: 0.31090868

federated global round: 29, step: 5
select part of clients to conduct local training
[6, 16, 13, 22]
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.35138052701950073, Reg Loss=0.7982529401779175
Clinet index 6, End of Epoch 1/6, Average Loss=1.1496334075927734, Class Loss=0.35138052701950073, Reg Loss=0.7982529401779175
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.3635839819908142, Reg Loss=0.8005703091621399
Clinet index 6, End of Epoch 2/6, Average Loss=1.164154291152954, Class Loss=0.3635839819908142, Reg Loss=0.8005703091621399
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Class Loss=0.3532356321811676, Reg Loss=0.798231303691864
Clinet index 6, End of Epoch 3/6, Average Loss=1.151466965675354, Class Loss=0.3532356321811676, Reg Loss=0.798231303691864
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Class Loss=0.35476556420326233, Reg Loss=0.7982786297798157
Clinet index 6, End of Epoch 4/6, Average Loss=1.1530442237854004, Class Loss=0.35476556420326233, Reg Loss=0.7982786297798157
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Class Loss=0.3608638644218445, Reg Loss=0.798358678817749
Clinet index 6, End of Epoch 5/6, Average Loss=1.1592226028442383, Class Loss=0.3608638644218445, Reg Loss=0.798358678817749
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Class Loss=0.3527163863182068, Reg Loss=0.7965313196182251
Clinet index 6, End of Epoch 6/6, Average Loss=1.149247646331787, Class Loss=0.3527163863182068, Reg Loss=0.7965313196182251
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3345796465873718, Reg Loss=0.8000149130821228
Clinet index 16, End of Epoch 1/6, Average Loss=1.1345945596694946, Class Loss=0.3345796465873718, Reg Loss=0.8000149130821228
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.32624784111976624, Reg Loss=0.8021136522293091
Clinet index 16, End of Epoch 2/6, Average Loss=1.128361463546753, Class Loss=0.32624784111976624, Reg Loss=0.8021136522293091
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.3239864408969879, Reg Loss=0.8012899160385132
Clinet index 16, End of Epoch 3/6, Average Loss=1.1252763271331787, Class Loss=0.3239864408969879, Reg Loss=0.8012899160385132
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.3227715492248535, Reg Loss=0.7998645305633545
Clinet index 16, End of Epoch 4/6, Average Loss=1.122636079788208, Class Loss=0.3227715492248535, Reg Loss=0.7998645305633545
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.3144508898258209, Reg Loss=0.8057796955108643
Clinet index 16, End of Epoch 5/6, Average Loss=1.1202305555343628, Class Loss=0.3144508898258209, Reg Loss=0.8057796955108643
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.3199322819709778, Reg Loss=0.7973544001579285
Clinet index 16, End of Epoch 6/6, Average Loss=1.1172866821289062, Class Loss=0.3199322819709778, Reg Loss=0.7973544001579285
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000372
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.38454386591911316, Reg Loss=0.7955547571182251
Clinet index 13, End of Epoch 1/6, Average Loss=1.1800986528396606, Class Loss=0.38454386591911316, Reg Loss=0.7955547571182251
Pseudo labeling is: None
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.3759087026119232, Reg Loss=0.7921887636184692
Clinet index 13, End of Epoch 2/6, Average Loss=1.1680974960327148, Class Loss=0.3759087026119232, Reg Loss=0.7921887636184692
Pseudo labeling is: None
Epoch 3, lr = 0.000258
Epoch 3, Class Loss=0.37618112564086914, Reg Loss=0.7983750104904175
Clinet index 13, End of Epoch 3/6, Average Loss=1.1745561361312866, Class Loss=0.37618112564086914, Reg Loss=0.7983750104904175
Pseudo labeling is: None
Epoch 4, lr = 0.000199
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.35796207189559937, Reg Loss=0.8002162575721741
Clinet index 13, End of Epoch 4/6, Average Loss=1.1581783294677734, Class Loss=0.35796207189559937, Reg Loss=0.8002162575721741
Pseudo labeling is: None
Epoch 5, lr = 0.000138
Epoch 5, Class Loss=0.36755481362342834, Reg Loss=0.800180971622467
Clinet index 13, End of Epoch 5/6, Average Loss=1.1677358150482178, Class Loss=0.36755481362342834, Reg Loss=0.800180971622467
Pseudo labeling is: None
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.3758210241794586, Reg Loss=0.7960115075111389
Clinet index 13, End of Epoch 6/6, Average Loss=1.17183256149292, Class Loss=0.3758210241794586, Reg Loss=0.7960115075111389
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3709665536880493, Reg Loss=0.7974253296852112
Clinet index 22, End of Epoch 1/6, Average Loss=1.1683919429779053, Class Loss=0.3709665536880493, Reg Loss=0.7974253296852112
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.35598769783973694, Reg Loss=0.8063011765480042
Clinet index 22, End of Epoch 2/6, Average Loss=1.1622889041900635, Class Loss=0.35598769783973694, Reg Loss=0.8063011765480042
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.3450552225112915, Reg Loss=0.8033275604248047
Clinet index 22, End of Epoch 3/6, Average Loss=1.1483827829360962, Class Loss=0.3450552225112915, Reg Loss=0.8033275604248047
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.3486635982990265, Reg Loss=0.8020952939987183
Clinet index 22, End of Epoch 4/6, Average Loss=1.1507588624954224, Class Loss=0.3486635982990265, Reg Loss=0.8020952939987183
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.3438686728477478, Reg Loss=0.8060643672943115
Clinet index 22, End of Epoch 5/6, Average Loss=1.149933099746704, Class Loss=0.3438686728477478, Reg Loss=0.8060643672943115
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.3305170238018036, Reg Loss=0.8024125099182129
Clinet index 22, End of Epoch 6/6, Average Loss=1.1329295635223389, Class Loss=0.3305170238018036, Reg Loss=0.8024125099182129
federated aggregation...
Validation, Class Loss=3.8905141353607178, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.265605
Mean Acc: 0.770216
FreqW Acc: 0.185172
Mean IoU: 0.493944
Class IoU:
	class 0: 0.0
	class 1: 0.6463071
	class 2: 0.374956
	class 3: 0.72663563
	class 4: 0.5103529
	class 5: 0.6911976
	class 6: 0.86152667
	class 7: 0.8279203
	class 8: 0.8581503
	class 9: 0.35707438
	class 10: 0.5648626
	class 11: 0.56558305
	class 12: 0.76021856
	class 13: 0.62196624
	class 14: 0.7539213
	class 15: 0.6564071
	class 16: 0.015188655
	class 17: 0.097005725
	class 18: 0.076500066
	class 19: 0.13444118
	class 20: 0.2726175
Class Acc:
	class 0: 0.0
	class 1: 0.9413872
	class 2: 0.94563353
	class 3: 0.7485737
	class 4: 0.8889776
	class 5: 0.83834654
	class 6: 0.92944115
	class 7: 0.89813167
	class 8: 0.918222
	class 9: 0.47842765
	class 10: 0.6025666
	class 11: 0.73501843
	class 12: 0.83107674
	class 13: 0.8228786
	class 14: 0.88518953
	class 15: 0.97328955
	class 16: 0.9231946
	class 17: 0.82831174
	class 18: 0.728024
	class 19: 0.8868577
	class 20: 0.37099606

voc_15-1_RCIL On GPUs 2
Run in 81041s
