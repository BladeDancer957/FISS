nohup: ignoring input
25
kvoc_4-4_OURS-FRC On GPUs 0\Writing in results/seed_2023-ov/2023-03-17_voc_4-4_OURS-FRC.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[6, 0, 7, 2]
Current Client Index:  6
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[8, 5, 3, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[5, 2, 0, 3]
Current Client Index:  5
Current Client Index:  2
Current Client Index:  0
Current Client Index:  3
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[3, 6, 1, 7]
Current Client Index:  3
Current Client Index:  6
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.09879439324140549, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.957759
Mean Acc: 0.761733
FreqW Acc: 0.923270
Mean IoU: 0.703965
Class IoU:
	class 0: 0.9539515191877496
	class 1: 0.7615770129403676
	class 2: 0.2054173196242969
	class 3: 0.8920301311931925
	class 4: 0.7068509839220561
Class Acc:
	class 0: 0.9848764620456163
	class 1: 0.7738026584549321
	class 2: 0.27733822617675347
	class 3: 0.9519182023560161
	class 4: 0.8207287142557349

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[0, 12, 6, 10]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/52, Loss=8.752467733621597
Loss made of: CE 0.9918259978294373, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.265520095825195 EntMin 0.0
Epoch 1, Batch 20/52, Loss=7.071468096971512
Loss made of: CE 0.8198803663253784, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.020350933074951 EntMin 0.0
Epoch 1, Batch 30/52, Loss=6.607319355010986
Loss made of: CE 0.7635049819946289, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.430253028869629 EntMin 0.0
Epoch 1, Batch 40/52, Loss=6.019214463233948
Loss made of: CE 0.5381882786750793, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.06054162979126 EntMin 0.0
Epoch 1, Batch 50/52, Loss=5.8994575381278995
Loss made of: CE 0.5589735507965088, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.742895603179932 EntMin 0.0
Epoch 1, Class Loss=0.8226524591445923, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.8226524591445923, Class Loss=0.8226524591445923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/52, Loss=5.5543346285820006
Loss made of: CE 0.5725143551826477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.77044677734375 EntMin 0.0
Epoch 2, Batch 20/52, Loss=5.267075511813164
Loss made of: CE 0.5973919630050659, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.66299295425415 EntMin 0.0
Epoch 2, Batch 30/52, Loss=5.399756535887718
Loss made of: CE 0.47862860560417175, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.642664909362793 EntMin 0.0
Epoch 2, Batch 40/52, Loss=5.13019385933876
Loss made of: CE 0.42485499382019043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.068973064422607 EntMin 0.0
Epoch 2, Batch 50/52, Loss=4.987418475747108
Loss made of: CE 0.5538339018821716, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0167036056518555 EntMin 0.0
Epoch 2, Class Loss=0.5377421975135803, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.5377421975135803, Class Loss=0.5377421975135803, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/52, Loss=5.044876977801323
Loss made of: CE 0.4064798355102539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.702230930328369 EntMin 0.0
Epoch 3, Batch 20/52, Loss=4.8011113703250885
Loss made of: CE 0.40322574973106384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.156904220581055 EntMin 0.0
Epoch 3, Batch 30/52, Loss=4.821800923347473
Loss made of: CE 0.46223849058151245, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7847583293914795 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.715130418539047
Loss made of: CE 0.35791587829589844, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9576029777526855 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.702215883135795
Loss made of: CE 0.3753891885280609, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.246007442474365 EntMin 0.0
Epoch 3, Class Loss=0.4324483275413513, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.4324483275413513, Class Loss=0.4324483275413513, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/52, Loss=4.805713057518005
Loss made of: CE 0.5796632766723633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.166640758514404 EntMin 0.0
Epoch 4, Batch 20/52, Loss=4.532645589113235
Loss made of: CE 0.3842937648296356, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.179682731628418 EntMin 0.0
Epoch 4, Batch 30/52, Loss=4.479267010092736
Loss made of: CE 0.27351078391075134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7171859741210938 EntMin 0.0
Epoch 4, Batch 40/52, Loss=4.462359893321991
Loss made of: CE 0.2814132869243622, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9026546478271484 EntMin 0.0
Epoch 4, Batch 50/52, Loss=4.26115072965622
Loss made of: CE 0.38570350408554077, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6811399459838867 EntMin 0.0
Epoch 4, Class Loss=0.370305597782135, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.370305597782135, Class Loss=0.370305597782135, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/52, Loss=4.365111240744591
Loss made of: CE 0.31834733486175537, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.620112419128418 EntMin 0.0
Epoch 5, Batch 20/52, Loss=4.35032608807087
Loss made of: CE 0.2966386675834656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8828351497650146 EntMin 0.0
Epoch 5, Batch 30/52, Loss=4.329030677676201
Loss made of: CE 0.4020206928253174, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8115477561950684 EntMin 0.0
Epoch 5, Batch 40/52, Loss=4.291888657212257
Loss made of: CE 0.3948715627193451, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332192420959473 EntMin 0.0
Epoch 5, Batch 50/52, Loss=4.2733728140592575
Loss made of: CE 0.3442016839981079, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8002424240112305 EntMin 0.0
Epoch 5, Class Loss=0.34685513377189636, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.34685513377189636, Class Loss=0.34685513377189636, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/52, Loss=4.079537090659142
Loss made of: CE 0.28223949670791626, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5804991722106934 EntMin 0.0
Epoch 6, Batch 20/52, Loss=4.283939242362976
Loss made of: CE 0.2953440546989441, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.457944393157959 EntMin 0.0
Epoch 6, Batch 30/52, Loss=4.389783635735512
Loss made of: CE 0.399446576833725, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.839301586151123 EntMin 0.0
Epoch 6, Batch 40/52, Loss=4.0229969918727875
Loss made of: CE 0.3373662829399109, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4491944313049316 EntMin 0.0
Epoch 6, Batch 50/52, Loss=4.181904041767121
Loss made of: CE 0.3928181529045105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1285481452941895 EntMin 0.0
Epoch 6, Class Loss=0.327837735414505, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.327837735414505, Class Loss=0.327837735414505, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=9.515335208177566
Loss made of: CE 0.9403616189956665, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.040741920471191 EntMin 0.0
Epoch 1, Batch 20/40, Loss=8.438762414455415
Loss made of: CE 0.6273688077926636, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.403205871582031 EntMin 0.0
Epoch 1, Batch 30/40, Loss=7.55564061999321
Loss made of: CE 0.6023046374320984, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.292862892150879 EntMin 0.0
Epoch 1, Batch 40/40, Loss=6.9830950736999515
Loss made of: CE 0.4407636523246765, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.380304336547852 EntMin 0.0
Epoch 1, Class Loss=0.7830581068992615, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.7830581068992615, Class Loss=0.7830581068992615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/40, Loss=6.798393669724464
Loss made of: CE 0.2729592025279999, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.500056266784668 EntMin 0.0
Epoch 2, Batch 20/40, Loss=6.26891188621521
Loss made of: CE 0.4990386962890625, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.104616641998291 EntMin 0.0
Epoch 2, Batch 30/40, Loss=6.085523572564125
Loss made of: CE 0.45847052335739136, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.425002574920654 EntMin 0.0
Epoch 2, Batch 40/40, Loss=6.243608057498932
Loss made of: CE 0.4825368821620941, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.133731842041016 EntMin 0.0
Epoch 2, Class Loss=0.49305328726768494, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.49305328726768494, Class Loss=0.49305328726768494, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/40, Loss=5.86498940885067
Loss made of: CE 0.3578603267669678, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.262143135070801 EntMin 0.0
Epoch 3, Batch 20/40, Loss=5.737694871425629
Loss made of: CE 0.5660461783409119, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.482999801635742 EntMin 0.0
Epoch 3, Batch 30/40, Loss=5.676789954304695
Loss made of: CE 0.33168870210647583, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.306806564331055 EntMin 0.0
Epoch 3, Batch 40/40, Loss=5.488353687524795
Loss made of: CE 0.29950517416000366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.907064914703369 EntMin 0.0
Epoch 3, Class Loss=0.4211871325969696, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.4211871325969696, Class Loss=0.4211871325969696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/40, Loss=5.334785306453705
Loss made of: CE 0.4167877435684204, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.429827690124512 EntMin 0.0
Epoch 4, Batch 20/40, Loss=5.373439116775989
Loss made of: CE 0.24662403762340546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.605441570281982 EntMin 0.0
Epoch 4, Batch 30/40, Loss=5.361398532986641
Loss made of: CE 0.343977689743042, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.057317733764648 EntMin 0.0
Epoch 4, Batch 40/40, Loss=5.439340862631798
Loss made of: CE 0.3547988831996918, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.212430477142334 EntMin 0.0
Epoch 4, Class Loss=0.37306708097457886, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.37306708097457886, Class Loss=0.37306708097457886, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/40, Loss=5.289914160966873
Loss made of: CE 0.3347764015197754, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.81961727142334 EntMin 0.0
Epoch 5, Batch 20/40, Loss=5.107814532518387
Loss made of: CE 0.4652822017669678, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.094526290893555 EntMin 0.0
Epoch 5, Batch 30/40, Loss=5.206400483846664
Loss made of: CE 0.26644232869148254, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4111328125 EntMin 0.0
Epoch 5, Batch 40/40, Loss=5.211576160788536
Loss made of: CE 0.3031008243560791, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.62140417098999 EntMin 0.0
Epoch 5, Class Loss=0.33974650502204895, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.33974650502204895, Class Loss=0.33974650502204895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/40, Loss=5.014813455939293
Loss made of: CE 0.29077818989753723, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.530639171600342 EntMin 0.0
Epoch 6, Batch 20/40, Loss=5.174643830955029
Loss made of: CE 0.30056899785995483, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.28094482421875 EntMin 0.0
Epoch 6, Batch 30/40, Loss=5.225505626201629
Loss made of: CE 0.31599774956703186, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.182210922241211 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.882375204563141
Loss made of: CE 0.21623091399669647, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.411389350891113 EntMin 0.0
Epoch 6, Class Loss=0.3253127634525299, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.3253127634525299, Class Loss=0.3253127634525299, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=9.975418710708619
Loss made of: CE 1.1576478481292725, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.164064407348633 EntMin 0.0
Epoch 1, Batch 20/34, Loss=8.452786910533906
Loss made of: CE 0.7596551775932312, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.399566650390625 EntMin 0.0
Epoch 1, Batch 30/34, Loss=8.032225966453552
Loss made of: CE 0.7597960233688354, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.771449089050293 EntMin 0.0
Epoch 1, Class Loss=0.9678666591644287, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.9678666591644287, Class Loss=0.9678666591644287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/34, Loss=7.185571712255478
Loss made of: CE 0.6732181310653687, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.34999942779541 EntMin 0.0
Epoch 2, Batch 20/34, Loss=6.599241292476654
Loss made of: CE 0.6719555854797363, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.584929466247559 EntMin 0.0
Epoch 2, Batch 30/34, Loss=6.3719638079404834
Loss made of: CE 0.5062929391860962, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.977882385253906 EntMin 0.0
Epoch 2, Class Loss=0.6232013702392578, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6232013702392578, Class Loss=0.6232013702392578, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/34, Loss=5.936092203855514
Loss made of: CE 0.5763556361198425, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.198175430297852 EntMin 0.0
Epoch 3, Batch 20/34, Loss=5.8593420535326
Loss made of: CE 0.48064398765563965, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.104248523712158 EntMin 0.0
Epoch 3, Batch 30/34, Loss=5.987543883919716
Loss made of: CE 0.4730646014213562, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.210683822631836 EntMin 0.0
Epoch 3, Class Loss=0.49124404788017273, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.49124404788017273, Class Loss=0.49124404788017273, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/34, Loss=5.6180899202823635
Loss made of: CE 0.42974337935447693, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.172372817993164 EntMin 0.0
Epoch 4, Batch 20/34, Loss=5.544083961844445
Loss made of: CE 0.38499459624290466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679718494415283 EntMin 0.0
Epoch 4, Batch 30/34, Loss=5.585513529181481
Loss made of: CE 0.5065625905990601, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.715354919433594 EntMin 0.0
Epoch 4, Class Loss=0.41769954562187195, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.41769954562187195, Class Loss=0.41769954562187195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/34, Loss=5.313112819194794
Loss made of: CE 0.3280513882637024, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.484479904174805 EntMin 0.0
Epoch 5, Batch 20/34, Loss=5.432876136898995
Loss made of: CE 0.40793412923812866, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5348100662231445 EntMin 0.0
Epoch 5, Batch 30/34, Loss=5.1069972038269045
Loss made of: CE 0.31951138377189636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.712525367736816 EntMin 0.0
Epoch 5, Class Loss=0.36161670088768005, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.36161670088768005, Class Loss=0.36161670088768005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/34, Loss=5.179699563980103
Loss made of: CE 0.37705251574516296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.883464336395264 EntMin 0.0
Epoch 6, Batch 20/34, Loss=5.156857028603554
Loss made of: CE 0.39828047156333923, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.630858898162842 EntMin 0.0
Epoch 6, Batch 30/34, Loss=5.238612857460976
Loss made of: CE 0.2913894057273865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.525514602661133 EntMin 0.0
Epoch 6, Class Loss=0.3416317105293274, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.3416317105293274, Class Loss=0.3416317105293274, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=9.198105430603027
Loss made of: CE 1.234454870223999, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.415727138519287 EntMin 0.0
Epoch 1, Batch 20/33, Loss=7.913147538900375
Loss made of: CE 0.9809482097625732, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.167609214782715 EntMin 0.0
Epoch 1, Batch 30/33, Loss=6.742008709907532
Loss made of: CE 0.7359100580215454, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.009740352630615 EntMin 0.0
Epoch 1, Class Loss=0.9620141983032227, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.9620141983032227, Class Loss=0.9620141983032227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/33, Loss=6.265403831005097
Loss made of: CE 0.71880042552948, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.187707901000977 EntMin 0.0
Epoch 2, Batch 20/33, Loss=5.921865439414978
Loss made of: CE 0.8086518049240112, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.57758092880249 EntMin 0.0
Epoch 2, Batch 30/33, Loss=5.498758625984192
Loss made of: CE 0.5936307311058044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.865837097167969 EntMin 0.0
Epoch 2, Class Loss=0.679638683795929, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.679638683795929, Class Loss=0.679638683795929, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/33, Loss=5.197088572382927
Loss made of: CE 0.5579771995544434, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.445017337799072 EntMin 0.0
Epoch 3, Batch 20/33, Loss=5.168282219767571
Loss made of: CE 0.4844653010368347, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.361411094665527 EntMin 0.0
Epoch 3, Batch 30/33, Loss=5.021446168422699
Loss made of: CE 0.5074114203453064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.262438774108887 EntMin 0.0
Epoch 3, Class Loss=0.5378361940383911, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5378361940383911, Class Loss=0.5378361940383911, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/33, Loss=4.735887822508812
Loss made of: CE 0.45160728693008423, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9209089279174805 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.856747263669968
Loss made of: CE 0.4469076991081238, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.286026954650879 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.670750522613526
Loss made of: CE 0.3679540753364563, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8984851837158203 EntMin 0.0
Epoch 4, Class Loss=0.45279785990715027, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.45279785990715027, Class Loss=0.45279785990715027, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/33, Loss=4.560293951630593
Loss made of: CE 0.48721787333488464, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.191690444946289 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.45785576403141
Loss made of: CE 0.316899836063385, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.788475513458252 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.647117733955383
Loss made of: CE 0.42864280939102173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9531354904174805 EntMin 0.0
Epoch 5, Class Loss=0.38887467980384827, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.38887467980384827, Class Loss=0.38887467980384827, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/33, Loss=4.404779404401779
Loss made of: CE 0.43672049045562744, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143805027008057 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.450313699245453
Loss made of: CE 0.3314489722251892, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.864025831222534 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.299697506427765
Loss made of: CE 0.350820392370224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.226143836975098 EntMin 0.0
Epoch 6, Class Loss=0.358571857213974, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.358571857213974, Class Loss=0.358571857213974, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.40849849581718445, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.845724
Mean Acc: 0.488532
FreqW Acc: 0.722575
Mean IoU: 0.393047
Class IoU:
	class 0: 0.8447371
	class 1: 0.7522287
	class 2: 0.28622928
	class 3: 0.63399637
	class 4: 0.5728478
	class 5: 0.0
	class 6: 0.0011592718
	class 7: 7.264225e-05
	class 8: 0.4461556
Class Acc:
	class 0: 0.9872604
	class 1: 0.809788
	class 2: 0.46192002
	class 3: 0.88962215
	class 4: 0.78221345
	class 5: 0.0
	class 6: 0.0011592718
	class 7: 7.264225e-05
	class 8: 0.4647545

federated global round: 6, step: 1
select part of clients to conduct local training
[11, 5, 8, 12]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/43, Loss=5.175743299722671
Loss made of: CE 0.4007955491542816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.753983497619629 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.729101672768593
Loss made of: CE 0.5729166269302368, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9079720973968506 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.55327759385109
Loss made of: CE 0.3809755742549896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.669314384460449 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.481260806322098
Loss made of: CE 0.40879178047180176, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.741147518157959 EntMin 0.0
Epoch 1, Class Loss=0.42878785729408264, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.42878785729408264, Class Loss=0.42878785729408264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/43, Loss=4.3029116809368135
Loss made of: CE 0.3622031807899475, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.86470365524292 EntMin 0.0
Epoch 2, Batch 20/43, Loss=4.161423560976982
Loss made of: CE 0.35937485098838806, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8254756927490234 EntMin 0.0
Epoch 2, Batch 30/43, Loss=4.198078510165215
Loss made of: CE 0.4287368059158325, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.263018608093262 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.262148529291153
Loss made of: CE 0.4014102816581726, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.082439422607422 EntMin 0.0
Epoch 2, Class Loss=0.35552701354026794, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.35552701354026794, Class Loss=0.35552701354026794, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/43, Loss=3.984434422850609
Loss made of: CE 0.4188781678676605, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5557003021240234 EntMin 0.0
Epoch 3, Batch 20/43, Loss=4.063380183279515
Loss made of: CE 0.3147367238998413, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.97590970993042 EntMin 0.0
Epoch 3, Batch 30/43, Loss=4.063528873026371
Loss made of: CE 0.3474540114402771, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6393375396728516 EntMin 0.0
Epoch 3, Batch 40/43, Loss=4.002265359461307
Loss made of: CE 0.23099379241466522, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.817432165145874 EntMin 0.0
Epoch 3, Class Loss=0.3292803168296814, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.3292803168296814, Class Loss=0.3292803168296814, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/43, Loss=3.886506313085556
Loss made of: CE 0.39223968982696533, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.038218021392822 EntMin 0.0
Epoch 4, Batch 20/43, Loss=3.924718776345253
Loss made of: CE 0.33809131383895874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.526951551437378 EntMin 0.0
Epoch 4, Batch 30/43, Loss=3.953055191040039
Loss made of: CE 0.3533264994621277, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.472034215927124 EntMin 0.0
Epoch 4, Batch 40/43, Loss=3.924901467561722
Loss made of: CE 0.2797016501426697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3592100143432617 EntMin 0.0
Epoch 4, Class Loss=0.3137019872665405, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.3137019872665405, Class Loss=0.3137019872665405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/43, Loss=3.909422901272774
Loss made of: CE 0.298808753490448, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.873467206954956 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.8711473509669303
Loss made of: CE 0.35911092162132263, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5830674171447754 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.9036283046007156
Loss made of: CE 0.3071961998939514, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6056199073791504 EntMin 0.0
Epoch 5, Batch 40/43, Loss=3.9426514178514482
Loss made of: CE 0.32821720838546753, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2290055751800537 EntMin 0.0
Epoch 5, Class Loss=0.31535089015960693, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.31535089015960693, Class Loss=0.31535089015960693, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/43, Loss=3.8057241916656492
Loss made of: CE 0.34286901354789734, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3247623443603516 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.8876686722040175
Loss made of: CE 0.3132407069206238, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4580702781677246 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.8051146447658537
Loss made of: CE 0.2537422180175781, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2059335708618164 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.68302790671587
Loss made of: CE 0.34959882497787476, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.353497266769409 EntMin 0.0
Epoch 6, Class Loss=0.3002218008041382, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.3002218008041382, Class Loss=0.3002218008041382, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=5.613297471404076
Loss made of: CE 0.5575637817382812, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.049592971801758 EntMin 0.0
Epoch 1, Batch 20/34, Loss=5.489105060696602
Loss made of: CE 0.3833061456680298, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.200068950653076 EntMin 0.0
Epoch 1, Batch 30/34, Loss=5.314906187355518
Loss made of: CE 0.24402308464050293, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.018054962158203 EntMin 0.0
Epoch 1, Class Loss=0.40577223896980286, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.40577223896980286, Class Loss=0.40577223896980286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/34, Loss=5.170097978413105
Loss made of: CE 0.33107465505599976, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.792483806610107 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.946415582299233
Loss made of: CE 0.30600500106811523, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.76875638961792 EntMin 0.0
Epoch 2, Batch 30/34, Loss=5.260015839338303
Loss made of: CE 0.3354529142379761, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.583930492401123 EntMin 0.0
Epoch 2, Class Loss=0.3212694227695465, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3212694227695465, Class Loss=0.3212694227695465, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/34, Loss=4.76740266084671
Loss made of: CE 0.3482840061187744, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.457588195800781 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.914917907118797
Loss made of: CE 0.2772808074951172, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.541959762573242 EntMin 0.0
Epoch 3, Batch 30/34, Loss=5.0019187301397325
Loss made of: CE 0.3710782527923584, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3419084548950195 EntMin 0.0
Epoch 3, Class Loss=0.29483839869499207, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.29483839869499207, Class Loss=0.29483839869499207, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/34, Loss=4.8812382340431215
Loss made of: CE 0.28486180305480957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.505806922912598 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.905629602074623
Loss made of: CE 0.3030017018318176, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.660632133483887 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.750179407000542
Loss made of: CE 0.28321170806884766, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.979949951171875 EntMin 0.0
Epoch 4, Class Loss=0.28754815459251404, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.28754815459251404, Class Loss=0.28754815459251404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/34, Loss=4.624903228878975
Loss made of: CE 0.27488696575164795, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.579427242279053 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.739630743861198
Loss made of: CE 0.2928552031517029, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.171750068664551 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.667561818659306
Loss made of: CE 0.3277684450149536, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.881015300750732 EntMin 0.0
Epoch 5, Class Loss=0.2853163182735443, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.2853163182735443, Class Loss=0.2853163182735443, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/34, Loss=4.808760875463486
Loss made of: CE 0.36204132437705994, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.131287574768066 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.581749247014523
Loss made of: CE 0.24838516116142273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500720024108887 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.5272852927446365
Loss made of: CE 0.46732959151268005, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.094242572784424 EntMin 0.0
Epoch 6, Class Loss=0.2951449155807495, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.2951449155807495, Class Loss=0.2951449155807495, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=5.707808890938759
Loss made of: CE 0.39732274413108826, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.373030662536621 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.547817641496659
Loss made of: CE 0.26329800486564636, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3379621505737305 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.525119183957576
Loss made of: CE 0.3332386016845703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.791585445404053 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.182926514744759
Loss made of: CE 0.20289981365203857, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3035783767700195 EntMin 0.0
Epoch 1, Class Loss=0.34591394662857056, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.34591394662857056, Class Loss=0.34591394662857056, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/40, Loss=5.239741340279579
Loss made of: CE 0.3710610270500183, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.557437419891357 EntMin 0.0
Epoch 2, Batch 20/40, Loss=4.905246345698833
Loss made of: CE 0.17307698726654053, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.436011791229248 EntMin 0.0
Epoch 2, Batch 30/40, Loss=5.218632289767266
Loss made of: CE 0.4029715955257416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.84344482421875 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.097203783690929
Loss made of: CE 0.30793067812919617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.976496696472168 EntMin 0.0
Epoch 2, Class Loss=0.3160885274410248, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.3160885274410248, Class Loss=0.3160885274410248, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/40, Loss=4.965340021252632
Loss made of: CE 0.2551294267177582, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.678424835205078 EntMin 0.0
Epoch 3, Batch 20/40, Loss=4.80704238563776
Loss made of: CE 0.27720803022384644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4472503662109375 EntMin 0.0
Epoch 3, Batch 30/40, Loss=4.946598142385483
Loss made of: CE 0.4038386940956116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249234676361084 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.895478875935078
Loss made of: CE 0.261167973279953, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.054384708404541 EntMin 0.0
Epoch 3, Class Loss=0.29596301913261414, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.29596301913261414, Class Loss=0.29596301913261414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/40, Loss=4.835325522720813
Loss made of: CE 0.40048640966415405, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7190423011779785 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.711622275412083
Loss made of: CE 0.32701611518859863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.490753173828125 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.794964665174485
Loss made of: CE 0.29114386439323425, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.991822242736816 EntMin 0.0
Epoch 4, Batch 40/40, Loss=4.7519072338938715
Loss made of: CE 0.2758806347846985, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.209148406982422 EntMin 0.0
Epoch 4, Class Loss=0.2908085286617279, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.2908085286617279, Class Loss=0.2908085286617279, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/40, Loss=4.7084542542696
Loss made of: CE 0.30303534865379333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.126062393188477 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.753865832090378
Loss made of: CE 0.3357616364955902, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.151315689086914 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.525872167944908
Loss made of: CE 0.2903081178665161, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.491960525512695 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.687438753247261
Loss made of: CE 0.2467440366744995, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4182233810424805 EntMin 0.0
Epoch 5, Class Loss=0.28972598910331726, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.28972598910331726, Class Loss=0.28972598910331726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/40, Loss=4.469689220190048
Loss made of: CE 0.24556730687618256, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.387016773223877 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.6201762959361075
Loss made of: CE 0.2532409131526947, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.118785381317139 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.493762646615505
Loss made of: CE 0.27044054865837097, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1739678382873535 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.589028753340244
Loss made of: CE 0.25303083658218384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.018251419067383 EntMin 0.0
Epoch 6, Class Loss=0.2753654420375824, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.2753654420375824, Class Loss=0.2753654420375824, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/40, Loss=5.756228968501091
Loss made of: CE 0.44561994075775146, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3861799240112305 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.641775062680244
Loss made of: CE 0.44699472188949585, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.080118179321289 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.470930454134941
Loss made of: CE 0.5542328953742981, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.858649253845215 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.399584099650383
Loss made of: CE 0.39132747054100037, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.026682376861572 EntMin 0.0
Epoch 1, Class Loss=0.47926145792007446, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.47926145792007446, Class Loss=0.47926145792007446, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/40, Loss=5.3652369171381
Loss made of: CE 0.23070555925369263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.344731330871582 EntMin 0.0
Epoch 2, Batch 20/40, Loss=5.159763586521149
Loss made of: CE 0.34412533044815063, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.286790370941162 EntMin 0.0
Epoch 2, Batch 30/40, Loss=5.057569196820259
Loss made of: CE 0.4555990695953369, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.445214748382568 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.276702833175659
Loss made of: CE 0.348942369222641, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.287839412689209 EntMin 0.0
Epoch 2, Class Loss=0.3780014216899872, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.3780014216899872, Class Loss=0.3780014216899872, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/40, Loss=5.101743486523628
Loss made of: CE 0.2781918942928314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.523443222045898 EntMin 0.0
Epoch 3, Batch 20/40, Loss=5.03566360771656
Loss made of: CE 0.5127007365226746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.768694877624512 EntMin 0.0
Epoch 3, Batch 30/40, Loss=5.003197558224201
Loss made of: CE 0.2710992097854614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.763038635253906 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.884581550955772
Loss made of: CE 0.2738751769065857, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.485289096832275 EntMin 0.0
Epoch 3, Class Loss=0.34765878319740295, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.34765878319740295, Class Loss=0.34765878319740295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/40, Loss=4.744102859497071
Loss made of: CE 0.35209691524505615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.314895153045654 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.891300466656685
Loss made of: CE 0.22440847754478455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.166602611541748 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.805923290550709
Loss made of: CE 0.31763243675231934, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.736026763916016 EntMin 0.0
Epoch 4, Batch 40/40, Loss=4.99821335375309
Loss made of: CE 0.3099147081375122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.802949905395508 EntMin 0.0
Epoch 4, Class Loss=0.324802964925766, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.324802964925766, Class Loss=0.324802964925766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/40, Loss=4.825412328541279
Loss made of: CE 0.27838051319122314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.376667499542236 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.655248442292214
Loss made of: CE 0.41754814982414246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.699810028076172 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.744354492425918
Loss made of: CE 0.24944254755973816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.180812358856201 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.68922338783741
Loss made of: CE 0.2711517810821533, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.201828956604004 EntMin 0.0
Epoch 5, Class Loss=0.29775819182395935, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.29775819182395935, Class Loss=0.29775819182395935, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/40, Loss=4.552288952469826
Loss made of: CE 0.2736561894416809, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.435333251953125 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.730230283737183
Loss made of: CE 0.2621079683303833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.711489677429199 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.806607995927334
Loss made of: CE 0.3195943832397461, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.07199764251709 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.464781820774078
Loss made of: CE 0.22054404020309448, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.146080017089844 EntMin 0.0
Epoch 6, Class Loss=0.28613516688346863, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.28613516688346863, Class Loss=0.28613516688346863, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3184078335762024, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.872019
Mean Acc: 0.549058
FreqW Acc: 0.768505
Mean IoU: 0.462723
Class IoU:
	class 0: 0.86833304
	class 1: 0.78130156
	class 2: 0.31473944
	class 3: 0.58582157
	class 4: 0.6083328
	class 5: 0.0006452526
	class 6: 0.283953
	class 7: 0.040824115
	class 8: 0.68055993
Class Acc:
	class 0: 0.98093385
	class 1: 0.844995
	class 2: 0.54950386
	class 3: 0.60999405
	class 4: 0.77694094
	class 5: 0.00064529834
	class 6: 0.285091
	class 7: 0.040885884
	class 8: 0.8525343

federated global round: 7, step: 1
select part of clients to conduct local training
[0, 6, 13, 5]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/52, Loss=4.45150939822197
Loss made of: CE 0.6259680986404419, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9742398262023926 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.167260029911995
Loss made of: CE 0.37178537249565125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5531814098358154 EntMin 0.0
Epoch 1, Batch 30/52, Loss=4.099467906355858
Loss made of: CE 0.3373115062713623, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.592811346054077 EntMin 0.0
Epoch 1, Batch 40/52, Loss=4.222313493490219
Loss made of: CE 0.37007570266723633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.194338798522949 EntMin 0.0
Epoch 1, Batch 50/52, Loss=4.079548457264901
Loss made of: CE 0.2721336781978607, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.412252187728882 EntMin 0.0
Epoch 1, Class Loss=0.39901304244995117, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.39901304244995117, Class Loss=0.39901304244995117, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/52, Loss=4.1240583032369615
Loss made of: CE 0.37342962622642517, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6264781951904297 EntMin 0.0
Epoch 2, Batch 20/52, Loss=4.001675036549568
Loss made of: CE 0.3481524884700775, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.453948497772217 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.1806103706359865
Loss made of: CE 0.4411543309688568, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.894148826599121 EntMin 0.0
Epoch 2, Batch 40/52, Loss=4.101137647032738
Loss made of: CE 0.2979379892349243, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.352581024169922 EntMin 0.0
Epoch 2, Batch 50/52, Loss=4.061659249663353
Loss made of: CE 0.3847061097621918, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.292975425720215 EntMin 0.0
Epoch 2, Class Loss=0.33698222041130066, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.33698222041130066, Class Loss=0.33698222041130066, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/52, Loss=4.04576755464077
Loss made of: CE 0.27565211057662964, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7832274436950684 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.9962897330522535
Loss made of: CE 0.3291330337524414, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.555218458175659 EntMin 0.0
Epoch 3, Batch 30/52, Loss=4.083070474863052
Loss made of: CE 0.3638845682144165, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3386383056640625 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.006796409189701
Loss made of: CE 0.2773427963256836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.63022518157959 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.015560612082481
Loss made of: CE 0.3203985095024109, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.526066303253174 EntMin 0.0
Epoch 3, Class Loss=0.32115718722343445, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.32115718722343445, Class Loss=0.32115718722343445, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/52, Loss=4.159131436049938
Loss made of: CE 0.39431631565093994, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.624840259552002 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.8044816315174104
Loss made of: CE 0.2555670440196991, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5765457153320312 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.822711481153965
Loss made of: CE 0.20833878219127655, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2045304775238037 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.875846591591835
Loss made of: CE 0.2477792203426361, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2395777702331543 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.67220628708601
Loss made of: CE 0.2676827311515808, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2125773429870605 EntMin 0.0
Epoch 4, Class Loss=0.2924211621284485, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.2924211621284485, Class Loss=0.2924211621284485, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/52, Loss=3.830134631693363
Loss made of: CE 0.2553700804710388, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.356952428817749 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.801818823814392
Loss made of: CE 0.24343079328536987, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.333387851715088 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.8525425255298615
Loss made of: CE 0.3128072917461395, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.525979518890381 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.805697563290596
Loss made of: CE 0.32956987619400024, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8074376583099365 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.756607362627983
Loss made of: CE 0.2725335955619812, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.343784809112549 EntMin 0.0
Epoch 5, Class Loss=0.28888455033302307, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.28888455033302307, Class Loss=0.28888455033302307, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/52, Loss=3.697783449292183
Loss made of: CE 0.2553616166114807, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.297912359237671 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.8101139426231385
Loss made of: CE 0.2628106474876404, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.146423816680908 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.9961747229099274
Loss made of: CE 0.33568045496940613, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5246992111206055 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.575478899478912
Loss made of: CE 0.2831389009952545, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3176684379577637 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.755758395791054
Loss made of: CE 0.28930675983428955, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5999715328216553 EntMin 0.0
Epoch 6, Class Loss=0.28106603026390076, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.28106603026390076, Class Loss=0.28106603026390076, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/34, Loss=5.344973421096801
Loss made of: CE 0.567251443862915, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.764762878417969 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.773484063148499
Loss made of: CE 0.29975271224975586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.037827491760254 EntMin 0.0
Epoch 1, Batch 30/34, Loss=5.037704369425773
Loss made of: CE 0.36958301067352295, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478107929229736 EntMin 0.0
Epoch 1, Class Loss=0.43607228994369507, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.43607228994369507, Class Loss=0.43607228994369507, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/34, Loss=4.7632242202758786
Loss made of: CE 0.3084943890571594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581774711608887 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.55348306298256
Loss made of: CE 0.35178321599960327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.140303611755371 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.518444427847863
Loss made of: CE 0.26885002851486206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.368391990661621 EntMin 0.0
Epoch 2, Class Loss=0.31774264574050903, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.31774264574050903, Class Loss=0.31774264574050903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/34, Loss=4.401371394097805
Loss made of: CE 0.31274664402008057, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.015428066253662 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.471118432283402
Loss made of: CE 0.2666374444961548, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.120759010314941 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.57172084748745
Loss made of: CE 0.34492728114128113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.116856575012207 EntMin 0.0
Epoch 3, Class Loss=0.30309754610061646, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.30309754610061646, Class Loss=0.30309754610061646, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/34, Loss=4.490761092305183
Loss made of: CE 0.290501207113266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0810546875 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.495449475944042
Loss made of: CE 0.31369897723197937, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9555506706237793 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.548585951328278
Loss made of: CE 0.3923085629940033, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.632628440856934 EntMin 0.0
Epoch 4, Class Loss=0.3046661615371704, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3046661615371704, Class Loss=0.3046661615371704, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/34, Loss=4.305647256970405
Loss made of: CE 0.25839442014694214, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.686657428741455 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.481960846483707
Loss made of: CE 0.4938713312149048, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.307199954986572 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.195443151891231
Loss made of: CE 0.26980528235435486, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.979569435119629 EntMin 0.0
Epoch 5, Class Loss=0.29508256912231445, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.29508256912231445, Class Loss=0.29508256912231445, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/34, Loss=4.336758647859097
Loss made of: CE 0.3320413827896118, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.123848915100098 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.2827507853508
Loss made of: CE 0.2930727005004883, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.102287292480469 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.338003970682621
Loss made of: CE 0.2243569791316986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.638888120651245 EntMin 0.0
Epoch 6, Class Loss=0.279416024684906, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.279416024684906, Class Loss=0.279416024684906, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=6.480957639217377
Loss made of: CE 0.5415077209472656, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.118066310882568 EntMin 0.0
Epoch 1, Batch 20/33, Loss=5.483041769266128
Loss made of: CE 0.6723462343215942, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.124778747558594 EntMin 0.0
Epoch 1, Batch 30/33, Loss=5.046677741408348
Loss made of: CE 0.3431525230407715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.436305046081543 EntMin 0.0
Epoch 1, Class Loss=0.5882641673088074, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5882641673088074, Class Loss=0.5882641673088074, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/33, Loss=4.779525774717331
Loss made of: CE 0.3690827786922455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.227725505828857 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.628163039684296
Loss made of: CE 0.2926989495754242, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.220981121063232 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.387634414434433
Loss made of: CE 0.35222476720809937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.213649749755859 EntMin 0.0
Epoch 2, Class Loss=0.3319912552833557, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.3319912552833557, Class Loss=0.3319912552833557, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/33, Loss=4.381562674045563
Loss made of: CE 0.3182239830493927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.584242343902588 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.3187123328447345
Loss made of: CE 0.2589695453643799, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.031979084014893 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.160633319616318
Loss made of: CE 0.2789168357849121, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7846553325653076 EntMin 0.0
Epoch 3, Class Loss=0.29883575439453125, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.29883575439453125, Class Loss=0.29883575439453125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/33, Loss=4.080651819705963
Loss made of: CE 0.24012736976146698, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7160747051239014 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.169480000436306
Loss made of: CE 0.29389578104019165, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.086738586425781 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.062038516998291
Loss made of: CE 0.2599824368953705, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5755109786987305 EntMin 0.0
Epoch 4, Class Loss=0.2765219211578369, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.2765219211578369, Class Loss=0.2765219211578369, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/33, Loss=4.1472083806991575
Loss made of: CE 0.3068423271179199, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.502824306488037 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.014630889892578
Loss made of: CE 0.2756772041320801, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.499647855758667 EntMin 0.0
Epoch 5, Batch 30/33, Loss=3.917305515706539
Loss made of: CE 0.293088436126709, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.781388282775879 EntMin 0.0
Epoch 5, Class Loss=0.27488473057746887, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.27488473057746887, Class Loss=0.27488473057746887, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/33, Loss=4.00171220600605
Loss made of: CE 0.3264648914337158, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7839322090148926 EntMin 0.0
Epoch 6, Batch 20/33, Loss=3.9434457615017893
Loss made of: CE 0.20783504843711853, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4653754234313965 EntMin 0.0
Epoch 6, Batch 30/33, Loss=3.8267165422439575
Loss made of: CE 0.30388009548187256, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.853199005126953 EntMin 0.0
Epoch 6, Class Loss=0.28095826506614685, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.28095826506614685, Class Loss=0.28095826506614685, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/34, Loss=5.270660871267319
Loss made of: CE 0.5815361738204956, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.867432594299316 EntMin 0.0
Epoch 1, Batch 20/34, Loss=5.069509220123291
Loss made of: CE 0.39037641882896423, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698527812957764 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.848285594582558
Loss made of: CE 0.3560637831687927, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.466376781463623 EntMin 0.0
Epoch 1, Class Loss=0.4277399182319641, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.4277399182319641, Class Loss=0.4277399182319641, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/34, Loss=4.788089323043823
Loss made of: CE 0.2997959852218628, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.312555313110352 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.5585970968008045
Loss made of: CE 0.3332765996456146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594231128692627 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.89316778331995
Loss made of: CE 0.36913570761680603, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.161584854125977 EntMin 0.0
Epoch 2, Class Loss=0.3384242653846741, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3384242653846741, Class Loss=0.3384242653846741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/34, Loss=4.4002313077449795
Loss made of: CE 0.3104063868522644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.039921283721924 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.5200785711407665
Loss made of: CE 0.26949742436408997, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.084804534912109 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.617604053020477
Loss made of: CE 0.32722124457359314, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.040421485900879 EntMin 0.0
Epoch 3, Class Loss=0.29791730642318726, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.29791730642318726, Class Loss=0.29791730642318726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/34, Loss=4.475504265725613
Loss made of: CE 0.282839834690094, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0179219245910645 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.5419537961483005
Loss made of: CE 0.3103778660297394, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.331465244293213 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.473664797842503
Loss made of: CE 0.27701637148857117, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8112447261810303 EntMin 0.0
Epoch 4, Class Loss=0.29144036769866943, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.29144036769866943, Class Loss=0.29144036769866943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/34, Loss=4.37994597107172
Loss made of: CE 0.2559730112552643, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.08731746673584 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.480176071822643
Loss made of: CE 0.2744916081428528, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.095938682556152 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.40807529091835
Loss made of: CE 0.2900797724723816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.414789199829102 EntMin 0.0
Epoch 5, Class Loss=0.28542056679725647, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.28542056679725647, Class Loss=0.28542056679725647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/34, Loss=4.6197804540395735
Loss made of: CE 0.3305184841156006, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9634006023406982 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.379210315644741
Loss made of: CE 0.2063283771276474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.127554416656494 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.295547565817833
Loss made of: CE 0.3678058087825775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5113630294799805 EntMin 0.0
Epoch 6, Class Loss=0.2853192389011383, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.2853192389011383, Class Loss=0.2853192389011383, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.27203547954559326, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.914022
Mean Acc: 0.668296
FreqW Acc: 0.845659
Mean IoU: 0.580292
Class IoU:
	class 0: 0.9158316
	class 1: 0.82345366
	class 2: 0.31434664
	class 3: 0.4408118
	class 4: 0.6761873
	class 5: 0.0
	class 6: 0.8252237
	class 7: 0.5694977
	class 8: 0.65727866
Class Acc:
	class 0: 0.9771912
	class 1: 0.86543924
	class 2: 0.56184584
	class 3: 0.4519871
	class 4: 0.8061388
	class 5: 0.0
	class 6: 0.8930525
	class 7: 0.5943525
	class 8: 0.86465716

federated global round: 8, step: 1
select part of clients to conduct local training
[4, 11, 9, 6]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=5.440545858442784
Loss made of: CE 0.46221280097961426, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.620381832122803 EntMin 0.0
Epoch 1, Batch 20/25, Loss=5.047661167383194
Loss made of: CE 0.31274211406707764, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.338325500488281 EntMin 0.0
Epoch 1, Class Loss=0.38881710171699524, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.38881710171699524, Class Loss=0.38881710171699524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/25, Loss=4.65930173844099
Loss made of: CE 0.2952602803707123, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.963006019592285 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.565284559130669
Loss made of: CE 0.3587113618850708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.477013111114502 EntMin 0.0
Epoch 2, Class Loss=0.3052197992801666, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.3052197992801666, Class Loss=0.3052197992801666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/25, Loss=4.4573188230395315
Loss made of: CE 0.30153387784957886, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.062686920166016 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.256740255653858
Loss made of: CE 0.3153432011604309, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.031599521636963 EntMin 0.0
Epoch 3, Class Loss=0.3022865355014801, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.3022865355014801, Class Loss=0.3022865355014801, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/25, Loss=4.369326402246952
Loss made of: CE 0.28762704133987427, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.160078525543213 EntMin 0.0
Epoch 4, Batch 20/25, Loss=4.172065302729607
Loss made of: CE 0.20073531568050385, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.032558441162109 EntMin 0.0
Epoch 4, Class Loss=0.27270233631134033, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.27270233631134033, Class Loss=0.27270233631134033, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/25, Loss=4.1761964216828344
Loss made of: CE 0.2160896509885788, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7282016277313232 EntMin 0.0
Epoch 5, Batch 20/25, Loss=4.3483838617801664
Loss made of: CE 0.48223018646240234, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.691863059997559 EntMin 0.0
Epoch 5, Class Loss=0.2929609715938568, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.2929609715938568, Class Loss=0.2929609715938568, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/25, Loss=4.056573474407196
Loss made of: CE 0.25227057933807373, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.925616979598999 EntMin 0.0
Epoch 6, Batch 20/25, Loss=4.12541515827179
Loss made of: CE 0.30746304988861084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.228724479675293 EntMin 0.0
Epoch 6, Class Loss=0.25758543610572815, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.25758543610572815, Class Loss=0.25758543610572815, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/43, Loss=5.101174888014794
Loss made of: CE 0.46543702483177185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.589825630187988 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.56018545627594
Loss made of: CE 0.5139623880386353, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.861189842224121 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.361296674609184
Loss made of: CE 0.31213414669036865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.356266498565674 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.205564641952515
Loss made of: CE 0.39218753576278687, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5396153926849365 EntMin 0.0
Epoch 1, Class Loss=0.4463476538658142, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.4463476538658142, Class Loss=0.4463476538658142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/43, Loss=4.06696797311306
Loss made of: CE 0.3255041241645813, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5637054443359375 EntMin 0.0
Epoch 2, Batch 20/43, Loss=3.9908306777477263
Loss made of: CE 0.3191153109073639, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8195767402648926 EntMin 0.0
Epoch 2, Batch 30/43, Loss=3.9425537928938867
Loss made of: CE 0.41676458716392517, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1061296463012695 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.121565175056458
Loss made of: CE 0.37386107444763184, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.878148078918457 EntMin 0.0
Epoch 2, Class Loss=0.33211398124694824, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.33211398124694824, Class Loss=0.33211398124694824, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/43, Loss=3.786637020111084
Loss made of: CE 0.34223753213882446, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2804694175720215 EntMin 0.0
Epoch 3, Batch 20/43, Loss=3.9398702681064606
Loss made of: CE 0.3037277162075043, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.868773937225342 EntMin 0.0
Epoch 3, Batch 30/43, Loss=3.926445743441582
Loss made of: CE 0.35615047812461853, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5437121391296387 EntMin 0.0
Epoch 3, Batch 40/43, Loss=3.855097970366478
Loss made of: CE 0.20299145579338074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6412830352783203 EntMin 0.0
Epoch 3, Class Loss=0.31083032488822937, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.31083032488822937, Class Loss=0.31083032488822937, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/43, Loss=3.7449648857116697
Loss made of: CE 0.30496275424957275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.861802577972412 EntMin 0.0
Epoch 4, Batch 20/43, Loss=3.7763588309288023
Loss made of: CE 0.3198525905609131, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.275385618209839 EntMin 0.0
Epoch 4, Batch 30/43, Loss=3.7694236814975737
Loss made of: CE 0.34811705350875854, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.434330463409424 EntMin 0.0
Epoch 4, Batch 40/43, Loss=3.752454997599125
Loss made of: CE 0.27153727412223816, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.213355541229248 EntMin 0.0
Epoch 4, Class Loss=0.29205894470214844, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.29205894470214844, Class Loss=0.29205894470214844, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/43, Loss=3.724158376455307
Loss made of: CE 0.24425670504570007, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7696571350097656 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.716031256318092
Loss made of: CE 0.33064931631088257, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.651871681213379 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.7722495213150977
Loss made of: CE 0.2761802673339844, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5031535625457764 EntMin 0.0
Epoch 5, Batch 40/43, Loss=3.774755047261715
Loss made of: CE 0.29093438386917114, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.053743362426758 EntMin 0.0
Epoch 5, Class Loss=0.2867273688316345, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.2867273688316345, Class Loss=0.2867273688316345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/43, Loss=3.6853653326630593
Loss made of: CE 0.3005375862121582, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1523733139038086 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.737257757782936
Loss made of: CE 0.2458978146314621, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.514694929122925 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.6769628033041952
Loss made of: CE 0.24468347430229187, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1753926277160645 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.498123823106289
Loss made of: CE 0.29035115242004395, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2150654792785645 EntMin 0.0
Epoch 6, Class Loss=0.27195870876312256, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.27195870876312256, Class Loss=0.27195870876312256, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/52, Loss=3.7849988773465157
Loss made of: CE 0.3165525794029236, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6342151165008545 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.090319186449051
Loss made of: CE 0.20069622993469238, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7645297050476074 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.9222420871257784
Loss made of: CE 0.16253253817558289, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.777751922607422 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.7016248628497124
Loss made of: CE 0.2513704001903534, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3891496658325195 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.8071845531463624
Loss made of: CE 0.20872899889945984, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.930536985397339 EntMin 0.0
Epoch 1, Class Loss=0.2267608344554901, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.2267608344554901, Class Loss=0.2267608344554901, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/52, Loss=3.8142584592103956
Loss made of: CE 0.22669103741645813, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.564375638961792 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.9002373680472373
Loss made of: CE 0.2700435221195221, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.553225517272949 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.00236774533987
Loss made of: CE 0.2205338180065155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3616161346435547 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.842518797516823
Loss made of: CE 0.22888702154159546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.408111095428467 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.866745926439762
Loss made of: CE 0.21673142910003662, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035366058349609 EntMin 0.0
Epoch 2, Class Loss=0.2433626800775528, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.2433626800775528, Class Loss=0.2433626800775528, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/52, Loss=3.84375888556242
Loss made of: CE 0.2480601966381073, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.686539649963379 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.7970394045114517
Loss made of: CE 0.3642542362213135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.423521041870117 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.7980080798268316
Loss made of: CE 0.27053529024124146, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3847436904907227 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.883822079002857
Loss made of: CE 0.20487108826637268, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6954855918884277 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.948618766665459
Loss made of: CE 0.2561001777648926, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.67012095451355 EntMin 0.0
Epoch 3, Class Loss=0.25067347288131714, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.25067347288131714, Class Loss=0.25067347288131714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/52, Loss=3.8534359291195868
Loss made of: CE 0.24079486727714539, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.506699323654175 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.7557529002428054
Loss made of: CE 0.26439571380615234, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.500497817993164 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.704005254805088
Loss made of: CE 0.2570294141769409, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.356679677963257 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.8371622666716574
Loss made of: CE 0.22340695559978485, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.500234365463257 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.6977713823318483
Loss made of: CE 0.16710016131401062, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8281302452087402 EntMin 0.0
Epoch 4, Class Loss=0.2527228891849518, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.2527228891849518, Class Loss=0.2527228891849518, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/52, Loss=3.4826474994421006
Loss made of: CE 0.2541281580924988, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3397483825683594 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.787441921234131
Loss made of: CE 0.2363971769809723, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.593233823776245 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.5417963802814483
Loss made of: CE 0.2936078906059265, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4914116859436035 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.883319042623043
Loss made of: CE 0.2505033016204834, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.564281940460205 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.8997913032770155
Loss made of: CE 0.3126095235347748, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.106019496917725 EntMin 0.0
Epoch 5, Class Loss=0.2579424977302551, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.2579424977302551, Class Loss=0.2579424977302551, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/52, Loss=3.6723838672041893
Loss made of: CE 0.16664087772369385, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2940220832824707 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.691208055615425
Loss made of: CE 0.2506510615348816, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.79642391204834 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.709644615650177
Loss made of: CE 0.22317978739738464, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.384340286254883 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.8821055755019187
Loss made of: CE 0.3766532838344574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.576626777648926 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.7410240694880486
Loss made of: CE 0.2212192267179489, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.671159505844116 EntMin 0.0
Epoch 6, Class Loss=0.26642483472824097, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.26642483472824097, Class Loss=0.26642483472824097, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/34, Loss=4.834096032381058
Loss made of: CE 0.39843183755874634, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.079686164855957 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.368714439868927
Loss made of: CE 0.2622329592704773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5426928997039795 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.696474415063858
Loss made of: CE 0.3004920482635498, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9763073921203613 EntMin 0.0
Epoch 1, Class Loss=0.35507163405418396, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.35507163405418396, Class Loss=0.35507163405418396, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/34, Loss=4.460286286473274
Loss made of: CE 0.2861207127571106, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.265529632568359 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.266223442554474
Loss made of: CE 0.31000959873199463, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9204225540161133 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.349295595288277
Loss made of: CE 0.3193332254886627, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.592076301574707 EntMin 0.0
Epoch 2, Class Loss=0.2952495515346527, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.2952495515346527, Class Loss=0.2952495515346527, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/34, Loss=4.189906433224678
Loss made of: CE 0.29051434993743896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.642554998397827 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.196701112389564
Loss made of: CE 0.24156412482261658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7304611206054688 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.33364953994751
Loss made of: CE 0.25971680879592896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7140965461730957 EntMin 0.0
Epoch 3, Class Loss=0.28444626927375793, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.28444626927375793, Class Loss=0.28444626927375793, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/34, Loss=4.219812139868736
Loss made of: CE 0.27264294028282166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8540477752685547 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.235488657653332
Loss made of: CE 0.29617202281951904, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.749661922454834 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.3121792152523994
Loss made of: CE 0.3192902207374573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.497732162475586 EntMin 0.0
Epoch 4, Class Loss=0.2839050889015198, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.2839050889015198, Class Loss=0.2839050889015198, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/34, Loss=4.114106610417366
Loss made of: CE 0.20976099371910095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4070491790771484 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.314277237653732
Loss made of: CE 0.3868051767349243, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.296572208404541 EntMin 0.0
Epoch 5, Batch 30/34, Loss=3.9649108380079268
Loss made of: CE 0.2571469843387604, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.74111270904541 EntMin 0.0
Epoch 5, Class Loss=0.2737729847431183, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.2737729847431183, Class Loss=0.2737729847431183, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/34, Loss=4.145516127347946
Loss made of: CE 0.3425041139125824, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.885324478149414 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.074341237545013
Loss made of: CE 0.32815656065940857, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0692973136901855 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.227262872457504
Loss made of: CE 0.2153954803943634, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4678282737731934 EntMin 0.0
Epoch 6, Class Loss=0.27196818590164185, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.27196818590164185, Class Loss=0.27196818590164185, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.24278444051742554, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.925490
Mean Acc: 0.722463
FreqW Acc: 0.864906
Mean IoU: 0.635223
Class IoU:
	class 0: 0.92090887
	class 1: 0.8378813
	class 2: 0.32635856
	class 3: 0.6847247
	class 4: 0.67685187
	class 5: 0.0037954499
	class 6: 0.85925597
	class 7: 0.68901235
	class 8: 0.71821404
Class Acc:
	class 0: 0.9765814
	class 1: 0.88685733
	class 2: 0.59381896
	class 3: 0.7320887
	class 4: 0.81833506
	class 5: 0.0037961507
	class 6: 0.895984
	class 7: 0.74846506
	class 8: 0.8462372

federated global round: 9, step: 1
select part of clients to conduct local training
[5, 0, 9, 2]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/34, Loss=5.070917317271233
Loss made of: CE 0.40014928579330444, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.368469715118408 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.857735270261765
Loss made of: CE 0.4017510414123535, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.543179988861084 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.709823459386826
Loss made of: CE 0.39434191584587097, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.331843852996826 EntMin 0.0
Epoch 1, Class Loss=0.40438273549079895, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.40438273549079895, Class Loss=0.40438273549079895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/34, Loss=4.596299615502358
Loss made of: CE 0.302151083946228, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.111429691314697 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.201690080761909
Loss made of: CE 0.29142504930496216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.247610092163086 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.659275133907795
Loss made of: CE 0.38858604431152344, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.043055534362793 EntMin 0.0
Epoch 2, Class Loss=0.31364327669143677, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.31364327669143677, Class Loss=0.31364327669143677, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/34, Loss=4.14519323259592
Loss made of: CE 0.28448307514190674, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8473944664001465 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.1910962358117105
Loss made of: CE 0.30291497707366943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.84359073638916 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.448375649750233
Loss made of: CE 0.34506726264953613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.864655017852783 EntMin 0.0
Epoch 3, Class Loss=0.2766965925693512, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.2766965925693512, Class Loss=0.2766965925693512, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/34, Loss=4.287709757685661
Loss made of: CE 0.2788771092891693, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.818960666656494 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.343095892667771
Loss made of: CE 0.29957637190818787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303602695465088 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.229324039816857
Loss made of: CE 0.2801035940647125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4245290756225586 EntMin 0.0
Epoch 4, Class Loss=0.2827426493167877, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.2827426493167877, Class Loss=0.2827426493167877, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/34, Loss=4.134915706515312
Loss made of: CE 0.2354145050048828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0164475440979 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.245510765910149
Loss made of: CE 0.2865423858165741, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.821046829223633 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.237743367254734
Loss made of: CE 0.27586066722869873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.349514961242676 EntMin 0.0
Epoch 5, Class Loss=0.2702387571334839, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.2702387571334839, Class Loss=0.2702387571334839, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/34, Loss=4.398799754679203
Loss made of: CE 0.3252440392971039, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.594193935394287 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.0610277146101
Loss made of: CE 0.21294216811656952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0415849685668945 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.052206753194332
Loss made of: CE 0.3625926375389099, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400269508361816 EntMin 0.0
Epoch 6, Class Loss=0.2709296643733978, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.2709296643733978, Class Loss=0.2709296643733978, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/52, Loss=4.126343166828155
Loss made of: CE 0.4640216827392578, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4669463634490967 EntMin 0.0
Epoch 1, Batch 20/52, Loss=3.6098922163248064
Loss made of: CE 0.312347412109375, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1859328746795654 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.7080265283584595
Loss made of: CE 0.3319261968135834, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.200233221054077 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.7984708935022353
Loss made of: CE 0.27128803730010986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7794241905212402 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.7764095932245256
Loss made of: CE 0.21245042979717255, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.164393901824951 EntMin 0.0
Epoch 1, Class Loss=0.3169896602630615, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.3169896602630615, Class Loss=0.3169896602630615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/52, Loss=3.7935451328754426
Loss made of: CE 0.3503452241420746, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.404672622680664 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.634388154745102
Loss made of: CE 0.270319402217865, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1486003398895264 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.9081443563103675
Loss made of: CE 0.3171026110649109, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.776911735534668 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.746447116136551
Loss made of: CE 0.29157745838165283, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1217401027679443 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.7179825454950333
Loss made of: CE 0.2973617911338806, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9809069633483887 EntMin 0.0
Epoch 2, Class Loss=0.2894738018512726, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.2894738018512726, Class Loss=0.2894738018512726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/52, Loss=3.727964624762535
Loss made of: CE 0.2486582100391388, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5097815990448 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.7019852995872498
Loss made of: CE 0.24671077728271484, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.347644805908203 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.774273134768009
Loss made of: CE 0.32161498069763184, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1666698455810547 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.7325334146618845
Loss made of: CE 0.29938584566116333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.368908643722534 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.7644919872283937
Loss made of: CE 0.2759298086166382, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3097331523895264 EntMin 0.0
Epoch 3, Class Loss=0.28436121344566345, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.28436121344566345, Class Loss=0.28436121344566345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/52, Loss=3.9342511713504793
Loss made of: CE 0.3658103942871094, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3175084590911865 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.5662617683410645
Loss made of: CE 0.23613521456718445, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.327385425567627 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.624020552635193
Loss made of: CE 0.1784271001815796, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0388123989105225 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.6741473719477655
Loss made of: CE 0.27486544847488403, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1563708782196045 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.493124435842037
Loss made of: CE 0.27475523948669434, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2509045600891113 EntMin 0.0
Epoch 4, Class Loss=0.2757395803928375, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.2757395803928375, Class Loss=0.2757395803928375, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/52, Loss=3.663876023888588
Loss made of: CE 0.24023881554603577, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3458735942840576 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.5623134419322016
Loss made of: CE 0.21811780333518982, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3164830207824707 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.582734201848507
Loss made of: CE 0.29330530762672424, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2896742820739746 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.571879896521568
Loss made of: CE 0.282440721988678, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6528878211975098 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.503517471253872
Loss made of: CE 0.27742135524749756, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0635814666748047 EntMin 0.0
Epoch 5, Class Loss=0.26479458808898926, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.26479458808898926, Class Loss=0.26479458808898926, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/52, Loss=3.4606023013591765
Loss made of: CE 0.2479168176651001, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.042667865753174 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.6027546525001526
Loss made of: CE 0.24423067271709442, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1604251861572266 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.7091063380241396
Loss made of: CE 0.29551762342453003, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.228149890899658 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.3959348857402802
Loss made of: CE 0.2933189868927002, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1264710426330566 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.552402472496033
Loss made of: CE 0.28939175605773926, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3292465209960938 EntMin 0.0
Epoch 6, Class Loss=0.2689376473426819, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.2689376473426819, Class Loss=0.2689376473426819, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/52, Loss=3.8341924875974653
Loss made of: CE 0.4373649060726166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4907565116882324 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.175570690631867
Loss made of: CE 0.32371681928634644, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7857532501220703 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.9151580527424814
Loss made of: CE 0.3025549650192261, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5846123695373535 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.656181162595749
Loss made of: CE 0.3565019965171814, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3074073791503906 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.8186565697193147
Loss made of: CE 0.25912976264953613, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8635473251342773 EntMin 0.0
Epoch 1, Class Loss=0.33209463953971863, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.33209463953971863, Class Loss=0.33209463953971863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/52, Loss=3.800338463485241
Loss made of: CE 0.2746339738368988, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4388985633850098 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.854201516509056
Loss made of: CE 0.30119621753692627, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2961244583129883 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.9301526293158533
Loss made of: CE 0.26638513803482056, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1744344234466553 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.7155579552054405
Loss made of: CE 0.2510361075401306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.256713390350342 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.720680499076843
Loss made of: CE 0.2544877827167511, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.000295162200928 EntMin 0.0
Epoch 2, Class Loss=0.2988872230052948, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.2988872230052948, Class Loss=0.2988872230052948, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/52, Loss=3.7240936934947966
Loss made of: CE 0.2865447998046875, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.457075595855713 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.851974278688431
Loss made of: CE 0.3659726083278656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.592350006103516 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.650394196808338
Loss made of: CE 0.2639997601509094, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2162063121795654 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.821213313937187
Loss made of: CE 0.3406772017478943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.732234477996826 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.8776374369859696
Loss made of: CE 0.2726480960845947, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6297194957733154 EntMin 0.0
Epoch 3, Class Loss=0.2901231050491333, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.2901231050491333, Class Loss=0.2901231050491333, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/52, Loss=3.794956687092781
Loss made of: CE 0.262912780046463, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.367786169052124 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.6429750263690948
Loss made of: CE 0.3212722837924957, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.289351224899292 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.6990056544542314
Loss made of: CE 0.2792387008666992, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.450199604034424 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.837321177124977
Loss made of: CE 0.2529112994670868, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.450669288635254 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.5253191605210303
Loss made of: CE 0.2084687054157257, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.566368579864502 EntMin 0.0
Epoch 4, Class Loss=0.28446412086486816, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.28446412086486816, Class Loss=0.28446412086486816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/52, Loss=3.4734421163797378
Loss made of: CE 0.23891721665859222, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3003716468811035 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.769429048895836
Loss made of: CE 0.24211427569389343, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.599872589111328 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.449409495294094
Loss made of: CE 0.3030933737754822, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.200989246368408 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.917188473045826
Loss made of: CE 0.30016064643859863, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6771247386932373 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.804103194177151
Loss made of: CE 0.315195232629776, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195901870727539 EntMin 0.0
Epoch 5, Class Loss=0.27692148089408875, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.27692148089408875, Class Loss=0.27692148089408875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/52, Loss=3.609666831791401
Loss made of: CE 0.2422952950000763, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.266505241394043 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.604332672059536
Loss made of: CE 0.21845245361328125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7132577896118164 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.5696385964751243
Loss made of: CE 0.20906631648540497, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1118721961975098 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.733490805327892
Loss made of: CE 0.32453522086143494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.482633590698242 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.607908894121647
Loss made of: CE 0.21909698843955994, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.416670560836792 EntMin 0.0
Epoch 6, Class Loss=0.2726724147796631, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.2726724147796631, Class Loss=0.2726724147796631, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=4.845261055231094
Loss made of: CE 0.3389250934123993, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2202534675598145 EntMin 0.0
Epoch 1, Batch 20/25, Loss=4.397217644751072
Loss made of: CE 0.3846970498561859, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.311684608459473 EntMin 0.0
Epoch 1, Class Loss=0.2942236065864563, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.2942236065864563, Class Loss=0.2942236065864563, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/25, Loss=4.2063241943717005
Loss made of: CE 0.2445732206106186, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6477699279785156 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.2502455651760105
Loss made of: CE 0.17067599296569824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.002089500427246 EntMin 0.0
Epoch 2, Class Loss=0.25410526990890503, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.25410526990890503, Class Loss=0.25410526990890503, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/25, Loss=4.058770552277565
Loss made of: CE 0.2052459716796875, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8154964447021484 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.286244603991508
Loss made of: CE 0.21973244845867157, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.649742603302002 EntMin 0.0
Epoch 3, Class Loss=0.25356990098953247, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.25356990098953247, Class Loss=0.25356990098953247, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/25, Loss=4.084040935337543
Loss made of: CE 0.23875269293785095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5319647789001465 EntMin 0.0
Epoch 4, Batch 20/25, Loss=4.06131672412157
Loss made of: CE 0.20562538504600525, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4220314025878906 EntMin 0.0
Epoch 4, Class Loss=0.251169890165329, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.251169890165329, Class Loss=0.251169890165329, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/25, Loss=3.973829361796379
Loss made of: CE 0.1843767613172531, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5112340450286865 EntMin 0.0
Epoch 5, Batch 20/25, Loss=4.010389792919159
Loss made of: CE 0.28395771980285645, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.548250198364258 EntMin 0.0
Epoch 5, Class Loss=0.25131940841674805, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.25131940841674805, Class Loss=0.25131940841674805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/25, Loss=3.945557951927185
Loss made of: CE 0.3367506265640259, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.704746961593628 EntMin 0.0
Epoch 6, Batch 20/25, Loss=3.981828214228153
Loss made of: CE 0.2748682200908661, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7489991188049316 EntMin 0.0
Epoch 6, Class Loss=0.2654292583465576, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.2654292583465576, Class Loss=0.2654292583465576, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.23760105669498444, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.922953
Mean Acc: 0.711754
FreqW Acc: 0.862522
Mean IoU: 0.614207
Class IoU:
	class 0: 0.9246214
	class 1: 0.84228194
	class 2: 0.32990134
	class 3: 0.4650379
	class 4: 0.68581676
	class 5: 0.01694546
	class 6: 0.8703227
	class 7: 0.7122979
	class 8: 0.68064135
Class Acc:
	class 0: 0.97231585
	class 1: 0.88837135
	class 2: 0.59828705
	class 3: 0.48082107
	class 4: 0.8311265
	class 5: 0.016989902
	class 6: 0.9087906
	class 7: 0.8033282
	class 8: 0.9057593

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[0, 15, 1, 4]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=7.1981582075357435
Loss made of: CE 0.371796190738678, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5845184326171875 EntMin 0.0
Epoch 1, Class Loss=0.4956595301628113, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.4956595301628113, Class Loss=0.4956595301628113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=5.9605096817016605
Loss made of: CE 0.6708809733390808, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1772613525390625 EntMin 0.0
Epoch 2, Class Loss=0.669299840927124, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.669299840927124, Class Loss=0.669299840927124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.610657000541687
Loss made of: CE 0.7137192487716675, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.538970947265625 EntMin 0.0
Epoch 3, Class Loss=0.710828423500061, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.710828423500061, Class Loss=0.710828423500061, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.498233598470688
Loss made of: CE 0.7741554975509644, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.275142669677734 EntMin 0.0
Epoch 4, Class Loss=0.7361414432525635, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.7361414432525635, Class Loss=0.7361414432525635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.174780458211899
Loss made of: CE 0.8006075620651245, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.527087211608887 EntMin 0.0
Epoch 5, Class Loss=0.7344051003456116, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.7344051003456116, Class Loss=0.7344051003456116, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.286926674842834
Loss made of: CE 0.6634559631347656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400177955627441 EntMin 0.0
Epoch 6, Class Loss=0.7130265235900879, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.7130265235900879, Class Loss=0.7130265235900879, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=7.063363912701607
Loss made of: CE 0.4663037955760956, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.503879547119141 EntMin 0.0
Epoch 1, Class Loss=0.49749329686164856, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.49749329686164856, Class Loss=0.49749329686164856, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.152387630939484
Loss made of: CE 0.7128831148147583, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.689179420471191 EntMin 0.0
Epoch 2, Class Loss=0.6537320017814636, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.6537320017814636, Class Loss=0.6537320017814636, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.645087939500809
Loss made of: CE 0.8600987195968628, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8123064041137695 EntMin 0.0
Epoch 3, Class Loss=0.7083665132522583, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.7083665132522583, Class Loss=0.7083665132522583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.473830777406692
Loss made of: CE 0.8057718276977539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.737425804138184 EntMin 0.0
Epoch 4, Class Loss=0.7447866797447205, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.7447866797447205, Class Loss=0.7447866797447205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.277237206697464
Loss made of: CE 0.6568681001663208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8451828956604 EntMin 0.0
Epoch 5, Class Loss=0.7506600022315979, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.7506600022315979, Class Loss=0.7506600022315979, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.246019178628922
Loss made of: CE 0.687809944152832, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9534575939178467 EntMin 0.0
Epoch 6, Class Loss=0.7302679419517517, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.7302679419517517, Class Loss=0.7302679419517517, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=8.4601632386446
Loss made of: CE 0.44148391485214233, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.6029181480407715 EntMin 0.0
Epoch 1, Batch 20/35, Loss=7.025475019216538
Loss made of: CE 0.3692428469657898, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.13934850692749 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.651025389134884
Loss made of: CE 0.3504638671875, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.55261754989624 EntMin 0.0
Epoch 1, Class Loss=0.40941479802131653, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.40941479802131653, Class Loss=0.40941479802131653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/35, Loss=6.316111436486244
Loss made of: CE 0.5130600929260254, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.815847396850586 EntMin 0.0
Epoch 2, Batch 20/35, Loss=6.3605676114559175
Loss made of: CE 0.5838958621025085, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.236224174499512 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.982840177416802
Loss made of: CE 0.3159368634223938, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8780364990234375 EntMin 0.0
Epoch 2, Class Loss=0.4967713952064514, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.4967713952064514, Class Loss=0.4967713952064514, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/35, Loss=5.808999675512314
Loss made of: CE 0.4545530080795288, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.118968486785889 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.988008603453636
Loss made of: CE 0.4245537221431732, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.550021648406982 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.714939638972282
Loss made of: CE 0.43172311782836914, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.665762901306152 EntMin 0.0
Epoch 3, Class Loss=0.49081408977508545, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.49081408977508545, Class Loss=0.49081408977508545, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/35, Loss=5.648082804679871
Loss made of: CE 0.4331597685813904, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4717206954956055 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.5394425928592685
Loss made of: CE 0.41392993927001953, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.937118053436279 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.708143624663353
Loss made of: CE 0.480172336101532, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.174234390258789 EntMin 0.0
Epoch 4, Class Loss=0.46459725499153137, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.46459725499153137, Class Loss=0.46459725499153137, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/35, Loss=5.498456394672393
Loss made of: CE 0.3874082565307617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.89161491394043 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.61396299302578
Loss made of: CE 0.4033695459365845, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.281697750091553 EntMin 0.0
Epoch 5, Batch 30/35, Loss=5.083503699302673
Loss made of: CE 0.3125664293766022, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.476384162902832 EntMin 0.0
Epoch 5, Class Loss=0.4168946444988251, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4168946444988251, Class Loss=0.4168946444988251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/35, Loss=5.200670298933983
Loss made of: CE 0.32377344369888306, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.804305553436279 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.291890183091164
Loss made of: CE 0.37006181478500366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.848723888397217 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.494151476025581
Loss made of: CE 0.4444027841091156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.639533996582031 EntMin 0.0
Epoch 6, Class Loss=0.3942582607269287, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.3942582607269287, Class Loss=0.3942582607269287, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=7.11075080037117
Loss made of: CE 0.5602285265922546, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.228778839111328 EntMin 0.0
Epoch 1, Class Loss=0.4943654537200928, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.4943654537200928, Class Loss=0.4943654537200928, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.102131962776184
Loss made of: CE 0.6000300049781799, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.164234638214111 EntMin 0.0
Epoch 2, Class Loss=0.6461315155029297, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.6461315155029297, Class Loss=0.6461315155029297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.77477964758873
Loss made of: CE 0.6234301924705505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.405506610870361 EntMin 0.0
Epoch 3, Class Loss=0.7145621180534363, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.7145621180534363, Class Loss=0.7145621180534363, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.437448567152023
Loss made of: CE 0.7502070665359497, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.39426851272583 EntMin 0.0
Epoch 4, Class Loss=0.7340786457061768, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.7340786457061768, Class Loss=0.7340786457061768, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.375145131349564
Loss made of: CE 0.8123628497123718, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.462735176086426 EntMin 0.0
Epoch 5, Class Loss=0.7474709749221802, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.7474709749221802, Class Loss=0.7474709749221802, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.282675421237945
Loss made of: CE 0.6484333276748657, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.487529754638672 EntMin 0.0
Epoch 6, Class Loss=0.7244738936424255, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.7244738936424255, Class Loss=0.7244738936424255, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5933816432952881, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.844329
Mean Acc: 0.388687
FreqW Acc: 0.723823
Mean IoU: 0.333788
Class IoU:
	class 0: 0.85234725
	class 1: 0.5418601
	class 2: 0.22936928
	class 3: 0.1691097
	class 4: 0.5644548
	class 5: 0.044124108
	class 6: 0.82170266
	class 7: 0.63228023
	class 8: 0.4839996
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.9895938
	class 1: 0.54451805
	class 2: 0.32960683
	class 3: 0.1775129
	class 4: 0.61381924
	class 5: 0.044183142
	class 6: 0.8886027
	class 7: 0.66988856
	class 8: 0.79520917
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[1, 13, 11, 12]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/35, Loss=7.545457500219345
Loss made of: CE 0.9393447637557983, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.687058448791504 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.512476778030395
Loss made of: CE 0.6887121200561523, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.122432708740234 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.254941874742508
Loss made of: CE 0.7216100096702576, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6696624755859375 EntMin 0.0
Epoch 1, Class Loss=0.832218587398529, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.832218587398529, Class Loss=0.832218587398529, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/35, Loss=5.879399475455284
Loss made of: CE 0.5628789067268372, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.098637104034424 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.975884085893631
Loss made of: CE 0.6687456965446472, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.810125350952148 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.696406581997872
Loss made of: CE 0.4269421100616455, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.367772102355957 EntMin 0.0
Epoch 2, Class Loss=0.5382015705108643, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.5382015705108643, Class Loss=0.5382015705108643, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/35, Loss=5.36351415514946
Loss made of: CE 0.45226091146469116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.959108352661133 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.6422105699777605
Loss made of: CE 0.363717645406723, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3654937744140625 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.439956906437874
Loss made of: CE 0.45696505904197693, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.321892261505127 EntMin 0.0
Epoch 3, Class Loss=0.4266155958175659, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.4266155958175659, Class Loss=0.4266155958175659, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/35, Loss=5.38598453104496
Loss made of: CE 0.3460063636302948, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.051485061645508 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.1698862433433534
Loss made of: CE 0.3368142247200012, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.750773906707764 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.353112268447876
Loss made of: CE 0.3528605103492737, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7838640213012695 EntMin 0.0
Epoch 4, Class Loss=0.37920746207237244, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.37920746207237244, Class Loss=0.37920746207237244, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/35, Loss=5.270284968614578
Loss made of: CE 0.3123430013656616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.651583671569824 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.347237566113472
Loss made of: CE 0.3703557252883911, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9382476806640625 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.847082087397576
Loss made of: CE 0.28019940853118896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.437618255615234 EntMin 0.0
Epoch 5, Class Loss=0.34683579206466675, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.34683579206466675, Class Loss=0.34683579206466675, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/35, Loss=4.9198736771941185
Loss made of: CE 0.2597358226776123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760534286499023 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.120495638251304
Loss made of: CE 0.292880117893219, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.302206039428711 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.2722894817590715
Loss made of: CE 0.4214157462120056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3359856605529785 EntMin 0.0
Epoch 6, Class Loss=0.3343791961669922, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.3343791961669922, Class Loss=0.3343791961669922, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.644544181227684
Loss made of: CE 0.23676729202270508, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.476207256317139 EntMin 0.0
Epoch 1, Class Loss=0.28709205985069275, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.28709205985069275, Class Loss=0.28709205985069275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/19, Loss=4.882124909758568
Loss made of: CE 0.40482163429260254, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.879229545593262 EntMin 0.0
Epoch 2, Class Loss=0.433316171169281, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.433316171169281, Class Loss=0.433316171169281, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/19, Loss=4.824228903651237
Loss made of: CE 0.4927612245082855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.390449047088623 EntMin 0.0
Epoch 3, Class Loss=0.4952138662338257, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.4952138662338257, Class Loss=0.4952138662338257, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/19, Loss=4.922643947601318
Loss made of: CE 0.6369144916534424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.179442405700684 EntMin 0.0
Epoch 4, Class Loss=0.5433262586593628, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5433262586593628, Class Loss=0.5433262586593628, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/19, Loss=4.929393103718757
Loss made of: CE 0.4778209626674652, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.628602981567383 EntMin 0.0
Epoch 5, Class Loss=0.5500461459159851, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5500461459159851, Class Loss=0.5500461459159851, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/19, Loss=4.771609500050545
Loss made of: CE 0.5704511404037476, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.279314041137695 EntMin 0.0
Epoch 6, Class Loss=0.5607361793518066, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5607361793518066, Class Loss=0.5607361793518066, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=4.959288105368614
Loss made of: CE 0.33794867992401123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.88243293762207 EntMin 0.0
Epoch 1, Batch 20/33, Loss=4.844107960164547
Loss made of: CE 0.23355452716350555, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.52703332901001 EntMin 0.0
Epoch 1, Batch 30/33, Loss=4.808788105845451
Loss made of: CE 0.1765495389699936, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.412352561950684 EntMin 0.0
Epoch 1, Class Loss=0.28433647751808167, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.28433647751808167, Class Loss=0.28433647751808167, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=5.142516753077507
Loss made of: CE 0.4663851261138916, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.329401016235352 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.718829014897347
Loss made of: CE 0.3578038811683655, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.822345733642578 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.711370024085045
Loss made of: CE 0.4313834607601166, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.498051643371582 EntMin 0.0
Epoch 2, Class Loss=0.41822993755340576, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.41822993755340576, Class Loss=0.41822993755340576, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.758838203549385
Loss made of: CE 0.40905094146728516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.426746368408203 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.8751353234052655
Loss made of: CE 0.5512262582778931, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.334948539733887 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.73949204981327
Loss made of: CE 0.5533676147460938, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.447601318359375 EntMin 0.0
Epoch 3, Class Loss=0.47099411487579346, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.47099411487579346, Class Loss=0.47099411487579346, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.757888749241829
Loss made of: CE 0.4557790756225586, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9867491722106934 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.841413044929505
Loss made of: CE 0.40988224744796753, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.282844066619873 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.7155682176351545
Loss made of: CE 0.4749618470668793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7472753524780273 EntMin 0.0
Epoch 4, Class Loss=0.4973773658275604, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4973773658275604, Class Loss=0.4973773658275604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.666692444682122
Loss made of: CE 0.4811822175979614, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8675944805145264 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.949446353316307
Loss made of: CE 0.4742646813392639, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9692494869232178 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.630635517835617
Loss made of: CE 0.596373975276947, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9860637187957764 EntMin 0.0
Epoch 5, Class Loss=0.5267742276191711, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5267742276191711, Class Loss=0.5267742276191711, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.541063505411148
Loss made of: CE 0.529523491859436, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9801557064056396 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.573322620987892
Loss made of: CE 0.49705633521080017, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.444330930709839 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.559251269698143
Loss made of: CE 0.42510271072387695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3946685791015625 EntMin 0.0
Epoch 6, Class Loss=0.5291397571563721, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5291397571563721, Class Loss=0.5291397571563721, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=4.999474082887173
Loss made of: CE 0.36802762746810913, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.220034122467041 EntMin 0.0
Epoch 1, Batch 20/33, Loss=4.72835831195116
Loss made of: CE 0.3250581920146942, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.540976524353027 EntMin 0.0
Epoch 1, Batch 30/33, Loss=4.6622169584035875
Loss made of: CE 0.22228513658046722, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.453378200531006 EntMin 0.0
Epoch 1, Class Loss=0.2763001620769501, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.2763001620769501, Class Loss=0.2763001620769501, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=4.749868270754814
Loss made of: CE 0.2655002474784851, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500574588775635 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.873829460144043
Loss made of: CE 0.4205397963523865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5138349533081055 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.751838085055351
Loss made of: CE 0.44573521614074707, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9560022354125977 EntMin 0.0
Epoch 2, Class Loss=0.41169747710227966, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.41169747710227966, Class Loss=0.41169747710227966, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.753205469250679
Loss made of: CE 0.5906888246536255, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9047956466674805 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.854893681406975
Loss made of: CE 0.4768730401992798, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.276896953582764 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.623967164754868
Loss made of: CE 0.5039862990379333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8755860328674316 EntMin 0.0
Epoch 3, Class Loss=0.47703859210014343, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.47703859210014343, Class Loss=0.47703859210014343, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.73488210439682
Loss made of: CE 0.45616722106933594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.234877109527588 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.634742799401283
Loss made of: CE 0.49047064781188965, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9587295055389404 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.566858890652656
Loss made of: CE 0.5870156288146973, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9010910987854 EntMin 0.0
Epoch 4, Class Loss=0.5046058297157288, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.5046058297157288, Class Loss=0.5046058297157288, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.709203925728798
Loss made of: CE 0.4865529537200928, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6603775024414062 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.484632694721222
Loss made of: CE 0.6714122295379639, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6077704429626465 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.472833880782128
Loss made of: CE 0.5880417227745056, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9965062141418457 EntMin 0.0
Epoch 5, Class Loss=0.5185434222221375, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5185434222221375, Class Loss=0.5185434222221375, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.522422653436661
Loss made of: CE 0.5660183429718018, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9891412258148193 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.678832828998566
Loss made of: CE 0.49913203716278076, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6400210857391357 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.727977299690247
Loss made of: CE 0.4731977880001068, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.504503011703491 EntMin 0.0
Epoch 6, Class Loss=0.530180811882019, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.530180811882019, Class Loss=0.530180811882019, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4902365505695343, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.855245
Mean Acc: 0.455925
FreqW Acc: 0.741986
Mean IoU: 0.389436
Class IoU:
	class 0: 0.8610667
	class 1: 0.5558214
	class 2: 0.303327
	class 3: 0.3385342
	class 4: 0.6224351
	class 5: 0.040976193
	class 6: 0.7187984
	class 7: 0.5147574
	class 8: 0.5383999
	class 9: 0.0
	class 10: 0.56854665
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.988756
	class 1: 0.5582996
	class 2: 0.51496273
	class 3: 0.37095606
	class 4: 0.709887
	class 5: 0.04105179
	class 6: 0.7379613
	class 7: 0.53040725
	class 8: 0.8219758
	class 9: 0.0
	class 10: 0.65276927
	class 11: 0.0
	class 12: 0.0

federated global round: 12, step: 2
select part of clients to conduct local training
[0, 16, 7, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.941113078594208
Loss made of: CE 0.7097322344779968, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.735876083374023 EntMin 0.0
Epoch 1, Class Loss=0.6304024457931519, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.6304024457931519, Class Loss=0.6304024457931519, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.664917171001434
Loss made of: CE 0.5626545548439026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.106405258178711 EntMin 0.0
Epoch 2, Class Loss=0.5786163210868835, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.5786163210868835, Class Loss=0.5786163210868835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.676426765322685
Loss made of: CE 0.5669283270835876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8185324668884277 EntMin 0.0
Epoch 3, Class Loss=0.5371565222740173, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.5371565222740173, Class Loss=0.5371565222740173, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.618407601118088
Loss made of: CE 0.6079434752464294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.673925399780273 EntMin 0.0
Epoch 4, Class Loss=0.5131967067718506, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.5131967067718506, Class Loss=0.5131967067718506, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.473148640990257
Loss made of: CE 0.5304824113845825, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9690539836883545 EntMin 0.0
Epoch 5, Class Loss=0.48559635877609253, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.48559635877609253, Class Loss=0.48559635877609253, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.496833789348602
Loss made of: CE 0.4742279052734375, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8108444213867188 EntMin 0.0
Epoch 6, Class Loss=0.4615665674209595, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.4615665674209595, Class Loss=0.4615665674209595, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/42, Loss=5.262514635920525
Loss made of: CE 0.2986947000026703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.188442707061768 EntMin 0.0
Epoch 1, Batch 20/42, Loss=4.974836504459381
Loss made of: CE 0.22066354751586914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.959583282470703 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.13007124364376
Loss made of: CE 0.26719698309898376, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.673923015594482 EntMin 0.0
Epoch 1, Batch 40/42, Loss=4.741771356761456
Loss made of: CE 0.2233213633298874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.996710777282715 EntMin 0.0
Epoch 1, Class Loss=0.27526792883872986, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.27526792883872986, Class Loss=0.27526792883872986, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/42, Loss=5.057874956727028
Loss made of: CE 0.24915313720703125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53712797164917 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.892343947291375
Loss made of: CE 0.25761908292770386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.934035301208496 EntMin 0.0
Epoch 2, Batch 30/42, Loss=4.782036563754081
Loss made of: CE 0.22606614232063293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6768798828125 EntMin 0.0
Epoch 2, Batch 40/42, Loss=4.8764514118433
Loss made of: CE 0.3108375668525696, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.25483512878418 EntMin 0.0
Epoch 2, Class Loss=0.31215980648994446, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.31215980648994446, Class Loss=0.31215980648994446, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/42, Loss=4.727729374170304
Loss made of: CE 0.3565148711204529, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.112061500549316 EntMin 0.0
Epoch 3, Batch 20/42, Loss=5.0036257117986676
Loss made of: CE 0.1923351287841797, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9022417068481445 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.681156405806542
Loss made of: CE 0.18969079852104187, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.622808456420898 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.6003027111291885
Loss made of: CE 0.18685922026634216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.605016708374023 EntMin 0.0
Epoch 3, Class Loss=0.31746557354927063, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.31746557354927063, Class Loss=0.31746557354927063, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/42, Loss=4.863093599677086
Loss made of: CE 0.37170442938804626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.27491569519043 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.692969545722008
Loss made of: CE 0.38426217436790466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.263833045959473 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.713067710399628
Loss made of: CE 0.3008350133895874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.99576735496521 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.722301724553108
Loss made of: CE 0.34942853450775146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.280149936676025 EntMin 0.0
Epoch 4, Class Loss=0.3528885245323181, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.3528885245323181, Class Loss=0.3528885245323181, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/42, Loss=4.538921794295311
Loss made of: CE 0.36237069964408875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.031353950500488 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.6952806413173676
Loss made of: CE 0.38289588689804077, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.21601676940918 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.737794896960258
Loss made of: CE 0.45938432216644287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.598306655883789 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.668269902467728
Loss made of: CE 0.3204993009567261, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.067802429199219 EntMin 0.0
Epoch 5, Class Loss=0.35616815090179443, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.35616815090179443, Class Loss=0.35616815090179443, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/42, Loss=4.758914735913277
Loss made of: CE 0.3422623574733734, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.810180187225342 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.522396671772003
Loss made of: CE 0.39505910873413086, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.136462211608887 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.698241528868675
Loss made of: CE 0.43448758125305176, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400609493255615 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.83275465965271
Loss made of: CE 0.36299246549606323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.777310371398926 EntMin 0.0
Epoch 6, Class Loss=0.3700733780860901, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.3700733780860901, Class Loss=0.3700733780860901, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/31, Loss=5.327489155530929
Loss made of: CE 0.3103369474411011, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.849377632141113 EntMin 0.0
Epoch 1, Batch 20/31, Loss=5.129689151048661
Loss made of: CE 0.2777782678604126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.934674263000488 EntMin 0.0
Epoch 1, Batch 30/31, Loss=4.928769542276859
Loss made of: CE 0.2368406504392624, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.45725154876709 EntMin 0.0
Epoch 1, Class Loss=0.2689988315105438, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.2689988315105438, Class Loss=0.2689988315105438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/31, Loss=4.818585598468781
Loss made of: CE 0.3592354655265808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.328193664550781 EntMin 0.0
Epoch 2, Batch 20/31, Loss=4.633106744289398
Loss made of: CE 0.3288916051387787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.089813709259033 EntMin 0.0
Epoch 2, Batch 30/31, Loss=5.066738584637642
Loss made of: CE 0.2905387282371521, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.87701678276062 EntMin 0.0
Epoch 2, Class Loss=0.36437055468559265, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.36437055468559265, Class Loss=0.36437055468559265, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/31, Loss=4.884847667813301
Loss made of: CE 0.3728255033493042, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3345489501953125 EntMin 0.0
Epoch 3, Batch 20/31, Loss=4.599996683001518
Loss made of: CE 0.49592113494873047, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.917712926864624 EntMin 0.0
Epoch 3, Batch 30/31, Loss=4.717051953077316
Loss made of: CE 0.3152253329753876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6452245712280273 EntMin 0.0
Epoch 3, Class Loss=0.42361316084861755, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.42361316084861755, Class Loss=0.42361316084861755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/31, Loss=4.596819207072258
Loss made of: CE 0.6000151038169861, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.644042015075684 EntMin 0.0
Epoch 4, Batch 20/31, Loss=4.675557923316956
Loss made of: CE 0.47176551818847656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.031008720397949 EntMin 0.0
Epoch 4, Batch 30/31, Loss=4.742909947037697
Loss made of: CE 0.6189915537834167, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.536193370819092 EntMin 0.0
Epoch 4, Class Loss=0.4619796574115753, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4619796574115753, Class Loss=0.4619796574115753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/31, Loss=4.535915741324425
Loss made of: CE 0.45927494764328003, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.081512451171875 EntMin 0.0
Epoch 5, Batch 20/31, Loss=4.609835475683212
Loss made of: CE 0.42415332794189453, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8123672008514404 EntMin 0.0
Epoch 5, Batch 30/31, Loss=4.831973493099213
Loss made of: CE 0.3668212294578552, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9636683464050293 EntMin 0.0
Epoch 5, Class Loss=0.4718250334262848, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4718250334262848, Class Loss=0.4718250334262848, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/31, Loss=4.558049258589745
Loss made of: CE 0.5885629653930664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9874520301818848 EntMin 0.0
Epoch 6, Batch 20/31, Loss=4.738956108689308
Loss made of: CE 0.46818065643310547, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.319058895111084 EntMin 0.0
Epoch 6, Batch 30/31, Loss=4.51192099750042
Loss made of: CE 0.5247551798820496, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7040882110595703 EntMin 0.0
Epoch 6, Class Loss=0.49092981219291687, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.49092981219291687, Class Loss=0.49092981219291687, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.870168852806091
Loss made of: CE 0.5089447498321533, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.344025611877441 EntMin 0.0
Epoch 1, Class Loss=0.6438508629798889, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.6438508629798889, Class Loss=0.6438508629798889, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.86251289844513
Loss made of: CE 0.5452532172203064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.255876064300537 EntMin 0.0
Epoch 2, Class Loss=0.5842211246490479, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.5842211246490479, Class Loss=0.5842211246490479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.716673961281776
Loss made of: CE 0.4504110813140869, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.589038610458374 EntMin 0.0
Epoch 3, Class Loss=0.5438570976257324, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.5438570976257324, Class Loss=0.5438570976257324, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.540446072816849
Loss made of: CE 0.46378207206726074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.680695056915283 EntMin 0.0
Epoch 4, Class Loss=0.5029707551002502, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5029707551002502, Class Loss=0.5029707551002502, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.521182760596275
Loss made of: CE 0.4912737309932709, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9716506004333496 EntMin 0.0
Epoch 5, Class Loss=0.4904491603374481, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.4904491603374481, Class Loss=0.4904491603374481, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.53440015912056
Loss made of: CE 0.42648908495903015, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.892786741256714 EntMin 0.0
Epoch 6, Class Loss=0.4721978008747101, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.4721978008747101, Class Loss=0.4721978008747101, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.45725587010383606, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.855233
Mean Acc: 0.475149
FreqW Acc: 0.743437
Mean IoU: 0.404442
Class IoU:
	class 0: 0.8611442
	class 1: 0.56178164
	class 2: 0.31546354
	class 3: 0.4648979
	class 4: 0.6313806
	class 5: 0.038201313
	class 6: 0.61310124
	class 7: 0.4686029
	class 8: 0.5531843
	class 9: 0.0
	class 10: 0.497708
	class 11: 0.24521834
	class 12: 0.0070646275
Class Acc:
	class 0: 0.9868512
	class 1: 0.5645735
	class 2: 0.5520238
	class 3: 0.5309916
	class 4: 0.7339529
	class 5: 0.03826968
	class 6: 0.6244507
	class 7: 0.4828946
	class 8: 0.82788485
	class 9: 0.0
	class 10: 0.53156847
	class 11: 0.29641107
	class 12: 0.007068886

federated global round: 13, step: 2
select part of clients to conduct local training
[14, 13, 0, 1]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.276544311642647
Loss made of: CE 0.1897745579481125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.869532346725464 EntMin 0.0
Epoch 1, Class Loss=0.18360984325408936, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.18360984325408936, Class Loss=0.18360984325408936, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=4.307623578608036
Loss made of: CE 0.2974696755409241, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.691610336303711 EntMin 0.0
Epoch 2, Class Loss=0.2791125774383545, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.2791125774383545, Class Loss=0.2791125774383545, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=4.4345331490039825
Loss made of: CE 0.2830873131752014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.482141494750977 EntMin 0.0
Epoch 3, Class Loss=0.33516809344291687, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.33516809344291687, Class Loss=0.33516809344291687, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=4.352889201045036
Loss made of: CE 0.36452817916870117, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8513667583465576 EntMin 0.0
Epoch 4, Class Loss=0.3811272084712982, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.3811272084712982, Class Loss=0.3811272084712982, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=4.303973621129989
Loss made of: CE 0.3495897650718689, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5640149116516113 EntMin 0.0
Epoch 5, Class Loss=0.40874314308166504, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.40874314308166504, Class Loss=0.40874314308166504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=4.3366413682699205
Loss made of: CE 0.3970712423324585, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7164125442504883 EntMin 0.0
Epoch 6, Class Loss=0.4246620833873749, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.4246620833873749, Class Loss=0.4246620833873749, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/19, Loss=4.460922199487686
Loss made of: CE 0.48216646909713745, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9286441802978516 EntMin 0.0
Epoch 1, Class Loss=0.5118609666824341, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5118609666824341, Class Loss=0.5118609666824341, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/19, Loss=4.483682182431221
Loss made of: CE 0.517597496509552, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.359988212585449 EntMin 0.0
Epoch 2, Class Loss=0.4777580201625824, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.4777580201625824, Class Loss=0.4777580201625824, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/19, Loss=4.452616509795189
Loss made of: CE 0.3789116144180298, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0290985107421875 EntMin 0.0
Epoch 3, Class Loss=0.45967206358909607, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.45967206358909607, Class Loss=0.45967206358909607, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/19, Loss=4.502587631344795
Loss made of: CE 0.4821421504020691, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.886937141418457 EntMin 0.0
Epoch 4, Class Loss=0.44474586844444275, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.44474586844444275, Class Loss=0.44474586844444275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/19, Loss=4.544481652975082
Loss made of: CE 0.34771618247032166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3450679779052734 EntMin 0.0
Epoch 5, Class Loss=0.4297831356525421, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.4297831356525421, Class Loss=0.4297831356525421, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/19, Loss=4.264452385902405
Loss made of: CE 0.4201493263244629, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.746124267578125 EntMin 0.0
Epoch 6, Class Loss=0.4297558665275574, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.4297558665275574, Class Loss=0.4297558665275574, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.502111786603928
Loss made of: CE 0.5100207328796387, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.307327747344971 EntMin 0.0
Epoch 1, Class Loss=0.5152261853218079, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.5152261853218079, Class Loss=0.5152261853218079, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/19, Loss=4.336461302638054
Loss made of: CE 0.553782045841217, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.961075782775879 EntMin 0.0
Epoch 2, Class Loss=0.4924362599849701, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.4924362599849701, Class Loss=0.4924362599849701, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/19, Loss=4.392430856823921
Loss made of: CE 0.5399936437606812, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.670271873474121 EntMin 0.0
Epoch 3, Class Loss=0.48060697317123413, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.48060697317123413, Class Loss=0.48060697317123413, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/19, Loss=4.437779662013054
Loss made of: CE 0.5803879499435425, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.638168811798096 EntMin 0.0
Epoch 4, Class Loss=0.4692617654800415, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.4692617654800415, Class Loss=0.4692617654800415, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/19, Loss=4.28208042383194
Loss made of: CE 0.4704135060310364, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.911365270614624 EntMin 0.0
Epoch 5, Class Loss=0.4446331858634949, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.4446331858634949, Class Loss=0.4446331858634949, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/19, Loss=4.392943397164345
Loss made of: CE 0.4851931929588318, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.721907615661621 EntMin 0.0
Epoch 6, Class Loss=0.44291210174560547, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.44291210174560547, Class Loss=0.44291210174560547, Reg Loss=0.0
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/35, Loss=7.5326887488365175
Loss made of: CE 0.7601169347763062, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.08524227142334 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.444025033712387
Loss made of: CE 0.6017497777938843, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3749518394470215 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.1092093050479885
Loss made of: CE 0.5276514291763306, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.639003753662109 EntMin 0.0
Epoch 1, Class Loss=0.6636356711387634, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.6636356711387634, Class Loss=0.6636356711387634, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000584
Epoch 2, Batch 10/35, Loss=5.753419861197472
Loss made of: CE 0.3821352422237396, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2908806800842285 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.809333580732345
Loss made of: CE 0.46717700362205505, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.709437847137451 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.5037105739116665
Loss made of: CE 0.36703503131866455, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.49063777923584 EntMin 0.0
Epoch 2, Class Loss=0.4213084578514099, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.4213084578514099, Class Loss=0.4213084578514099, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/35, Loss=5.146267223358154
Loss made of: CE 0.36095112562179565, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8020124435424805 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.492105740308761
Loss made of: CE 0.32603517174720764, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.245759010314941 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.286248952150345
Loss made of: CE 0.35576507449150085, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.128255844116211 EntMin 0.0
Epoch 3, Class Loss=0.3556658625602722, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3556658625602722, Class Loss=0.3556658625602722, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000487
Epoch 4, Batch 10/35, Loss=5.2267857760190966
Loss made of: CE 0.2708318829536438, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.054919719696045 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.102318957448006
Loss made of: CE 0.30619487166404724, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6482744216918945 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.226148828864098
Loss made of: CE 0.318665087223053, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.959132194519043 EntMin 0.0
Epoch 4, Class Loss=0.3314078152179718, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.3314078152179718, Class Loss=0.3314078152179718, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000438
Epoch 5, Batch 10/35, Loss=5.236246970295906
Loss made of: CE 0.29946261644363403, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.514219284057617 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.241881263256073
Loss made of: CE 0.311296284198761, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.016840934753418 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.795877701044082
Loss made of: CE 0.252179890871048, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.257423400878906 EntMin 0.0
Epoch 5, Class Loss=0.31211307644844055, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.31211307644844055, Class Loss=0.31211307644844055, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000389
Epoch 6, Batch 10/35, Loss=4.890793450176716
Loss made of: CE 0.24956576526165009, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4794602394104 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.088575252890587
Loss made of: CE 0.2681076228618622, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.313731670379639 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.151091152429581
Loss made of: CE 0.34507691860198975, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.378561496734619 EntMin 0.0
Epoch 6, Class Loss=0.30679720640182495, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.30679720640182495, Class Loss=0.30679720640182495, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.43604424595832825, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.859510
Mean Acc: 0.496362
FreqW Acc: 0.753415
Mean IoU: 0.408378
Class IoU:
	class 0: 0.8708728
	class 1: 0.5567858
	class 2: 0.31698805
	class 3: 0.49025056
	class 4: 0.6390273
	class 5: 0.029628985
	class 6: 0.5530361
	class 7: 0.41860652
	class 8: 0.5946815
	class 9: 0.0
	class 10: 0.47729716
	class 11: 0.23852062
	class 12: 0.12322522
Class Acc:
	class 0: 0.98513424
	class 1: 0.5597319
	class 2: 0.5552534
	class 3: 0.5439012
	class 4: 0.75083816
	class 5: 0.029665701
	class 6: 0.56124705
	class 7: 0.4306215
	class 8: 0.77426666
	class 9: 0.0
	class 10: 0.847454
	class 11: 0.28957942
	class 12: 0.12501457

federated global round: 14, step: 2
select part of clients to conduct local training
[16, 9, 4, 0]
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/42, Loss=5.4076972007751465
Loss made of: CE 0.6430985927581787, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.252395153045654 EntMin 0.0
Epoch 1, Batch 20/42, Loss=5.142324709892273
Loss made of: CE 0.48388344049453735, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.557489395141602 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.2593128889799114
Loss made of: CE 0.48234647512435913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.571458339691162 EntMin 0.0
Epoch 1, Batch 40/42, Loss=4.791034898161888
Loss made of: CE 0.3379186987876892, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.884169340133667 EntMin 0.0
Epoch 1, Class Loss=0.5445097088813782, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.5445097088813782, Class Loss=0.5445097088813782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/42, Loss=4.926291370391846
Loss made of: CE 0.3417132794857025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.260137557983398 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.704773709177971
Loss made of: CE 0.36084210872650146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.865143299102783 EntMin 0.0
Epoch 2, Batch 30/42, Loss=4.653276181221008
Loss made of: CE 0.34246551990509033, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.437932968139648 EntMin 0.0
Epoch 2, Batch 40/42, Loss=4.789609679579735
Loss made of: CE 0.4444770812988281, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.840196371078491 EntMin 0.0
Epoch 2, Class Loss=0.41034117341041565, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.41034117341041565, Class Loss=0.41034117341041565, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/42, Loss=4.615709611773491
Loss made of: CE 0.39390525221824646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.934593915939331 EntMin 0.0
Epoch 3, Batch 20/42, Loss=4.9205413401126865
Loss made of: CE 0.33888325095176697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8663177490234375 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.56838498711586
Loss made of: CE 0.35047754645347595, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.486581325531006 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.540692126750946
Loss made of: CE 0.3313806653022766, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.710912227630615 EntMin 0.0
Epoch 3, Class Loss=0.3759639263153076, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.3759639263153076, Class Loss=0.3759639263153076, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/42, Loss=4.664825487136841
Loss made of: CE 0.3366480767726898, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9532198905944824 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.468065720796585
Loss made of: CE 0.3869195580482483, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.116429805755615 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.453417792916298
Loss made of: CE 0.30909907817840576, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5374574661254883 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.585399615764618
Loss made of: CE 0.3419376313686371, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.522939682006836 EntMin 0.0
Epoch 4, Class Loss=0.3643423914909363, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.3643423914909363, Class Loss=0.3643423914909363, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/42, Loss=4.301912018656731
Loss made of: CE 0.37302833795547485, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8428993225097656 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.613579076528549
Loss made of: CE 0.42028141021728516, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9358296394348145 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.660544732213021
Loss made of: CE 0.4377230405807495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.685502052307129 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.487894517183304
Loss made of: CE 0.40175220370292664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.980884313583374 EntMin 0.0
Epoch 5, Class Loss=0.3586924374103546, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3586924374103546, Class Loss=0.3586924374103546, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/42, Loss=4.602555483579636
Loss made of: CE 0.2968065142631531, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6911113262176514 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.335075542330742
Loss made of: CE 0.38593220710754395, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.851606845855713 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.580450162291527
Loss made of: CE 0.46762531995773315, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.098898410797119 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.659738773107529
Loss made of: CE 0.3919714391231537, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.618165969848633 EntMin 0.0
Epoch 6, Class Loss=0.36396729946136475, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.36396729946136475, Class Loss=0.36396729946136475, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=6.222041606903076
Loss made of: CE 0.32188528776168823, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.362943649291992 EntMin 0.0
Epoch 1, Batch 20/35, Loss=5.728758729994297
Loss made of: CE 0.1497555524110794, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.16114616394043 EntMin 0.0
Epoch 1, Batch 30/35, Loss=5.367807393521071
Loss made of: CE 0.1751633584499359, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.268402576446533 EntMin 0.0
Epoch 1, Class Loss=0.18601220846176147, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.18601220846176147, Class Loss=0.18601220846176147, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/35, Loss=5.565842013061046
Loss made of: CE 0.2988300919532776, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.631949424743652 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.293872293829918
Loss made of: CE 0.1741289347410202, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.757997989654541 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.132111026346683
Loss made of: CE 0.1980266273021698, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.691803932189941 EntMin 0.0
Epoch 2, Class Loss=0.2137255072593689, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.2137255072593689, Class Loss=0.2137255072593689, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/35, Loss=5.259041273593903
Loss made of: CE 0.18080346286296844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.827735900878906 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.186113545298577
Loss made of: CE 0.20132598280906677, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.67991828918457 EntMin 0.0
Epoch 3, Batch 30/35, Loss=4.989785397052765
Loss made of: CE 0.20841282606124878, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.367889404296875 EntMin 0.0
Epoch 3, Class Loss=0.23021695017814636, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.23021695017814636, Class Loss=0.23021695017814636, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/35, Loss=5.162203525006771
Loss made of: CE 0.22809582948684692, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0925116539001465 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.160129974782467
Loss made of: CE 0.2729327380657196, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.23995304107666 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.040917490422726
Loss made of: CE 0.27326714992523193, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.291290283203125 EntMin 0.0
Epoch 4, Class Loss=0.27060744166374207, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.27060744166374207, Class Loss=0.27060744166374207, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/35, Loss=4.7539810061454775
Loss made of: CE 0.2702043950557709, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.773980617523193 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.085855887830258
Loss made of: CE 0.24397914111614227, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3585333824157715 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.833906763792038
Loss made of: CE 0.35441523790359497, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.381379127502441 EntMin 0.0
Epoch 5, Class Loss=0.29273101687431335, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.29273101687431335, Class Loss=0.29273101687431335, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/35, Loss=5.050271445512772
Loss made of: CE 0.27795082330703735, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.00270938873291 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.7153707697987555
Loss made of: CE 0.4268206059932709, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7008185386657715 EntMin 0.0
Epoch 6, Batch 30/35, Loss=4.876481238007545
Loss made of: CE 0.24823907017707825, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.585975646972656 EntMin 0.0
Epoch 6, Class Loss=0.303035706281662, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.303035706281662, Class Loss=0.303035706281662, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.356082078814507
Loss made of: CE 0.3893052041530609, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8211116790771484 EntMin 0.0
Epoch 1, Class Loss=0.47724223136901855, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.47724223136901855, Class Loss=0.47724223136901855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/19, Loss=4.51450497508049
Loss made of: CE 0.4211268424987793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.902615547180176 EntMin 0.0
Epoch 2, Class Loss=0.4493611454963684, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.4493611454963684, Class Loss=0.4493611454963684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/19, Loss=4.360351487994194
Loss made of: CE 0.373239129781723, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.283583641052246 EntMin 0.0
Epoch 3, Class Loss=0.4424472749233246, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.4424472749233246, Class Loss=0.4424472749233246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/19, Loss=4.240742847323418
Loss made of: CE 0.42405176162719727, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.450252056121826 EntMin 0.0
Epoch 4, Class Loss=0.4284668564796448, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.4284668564796448, Class Loss=0.4284668564796448, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/19, Loss=4.207892677187919
Loss made of: CE 0.46599340438842773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5703110694885254 EntMin 0.0
Epoch 5, Class Loss=0.42417386174201965, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.42417386174201965, Class Loss=0.42417386174201965, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/19, Loss=4.281372275948525
Loss made of: CE 0.39225539565086365, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7287282943725586 EntMin 0.0
Epoch 6, Class Loss=0.4269566237926483, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.4269566237926483, Class Loss=0.4269566237926483, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000304
Epoch 1, Batch 10/19, Loss=4.433370566368103
Loss made of: CE 0.48604628443717957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.350367069244385 EntMin 0.0
Epoch 1, Class Loss=0.4690026342868805, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.4690026342868805, Class Loss=0.4690026342868805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000258
Epoch 2, Batch 10/19, Loss=4.242435890436172
Loss made of: CE 0.5082354545593262, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.865189552307129 EntMin 0.0
Epoch 2, Class Loss=0.4603266417980194, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.4603266417980194, Class Loss=0.4603266417980194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000211
Epoch 3, Batch 10/19, Loss=4.274110040068626
Loss made of: CE 0.47124528884887695, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.447477102279663 EntMin 0.0
Epoch 3, Class Loss=0.448748379945755, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.448748379945755, Class Loss=0.448748379945755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000163
Epoch 4, Batch 10/19, Loss=4.300315484404564
Loss made of: CE 0.5624594688415527, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.408941268920898 EntMin 0.0
Epoch 4, Class Loss=0.4425104558467865, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.4425104558467865, Class Loss=0.4425104558467865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000113
Epoch 5, Batch 10/19, Loss=4.1805458694696425
Loss made of: CE 0.44473475217819214, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.736393451690674 EntMin 0.0
Epoch 5, Class Loss=0.43750110268592834, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.43750110268592834, Class Loss=0.43750110268592834, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000061
Epoch 6, Batch 10/19, Loss=4.350504156947136
Loss made of: CE 0.46555250883102417, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5451388359069824 EntMin 0.0
Epoch 6, Class Loss=0.44430506229400635, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.44430506229400635, Class Loss=0.44430506229400635, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4201453924179077, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.864539
Mean Acc: 0.511963
FreqW Acc: 0.769263
Mean IoU: 0.432266
Class IoU:
	class 0: 0.88299763
	class 1: 0.577334
	class 2: 0.31672174
	class 3: 0.44520962
	class 4: 0.63853705
	class 5: 0.03181772
	class 6: 0.5760926
	class 7: 0.43636692
	class 8: 0.34663382
	class 9: 0.00016488788
	class 10: 0.68513477
	class 11: 0.26084375
	class 12: 0.4215984
Class Acc:
	class 0: 0.9806111
	class 1: 0.5812615
	class 2: 0.5543969
	class 3: 0.47027424
	class 4: 0.77141833
	class 5: 0.031850968
	class 6: 0.5863514
	class 7: 0.45102325
	class 8: 0.35687935
	class 9: 0.00016490511
	class 10: 0.70452195
	class 11: 0.32081226
	class 12: 0.8459546

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[11, 6, 10, 7]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=6.629748749732971
Loss made of: CE 0.2643203139305115, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.995912551879883 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.6043878346681595
Loss made of: CE 0.2687700390815735, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.096019268035889 EntMin 0.0
Epoch 1, Batch 30/102, Loss=5.781261512637139
Loss made of: CE 0.1542966514825821, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.649557113647461 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.580859012901783
Loss made of: CE 0.15405794978141785, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.245104789733887 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.3926878429949285
Loss made of: CE 0.21758270263671875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.491969108581543 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.3426441617310045
Loss made of: CE 0.15650682151317596, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.190067291259766 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.022711222618819
Loss made of: CE 0.22187793254852295, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.614887714385986 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.896894393861293
Loss made of: CE 0.1827114075422287, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.873839378356934 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.834769988805055
Loss made of: CE 0.14373037219047546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.258174419403076 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.88342587351799
Loss made of: CE 0.15485131740570068, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.646480560302734 EntMin 0.0
Epoch 1, Class Loss=0.1848907321691513, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.1848907321691513, Class Loss=0.1848907321691513, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=4.831502297520638
Loss made of: CE 0.31782251596450806, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.03504753112793 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.817160829901695
Loss made of: CE 0.3013973534107208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464696407318115 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.933405637741089
Loss made of: CE 0.3089364171028137, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.704400062561035 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.883123682439328
Loss made of: CE 0.2324541211128235, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.046472549438477 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.812517505884171
Loss made of: CE 0.1946735829114914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.254833221435547 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.98662156611681
Loss made of: CE 0.2689724564552307, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.167961120605469 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.458498567342758
Loss made of: CE 0.18277037143707275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8397092819213867 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.692820236086845
Loss made of: CE 0.15098226070404053, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.469634056091309 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.862568821012974
Loss made of: CE 0.28368324041366577, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.715065002441406 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.82519049346447
Loss made of: CE 0.1916045993566513, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.970108509063721 EntMin 0.0
Epoch 2, Class Loss=0.2632434666156769, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.2632434666156769, Class Loss=0.2632434666156769, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=4.760461027920246
Loss made of: CE 0.3096562623977661, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9796934127807617 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.7896326839923855
Loss made of: CE 0.46078869700431824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.043927192687988 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.818710762262344
Loss made of: CE 0.2831449508666992, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.049233913421631 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.7965853929519655
Loss made of: CE 0.228291854262352, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.276251792907715 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.817818537354469
Loss made of: CE 0.28902915120124817, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.117227554321289 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.610782553255558
Loss made of: CE 0.3291637897491455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195265769958496 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.51727225035429
Loss made of: CE 0.23819586634635925, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.390181541442871 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 80/102, Loss=4.712392151355743
Loss made of: CE 0.22142374515533447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.636504173278809 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.763884626328945
Loss made of: CE 0.35729533433914185, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.847414016723633 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.521719475090504
Loss made of: CE 0.2470928281545639, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6283984184265137 EntMin 0.0
Epoch 3, Class Loss=0.3067014515399933, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.3067014515399933, Class Loss=0.3067014515399933, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.508709609508514
Loss made of: CE 0.37561625242233276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.148100852966309 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.766483917832375
Loss made of: CE 0.3339272737503052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.957451343536377 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.570560386776924
Loss made of: CE 0.38656675815582275, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.138764381408691 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.5982792496681215
Loss made of: CE 0.3356725573539734, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8175694942474365 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.763485190272331
Loss made of: CE 0.29970473051071167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332163333892822 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.834752756357193
Loss made of: CE 0.4199502468109131, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.51372766494751 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.671129456162452
Loss made of: CE 0.3487272262573242, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.210799694061279 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.76813771724701
Loss made of: CE 0.4375708997249603, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.699504852294922 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.697403424978257
Loss made of: CE 0.2813527584075928, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.205926895141602 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.593720278143882
Loss made of: CE 0.3458094000816345, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.808093070983887 EntMin 0.0
Epoch 4, Class Loss=0.3414325714111328, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.3414325714111328, Class Loss=0.3414325714111328, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=4.9244355142116545
Loss made of: CE 0.33580833673477173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.160916805267334 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.7858937054872515
Loss made of: CE 0.33274418115615845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.092284202575684 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.756639060378075
Loss made of: CE 0.39870211482048035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.153371810913086 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.425941371917725
Loss made of: CE 0.40736573934555054, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9628942012786865 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.744014775753021
Loss made of: CE 0.3610641658306122, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.023926734924316 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.26407767534256
Loss made of: CE 0.4700116515159607, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9983725547790527 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.533620741963387
Loss made of: CE 0.33723995089530945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6566009521484375 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.34853223413229
Loss made of: CE 0.3434964120388031, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.580322265625 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.6863242268562315
Loss made of: CE 0.39531081914901733, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6930196285247803 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.619996526837349
Loss made of: CE 0.345768541097641, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.568973541259766 EntMin 0.0
Epoch 5, Class Loss=0.3619266748428345, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.3619266748428345, Class Loss=0.3619266748428345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.695058161020279
Loss made of: CE 0.37125134468078613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249917984008789 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.422563007473945
Loss made of: CE 0.38295722007751465, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.03446102142334 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.279961603879928
Loss made of: CE 0.3951631188392639, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.057995319366455 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.837954044342041
Loss made of: CE 0.5060995817184448, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.98464298248291 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.349866023659706
Loss made of: CE 0.39013463258743286, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8993852138519287 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.535759407281875
Loss made of: CE 0.421026349067688, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0201616287231445 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.379520979523659
Loss made of: CE 0.3534132242202759, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.204301834106445 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.514309969544411
Loss made of: CE 0.33590763807296753, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7893528938293457 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.476247107982635
Loss made of: CE 0.3522031009197235, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8889987468719482 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.598032268881798
Loss made of: CE 0.35386452078819275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.798048496246338 EntMin 0.0
Epoch 6, Class Loss=0.38405516743659973, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.38405516743659973, Class Loss=0.38405516743659973, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=6.414406925439835
Loss made of: CE 0.2120327204465866, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.536288261413574 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.506656335294247
Loss made of: CE 0.11010154336690903, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.041335582733154 EntMin 0.0
Epoch 1, Class Loss=0.14847034215927124, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.14847034215927124, Class Loss=0.14847034215927124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=5.3373754024505615
Loss made of: CE 0.3455967307090759, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.916490077972412 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.971123698353767
Loss made of: CE 0.36565348505973816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.51550817489624 EntMin 0.0
Epoch 2, Class Loss=0.3589453101158142, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3589453101158142, Class Loss=0.3589453101158142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=4.980064749717712
Loss made of: CE 0.4284016788005829, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.476443767547607 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.892600876092911
Loss made of: CE 0.5998479127883911, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.177634239196777 EntMin 0.0
Epoch 3, Class Loss=0.4916367828845978, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4916367828845978, Class Loss=0.4916367828845978, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.656897246837616
Loss made of: CE 0.6395289301872253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.367864608764648 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.728366428613663
Loss made of: CE 0.5437309741973877, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6030449867248535 EntMin 0.0
Epoch 4, Class Loss=0.578131377696991, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.578131377696991, Class Loss=0.578131377696991, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=4.724205935001374
Loss made of: CE 0.6289212703704834, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.10012674331665 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.694100332260132
Loss made of: CE 0.6352006196975708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.468868732452393 EntMin 0.0
Epoch 5, Class Loss=0.6381589770317078, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.6381589770317078, Class Loss=0.6381589770317078, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.653215712308883
Loss made of: CE 0.6863898634910583, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.164434909820557 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.670149546861649
Loss made of: CE 0.6064888834953308, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4818825721740723 EntMin 0.0
Epoch 6, Class Loss=0.6455095410346985, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.6455095410346985, Class Loss=0.6455095410346985, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=6.612624117732048
Loss made of: CE 0.14870688319206238, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.936866760253906 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.5908659137785435
Loss made of: CE 0.18467700481414795, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.423696517944336 EntMin 0.0
Epoch 1, Class Loss=0.15500937402248383, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.15500937402248383, Class Loss=0.15500937402248383, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=5.166569408774376
Loss made of: CE 0.3040684759616852, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.429333686828613 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.884466826915741
Loss made of: CE 0.31060534715652466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.721460342407227 EntMin 0.0
Epoch 2, Class Loss=0.3510763347148895, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.3510763347148895, Class Loss=0.3510763347148895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=4.845578491687775
Loss made of: CE 0.4628908634185791, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7463274002075195 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.939893686771393
Loss made of: CE 0.4594214856624603, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6015214920043945 EntMin 0.0
Epoch 3, Class Loss=0.5202000737190247, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5202000737190247, Class Loss=0.5202000737190247, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.75879545211792
Loss made of: CE 0.6456759572029114, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.133584022521973 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.775451815128326
Loss made of: CE 0.6132926344871521, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.909176826477051 EntMin 0.0
Epoch 4, Class Loss=0.5992426872253418, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.5992426872253418, Class Loss=0.5992426872253418, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=4.9753864169120785
Loss made of: CE 0.6512019038200378, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8778560161590576 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.664988762140274
Loss made of: CE 0.660557210445404, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.824469566345215 EntMin 0.0
Epoch 5, Class Loss=0.6393362879753113, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.6393362879753113, Class Loss=0.6393362879753113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.660884207487106
Loss made of: CE 0.5491892099380493, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.037103176116943 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 20/23, Loss=4.681588590145111
Loss made of: CE 0.6668351888656616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18404483795166 EntMin 0.0
Epoch 6, Class Loss=0.6568570137023926, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6568570137023926, Class Loss=0.6568570137023926, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=6.7105294778943065
Loss made of: CE 0.12986311316490173, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.481421947479248 EntMin 0.0
Epoch 1, Batch 20/102, Loss=6.2876232951879505
Loss made of: CE 0.2149149477481842, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.261556148529053 EntMin 0.0
Epoch 1, Batch 30/102, Loss=5.720773963630199
Loss made of: CE 0.2591577470302582, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.806582927703857 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.335713091492653
Loss made of: CE 0.18188557028770447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.641762733459473 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.370392570644617
Loss made of: CE 0.24654942750930786, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.632388114929199 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.2562586411833765
Loss made of: CE 0.1965728998184204, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.556078910827637 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.218650203943253
Loss made of: CE 0.11591537296772003, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.507633209228516 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.985000158846378
Loss made of: CE 0.14787940680980682, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.207855224609375 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.405238906294107
Loss made of: CE 0.1601850986480713, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.144559383392334 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.5089177504181865
Loss made of: CE 0.21247051656246185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.330807685852051 EntMin 0.0
Epoch 1, Class Loss=0.1894206553697586, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.1894206553697586, Class Loss=0.1894206553697586, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=5.198353728652
Loss made of: CE 0.2061394453048706, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.157308578491211 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.992989841103554
Loss made of: CE 0.3105919063091278, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.14906120300293 EntMin 0.0
Epoch 2, Batch 30/102, Loss=5.065741984546184
Loss made of: CE 0.22159431874752045, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.962176322937012 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.873211258649826
Loss made of: CE 0.19973647594451904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.943648815155029 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.532108646631241
Loss made of: CE 0.23505966365337372, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.978938102722168 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.790561172366142
Loss made of: CE 0.27720341086387634, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.626070499420166 EntMin 0.0
Epoch 2, Batch 70/102, Loss=5.187165074795485
Loss made of: CE 0.3351152539253235, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.235002517700195 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.612173998355866
Loss made of: CE 0.1815231740474701, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7946908473968506 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.550017210841179
Loss made of: CE 0.2699333429336548, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.657987594604492 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.899224402010441
Loss made of: CE 0.2935839295387268, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.994903564453125 EntMin 0.0
Epoch 2, Class Loss=0.26293960213661194, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.26293960213661194, Class Loss=0.26293960213661194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=5.127902653813362
Loss made of: CE 0.29350629448890686, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.521073818206787 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.914207065105439
Loss made of: CE 0.36778926849365234, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6122303009033203 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.679945161938667
Loss made of: CE 0.40780144929885864, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.675393581390381 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.700313612818718
Loss made of: CE 0.2782396674156189, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389743804931641 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.733548194169998
Loss made of: CE 0.25493133068084717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.833193778991699 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.564532425999642
Loss made of: CE 0.23932594060897827, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9667158126831055 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.81304124891758
Loss made of: CE 0.2577609717845917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.870098829269409 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.434398899972439
Loss made of: CE 0.21628199517726898, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9395768642425537 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.50856118798256
Loss made of: CE 0.28359824419021606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.94691801071167 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.49362230449915
Loss made of: CE 0.22894319891929626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.10523796081543 EntMin 0.0
Epoch 3, Class Loss=0.3102380037307739, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3102380037307739, Class Loss=0.3102380037307739, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.9802239567041395
Loss made of: CE 0.3883543014526367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3315019607543945 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.617377537488937
Loss made of: CE 0.3904958963394165, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.335309028625488 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.576519885659218
Loss made of: CE 0.2943800389766693, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7329368591308594 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.5329256936907765
Loss made of: CE 0.3548601567745209, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.064958572387695 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.62346695959568
Loss made of: CE 0.3869798183441162, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.26075553894043 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.71001128256321
Loss made of: CE 0.3199343979358673, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.461800575256348 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.462844809889793
Loss made of: CE 0.3259025514125824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.027416229248047 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.5957052409648895
Loss made of: CE 0.3231409192085266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.129794597625732 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.736780872941017
Loss made of: CE 0.35934218764305115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.682909965515137 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.867874127626419
Loss made of: CE 0.27930665016174316, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.10732364654541 EntMin 0.0
Epoch 4, Class Loss=0.34579139947891235, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.34579139947891235, Class Loss=0.34579139947891235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/102, Loss=4.402685531973839
Loss made of: CE 0.3564786911010742, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.86604118347168 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.874718642234802
Loss made of: CE 0.3563002049922943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.477366924285889 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.783188888430596
Loss made of: CE 0.39876359701156616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.139325141906738 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.61811814904213
Loss made of: CE 0.4082537591457367, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8463683128356934 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.406655701994896
Loss made of: CE 0.41508573293685913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1229143142700195 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.542173889279366
Loss made of: CE 0.3324815034866333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.49764347076416 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.6609938710927965
Loss made of: CE 0.3000950217247009, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.503631114959717 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.663166108727455
Loss made of: CE 0.2899209260940552, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.047534942626953 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.505336987972259
Loss made of: CE 0.4958333969116211, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.072931289672852 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.3518613398075106
Loss made of: CE 0.3854215741157532, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734638690948486 EntMin 0.0
Epoch 5, Class Loss=0.36305907368659973, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.36305907368659973, Class Loss=0.36305907368659973, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.478708317875862
Loss made of: CE 0.4486098289489746, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5327308177948 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.88866015970707
Loss made of: CE 0.3430497646331787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.663130760192871 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.123849695920944
Loss made of: CE 0.37891143560409546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.566826581954956 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.520523369312286
Loss made of: CE 0.38807791471481323, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7275893688201904 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.59613019824028
Loss made of: CE 0.39459142088890076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0331830978393555 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.702690908312798
Loss made of: CE 0.31529176235198975, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8699073791503906 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.516723150014878
Loss made of: CE 0.32215893268585205, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9952056407928467 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.377414426207542
Loss made of: CE 0.33217722177505493, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.154208183288574 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.5610503405332565
Loss made of: CE 0.33555254340171814, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.258694171905518 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.530541917681694
Loss made of: CE 0.4569559693336487, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.899231433868408 EntMin 0.0
Epoch 6, Class Loss=0.3795173764228821, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.3795173764228821, Class Loss=0.3795173764228821, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6684637665748596, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.787588
Mean Acc: 0.376248
FreqW Acc: 0.636506
Mean IoU: 0.306739
Class IoU:
	class 0: 0.794772
	class 1: 0.41265565
	class 2: 0.30976984
	class 3: 0.03852929
	class 4: 0.5829332
	class 5: 0.01622789
	class 6: 0.6779871
	class 7: 0.44984666
	class 8: 0.27698323
	class 9: 0.0016525871
	class 10: 0.4594565
	class 11: 0.26334748
	class 12: 0.35667077
	class 13: 0.0
	class 14: 0.51403487
	class 15: 0.05969591
	class 16: 0.0
Class Acc:
	class 0: 0.98750865
	class 1: 0.41368353
	class 2: 0.58271545
	class 3: 0.038774084
	class 4: 0.7114186
	class 5: 0.016243555
	class 6: 0.6963504
	class 7: 0.46180156
	class 8: 0.28137267
	class 9: 0.0016525871
	class 10: 0.6263133
	class 11: 0.31723937
	class 12: 0.66045487
	class 13: 0.0
	class 14: 0.54064226
	class 15: 0.060041554
	class 16: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[7, 11, 16, 18]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=4.998608100414276
Loss made of: CE 0.6098027229309082, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7135977745056152 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.00502290725708
Loss made of: CE 0.526239275932312, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.018777370452881 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.783924642205238
Loss made of: CE 0.4640657901763916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.334174156188965 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.7983806848526
Loss made of: CE 0.45912429690361023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.09718132019043 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.012049290537834
Loss made of: CE 0.5591714978218079, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7182512283325195 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 60/102, Loss=4.921979525685311
Loss made of: CE 0.46598684787750244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7560575008392334 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.910852986574173
Loss made of: CE 0.49496349692344666, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.391580581665039 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.830411773920059
Loss made of: CE 0.4914087653160095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8803882598876953 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.321607378125191
Loss made of: CE 0.4238830506801605, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7255427837371826 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.281076893210411
Loss made of: CE 0.4616397023200989, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.956615924835205 EntMin 0.0
Epoch 1, Class Loss=0.4944627285003662, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.4944627285003662, Class Loss=0.4944627285003662, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=5.149287813901902
Loss made of: CE 0.7325498461723328, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.648627996444702 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.712994116544723
Loss made of: CE 0.6591826677322388, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2063140869140625 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.888148009777069
Loss made of: CE 0.5951094627380371, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.989457130432129 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.80932184457779
Loss made of: CE 0.5988298058509827, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4438982009887695 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.558723258972168
Loss made of: CE 0.5454384088516235, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6956825256347656 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.903732734918594
Loss made of: CE 0.6396679878234863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.35809850692749 EntMin 0.0
Epoch 2, Batch 70/102, Loss=5.033087712526322
Loss made of: CE 0.6965329051017761, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.461674690246582 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.713206505775451
Loss made of: CE 0.5565792322158813, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.699951171875 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.667502403259277
Loss made of: CE 0.5952588319778442, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.127620220184326 EntMin 0.0
Epoch 2, Batch 100/102, Loss=5.000685036182404
Loss made of: CE 0.6064500212669373, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.595400333404541 EntMin 0.0
Epoch 2, Class Loss=0.6680223941802979, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.6680223941802979, Class Loss=0.6680223941802979, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.770104071497917
Loss made of: CE 0.43604597449302673, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.073659896850586 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.5232834815979
Loss made of: CE 0.3962603807449341, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3971991539001465 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.397807532548905
Loss made of: CE 0.44792839884757996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.894263744354248 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.557265600562095
Loss made of: CE 0.29715102910995483, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.366915702819824 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.416233080625534
Loss made of: CE 0.37586483359336853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.451786041259766 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.343736445903778
Loss made of: CE 0.3841884732246399, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6231677532196045 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.645379614830017
Loss made of: CE 0.33884209394454956, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.692441701889038 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.373983883857727
Loss made of: CE 0.39063090085983276, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7759056091308594 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.368421027064324
Loss made of: CE 0.40966054797172546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.719536304473877 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.329932218790054
Loss made of: CE 0.34272369742393494, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7920327186584473 EntMin 0.0
Epoch 3, Class Loss=0.38876134157180786, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.38876134157180786, Class Loss=0.38876134157180786, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.595334112644196
Loss made of: CE 0.367790549993515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.01244592666626 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.408004119992256
Loss made of: CE 0.36362001299858093, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.003316879272461 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.4460294365882875
Loss made of: CE 0.333623468875885, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.516554594039917 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.431011787056923
Loss made of: CE 0.314476877450943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8447628021240234 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.383580991625786
Loss made of: CE 0.40245485305786133, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.777177333831787 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.503341561555862
Loss made of: CE 0.38717398047447205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2433624267578125 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.316205629706383
Loss made of: CE 0.3807545304298401, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9429244995117188 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.550415098667145
Loss made of: CE 0.3961583971977234, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.916914701461792 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.409665155410766
Loss made of: CE 0.38812360167503357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.097894191741943 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.641665968298912
Loss made of: CE 0.29756712913513184, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7993457317352295 EntMin 0.0
Epoch 4, Class Loss=0.37723952531814575, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.37723952531814575, Class Loss=0.37723952531814575, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.433648037910461
Loss made of: CE 0.7768619060516357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.155865669250488 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.799068874120712
Loss made of: CE 0.7240771055221558, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.907744884490967 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.931565362215042
Loss made of: CE 0.724403977394104, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.987626791000366 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.581292963027954
Loss made of: CE 0.5985798835754395, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5518290996551514 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.5186379045248035
Loss made of: CE 0.5993064641952515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.234662055969238 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.548435950279236
Loss made of: CE 0.5935781002044678, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4683327674865723 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.6451175063848495
Loss made of: CE 0.5130102038383484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3686909675598145 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.765912091732025
Loss made of: CE 0.5465648174285889, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1437578201293945 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.512612676620483
Loss made of: CE 0.6290630102157593, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.763075351715088 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.549606853723526
Loss made of: CE 0.6166641116142273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.700166702270508 EntMin 0.0
Epoch 5, Class Loss=0.602900505065918, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.602900505065918, Class Loss=0.602900505065918, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.497371232509613
Loss made of: CE 0.5979326367378235, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2563862800598145 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.824735850095749
Loss made of: CE 0.5448341965675354, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.552497625350952 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.218890303373337
Loss made of: CE 0.5259222984313965, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.270115375518799 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.64320473074913
Loss made of: CE 0.5816901922225952, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.681025981903076 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.612321346998215
Loss made of: CE 0.6030124425888062, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8829572200775146 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.429254242777825
Loss made of: CE 0.5930630564689636, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8920044898986816 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.508386442065239
Loss made of: CE 0.5470173358917236, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8539483547210693 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.396983975172043
Loss made of: CE 0.5666710138320923, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.839686155319214 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.537168118357658
Loss made of: CE 0.5862623453140259, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.027101993560791 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.480105102062225
Loss made of: CE 0.6182449460029602, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5245893001556396 EntMin 0.0
Epoch 6, Class Loss=0.5708909630775452, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.5708909630775452, Class Loss=0.5708909630775452, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=5.364375281333923
Loss made of: CE 0.9606965780258179, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.446506500244141 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.97345849275589
Loss made of: CE 0.7939456701278687, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.215101718902588 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 30/102, Loss=5.046609193086624
Loss made of: CE 0.7873980402946472, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.453960418701172 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.277389341592789
Loss made of: CE 0.6663591265678406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.562664031982422 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.233540630340576
Loss made of: CE 0.6165695786476135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.225563049316406 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.240890103578567
Loss made of: CE 0.66004478931427, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7574095726013184 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.008664101362228
Loss made of: CE 0.6880986094474792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.025674819946289 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.969066375494004
Loss made of: CE 0.6181163191795349, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0313520431518555 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.834202927350998
Loss made of: CE 0.7276906371116638, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7406041622161865 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.853993362188339
Loss made of: CE 0.7677026987075806, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9915246963500977 EntMin 0.0
Epoch 1, Class Loss=0.750974178314209, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.750974178314209, Class Loss=0.750974178314209, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.71330715417862
Loss made of: CE 0.6061954498291016, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7820491790771484 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.658264440298081
Loss made of: CE 0.5636489391326904, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9844882488250732 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.758304291963578
Loss made of: CE 0.6950196623802185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.174493789672852 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.90596421957016
Loss made of: CE 0.6903083324432373, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9603686332702637 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.933001452684403
Loss made of: CE 0.6268417835235596, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1336212158203125 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.958899170160294
Loss made of: CE 0.7228142619132996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.952432155609131 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.587606489658356
Loss made of: CE 0.6368470191955566, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5729923248291016 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.699162882566452
Loss made of: CE 0.7466208338737488, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.099821090698242 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.869377619028091
Loss made of: CE 0.7319700717926025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7293701171875 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.968039965629577
Loss made of: CE 0.5222598314285278, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.501338005065918 EntMin 0.0
Epoch 2, Class Loss=0.636075496673584, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.636075496673584, Class Loss=0.636075496673584, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.657015639543533
Loss made of: CE 0.6859158277511597, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9370410442352295 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.616655048727989
Loss made of: CE 0.6459383964538574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6828248500823975 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.89195242524147
Loss made of: CE 0.5880693197250366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.822939872741699 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.697045242786407
Loss made of: CE 0.5644407272338867, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9186971187591553 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.808886593580246
Loss made of: CE 0.6280645132064819, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.950901746749878 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.656507360935211
Loss made of: CE 0.5673284530639648, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.052272796630859 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.665324386954308
Loss made of: CE 0.4855283200740814, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.925959348678589 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.87250235080719
Loss made of: CE 0.5134484171867371, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.587743282318115 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.831736919283867
Loss made of: CE 0.4726962745189667, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6838748455047607 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.678138881921768
Loss made of: CE 0.5181240439414978, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7968575954437256 EntMin 0.0
Epoch 3, Class Loss=0.5982208847999573, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.5982208847999573, Class Loss=0.5982208847999573, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.649452298879623
Loss made of: CE 0.5522959232330322, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8715121746063232 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.717698583006859
Loss made of: CE 0.49430951476097107, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.338801383972168 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.5843173384666445
Loss made of: CE 0.7323529720306396, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9751477241516113 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.679330503940582
Loss made of: CE 0.534239649772644, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7327709197998047 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.818127971887589
Loss made of: CE 0.6310828924179077, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.386840343475342 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.771058201789856
Loss made of: CE 0.5085739493370056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.282129287719727 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.705512258410454
Loss made of: CE 0.7520275115966797, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.222524642944336 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.743262386322021
Loss made of: CE 0.7325133681297302, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.407109260559082 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.719632175564766
Loss made of: CE 0.4903658330440521, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.870704412460327 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.756219694018364
Loss made of: CE 0.6269848942756653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.97684907913208 EntMin 0.0
Epoch 4, Class Loss=0.5810861587524414, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5810861587524414, Class Loss=0.5810861587524414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.8960444062948225
Loss made of: CE 0.630407989025116, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6818196773529053 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.8240415215492245
Loss made of: CE 0.6200346350669861, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.997253179550171 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.657101935148239
Loss made of: CE 0.46445512771606445, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.727623701095581 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.39469184577465
Loss made of: CE 0.5939550399780273, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7798423767089844 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.626389223337173
Loss made of: CE 0.5810205340385437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3065643310546875 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.355280321836472
Loss made of: CE 0.7379627227783203, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.759087085723877 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.4466993629932405
Loss made of: CE 0.6063852310180664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5886788368225098 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.477232339978218
Loss made of: CE 0.4240942895412445, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.137858867645264 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.47804623246193
Loss made of: CE 0.6466723680496216, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4907901287078857 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.540823769569397
Loss made of: CE 0.560903012752533, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.193022727966309 EntMin 0.0
Epoch 5, Class Loss=0.5589589476585388, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5589589476585388, Class Loss=0.5589589476585388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.800617614388466
Loss made of: CE 0.5102874040603638, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9975295066833496 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.351466172933579
Loss made of: CE 0.5624613165855408, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.64790415763855 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.237448528409004
Loss made of: CE 0.5531522631645203, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.926396369934082 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.794083079695701
Loss made of: CE 0.6319446563720703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9373674392700195 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.403318792581558
Loss made of: CE 0.4844699501991272, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6473207473754883 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.646090564131737
Loss made of: CE 0.5815917253494263, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.987365484237671 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.286992010474205
Loss made of: CE 0.5807962417602539, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9162769317626953 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.474567246437073
Loss made of: CE 0.5506263971328735, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.744413137435913 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.510168573260307
Loss made of: CE 0.5478217005729675, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.558591604232788 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.6204479217529295
Loss made of: CE 0.6182405352592468, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.813406467437744 EntMin 0.0
Epoch 6, Class Loss=0.546160876750946, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.546160876750946, Class Loss=0.546160876750946, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/105, Loss=4.834854523837566
Loss made of: CE 0.21816299855709076, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.315352439880371 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.384629567340016
Loss made of: CE 0.07876795530319214, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.602724075317383 EntMin 0.0
Epoch 1, Batch 30/105, Loss=4.1710627194494005
Loss made of: CE 0.08691682666540146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332488059997559 EntMin 0.0
Epoch 1, Batch 40/105, Loss=4.429803463816643
Loss made of: CE 0.06917008012533188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.681645393371582 EntMin 0.0
Epoch 1, Batch 50/105, Loss=4.2709591798484325
Loss made of: CE 0.11549261212348938, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.946572780609131 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.314882200956345
Loss made of: CE 0.12966173887252808, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.11629581451416 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.3004578806459905
Loss made of: CE 0.11704406142234802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.380797386169434 EntMin 0.0
Epoch 1, Batch 80/105, Loss=4.586266351863742
Loss made of: CE 0.08021676540374756, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.851978302001953 EntMin 0.0
Epoch 1, Batch 90/105, Loss=4.4968184500932695
Loss made of: CE 0.0659964308142662, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3471174240112305 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.2557257656008005
Loss made of: CE 0.06443844735622406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.35847806930542 EntMin 0.0
Epoch 1, Class Loss=0.08896548300981522, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.08896548300981522, Class Loss=0.08896548300981522, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/105, Loss=4.216207067668438
Loss made of: CE 0.2201773226261139, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9851269721984863 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.351241046190262
Loss made of: CE 0.18139629065990448, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.030935287475586 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.441027350723743
Loss made of: CE 0.15160994231700897, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623238563537598 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.391979658603669
Loss made of: CE 0.17382942140102386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.296669006347656 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.22100021019578
Loss made of: CE 0.1080789789557457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6371326446533203 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 60/105, Loss=4.38561776727438
Loss made of: CE 0.12849333882331848, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8579423427581787 EntMin 0.0
Epoch 2, Batch 70/105, Loss=4.718034278601408
Loss made of: CE 0.14624916017055511, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8498857021331787 EntMin 0.0
Epoch 2, Batch 80/105, Loss=4.422804974019527
Loss made of: CE 0.14518532156944275, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.158404350280762 EntMin 0.0
Epoch 2, Batch 90/105, Loss=4.6268511332571505
Loss made of: CE 0.13240014016628265, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.573442459106445 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.2967556022107605
Loss made of: CE 0.11748626083135605, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.889158010482788 EntMin 0.0
Epoch 2, Class Loss=0.16637291014194489, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.16637291014194489, Class Loss=0.16637291014194489, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/105, Loss=4.343664035201073
Loss made of: CE 0.11559593677520752, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.931685209274292 EntMin 0.0
Epoch 3, Batch 20/105, Loss=4.587405696511269
Loss made of: CE 0.19467870891094208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.369479179382324 EntMin 0.0
Epoch 3, Batch 30/105, Loss=4.297859521210194
Loss made of: CE 0.2029089629650116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.197171688079834 EntMin 0.0
Epoch 3, Batch 40/105, Loss=4.628873677551747
Loss made of: CE 0.26411598920822144, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.078879356384277 EntMin 0.0
Epoch 3, Batch 50/105, Loss=4.328228983283043
Loss made of: CE 0.1526494324207306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.902071714401245 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.452737936377526
Loss made of: CE 0.23846027255058289, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7956204414367676 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.289530318975449
Loss made of: CE 0.2161494493484497, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7132515907287598 EntMin 0.0
Epoch 3, Batch 80/105, Loss=4.4280545428395275
Loss made of: CE 0.31249117851257324, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.146170616149902 EntMin 0.0
Epoch 3, Batch 90/105, Loss=4.263002802431584
Loss made of: CE 0.18357157707214355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.060726165771484 EntMin 0.0
Epoch 3, Batch 100/105, Loss=4.285210247337818
Loss made of: CE 0.20943531394004822, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.553600788116455 EntMin 0.0
Epoch 3, Class Loss=0.22910527884960175, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.22910527884960175, Class Loss=0.22910527884960175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/105, Loss=4.197734686732292
Loss made of: CE 0.2731747627258301, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.764565944671631 EntMin 0.0
Epoch 4, Batch 20/105, Loss=4.3543072387576105
Loss made of: CE 0.23931090533733368, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8638548851013184 EntMin 0.0
Epoch 4, Batch 30/105, Loss=4.238540674746036
Loss made of: CE 0.26356053352355957, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8334739208221436 EntMin 0.0
Epoch 4, Batch 40/105, Loss=4.488746163249016
Loss made of: CE 0.21712931990623474, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7638213634490967 EntMin 0.0
Epoch 4, Batch 50/105, Loss=4.332047910988331
Loss made of: CE 0.2606002390384674, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9660511016845703 EntMin 0.0
Epoch 4, Batch 60/105, Loss=4.348602768778801
Loss made of: CE 0.2497287094593048, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8807430267333984 EntMin 0.0
Epoch 4, Batch 70/105, Loss=4.419247263669968
Loss made of: CE 0.2958623170852661, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6617889404296875 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.2308988779783245
Loss made of: CE 0.3229537606239319, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0697174072265625 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.448698191344738
Loss made of: CE 0.34054994583129883, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.546792030334473 EntMin 0.0
Epoch 4, Batch 100/105, Loss=4.363116227090359
Loss made of: CE 0.24681691825389862, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.864715099334717 EntMin 0.0
Epoch 4, Class Loss=0.2864428758621216, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.2864428758621216, Class Loss=0.2864428758621216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/105, Loss=4.7773652613163
Loss made of: CE 0.37078773975372314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.045895576477051 EntMin 0.0
Epoch 5, Batch 20/105, Loss=4.3932875841856
Loss made of: CE 0.2962143123149872, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.043472766876221 EntMin 0.0
Epoch 5, Batch 30/105, Loss=4.316830244660378
Loss made of: CE 0.32201898097991943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6376404762268066 EntMin 0.0
Epoch 5, Batch 40/105, Loss=4.835700815916061
Loss made of: CE 0.34609436988830566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.894534111022949 EntMin 0.0
Epoch 5, Batch 50/105, Loss=4.30629960000515
Loss made of: CE 0.3530126214027405, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.144968509674072 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.4497689485549925
Loss made of: CE 0.36406511068344116, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.863879680633545 EntMin 0.0
Epoch 5, Batch 70/105, Loss=4.21843850016594
Loss made of: CE 0.29473060369491577, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6968142986297607 EntMin 0.0
Epoch 5, Batch 80/105, Loss=4.565779902040958
Loss made of: CE 0.3273753821849823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.546511173248291 EntMin 0.0
Epoch 5, Batch 90/105, Loss=4.458403933048248
Loss made of: CE 0.3282555341720581, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9475021362304688 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.350054654479027
Loss made of: CE 0.40581950545310974, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.490793228149414 EntMin 0.0
Epoch 5, Class Loss=0.3330785632133484, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3330785632133484, Class Loss=0.3330785632133484, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/105, Loss=4.34324101805687
Loss made of: CE 0.3357292115688324, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.669595718383789 EntMin 0.0
Epoch 6, Batch 20/105, Loss=4.777694770693779
Loss made of: CE 0.5333883762359619, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.2378435134887695 EntMin 0.0
Epoch 6, Batch 30/105, Loss=4.300885352492332
Loss made of: CE 0.40959101915359497, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.360849380493164 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.188927213847637
Loss made of: CE 0.2931182086467743, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4222073554992676 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.2839664876461026
Loss made of: CE 0.31037065386772156, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.608293056488037 EntMin 0.0
Epoch 6, Batch 60/105, Loss=4.464114490151405
Loss made of: CE 0.3634631633758545, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.147512912750244 EntMin 0.0
Epoch 6, Batch 70/105, Loss=4.19512993991375
Loss made of: CE 0.2458033561706543, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.103590488433838 EntMin 0.0
Epoch 6, Batch 80/105, Loss=4.6090398013591765
Loss made of: CE 0.3293188810348511, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481544494628906 EntMin 0.0
Epoch 6, Batch 90/105, Loss=4.2544467687606815
Loss made of: CE 0.28584349155426025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.419131278991699 EntMin 0.0
Epoch 6, Batch 100/105, Loss=4.287696877121926
Loss made of: CE 0.29226934909820557, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.736872434616089 EntMin 0.0
Epoch 6, Class Loss=0.36680877208709717, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.36680877208709717, Class Loss=0.36680877208709717, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=4.18762362934649
Loss made of: CE 0.08126905560493469, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.481107234954834 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.906159930676222
Loss made of: CE 0.10771675407886505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.866147041320801 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.412965153902769
Loss made of: CE 0.057932667434215546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6980338096618652 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.250549090281129
Loss made of: CE 0.06818298995494843, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8175323009490967 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.077482226118446
Loss made of: CE 0.06010974198579788, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7532873153686523 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.871140566468239
Loss made of: CE 0.10403569787740707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.089796543121338 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.5531997688114645
Loss made of: CE 0.10497318208217621, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.613214015960693 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.228947437927127
Loss made of: CE 0.07073383033275604, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.898040533065796 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.423090926557779
Loss made of: CE 0.0737980529665947, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6074132919311523 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.4676745686680075
Loss made of: CE 0.07029125094413757, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.012762069702148 EntMin 0.0
Epoch 1, Class Loss=0.08713797479867935, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.08713797479867935, Class Loss=0.08713797479867935, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/102, Loss=4.188176077604294
Loss made of: CE 0.1737741231918335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8489508628845215 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.31572150811553
Loss made of: CE 0.10396566241979599, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.928910255432129 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 30/102, Loss=4.3534688673913475
Loss made of: CE 0.14823588728904724, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0409255027771 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.41703133136034
Loss made of: CE 0.23251178860664368, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.703420639038086 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.154670717567205
Loss made of: CE 0.15235662460327148, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4962778091430664 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.530223979800939
Loss made of: CE 0.09936880320310593, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.465171813964844 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.60508832782507
Loss made of: CE 0.1500406563282013, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7299981117248535 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.3717660017311575
Loss made of: CE 0.1737411469221115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.670590400695801 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.178530537337065
Loss made of: CE 0.19873234629631042, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.443035125732422 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.158060502260923
Loss made of: CE 0.11787300556898117, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.963912010192871 EntMin 0.0
Epoch 2, Class Loss=0.15736956894397736, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.15736956894397736, Class Loss=0.15736956894397736, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/102, Loss=4.50569429397583
Loss made of: CE 0.18777206540107727, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8834893703460693 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.731994861364365
Loss made of: CE 0.28223633766174316, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7248454093933105 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.216886891424656
Loss made of: CE 0.2926236391067505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.248892784118652 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.4178122818470005
Loss made of: CE 0.18746623396873474, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8120059967041016 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.493770308792591
Loss made of: CE 0.27413496375083923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.514122009277344 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.230473470687866
Loss made of: CE 0.21191221475601196, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7236907482147217 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.393355141580105
Loss made of: CE 0.19697408378124237, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.072202682495117 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.3314306572079655
Loss made of: CE 0.24372106790542603, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.013275146484375 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.243712522089481
Loss made of: CE 0.19952604174613953, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.826906204223633 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.247953213751316
Loss made of: CE 0.17182737588882446, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.796800136566162 EntMin 0.0
Epoch 3, Class Loss=0.2214033007621765, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.2214033007621765, Class Loss=0.2214033007621765, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/102, Loss=4.527704082429409
Loss made of: CE 0.2533491551876068, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9677863121032715 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.436564901471138
Loss made of: CE 0.24594801664352417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.168013572692871 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.523220589756965
Loss made of: CE 0.2983221709728241, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7651963233947754 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.0340384393930435
Loss made of: CE 0.24755963683128357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.022966384887695 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.499809804558754
Loss made of: CE 0.2735316753387451, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.112176895141602 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.262094761431217
Loss made of: CE 0.2507377564907074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6911544799804688 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.212690472602844
Loss made of: CE 0.2350105494260788, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.276908874511719 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.117720821499825
Loss made of: CE 0.23683315515518188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.865816831588745 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.684778414666653
Loss made of: CE 0.26961517333984375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.26199197769165 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.1997557684779165
Loss made of: CE 0.21772176027297974, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7397210597991943 EntMin 0.0
Epoch 4, Class Loss=0.2702624797821045, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.2702624797821045, Class Loss=0.2702624797821045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/102, Loss=4.15738225877285
Loss made of: CE 0.3338025212287903, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.580904483795166 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.540707927942276
Loss made of: CE 0.30311042070388794, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.254522800445557 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.314940167963504
Loss made of: CE 0.2185576856136322, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.985295295715332 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.236024111509323
Loss made of: CE 0.2671111524105072, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.306273937225342 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.238913394510746
Loss made of: CE 0.3154548406600952, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.548489570617676 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.213640293478965
Loss made of: CE 0.3374534547328949, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.089725494384766 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.055158242583275
Loss made of: CE 0.31079453229904175, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6347923278808594 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.13493483364582
Loss made of: CE 0.25272467732429504, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.01639461517334 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.772904318571091
Loss made of: CE 0.3857917785644531, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.229540824890137 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.129143726825714
Loss made of: CE 0.23769310116767883, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5808205604553223 EntMin 0.0
Epoch 5, Class Loss=0.3058599829673767, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.3058599829673767, Class Loss=0.3058599829673767, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/102, Loss=4.471649369597435
Loss made of: CE 0.3579065501689911, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9199013710021973 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.436189138889313
Loss made of: CE 0.3650745749473572, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.883823871612549 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.5044619858264925
Loss made of: CE 0.31234484910964966, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.241055011749268 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.256280699372292
Loss made of: CE 0.3512127995491028, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.903114080429077 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.737911838293075
Loss made of: CE 0.26615816354751587, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9018514156341553 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.298153755068779
Loss made of: CE 0.36556053161621094, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.110202789306641 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.458450904488563
Loss made of: CE 0.3487445116043091, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.24239444732666 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.375007581710816
Loss made of: CE 0.32842689752578735, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7467610836029053 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.323954063653946
Loss made of: CE 0.2817417085170746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.761300086975098 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.221463364362717
Loss made of: CE 0.35514646768569946, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2960991859436035 EntMin 0.0
Epoch 6, Class Loss=0.34012162685394287, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.34012162685394287, Class Loss=0.34012162685394287, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5359854102134705, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.832188
Mean Acc: 0.432584
FreqW Acc: 0.710932
Mean IoU: 0.364374
Class IoU:
	class 0: 0.8411681
	class 1: 0.5840433
	class 2: 0.32424647
	class 3: 0.34367603
	class 4: 0.587145
	class 5: 0.00071750244
	class 6: 0.6893896
	class 7: 0.46565548
	class 8: 0.2304327
	class 9: 0.0
	class 10: 0.3830372
	class 11: 0.17056176
	class 12: 0.31592542
	class 13: 0.0
	class 14: 0.6420653
	class 15: 0.61628836
	class 16: 0.0
Class Acc:
	class 0: 0.98176736
	class 1: 0.5871169
	class 2: 0.6096378
	class 3: 0.34894413
	class 4: 0.78707105
	class 5: 0.00071798306
	class 6: 0.70307666
	class 7: 0.4732545
	class 8: 0.23216422
	class 9: 0.0
	class 10: 0.45472017
	class 11: 0.17531888
	class 12: 0.54307646
	class 13: 0.0
	class 14: 0.6896622
	class 15: 0.76739657
	class 16: 0.0

federated global round: 17, step: 3
select part of clients to conduct local training
[5, 3, 17, 11]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=3.6919180288910867
Loss made of: CE 0.04609299823641777, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.311100482940674 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/102, Loss=4.1138000367209315
Loss made of: CE 0.024255966767668724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.606799602508545 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.747624667733908
Loss made of: CE 0.08218541741371155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.365791082382202 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.8043971978127957
Loss made of: CE 0.037464290857315063, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.015033721923828 EntMin 0.0
Epoch 1, Batch 50/102, Loss=3.848124926164746
Loss made of: CE 0.047037433832883835, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.917945623397827 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.182305963896215
Loss made of: CE 0.045911114662885666, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389578819274902 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.284111035987735
Loss made of: CE 0.049371927976608276, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9206292629241943 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.105897924304008
Loss made of: CE 0.06429698318243027, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.125482559204102 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.9666303945705295
Loss made of: CE 0.03873369097709656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8949944972991943 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.7530812066048385
Loss made of: CE 0.04955050349235535, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.202363967895508 EntMin 0.0
Epoch 1, Class Loss=0.05344788357615471, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.05344788357615471, Class Loss=0.05344788357615471, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.162541672587395
Loss made of: CE 0.10767415165901184, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8441405296325684 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.04804166033864
Loss made of: CE 0.10759278386831284, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.418771743774414 EntMin 0.0
Epoch 2, Batch 30/102, Loss=3.9991617657244207
Loss made of: CE 0.08603069186210632, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.579432964324951 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.1386316388845446
Loss made of: CE 0.15004853904247284, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.152705192565918 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.325204526633025
Loss made of: CE 0.11188213527202606, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.160045623779297 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.155680161714554
Loss made of: CE 0.14667744934558868, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7860794067382812 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.9868952728807927
Loss made of: CE 0.15954218804836273, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.904667854309082 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.2588587261736395
Loss made of: CE 0.13799110054969788, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.694399833679199 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.9943079106509685
Loss made of: CE 0.09720174968242645, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.387392044067383 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.368816647678614
Loss made of: CE 0.12856461107730865, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.890413761138916 EntMin 0.0
Epoch 2, Class Loss=0.11332947760820389, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.11332947760820389, Class Loss=0.11332947760820389, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.063938847184181
Loss made of: CE 0.17299441993236542, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8350934982299805 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.134222580492496
Loss made of: CE 0.1441778987646103, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9125754833221436 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.0436062321066855
Loss made of: CE 0.16649144887924194, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3690900802612305 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.3116018861532215
Loss made of: CE 0.18642282485961914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.299224376678467 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.098293601721525
Loss made of: CE 0.20301871001720428, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5985779762268066 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.01249602213502
Loss made of: CE 0.20751458406448364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.158590316772461 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.013552898168564
Loss made of: CE 0.14944343268871307, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.760266065597534 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.38793079406023
Loss made of: CE 0.13283184170722961, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.567434072494507 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.147456155717373
Loss made of: CE 0.13470357656478882, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.642230987548828 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.87184776738286
Loss made of: CE 0.1604619324207306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.828892230987549 EntMin 0.0
Epoch 3, Class Loss=0.16895227134227753, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.16895227134227753, Class Loss=0.16895227134227753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.196939535439014
Loss made of: CE 0.20470675826072693, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7659192085266113 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.8688111677765846
Loss made of: CE 0.2159898728132248, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.778803825378418 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.390425616502762
Loss made of: CE 0.2236158549785614, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.961008071899414 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.216407579183579
Loss made of: CE 0.20272141695022583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8680152893066406 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.208597478270531
Loss made of: CE 0.18173804879188538, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8768882751464844 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.220437870919705
Loss made of: CE 0.29057174921035767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.64277720451355 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.154583752155304
Loss made of: CE 0.24009057879447937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2793121337890625 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.050806182622909
Loss made of: CE 0.25286203622817993, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6437602043151855 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.1655901536345485
Loss made of: CE 0.27962639927864075, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7460744380950928 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.2905799254775046
Loss made of: CE 0.17564791440963745, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8785557746887207 EntMin 0.0
Epoch 4, Class Loss=0.2161617875099182, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.2161617875099182, Class Loss=0.2161617875099182, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.163009180128574
Loss made of: CE 0.2532138228416443, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.858952045440674 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.057529984414577
Loss made of: CE 0.2608890235424042, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.798530340194702 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.965207812190056
Loss made of: CE 0.25811463594436646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.842174530029297 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.032809655368328
Loss made of: CE 0.30794793367385864, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5693793296813965 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.122279290854931
Loss made of: CE 0.23138773441314697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7135329246520996 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.040579338371754
Loss made of: CE 0.24949350953102112, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.424751043319702 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.14485455006361
Loss made of: CE 0.2059837430715561, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.802515745162964 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.596149006485939
Loss made of: CE 0.3006771504878998, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.548549175262451 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.959540309011936
Loss made of: CE 0.2617945373058319, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7789788246154785 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.134948340058327
Loss made of: CE 0.23351821303367615, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.71484375 EntMin 0.0
Epoch 5, Class Loss=0.25839143991470337, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.25839143991470337, Class Loss=0.25839143991470337, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.11047939658165
Loss made of: CE 0.2910114526748657, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7753114700317383 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.035065686702728
Loss made of: CE 0.30930137634277344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.313109397888184 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.01158698797226
Loss made of: CE 0.32896387577056885, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5765066146850586 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.329187706112862
Loss made of: CE 0.2777419686317444, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.693645477294922 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.264217206835747
Loss made of: CE 0.41455501317977905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8555097579956055 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.068125125765801
Loss made of: CE 0.23546215891838074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.344634771347046 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.004273208975792
Loss made of: CE 0.2682505249977112, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.504551410675049 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.8900529474020002
Loss made of: CE 0.28583845496177673, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.715729236602783 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 90/102, Loss=4.360652269423008
Loss made of: CE 0.2794714570045471, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8762364387512207 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.338608819246292
Loss made of: CE 0.31620657444000244, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.619436264038086 EntMin 0.0
Epoch 6, Class Loss=0.29992035031318665, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.29992035031318665, Class Loss=0.29992035031318665, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=3.844853216409683
Loss made of: CE 0.04703778028488159, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427835464477539 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.821853982284665
Loss made of: CE 0.03201157972216606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.256416082382202 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.056142490729689
Loss made of: CE 0.03927719593048096, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3436079025268555 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.9694103438407184
Loss made of: CE 0.15744008123874664, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.366559982299805 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.100501485913992
Loss made of: CE 0.05476181209087372, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1339616775512695 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.130054010637105
Loss made of: CE 0.029054909944534302, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6407370567321777 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.030919918976724
Loss made of: CE 0.06794565171003342, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.866994857788086 EntMin 0.0
Epoch 1, Batch 80/102, Loss=3.9672643976286053
Loss made of: CE 0.050553541630506516, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.276608467102051 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.19920497983694
Loss made of: CE 0.04958753287792206, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6710782051086426 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.074551093764603
Loss made of: CE 0.028770064935088158, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.570528745651245 EntMin 0.0
Epoch 1, Class Loss=0.050483785569667816, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.050483785569667816, Class Loss=0.050483785569667816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.157266739010811
Loss made of: CE 0.15344904363155365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.132479667663574 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.9357556156814097
Loss made of: CE 0.14768680930137634, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6508655548095703 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.301536644250154
Loss made of: CE 0.11289142072200775, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.646531581878662 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.032910999655724
Loss made of: CE 0.07818304002285004, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3117785453796387 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.1098082952201365
Loss made of: CE 0.09209750592708588, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3774030208587646 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.100270502269268
Loss made of: CE 0.1170930415391922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.578475475311279 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.137758449465037
Loss made of: CE 0.12359744310379028, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.170895576477051 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.151327547430992
Loss made of: CE 0.10037370026111603, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.257938861846924 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.193403003364802
Loss made of: CE 0.07944929599761963, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5960922241210938 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.3161604180932045
Loss made of: CE 0.10430361330509186, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6857316493988037 EntMin 0.0
Epoch 2, Class Loss=0.11024010926485062, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.11024010926485062, Class Loss=0.11024010926485062, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.242552909255028
Loss made of: CE 0.15741212666034698, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.899254322052002 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.048078908771276
Loss made of: CE 0.1852247416973114, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3553266525268555 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.285955727100372
Loss made of: CE 0.2290528267621994, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.011897563934326 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.074370867013931
Loss made of: CE 0.15821105241775513, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.12510347366333 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.481405843049288
Loss made of: CE 0.22459396719932556, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.069624900817871 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.941315843164921
Loss made of: CE 0.1763414442539215, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.393052101135254 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.29023985043168
Loss made of: CE 0.12116231769323349, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0163774490356445 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.2759894877672195
Loss made of: CE 0.13844603300094604, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.796527624130249 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.042217303812504
Loss made of: CE 0.1350165605545044, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5019748210906982 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.106150296330452
Loss made of: CE 0.19627925753593445, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2364091873168945 EntMin 0.0
Epoch 3, Class Loss=0.16937129199504852, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.16937129199504852, Class Loss=0.16937129199504852, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.016839390993118
Loss made of: CE 0.19167183339595795, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6309142112731934 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.361935269832611
Loss made of: CE 0.2564125061035156, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.752439260482788 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.0843544319272045
Loss made of: CE 0.21161150932312012, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5870165824890137 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.364300130307674
Loss made of: CE 0.2179255485534668, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.537189483642578 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.23894888907671
Loss made of: CE 0.21341638267040253, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8135218620300293 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.135688592493534
Loss made of: CE 0.23275348544120789, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.970573902130127 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 70/102, Loss=4.603933914005756
Loss made of: CE 0.2533006966114044, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.964876174926758 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.340585790574551
Loss made of: CE 0.1584719568490982, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.020523548126221 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.148804061114788
Loss made of: CE 0.1819457709789276, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4485347270965576 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.13998030424118
Loss made of: CE 0.24340955913066864, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7944679260253906 EntMin 0.0
Epoch 4, Class Loss=0.21898698806762695, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.21898698806762695, Class Loss=0.21898698806762695, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.1042717948555945
Loss made of: CE 0.31759339570999146, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.751220464706421 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.2159506410360335
Loss made of: CE 0.2647244334220886, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.614449977874756 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.070411574840546
Loss made of: CE 0.26449859142303467, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.403885841369629 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.193424007296562
Loss made of: CE 0.30035173892974854, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4018352031707764 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.325232689082623
Loss made of: CE 0.1866513341665268, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.686894178390503 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.040993919968605
Loss made of: CE 0.24123987555503845, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.561758041381836 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.353655947744846
Loss made of: CE 0.22704677283763885, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.82216739654541 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.576973928511142
Loss made of: CE 0.2772810459136963, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.629186630249023 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.0789566993713375
Loss made of: CE 0.22435781359672546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9266695976257324 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.087355633080006
Loss made of: CE 0.2451159954071045, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.592292308807373 EntMin 0.0
Epoch 5, Class Loss=0.26179569959640503, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.26179569959640503, Class Loss=0.26179569959640503, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.102662259340287
Loss made of: CE 0.3141607940196991, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4156956672668457 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.0809020608663555
Loss made of: CE 0.2993417978286743, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.526944637298584 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.189109854400158
Loss made of: CE 0.29045116901397705, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.485446929931641 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.8843686282634735
Loss made of: CE 0.23507019877433777, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3363285064697266 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.133729077875614
Loss made of: CE 0.2659929692745209, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.354935646057129 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.110421714186669
Loss made of: CE 0.27973687648773193, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5342421531677246 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.855883763730526
Loss made of: CE 0.27334892749786377, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.556915044784546 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.041422480344773
Loss made of: CE 0.3303387761116028, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.554948568344116 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.365250265598297
Loss made of: CE 0.2780624330043793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.398527145385742 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.204499433934688
Loss made of: CE 0.25767090916633606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6340208053588867 EntMin 0.0
Epoch 6, Class Loss=0.296628475189209, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.296628475189209, Class Loss=0.296628475189209, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=5.318782418221235
Loss made of: CE 0.2629792094230652, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7401628494262695 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.75405382886529
Loss made of: CE 0.10792669653892517, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.531578063964844 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.14000938832759857, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.14000938832759857, Class Loss=0.14000938832759857, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/23, Loss=4.6332405537366865
Loss made of: CE 0.31535694003105164, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7923972606658936 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.4887637227773665
Loss made of: CE 0.22141987085342407, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.154814720153809 EntMin 0.0
Epoch 2, Class Loss=0.2897045910358429, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.2897045910358429, Class Loss=0.2897045910358429, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/23, Loss=4.557218363881111
Loss made of: CE 0.36475443840026855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.048247337341309 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.53413662314415
Loss made of: CE 0.5289158225059509, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.290508270263672 EntMin 0.0
Epoch 3, Class Loss=0.4039510190486908, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.4039510190486908, Class Loss=0.4039510190486908, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/23, Loss=4.686231020092964
Loss made of: CE 0.4950231909751892, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8026928901672363 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.39816030561924
Loss made of: CE 0.3315412402153015, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.12437629699707 EntMin 0.0
Epoch 4, Class Loss=0.43261411786079407, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.43261411786079407, Class Loss=0.43261411786079407, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/23, Loss=4.5549837946891785
Loss made of: CE 0.41454386711120605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.222787857055664 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.256000289320946
Loss made of: CE 0.33527156710624695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.019569396972656 EntMin 0.0
Epoch 5, Class Loss=0.4358890950679779, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.4358890950679779, Class Loss=0.4358890950679779, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/23, Loss=4.469071117043495
Loss made of: CE 0.4806938171386719, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.783348560333252 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.347032874822617
Loss made of: CE 0.5001734495162964, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.83225154876709 EntMin 0.0
Epoch 6, Class Loss=0.45341524481773376, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.45341524481773376, Class Loss=0.45341524481773376, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.311400657892227
Loss made of: CE 0.6499776840209961, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.865713119506836 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.398443549871445
Loss made of: CE 0.6184341311454773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7605502605438232 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.364283531904221
Loss made of: CE 0.5625214576721191, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.156702995300293 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.686668041348457
Loss made of: CE 0.4894365966320038, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.16183614730835 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.636122843623161
Loss made of: CE 0.4719003140926361, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.879828453063965 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.596522870659828
Loss made of: CE 0.5221414566040039, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.381258726119995 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.414250785112381
Loss made of: CE 0.5412687659263611, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8414998054504395 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.526066398620605
Loss made of: CE 0.5849895477294922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4277167320251465 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.26791110932827
Loss made of: CE 0.5719362497329712, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.293635845184326 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.384457165002823
Loss made of: CE 0.6478834748268127, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7979302406311035 EntMin 0.0
Epoch 1, Class Loss=0.5923234820365906, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.5923234820365906, Class Loss=0.5923234820365906, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000600
Epoch 2, Batch 10/102, Loss=4.332918813824653
Loss made of: CE 0.5026674866676331, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.470900297164917 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.2437185674905775
Loss made of: CE 0.5099778771400452, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6939735412597656 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.388866958022118
Loss made of: CE 0.6832011938095093, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6207337379455566 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.410554817318916
Loss made of: CE 0.6173219680786133, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3720109462738037 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.439036962389946
Loss made of: CE 0.5322020053863525, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.762341022491455 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.474446594715118
Loss made of: CE 0.6590667963027954, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.466435432434082 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.099119251966476
Loss made of: CE 0.5770289301872253, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3414292335510254 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.42780020236969
Loss made of: CE 0.6801633834838867, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2843122482299805 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.383493089675904
Loss made of: CE 0.6422533392906189, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.641598701477051 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.52906944155693
Loss made of: CE 0.4937232732772827, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.021380424499512 EntMin 0.0
Epoch 2, Class Loss=0.5685049295425415, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.5685049295425415, Class Loss=0.5685049295425415, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/102, Loss=4.332919120788574
Loss made of: CE 0.5895696878433228, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.017206192016602 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.2673068106174465
Loss made of: CE 0.5390397906303406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2687973976135254 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.34964399933815
Loss made of: CE 0.5457587242126465, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4088358879089355 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.3501846343278885
Loss made of: CE 0.5810397863388062, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7736117839813232 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.51070022881031
Loss made of: CE 0.615676760673523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.396015167236328 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.3144447147846225
Loss made of: CE 0.5507476329803467, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.800461530685425 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.183402079343796
Loss made of: CE 0.4518760144710541, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8022608757019043 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.5649871826171875
Loss made of: CE 0.5442941188812256, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.217372417449951 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.447399866580963
Loss made of: CE 0.4271836280822754, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2799670696258545 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.339846417307854
Loss made of: CE 0.5687386393547058, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.193204879760742 EntMin 0.0
Epoch 3, Class Loss=0.5463268160820007, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.5463268160820007, Class Loss=0.5463268160820007, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/102, Loss=4.260067576169968
Loss made of: CE 0.5599372386932373, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2851996421813965 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.500902563333511
Loss made of: CE 0.4967937469482422, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.347020626068115 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.326205867528915
Loss made of: CE 0.6273086667060852, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8791756629943848 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.191427224874497
Loss made of: CE 0.4641150236129761, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2214550971984863 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.480212172865867
Loss made of: CE 0.621710479259491, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.008932590484619 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.470084086060524
Loss made of: CE 0.4707201421260834, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8550643920898438 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.39772869348526
Loss made of: CE 0.7020167112350464, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6860690116882324 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.484577739238739
Loss made of: CE 0.6836113333702087, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.793632507324219 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.45684899687767
Loss made of: CE 0.42973482608795166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.597851276397705 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.3589649528265
Loss made of: CE 0.6104613542556763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.711124897003174 EntMin 0.0
Epoch 4, Class Loss=0.5465166568756104, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5465166568756104, Class Loss=0.5465166568756104, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000504
Epoch 5, Batch 10/102, Loss=4.588585546612739
Loss made of: CE 0.5820972323417664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5235066413879395 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.53187462091446
Loss made of: CE 0.6153745651245117, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7761034965515137 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.376902115345001
Loss made of: CE 0.457817405462265, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.482572317123413 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.177667474746704
Loss made of: CE 0.5690973997116089, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7676711082458496 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.436092242598534
Loss made of: CE 0.5769904255867004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.180603981018066 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.088826209306717
Loss made of: CE 0.7575650215148926, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8267574310302734 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.168992844223976
Loss made of: CE 0.5796996355056763, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.594672203063965 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.275190359354019
Loss made of: CE 0.38039708137512207, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8248233795166016 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.319808334112167
Loss made of: CE 0.5412458777427673, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.319580554962158 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.3829954892396925
Loss made of: CE 0.5311600565910339, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1197309494018555 EntMin 0.0
Epoch 5, Class Loss=0.5299908518791199, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5299908518791199, Class Loss=0.5299908518791199, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000471
Epoch 6, Batch 10/102, Loss=4.596747151017189
Loss made of: CE 0.48553335666656494, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.519866704940796 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.162103626132011
Loss made of: CE 0.533759593963623, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5900349617004395 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.029714491963387
Loss made of: CE 0.49738869071006775, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.677920341491699 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.408730933070183
Loss made of: CE 0.607589840888977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.672004222869873 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.155704540014267
Loss made of: CE 0.47948384284973145, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.319749593734741 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.290643399953842
Loss made of: CE 0.5852175951004028, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.684813976287842 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.026350483298302
Loss made of: CE 0.5865942239761353, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5665411949157715 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.2278516560792925
Loss made of: CE 0.5376366972923279, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.57241153717041 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.326239040493965
Loss made of: CE 0.5560631155967712, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.680668354034424 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.3566688925027846
Loss made of: CE 0.581402063369751, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.400798797607422 EntMin 0.0
Epoch 6, Class Loss=0.5265011191368103, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5265011191368103, Class Loss=0.5265011191368103, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4968429207801819, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.844255
Mean Acc: 0.484349
FreqW Acc: 0.734894
Mean IoU: 0.396062
Class IoU:
	class 0: 0.8618105
	class 1: 0.6456351
	class 2: 0.34524947
	class 3: 0.3709645
	class 4: 0.5704197
	class 5: 0.0016041772
	class 6: 0.7484888
	class 7: 0.55393195
	class 8: 0.2639322
	class 9: 0.0
	class 10: 0.4266895
	class 11: 0.24059452
	class 12: 0.34126708
	class 13: 0.033811018
	class 14: 0.707903
	class 15: 0.62076074
	class 16: 0.0
Class Acc:
	class 0: 0.9759981
	class 1: 0.6510721
	class 2: 0.6938157
	class 3: 0.3776907
	class 4: 0.8358695
	class 5: 0.0016052683
	class 6: 0.7745636
	class 7: 0.5660304
	class 8: 0.2659865
	class 9: 0.0
	class 10: 0.5315657
	class 11: 0.26202524
	class 12: 0.64317554
	class 13: 0.033832554
	class 14: 0.8289779
	class 15: 0.7917249
	class 16: 0.0

federated global round: 18, step: 3
select part of clients to conduct local training
[9, 11, 13, 10]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.194126445613802
Loss made of: CE 0.054463937878608704, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3796324729919434 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.8573039228096606
Loss made of: CE 0.03481905162334442, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3997669219970703 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.948229576274753
Loss made of: CE 0.040445588529109955, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.98061466217041 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.817897369712591
Loss made of: CE 0.03532583266496658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.482964277267456 EntMin 0.0
Epoch 1, Batch 50/102, Loss=3.910999082773924
Loss made of: CE 0.09259150922298431, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8307857513427734 EntMin 0.0
Epoch 1, Batch 60/102, Loss=3.849865296855569
Loss made of: CE 0.05095229670405388, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.022383213043213 EntMin 0.0
Epoch 1, Batch 70/102, Loss=3.9021727776154878
Loss made of: CE 0.047707557678222656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.645658254623413 EntMin 0.0
Epoch 1, Batch 80/102, Loss=3.9954132948070766
Loss made of: CE 0.0517597496509552, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7879140377044678 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.8849250309169294
Loss made of: CE 0.04305257648229599, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.485741138458252 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.763554248586297
Loss made of: CE 0.033784665167331696, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5246598720550537 EntMin 0.0
Epoch 1, Class Loss=0.044204406440258026, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.044204406440258026, Class Loss=0.044204406440258026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/102, Loss=3.88402144536376
Loss made of: CE 0.0809868574142456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4591662883758545 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.131802503019571
Loss made of: CE 0.09180520474910736, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8579559326171875 EntMin 0.0
Epoch 2, Batch 30/102, Loss=3.9980421513319016
Loss made of: CE 0.15613281726837158, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7068004608154297 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.115197483450174
Loss made of: CE 0.11375492811203003, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.551812171936035 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.24916112869978
Loss made of: CE 0.07636290043592453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.535367965698242 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.036221377924084
Loss made of: CE 0.07419037818908691, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8289668560028076 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.9594184808433055
Loss made of: CE 0.0996924489736557, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.793412685394287 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.014815702289343
Loss made of: CE 0.09815149009227753, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5822741985321045 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.954402939230204
Loss made of: CE 0.07135049998760223, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6565628051757812 EntMin 0.0
Epoch 2, Batch 100/102, Loss=3.7305154621601107
Loss made of: CE 0.08432674407958984, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.581794500350952 EntMin 0.0
Epoch 2, Class Loss=0.10016846656799316, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.10016846656799316, Class Loss=0.10016846656799316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/102, Loss=4.037878820300103
Loss made of: CE 0.14832176268100739, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.294538974761963 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.206995300203562
Loss made of: CE 0.15897537767887115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.171626091003418 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.9187435448169707
Loss made of: CE 0.14701926708221436, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7662596702575684 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.133488772064448
Loss made of: CE 0.1255846917629242, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.660658597946167 EntMin 0.0
Epoch 3, Batch 50/102, Loss=3.766045345366001
Loss made of: CE 0.19658808410167694, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.733816146850586 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.9688676431775094
Loss made of: CE 0.2025444507598877, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8747124671936035 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.279529143124819
Loss made of: CE 0.2083585560321808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.364574432373047 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.030639478564263
Loss made of: CE 0.1498488187789917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.35275936126709 EntMin 0.0
Epoch 3, Batch 90/102, Loss=3.759888207912445
Loss made of: CE 0.14617975056171417, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7299880981445312 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.9111417971551417
Loss made of: CE 0.15230876207351685, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6337249279022217 EntMin 0.0
Epoch 3, Class Loss=0.15017884969711304, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.15017884969711304, Class Loss=0.15017884969711304, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/102, Loss=4.12598232626915
Loss made of: CE 0.24607425928115845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481598854064941 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.437043011188507
Loss made of: CE 0.18391025066375732, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.068786144256592 EntMin 0.0
Epoch 4, Batch 30/102, Loss=3.9617760628461838
Loss made of: CE 0.21531942486763, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7566895484924316 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.183884686231613
Loss made of: CE 0.1990862488746643, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8857431411743164 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.056722102314234
Loss made of: CE 0.1217370256781578, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8591105937957764 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.103277362883091
Loss made of: CE 0.14121122658252716, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.22381591796875 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.158889766037464
Loss made of: CE 0.22405469417572021, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7944588661193848 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.074759018421173
Loss made of: CE 0.22473309934139252, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.28951358795166 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.1948567688465115
Loss made of: CE 0.1958259791135788, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.648177146911621 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.1236162096261975
Loss made of: CE 0.2173268049955368, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2234601974487305 EntMin 0.0
Epoch 4, Class Loss=0.20663823187351227, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.20663823187351227, Class Loss=0.20663823187351227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.023381596803665
Loss made of: CE 0.275651752948761, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.184313774108887 EntMin 0.0
Epoch 5, Batch 20/102, Loss=3.878959284722805
Loss made of: CE 0.22790774703025818, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6990437507629395 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 30/102, Loss=3.896218179166317
Loss made of: CE 0.23863033950328827, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.88913631439209 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.227637256681919
Loss made of: CE 0.24637995660305023, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6631808280944824 EntMin 0.0
Epoch 5, Batch 50/102, Loss=3.9333245396614074
Loss made of: CE 0.1894199550151825, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5120866298675537 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.060851588845253
Loss made of: CE 0.3861139416694641, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.811077117919922 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.1196448907256125
Loss made of: CE 0.2824849784374237, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7664716243743896 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.093059132993221
Loss made of: CE 0.18776831030845642, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6531848907470703 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.0776103138923645
Loss made of: CE 0.2633852958679199, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2142608165740967 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.056558965146541
Loss made of: CE 0.21047084033489227, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3242881298065186 EntMin 0.0
Epoch 5, Class Loss=0.24321047961711884, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.24321047961711884, Class Loss=0.24321047961711884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/102, Loss=4.183363875746727
Loss made of: CE 0.2975381016731262, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7497382164001465 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.8572983115911486
Loss made of: CE 0.2985076904296875, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6147754192352295 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.9530576318502426
Loss made of: CE 0.29787859320640564, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3109912872314453 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.068552595376969
Loss made of: CE 0.2271668165922165, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2439870834350586 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.016708914935589
Loss made of: CE 0.27739420533180237, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.743072271347046 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.14745702445507
Loss made of: CE 0.24132561683654785, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3127810955047607 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.1481868654489515
Loss made of: CE 0.2968151569366455, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0841875076293945 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.9981905609369277
Loss made of: CE 0.2838355600833893, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.884028196334839 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.9296949923038484
Loss made of: CE 0.29969578981399536, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.670013189315796 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.826075814664364
Loss made of: CE 0.24570906162261963, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.213966131210327 EntMin 0.0
Epoch 6, Class Loss=0.28284311294555664, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.28284311294555664, Class Loss=0.28284311294555664, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000438
Epoch 1, Batch 10/102, Loss=4.424898529052735
Loss made of: CE 0.7207368612289429, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.484019756317139 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/102, Loss=4.183466696739197
Loss made of: CE 0.5627927184104919, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.506194591522217 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.193332940340042
Loss made of: CE 0.5848730206489563, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9425628185272217 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.41624940931797
Loss made of: CE 0.4723125696182251, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6763627529144287 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.324123910069465
Loss made of: CE 0.45175838470458984, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6275928020477295 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.475546985864639
Loss made of: CE 0.4891941547393799, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.258768081665039 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.139703446626664
Loss made of: CE 0.5749595761299133, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.676302909851074 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.265140390396118
Loss made of: CE 0.5606681108474731, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8189773559570312 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.048550379276276
Loss made of: CE 0.5401214957237244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2413010597229004 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.083185932040214
Loss made of: CE 0.5999481678009033, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.505507469177246 EntMin 0.0
Epoch 1, Class Loss=0.5811656713485718, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.5811656713485718, Class Loss=0.5811656713485718, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000405
Epoch 2, Batch 10/102, Loss=4.088089793920517
Loss made of: CE 0.4949638843536377, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.156384229660034 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.016188025474548
Loss made of: CE 0.5028981566429138, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.480259656906128 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.177433747053146
Loss made of: CE 0.655215859413147, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4752416610717773 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.224725276231766
Loss made of: CE 0.5885863900184631, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.397921085357666 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.233939787745475
Loss made of: CE 0.5675920248031616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4996840953826904 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.333510041236877
Loss made of: CE 0.6351820826530457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6665549278259277 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.9226332902908325
Loss made of: CE 0.5822800397872925, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.230757474899292 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.276294714212417
Loss made of: CE 0.627249002456665, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.820852041244507 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.14639336168766
Loss made of: CE 0.6479862928390503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.395055770874023 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.230204436182976
Loss made of: CE 0.5406250357627869, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7778987884521484 EntMin 0.0
Epoch 2, Class Loss=0.5524882078170776, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.5524882078170776, Class Loss=0.5524882078170776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/102, Loss=4.1239800423383715
Loss made of: CE 0.5457723140716553, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.582838773727417 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 20/102, Loss=4.111164852976799
Loss made of: CE 0.5371769666671753, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3605751991271973 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.209571036696434
Loss made of: CE 0.5862387418746948, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.428046226501465 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.173440173268318
Loss made of: CE 0.5077663660049438, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6919875144958496 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.397680085897446
Loss made of: CE 0.5674378275871277, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.385138988494873 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.157331186532974
Loss made of: CE 0.5160291194915771, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4414210319519043 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.1332492411136625
Loss made of: CE 0.4274548292160034, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8059897422790527 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.260443821549416
Loss made of: CE 0.49567660689353943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8949031829833984 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.290760043263435
Loss made of: CE 0.4362841546535492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.304349660873413 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.136810123920441
Loss made of: CE 0.495919793844223, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.221177101135254 EntMin 0.0
Epoch 3, Class Loss=0.5418591499328613, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.5418591499328613, Class Loss=0.5418591499328613, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=4.064314490556717
Loss made of: CE 0.5322858691215515, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3467020988464355 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.204574909806252
Loss made of: CE 0.45563969016075134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6054370403289795 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.253086519241333
Loss made of: CE 0.6214761137962341, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.724392890930176 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.0955330550670626
Loss made of: CE 0.4569496512413025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.05176043510437 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.231403183937073
Loss made of: CE 0.5406275987625122, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3924102783203125 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.387986972928047
Loss made of: CE 0.4250900149345398, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9457316398620605 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.206336140632629
Loss made of: CE 0.6786521673202515, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6344919204711914 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.171170541644097
Loss made of: CE 0.6145771741867065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.368830680847168 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.22928104698658
Loss made of: CE 0.43100985884666443, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.421598196029663 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.058008733391762
Loss made of: CE 0.5598421096801758, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.078119277954102 EntMin 0.0
Epoch 4, Class Loss=0.5261483192443848, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5261483192443848, Class Loss=0.5261483192443848, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/102, Loss=4.331720662117005
Loss made of: CE 0.6387237310409546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4433207511901855 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.3157954722642895
Loss made of: CE 0.5852476954460144, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6285245418548584 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.152049469947815
Loss made of: CE 0.4487738013267517, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1709094047546387 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.9702682226896284
Loss made of: CE 0.5470726490020752, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.348095417022705 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.211108103394508
Loss made of: CE 0.5757704377174377, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.318933486938477 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.9166541039943694
Loss made of: CE 0.6264424324035645, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5641753673553467 EntMin 0.0
Epoch 5, Batch 70/102, Loss=3.931720662117004
Loss made of: CE 0.5092744827270508, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2208290100097656 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.014587700366974
Loss made of: CE 0.3394729495048523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.726984739303589 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.092125543951989
Loss made of: CE 0.5387352108955383, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.114567279815674 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.111104303598404
Loss made of: CE 0.5365355014801025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8734443187713623 EntMin 0.0
Epoch 5, Class Loss=0.5293351411819458, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5293351411819458, Class Loss=0.5293351411819458, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000270
Epoch 6, Batch 10/102, Loss=4.263347882032394
Loss made of: CE 0.4822029769420624, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2994909286499023 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.962566006183624
Loss made of: CE 0.522517204284668, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4681200981140137 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.8955710887908936
Loss made of: CE 0.49099835753440857, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8236775398254395 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.266567248106003
Loss made of: CE 0.5556641817092896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8270957469940186 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.005675655603409
Loss made of: CE 0.44242581725120544, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.259007453918457 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.109277844429016
Loss made of: CE 0.5316862463951111, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4461238384246826 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.8937324404716493
Loss made of: CE 0.5733603239059448, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.264899730682373 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.03147734105587
Loss made of: CE 0.5457805395126343, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.407883644104004 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.014745736122132
Loss made of: CE 0.45895159244537354, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2267379760742188 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.199916726350784
Loss made of: CE 0.586338460445404, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4020156860351562 EntMin 0.0
Epoch 6, Class Loss=0.514559268951416, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.514559268951416, Class Loss=0.514559268951416, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=4.564531858637929
Loss made of: CE 0.12788540124893188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.840597152709961 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.4418356694281105
Loss made of: CE 0.09561266005039215, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9690968990325928 EntMin 0.0
Epoch 1, Class Loss=0.10651442408561707, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.10651442408561707, Class Loss=0.10651442408561707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/23, Loss=4.3299412950873375
Loss made of: CE 0.3177802264690399, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195619583129883 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.44618868380785
Loss made of: CE 0.20702698826789856, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.016382217407227 EntMin 0.0
Epoch 2, Class Loss=0.22567926347255707, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.22567926347255707, Class Loss=0.22567926347255707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/23, Loss=4.204417596757412
Loss made of: CE 0.4679695963859558, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4281463623046875 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.257218086719513
Loss made of: CE 0.3262336850166321, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.719912528991699 EntMin 0.0
Epoch 3, Class Loss=0.30692046880722046, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.30692046880722046, Class Loss=0.30692046880722046, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/23, Loss=4.084032711386681
Loss made of: CE 0.375755250453949, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.770108699798584 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.4064006268978115
Loss made of: CE 0.2975064218044281, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.509026527404785 EntMin 0.0
Epoch 4, Class Loss=0.3433598279953003, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.3433598279953003, Class Loss=0.3433598279953003, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/23, Loss=4.256052312254906
Loss made of: CE 0.33406901359558105, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.61029314994812 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.166975066065788
Loss made of: CE 0.302765429019928, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.334855079650879 EntMin 0.0
Epoch 5, Class Loss=0.3959982097148895, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.3959982097148895, Class Loss=0.3959982097148895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/23, Loss=4.1983423084020615
Loss made of: CE 0.5704934597015381, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.076880931854248 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.134262642264366
Loss made of: CE 0.30690690875053406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9882960319519043 EntMin 0.0
Epoch 6, Class Loss=0.42395344376564026, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.42395344376564026, Class Loss=0.42395344376564026, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/23, Loss=5.299138849973678
Loss made of: CE 0.6618545651435852, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.11405611038208 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.616704931855201
Loss made of: CE 0.737151563167572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0686869621276855 EntMin 0.0
Epoch 1, Class Loss=0.620395839214325, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.620395839214325, Class Loss=0.620395839214325, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/23, Loss=4.5469756603240965
Loss made of: CE 0.4648534655570984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.374551296234131 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.1647926479578015
Loss made of: CE 0.4606543481349945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9370827674865723 EntMin 0.0
Epoch 2, Class Loss=0.5140215754508972, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.5140215754508972, Class Loss=0.5140215754508972, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/23, Loss=4.199960151314736
Loss made of: CE 0.4127504825592041, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9564197063446045 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.243128997087479
Loss made of: CE 0.3360350728034973, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.039143085479736 EntMin 0.0
Epoch 3, Class Loss=0.46696770191192627, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.46696770191192627, Class Loss=0.46696770191192627, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/23, Loss=4.017127043008804
Loss made of: CE 0.40023431181907654, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8516111373901367 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.138548950850963
Loss made of: CE 0.5301969647407532, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.56583309173584 EntMin 0.0
Epoch 4, Class Loss=0.4121606647968292, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.4121606647968292, Class Loss=0.4121606647968292, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/23, Loss=4.255119156837464
Loss made of: CE 0.4349745213985443, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5044617652893066 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.03599160015583
Loss made of: CE 0.5491742491722107, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.041929244995117 EntMin 0.0
Epoch 5, Class Loss=0.4141410291194916, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.4141410291194916, Class Loss=0.4141410291194916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/23, Loss=4.034658741950989
Loss made of: CE 0.3064653277397156, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.701505661010742 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.0348880112171175
Loss made of: CE 0.3991217017173767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.700326919555664 EntMin 0.0
Epoch 6, Class Loss=0.4017120897769928, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.4017120897769928, Class Loss=0.4017120897769928, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.47542688250541687, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.852654
Mean Acc: 0.510780
FreqW Acc: 0.748683
Mean IoU: 0.421923
Class IoU:
	class 0: 0.868005
	class 1: 0.6546879
	class 2: 0.3327892
	class 3: 0.48849198
	class 4: 0.57221997
	class 5: 0.0024401213
	class 6: 0.74188644
	class 7: 0.53035194
	class 8: 0.30906546
	class 9: 0.0
	class 10: 0.3734165
	class 11: 0.2555682
	class 12: 0.37556756
	class 13: 0.3165514
	class 14: 0.6926325
	class 15: 0.6590195
	class 16: 0.0
Class Acc:
	class 0: 0.97510797
	class 1: 0.6603264
	class 2: 0.64642656
	class 3: 0.5074529
	class 4: 0.83364064
	class 5: 0.0024417334
	class 6: 0.7647648
	class 7: 0.5392823
	class 8: 0.3112104
	class 9: 0.0
	class 10: 0.3985982
	class 11: 0.2891896
	class 12: 0.7174219
	class 13: 0.37414867
	class 14: 0.8597919
	class 15: 0.8034562
	class 16: 0.0

federated global round: 19, step: 3
select part of clients to conduct local training
[4, 7, 17, 15]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/105, Loss=4.194760709255934
Loss made of: CE 0.08958606421947479, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9170823097229 EntMin 0.0
Epoch 1, Batch 20/105, Loss=3.930627355352044
Loss made of: CE 0.06951600313186646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.843715190887451 EntMin 0.0
Epoch 1, Batch 30/105, Loss=3.8036299547180534
Loss made of: CE 0.05929456651210785, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251575469970703 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 40/105, Loss=3.7172612581402062
Loss made of: CE 0.06410979479551315, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5025954246520996 EntMin 0.0
Epoch 1, Batch 50/105, Loss=3.556585912965238
Loss made of: CE 0.05090084299445152, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.289867877960205 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.042655427567661
Loss made of: CE 0.047202058136463165, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3869876861572266 EntMin 0.0
Epoch 1, Batch 70/105, Loss=3.9167879309505222
Loss made of: CE 0.03054770827293396, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5919864177703857 EntMin 0.0
Epoch 1, Batch 80/105, Loss=3.8193665739148854
Loss made of: CE 0.036727674305438995, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.533604145050049 EntMin 0.0
Epoch 1, Batch 90/105, Loss=3.8613268077373504
Loss made of: CE 0.026485305279493332, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4590651988983154 EntMin 0.0
Epoch 1, Batch 100/105, Loss=3.920589313656092
Loss made of: CE 0.04538431391119957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.086436748504639 EntMin 0.0
Epoch 1, Class Loss=0.055543191730976105, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.055543191730976105, Class Loss=0.055543191730976105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/105, Loss=3.93225751966238
Loss made of: CE 0.115137480199337, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3663222789764404 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.100817310065031
Loss made of: CE 0.06670982390642166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9443788528442383 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.087776380777359
Loss made of: CE 0.07391130179166794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.283385276794434 EntMin 0.0
Epoch 2, Batch 40/105, Loss=3.955209558457136
Loss made of: CE 0.10395839065313339, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7480673789978027 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.036327278241515
Loss made of: CE 0.11572445929050446, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8065595626831055 EntMin 0.0
Epoch 2, Batch 60/105, Loss=4.0167180698364975
Loss made of: CE 0.11679196357727051, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.931490421295166 EntMin 0.0
Epoch 2, Batch 70/105, Loss=3.9590201780200003
Loss made of: CE 0.1201692745089531, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.928892135620117 EntMin 0.0
Epoch 2, Batch 80/105, Loss=3.870131842792034
Loss made of: CE 0.08529933542013168, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.534501791000366 EntMin 0.0
Epoch 2, Batch 90/105, Loss=3.8565599411725997
Loss made of: CE 0.0834658294916153, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5055766105651855 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.069136819243431
Loss made of: CE 0.1268027126789093, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9498391151428223 EntMin 0.0
Epoch 2, Class Loss=0.11216080188751221, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.11216080188751221, Class Loss=0.11216080188751221, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/105, Loss=4.252702009677887
Loss made of: CE 0.18780802190303802, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6236977577209473 EntMin 0.0
Epoch 3, Batch 20/105, Loss=3.720002194494009
Loss made of: CE 0.1477060616016388, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.491568088531494 EntMin 0.0
Epoch 3, Batch 30/105, Loss=3.953144282847643
Loss made of: CE 0.15270820260047913, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.488558530807495 EntMin 0.0
Epoch 3, Batch 40/105, Loss=3.984806074202061
Loss made of: CE 0.1738395392894745, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.026674270629883 EntMin 0.0
Epoch 3, Batch 50/105, Loss=3.853409044444561
Loss made of: CE 0.19046208262443542, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4960601329803467 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.073630978167057
Loss made of: CE 0.14431670308113098, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4225850105285645 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.087983812391758
Loss made of: CE 0.25963079929351807, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.950721263885498 EntMin 0.0
Epoch 3, Batch 80/105, Loss=3.917922901362181
Loss made of: CE 0.10607586801052094, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0410897731781006 EntMin 0.0
Epoch 3, Batch 90/105, Loss=3.806033509969711
Loss made of: CE 0.16734234988689423, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7175498008728027 EntMin 0.0
Epoch 3, Batch 100/105, Loss=3.911839742958546
Loss made of: CE 0.11275355517864227, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3324532508850098 EntMin 0.0
Epoch 3, Class Loss=0.16768942773342133, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.16768942773342133, Class Loss=0.16768942773342133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/105, Loss=3.9937956497073173
Loss made of: CE 0.23564943671226501, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2954277992248535 EntMin 0.0
Epoch 4, Batch 20/105, Loss=3.998508369922638
Loss made of: CE 0.23489037156105042, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5652663707733154 EntMin 0.0
Epoch 4, Batch 30/105, Loss=3.962564717233181
Loss made of: CE 0.20555274188518524, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.700181007385254 EntMin 0.0
Epoch 4, Batch 40/105, Loss=3.8668134421110154
Loss made of: CE 0.2749137282371521, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4398627281188965 EntMin 0.0
Epoch 4, Batch 50/105, Loss=3.8719610214233398
Loss made of: CE 0.2750384211540222, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3544907569885254 EntMin 0.0
Epoch 4, Batch 60/105, Loss=3.863484610617161
Loss made of: CE 0.2748241126537323, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9182889461517334 EntMin 0.0
Epoch 4, Batch 70/105, Loss=3.723774220049381
Loss made of: CE 0.22912149131298065, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1497435569763184 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.062966339290142
Loss made of: CE 0.20008349418640137, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.087949275970459 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.026607249677181
Loss made of: CE 0.19487562775611877, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4139838218688965 EntMin 0.0
Epoch 4, Batch 100/105, Loss=3.699726139008999
Loss made of: CE 0.18028610944747925, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0987753868103027 EntMin 0.0
Epoch 4, Class Loss=0.21863816678524017, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.21863816678524017, Class Loss=0.21863816678524017, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/105, Loss=3.8734774351119996
Loss made of: CE 0.2662205100059509, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.518003463745117 EntMin 0.0
Epoch 5, Batch 20/105, Loss=3.956954814493656
Loss made of: CE 0.3964538276195526, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400928974151611 EntMin 0.0
Epoch 5, Batch 30/105, Loss=3.8201343566179276
Loss made of: CE 0.2675282061100006, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.606876850128174 EntMin 0.0
Epoch 5, Batch 40/105, Loss=4.03624182343483
Loss made of: CE 0.26678287982940674, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.453658103942871 EntMin 0.0
Epoch 5, Batch 50/105, Loss=3.7448070764541628
Loss made of: CE 0.21597914397716522, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1629159450531006 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.232148624956608
Loss made of: CE 0.26703429222106934, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.244422912597656 EntMin 0.0
Epoch 5, Batch 70/105, Loss=3.8922005012631415
Loss made of: CE 0.3014453053474426, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.584599494934082 EntMin 0.0
Epoch 5, Batch 80/105, Loss=3.8512638375163077
Loss made of: CE 0.2322254627943039, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6441049575805664 EntMin 0.0
Epoch 5, Batch 90/105, Loss=3.8476543188095094
Loss made of: CE 0.20096588134765625, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.075937032699585 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.024880701303482
Loss made of: CE 0.2713575065135956, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.523960590362549 EntMin 0.0
Epoch 5, Class Loss=0.273904025554657, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.273904025554657, Class Loss=0.273904025554657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/105, Loss=4.167716792225837
Loss made of: CE 0.29384303092956543, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.286651611328125 EntMin 0.0
Epoch 6, Batch 20/105, Loss=3.8147313445806503
Loss made of: CE 0.29587647318840027, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3199918270111084 EntMin 0.0
Epoch 6, Batch 30/105, Loss=3.738060733675957
Loss made of: CE 0.28017956018447876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8742902278900146 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.012575483322143
Loss made of: CE 0.3105178475379944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0002593994140625 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.074583166837693
Loss made of: CE 0.4056236743927002, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.096682548522949 EntMin 0.0
Epoch 6, Batch 60/105, Loss=3.8313731968402864
Loss made of: CE 0.28934770822525024, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6267237663269043 EntMin 0.0
Epoch 6, Batch 70/105, Loss=3.7405557304620745
Loss made of: CE 0.30204638838768005, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.23905086517334 EntMin 0.0
Epoch 6, Batch 80/105, Loss=3.9408364295959473
Loss made of: CE 0.2805612087249756, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.464449405670166 EntMin 0.0
Epoch 6, Batch 90/105, Loss=3.7716897785663606
Loss made of: CE 0.358803391456604, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.579554319381714 EntMin 0.0
Epoch 6, Batch 100/105, Loss=3.6727817565202714
Loss made of: CE 0.28500550985336304, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.24554443359375 EntMin 0.0
Epoch 6, Class Loss=0.31623098254203796, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.31623098254203796, Class Loss=0.31623098254203796, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.3615469336509705
Loss made of: CE 0.6315386295318604, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.449155807495117 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.452146703004837
Loss made of: CE 0.6306682825088501, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035571575164795 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.362515896558762
Loss made of: CE 0.5029006004333496, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.074154376983643 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.3375995576381685
Loss made of: CE 0.6437249183654785, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6966216564178467 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.396962523460388
Loss made of: CE 0.6140017509460449, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.131914138793945 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.362995746731758
Loss made of: CE 0.494931161403656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.417912244796753 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.387924167513847
Loss made of: CE 0.6018804907798767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8884596824645996 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.367428278923034
Loss made of: CE 0.582313060760498, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4053478240966797 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/102, Loss=3.9404932796955108
Loss made of: CE 0.5204123258590698, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0881621837615967 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.084942722320557
Loss made of: CE 0.5506146550178528, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.428750991821289 EntMin 0.0
Epoch 1, Class Loss=0.5894483327865601, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.5894483327865601, Class Loss=0.5894483327865601, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/102, Loss=4.2620904505252835
Loss made of: CE 0.49082255363464355, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.336350202560425 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.08689875304699
Loss made of: CE 0.5305942296981812, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8063158988952637 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.367494356632233
Loss made of: CE 0.477510541677475, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.552491188049316 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.244826856255531
Loss made of: CE 0.5308539271354675, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.109992504119873 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.031027474999428
Loss made of: CE 0.4599900245666504, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3107547760009766 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.149471619725228
Loss made of: CE 0.48869484663009644, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7028021812438965 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.465701243281364
Loss made of: CE 0.5604270100593567, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.905972957611084 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.052633029222489
Loss made of: CE 0.46775779128074646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2306337356567383 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.140866795182228
Loss made of: CE 0.4743231236934662, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3702917098999023 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.346454364061356
Loss made of: CE 0.5366847515106201, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084811210632324 EntMin 0.0
Epoch 2, Class Loss=0.5382398962974548, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.5382398962974548, Class Loss=0.5382398962974548, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/102, Loss=4.4327852636575695
Loss made of: CE 0.5281110405921936, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.278419017791748 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.218890425562859
Loss made of: CE 0.572298526763916, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.957857131958008 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.050225508213043
Loss made of: CE 0.574338436126709, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5318961143493652 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.1137574225664135
Loss made of: CE 0.46676182746887207, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9449753761291504 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.200434589385987
Loss made of: CE 0.5253368616104126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.24837589263916 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.037398418784141
Loss made of: CE 0.566013514995575, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2419497966766357 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.209082090854645
Loss made of: CE 0.6382368206977844, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.505976438522339 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.083151739835739
Loss made of: CE 0.5375142097473145, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4679136276245117 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.139158624410629
Loss made of: CE 0.5632415413856506, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.227999210357666 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.9500977605581284
Loss made of: CE 0.5168906450271606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3493170738220215 EntMin 0.0
Epoch 3, Class Loss=0.5260181427001953, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.5260181427001953, Class Loss=0.5260181427001953, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=4.022796083986759
Loss made of: CE 0.32919225096702576, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5570178031921387 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.845838966965675
Loss made of: CE 0.38819390535354614, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.514193296432495 EntMin 0.0
Epoch 4, Batch 30/102, Loss=3.8790099143981935
Loss made of: CE 0.2563711404800415, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1193251609802246 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.893917353451252
Loss made of: CE 0.28950923681259155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.430964946746826 EntMin 0.0
Epoch 4, Batch 50/102, Loss=3.8171336144208907
Loss made of: CE 0.28148818016052246, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0859410762786865 EntMin 0.0
Epoch 4, Batch 60/102, Loss=3.9982674419879913
Loss made of: CE 0.3331995904445648, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.521688938140869 EntMin 0.0
Epoch 4, Batch 70/102, Loss=3.8649839967489243
Loss made of: CE 0.31322839856147766, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.493452548980713 EntMin 0.0
Epoch 4, Batch 80/102, Loss=3.9507409751415254
Loss made of: CE 0.30391421914100647, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5023105144500732 EntMin 0.0
Epoch 4, Batch 90/102, Loss=3.9656806230545043
Loss made of: CE 0.35515326261520386, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.944324493408203 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.099089515209198
Loss made of: CE 0.25658461451530457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.324751377105713 EntMin 0.0
Epoch 4, Class Loss=0.32164523005485535, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.32164523005485535, Class Loss=0.32164523005485535, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/102, Loss=4.027035677433014
Loss made of: CE 0.6577354669570923, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.483759641647339 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.215025985240937
Loss made of: CE 0.5997426509857178, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.80332612991333 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.328456979990006
Loss made of: CE 0.599661111831665, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6489169597625732 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.136766025424004
Loss made of: CE 0.4831690788269043, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1711297035217285 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.007806915044784
Loss made of: CE 0.61632239818573, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6783881187438965 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.033012264966965
Loss made of: CE 0.4855338931083679, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0221879482269287 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.179367581009865
Loss made of: CE 0.4674825668334961, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7203969955444336 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.209374040365219
Loss made of: CE 0.48303067684173584, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4357876777648926 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.9598664581775664
Loss made of: CE 0.6052085161209106, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.472978115081787 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.041937151551247
Loss made of: CE 0.48474758863449097, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.275259971618652 EntMin 0.0
Epoch 5, Class Loss=0.5379989743232727, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.5379989743232727, Class Loss=0.5379989743232727, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/102, Loss=4.049290028214455
Loss made of: CE 0.4964005649089813, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0896050930023193 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.260119181871414
Loss made of: CE 0.496456503868103, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.8340444564819336 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.818791648745537
Loss made of: CE 0.5206219553947449, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.189919948577881 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.028169357776642
Loss made of: CE 0.49631667137145996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1554136276245117 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.074004963040352
Loss made of: CE 0.5519527792930603, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5040042400360107 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.9771828979253767
Loss made of: CE 0.5361216068267822, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3626327514648438 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.064576405286789
Loss made of: CE 0.46865472197532654, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.626737117767334 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.9238525152206423
Loss made of: CE 0.5354125499725342, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5479393005371094 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.046448624134063
Loss made of: CE 0.5774742364883423, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.246046781539917 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.993790251016617
Loss made of: CE 0.5621659159660339, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.918180465698242 EntMin 0.0
Epoch 6, Class Loss=0.521682858467102, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.521682858467102, Class Loss=0.521682858467102, Reg Loss=0.0
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/23, Loss=5.141770017147064
Loss made of: CE 0.6282259225845337, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.316664695739746 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.745530980825424
Loss made of: CE 0.5540985465049744, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.067656517028809 EntMin 0.0
Epoch 1, Class Loss=0.5996039509773254, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.5996039509773254, Class Loss=0.5996039509773254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/23, Loss=4.624804410338402
Loss made of: CE 0.6045227646827698, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9431047439575195 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.4302925825119015
Loss made of: CE 0.35561785101890564, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.876631498336792 EntMin 0.0
Epoch 2, Class Loss=0.5109474062919617, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.5109474062919617, Class Loss=0.5109474062919617, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/23, Loss=4.364245131611824
Loss made of: CE 0.3656897246837616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6216037273406982 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.448962152004242
Loss made of: CE 0.5388346314430237, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.449917316436768 EntMin 0.0
Epoch 3, Class Loss=0.4637841582298279, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.4637841582298279, Class Loss=0.4637841582298279, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/23, Loss=4.4804560273885725
Loss made of: CE 0.613666832447052, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6418752670288086 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.272434002161026
Loss made of: CE 0.32477128505706787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.816803455352783 EntMin 0.0
Epoch 4, Class Loss=0.4459540545940399, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.4459540545940399, Class Loss=0.4459540545940399, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/23, Loss=4.364617961645126
Loss made of: CE 0.38802194595336914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.103081703186035 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.168046066164971
Loss made of: CE 0.3718709647655487, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.01788854598999 EntMin 0.0
Epoch 5, Class Loss=0.42847883701324463, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.42847883701324463, Class Loss=0.42847883701324463, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/23, Loss=4.3053924441337585
Loss made of: CE 0.4182158410549164, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.807518482208252 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.172911751270294
Loss made of: CE 0.44252094626426697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.419289588928223 EntMin 0.0
Epoch 6, Class Loss=0.41892996430397034, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.41892996430397034, Class Loss=0.41892996430397034, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/103, Loss=3.520696360990405
Loss made of: CE 0.06244395673274994, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5382080078125 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/103, Loss=3.6065793726593256
Loss made of: CE 0.04444512352347374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4718985557556152 EntMin 0.0
Epoch 1, Batch 30/103, Loss=3.7127707745879888
Loss made of: CE 0.060602277517318726, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6247968673706055 EntMin 0.0
Epoch 1, Batch 40/103, Loss=3.95843840688467
Loss made of: CE 0.04210884869098663, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.284374237060547 EntMin 0.0
Epoch 1, Batch 50/103, Loss=4.107757380791009
Loss made of: CE 0.10521920770406723, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.86613655090332 EntMin 0.0
Epoch 1, Batch 60/103, Loss=3.8411781430244445
Loss made of: CE 0.04742501303553581, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.876908302307129 EntMin 0.0
Epoch 1, Batch 70/103, Loss=3.833931247703731
Loss made of: CE 0.025846680626273155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.745896339416504 EntMin 0.0
Epoch 1, Batch 80/103, Loss=4.085365615785122
Loss made of: CE 0.0703808069229126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.415936470031738 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/103, Loss=3.723876244574785
Loss made of: CE 0.05129151791334152, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1702613830566406 EntMin 0.0
Epoch 1, Batch 100/103, Loss=3.9241955656558276
Loss made of: CE 0.03683337941765785, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6479525566101074 EntMin 0.0
Epoch 1, Class Loss=0.0483529195189476, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.0483529195189476, Class Loss=0.0483529195189476, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/103, Loss=3.8362242229282857
Loss made of: CE 0.08360372483730316, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.525033950805664 EntMin 0.0
Epoch 2, Batch 20/103, Loss=4.065106273442507
Loss made of: CE 0.11011480540037155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.604518413543701 EntMin 0.0
Epoch 2, Batch 30/103, Loss=3.864787174016237
Loss made of: CE 0.09091876447200775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.159310817718506 EntMin 0.0
Epoch 2, Batch 40/103, Loss=4.044689080119133
Loss made of: CE 0.1275080144405365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.09804630279541 EntMin 0.0
Epoch 2, Batch 50/103, Loss=3.7368783332407474
Loss made of: CE 0.08119561523199081, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.293090343475342 EntMin 0.0
Epoch 2, Batch 60/103, Loss=4.110068193823099
Loss made of: CE 0.08924464881420135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.188103199005127 EntMin 0.0
Epoch 2, Batch 70/103, Loss=3.8934851184487345
Loss made of: CE 0.12403440475463867, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.796800136566162 EntMin 0.0
Epoch 2, Batch 80/103, Loss=3.8592552080750466
Loss made of: CE 0.12983690202236176, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.296873092651367 EntMin 0.0
Epoch 2, Batch 90/103, Loss=4.0660693041980265
Loss made of: CE 0.1474667489528656, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.168365955352783 EntMin 0.0
Epoch 2, Batch 100/103, Loss=4.2393709845840934
Loss made of: CE 0.08450745791196823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.053474426269531 EntMin 0.0
Epoch 2, Class Loss=0.1007457748055458, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.1007457748055458, Class Loss=0.1007457748055458, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/103, Loss=4.00357049703598
Loss made of: CE 0.16043108701705933, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5278098583221436 EntMin 0.0
Epoch 3, Batch 20/103, Loss=3.921032141149044
Loss made of: CE 0.1934366524219513, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.732832908630371 EntMin 0.0
Epoch 3, Batch 30/103, Loss=3.9354592986404895
Loss made of: CE 0.2020811140537262, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3379666805267334 EntMin 0.0
Epoch 3, Batch 40/103, Loss=4.007856839895249
Loss made of: CE 0.12443313002586365, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6207141876220703 EntMin 0.0
Epoch 3, Batch 50/103, Loss=4.149693267047406
Loss made of: CE 0.2045116424560547, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.498885154724121 EntMin 0.0
Epoch 3, Batch 60/103, Loss=3.911676415801048
Loss made of: CE 0.11818195879459381, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.502209186553955 EntMin 0.0
Epoch 3, Batch 70/103, Loss=4.018797857314349
Loss made of: CE 0.11445575207471848, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.480532646179199 EntMin 0.0
Epoch 3, Batch 80/103, Loss=3.955357110500336
Loss made of: CE 0.13245317339897156, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6734063625335693 EntMin 0.0
Epoch 3, Batch 90/103, Loss=3.9261393025517464
Loss made of: CE 0.11930352449417114, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.413663625717163 EntMin 0.0
Epoch 3, Batch 100/103, Loss=3.9049420818686484
Loss made of: CE 0.17069664597511292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.201416492462158 EntMin 0.0
Epoch 3, Class Loss=0.15159139037132263, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.15159139037132263, Class Loss=0.15159139037132263, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/103, Loss=3.912143570184708
Loss made of: CE 0.2155400961637497, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3199238777160645 EntMin 0.0
Epoch 4, Batch 20/103, Loss=3.9949690237641335
Loss made of: CE 0.25696083903312683, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.536130905151367 EntMin 0.0
Epoch 4, Batch 30/103, Loss=3.919571229815483
Loss made of: CE 0.22160717844963074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.228116035461426 EntMin 0.0
Epoch 4, Batch 40/103, Loss=3.702900217473507
Loss made of: CE 0.20658116042613983, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.435544013977051 EntMin 0.0
Epoch 4, Batch 50/103, Loss=3.8814672753214836
Loss made of: CE 0.2001330852508545, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.913522481918335 EntMin 0.0
Epoch 4, Batch 60/103, Loss=3.8891719609498976
Loss made of: CE 0.18095926940441132, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6088168621063232 EntMin 0.0
Epoch 4, Batch 70/103, Loss=3.7754569813609122
Loss made of: CE 0.2049715518951416, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7912347316741943 EntMin 0.0
Epoch 4, Batch 80/103, Loss=3.9377381548285486
Loss made of: CE 0.1989310383796692, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2743377685546875 EntMin 0.0
Epoch 4, Batch 90/103, Loss=3.716822734475136
Loss made of: CE 0.19806957244873047, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5605525970458984 EntMin 0.0
Epoch 4, Batch 100/103, Loss=4.0379997998476025
Loss made of: CE 0.2420976608991623, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.421229362487793 EntMin 0.0
Epoch 4, Class Loss=0.20209439098834991, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.20209439098834991, Class Loss=0.20209439098834991, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/103, Loss=3.9611888870596887
Loss made of: CE 0.2786586880683899, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.676454544067383 EntMin 0.0
Epoch 5, Batch 20/103, Loss=4.0997179672122
Loss made of: CE 0.23603187501430511, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.86313009262085 EntMin 0.0
Epoch 5, Batch 30/103, Loss=3.81307313144207
Loss made of: CE 0.2628389596939087, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.232877254486084 EntMin 0.0
Epoch 5, Batch 40/103, Loss=3.934976026415825
Loss made of: CE 0.22096054255962372, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.104127883911133 EntMin 0.0
Epoch 5, Batch 50/103, Loss=3.9572896271944047
Loss made of: CE 0.22007128596305847, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3153791427612305 EntMin 0.0
Epoch 5, Batch 60/103, Loss=3.7960340052843096
Loss made of: CE 0.326393187046051, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4795289039611816 EntMin 0.0
Epoch 5, Batch 70/103, Loss=3.7113274455070497
Loss made of: CE 0.24647942185401917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5264596939086914 EntMin 0.0
Epoch 5, Batch 80/103, Loss=4.046890741586685
Loss made of: CE 0.24045464396476746, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.373602867126465 EntMin 0.0
Epoch 5, Batch 90/103, Loss=3.962198816239834
Loss made of: CE 0.24948112666606903, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.537322998046875 EntMin 0.0
Epoch 5, Batch 100/103, Loss=3.934358634054661
Loss made of: CE 0.21509605646133423, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5342214107513428 EntMin 0.0
Epoch 5, Class Loss=0.24982960522174835, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.24982960522174835, Class Loss=0.24982960522174835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/103, Loss=3.8052778124809263
Loss made of: CE 0.2888619303703308, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2246108055114746 EntMin 0.0
Epoch 6, Batch 20/103, Loss=3.8589007288217543
Loss made of: CE 0.24462160468101501, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0222325325012207 EntMin 0.0
Epoch 6, Batch 30/103, Loss=3.8072103694081307
Loss made of: CE 0.27162790298461914, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.490656614303589 EntMin 0.0
Epoch 6, Batch 40/103, Loss=3.7180317148566244
Loss made of: CE 0.26283037662506104, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1040875911712646 EntMin 0.0
Epoch 6, Batch 50/103, Loss=3.8476118758320808
Loss made of: CE 0.26842182874679565, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4899110794067383 EntMin 0.0
Epoch 6, Batch 60/103, Loss=3.92253777384758
Loss made of: CE 0.2880791425704956, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.488033294677734 EntMin 0.0
Epoch 6, Batch 70/103, Loss=4.174347823858261
Loss made of: CE 0.25827035307884216, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6893858909606934 EntMin 0.0
Epoch 6, Batch 80/103, Loss=4.0364194676280025
Loss made of: CE 0.3521058261394501, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.472855567932129 EntMin 0.0
Epoch 6, Batch 90/103, Loss=4.109539321064949
Loss made of: CE 0.24927467107772827, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5323586463928223 EntMin 0.0
Epoch 6, Batch 100/103, Loss=3.972734680771828
Loss made of: CE 0.31132030487060547, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.723381757736206 EntMin 0.0
Epoch 6, Class Loss=0.29804131388664246, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.29804131388664246, Class Loss=0.29804131388664246, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4731282889842987, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.851925
Mean Acc: 0.516052
FreqW Acc: 0.750220
Mean IoU: 0.413554
Class IoU:
	class 0: 0.8741511
	class 1: 0.6691902
	class 2: 0.34420055
	class 3: 0.51098424
	class 4: 0.57407105
	class 5: 0.0031708048
	class 6: 0.75923675
	class 7: 0.57321596
	class 8: 0.30473918
	class 9: 0.0
	class 10: 0.048616283
	class 11: 0.26076105
	class 12: 0.3790682
	class 13: 0.36649296
	class 14: 0.7082487
	class 15: 0.65427876
	class 16: 0.0
Class Acc:
	class 0: 0.9728787
	class 1: 0.6755023
	class 2: 0.6965665
	class 3: 0.5301176
	class 4: 0.8446465
	class 5: 0.0031727168
	class 6: 0.788237
	class 7: 0.58515066
	class 8: 0.3066003
	class 9: 0.0
	class 10: 0.048628386
	class 11: 0.29729834
	class 12: 0.7080824
	class 13: 0.6383498
	class 14: 0.8686171
	class 15: 0.8090276
	class 16: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[19, 23, 1, 8]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=8.65990365743637
Loss made of: CE 0.701945960521698, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.484452724456787 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=7.607376313209533
Loss made of: CE 0.576373815536499, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.024173259735107 EntMin 0.0
Epoch 1, Class Loss=0.7069395780563354, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.7069395780563354, Class Loss=0.7069395780563354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=7.5949661314487455
Loss made of: CE 0.7041780948638916, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.05885648727417 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.932150512933731
Loss made of: CE 0.7258955836296082, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.204612731933594 EntMin 0.0
Epoch 2, Class Loss=0.7737090587615967, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.7737090587615967, Class Loss=0.7737090587615967, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=6.4770767271518705
Loss made of: CE 0.6570487022399902, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.558501243591309 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.158400887250901
Loss made of: CE 0.6264004111289978, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.947943687438965 EntMin 0.0
Epoch 3, Class Loss=0.7327155470848083, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.7327155470848083, Class Loss=0.7327155470848083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=5.888549828529358
Loss made of: CE 0.7077142596244812, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.156436920166016 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.320416009426117
Loss made of: CE 0.6339949369430542, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.292509078979492 EntMin 0.0
Epoch 4, Class Loss=0.6830736994743347, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.6830736994743347, Class Loss=0.6830736994743347, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=5.618783342838287
Loss made of: CE 0.6688908934593201, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.654506683349609 EntMin 0.0
Epoch 5, Batch 20/24, Loss=6.045246756076812
Loss made of: CE 0.5472197532653809, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.654799461364746 EntMin 0.0
Epoch 5, Class Loss=0.6515758037567139, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.6515758037567139, Class Loss=0.6515758037567139, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 10/24, Loss=5.924983698129654
Loss made of: CE 0.6179789304733276, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.091220378875732 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.473168575763703
Loss made of: CE 0.6663747429847717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.584041595458984 EntMin 0.0
Epoch 6, Class Loss=0.6290760636329651, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.6290760636329651, Class Loss=0.6290760636329651, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=7.843664342164994
Loss made of: CE 0.44052088260650635, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.632609844207764 EntMin 0.0
Epoch 1, Batch 20/21, Loss=6.907380527257919
Loss made of: CE 0.5446084141731262, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.944249153137207 EntMin 0.0
Epoch 1, Class Loss=0.4598237872123718, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.4598237872123718, Class Loss=0.4598237872123718, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/21, Loss=6.440181839466095
Loss made of: CE 0.5604860782623291, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.403938293457031 EntMin 0.0
Epoch 2, Batch 20/21, Loss=6.0482184290885925
Loss made of: CE 0.5131732225418091, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.945004940032959 EntMin 0.0
Epoch 2, Class Loss=0.5852601528167725, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.5852601528167725, Class Loss=0.5852601528167725, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/21, Loss=6.332070934772491
Loss made of: CE 0.47933363914489746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.945844650268555 EntMin 0.0
Epoch 3, Batch 20/21, Loss=6.005510205030442
Loss made of: CE 0.652577817440033, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.800749778747559 EntMin 0.0
Epoch 3, Class Loss=0.6190513372421265, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.6190513372421265, Class Loss=0.6190513372421265, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/21, Loss=6.170987355709076
Loss made of: CE 0.6863264441490173, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.613600730895996 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.394418925046921
Loss made of: CE 0.4775022864341736, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.41845703125 EntMin 0.0
Epoch 4, Class Loss=0.6321874260902405, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.6321874260902405, Class Loss=0.6321874260902405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/21, Loss=5.767134183645249
Loss made of: CE 0.598894476890564, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.157739639282227 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.736516791582107
Loss made of: CE 0.661178469657898, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.947848796844482 EntMin 0.0
Epoch 5, Class Loss=0.6217952966690063, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.6217952966690063, Class Loss=0.6217952966690063, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/21, Loss=5.61411612033844
Loss made of: CE 0.630527138710022, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.890016555786133 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.6030579805374146
Loss made of: CE 0.7033793926239014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.346085548400879 EntMin 0.0
Epoch 6, Class Loss=0.6388882994651794, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.6388882994651794, Class Loss=0.6388882994651794, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=9.857862430810929
Loss made of: CE 0.8427573442459106, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.28630256652832 EntMin 0.0
Epoch 1, Class Loss=0.7474263906478882, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.7474263906478882, Class Loss=0.7474263906478882, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/19, Loss=7.959704858064652
Loss made of: CE 0.8333736658096313, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.12198543548584 EntMin 0.0
Epoch 2, Class Loss=0.8105559945106506, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.8105559945106506, Class Loss=0.8105559945106506, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=7.3612444519996645
Loss made of: CE 0.8060216307640076, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.695924758911133 EntMin 0.0
Epoch 3, Class Loss=0.7585563063621521, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.7585563063621521, Class Loss=0.7585563063621521, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=6.85506243109703
Loss made of: CE 0.6472359299659729, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.523690700531006 EntMin 0.0
Epoch 4, Class Loss=0.6938040852546692, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.6938040852546692, Class Loss=0.6938040852546692, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=6.619320940971375
Loss made of: CE 0.611430823802948, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.156120300292969 EntMin 0.0
Epoch 5, Class Loss=0.6173326373100281, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.6173326373100281, Class Loss=0.6173326373100281, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=6.211245584487915
Loss made of: CE 0.5316149592399597, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.303679466247559 EntMin 0.0
Epoch 6, Class Loss=0.5657207369804382, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.5657207369804382, Class Loss=0.5657207369804382, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=8.71067809164524
Loss made of: CE 0.29932087659835815, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.981767654418945 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=7.323366194963455
Loss made of: CE 0.3404921889305115, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.294397354125977 EntMin 0.0
Epoch 1, Class Loss=0.44638723134994507, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.44638723134994507, Class Loss=0.44638723134994507, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=7.016645336151123
Loss made of: CE 0.5220136642456055, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.114459037780762 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.670290243625641
Loss made of: CE 0.531684160232544, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.642751216888428 EntMin 0.0
Epoch 2, Class Loss=0.5793197751045227, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.5793197751045227, Class Loss=0.5793197751045227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=6.61491189301014
Loss made of: CE 0.6810503005981445, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.607574462890625 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.228487884998321
Loss made of: CE 0.5052766799926758, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.944035530090332 EntMin 0.0
Epoch 3, Class Loss=0.6014932990074158, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.6014932990074158, Class Loss=0.6014932990074158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=6.024659603834152
Loss made of: CE 0.7033432722091675, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.663442134857178 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.150473257899284
Loss made of: CE 0.43161556124687195, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.210578441619873 EntMin 0.0
Epoch 4, Class Loss=0.6224822402000427, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.6224822402000427, Class Loss=0.6224822402000427, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=6.316728711128235
Loss made of: CE 0.8244129419326782, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.519137859344482 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.785182005167007
Loss made of: CE 0.5562153458595276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.94453239440918 EntMin 0.0
Epoch 5, Class Loss=0.6466586589813232, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6466586589813232, Class Loss=0.6466586589813232, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=6.016243660449982
Loss made of: CE 0.7021598219871521, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.792707443237305 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.780846667289734
Loss made of: CE 0.611651599407196, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.275382041931152 EntMin 0.0
Epoch 6, Class Loss=0.680501401424408, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.680501401424408, Class Loss=0.680501401424408, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7104671001434326, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.796982
Mean Acc: 0.319113
FreqW Acc: 0.653804
Mean IoU: 0.255450
Class IoU:
	class 0: 0.8075379
	class 1: 0.3082006
	class 2: 0.23518428
	class 3: 0.062265992
	class 4: 0.5124513
	class 5: 5.318261e-06
	class 6: 0.6035296
	class 7: 0.36354753
	class 8: 0.19178686
	class 9: 0.0
	class 10: 0.030796083
	class 11: 0.27698967
	class 12: 0.34894451
	class 13: 0.2960167
	class 14: 0.64583635
	class 15: 0.68136644
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0
Class Acc:
	class 0: 0.9867574
	class 1: 0.30893335
	class 2: 0.3775493
	class 3: 0.062324353
	class 4: 0.72696596
	class 5: 5.318393e-06
	class 6: 0.6355419
	class 7: 0.36580506
	class 8: 0.19201727
	class 9: 0.0
	class 10: 0.030890884
	class 11: 0.3007164
	class 12: 0.7175486
	class 13: 0.4738398
	class 14: 0.7844207
	class 15: 0.7380601
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[23, 14, 4, 11]
Current Client Index:  23
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=6.354767280817032
Loss made of: CE 0.7533582448959351, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.696083068847656 EntMin 0.0
Epoch 1, Batch 20/21, Loss=6.033915197849273
Loss made of: CE 0.8573771715164185, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.4521284103393555 EntMin 0.0
Epoch 1, Class Loss=0.8517470955848694, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.8517470955848694, Class Loss=0.8517470955848694, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/21, Loss=5.731279504299164
Loss made of: CE 0.7580705881118774, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4370222091674805 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.573209536075592
Loss made of: CE 0.712967574596405, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.709290981292725 EntMin 0.0
Epoch 2, Class Loss=0.7686182856559753, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.7686182856559753, Class Loss=0.7686182856559753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/21, Loss=6.044667959213257
Loss made of: CE 0.6435167789459229, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.817585468292236 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.643317759037018
Loss made of: CE 0.7881349325180054, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2729387283325195 EntMin 0.0
Epoch 3, Class Loss=0.6885879039764404, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.6885879039764404, Class Loss=0.6885879039764404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/21, Loss=5.634925949573517
Loss made of: CE 0.6186249256134033, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.005381107330322 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.061738634109497
Loss made of: CE 0.519220769405365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.167998313903809 EntMin 0.0
Epoch 4, Class Loss=0.6144572496414185, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.6144572496414185, Class Loss=0.6144572496414185, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=5.3571486353874205
Loss made of: CE 0.5121909379959106, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.755752086639404 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.375885602831841
Loss made of: CE 0.6386330127716064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.543894290924072 EntMin 0.0
Epoch 5, Class Loss=0.5638617873191833, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.5638617873191833, Class Loss=0.5638617873191833, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/21, Loss=5.382691910862922
Loss made of: CE 0.5076800584793091, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.329495429992676 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.130295625329017
Loss made of: CE 0.5321158170700073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.186967849731445 EntMin 0.0
Epoch 6, Class Loss=0.5347772836685181, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.5347772836685181, Class Loss=0.5347772836685181, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=6.837235221266747
Loss made of: CE 0.3246241807937622, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.648784637451172 EntMin 0.0
Epoch 1, Batch 20/26, Loss=6.121443775296211
Loss made of: CE 0.31354820728302, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.784770965576172 EntMin 0.0
Epoch 1, Class Loss=0.37487155199050903, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.37487155199050903, Class Loss=0.37487155199050903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=6.243350526690483
Loss made of: CE 0.5070981979370117, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.07052755355835 EntMin 0.0
Epoch 2, Batch 20/26, Loss=5.978957462310791
Loss made of: CE 0.5755487680435181, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.916404724121094 EntMin 0.0
Epoch 2, Class Loss=0.46115782856941223, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.46115782856941223, Class Loss=0.46115782856941223, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/26, Loss=5.709064078330994
Loss made of: CE 0.49100828170776367, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.241290092468262 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.7654480040073395
Loss made of: CE 0.45738014578819275, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.709446907043457 EntMin 0.0
Epoch 3, Class Loss=0.48828309774398804, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.48828309774398804, Class Loss=0.48828309774398804, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.404987394809723
Loss made of: CE 0.5464448928833008, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.091601371765137 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.3923111140727995
Loss made of: CE 0.5186226963996887, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.628260135650635 EntMin 0.0
Epoch 4, Class Loss=0.5026430487632751, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.5026430487632751, Class Loss=0.5026430487632751, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.305518800020218
Loss made of: CE 0.7100499868392944, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0342254638671875 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.374230343103409
Loss made of: CE 0.5259224772453308, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.888044357299805 EntMin 0.0
Epoch 5, Class Loss=0.5157214999198914, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5157214999198914, Class Loss=0.5157214999198914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.310604071617126
Loss made of: CE 0.5005089640617371, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.048089027404785 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.267779165506363
Loss made of: CE 0.5001359581947327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.947730541229248 EntMin 0.0
Epoch 6, Class Loss=0.5283895134925842, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5283895134925842, Class Loss=0.5283895134925842, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=6.48307800590992
Loss made of: CE 0.3450692296028137, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.08152437210083 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/26, Loss=6.2626905426383015
Loss made of: CE 0.42233118414878845, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.813019752502441 EntMin 0.0
Epoch 1, Class Loss=0.37056997418403625, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.37056997418403625, Class Loss=0.37056997418403625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=5.832290118932724
Loss made of: CE 0.5966637134552002, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.762773513793945 EntMin 0.0
Epoch 2, Batch 20/26, Loss=6.052493694424629
Loss made of: CE 0.5131739974021912, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.618927001953125 EntMin 0.0
Epoch 2, Class Loss=0.4667607247829437, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.4667607247829437, Class Loss=0.4667607247829437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=5.923695710301399
Loss made of: CE 0.5298544764518738, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.058032035827637 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.493211168050766
Loss made of: CE 0.4737747311592102, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9772725105285645 EntMin 0.0
Epoch 3, Class Loss=0.48100781440734863, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.48100781440734863, Class Loss=0.48100781440734863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.409689718484879
Loss made of: CE 0.4146812856197357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.592489242553711 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.281077945232392
Loss made of: CE 0.43047353625297546, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.624221324920654 EntMin 0.0
Epoch 4, Class Loss=0.5030519962310791, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5030519962310791, Class Loss=0.5030519962310791, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.529421451687813
Loss made of: CE 0.4585598409175873, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.386334419250488 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.517459008097648
Loss made of: CE 0.519328773021698, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.630339622497559 EntMin 0.0
Epoch 5, Class Loss=0.5161415338516235, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5161415338516235, Class Loss=0.5161415338516235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.674157348275185
Loss made of: CE 0.4158949851989746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.544791221618652 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.105408316850662
Loss made of: CE 0.5991675853729248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.242465019226074 EntMin 0.0
Epoch 6, Class Loss=0.5185701251029968, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5185701251029968, Class Loss=0.5185701251029968, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=6.41492700278759
Loss made of: CE 0.40599924325942993, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.384214401245117 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=6.2034132272005085
Loss made of: CE 0.456850528717041, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.825285911560059 EntMin 0.0
Epoch 1, Class Loss=0.34388861060142517, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.34388861060142517, Class Loss=0.34388861060142517, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/24, Loss=6.069245675206185
Loss made of: CE 0.58783358335495, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.304337501525879 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.030809393525123
Loss made of: CE 0.44984158873558044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.997452259063721 EntMin 0.0
Epoch 2, Class Loss=0.4336203932762146, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.4336203932762146, Class Loss=0.4336203932762146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/24, Loss=5.845197176933288
Loss made of: CE 0.49581870436668396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.88452672958374 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.048422512412071
Loss made of: CE 0.33148255944252014, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.744028091430664 EntMin 0.0
Epoch 3, Class Loss=0.4837988317012787, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4837988317012787, Class Loss=0.4837988317012787, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/24, Loss=5.378511801362038
Loss made of: CE 0.5287996530532837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746232032775879 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.161213859915733
Loss made of: CE 0.5296639204025269, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.216807842254639 EntMin 0.0
Epoch 4, Class Loss=0.5167298913002014, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5167298913002014, Class Loss=0.5167298913002014, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/24, Loss=5.886091008782387
Loss made of: CE 0.5321528911590576, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.958311557769775 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.772509855031967
Loss made of: CE 0.4766497015953064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.305804252624512 EntMin 0.0
Epoch 5, Class Loss=0.5664645433425903, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5664645433425903, Class Loss=0.5664645433425903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/24, Loss=5.561564835906029
Loss made of: CE 0.6595526933670044, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.084081172943115 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.520305091142655
Loss made of: CE 0.6473863124847412, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.730745315551758 EntMin 0.0
Epoch 6, Class Loss=0.5822954177856445, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5822954177856445, Class Loss=0.5822954177856445, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.651512861251831, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.799393
Mean Acc: 0.326594
FreqW Acc: 0.659337
Mean IoU: 0.263154
Class IoU:
	class 0: 0.8131321
	class 1: 0.3686608
	class 2: 0.22322163
	class 3: 0.04794524
	class 4: 0.50645834
	class 5: 6.943199e-05
	class 6: 0.5568155
	class 7: 0.33730125
	class 8: 0.21217577
	class 9: 0.0
	class 10: 0.0909698
	class 11: 0.27445433
	class 12: 0.34782717
	class 13: 0.2987578
	class 14: 0.6443646
	class 15: 0.6689168
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.092533626
	class 20: 0.042632267
Class Acc:
	class 0: 0.98752415
	class 1: 0.3696942
	class 2: 0.33828926
	class 3: 0.04797794
	class 4: 0.7410477
	class 5: 6.943457e-05
	class 6: 0.5773145
	class 7: 0.33940834
	class 8: 0.21258295
	class 9: 0.0
	class 10: 0.09248183
	class 11: 0.30526164
	class 12: 0.7866014
	class 13: 0.4532064
	class 14: 0.76031864
	class 15: 0.7108167
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.09282946
	class 20: 0.043048903

federated global round: 22, step: 4
select part of clients to conduct local training
[0, 8, 16, 14]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.206005173921585
Loss made of: CE 0.3696044683456421, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.100829124450684 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.7349013313651085
Loss made of: CE 0.35332632064819336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.677892684936523 EntMin 0.0
Epoch 1, Class Loss=0.2856758236885071, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.2856758236885071, Class Loss=0.2856758236885071, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/24, Loss=5.686901009082794
Loss made of: CE 0.2536163032054901, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.02741813659668 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.295709554851055
Loss made of: CE 0.36935311555862427, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.194759368896484 EntMin 0.0
Epoch 2, Class Loss=0.3459084928035736, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.3459084928035736, Class Loss=0.3459084928035736, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/24, Loss=5.48388987928629
Loss made of: CE 0.5064284205436707, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.108914375305176 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.892195108532905
Loss made of: CE 0.43299350142478943, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.122854232788086 EntMin 0.0
Epoch 3, Class Loss=0.40242093801498413, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.40242093801498413, Class Loss=0.40242093801498413, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/24, Loss=5.331014657020569
Loss made of: CE 0.46833324432373047, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.812522888183594 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.703325846791268
Loss made of: CE 0.5088285207748413, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2950310707092285 EntMin 0.0
Epoch 4, Class Loss=0.45026838779449463, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.45026838779449463, Class Loss=0.45026838779449463, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/24, Loss=5.725236359238624
Loss made of: CE 0.47880372405052185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.637953281402588 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.298914858698845
Loss made of: CE 0.4587361216545105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.648899078369141 EntMin 0.0
Epoch 5, Class Loss=0.4858033359050751, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.4858033359050751, Class Loss=0.4858033359050751, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/24, Loss=5.389609837532044
Loss made of: CE 0.5779539942741394, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.666890621185303 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.478532710671425
Loss made of: CE 0.464610755443573, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9415547847747803 EntMin 0.0
Epoch 6, Class Loss=0.5287911295890808, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.5287911295890808, Class Loss=0.5287911295890808, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.576945507526398
Loss made of: CE 0.7707318067550659, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.214864730834961 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.741367822885513
Loss made of: CE 0.6492051482200623, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.404382228851318 EntMin 0.0
Epoch 1, Class Loss=0.8178282976150513, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.8178282976150513, Class Loss=0.8178282976150513, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/24, Loss=5.950577735900879
Loss made of: CE 0.5601103901863098, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.631192207336426 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.662958961725235
Loss made of: CE 0.6442698240280151, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.972440719604492 EntMin 0.0
Epoch 2, Class Loss=0.6992164850234985, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.6992164850234985, Class Loss=0.6992164850234985, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/24, Loss=5.6465373694896694
Loss made of: CE 0.643033504486084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.493727684020996 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.657549172639847
Loss made of: CE 0.715779185295105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.799176216125488 EntMin 0.0
Epoch 3, Class Loss=0.6347230076789856, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.6347230076789856, Class Loss=0.6347230076789856, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/24, Loss=5.495575124025345
Loss made of: CE 0.6598898768424988, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0677361488342285 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.4884508609771725
Loss made of: CE 0.5726135969161987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.475181579589844 EntMin 0.0
Epoch 4, Class Loss=0.5999031066894531, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5999031066894531, Class Loss=0.5999031066894531, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/24, Loss=5.658370557427406
Loss made of: CE 0.7420541644096375, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.808429718017578 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.200145760178566
Loss made of: CE 0.5711495280265808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421498775482178 EntMin 0.0
Epoch 5, Class Loss=0.5748693943023682, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.5748693943023682, Class Loss=0.5748693943023682, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/24, Loss=5.3680410295724865
Loss made of: CE 0.5864110589027405, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.514470100402832 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.154937669634819
Loss made of: CE 0.5515267848968506, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.447954177856445 EntMin 0.0
Epoch 6, Class Loss=0.5585566759109497, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.5585566759109497, Class Loss=0.5585566759109497, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=7.944418412446976
Loss made of: CE 0.363452672958374, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.455226421356201 EntMin 0.0
Epoch 1, Class Loss=0.38495996594429016, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.38495996594429016, Class Loss=0.38495996594429016, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/19, Loss=6.662008985877037
Loss made of: CE 0.46013230085372925, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.160188674926758 EntMin 0.0
Epoch 2, Class Loss=0.4570223093032837, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.4570223093032837, Class Loss=0.4570223093032837, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/19, Loss=6.0875810295343395
Loss made of: CE 0.5684889554977417, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.426316738128662 EntMin 0.0
Epoch 3, Class Loss=0.456745445728302, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.456745445728302, Class Loss=0.456745445728302, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/19, Loss=5.90519913136959
Loss made of: CE 0.5474475622177124, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.628200054168701 EntMin 0.0
Epoch 4, Class Loss=0.444511353969574, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.444511353969574, Class Loss=0.444511353969574, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/19, Loss=5.74378564953804
Loss made of: CE 0.3364536464214325, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.067269325256348 EntMin 0.0
Epoch 5, Class Loss=0.4194987714290619, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.4194987714290619, Class Loss=0.4194987714290619, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/19, Loss=5.461472123861313
Loss made of: CE 0.3453388214111328, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.421917915344238 EntMin 0.0
Epoch 6, Class Loss=0.4102444052696228, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.4102444052696228, Class Loss=0.4102444052696228, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=6.324130666255951
Loss made of: CE 0.695689857006073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734566688537598 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.495850676298142
Loss made of: CE 0.6010775566101074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.140707969665527 EntMin 0.0
Epoch 1, Class Loss=0.6889365315437317, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.6889365315437317, Class Loss=0.6889365315437317, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/26, Loss=5.771226391196251
Loss made of: CE 0.629673957824707, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.38192081451416 EntMin 0.0
Epoch 2, Batch 20/26, Loss=5.632475990056991
Loss made of: CE 0.5358888506889343, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.625904083251953 EntMin 0.0
Epoch 2, Class Loss=0.5843416452407837, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.5843416452407837, Class Loss=0.5843416452407837, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/26, Loss=5.3855537444353105
Loss made of: CE 0.7157847881317139, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.584800720214844 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.392477911710739
Loss made of: CE 0.506919264793396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.393257141113281 EntMin 0.0
Epoch 3, Class Loss=0.5451039671897888, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.5451039671897888, Class Loss=0.5451039671897888, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/26, Loss=5.198443326354027
Loss made of: CE 0.48365074396133423, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.99514102935791 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.093403908610344
Loss made of: CE 0.533270001411438, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.131011962890625 EntMin 0.0
Epoch 4, Class Loss=0.5095734596252441, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.5095734596252441, Class Loss=0.5095734596252441, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/26, Loss=5.044389390945435
Loss made of: CE 0.641963005065918, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.300184726715088 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.056033685803413
Loss made of: CE 0.5338047742843628, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.916595458984375 EntMin 0.0
Epoch 5, Class Loss=0.4900515079498291, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.4900515079498291, Class Loss=0.4900515079498291, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/26, Loss=4.970551902055741
Loss made of: CE 0.44195184111595154, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.385659217834473 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.9849206656217575
Loss made of: CE 0.4298741817474365, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.117088317871094 EntMin 0.0
Epoch 6, Class Loss=0.4752641022205353, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.4752641022205353, Class Loss=0.4752641022205353, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6213897466659546, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.808983
Mean Acc: 0.353520
FreqW Acc: 0.677230
Mean IoU: 0.290651
Class IoU:
	class 0: 0.8260564
	class 1: 0.40733105
	class 2: 0.22461307
	class 3: 0.06474098
	class 4: 0.54939693
	class 5: 7.977351e-05
	class 6: 0.55171067
	class 7: 0.3620013
	class 8: 0.25169075
	class 9: 0.0
	class 10: 0.113196604
	class 11: 0.24876837
	class 12: 0.34380347
	class 13: 0.28718093
	class 14: 0.64492184
	class 15: 0.6733463
	class 16: 0.0
	class 17: 0.0
	class 18: 0.003255946
	class 19: 0.37989992
	class 20: 0.17168467
Class Acc:
	class 0: 0.98653275
	class 1: 0.4086201
	class 2: 0.34345683
	class 3: 0.06480437
	class 4: 0.73982096
	class 5: 7.977589e-05
	class 6: 0.56471133
	class 7: 0.36441717
	class 8: 0.25264075
	class 9: 0.0
	class 10: 0.115285814
	class 11: 0.2751384
	class 12: 0.83512807
	class 13: 0.41112125
	class 14: 0.74391156
	class 15: 0.7180956
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0032674114
	class 19: 0.4199495
	class 20: 0.1769283

federated global round: 23, step: 4
select part of clients to conduct local training
[12, 10, 9, 6]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=5.152170605957508
Loss made of: CE 0.21073052287101746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.099672794342041 EntMin 0.0
Epoch 1, Batch 20/26, Loss=4.865842624008655
Loss made of: CE 0.17187514901161194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0449934005737305 EntMin 0.0
Epoch 1, Class Loss=0.23759494721889496, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.23759494721889496, Class Loss=0.23759494721889496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/26, Loss=4.853860275447369
Loss made of: CE 0.31937283277511597, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.933727264404297 EntMin 0.0
Epoch 2, Batch 20/26, Loss=4.920646776258946
Loss made of: CE 0.2177848219871521, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581656455993652 EntMin 0.0
Epoch 2, Class Loss=0.27903151512145996, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.27903151512145996, Class Loss=0.27903151512145996, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/26, Loss=5.074593743681907
Loss made of: CE 0.2970542013645172, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.850150108337402 EntMin 0.0
Epoch 3, Batch 20/26, Loss=4.916612923145294
Loss made of: CE 0.3555147051811218, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.111799240112305 EntMin 0.0
Epoch 3, Class Loss=0.3276384472846985, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.3276384472846985, Class Loss=0.3276384472846985, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/26, Loss=4.764340499043465
Loss made of: CE 0.3766458034515381, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.727972507476807 EntMin 0.0
Epoch 4, Batch 20/26, Loss=4.75470906496048
Loss made of: CE 0.4075171947479248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0177812576293945 EntMin 0.0
Epoch 4, Class Loss=0.35586562752723694, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.35586562752723694, Class Loss=0.35586562752723694, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/26, Loss=4.696327403187752
Loss made of: CE 0.4000650644302368, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.216996192932129 EntMin 0.0
Epoch 5, Batch 20/26, Loss=4.984137442708016
Loss made of: CE 0.47291892766952515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.842164993286133 EntMin 0.0
Epoch 5, Class Loss=0.39712873101234436, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.39712873101234436, Class Loss=0.39712873101234436, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/26, Loss=4.586398553848267
Loss made of: CE 0.4041981101036072, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6506500244140625 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.471165969967842
Loss made of: CE 0.39825162291526794, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.855771064758301 EntMin 0.0
Epoch 6, Class Loss=0.4241243004798889, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.4241243004798889, Class Loss=0.4241243004798889, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=6.558383184671402
Loss made of: CE 0.2974948287010193, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.082969665527344 EntMin 0.0
Epoch 1, Class Loss=0.35245460271835327, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.35245460271835327, Class Loss=0.35245460271835327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=5.873013371229172
Loss made of: CE 0.37876150012016296, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.506509304046631 EntMin 0.0
Epoch 2, Class Loss=0.4531500041484833, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.4531500041484833, Class Loss=0.4531500041484833, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=5.440466374158859
Loss made of: CE 0.4747353494167328, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679025650024414 EntMin 0.0
Epoch 3, Class Loss=0.5079342722892761, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5079342722892761, Class Loss=0.5079342722892761, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=5.5757099986076355
Loss made of: CE 0.5256286263465881, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.214332103729248 EntMin 0.0
Epoch 4, Class Loss=0.5129253268241882, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.5129253268241882, Class Loss=0.5129253268241882, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=5.476045545935631
Loss made of: CE 0.5753069519996643, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.621184349060059 EntMin 0.0
Epoch 5, Class Loss=0.5475234389305115, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.5475234389305115, Class Loss=0.5475234389305115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=5.1646087795495985
Loss made of: CE 0.5906144976615906, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1383137702941895 EntMin 0.0
Epoch 6, Class Loss=0.5609315633773804, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.5609315633773804, Class Loss=0.5609315633773804, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=5.415453538298607
Loss made of: CE 0.20280955731868744, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.122572898864746 EntMin 0.0
Epoch 1, Batch 20/21, Loss=4.839344112575054
Loss made of: CE 0.16982334852218628, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.547858238220215 EntMin 0.0
Epoch 1, Class Loss=0.19709140062332153, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.19709140062332153, Class Loss=0.19709140062332153, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/21, Loss=4.779511541128159
Loss made of: CE 0.2629244029521942, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.593600749969482 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.086937679350376
Loss made of: CE 0.2217843383550644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.140525817871094 EntMin 0.0
Epoch 2, Class Loss=0.30337855219841003, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.30337855219841003, Class Loss=0.30337855219841003, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/21, Loss=5.201793703436851
Loss made of: CE 0.3705623149871826, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.134640216827393 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.047645196318626
Loss made of: CE 0.42602047324180603, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7487711906433105 EntMin 0.0
Epoch 3, Class Loss=0.3620857000350952, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.3620857000350952, Class Loss=0.3620857000350952, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/21, Loss=4.719910964369774
Loss made of: CE 0.4168670177459717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.115266799926758 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.9151893854141235
Loss made of: CE 0.3631880283355713, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8168153762817383 EntMin 0.0
Epoch 4, Class Loss=0.3832142651081085, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.3832142651081085, Class Loss=0.3832142651081085, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=4.7015388935804365
Loss made of: CE 0.3535638749599457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.654501438140869 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.760677587985993
Loss made of: CE 0.3615577816963196, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.686577796936035 EntMin 0.0
Epoch 5, Class Loss=0.41252341866493225, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.41252341866493225, Class Loss=0.41252341866493225, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/21, Loss=5.117212414741516
Loss made of: CE 0.45017027854919434, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.382747173309326 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.921936094760895
Loss made of: CE 0.46979305148124695, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7054123878479004 EntMin 0.0
Epoch 6, Class Loss=0.44840487837791443, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.44840487837791443, Class Loss=0.44840487837791443, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=5.634355339407921
Loss made of: CE 0.21471552550792694, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.204000949859619 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.533003163337708
Loss made of: CE 0.2427273839712143, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726862907409668 EntMin 0.0
Epoch 1, Class Loss=0.23790119588375092, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.23790119588375092, Class Loss=0.23790119588375092, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/24, Loss=5.174608483910561
Loss made of: CE 0.31174182891845703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.566059112548828 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.385751888155937
Loss made of: CE 0.3020075559616089, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.498424530029297 EntMin 0.0
Epoch 2, Class Loss=0.29540395736694336, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.29540395736694336, Class Loss=0.29540395736694336, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/24, Loss=5.293078815937042
Loss made of: CE 0.3370475172996521, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.608026504516602 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.095443078875542
Loss made of: CE 0.37284526228904724, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623642921447754 EntMin 0.0
Epoch 3, Class Loss=0.35502201318740845, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.35502201318740845, Class Loss=0.35502201318740845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/24, Loss=5.173789697885513
Loss made of: CE 0.35193246603012085, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.672396659851074 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.0192086607217785
Loss made of: CE 0.48204725980758667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.473876953125 EntMin 0.0
Epoch 4, Class Loss=0.4100974202156067, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4100974202156067, Class Loss=0.4100974202156067, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/24, Loss=5.126467397809028
Loss made of: CE 0.6679942011833191, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.465151786804199 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.492550736665725
Loss made of: CE 0.3693947196006775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.172279357910156 EntMin 0.0
Epoch 5, Class Loss=0.45284396409988403, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.45284396409988403, Class Loss=0.45284396409988403, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/24, Loss=5.294873505830765
Loss made of: CE 0.5680074691772461, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.453157424926758 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.120770865678788
Loss made of: CE 0.5780539512634277, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.676975250244141 EntMin 0.0
Epoch 6, Class Loss=0.5048794746398926, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5048794746398926, Class Loss=0.5048794746398926, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6092532873153687, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.813945
Mean Acc: 0.383286
FreqW Acc: 0.688096
Mean IoU: 0.313127
Class IoU:
	class 0: 0.8341475
	class 1: 0.4057417
	class 2: 0.23614717
	class 3: 0.08608849
	class 4: 0.54071885
	class 5: 0.00025113908
	class 6: 0.5183607
	class 7: 0.3729836
	class 8: 0.27805302
	class 9: 0.0
	class 10: 0.11373767
	class 11: 0.27304327
	class 12: 0.34602627
	class 13: 0.29719195
	class 14: 0.6464514
	class 15: 0.6818477
	class 16: 0.0
	class 17: 0.08963559
	class 18: 0.1258336
	class 19: 0.36089346
	class 20: 0.36851713
Class Acc:
	class 0: 0.98256373
	class 1: 0.40687525
	class 2: 0.36847055
	class 3: 0.08627566
	class 4: 0.77200055
	class 5: 0.00025114632
	class 6: 0.5295451
	class 7: 0.37597337
	class 8: 0.2801521
	class 9: 0.0
	class 10: 0.11520139
	class 11: 0.31438458
	class 12: 0.8720844
	class 13: 0.42258593
	class 14: 0.7492952
	class 15: 0.73356
	class 16: 0.0
	class 17: 0.09012276
	class 18: 0.13721661
	class 19: 0.4017502
	class 20: 0.41069508

federated global round: 24, step: 4
select part of clients to conduct local training
[18, 24, 25, 10]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=6.130543836951256
Loss made of: CE 0.3083164691925049, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.117790222167969 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.26764196157455444, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.26764196157455444, Class Loss=0.26764196157455444, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/19, Loss=5.594093000888824
Loss made of: CE 0.44468802213668823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8755998611450195 EntMin 0.0
Epoch 2, Class Loss=0.36099451780319214, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.36099451780319214, Class Loss=0.36099451780319214, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/19, Loss=5.413808882236481
Loss made of: CE 0.470595121383667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.552707672119141 EntMin 0.0
Epoch 3, Class Loss=0.4191639721393585, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.4191639721393585, Class Loss=0.4191639721393585, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/19, Loss=5.318372341990471
Loss made of: CE 0.3506828248500824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.121266841888428 EntMin 0.0
Epoch 4, Class Loss=0.4583010971546173, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.4583010971546173, Class Loss=0.4583010971546173, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/19, Loss=5.400763812661171
Loss made of: CE 0.4252055883407593, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.828220844268799 EntMin 0.0
Epoch 5, Class Loss=0.495163232088089, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.495163232088089, Class Loss=0.495163232088089, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/19, Loss=5.0845673173666
Loss made of: CE 0.5569260120391846, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.463703155517578 EntMin 0.0
Epoch 6, Class Loss=0.5309552550315857, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.5309552550315857, Class Loss=0.5309552550315857, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/21, Loss=4.523600272834301
Loss made of: CE 0.13403376936912537, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143697261810303 EntMin 0.0
Epoch 1, Batch 20/21, Loss=4.936245852708817
Loss made of: CE 0.1633853018283844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807551383972168 EntMin 0.0
Epoch 1, Class Loss=0.17556077241897583, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.17556077241897583, Class Loss=0.17556077241897583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/21, Loss=4.583908370137214
Loss made of: CE 0.2652735710144043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.022290229797363 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.204798971116543
Loss made of: CE 0.3623896539211273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.893293380737305 EntMin 0.0
Epoch 2, Class Loss=0.26503098011016846, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.26503098011016846, Class Loss=0.26503098011016846, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/21, Loss=4.970087410509587
Loss made of: CE 0.39066267013549805, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.372797012329102 EntMin 0.0
Epoch 3, Batch 20/21, Loss=4.724151760339737
Loss made of: CE 0.35687193274497986, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.435969352722168 EntMin 0.0
Epoch 3, Class Loss=0.3267107605934143, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.3267107605934143, Class Loss=0.3267107605934143, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/21, Loss=4.506913563609123
Loss made of: CE 0.3871135711669922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4961676597595215 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.6094034075737
Loss made of: CE 0.4027181565761566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.211146354675293 EntMin 0.0
Epoch 4, Class Loss=0.35601967573165894, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.35601967573165894, Class Loss=0.35601967573165894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/21, Loss=4.5861227184534075
Loss made of: CE 0.4710090756416321, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.26740837097168 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.592573973536491
Loss made of: CE 0.38081490993499756, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.021470069885254 EntMin 0.0
Epoch 5, Class Loss=0.4099559783935547, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.4099559783935547, Class Loss=0.4099559783935547, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/21, Loss=4.903384330868721
Loss made of: CE 0.5556460022926331, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.341919898986816 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.360311734676361
Loss made of: CE 0.4635319709777832, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5876970291137695 EntMin 0.0
Epoch 6, Class Loss=0.44936317205429077, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.44936317205429077, Class Loss=0.44936317205429077, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=5.086951678991317
Loss made of: CE 0.21544460952281952, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.68010139465332 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=4.976066859066487
Loss made of: CE 0.23985636234283447, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.23960018157959 EntMin 0.0
Epoch 1, Class Loss=0.19462883472442627, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.19462883472442627, Class Loss=0.19462883472442627, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/24, Loss=5.20944297760725
Loss made of: CE 0.33946770429611206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.796326637268066 EntMin 0.0
Epoch 2, Batch 20/24, Loss=4.840618532896042
Loss made of: CE 0.21774785220623016, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.703946113586426 EntMin 0.0
Epoch 2, Class Loss=0.26792070269584656, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.26792070269584656, Class Loss=0.26792070269584656, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/24, Loss=5.218974414467811
Loss made of: CE 0.3051047623157501, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.856848239898682 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.797619253396988
Loss made of: CE 0.21534332633018494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.191019535064697 EntMin 0.0
Epoch 3, Class Loss=0.33572280406951904, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.33572280406951904, Class Loss=0.33572280406951904, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/24, Loss=5.103706735372543
Loss made of: CE 0.3360825181007385, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.20510721206665 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.002939966320992
Loss made of: CE 0.38900861144065857, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.637032508850098 EntMin 0.0
Epoch 4, Class Loss=0.38782912492752075, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.38782912492752075, Class Loss=0.38782912492752075, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/24, Loss=4.992439970374107
Loss made of: CE 0.44291120767593384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1238555908203125 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.962454307079315
Loss made of: CE 0.4722368121147156, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.368542671203613 EntMin 0.0
Epoch 5, Class Loss=0.4593183994293213, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.4593183994293213, Class Loss=0.4593183994293213, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/24, Loss=4.643809503316879
Loss made of: CE 0.5875625014305115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.265161514282227 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.886512187123299
Loss made of: CE 0.5801942944526672, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.636171817779541 EntMin 0.0
Epoch 6, Class Loss=0.5166181921958923, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.5166181921958923, Class Loss=0.5166181921958923, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=6.398485463857651
Loss made of: CE 0.7398900985717773, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.479640960693359 EntMin 0.0
Epoch 1, Class Loss=0.8149361610412598, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.8149361610412598, Class Loss=0.8149361610412598, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/19, Loss=5.755656766891479
Loss made of: CE 0.5370567440986633, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.002682685852051 EntMin 0.0
Epoch 2, Class Loss=0.723591148853302, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.723591148853302, Class Loss=0.723591148853302, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/19, Loss=5.421457666158676
Loss made of: CE 0.6066126823425293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734526634216309 EntMin 0.0
Epoch 3, Class Loss=0.6883776187896729, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.6883776187896729, Class Loss=0.6883776187896729, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/19, Loss=5.6268865168094635
Loss made of: CE 0.6892062425613403, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.129960536956787 EntMin 0.0
Epoch 4, Class Loss=0.6302642226219177, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.6302642226219177, Class Loss=0.6302642226219177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/19, Loss=5.456876307725906
Loss made of: CE 0.6081598997116089, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.434873580932617 EntMin 0.0
Epoch 5, Class Loss=0.6245278120040894, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.6245278120040894, Class Loss=0.6245278120040894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/19, Loss=5.114495664834976
Loss made of: CE 0.6176952123641968, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.164309978485107 EntMin 0.0
Epoch 6, Class Loss=0.6027700901031494, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6027700901031494, Class Loss=0.6027700901031494, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.6306471824645996, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.813296
Mean Acc: 0.413532
FreqW Acc: 0.689588
Mean IoU: 0.322788
Class IoU:
	class 0: 0.8345446
	class 1: 0.37295932
	class 2: 0.2540084
	class 3: 0.11164622
	class 4: 0.49015704
	class 5: 0.00036489588
	class 6: 0.5001104
	class 7: 0.34172034
	class 8: 0.32718384
	class 9: 0.0
	class 10: 0.03807678
	class 11: 0.2853717
	class 12: 0.37748736
	class 13: 0.30409586
	class 14: 0.6402842
	class 15: 0.6682643
	class 16: 0.0
	class 17: 0.35664776
	class 18: 0.22570226
	class 19: 0.25130686
	class 20: 0.39862525
Class Acc:
	class 0: 0.9755723
	class 1: 0.3738272
	class 2: 0.41302148
	class 3: 0.112035245
	class 4: 0.8048384
	class 5: 0.00036490086
	class 6: 0.5141264
	class 7: 0.34425548
	class 8: 0.33070272
	class 9: 0.0
	class 10: 0.038172863
	class 11: 0.33457866
	class 12: 0.85828006
	class 13: 0.38103327
	class 14: 0.7462006
	class 15: 0.7164174
	class 16: 0.0
	class 17: 0.6714908
	class 18: 0.3400401
	class 19: 0.26365656
	class 20: 0.46555984

voc_4-4_OURS-FRC On GPUs 0
Run in 87808s
