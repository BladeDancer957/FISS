nohup: ignoring input
30
kvoc_15-1_OURS On GPUs 2\Writing in results/seed_2023-ov/2023-03-06_voc_15-1_OURS.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/198, Loss=1.7367741227149964
Loss made of: CE 1.0693084001541138, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/198, Loss=1.0547017991542815
Loss made of: CE 1.097031593322754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/198, Loss=0.8315122365951538
Loss made of: CE 0.7505773305892944, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/198, Loss=0.7375621676445008
Loss made of: CE 0.5982898473739624, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/198, Loss=0.5906625777482987
Loss made of: CE 0.61363285779953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/198, Loss=0.5713244616985321
Loss made of: CE 0.4206603169441223, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/198, Loss=0.5693920224905014
Loss made of: CE 0.5019385814666748, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/198, Loss=0.5267238050699234
Loss made of: CE 0.4173940122127533, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/198, Loss=0.47953246235847474
Loss made of: CE 0.390399694442749, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/198, Loss=0.4940240651369095
Loss made of: CE 0.4677221477031708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/198, Loss=0.47212354838848114
Loss made of: CE 0.4058769643306732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/198, Loss=0.5114106565713883
Loss made of: CE 0.4004085063934326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/198, Loss=0.43590937852859496
Loss made of: CE 0.4918178915977478, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/198, Loss=0.41321343183517456
Loss made of: CE 0.341902494430542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/198, Loss=0.4420814573764801
Loss made of: CE 0.7263855934143066, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/198, Loss=0.3941949397325516
Loss made of: CE 0.33782076835632324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/198, Loss=0.4350924640893936
Loss made of: CE 0.37266597151756287, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/198, Loss=0.380178102850914
Loss made of: CE 0.33501872420310974, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/198, Loss=0.360734561085701
Loss made of: CE 0.4254007935523987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5917954444885254, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.5917954444885254, Class Loss=0.5917954444885254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/198, Loss=0.32779050469398496
Loss made of: CE 0.21742412447929382, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/198, Loss=0.324105042219162
Loss made of: CE 0.24738547205924988, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/198, Loss=0.31691216230392455
Loss made of: CE 0.24447819590568542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/198, Loss=0.326066717505455
Loss made of: CE 0.3398423194885254, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/198, Loss=0.32306745648384094
Loss made of: CE 0.31620046496391296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/198, Loss=0.34163424372673035
Loss made of: CE 0.29530781507492065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/198, Loss=0.3416931167244911
Loss made of: CE 0.2479678839445114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/198, Loss=0.2982894405722618
Loss made of: CE 0.23124417662620544, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/198, Loss=0.34861619472503663
Loss made of: CE 0.41245001554489136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/198, Loss=0.3460982903838158
Loss made of: CE 0.3488064706325531, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/198, Loss=0.31418093740940095
Loss made of: CE 0.22828346490859985, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/198, Loss=0.31585911512374876
Loss made of: CE 0.3309032917022705, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/198, Loss=0.281070365011692
Loss made of: CE 0.337754487991333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/198, Loss=0.3312512099742889
Loss made of: CE 0.44603487849235535, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/198, Loss=0.34466719031333926
Loss made of: CE 0.3037008047103882, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/198, Loss=0.3105153188109398
Loss made of: CE 0.2650614082813263, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/198, Loss=0.3373629629611969
Loss made of: CE 0.3782726526260376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/198, Loss=0.3261917978525162
Loss made of: CE 0.36476755142211914, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/198, Loss=0.31496371030807496
Loss made of: CE 0.42135554552078247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3240017890930176, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3240017890930176, Class Loss=0.3240017890930176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/198, Loss=0.2596866965293884
Loss made of: CE 0.24459658563137054, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/198, Loss=0.23456123918294908
Loss made of: CE 0.24360167980194092, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/198, Loss=0.25929400473833086
Loss made of: CE 0.2367403507232666, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/198, Loss=0.24928345084190368
Loss made of: CE 0.1970011442899704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/198, Loss=0.23123835027217865
Loss made of: CE 0.20243196189403534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/198, Loss=0.26596964001655576
Loss made of: CE 0.35239529609680176, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/198, Loss=0.26017330437898634
Loss made of: CE 0.2946981191635132, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/198, Loss=0.2683702141046524
Loss made of: CE 0.3507412374019623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/198, Loss=0.28003018647432326
Loss made of: CE 0.3216507136821747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/198, Loss=0.2453071042895317
Loss made of: CE 0.16961996257305145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/198, Loss=0.26036375015974045
Loss made of: CE 0.18095329403877258, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/198, Loss=0.24721750020980834
Loss made of: CE 0.2561988830566406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/198, Loss=0.2549889236688614
Loss made of: CE 0.27718016505241394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/198, Loss=0.25535187423229216
Loss made of: CE 0.19899730384349823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/198, Loss=0.29633956402540207
Loss made of: CE 0.4404263198375702, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/198, Loss=0.25879233330488205
Loss made of: CE 0.28744277358055115, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/198, Loss=0.24428222179412842
Loss made of: CE 0.22103440761566162, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/198, Loss=0.26658440977334974
Loss made of: CE 0.25180816650390625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/198, Loss=0.2620586082339287
Loss made of: CE 0.26331162452697754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2576129138469696, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.2576129138469696, Class Loss=0.2576129138469696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/198, Loss=0.23305050432682037
Loss made of: CE 0.209153413772583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/198, Loss=0.21735777705907822
Loss made of: CE 0.2887091040611267, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/198, Loss=0.23080435693264006
Loss made of: CE 0.255376398563385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/198, Loss=0.2068722888827324
Loss made of: CE 0.2080220878124237, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/198, Loss=0.23164940625429153
Loss made of: CE 0.2213444709777832, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/198, Loss=0.1927313968539238
Loss made of: CE 0.1829698085784912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/198, Loss=0.21887647584080697
Loss made of: CE 0.29975026845932007, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/198, Loss=0.2181255966424942
Loss made of: CE 0.23452655971050262, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/198, Loss=0.18427281752228736
Loss made of: CE 0.24959012866020203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/198, Loss=0.23171558380126953
Loss made of: CE 0.25040021538734436, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/198, Loss=0.22863137423992158
Loss made of: CE 0.26603659987449646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/198, Loss=0.19812091588973998
Loss made of: CE 0.20786522328853607, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/198, Loss=0.2275398701429367
Loss made of: CE 0.1975977122783661, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/198, Loss=0.23641784936189653
Loss made of: CE 0.31065744161605835, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/198, Loss=0.21926183253526688
Loss made of: CE 0.2943250238895416, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/198, Loss=0.22029877156019212
Loss made of: CE 0.1936439722776413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/198, Loss=0.2248044490814209
Loss made of: CE 0.25380611419677734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/198, Loss=0.23118882179260253
Loss made of: CE 0.2594640254974365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/198, Loss=0.22794462442398072
Loss made of: CE 0.23938779532909393, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.21977679431438446, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.21977679431438446, Class Loss=0.21977679431438446, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/198, Loss=0.20597985982894898
Loss made of: CE 0.22165006399154663, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/198, Loss=0.19032803326845169
Loss made of: CE 0.22503578662872314, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/198, Loss=0.1787453681230545
Loss made of: CE 0.19416090846061707, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/198, Loss=0.18856136202812196
Loss made of: CE 0.20098654925823212, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/198, Loss=0.1846199095249176
Loss made of: CE 0.23443233966827393, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/198, Loss=0.2060439929366112
Loss made of: CE 0.1431075632572174, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/198, Loss=0.21329016536474227
Loss made of: CE 0.20974771678447723, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/198, Loss=0.18680651038885115
Loss made of: CE 0.1262659728527069, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/198, Loss=0.18225466907024385
Loss made of: CE 0.16055023670196533, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/198, Loss=0.19063387662172318
Loss made of: CE 0.22508351504802704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/198, Loss=0.21718743145465852
Loss made of: CE 0.23622439801692963, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/198, Loss=0.21613450199365616
Loss made of: CE 0.2571374773979187, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/198, Loss=0.19704177230596542
Loss made of: CE 0.21276389062404633, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/198, Loss=0.19861938208341598
Loss made of: CE 0.1790655553340912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/198, Loss=0.19875292479991913
Loss made of: CE 0.2394007295370102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/198, Loss=0.1852606251835823
Loss made of: CE 0.1628040075302124, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/198, Loss=0.18599909991025926
Loss made of: CE 0.1388915777206421, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/198, Loss=0.20512572973966597
Loss made of: CE 0.23755469918251038, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/198, Loss=0.19505521059036254
Loss made of: CE 0.13801491260528564, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.19507381319999695, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.19507381319999695, Class Loss=0.19507381319999695, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/198, Loss=0.1781918540596962
Loss made of: CE 0.19286684691905975, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/198, Loss=0.1783569261431694
Loss made of: CE 0.14641600847244263, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/198, Loss=0.17841265648603438
Loss made of: CE 0.18134014308452606, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/198, Loss=0.1948217988014221
Loss made of: CE 0.13901546597480774, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/198, Loss=0.16915287226438522
Loss made of: CE 0.1588343232870102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/198, Loss=0.18994344919919967
Loss made of: CE 0.17689667642116547, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/198, Loss=0.16781322360038758
Loss made of: CE 0.14631859958171844, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/198, Loss=0.15813027918338776
Loss made of: CE 0.1925470530986786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/198, Loss=0.16380186527967452
Loss made of: CE 0.19680319726467133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/198, Loss=0.1536640226840973
Loss made of: CE 0.10865907371044159, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/198, Loss=0.1821421816945076
Loss made of: CE 0.203902006149292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/198, Loss=0.19622774422168732
Loss made of: CE 0.1624123454093933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/198, Loss=0.18692150563001633
Loss made of: CE 0.16119694709777832, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/198, Loss=0.19003861397504807
Loss made of: CE 0.19681131839752197, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/198, Loss=0.18238366693258284
Loss made of: CE 0.2168470025062561, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/198, Loss=0.1712133839726448
Loss made of: CE 0.13599227368831635, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/198, Loss=0.18908371329307555
Loss made of: CE 0.14879897236824036, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/198, Loss=0.15796901434659957
Loss made of: CE 0.15510274469852448, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/198, Loss=0.16962672472000123
Loss made of: CE 0.19825296103954315, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1761893332004547, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.1761893332004547, Class Loss=0.1761893332004547, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/157, Loss=1.8222456097602844
Loss made of: CE 1.5946029424667358, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=1.0384925723075866
Loss made of: CE 0.7878175973892212, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.782558661699295
Loss made of: CE 0.7732259631156921, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.607897835969925
Loss made of: CE 0.59024578332901, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.5672415435314179
Loss made of: CE 0.4299640357494354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.5585052132606506
Loss made of: CE 0.39017200469970703, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.5125642627477646
Loss made of: CE 0.4950188100337982, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.4618756115436554
Loss made of: CE 0.40704435110092163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.39992506206035616
Loss made of: CE 0.5969035029411316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.3950607568025589
Loss made of: CE 0.32705140113830566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.43194475769996643
Loss made of: CE 0.5246882438659668, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.45232571065425875
Loss made of: CE 0.36551615595817566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.40626058876514437
Loss made of: CE 0.428719699382782, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.4141018450260162
Loss made of: CE 0.38736793398857117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.375346302986145
Loss made of: CE 0.33686643838882446, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6058973073959351, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.6058973073959351, Class Loss=0.6058973073959351, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/157, Loss=0.30736576616764066
Loss made of: CE 0.26535534858703613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.36814701557159424
Loss made of: CE 0.29668694734573364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.3644476905465126
Loss made of: CE 0.3201921284198761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.3400861456990242
Loss made of: CE 0.2768648564815521, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.3163566619157791
Loss made of: CE 0.3227360248565674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.3168773740530014
Loss made of: CE 0.30868446826934814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.27530827671289443
Loss made of: CE 0.266151487827301, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.29153951108455656
Loss made of: CE 0.2674505114555359, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.30064920634031295
Loss made of: CE 0.2513362765312195, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.2820805624127388
Loss made of: CE 0.24270646274089813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.26625809222459795
Loss made of: CE 0.28177160024642944, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.2873518273234367
Loss made of: CE 0.3149031400680542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.3260909616947174
Loss made of: CE 0.36403316259384155, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.2827531397342682
Loss made of: CE 0.30228662490844727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.3462762743234634
Loss made of: CE 0.32664769887924194, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3134560286998749, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.3134560286998749, Class Loss=0.3134560286998749, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/157, Loss=0.25742848962545395
Loss made of: CE 0.29531383514404297, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.2394956797361374
Loss made of: CE 0.20314902067184448, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.24395503252744674
Loss made of: CE 0.28728485107421875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.2507055774331093
Loss made of: CE 0.3188720941543579, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.2657062545418739
Loss made of: CE 0.30179381370544434, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.24520350694656373
Loss made of: CE 0.23086300492286682, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.2757071599364281
Loss made of: CE 0.16526907682418823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.24353983700275422
Loss made of: CE 0.20019279420375824, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.22814580351114272
Loss made of: CE 0.28491705656051636, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.2329177141189575
Loss made of: CE 0.17782750725746155, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.2895498424768448
Loss made of: CE 0.2479555606842041, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.22091751247644426
Loss made of: CE 0.22665619850158691, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.2352679818868637
Loss made of: CE 0.2409748136997223, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.269315105676651
Loss made of: CE 0.2867995798587799, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.2133595272898674
Loss made of: CE 0.1744464933872223, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.24668598175048828, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.24668598175048828, Class Loss=0.24668598175048828, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/157, Loss=0.19865429401397705
Loss made of: CE 0.2297334372997284, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.234149369597435
Loss made of: CE 0.1420823633670807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.23748731315135957
Loss made of: CE 0.2530592381954193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.2060959368944168
Loss made of: CE 0.1764575093984604, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.21981555968523026
Loss made of: CE 0.3175964951515198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.20228726267814637
Loss made of: CE 0.25858351588249207, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.19183053374290465
Loss made of: CE 0.1613333374261856, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.209969662129879
Loss made of: CE 0.20702490210533142, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.20889477282762528
Loss made of: CE 0.1808982789516449, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.21376365125179292
Loss made of: CE 0.1868792027235031, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.20666387677192688
Loss made of: CE 0.1688663214445114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.16453682333230973
Loss made of: CE 0.13267992436885834, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.2032404586672783
Loss made of: CE 0.18989932537078857, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.21197854727506638
Loss made of: CE 0.14358749985694885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.20217205584049225
Loss made of: CE 0.3122931718826294, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.20681869983673096, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.20681869983673096, Class Loss=0.20681869983673096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/157, Loss=0.17163583636283875
Loss made of: CE 0.15674974024295807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.19708716869354248
Loss made of: CE 0.23198914527893066, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.20447123795747757
Loss made of: CE 0.3842512369155884, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.1689612463116646
Loss made of: CE 0.20140966773033142, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.19368691444396974
Loss made of: CE 0.1440478265285492, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.20911203026771547
Loss made of: CE 0.27778828144073486, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.1747846856713295
Loss made of: CE 0.1867060661315918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.1832514502108097
Loss made of: CE 0.1537960320711136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.17567740231752396
Loss made of: CE 0.2153128832578659, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.18662622421979905
Loss made of: CE 0.21956633031368256, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.2102501630783081
Loss made of: CE 0.19184529781341553, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.1863325744867325
Loss made of: CE 0.18629321455955505, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.17344334423542024
Loss made of: CE 0.16503778100013733, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.1730905905365944
Loss made of: CE 0.2179056704044342, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.19218629151582717
Loss made of: CE 0.18166157603263855, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.18639269471168518, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.18639269471168518, Class Loss=0.18639269471168518, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/157, Loss=0.18733950704336166
Loss made of: CE 0.15149658918380737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.1638594724237919
Loss made of: CE 0.14071789383888245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.17369312942028045
Loss made of: CE 0.21975964307785034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.14861394092440605
Loss made of: CE 0.12720206379890442, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.1581375628709793
Loss made of: CE 0.17226462066173553, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.15861610993742942
Loss made of: CE 0.12900285422801971, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.1539252743124962
Loss made of: CE 0.12828212976455688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.14275022223591805
Loss made of: CE 0.11977323144674301, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.15486807376146317
Loss made of: CE 0.15392693877220154, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.15538626089692115
Loss made of: CE 0.14698581397533417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.1455581769347191
Loss made of: CE 0.132993683218956, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.1794235125184059
Loss made of: CE 0.07906055450439453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.18279441222548484
Loss made of: CE 0.20209437608718872, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.1653319075703621
Loss made of: CE 0.18603193759918213, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.17617354393005372
Loss made of: CE 0.14383360743522644, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.162858784198761, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.162858784198761, Class Loss=0.162858784198761, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/205, Loss=1.6530284345149995
Loss made of: CE 1.1237244606018066, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/205, Loss=0.9842446982860565
Loss made of: CE 0.6704542636871338, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/205, Loss=0.7987784445285797
Loss made of: CE 0.6418031454086304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/205, Loss=0.7242554783821106
Loss made of: CE 0.7044026255607605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/205, Loss=0.6422622740268707
Loss made of: CE 0.6154153347015381, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/205, Loss=0.6233437716960907
Loss made of: CE 0.5657455325126648, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/205, Loss=0.567100641131401
Loss made of: CE 0.5791637897491455, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/205, Loss=0.5951478660106659
Loss made of: CE 0.6691330671310425, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/205, Loss=0.4583679407835007
Loss made of: CE 0.38853955268859863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/205, Loss=0.48915736079216005
Loss made of: CE 0.43416520953178406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/205, Loss=0.4893257588148117
Loss made of: CE 0.4222751557826996, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/205, Loss=0.5023457169532776
Loss made of: CE 0.5400238037109375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/205, Loss=0.5080754816532135
Loss made of: CE 0.38931339979171753, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/205, Loss=0.42431698441505433
Loss made of: CE 0.42601221799850464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/205, Loss=0.4402722179889679
Loss made of: CE 0.400664746761322, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/205, Loss=0.4095433592796326
Loss made of: CE 0.38355761766433716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/205, Loss=0.4052463173866272
Loss made of: CE 0.3242233991622925, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/205, Loss=0.37217831015586855
Loss made of: CE 0.3436305522918701, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/205, Loss=0.3913867622613907
Loss made of: CE 0.3620774447917938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/205, Loss=0.47824797332286834
Loss made of: CE 0.45801061391830444, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5931515693664551, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5931515693664551, Class Loss=0.5931515693664551, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/205, Loss=0.35579552948474885
Loss made of: CE 0.4496917128562927, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/205, Loss=0.38130798637866975
Loss made of: CE 0.38744187355041504, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/205, Loss=0.3513784945011139
Loss made of: CE 0.35162919759750366, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/205, Loss=0.30543553829193115
Loss made of: CE 0.3167152404785156, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/205, Loss=0.32352435439825056
Loss made of: CE 0.38756394386291504, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/205, Loss=0.35683503448963166
Loss made of: CE 0.40710777044296265, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/205, Loss=0.3563164442777634
Loss made of: CE 0.4297880530357361, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/205, Loss=0.29447475671768186
Loss made of: CE 0.27054160833358765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/205, Loss=0.32493079602718355
Loss made of: CE 0.37980467081069946, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/205, Loss=0.3791644275188446
Loss made of: CE 0.41942864656448364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/205, Loss=0.38264493048191073
Loss made of: CE 0.31092461943626404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/205, Loss=0.3622533768415451
Loss made of: CE 0.6520577669143677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/205, Loss=0.3212950587272644
Loss made of: CE 0.3726436495780945, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/205, Loss=0.30551466047763826
Loss made of: CE 0.28291207551956177, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/205, Loss=0.2990688309073448
Loss made of: CE 0.3177019953727722, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/205, Loss=0.33284522593021393
Loss made of: CE 0.26998379826545715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/205, Loss=0.29773343801498414
Loss made of: CE 0.37918418645858765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/205, Loss=0.28932880610227585
Loss made of: CE 0.29180729389190674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/205, Loss=0.2882017746567726
Loss made of: CE 0.33654388785362244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/205, Loss=0.3221510022878647
Loss made of: CE 0.23580226302146912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3307648003101349, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.3307648003101349, Class Loss=0.3307648003101349, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/205, Loss=0.26011573523283005
Loss made of: CE 0.2632620334625244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/205, Loss=0.24660468846559525
Loss made of: CE 0.16858166456222534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/205, Loss=0.27132742553949357
Loss made of: CE 0.22543823719024658, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/205, Loss=0.27724572867155073
Loss made of: CE 0.2589196562767029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/205, Loss=0.25412309616804124
Loss made of: CE 0.274229496717453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/205, Loss=0.26221824884414674
Loss made of: CE 0.23657312989234924, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/205, Loss=0.2748103067278862
Loss made of: CE 0.18367299437522888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/205, Loss=0.2785036310553551
Loss made of: CE 0.3145560920238495, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/205, Loss=0.25207672715187074
Loss made of: CE 0.2690994143486023, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/205, Loss=0.24636628478765488
Loss made of: CE 0.20478907227516174, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/205, Loss=0.27282912880182264
Loss made of: CE 0.3888281285762787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/205, Loss=0.26128952354192736
Loss made of: CE 0.2533567547798157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/205, Loss=0.2614236995577812
Loss made of: CE 0.27790284156799316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/205, Loss=0.28376677334308625
Loss made of: CE 0.2989274263381958, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/205, Loss=0.24446618258953096
Loss made of: CE 0.2492809295654297, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/205, Loss=0.2730628073215485
Loss made of: CE 0.2375960797071457, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/205, Loss=0.2610066160559654
Loss made of: CE 0.2393198013305664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/205, Loss=0.2482771545648575
Loss made of: CE 0.22216200828552246, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/205, Loss=0.2735281839966774
Loss made of: CE 0.36835575103759766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/205, Loss=0.2421504095196724
Loss made of: CE 0.30552154779434204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.260610431432724, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.260610431432724, Class Loss=0.260610431432724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/205, Loss=0.2406378999352455
Loss made of: CE 0.1968858540058136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/205, Loss=0.22275035083293915
Loss made of: CE 0.18541836738586426, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/205, Loss=0.2132470041513443
Loss made of: CE 0.15565763413906097, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/205, Loss=0.22168535441160203
Loss made of: CE 0.2661550045013428, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/205, Loss=0.24275168925523757
Loss made of: CE 0.15786400437355042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/205, Loss=0.22111063003540038
Loss made of: CE 0.23452743887901306, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/205, Loss=0.21137185543775558
Loss made of: CE 0.18502716720104218, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/205, Loss=0.22555572986602784
Loss made of: CE 0.2176276296377182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/205, Loss=0.24497219026088715
Loss made of: CE 0.18058764934539795, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/205, Loss=0.2168237864971161
Loss made of: CE 0.21495983004570007, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/205, Loss=0.23140835165977477
Loss made of: CE 0.22065940499305725, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/205, Loss=0.21823561638593675
Loss made of: CE 0.2408972680568695, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/205, Loss=0.2538678675889969
Loss made of: CE 0.308450847864151, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/205, Loss=0.2341161847114563
Loss made of: CE 0.2644512355327606, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/205, Loss=0.23577677309513093
Loss made of: CE 0.21465246379375458, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/205, Loss=0.24442220181226731
Loss made of: CE 0.20457372069358826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/205, Loss=0.20555244833230973
Loss made of: CE 0.18401746451854706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/205, Loss=0.2525815322995186
Loss made of: CE 0.1964515894651413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/205, Loss=0.22500114589929582
Loss made of: CE 0.31296998262405396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/205, Loss=0.2074238896369934
Loss made of: CE 0.2090967893600464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.22845016419887543, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.22845016419887543, Class Loss=0.22845016419887543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/205, Loss=0.21293947249650955
Loss made of: CE 0.31920674443244934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/205, Loss=0.20796189159154893
Loss made of: CE 0.19330547749996185, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/205, Loss=0.22809630781412124
Loss made of: CE 0.2120814025402069, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/205, Loss=0.19144757390022277
Loss made of: CE 0.15311014652252197, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/205, Loss=0.17649426460266113
Loss made of: CE 0.16641832888126373, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/205, Loss=0.20340600460767747
Loss made of: CE 0.18199776113033295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/205, Loss=0.1908208668231964
Loss made of: CE 0.16186025738716125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/205, Loss=0.1628200426697731
Loss made of: CE 0.14809349179267883, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/205, Loss=0.19809966087341307
Loss made of: CE 0.19118639826774597, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/205, Loss=0.1851317062973976
Loss made of: CE 0.1957559585571289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/205, Loss=0.18866583108901977
Loss made of: CE 0.2230442613363266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/205, Loss=0.20672611594200135
Loss made of: CE 0.23398320376873016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/205, Loss=0.1842637225985527
Loss made of: CE 0.18881134688854218, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/205, Loss=0.1938943423330784
Loss made of: CE 0.21865449845790863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/205, Loss=0.19623994082212448
Loss made of: CE 0.15829788148403168, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/205, Loss=0.19545358717441558
Loss made of: CE 0.1433318853378296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/205, Loss=0.19120501577854157
Loss made of: CE 0.16342583298683167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/205, Loss=0.20192355662584305
Loss made of: CE 0.16813115775585175, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/205, Loss=0.21888547390699387
Loss made of: CE 0.17465488612651825, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/205, Loss=0.1975601553916931
Loss made of: CE 0.12484264373779297, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.19627255201339722, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.19627255201339722, Class Loss=0.19627255201339722, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/205, Loss=0.15946762040257453
Loss made of: CE 0.16881625354290009, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/205, Loss=0.1734791412949562
Loss made of: CE 0.14691966772079468, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/205, Loss=0.1988445296883583
Loss made of: CE 0.21900562942028046, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/205, Loss=0.18549036383628845
Loss made of: CE 0.2900150418281555, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/205, Loss=0.18178326338529588
Loss made of: CE 0.17098447680473328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/205, Loss=0.1937008887529373
Loss made of: CE 0.11908406019210815, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/205, Loss=0.20941275954246522
Loss made of: CE 0.2521221935749054, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/205, Loss=0.17858095616102218
Loss made of: CE 0.18444907665252686, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/205, Loss=0.16769986152648925
Loss made of: CE 0.17422346770763397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/205, Loss=0.18495049178600312
Loss made of: CE 0.1711580604314804, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/205, Loss=0.1585199438035488
Loss made of: CE 0.16715949773788452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/205, Loss=0.16784238815307617
Loss made of: CE 0.19707117974758148, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/205, Loss=0.18901838213205338
Loss made of: CE 0.19771459698677063, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/205, Loss=0.1781454548239708
Loss made of: CE 0.11745402216911316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/205, Loss=0.16347957253456116
Loss made of: CE 0.14122125506401062, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/205, Loss=0.14654485359787942
Loss made of: CE 0.12904487550258636, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/205, Loss=0.17375214099884034
Loss made of: CE 0.14370493590831757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/205, Loss=0.1700240895152092
Loss made of: CE 0.13380728662014008, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/205, Loss=0.1827831372618675
Loss made of: CE 0.13725708425045013, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/205, Loss=0.18023595958948135
Loss made of: CE 0.17623266577720642, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.17816336452960968, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.17816336452960968, Class Loss=0.17816336452960968, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/200, Loss=1.7427697420120238
Loss made of: CE 1.2266781330108643, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/200, Loss=0.9457795917987823
Loss made of: CE 0.7607397437095642, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/200, Loss=0.769982385635376
Loss made of: CE 0.6084710359573364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/200, Loss=0.6752544611692428
Loss made of: CE 0.4867341220378876, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/200, Loss=0.6269531071186065
Loss made of: CE 0.6480826139450073, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/200, Loss=0.5564530998468399
Loss made of: CE 0.4582066237926483, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/200, Loss=0.5538380652666092
Loss made of: CE 0.3602450489997864, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/200, Loss=0.51283999979496
Loss made of: CE 0.4221840798854828, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/200, Loss=0.5031124174594879
Loss made of: CE 0.6176394820213318, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/200, Loss=0.4484620362520218
Loss made of: CE 0.4312356114387512, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/200, Loss=0.4577352374792099
Loss made of: CE 0.45538100600242615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/200, Loss=0.4393111109733582
Loss made of: CE 0.34349536895751953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/200, Loss=0.4391899436712265
Loss made of: CE 0.5153089761734009, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/200, Loss=0.37477971613407135
Loss made of: CE 0.3351104259490967, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/200, Loss=0.3998678743839264
Loss made of: CE 0.38224348425865173, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/200, Loss=0.4498398840427399
Loss made of: CE 0.5245254039764404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/200, Loss=0.4451961010694504
Loss made of: CE 0.4158802926540375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/200, Loss=0.42987261414527894
Loss made of: CE 0.3155508041381836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/200, Loss=0.34708358645439147
Loss made of: CE 0.30916261672973633, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/200, Loss=0.37503168284893035
Loss made of: CE 0.3568163514137268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5746676325798035, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.5746676325798035, Class Loss=0.5746676325798035, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/200, Loss=0.35620211660861967
Loss made of: CE 0.2975643575191498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/200, Loss=0.31630982756614684
Loss made of: CE 0.3160080909729004, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/200, Loss=0.29394119679927827
Loss made of: CE 0.3979582190513611, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/200, Loss=0.3241732269525528
Loss made of: CE 0.3466000556945801, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/200, Loss=0.3357511803507805
Loss made of: CE 0.3908073306083679, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/200, Loss=0.3418062388896942
Loss made of: CE 0.2537420392036438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/200, Loss=0.31129154860973357
Loss made of: CE 0.18610110878944397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/200, Loss=0.3120601952075958
Loss made of: CE 0.377315491437912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/200, Loss=0.2968305140733719
Loss made of: CE 0.34535878896713257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/200, Loss=0.2887743219733238
Loss made of: CE 0.3108024001121521, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/200, Loss=0.29599963277578356
Loss made of: CE 0.2548025846481323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/200, Loss=0.3167256101965904
Loss made of: CE 0.2946637272834778, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/200, Loss=0.3112553387880325
Loss made of: CE 0.25816810131073, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/200, Loss=0.3559999719262123
Loss made of: CE 0.41762757301330566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/200, Loss=0.34892573654651643
Loss made of: CE 0.26582297682762146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/200, Loss=0.3299351274967194
Loss made of: CE 0.3191981315612793, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/200, Loss=0.3367242977023125
Loss made of: CE 0.41362592577934265, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/200, Loss=0.30161117017269135
Loss made of: CE 0.4270033836364746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/200, Loss=0.32256471216678617
Loss made of: CE 0.33153554797172546, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/200, Loss=0.31585378050804136
Loss made of: CE 0.3984057605266571, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3206367790699005, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.3206367790699005, Class Loss=0.3206367790699005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/200, Loss=0.25327680855989454
Loss made of: CE 0.28004762530326843, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/200, Loss=0.3000910937786102
Loss made of: CE 0.4050154685974121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/200, Loss=0.26247967928647997
Loss made of: CE 0.22251245379447937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/200, Loss=0.25728475749492646
Loss made of: CE 0.2375430315732956, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/200, Loss=0.2539379373192787
Loss made of: CE 0.22352385520935059, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/200, Loss=0.2573472365736961
Loss made of: CE 0.2508877217769623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/200, Loss=0.24065360873937608
Loss made of: CE 0.2962788939476013, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/200, Loss=0.24674026072025299
Loss made of: CE 0.25632917881011963, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/200, Loss=0.2643833622336388
Loss made of: CE 0.1499263495206833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/200, Loss=0.2617752432823181
Loss made of: CE 0.3203558623790741, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/200, Loss=0.24456629604101182
Loss made of: CE 0.25916367769241333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/200, Loss=0.251892626285553
Loss made of: CE 0.25649404525756836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/200, Loss=0.25096389800310137
Loss made of: CE 0.28849223256111145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/200, Loss=0.2732632264494896
Loss made of: CE 0.2901296317577362, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/200, Loss=0.25402705520391466
Loss made of: CE 0.2245291918516159, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/200, Loss=0.27778004109859467
Loss made of: CE 0.26383399963378906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/200, Loss=0.2316899672150612
Loss made of: CE 0.2490595281124115, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/200, Loss=0.25283742398023606
Loss made of: CE 0.23885218799114227, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/200, Loss=0.25454591810703275
Loss made of: CE 0.2484530508518219, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/200, Loss=0.25696021020412446
Loss made of: CE 0.22149240970611572, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.25732481479644775, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.25732481479644775, Class Loss=0.25732481479644775, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/200, Loss=0.2285495162010193
Loss made of: CE 0.23442310094833374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/200, Loss=0.19240019768476485
Loss made of: CE 0.2702968120574951, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/200, Loss=0.22304999977350234
Loss made of: CE 0.2127288579940796, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/200, Loss=0.22749973088502884
Loss made of: CE 0.2012377679347992, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/200, Loss=0.23012978285551072
Loss made of: CE 0.25339436531066895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/200, Loss=0.20409645289182662
Loss made of: CE 0.19726523756980896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/200, Loss=0.23678969591856003
Loss made of: CE 0.24997049570083618, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/200, Loss=0.20341093838214874
Loss made of: CE 0.22632953524589539, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/200, Loss=0.2302835166454315
Loss made of: CE 0.2069755494594574, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/200, Loss=0.2160433053970337
Loss made of: CE 0.206750750541687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/200, Loss=0.20784882754087447
Loss made of: CE 0.22617051005363464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/200, Loss=0.2123086765408516
Loss made of: CE 0.17910239100456238, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/200, Loss=0.19977775514125823
Loss made of: CE 0.17004486918449402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/200, Loss=0.21060921549797057
Loss made of: CE 0.18046002089977264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/200, Loss=0.21757364571094512
Loss made of: CE 0.1735159158706665, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/200, Loss=0.22471915781497956
Loss made of: CE 0.1806066483259201, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/200, Loss=0.19962845295667647
Loss made of: CE 0.17690539360046387, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/200, Loss=0.23717087358236313
Loss made of: CE 0.28995323181152344, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/200, Loss=0.21410656124353408
Loss made of: CE 0.17902207374572754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/200, Loss=0.19071491956710815
Loss made of: CE 0.2277110069990158, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.21533554792404175, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.21533554792404175, Class Loss=0.21533554792404175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/200, Loss=0.19646599292755126
Loss made of: CE 0.1979769915342331, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/200, Loss=0.19902317821979523
Loss made of: CE 0.20717541873455048, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/200, Loss=0.1912044271826744
Loss made of: CE 0.2666015625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/200, Loss=0.1890002653002739
Loss made of: CE 0.1885422170162201, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/200, Loss=0.1919767901301384
Loss made of: CE 0.26768800616264343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/200, Loss=0.1745983451604843
Loss made of: CE 0.15140533447265625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/200, Loss=0.1990273952484131
Loss made of: CE 0.15693692862987518, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/200, Loss=0.20295379757881166
Loss made of: CE 0.13935290277004242, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/200, Loss=0.1993173986673355
Loss made of: CE 0.1507442444562912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/200, Loss=0.2070865511894226
Loss made of: CE 0.22254452109336853, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/200, Loss=0.17593978345394135
Loss made of: CE 0.18002796173095703, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/200, Loss=0.183501672744751
Loss made of: CE 0.16000975668430328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/200, Loss=0.17222848385572434
Loss made of: CE 0.19909320771694183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/200, Loss=0.21258146464824676
Loss made of: CE 0.21477556228637695, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/200, Loss=0.21801881045103072
Loss made of: CE 0.20863954722881317, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/200, Loss=0.1954021245241165
Loss made of: CE 0.17312884330749512, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/200, Loss=0.19276473224163054
Loss made of: CE 0.18318313360214233, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/200, Loss=0.18483166098594667
Loss made of: CE 0.1592489331960678, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/200, Loss=0.20918295979499818
Loss made of: CE 0.19424760341644287, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/200, Loss=0.18397662341594695
Loss made of: CE 0.1544485092163086, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.19395412504673004, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.19395412504673004, Class Loss=0.19395412504673004, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/200, Loss=0.17794255763292313
Loss made of: CE 0.2018188089132309, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/200, Loss=0.17472147047519684
Loss made of: CE 0.16811761260032654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/200, Loss=0.17264266759157182
Loss made of: CE 0.20771273970603943, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/200, Loss=0.16246208250522615
Loss made of: CE 0.18003496527671814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/200, Loss=0.15970151275396346
Loss made of: CE 0.16241616010665894, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/200, Loss=0.16928192228078842
Loss made of: CE 0.15765415132045746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/200, Loss=0.1532256156206131
Loss made of: CE 0.13506031036376953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/200, Loss=0.20314520448446274
Loss made of: CE 0.2371964454650879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/200, Loss=0.16561778485774994
Loss made of: CE 0.15749621391296387, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/200, Loss=0.1762054219841957
Loss made of: CE 0.18215391039848328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/200, Loss=0.17405378073453903
Loss made of: CE 0.1819457709789276, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/200, Loss=0.17515462785959243
Loss made of: CE 0.15032170712947845, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/200, Loss=0.19113415330648423
Loss made of: CE 0.1816500425338745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/200, Loss=0.17455825656652452
Loss made of: CE 0.22457031905651093, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/200, Loss=0.18495058119297028
Loss made of: CE 0.16094796359539032, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/200, Loss=0.1794370800256729
Loss made of: CE 0.12834781408309937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/200, Loss=0.17274579480290414
Loss made of: CE 0.18826471269130707, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/200, Loss=0.1601518914103508
Loss made of: CE 0.24209213256835938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/200, Loss=0.17993796318769456
Loss made of: CE 0.17562389373779297, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/200, Loss=0.1860267624258995
Loss made of: CE 0.20728909969329834, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.17465485632419586, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.17465485632419586, Class Loss=0.17465485632419586, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.22700235247612, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.919141
Mean Acc: 0.735732
FreqW Acc: 0.855635
Mean IoU: 0.633535
Class IoU:
	class 0: 0.9209057
	class 1: 0.8248675
	class 2: 0.39608732
	class 3: 1.1825521e-05
	class 4: 0.64782625
	class 5: 0.71294814
	class 6: 0.92755437
	class 7: 0.8905905
	class 8: 0.8409963
	class 9: 0.32966825
	class 10: 0.10552172
	class 11: 0.55598825
	class 12: 0.75839853
	class 13: 0.5277828
	class 14: 0.85425013
	class 15: 0.84315854
Class Acc:
	class 0: 0.97288984
	class 1: 0.8658228
	class 2: 0.85237825
	class 3: 1.1825521e-05
	class 4: 0.72514445
	class 5: 0.7899078
	class 6: 0.9663803
	class 7: 0.92505306
	class 8: 0.891113
	class 9: 0.37883204
	class 10: 0.10564758
	class 11: 0.58263516
	class 12: 0.9345754
	class 13: 0.9082201
	class 14: 0.94324106
	class 15: 0.929861

federated global round: 1, step: 0
select part of clients to conduct local training
[1, 7, 2, 9]
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/164, Loss=0.21862196773290635
Loss made of: CE 0.21801601350307465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/164, Loss=0.22763398736715318
Loss made of: CE 0.1225147694349289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/164, Loss=0.24038459807634355
Loss made of: CE 0.20568899810314178, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/164, Loss=0.26673552840948106
Loss made of: CE 0.16464774310588837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/164, Loss=0.22714595645666122
Loss made of: CE 0.30188310146331787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/164, Loss=0.21139391660690307
Loss made of: CE 0.19735483825206757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/164, Loss=0.2032899335026741
Loss made of: CE 0.17529499530792236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/164, Loss=0.22812971323728562
Loss made of: CE 0.2276376336812973, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/164, Loss=0.23340326249599458
Loss made of: CE 0.2016439437866211, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/164, Loss=0.22056282609701156
Loss made of: CE 0.2144739031791687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/164, Loss=0.23272821456193923
Loss made of: CE 0.22119516134262085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/164, Loss=0.2638502553105354
Loss made of: CE 0.23261600732803345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/164, Loss=0.20643966197967528
Loss made of: CE 0.1483040153980255, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/164, Loss=0.24877044111490249
Loss made of: CE 0.2417481392621994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/164, Loss=0.20800199657678603
Loss made of: CE 0.1997905969619751, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/164, Loss=0.23650473952293397
Loss made of: CE 0.27408093214035034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2286105453968048, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.2286105453968048, Class Loss=0.2286105453968048, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/164, Loss=0.18219227492809295
Loss made of: CE 0.20738670229911804, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/164, Loss=0.20727269798517228
Loss made of: CE 0.2982882559299469, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/164, Loss=0.20719677358865737
Loss made of: CE 0.2385951727628708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/164, Loss=0.21078690141439438
Loss made of: CE 0.17746862769126892, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/164, Loss=0.17604876458644866
Loss made of: CE 0.1864856779575348, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/164, Loss=0.20194084644317628
Loss made of: CE 0.17705798149108887, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/164, Loss=0.18238871693611144
Loss made of: CE 0.14014068245887756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/164, Loss=0.19983469247817992
Loss made of: CE 0.14337517321109772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/164, Loss=0.19021878242492676
Loss made of: CE 0.18505045771598816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/164, Loss=0.19590777158737183
Loss made of: CE 0.16445109248161316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/164, Loss=0.17617713958024978
Loss made of: CE 0.21275708079338074, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/164, Loss=0.21015654355287552
Loss made of: CE 0.2615213990211487, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/164, Loss=0.21922203600406648
Loss made of: CE 0.22588026523590088, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/164, Loss=0.1718670278787613
Loss made of: CE 0.13583211600780487, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/164, Loss=0.206808640062809
Loss made of: CE 0.21235530078411102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/164, Loss=0.19371706396341323
Loss made of: CE 0.1511855274438858, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.19595380127429962, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.19595380127429962, Class Loss=0.19595380127429962, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/164, Loss=0.18393724262714387
Loss made of: CE 0.1426853984594345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/164, Loss=0.19856775552034378
Loss made of: CE 0.1770639270544052, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/164, Loss=0.18715887516736984
Loss made of: CE 0.3030904531478882, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/164, Loss=0.15938455387949943
Loss made of: CE 0.19396065175533295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/164, Loss=0.17124280482530593
Loss made of: CE 0.15489234030246735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/164, Loss=0.17054742574691772
Loss made of: CE 0.18449907004833221, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/164, Loss=0.16831169575452803
Loss made of: CE 0.20711950957775116, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/164, Loss=0.18007844984531401
Loss made of: CE 0.20242521166801453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/164, Loss=0.1803465738892555
Loss made of: CE 0.13474051654338837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/164, Loss=0.16903600469231606
Loss made of: CE 0.1785016655921936, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/164, Loss=0.1647843338549137
Loss made of: CE 0.13918745517730713, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/164, Loss=0.17018545120954515
Loss made of: CE 0.17644047737121582, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/164, Loss=0.18102847337722777
Loss made of: CE 0.18156227469444275, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/164, Loss=0.17625333815813066
Loss made of: CE 0.18361404538154602, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/164, Loss=0.14869373589754104
Loss made of: CE 0.12962208688259125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/164, Loss=0.16876333504915236
Loss made of: CE 0.15008792281150818, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1733376681804657, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.1733376681804657, Class Loss=0.1733376681804657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/164, Loss=0.14467695653438567
Loss made of: CE 0.1636396050453186, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/164, Loss=0.16669510006904603
Loss made of: CE 0.12405413389205933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/164, Loss=0.13727387934923171
Loss made of: CE 0.15693950653076172, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/164, Loss=0.1492077149450779
Loss made of: CE 0.16350147128105164, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/164, Loss=0.15580796748399733
Loss made of: CE 0.17951253056526184, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/164, Loss=0.14282763451337815
Loss made of: CE 0.11584831774234772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/164, Loss=0.15667688548564912
Loss made of: CE 0.15271875262260437, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/164, Loss=0.16547580882906915
Loss made of: CE 0.18277567625045776, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/164, Loss=0.14892452955245972
Loss made of: CE 0.12166349589824677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/164, Loss=0.14563871175050735
Loss made of: CE 0.15323351323604584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/164, Loss=0.13115834221243858
Loss made of: CE 0.1738886833190918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/164, Loss=0.1444941908121109
Loss made of: CE 0.24913424253463745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/164, Loss=0.14460280388593674
Loss made of: CE 0.16206054389476776, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/164, Loss=0.14959910809993743
Loss made of: CE 0.12770335376262665, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/164, Loss=0.14647304639220238
Loss made of: CE 0.1100483387708664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/164, Loss=0.15221073403954505
Loss made of: CE 0.1436322033405304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1488335132598877, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.1488335132598877, Class Loss=0.1488335132598877, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/164, Loss=0.13629273772239686
Loss made of: CE 0.14982163906097412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/164, Loss=0.1475107640028
Loss made of: CE 0.19095772504806519, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/164, Loss=0.14739815145730972
Loss made of: CE 0.1707809567451477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/164, Loss=0.12632616534829139
Loss made of: CE 0.15986225008964539, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/164, Loss=0.13207993954420089
Loss made of: CE 0.09730961173772812, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/164, Loss=0.1386975921690464
Loss made of: CE 0.1413785219192505, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/164, Loss=0.13085986971855162
Loss made of: CE 0.15543341636657715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/164, Loss=0.12235298156738281
Loss made of: CE 0.11976578831672668, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/164, Loss=0.13398996070027352
Loss made of: CE 0.11079951375722885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/164, Loss=0.1257808655500412
Loss made of: CE 0.1244504377245903, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/164, Loss=0.1518559753894806
Loss made of: CE 0.1475057452917099, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/164, Loss=0.1445628896355629
Loss made of: CE 0.1663820743560791, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/164, Loss=0.13948982954025269
Loss made of: CE 0.11989925801753998, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/164, Loss=0.13095274120569228
Loss made of: CE 0.1232696995139122, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/164, Loss=0.1389326810836792
Loss made of: CE 0.13071990013122559, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/164, Loss=0.11624887511134148
Loss made of: CE 0.112723708152771, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.13531537353992462, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.13531537353992462, Class Loss=0.13531537353992462, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/164, Loss=0.11366437822580337
Loss made of: CE 0.10127659887075424, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/164, Loss=0.13434130400419236
Loss made of: CE 0.1116727814078331, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/164, Loss=0.1278551697731018
Loss made of: CE 0.11360349506139755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/164, Loss=0.12118742614984512
Loss made of: CE 0.14204028248786926, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/164, Loss=0.1254454992711544
Loss made of: CE 0.13787972927093506, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/164, Loss=0.12473054975271225
Loss made of: CE 0.15761546790599823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/164, Loss=0.12432466447353363
Loss made of: CE 0.08167652785778046, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/164, Loss=0.13621833324432372
Loss made of: CE 0.13356177508831024, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/164, Loss=0.12139794752001762
Loss made of: CE 0.09983959794044495, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/164, Loss=0.11786649972200394
Loss made of: CE 0.11300387978553772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/164, Loss=0.12859196737408637
Loss made of: CE 0.12563803791999817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/164, Loss=0.12778037637472153
Loss made of: CE 0.09214520454406738, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/164, Loss=0.12989185005426407
Loss made of: CE 0.16146276891231537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/164, Loss=0.12621726468205452
Loss made of: CE 0.10790230333805084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/164, Loss=0.13085630536079407
Loss made of: CE 0.11604402959346771, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/164, Loss=0.1297701321542263
Loss made of: CE 0.12408183515071869, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.12660549581050873, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.12660549581050873, Class Loss=0.12660549581050873, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/157, Loss=0.27568734288215635
Loss made of: CE 0.23929846286773682, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=0.22933228462934493
Loss made of: CE 0.1546100676059723, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.20752138495445252
Loss made of: CE 0.26374363899230957, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.1648830369114876
Loss made of: CE 0.1723947376012802, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.1891443118453026
Loss made of: CE 0.150933638215065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.2046639785170555
Loss made of: CE 0.19366328418254852, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.20189761817455293
Loss made of: CE 0.2048346847295761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.19284982085227967
Loss made of: CE 0.19955651462078094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.18736604899168013
Loss made of: CE 0.26566487550735474, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.19440162181854248
Loss made of: CE 0.16759538650512695, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.18647530674934387
Loss made of: CE 0.2251019924879074, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.20253449231386184
Loss made of: CE 0.17430774867534637, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.1812928006052971
Loss made of: CE 0.20427030324935913, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.21125205755233764
Loss made of: CE 0.26684725284576416, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.1832519993185997
Loss made of: CE 0.1475253701210022, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.201036736369133, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.201036736369133, Class Loss=0.201036736369133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/157, Loss=0.14438418447971343
Loss made of: CE 0.15718190371990204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.16798238232731819
Loss made of: CE 0.15314540266990662, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.1784406691789627
Loss made of: CE 0.17331665754318237, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.17722936123609542
Loss made of: CE 0.17814040184020996, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.15142450407147406
Loss made of: CE 0.12755119800567627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.16577739492058755
Loss made of: CE 0.20126357674598694, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.15573745891451835
Loss made of: CE 0.1222286969423294, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.1537417784333229
Loss made of: CE 0.11469471454620361, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.15860369354486464
Loss made of: CE 0.17112693190574646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.15855453088879584
Loss made of: CE 0.1385561227798462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.13862100765109062
Loss made of: CE 0.20427869260311127, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.15130732506513594
Loss made of: CE 0.17702102661132812, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.1498126968741417
Loss made of: CE 0.15532144904136658, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.13610870763659477
Loss made of: CE 0.11580514162778854, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.1884598210453987
Loss made of: CE 0.1744430512189865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.15925182402133942, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.15925182402133942, Class Loss=0.15925182402133942, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/157, Loss=0.13553487434983252
Loss made of: CE 0.1652078628540039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.1384465627372265
Loss made of: CE 0.11085917800664902, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.13229381144046784
Loss made of: CE 0.1325216293334961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.13454643338918687
Loss made of: CE 0.1331588625907898, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.15119560733437537
Loss made of: CE 0.185622438788414, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.1367444410920143
Loss made of: CE 0.14361615478992462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.1523858904838562
Loss made of: CE 0.1137094795703888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.14617668315768242
Loss made of: CE 0.12112488597631454, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.12892334461212157
Loss made of: CE 0.14615114033222198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.14012175649404526
Loss made of: CE 0.11934298276901245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.15615717619657515
Loss made of: CE 0.1654241681098938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.12656250670552255
Loss made of: CE 0.15112049877643585, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.1402721107006073
Loss made of: CE 0.1508921980857849, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.1676322840154171
Loss made of: CE 0.2041175812482834, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.1299649566411972
Loss made of: CE 0.11215512454509735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1411348134279251, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.1411348134279251, Class Loss=0.1411348134279251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/157, Loss=0.12537585198879242
Loss made of: CE 0.14937882125377655, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.13902614414691924
Loss made of: CE 0.07866968214511871, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.1439221739768982
Loss made of: CE 0.1920003592967987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.12256687730550767
Loss made of: CE 0.11619383096694946, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.12771069556474685
Loss made of: CE 0.13099569082260132, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.12881140410900116
Loss made of: CE 0.15610888600349426, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.12393241226673127
Loss made of: CE 0.1343420445919037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.13260655701160431
Loss made of: CE 0.1361980438232422, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.1265214942395687
Loss made of: CE 0.11465680599212646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.14119466990232468
Loss made of: CE 0.15478716790676117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.13916849344968796
Loss made of: CE 0.12769700586795807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.11114053800702095
Loss made of: CE 0.09522005915641785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.12915244325995445
Loss made of: CE 0.11788751929998398, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.13569122701883315
Loss made of: CE 0.08848017454147339, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.12538527697324753
Loss made of: CE 0.14438501000404358, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.13071289658546448, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.13071289658546448, Class Loss=0.13071289658546448, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/157, Loss=0.13975703939795495
Loss made of: CE 0.21110275387763977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.13453076928853988
Loss made of: CE 0.1486433744430542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.14857104495167733
Loss made of: CE 0.2807656228542328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.12771595939993857
Loss made of: CE 0.14614830911159515, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.11811556443572044
Loss made of: CE 0.09654319286346436, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.125719553232193
Loss made of: CE 0.14043794572353363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.1122864343225956
Loss made of: CE 0.09328871965408325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.12474553287029266
Loss made of: CE 0.1070624440908432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.1226925253868103
Loss made of: CE 0.10534026473760605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.1212863102555275
Loss made of: CE 0.1196235865354538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.12394488975405693
Loss made of: CE 0.09657057374715805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.12675093933939935
Loss made of: CE 0.13172563910484314, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.13254375085234643
Loss made of: CE 0.15464666485786438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.11878902465105057
Loss made of: CE 0.13320082426071167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.14044717624783515
Loss made of: CE 0.1089945062994957, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.12810572981834412, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.12810572981834412, Class Loss=0.12810572981834412, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/157, Loss=0.12471671849489212
Loss made of: CE 0.09960898011922836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.11803958341479301
Loss made of: CE 0.12691684067249298, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.11018426492810249
Loss made of: CE 0.16425107419490814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.10615507513284683
Loss made of: CE 0.09509003162384033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.11261855959892272
Loss made of: CE 0.13410653173923492, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.10751776322722435
Loss made of: CE 0.09327980130910873, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.11550203785300255
Loss made of: CE 0.09718573093414307, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.09834693297743798
Loss made of: CE 0.0937165841460228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.11685180738568306
Loss made of: CE 0.106185182929039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.11883124485611915
Loss made of: CE 0.09590177983045578, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.10310190320014953
Loss made of: CE 0.09448561072349548, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.12111042961478233
Loss made of: CE 0.0703141987323761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.11453563794493675
Loss made of: CE 0.1626298576593399, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.1184929296374321
Loss made of: CE 0.11802862584590912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.11632947400212287
Loss made of: CE 0.1095975786447525, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11383619159460068, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.11383619159460068, Class Loss=0.11383619159460068, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/200, Loss=0.2398915633559227
Loss made of: CE 0.2876468300819397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/200, Loss=0.18853863179683686
Loss made of: CE 0.13035748898983002, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/200, Loss=0.19303707182407379
Loss made of: CE 0.15332770347595215, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/200, Loss=0.19456211030483245
Loss made of: CE 0.1587442010641098, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/200, Loss=0.1819068342447281
Loss made of: CE 0.20674070715904236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/200, Loss=0.1836605191230774
Loss made of: CE 0.18401150405406952, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/200, Loss=0.20283795446157454
Loss made of: CE 0.14461296796798706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/200, Loss=0.19157864153385162
Loss made of: CE 0.16392450034618378, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/200, Loss=0.18119544982910157
Loss made of: CE 0.19846205413341522, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/200, Loss=0.17356052547693251
Loss made of: CE 0.16266493499279022, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/200, Loss=0.20814636647701262
Loss made of: CE 0.22530822455883026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/200, Loss=0.2144351840019226
Loss made of: CE 0.20139479637145996, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/200, Loss=0.18619046956300736
Loss made of: CE 0.22664664685726166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/200, Loss=0.1806845024228096
Loss made of: CE 0.17309117317199707, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/200, Loss=0.20969635546207427
Loss made of: CE 0.21244490146636963, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/200, Loss=0.21602283418178558
Loss made of: CE 0.2579733729362488, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/200, Loss=0.22261178493499756
Loss made of: CE 0.17920923233032227, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/200, Loss=0.19982602894306184
Loss made of: CE 0.15827709436416626, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/200, Loss=0.18330363631248475
Loss made of: CE 0.1607530564069748, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/200, Loss=0.20063927620649338
Loss made of: CE 0.1988256424665451, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.19761629402637482, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.19761629402637482, Class Loss=0.19761629402637482, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/200, Loss=0.18201402574777603
Loss made of: CE 0.16805367171764374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/200, Loss=0.16525061950087547
Loss made of: CE 0.15211403369903564, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/200, Loss=0.15758350417017936
Loss made of: CE 0.2310926616191864, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/200, Loss=0.17214793860912322
Loss made of: CE 0.15672825276851654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/200, Loss=0.18291859924793244
Loss made of: CE 0.22847482562065125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/200, Loss=0.17012789845466614
Loss made of: CE 0.12912116944789886, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/200, Loss=0.1606668785214424
Loss made of: CE 0.11626138538122177, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/200, Loss=0.19181966334581374
Loss made of: CE 0.21750810742378235, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/200, Loss=0.1485813930630684
Loss made of: CE 0.21830585598945618, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/200, Loss=0.15979683250188828
Loss made of: CE 0.17489346861839294, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/200, Loss=0.17488846331834793
Loss made of: CE 0.15931877493858337, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/200, Loss=0.1634382948279381
Loss made of: CE 0.18732449412345886, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/200, Loss=0.17569437026977539
Loss made of: CE 0.14069384336471558, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/200, Loss=0.19162013828754426
Loss made of: CE 0.21588534116744995, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/200, Loss=0.18446055203676223
Loss made of: CE 0.138680100440979, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/200, Loss=0.1895153746008873
Loss made of: CE 0.18212178349494934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/200, Loss=0.17854830622673035
Loss made of: CE 0.1964048594236374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/200, Loss=0.1791170820593834
Loss made of: CE 0.25273361802101135, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/200, Loss=0.20863644182682037
Loss made of: CE 0.22777508199214935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/200, Loss=0.19214763790369033
Loss made of: CE 0.1927264928817749, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.17644870281219482, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.17644870281219482, Class Loss=0.17644870281219482, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/200, Loss=0.1508145362138748
Loss made of: CE 0.13224062323570251, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/200, Loss=0.1774823322892189
Loss made of: CE 0.2353108525276184, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/200, Loss=0.15953400433063508
Loss made of: CE 0.1465168297290802, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/200, Loss=0.16101571470499038
Loss made of: CE 0.15081360936164856, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/200, Loss=0.14987512528896332
Loss made of: CE 0.15625523030757904, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/200, Loss=0.1745939388871193
Loss made of: CE 0.21485364437103271, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/200, Loss=0.1556207537651062
Loss made of: CE 0.15713344514369965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/200, Loss=0.15119608193635942
Loss made of: CE 0.13830193877220154, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/200, Loss=0.15553296804428102
Loss made of: CE 0.10383527725934982, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/200, Loss=0.1515421077609062
Loss made of: CE 0.16935591399669647, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/200, Loss=0.13853169307112695
Loss made of: CE 0.13314078748226166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/200, Loss=0.16346743032336236
Loss made of: CE 0.1484053134918213, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/200, Loss=0.15958341509103774
Loss made of: CE 0.15828895568847656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/200, Loss=0.16897364109754562
Loss made of: CE 0.18734979629516602, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/200, Loss=0.1607203871011734
Loss made of: CE 0.14445611834526062, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/200, Loss=0.183188147097826
Loss made of: CE 0.15114447474479675, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/200, Loss=0.15169942304491996
Loss made of: CE 0.1601250320672989, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/200, Loss=0.15790909081697463
Loss made of: CE 0.14422756433486938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/200, Loss=0.15260079130530357
Loss made of: CE 0.1835978627204895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/200, Loss=0.16358766853809356
Loss made of: CE 0.14628741145133972, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1593734622001648, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.1593734622001648, Class Loss=0.1593734622001648, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/200, Loss=0.16195990443229674
Loss made of: CE 0.1394467055797577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/200, Loss=0.13826745226979256
Loss made of: CE 0.1675586700439453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/200, Loss=0.144589052349329
Loss made of: CE 0.13819557428359985, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/200, Loss=0.1420276328921318
Loss made of: CE 0.14139972627162933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/200, Loss=0.14816291481256486
Loss made of: CE 0.15885382890701294, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/200, Loss=0.13037705421447754
Loss made of: CE 0.13220131397247314, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/200, Loss=0.15679553002119065
Loss made of: CE 0.18039155006408691, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/200, Loss=0.14694524109363555
Loss made of: CE 0.14106032252311707, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/200, Loss=0.14616079926490783
Loss made of: CE 0.1352657824754715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/200, Loss=0.1428128257393837
Loss made of: CE 0.11931481957435608, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/200, Loss=0.14303747788071633
Loss made of: CE 0.1505078673362732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/200, Loss=0.15198338329792022
Loss made of: CE 0.14542189240455627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/200, Loss=0.13582299575209617
Loss made of: CE 0.11534561961889267, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/200, Loss=0.14140962511301042
Loss made of: CE 0.1143520399928093, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/200, Loss=0.15843726843595504
Loss made of: CE 0.12979085743427277, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/200, Loss=0.15729798674583434
Loss made of: CE 0.1282162368297577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/200, Loss=0.1416855663061142
Loss made of: CE 0.13795146346092224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/200, Loss=0.15375179350376128
Loss made of: CE 0.16581164300441742, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/200, Loss=0.1358928233385086
Loss made of: CE 0.15772411227226257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/200, Loss=0.13563094213604926
Loss made of: CE 0.138997420668602, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1456524133682251, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.1456524133682251, Class Loss=0.1456524133682251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/200, Loss=0.13643657565116882
Loss made of: CE 0.13169896602630615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/200, Loss=0.14761413782835006
Loss made of: CE 0.12220610678195953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/200, Loss=0.12780145704746246
Loss made of: CE 0.16076645255088806, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/200, Loss=0.12843225598335267
Loss made of: CE 0.12116463482379913, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/200, Loss=0.1351109527051449
Loss made of: CE 0.19269484281539917, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/200, Loss=0.1270105227828026
Loss made of: CE 0.1425863653421402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/200, Loss=0.13726126104593278
Loss made of: CE 0.12851642072200775, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/200, Loss=0.14590243771672248
Loss made of: CE 0.08814498037099838, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/200, Loss=0.13218768388032914
Loss made of: CE 0.11023446172475815, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/200, Loss=0.14639148488640785
Loss made of: CE 0.12571805715560913, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/200, Loss=0.1179407998919487
Loss made of: CE 0.1105547547340393, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/200, Loss=0.12625689804553986
Loss made of: CE 0.11366604268550873, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/200, Loss=0.11920014768838882
Loss made of: CE 0.12143784761428833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/200, Loss=0.14207483753561972
Loss made of: CE 0.13293251395225525, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/200, Loss=0.1389112189412117
Loss made of: CE 0.12917521595954895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/200, Loss=0.13761162310838698
Loss made of: CE 0.11951997131109238, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/200, Loss=0.13909890204668046
Loss made of: CE 0.1288163959980011, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/200, Loss=0.12933532148599625
Loss made of: CE 0.11714675277471542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/200, Loss=0.13764699548482895
Loss made of: CE 0.13316015899181366, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/200, Loss=0.13429972603917123
Loss made of: CE 0.11989005655050278, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1343262493610382, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.1343262493610382, Class Loss=0.1343262493610382, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/200, Loss=0.12388266697525978
Loss made of: CE 0.1443633884191513, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/200, Loss=0.12651106119155883
Loss made of: CE 0.12416867911815643, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/200, Loss=0.12860336676239967
Loss made of: CE 0.16567884385585785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/200, Loss=0.11992111131548881
Loss made of: CE 0.12841638922691345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/200, Loss=0.11376323997974395
Loss made of: CE 0.11524617671966553, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/200, Loss=0.1180652730166912
Loss made of: CE 0.10193919390439987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/200, Loss=0.1205087848007679
Loss made of: CE 0.1080440878868103, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/200, Loss=0.1448306292295456
Loss made of: CE 0.138204887509346, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/200, Loss=0.12379335761070251
Loss made of: CE 0.1070445328950882, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/200, Loss=0.12978297621011733
Loss made of: CE 0.13825565576553345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/200, Loss=0.1311784155666828
Loss made of: CE 0.1737726777791977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/200, Loss=0.1242200680077076
Loss made of: CE 0.10036589950323105, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/200, Loss=0.1466791607439518
Loss made of: CE 0.113015316426754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/200, Loss=0.1391390509903431
Loss made of: CE 0.1620737463235855, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/200, Loss=0.13537912219762802
Loss made of: CE 0.1328069418668747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/200, Loss=0.14705105870962143
Loss made of: CE 0.11379371583461761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/200, Loss=0.12453863695263863
Loss made of: CE 0.1648900806903839, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/200, Loss=0.11731318682432175
Loss made of: CE 0.13863569498062134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/200, Loss=0.12999819964170456
Loss made of: CE 0.14709016680717468, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/200, Loss=0.13669098019599915
Loss made of: CE 0.14199456572532654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1290925145149231, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.1290925145149231, Class Loss=0.1290925145149231, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/205, Loss=0.21596865057945253
Loss made of: CE 0.15209601819515228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/205, Loss=0.19710940718650818
Loss made of: CE 0.11865746974945068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/205, Loss=0.22295501381158828
Loss made of: CE 0.23335866630077362, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/205, Loss=0.1986887440085411
Loss made of: CE 0.23546943068504333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/205, Loss=0.23975372314453125
Loss made of: CE 0.20829252898693085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/205, Loss=0.25265937596559523
Loss made of: CE 0.18370962142944336, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/205, Loss=0.21705590784549714
Loss made of: CE 0.1582963466644287, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/205, Loss=0.23545449674129487
Loss made of: CE 0.30441808700561523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/205, Loss=0.19458284825086594
Loss made of: CE 0.16054491698741913, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/205, Loss=0.19825990051031112
Loss made of: CE 0.15226143598556519, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/205, Loss=0.2068896010518074
Loss made of: CE 0.17127725481987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/205, Loss=0.2279697060585022
Loss made of: CE 0.20843565464019775, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/205, Loss=0.21719108521938324
Loss made of: CE 0.17632316052913666, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/205, Loss=0.2044868841767311
Loss made of: CE 0.17904464900493622, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/205, Loss=0.22389138340950013
Loss made of: CE 0.19340555369853973, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/205, Loss=0.2072306290268898
Loss made of: CE 0.2118566483259201, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/205, Loss=0.2099529504776001
Loss made of: CE 0.2398330420255661, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/205, Loss=0.19795003533363342
Loss made of: CE 0.1618422120809555, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/205, Loss=0.1967156082391739
Loss made of: CE 0.18390235304832458, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/205, Loss=0.22893041223287583
Loss made of: CE 0.23079541325569153, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.21472467482089996, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.21472467482089996, Class Loss=0.21472467482089996, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/205, Loss=0.19896477907896043
Loss made of: CE 0.27023422718048096, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/205, Loss=0.19747575074434282
Loss made of: CE 0.21411508321762085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/205, Loss=0.16670961081981658
Loss made of: CE 0.17808595299720764, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/205, Loss=0.16778872013092042
Loss made of: CE 0.16690468788146973, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/205, Loss=0.1827942907810211
Loss made of: CE 0.2111978977918625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/205, Loss=0.2091544285416603
Loss made of: CE 0.1971292495727539, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/205, Loss=0.20099963247776031
Loss made of: CE 0.2412765920162201, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/205, Loss=0.1635000891983509
Loss made of: CE 0.137032151222229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/205, Loss=0.17547111958265305
Loss made of: CE 0.17374448478221893, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/205, Loss=0.20117939859628678
Loss made of: CE 0.1968878209590912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/205, Loss=0.20086469873785973
Loss made of: CE 0.14258703589439392, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/205, Loss=0.1976114481687546
Loss made of: CE 0.31527379155158997, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/205, Loss=0.18383515030145645
Loss made of: CE 0.14395681023597717, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/205, Loss=0.1954849362373352
Loss made of: CE 0.22035416960716248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/205, Loss=0.1762782096862793
Loss made of: CE 0.22492961585521698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/205, Loss=0.19233019948005675
Loss made of: CE 0.14617520570755005, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/205, Loss=0.17879251688718795
Loss made of: CE 0.24756717681884766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/205, Loss=0.17354028970003127
Loss made of: CE 0.15182945132255554, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/205, Loss=0.17307138592004775
Loss made of: CE 0.13967737555503845, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/205, Loss=0.1737879619002342
Loss made of: CE 0.1580457091331482, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1849433034658432, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.1849433034658432, Class Loss=0.1849433034658432, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/205, Loss=0.1527984097599983
Loss made of: CE 0.15845192968845367, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/205, Loss=0.15418611317873002
Loss made of: CE 0.10894271731376648, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/205, Loss=0.16327684968709946
Loss made of: CE 0.15539738535881042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/205, Loss=0.15865878984332085
Loss made of: CE 0.11798517405986786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/205, Loss=0.16125294417142869
Loss made of: CE 0.1631605327129364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/205, Loss=0.17246965765953065
Loss made of: CE 0.13508372008800507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/205, Loss=0.17231623828411102
Loss made of: CE 0.13536646962165833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/205, Loss=0.17417487353086472
Loss made of: CE 0.21302127838134766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/205, Loss=0.15803980678319932
Loss made of: CE 0.19301025569438934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/205, Loss=0.15721440613269805
Loss made of: CE 0.1483677178621292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/205, Loss=0.16710994839668275
Loss made of: CE 0.24500557780265808, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/205, Loss=0.16067125797271728
Loss made of: CE 0.13739435374736786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/205, Loss=0.16535015031695366
Loss made of: CE 0.1784742772579193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/205, Loss=0.16732852756977082
Loss made of: CE 0.1674862802028656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/205, Loss=0.16024544537067414
Loss made of: CE 0.1416226625442505, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/205, Loss=0.16693921983242035
Loss made of: CE 0.16009047627449036, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/205, Loss=0.15466800183057786
Loss made of: CE 0.1458832323551178, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/205, Loss=0.15311696156859397
Loss made of: CE 0.15676765143871307, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/205, Loss=0.1770467460155487
Loss made of: CE 0.2289266586303711, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/205, Loss=0.16042851731181146
Loss made of: CE 0.15908530354499817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.16227491199970245, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.16227491199970245, Class Loss=0.16227491199970245, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/205, Loss=0.1568767510354519
Loss made of: CE 0.1343085616827011, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/205, Loss=0.1512955591082573
Loss made of: CE 0.12979534268379211, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/205, Loss=0.14908557385206223
Loss made of: CE 0.10017725825309753, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/205, Loss=0.14856664463877678
Loss made of: CE 0.14397887885570526, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/205, Loss=0.15324135422706603
Loss made of: CE 0.1145077794790268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/205, Loss=0.1493039458990097
Loss made of: CE 0.19434458017349243, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/205, Loss=0.13859977573156357
Loss made of: CE 0.1345929354429245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/205, Loss=0.14451698511838912
Loss made of: CE 0.15519370138645172, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/205, Loss=0.16212700307369232
Loss made of: CE 0.12618114054203033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/205, Loss=0.13350519463419913
Loss made of: CE 0.13291263580322266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/205, Loss=0.15047094747424125
Loss made of: CE 0.14408940076828003, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/205, Loss=0.1432127833366394
Loss made of: CE 0.18175844848155975, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/205, Loss=0.15423552244901656
Loss made of: CE 0.1278534233570099, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/205, Loss=0.1544780135154724
Loss made of: CE 0.15339617431163788, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/205, Loss=0.1529544100165367
Loss made of: CE 0.12374603748321533, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/205, Loss=0.17200186997652053
Loss made of: CE 0.147050678730011, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/205, Loss=0.1375187359750271
Loss made of: CE 0.1338823139667511, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/205, Loss=0.1640805646777153
Loss made of: CE 0.1648716926574707, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/205, Loss=0.1451410472393036
Loss made of: CE 0.17442171275615692, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/205, Loss=0.1336917206645012
Loss made of: CE 0.14532464742660522, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.14987917244434357, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.14987917244434357, Class Loss=0.14987917244434357, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/205, Loss=0.15278064385056495
Loss made of: CE 0.2158619463443756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/205, Loss=0.15140250474214553
Loss made of: CE 0.14387208223342896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/205, Loss=0.15052204057574273
Loss made of: CE 0.12544889748096466, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/205, Loss=0.1316036969423294
Loss made of: CE 0.11969910562038422, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/205, Loss=0.13469432890415192
Loss made of: CE 0.11264020204544067, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/205, Loss=0.13760844841599465
Loss made of: CE 0.1279682219028473, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/205, Loss=0.14512153044342996
Loss made of: CE 0.13406221568584442, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/205, Loss=0.11812989637255669
Loss made of: CE 0.10070904344320297, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/205, Loss=0.15034412890672683
Loss made of: CE 0.15431156754493713, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/205, Loss=0.1338749870657921
Loss made of: CE 0.1316211223602295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/205, Loss=0.13700592666864395
Loss made of: CE 0.1852596402168274, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/205, Loss=0.14553546607494355
Loss made of: CE 0.13126127421855927, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/205, Loss=0.1352979615330696
Loss made of: CE 0.12443872541189194, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/205, Loss=0.12910320237278938
Loss made of: CE 0.18524491786956787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/205, Loss=0.13916659280657767
Loss made of: CE 0.1227802261710167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/205, Loss=0.13668474033474923
Loss made of: CE 0.11902463436126709, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/205, Loss=0.13943319246172906
Loss made of: CE 0.12304174154996872, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/205, Loss=0.14259985238313674
Loss made of: CE 0.11186522245407104, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/205, Loss=0.14357265308499337
Loss made of: CE 0.10957513749599457, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/205, Loss=0.1412864089012146
Loss made of: CE 0.1151977926492691, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1398707777261734, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.1398707777261734, Class Loss=0.1398707777261734, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/205, Loss=0.11539609879255294
Loss made of: CE 0.11430436372756958, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/205, Loss=0.1407031573355198
Loss made of: CE 0.1408940851688385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/205, Loss=0.14122232496738435
Loss made of: CE 0.1269446611404419, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/205, Loss=0.13573808446526528
Loss made of: CE 0.15700849890708923, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/205, Loss=0.13755681216716767
Loss made of: CE 0.12850117683410645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/205, Loss=0.13242307379841806
Loss made of: CE 0.09141459316015244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/205, Loss=0.14848919883370398
Loss made of: CE 0.15119987726211548, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/205, Loss=0.1293683961033821
Loss made of: CE 0.11249123513698578, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/205, Loss=0.1303423285484314
Loss made of: CE 0.12281950563192368, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/205, Loss=0.13706065937876702
Loss made of: CE 0.11457812786102295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/205, Loss=0.11394400522112846
Loss made of: CE 0.10765151679515839, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/205, Loss=0.12358607053756714
Loss made of: CE 0.13066230714321136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/205, Loss=0.13533999696373938
Loss made of: CE 0.13377460837364197, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/205, Loss=0.1317903943359852
Loss made of: CE 0.10555429756641388, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/205, Loss=0.1283569984138012
Loss made of: CE 0.11175992339849472, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/205, Loss=0.11189227700233459
Loss made of: CE 0.09879385679960251, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/205, Loss=0.13188517168164254
Loss made of: CE 0.10886754840612411, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/205, Loss=0.12841771468520163
Loss made of: CE 0.10224170982837677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/205, Loss=0.1404339276254177
Loss made of: CE 0.1385008990764618, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/205, Loss=0.144677996635437
Loss made of: CE 0.12416782230138779, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.13245898485183716, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.13245898485183716, Class Loss=0.13245898485183716, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.16671863198280334, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.940838
Mean Acc: 0.844200
FreqW Acc: 0.893692
Mean IoU: 0.742955
Class IoU:
	class 0: 0.9368852
	class 1: 0.8925751
	class 2: 0.4104102
	class 3: 0.66989815
	class 4: 0.67983866
	class 5: 0.7780525
	class 6: 0.93362486
	class 7: 0.90865576
	class 8: 0.87930375
	class 9: 0.42931968
	class 10: 0.55368865
	class 11: 0.60035115
	class 12: 0.824524
	class 13: 0.6666601
	class 14: 0.86281204
	class 15: 0.86067563
Class Acc:
	class 0: 0.9701979
	class 1: 0.95534956
	class 2: 0.8839808
	class 3: 0.67457914
	class 4: 0.83140546
	class 5: 0.8713614
	class 6: 0.9649796
	class 7: 0.9375585
	class 8: 0.9336708
	class 9: 0.5620882
	class 10: 0.57164687
	class 11: 0.6341721
	class 12: 0.9615774
	class 13: 0.9040418
	class 14: 0.92729145
	class 15: 0.92329973

federated global round: 2, step: 0
select part of clients to conduct local training
[8, 6, 2, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/229, Loss=0.16763869673013687
Loss made of: CE 0.13157466053962708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/229, Loss=0.18189111351966858
Loss made of: CE 0.25830066204071045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/229, Loss=0.1805338069796562
Loss made of: CE 0.1819123923778534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/229, Loss=0.18515874445438385
Loss made of: CE 0.1269548237323761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/229, Loss=0.16285233944654465
Loss made of: CE 0.16777417063713074, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/229, Loss=0.18515172600746155
Loss made of: CE 0.19340740144252777, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/229, Loss=0.17281800284981727
Loss made of: CE 0.18335473537445068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/229, Loss=0.16856953650712966
Loss made of: CE 0.15301236510276794, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/229, Loss=0.20022183656692505
Loss made of: CE 0.16775190830230713, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/229, Loss=0.18150819838047028
Loss made of: CE 0.14699453115463257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/229, Loss=0.18806199207901955
Loss made of: CE 0.1729101538658142, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/229, Loss=0.1760719373822212
Loss made of: CE 0.1254383623600006, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/229, Loss=0.177527317404747
Loss made of: CE 0.2592204809188843, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/229, Loss=0.18162642493844033
Loss made of: CE 0.13750039041042328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/229, Loss=0.15791502296924592
Loss made of: CE 0.1421857476234436, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/229, Loss=0.1802744135260582
Loss made of: CE 0.13853570818901062, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/229, Loss=0.2111010178923607
Loss made of: CE 0.18406018614768982, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/229, Loss=0.19624873101711274
Loss made of: CE 0.151164710521698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/229, Loss=0.19724728167057037
Loss made of: CE 0.21108806133270264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/229, Loss=0.18073028326034546
Loss made of: CE 0.18363869190216064, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 210/229, Loss=0.18073976486921312
Loss made of: CE 0.1523585319519043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 220/229, Loss=0.21810753792524337
Loss made of: CE 0.3791165351867676, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.184433713555336, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.184433713555336, Class Loss=0.184433713555336, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/229, Loss=0.1717894271016121
Loss made of: CE 0.1785002201795578, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/229, Loss=0.1663871869444847
Loss made of: CE 0.16575278341770172, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/229, Loss=0.15677628368139268
Loss made of: CE 0.16593964397907257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/229, Loss=0.1661214992403984
Loss made of: CE 0.15088263154029846, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/229, Loss=0.16200872361660004
Loss made of: CE 0.16638019680976868, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/229, Loss=0.1630796879529953
Loss made of: CE 0.13501766324043274, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/229, Loss=0.16855108588933945
Loss made of: CE 0.15108537673950195, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/229, Loss=0.1552007883787155
Loss made of: CE 0.13129667937755585, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/229, Loss=0.1705905020236969
Loss made of: CE 0.15809109807014465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/229, Loss=0.1563425302505493
Loss made of: CE 0.20254108309745789, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/229, Loss=0.14589780196547508
Loss made of: CE 0.18673720955848694, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/229, Loss=0.15755035281181334
Loss made of: CE 0.17402347922325134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/229, Loss=0.16977976262569427
Loss made of: CE 0.13081562519073486, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/229, Loss=0.14509818255901336
Loss made of: CE 0.1587667465209961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/229, Loss=0.13890512511134148
Loss made of: CE 0.14014574885368347, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/229, Loss=0.17016337662935258
Loss made of: CE 0.16313280165195465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/229, Loss=0.14867633879184722
Loss made of: CE 0.1345222443342209, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/229, Loss=0.14811425730586053
Loss made of: CE 0.150596022605896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/229, Loss=0.16045133620500565
Loss made of: CE 0.2332042157649994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/229, Loss=0.1527373105287552
Loss made of: CE 0.19177258014678955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 210/229, Loss=0.15357379168272017
Loss made of: CE 0.16803090274333954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 220/229, Loss=0.15172735899686812
Loss made of: CE 0.1509169638156891, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.15821534395217896, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.15821534395217896, Class Loss=0.15821534395217896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/229, Loss=0.12815090864896775
Loss made of: CE 0.12999024987220764, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/229, Loss=0.14522599205374717
Loss made of: CE 0.19113841652870178, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/229, Loss=0.14679838940501214
Loss made of: CE 0.09771160781383514, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/229, Loss=0.13431418761610986
Loss made of: CE 0.1277294009923935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/229, Loss=0.13296499028801917
Loss made of: CE 0.10872223228216171, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/229, Loss=0.1400147333741188
Loss made of: CE 0.17695912718772888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/229, Loss=0.13531744629144668
Loss made of: CE 0.14942604303359985, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/229, Loss=0.13265076354146005
Loss made of: CE 0.11345076560974121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/229, Loss=0.12988854497671126
Loss made of: CE 0.1745358407497406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/229, Loss=0.13418802618980408
Loss made of: CE 0.12021725624799728, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/229, Loss=0.14404067620635033
Loss made of: CE 0.16361314058303833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/229, Loss=0.13670414090156555
Loss made of: CE 0.12003444135189056, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/229, Loss=0.14266453087329864
Loss made of: CE 0.23447632789611816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/229, Loss=0.14008263349533082
Loss made of: CE 0.10246045887470245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/229, Loss=0.14251287803053855
Loss made of: CE 0.1106882318854332, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/229, Loss=0.13941698893904686
Loss made of: CE 0.12457945942878723, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/229, Loss=0.13117210045456887
Loss made of: CE 0.09469442069530487, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/229, Loss=0.13559678941965103
Loss made of: CE 0.12728342413902283, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/229, Loss=0.1305275082588196
Loss made of: CE 0.09744797646999359, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/229, Loss=0.14355523586273194
Loss made of: CE 0.1113104447722435, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 210/229, Loss=0.12307539656758308
Loss made of: CE 0.09802504628896713, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 220/229, Loss=0.1444228246808052
Loss made of: CE 0.15541133284568787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.13692805171012878, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.13692805171012878, Class Loss=0.13692805171012878, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/229, Loss=0.11920824125409127
Loss made of: CE 0.12459125369787216, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/229, Loss=0.13064365163445474
Loss made of: CE 0.15688233077526093, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/229, Loss=0.12094754427671432
Loss made of: CE 0.16153505444526672, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/229, Loss=0.11292432099580765
Loss made of: CE 0.12465472519397736, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/229, Loss=0.1381417579948902
Loss made of: CE 0.09804024547338486, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/229, Loss=0.10923789441585541
Loss made of: CE 0.10917127132415771, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/229, Loss=0.1281539037823677
Loss made of: CE 0.11353187263011932, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/229, Loss=0.12749993354082106
Loss made of: CE 0.11207704246044159, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/229, Loss=0.12852306962013244
Loss made of: CE 0.11630445718765259, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/229, Loss=0.11558851301670074
Loss made of: CE 0.11405929923057556, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/229, Loss=0.12536170408129693
Loss made of: CE 0.10214230418205261, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/229, Loss=0.13732408061623574
Loss made of: CE 0.137093648314476, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/229, Loss=0.12642062231898307
Loss made of: CE 0.12788128852844238, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/229, Loss=0.11787196770310401
Loss made of: CE 0.07697179913520813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/229, Loss=0.13649825006723404
Loss made of: CE 0.16975104808807373, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/229, Loss=0.12722521275281906
Loss made of: CE 0.1384313702583313, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/229, Loss=0.11915961652994156
Loss made of: CE 0.11523362994194031, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/229, Loss=0.13038969784975052
Loss made of: CE 0.11513684689998627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/229, Loss=0.11422282755374909
Loss made of: CE 0.12758482992649078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/229, Loss=0.13275560960173607
Loss made of: CE 0.13552598655223846, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 210/229, Loss=0.12828244715929032
Loss made of: CE 0.1421690285205841, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 220/229, Loss=0.11519652009010314
Loss made of: CE 0.12140201032161713, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.12491840124130249, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.12491840124130249, Class Loss=0.12491840124130249, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/229, Loss=0.1221185103058815
Loss made of: CE 0.1416962444782257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/229, Loss=0.12036958336830139
Loss made of: CE 0.07679420709609985, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/229, Loss=0.12904277592897415
Loss made of: CE 0.08503326773643494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/229, Loss=0.13402469232678413
Loss made of: CE 0.11075465381145477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/229, Loss=0.11470256969332696
Loss made of: CE 0.15261195600032806, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/229, Loss=0.1286347396671772
Loss made of: CE 0.09613087773323059, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/229, Loss=0.1097127802670002
Loss made of: CE 0.08235208690166473, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/229, Loss=0.10827775523066521
Loss made of: CE 0.11035515367984772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/229, Loss=0.10883121192455292
Loss made of: CE 0.1256251186132431, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/229, Loss=0.1106324814260006
Loss made of: CE 0.12672100961208344, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/229, Loss=0.10909664556384087
Loss made of: CE 0.10030104964971542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/229, Loss=0.11922357603907585
Loss made of: CE 0.20173391699790955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/229, Loss=0.1213902123272419
Loss made of: CE 0.11881838738918304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/229, Loss=0.11725918501615525
Loss made of: CE 0.13782165944576263, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/229, Loss=0.10529729649424553
Loss made of: CE 0.09961148351430893, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/229, Loss=0.10618158876895904
Loss made of: CE 0.09891808032989502, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/229, Loss=0.10457958728075027
Loss made of: CE 0.08954325318336487, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/229, Loss=0.11230837032198906
Loss made of: CE 0.11533521115779877, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/229, Loss=0.11992608681321144
Loss made of: CE 0.1409456580877304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/229, Loss=0.12140141129493713
Loss made of: CE 0.14777344465255737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 210/229, Loss=0.11379953771829605
Loss made of: CE 0.11343243718147278, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 220/229, Loss=0.11828358545899391
Loss made of: CE 0.12171176075935364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.11653828620910645, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.11653828620910645, Class Loss=0.11653828620910645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/229, Loss=0.10175788402557373
Loss made of: CE 0.13434326648712158, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/229, Loss=0.12664474472403525
Loss made of: CE 0.1433728188276291, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/229, Loss=0.10234294757246971
Loss made of: CE 0.11254309862852097, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/229, Loss=0.09873141348361969
Loss made of: CE 0.08393999189138412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/229, Loss=0.10431041941046715
Loss made of: CE 0.09259700775146484, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/229, Loss=0.10725896283984185
Loss made of: CE 0.10602615773677826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/229, Loss=0.10901401937007904
Loss made of: CE 0.10763045400381088, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/229, Loss=0.10663284510374069
Loss made of: CE 0.10210317373275757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/229, Loss=0.10515358895063401
Loss made of: CE 0.09886574745178223, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/229, Loss=0.1095017746090889
Loss made of: CE 0.10847631096839905, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/229, Loss=0.10161104053258896
Loss made of: CE 0.08821287006139755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/229, Loss=0.10683770924806595
Loss made of: CE 0.08869373798370361, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/229, Loss=0.10851076617836952
Loss made of: CE 0.12818604707717896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/229, Loss=0.11917500272393226
Loss made of: CE 0.0940840095281601, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/229, Loss=0.10918683558702469
Loss made of: CE 0.11243202537298203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/229, Loss=0.11619584262371063
Loss made of: CE 0.08815538883209229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/229, Loss=0.10637975335121155
Loss made of: CE 0.10248756408691406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/229, Loss=0.10512561053037643
Loss made of: CE 0.11640431731939316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/229, Loss=0.1114464595913887
Loss made of: CE 0.123788021504879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/229, Loss=0.11027626022696495
Loss made of: CE 0.1500709056854248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 210/229, Loss=0.10872062593698502
Loss made of: CE 0.09465217590332031, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 220/229, Loss=0.10735106095671654
Loss made of: CE 0.08412636816501617, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10841251164674759, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.10841251164674759, Class Loss=0.10841251164674759, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/198, Loss=0.16643133163452148
Loss made of: CE 0.10395409911870956, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/198, Loss=0.1793604761362076
Loss made of: CE 0.2224808931350708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/198, Loss=0.179495407640934
Loss made of: CE 0.15659372508525848, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/198, Loss=0.17362550050020217
Loss made of: CE 0.16691574454307556, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/198, Loss=0.15483390241861344
Loss made of: CE 0.15168870985507965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/198, Loss=0.16927314400672913
Loss made of: CE 0.1379612535238266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/198, Loss=0.17216456234455108
Loss made of: CE 0.15041284263134003, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/198, Loss=0.1532393492758274
Loss made of: CE 0.12737315893173218, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/198, Loss=0.16523505002260208
Loss made of: CE 0.13105934858322144, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/198, Loss=0.15937652438879013
Loss made of: CE 0.16375355422496796, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/198, Loss=0.1557621218264103
Loss made of: CE 0.16111662983894348, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/198, Loss=0.16106032878160476
Loss made of: CE 0.13906046748161316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/198, Loss=0.1619223438203335
Loss made of: CE 0.1700342893600464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/198, Loss=0.15225009620189667
Loss made of: CE 0.14097896218299866, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/198, Loss=0.16887881457805634
Loss made of: CE 0.20479440689086914, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/198, Loss=0.16354539543390273
Loss made of: CE 0.14262497425079346, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/198, Loss=0.17651647478342056
Loss made of: CE 0.17001734673976898, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/198, Loss=0.14994777962565423
Loss made of: CE 0.17521685361862183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/198, Loss=0.14792225807905196
Loss made of: CE 0.18070094287395477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.16324003040790558, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.16324003040790558, Class Loss=0.16324003040790558, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007770
Epoch 2, Batch 10/198, Loss=0.13979826495051384
Loss made of: CE 0.11606808751821518, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/198, Loss=0.1381656587123871
Loss made of: CE 0.11208775639533997, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/198, Loss=0.13222290500998496
Loss made of: CE 0.1028052419424057, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/198, Loss=0.1380992129445076
Loss made of: CE 0.12729015946388245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/198, Loss=0.1409892126917839
Loss made of: CE 0.1329764425754547, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/198, Loss=0.15014863088726998
Loss made of: CE 0.16155919432640076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/198, Loss=0.1475451610982418
Loss made of: CE 0.12305941432714462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/198, Loss=0.13742803484201432
Loss made of: CE 0.1150130182504654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/198, Loss=0.14726294353604316
Loss made of: CE 0.15945014357566833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/198, Loss=0.1448034793138504
Loss made of: CE 0.1331453174352646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/198, Loss=0.13836038038134574
Loss made of: CE 0.12330516427755356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/198, Loss=0.14964059442281724
Loss made of: CE 0.15288494527339935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/198, Loss=0.14035978242754937
Loss made of: CE 0.1523970067501068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/198, Loss=0.16851798817515373
Loss made of: CE 0.2125745266675949, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/198, Loss=0.16976341381669044
Loss made of: CE 0.2465987205505371, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/198, Loss=0.15980198383331298
Loss made of: CE 0.1352006494998932, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/198, Loss=0.15405014380812646
Loss made of: CE 0.14100754261016846, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/198, Loss=0.16229218244552612
Loss made of: CE 0.19820629060268402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/198, Loss=0.16713743209838866
Loss made of: CE 0.21394149959087372, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.14890094101428986, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.14890094101428986, Class Loss=0.14890094101428986, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.007358
Epoch 3, Batch 10/198, Loss=0.14137668386101723
Loss made of: CE 0.13397270441055298, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/198, Loss=0.13065323755145072
Loss made of: CE 0.14011496305465698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/198, Loss=0.13659374117851258
Loss made of: CE 0.15660464763641357, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/198, Loss=0.14289647415280343
Loss made of: CE 0.13669666647911072, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/198, Loss=0.12645227685570717
Loss made of: CE 0.1057324931025505, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/198, Loss=0.13189970552921296
Loss made of: CE 0.12570051848888397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/198, Loss=0.14275100082159042
Loss made of: CE 0.18070858716964722, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/198, Loss=0.13809147849678993
Loss made of: CE 0.1697031706571579, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/198, Loss=0.1378348506987095
Loss made of: CE 0.15199600160121918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/198, Loss=0.1339823305606842
Loss made of: CE 0.13165150582790375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/198, Loss=0.1242704525589943
Loss made of: CE 0.09461262822151184, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/198, Loss=0.12214859127998352
Loss made of: CE 0.11980819702148438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/198, Loss=0.13413961678743364
Loss made of: CE 0.17868179082870483, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/198, Loss=0.14696759805083276
Loss made of: CE 0.12465833127498627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/198, Loss=0.1564677134156227
Loss made of: CE 0.19045710563659668, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/198, Loss=0.14093881025910376
Loss made of: CE 0.1533776968717575, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/198, Loss=0.1265484496951103
Loss made of: CE 0.1080751046538353, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/198, Loss=0.12757935672998427
Loss made of: CE 0.1328144371509552, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/198, Loss=0.1363940306007862
Loss made of: CE 0.14284595847129822, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.13593627512454987, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.13593627512454987, Class Loss=0.13593627512454987, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.006943
Epoch 4, Batch 10/198, Loss=0.12661346718668937
Loss made of: CE 0.1154499277472496, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/198, Loss=0.1416548654437065
Loss made of: CE 0.2442566603422165, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/198, Loss=0.12836058288812638
Loss made of: CE 0.13695646822452545, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/198, Loss=0.12011287361383438
Loss made of: CE 0.11388247460126877, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/198, Loss=0.13386597931385041
Loss made of: CE 0.14659926295280457, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/198, Loss=0.11972999945282936
Loss made of: CE 0.11504291743040085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/198, Loss=0.1278962604701519
Loss made of: CE 0.21767275035381317, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/198, Loss=0.11744061782956124
Loss made of: CE 0.12857292592525482, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/198, Loss=0.11351776868104935
Loss made of: CE 0.1270463466644287, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/198, Loss=0.13304926306009293
Loss made of: CE 0.154358372092247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/198, Loss=0.12771134600043296
Loss made of: CE 0.14023733139038086, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/198, Loss=0.11402988657355309
Loss made of: CE 0.12716040015220642, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/198, Loss=0.1320133052766323
Loss made of: CE 0.1188557967543602, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/198, Loss=0.13168452605605124
Loss made of: CE 0.14611965417861938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/198, Loss=0.12145765200257301
Loss made of: CE 0.15768931806087494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/198, Loss=0.13452689871191978
Loss made of: CE 0.1172369122505188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/198, Loss=0.12520754486322402
Loss made of: CE 0.13134503364562988, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/198, Loss=0.12269091457128525
Loss made of: CE 0.13628381490707397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/198, Loss=0.1252727061510086
Loss made of: CE 0.1342485547065735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.12561386823654175, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.12561386823654175, Class Loss=0.12561386823654175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006525
Epoch 5, Batch 10/198, Loss=0.11984821036458015
Loss made of: CE 0.14628222584724426, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/198, Loss=0.11648000031709671
Loss made of: CE 0.1515985131263733, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/198, Loss=0.11667029783129693
Loss made of: CE 0.11064192652702332, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/198, Loss=0.11194269508123397
Loss made of: CE 0.10928715765476227, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/198, Loss=0.11448800191283226
Loss made of: CE 0.13560444116592407, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/198, Loss=0.11541351079940795
Loss made of: CE 0.09587810933589935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/198, Loss=0.12317423447966576
Loss made of: CE 0.10452732443809509, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/198, Loss=0.10929108038544655
Loss made of: CE 0.07894841581583023, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/198, Loss=0.11466532871127129
Loss made of: CE 0.10538429021835327, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/198, Loss=0.12483079358935356
Loss made of: CE 0.1482953131198883, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/198, Loss=0.1333044819533825
Loss made of: CE 0.1408206820487976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/198, Loss=0.12523580193519593
Loss made of: CE 0.1585121601819992, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/198, Loss=0.11811078786849975
Loss made of: CE 0.11937195062637329, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/198, Loss=0.11721534579992295
Loss made of: CE 0.12057163566350937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/198, Loss=0.12170279249548913
Loss made of: CE 0.1291801780462265, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/198, Loss=0.11665669679641724
Loss made of: CE 0.0911276638507843, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/198, Loss=0.11745945364236832
Loss made of: CE 0.09182938188314438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/198, Loss=0.11792493090033532
Loss made of: CE 0.10560917854309082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/198, Loss=0.11676154136657715
Loss made of: CE 0.08598838746547699, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.11803929507732391, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.11803929507732391, Class Loss=0.11803929507732391, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006104
Epoch 6, Batch 10/198, Loss=0.1064572960138321
Loss made of: CE 0.11480310559272766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/198, Loss=0.11152100488543511
Loss made of: CE 0.11498107016086578, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/198, Loss=0.11563072130084037
Loss made of: CE 0.12094529718160629, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/198, Loss=0.12312056422233582
Loss made of: CE 0.10694508999586105, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/198, Loss=0.11674485877156257
Loss made of: CE 0.09541179239749908, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/198, Loss=0.11790988147258759
Loss made of: CE 0.11415835469961166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/198, Loss=0.11267470419406891
Loss made of: CE 0.10908277332782745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/198, Loss=0.10497049391269683
Loss made of: CE 0.10337337106466293, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/198, Loss=0.11156815886497498
Loss made of: CE 0.13824820518493652, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/198, Loss=0.10507255792617798
Loss made of: CE 0.07054226100444794, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/198, Loss=0.11019524708390235
Loss made of: CE 0.13692650198936462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/198, Loss=0.12334792092442512
Loss made of: CE 0.10109329223632812, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/198, Loss=0.12489932850003242
Loss made of: CE 0.11245745420455933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/198, Loss=0.14347372949123383
Loss made of: CE 0.13309085369110107, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/198, Loss=0.12191027477383613
Loss made of: CE 0.15359455347061157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/198, Loss=0.12218222171068191
Loss made of: CE 0.09142747521400452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/198, Loss=0.124048163741827
Loss made of: CE 0.10921667516231537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/198, Loss=0.10896625891327857
Loss made of: CE 0.08469340205192566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/198, Loss=0.11348446607589721
Loss made of: CE 0.09579645097255707, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11659163981676102, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.11659163981676102, Class Loss=0.11659163981676102, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/200, Loss=0.15825520306825638
Loss made of: CE 0.2006016969680786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/200, Loss=0.13873248249292375
Loss made of: CE 0.10733450949192047, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/200, Loss=0.14045201390981674
Loss made of: CE 0.11717637628316879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/200, Loss=0.1450148843228817
Loss made of: CE 0.10069718211889267, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/200, Loss=0.13236212357878685
Loss made of: CE 0.13722796738147736, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/200, Loss=0.14420067965984346
Loss made of: CE 0.1480952650308609, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/200, Loss=0.1557817630469799
Loss made of: CE 0.125921368598938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/200, Loss=0.1401334933936596
Loss made of: CE 0.12481661885976791, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/200, Loss=0.1370818242430687
Loss made of: CE 0.16567379236221313, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/200, Loss=0.1342733696103096
Loss made of: CE 0.13688629865646362, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/200, Loss=0.1449219547212124
Loss made of: CE 0.13008852303028107, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/200, Loss=0.14110936000943183
Loss made of: CE 0.1010347530245781, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/200, Loss=0.13143713772296906
Loss made of: CE 0.13216152787208557, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/200, Loss=0.13372685387730598
Loss made of: CE 0.1478966474533081, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/200, Loss=0.1483457051217556
Loss made of: CE 0.16143885254859924, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/200, Loss=0.137547005712986
Loss made of: CE 0.19202248752117157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/200, Loss=0.15999046787619592
Loss made of: CE 0.11937403678894043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/200, Loss=0.1459694355726242
Loss made of: CE 0.12177982181310654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/200, Loss=0.13670825958251953
Loss made of: CE 0.11976718157529831, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/200, Loss=0.1527685508131981
Loss made of: CE 0.17217662930488586, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.14294062554836273, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.14294062554836273, Class Loss=0.14294062554836273, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.005998
Epoch 2, Batch 10/200, Loss=0.13272676095366479
Loss made of: CE 0.13940781354904175, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/200, Loss=0.12724756076931953
Loss made of: CE 0.12487509101629257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/200, Loss=0.13330674320459365
Loss made of: CE 0.1647762507200241, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/200, Loss=0.14049545526504517
Loss made of: CE 0.12733016908168793, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/200, Loss=0.13350869864225387
Loss made of: CE 0.15051871538162231, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/200, Loss=0.1394938200712204
Loss made of: CE 0.100677490234375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/200, Loss=0.12644778937101364
Loss made of: CE 0.0949820727109909, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/200, Loss=0.14390433132648467
Loss made of: CE 0.17736396193504333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/200, Loss=0.11075121611356735
Loss made of: CE 0.14319340884685516, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/200, Loss=0.12653702348470688
Loss made of: CE 0.14659255743026733, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/200, Loss=0.12905299812555313
Loss made of: CE 0.12042149156332016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/200, Loss=0.127511228621006
Loss made of: CE 0.1434110701084137, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/200, Loss=0.1327168218791485
Loss made of: CE 0.1154293566942215, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/200, Loss=0.14643370658159255
Loss made of: CE 0.1881433129310608, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/200, Loss=0.14530822560191153
Loss made of: CE 0.0989149808883667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/200, Loss=0.14181202724575998
Loss made of: CE 0.15568017959594727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/200, Loss=0.1344473399221897
Loss made of: CE 0.1330842524766922, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/200, Loss=0.12943973615765572
Loss made of: CE 0.1678214967250824, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/200, Loss=0.14971281215548515
Loss made of: CE 0.17831110954284668, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/200, Loss=0.15106670409440995
Loss made of: CE 0.22518980503082275, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.13509605824947357, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.13509605824947357, Class Loss=0.13509605824947357, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.005679
Epoch 3, Batch 10/200, Loss=0.11712202429771423
Loss made of: CE 0.13587288558483124, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/200, Loss=0.13952186927199364
Loss made of: CE 0.15799641609191895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/200, Loss=0.1279684193432331
Loss made of: CE 0.12910108268260956, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/200, Loss=0.1264389805495739
Loss made of: CE 0.13577690720558167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/200, Loss=0.1276750572025776
Loss made of: CE 0.11748506873846054, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/200, Loss=0.12943007946014404
Loss made of: CE 0.15433615446090698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/200, Loss=0.11995181366801262
Loss made of: CE 0.12020047754049301, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/200, Loss=0.12372298762202263
Loss made of: CE 0.11989929527044296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/200, Loss=0.12163589894771576
Loss made of: CE 0.07948633283376694, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/200, Loss=0.12217426523566247
Loss made of: CE 0.13043761253356934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/200, Loss=0.1103728249669075
Loss made of: CE 0.1057543158531189, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/200, Loss=0.12294313386082649
Loss made of: CE 0.10203182697296143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/200, Loss=0.12461751997470856
Loss made of: CE 0.12747901678085327, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/200, Loss=0.13340115770697594
Loss made of: CE 0.16381129622459412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/200, Loss=0.1264955647289753
Loss made of: CE 0.09876076877117157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/200, Loss=0.13539212793111802
Loss made of: CE 0.12202344834804535, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/200, Loss=0.11809111386537552
Loss made of: CE 0.11458264291286469, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/200, Loss=0.1244136780500412
Loss made of: CE 0.10326099395751953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/200, Loss=0.11872169673442841
Loss made of: CE 0.14962510764598846, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/200, Loss=0.13316142559051514
Loss made of: CE 0.12565413117408752, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.12516258656978607, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.12516258656978607, Class Loss=0.12516258656978607, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/200, Loss=0.12573894411325454
Loss made of: CE 0.12105663865804672, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/200, Loss=0.1075180046260357
Loss made of: CE 0.1086915135383606, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/200, Loss=0.10987187325954437
Loss made of: CE 0.1009829044342041, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/200, Loss=0.11559487134218216
Loss made of: CE 0.1100044995546341, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/200, Loss=0.12263514921069145
Loss made of: CE 0.14283975958824158, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/200, Loss=0.10901439636945724
Loss made of: CE 0.13153409957885742, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/200, Loss=0.12321835681796074
Loss made of: CE 0.12139957398176193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/200, Loss=0.1218449391424656
Loss made of: CE 0.10249682515859604, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/200, Loss=0.11535636782646179
Loss made of: CE 0.11953817307949066, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/200, Loss=0.11442252546548844
Loss made of: CE 0.10051661729812622, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/200, Loss=0.10977939814329148
Loss made of: CE 0.09667758643627167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/200, Loss=0.11977461725473404
Loss made of: CE 0.11559106409549713, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/200, Loss=0.11012763753533364
Loss made of: CE 0.09501683712005615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/200, Loss=0.11849303841590882
Loss made of: CE 0.09465311467647552, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/200, Loss=0.13014675974845885
Loss made of: CE 0.11584332585334778, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/200, Loss=0.12583990395069122
Loss made of: CE 0.11098935455083847, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/200, Loss=0.11435715183615684
Loss made of: CE 0.11346806585788727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/200, Loss=0.11922675669193268
Loss made of: CE 0.12638738751411438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/200, Loss=0.1207824245095253
Loss made of: CE 0.12979429960250854, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/200, Loss=0.1141108863055706
Loss made of: CE 0.10715334862470627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.11739269644021988, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.11739269644021988, Class Loss=0.11739269644021988, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.005036
Epoch 5, Batch 10/200, Loss=0.11948855370283126
Loss made of: CE 0.12347125262022018, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/200, Loss=0.12320116609334945
Loss made of: CE 0.12602338194847107, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/200, Loss=0.11146885603666305
Loss made of: CE 0.13810503482818604, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/200, Loss=0.11152711063623429
Loss made of: CE 0.12300407886505127, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/200, Loss=0.11710311397910118
Loss made of: CE 0.16408202052116394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/200, Loss=0.10778933241963387
Loss made of: CE 0.11157521605491638, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/200, Loss=0.11597592756152153
Loss made of: CE 0.10045114159584045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/200, Loss=0.11979775726795197
Loss made of: CE 0.08492152392864227, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/200, Loss=0.11634311825037003
Loss made of: CE 0.08875934779644012, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/200, Loss=0.11852271556854248
Loss made of: CE 0.1026102751493454, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/200, Loss=0.1020616427063942
Loss made of: CE 0.1047477126121521, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/200, Loss=0.10773789137601852
Loss made of: CE 0.11234241724014282, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/200, Loss=0.10121198147535324
Loss made of: CE 0.10219913721084595, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/200, Loss=0.11781847178936004
Loss made of: CE 0.1244269460439682, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/200, Loss=0.11743314489722252
Loss made of: CE 0.11725041270256042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/200, Loss=0.11795644313097
Loss made of: CE 0.13001608848571777, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/200, Loss=0.12080324366688729
Loss made of: CE 0.14329937100410461, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/200, Loss=0.10597799569368363
Loss made of: CE 0.09753452986478806, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/200, Loss=0.11345329955220222
Loss made of: CE 0.12219547480344772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/200, Loss=0.11198530867695808
Loss made of: CE 0.09331601858139038, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.11388285458087921, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.11388285458087921, Class Loss=0.11388285458087921, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.004711
Epoch 6, Batch 10/200, Loss=0.11026760041713715
Loss made of: CE 0.10735844075679779, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/200, Loss=0.11243971809744835
Loss made of: CE 0.11814248561859131, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/200, Loss=0.11463042497634887
Loss made of: CE 0.1407165229320526, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/200, Loss=0.10398281440138817
Loss made of: CE 0.09502393007278442, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/200, Loss=0.100334682315588
Loss made of: CE 0.10728223621845245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/200, Loss=0.10354413837194443
Loss made of: CE 0.0922446995973587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/200, Loss=0.10187224075198173
Loss made of: CE 0.0857023224234581, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/200, Loss=0.12209700122475624
Loss made of: CE 0.13474224507808685, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/200, Loss=0.10764394402503967
Loss made of: CE 0.09186875820159912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/200, Loss=0.11653394475579262
Loss made of: CE 0.130281001329422, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/200, Loss=0.10856567695736885
Loss made of: CE 0.1338314414024353, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/200, Loss=0.11407595351338387
Loss made of: CE 0.09257814288139343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/200, Loss=0.12297161146998406
Loss made of: CE 0.11866148561239243, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/200, Loss=0.112690881639719
Loss made of: CE 0.1257036328315735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/200, Loss=0.11988417357206345
Loss made of: CE 0.11077295988798141, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/200, Loss=0.11686067804694175
Loss made of: CE 0.08718323707580566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/200, Loss=0.1074196457862854
Loss made of: CE 0.12442182004451752, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/200, Loss=0.10221868082880974
Loss made of: CE 0.14018867909908295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/200, Loss=0.11766062006354332
Loss made of: CE 0.12294679880142212, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/200, Loss=0.12003788277506829
Loss made of: CE 0.12533064186573029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1117866113781929, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.1117866113781929, Class Loss=0.1117866113781929, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/157, Loss=0.15666443556547166
Loss made of: CE 0.14866650104522705, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=0.1402856469154358
Loss made of: CE 0.10016186535358429, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.1357472538948059
Loss made of: CE 0.17764800786972046, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.11188463345170022
Loss made of: CE 0.09142537415027618, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.12623735070228576
Loss made of: CE 0.08069564402103424, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.14093848019838334
Loss made of: CE 0.12922507524490356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.13481768146157264
Loss made of: CE 0.14295262098312378, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.12823315113782882
Loss made of: CE 0.11464637517929077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.12072509974241256
Loss made of: CE 0.1524522602558136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.12330140322446823
Loss made of: CE 0.10774761438369751, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.11913502961397171
Loss made of: CE 0.10272811353206635, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.1301306091248989
Loss made of: CE 0.12766805291175842, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.11420046612620353
Loss made of: CE 0.12985745072364807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.1310035802423954
Loss made of: CE 0.1242605671286583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.12138664871454238
Loss made of: CE 0.12514157593250275, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.12928403913974762, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.12928403913974762, Class Loss=0.12928403913974762, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.005998
Epoch 2, Batch 10/157, Loss=0.10769502818584442
Loss made of: CE 0.10576623678207397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.12669689506292342
Loss made of: CE 0.10167388617992401, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.12723905816674233
Loss made of: CE 0.115793876349926, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.10862787216901779
Loss made of: CE 0.0906113013625145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.11271901726722718
Loss made of: CE 0.08549736440181732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.11874855309724808
Loss made of: CE 0.14019063115119934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.11197221428155898
Loss made of: CE 0.1150696724653244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.1083385244011879
Loss made of: CE 0.10313557088375092, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.11700470075011253
Loss made of: CE 0.12172645330429077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.11187072321772576
Loss made of: CE 0.10208195447921753, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.09831395372748375
Loss made of: CE 0.10924004018306732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.11153362318873405
Loss made of: CE 0.11171853542327881, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.11559465900063515
Loss made of: CE 0.1245947778224945, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.10129089206457138
Loss made of: CE 0.09202808141708374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.12120961472392082
Loss made of: CE 0.11523677408695221, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.11365941911935806, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.11365941911935806, Class Loss=0.11365941911935806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.005679
Epoch 3, Batch 10/157, Loss=0.10343111008405685
Loss made of: CE 0.10618355125188828, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.09978316500782966
Loss made of: CE 0.07804529368877411, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.10379857793450356
Loss made of: CE 0.11135639995336533, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.10784865468740464
Loss made of: CE 0.09683199971914291, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.11218337342143059
Loss made of: CE 0.1254976987838745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.10291673466563225
Loss made of: CE 0.10473424196243286, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.11540808230638504
Loss made of: CE 0.08970394730567932, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.10069008022546769
Loss made of: CE 0.10811452567577362, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.09575074091553688
Loss made of: CE 0.11699914932250977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.10489638894796371
Loss made of: CE 0.10645382851362228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.11496188938617706
Loss made of: CE 0.11650803685188293, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.10291792750358582
Loss made of: CE 0.1108035296201706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.11232982501387596
Loss made of: CE 0.14893901348114014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.12431520819664002
Loss made of: CE 0.12178540229797363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.09825781658291817
Loss made of: CE 0.09053241461515427, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10674459487199783, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.10674459487199783, Class Loss=0.10674459487199783, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/157, Loss=0.09622789248824119
Loss made of: CE 0.11698250472545624, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.10260453373193741
Loss made of: CE 0.07958616316318512, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.1131107896566391
Loss made of: CE 0.12882128357887268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.09225677326321602
Loss made of: CE 0.08540850132703781, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.1102097675204277
Loss made of: CE 0.12453530728816986, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.10510659068822861
Loss made of: CE 0.1374128758907318, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.09891825765371323
Loss made of: CE 0.0953744575381279, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.10991977155208588
Loss made of: CE 0.10324370861053467, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.10530906692147254
Loss made of: CE 0.09120821207761765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.11066353172063828
Loss made of: CE 0.10947293043136597, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.10915595293045044
Loss made of: CE 0.09741771221160889, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.09302421435713767
Loss made of: CE 0.07721209526062012, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.10493247210979462
Loss made of: CE 0.1266385316848755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.11433185338973999
Loss made of: CE 0.08507116883993149, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.09920493736863137
Loss made of: CE 0.10340549796819687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10367260873317719, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.10367260873317719, Class Loss=0.10367260873317719, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.005036
Epoch 5, Batch 10/157, Loss=0.08759161233901977
Loss made of: CE 0.0942903384566307, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.09646570980548859
Loss made of: CE 0.09436112642288208, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.11211481839418411
Loss made of: CE 0.2056109756231308, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.09023001007735729
Loss made of: CE 0.114530548453331, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.09283006042242051
Loss made of: CE 0.07905098795890808, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.10086692497134209
Loss made of: CE 0.11491944640874863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.08861962780356407
Loss made of: CE 0.06854023784399033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.10223934277892113
Loss made of: CE 0.09967061132192612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.09996171146631241
Loss made of: CE 0.1051269993185997, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.10969523563981057
Loss made of: CE 0.1036612018942833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.09959159716963768
Loss made of: CE 0.07227014750242233, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.09858565106987953
Loss made of: CE 0.10997562110424042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.10520986765623093
Loss made of: CE 0.09710739552974701, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.09509759470820427
Loss made of: CE 0.09064420312643051, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.10428201109170913
Loss made of: CE 0.0834263265132904, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0987531915307045, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.0987531915307045, Class Loss=0.0987531915307045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.004711
Epoch 6, Batch 10/157, Loss=0.10290373265743255
Loss made of: CE 0.08391174674034119, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.09354478940367698
Loss made of: CE 0.08648612350225449, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.09673735573887825
Loss made of: CE 0.12855830788612366, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.0860188752412796
Loss made of: CE 0.06520266830921173, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.09707980528473854
Loss made of: CE 0.08734974265098572, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.08848879784345627
Loss made of: CE 0.07362164556980133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.09838818833231926
Loss made of: CE 0.09077639877796173, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.08584232777357101
Loss made of: CE 0.07931235432624817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.0962315820157528
Loss made of: CE 0.08979298174381256, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.09496680721640587
Loss made of: CE 0.07518397271633148, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.09360421299934388
Loss made of: CE 0.09026902168989182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.09974531717598438
Loss made of: CE 0.05796289071440697, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.10204781815409661
Loss made of: CE 0.13873201608657837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.10589314848184586
Loss made of: CE 0.14208278059959412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.09455567747354507
Loss made of: CE 0.09017624706029892, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09582873433828354, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.09582873433828354, Class Loss=0.09582873433828354, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.14748837053775787, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.948965
Mean Acc: 0.876175
FreqW Acc: 0.907663
Mean IoU: 0.782762
Class IoU:
	class 0: 0.94141954
	class 1: 0.9081853
	class 2: 0.4177852
	class 3: 0.88766587
	class 4: 0.7069289
	class 5: 0.7965567
	class 6: 0.9323709
	class 7: 0.91185117
	class 8: 0.9149782
	class 9: 0.42525613
	class 10: 0.7179885
	class 11: 0.60759324
	class 12: 0.8754085
	class 13: 0.7391525
	class 14: 0.87261677
	class 15: 0.86844194
Class Acc:
	class 0: 0.9711638
	class 1: 0.9562735
	class 2: 0.9049042
	class 3: 0.9248478
	class 4: 0.85836816
	class 5: 0.8848101
	class 6: 0.959933
	class 7: 0.94269216
	class 8: 0.96266764
	class 9: 0.5547845
	class 10: 0.75397766
	class 11: 0.6358341
	class 12: 0.9553429
	class 13: 0.88737106
	class 14: 0.94083816
	class 15: 0.92498434

federated global round: 3, step: 0
select part of clients to conduct local training
[9, 5, 0, 3]
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/205, Loss=0.12161658778786659
Loss made of: CE 0.09503091871738434, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/205, Loss=0.12895311638712884
Loss made of: CE 0.08726662397384644, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/205, Loss=0.12681357711553573
Loss made of: CE 0.12720513343811035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/205, Loss=0.11819368898868561
Loss made of: CE 0.15144997835159302, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/205, Loss=0.13598278984427453
Loss made of: CE 0.1352354884147644, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/205, Loss=0.14758940562605857
Loss made of: CE 0.12594150006771088, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/205, Loss=0.12761566787958145
Loss made of: CE 0.09786572307348251, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/205, Loss=0.1348594233393669
Loss made of: CE 0.1719355583190918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/205, Loss=0.1191500723361969
Loss made of: CE 0.09603725373744965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/205, Loss=0.12660820111632348
Loss made of: CE 0.09932897984981537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/205, Loss=0.12362692803144455
Loss made of: CE 0.11140042543411255, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/205, Loss=0.1371066153049469
Loss made of: CE 0.120797298848629, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/205, Loss=0.13015434220433236
Loss made of: CE 0.10727053880691528, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/205, Loss=0.12054664939641953
Loss made of: CE 0.08985739946365356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/205, Loss=0.11965066120028496
Loss made of: CE 0.1260150671005249, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/205, Loss=0.11830396056175232
Loss made of: CE 0.13506031036376953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/205, Loss=0.12650945633649827
Loss made of: CE 0.14302778244018555, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/205, Loss=0.12431226670742035
Loss made of: CE 0.11442701518535614, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/205, Loss=0.12869565710425376
Loss made of: CE 0.11609362810850143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/205, Loss=0.12671561539173126
Loss made of: CE 0.125496968626976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.12702102959156036, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.12702102959156036, Class Loss=0.12702102959156036, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.005839
Epoch 2, Batch 10/205, Loss=0.1294579304754734
Loss made of: CE 0.17255625128746033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/205, Loss=0.1294965960085392
Loss made of: CE 0.12839137017726898, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/205, Loss=0.11762218177318573
Loss made of: CE 0.12305265665054321, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/205, Loss=0.12636946588754655
Loss made of: CE 0.11601993441581726, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/205, Loss=0.12175104767084122
Loss made of: CE 0.11584043502807617, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/205, Loss=0.13463366255164147
Loss made of: CE 0.14650270342826843, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/205, Loss=0.13536717966198922
Loss made of: CE 0.22725114226341248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/205, Loss=0.10653392970561981
Loss made of: CE 0.09422217309474945, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/205, Loss=0.11659917458891869
Loss made of: CE 0.13522303104400635, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/205, Loss=0.12994223535060884
Loss made of: CE 0.11104956269264221, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/205, Loss=0.12279241532087326
Loss made of: CE 0.10427473485469818, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/205, Loss=0.11188407018780708
Loss made of: CE 0.12576787173748016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/205, Loss=0.1186977781355381
Loss made of: CE 0.11012379825115204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/205, Loss=0.11446025967597961
Loss made of: CE 0.09804477542638779, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/205, Loss=0.1246537446975708
Loss made of: CE 0.15590737760066986, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/205, Loss=0.12076876908540726
Loss made of: CE 0.09713529050350189, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/205, Loss=0.11419806405901908
Loss made of: CE 0.15986531972885132, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/205, Loss=0.11205294281244278
Loss made of: CE 0.08213235437870026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/205, Loss=0.10899170562624931
Loss made of: CE 0.11725613474845886, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/205, Loss=0.12318282723426818
Loss made of: CE 0.13029350340366364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.12061398476362228, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.12061398476362228, Class Loss=0.12061398476362228, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.005359
Epoch 3, Batch 10/205, Loss=0.1030948780477047
Loss made of: CE 0.12367501109838486, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/205, Loss=0.10474048256874084
Loss made of: CE 0.08846994489431381, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/205, Loss=0.11203866899013519
Loss made of: CE 0.10730405896902084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/205, Loss=0.10640229433774948
Loss made of: CE 0.10315807908773422, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/205, Loss=0.10400627925992012
Loss made of: CE 0.09381775557994843, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/205, Loss=0.1068305291235447
Loss made of: CE 0.10811123251914978, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/205, Loss=0.11800361350178719
Loss made of: CE 0.09463916718959808, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/205, Loss=0.1159946583211422
Loss made of: CE 0.15036001801490784, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/205, Loss=0.10912945047020912
Loss made of: CE 0.11457107961177826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/205, Loss=0.11073312386870385
Loss made of: CE 0.10626807063817978, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/205, Loss=0.11054906696081161
Loss made of: CE 0.13459861278533936, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/205, Loss=0.10644392892718316
Loss made of: CE 0.1109004020690918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/205, Loss=0.12057642042636871
Loss made of: CE 0.12796932458877563, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/205, Loss=0.11450710445642472
Loss made of: CE 0.12838055193424225, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/205, Loss=0.11213798671960831
Loss made of: CE 0.105034738779068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/205, Loss=0.12093129605054856
Loss made of: CE 0.12271255254745483, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/205, Loss=0.11021970063447953
Loss made of: CE 0.11156080663204193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/205, Loss=0.10690399557352066
Loss made of: CE 0.10768498480319977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/205, Loss=0.12055051401257515
Loss made of: CE 0.14842817187309265, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/205, Loss=0.10959018990397454
Loss made of: CE 0.14093126356601715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.11079569905996323, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.11079569905996323, Class Loss=0.11079569905996323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.004874
Epoch 4, Batch 10/205, Loss=0.11118144541978836
Loss made of: CE 0.10162290185689926, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/205, Loss=0.11017868667840958
Loss made of: CE 0.09646805375814438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/205, Loss=0.10895266905426979
Loss made of: CE 0.08250647783279419, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/205, Loss=0.10414696559309959
Loss made of: CE 0.0842149406671524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/205, Loss=0.10934173688292503
Loss made of: CE 0.07815587520599365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/205, Loss=0.09823822379112243
Loss made of: CE 0.11447818577289581, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/205, Loss=0.10413735583424569
Loss made of: CE 0.10097324848175049, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/205, Loss=0.09826129376888275
Loss made of: CE 0.11176782101392746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/205, Loss=0.11793431863188744
Loss made of: CE 0.09919990599155426, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/205, Loss=0.09754701256752014
Loss made of: CE 0.10458268970251083, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/205, Loss=0.10740175396203995
Loss made of: CE 0.11535851657390594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/205, Loss=0.10328320190310478
Loss made of: CE 0.12940376996994019, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/205, Loss=0.10883088558912277
Loss made of: CE 0.09261302649974823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/205, Loss=0.10680907145142556
Loss made of: CE 0.11833955347537994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/205, Loss=0.10929068922996521
Loss made of: CE 0.09087103605270386, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/205, Loss=0.11839514151215554
Loss made of: CE 0.1019604429602623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/205, Loss=0.1060338594019413
Loss made of: CE 0.10346626490354538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/205, Loss=0.11198463141918183
Loss made of: CE 0.10223372280597687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/205, Loss=0.10543703511357308
Loss made of: CE 0.11091096699237823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/205, Loss=0.09650013446807862
Loss made of: CE 0.11292313784360886, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10678896307945251, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.10678896307945251, Class Loss=0.10678896307945251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.004384
Epoch 5, Batch 10/205, Loss=0.1082270585000515
Loss made of: CE 0.14026504755020142, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/205, Loss=0.11398961469531059
Loss made of: CE 0.11088636517524719, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/205, Loss=0.1089689664542675
Loss made of: CE 0.08879416435956955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/205, Loss=0.09759158864617348
Loss made of: CE 0.08937474340200424, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/205, Loss=0.09861537665128708
Loss made of: CE 0.0811198279261589, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/205, Loss=0.10631169527769088
Loss made of: CE 0.10903051495552063, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/205, Loss=0.1032511256635189
Loss made of: CE 0.09520016610622406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/205, Loss=0.09268398508429528
Loss made of: CE 0.0819234549999237, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/205, Loss=0.10346875563263894
Loss made of: CE 0.1106526255607605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/205, Loss=0.0989773154258728
Loss made of: CE 0.09681856632232666, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/205, Loss=0.10271668955683708
Loss made of: CE 0.11920440196990967, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/205, Loss=0.10352929905056954
Loss made of: CE 0.09390699863433838, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/205, Loss=0.09646707251667977
Loss made of: CE 0.08776728808879852, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/205, Loss=0.09906161576509476
Loss made of: CE 0.1482488214969635, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/205, Loss=0.10358971878886222
Loss made of: CE 0.1049504280090332, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/205, Loss=0.10059592053294182
Loss made of: CE 0.08076734840869904, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/205, Loss=0.09978259056806564
Loss made of: CE 0.09012114256620407, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/205, Loss=0.1053106166422367
Loss made of: CE 0.06988389790058136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/205, Loss=0.10223297476768493
Loss made of: CE 0.09309874475002289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/205, Loss=0.10536643862724304
Loss made of: CE 0.08428305387496948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1022859513759613, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.1022859513759613, Class Loss=0.1022859513759613, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.003887
Epoch 6, Batch 10/205, Loss=0.09530469626188279
Loss made of: CE 0.09656769037246704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/205, Loss=0.10605910941958427
Loss made of: CE 0.09817056357860565, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/205, Loss=0.1079803854227066
Loss made of: CE 0.10576729476451874, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/205, Loss=0.10272234007716179
Loss made of: CE 0.12521782517433167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/205, Loss=0.09932873919606208
Loss made of: CE 0.10118591785430908, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/205, Loss=0.09589313715696335
Loss made of: CE 0.0689648985862732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/205, Loss=0.10925763919949531
Loss made of: CE 0.08662150800228119, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/205, Loss=0.09796728417277337
Loss made of: CE 0.09053536504507065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/205, Loss=0.10578686073422432
Loss made of: CE 0.09454405307769775, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/205, Loss=0.10607203170657158
Loss made of: CE 0.07915695756673813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/205, Loss=0.09093230217695236
Loss made of: CE 0.08402252197265625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/205, Loss=0.08799991458654403
Loss made of: CE 0.08483608812093735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/205, Loss=0.09624233245849609
Loss made of: CE 0.09880028665065765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/205, Loss=0.10090772956609725
Loss made of: CE 0.07847794890403748, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/205, Loss=0.09127030447125435
Loss made of: CE 0.08343689143657684, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/205, Loss=0.08651581779122353
Loss made of: CE 0.0794481486082077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/205, Loss=0.10275886952877045
Loss made of: CE 0.1002403125166893, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/205, Loss=0.0941610038280487
Loss made of: CE 0.0747271478176117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/205, Loss=0.10397695749998093
Loss made of: CE 0.08601771295070648, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/205, Loss=0.1011532947421074
Loss made of: CE 0.09436851739883423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09955272823572159, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.09955272823572159, Class Loss=0.09955272823572159, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/245, Loss=0.10984606221318245
Loss made of: CE 0.12195001542568207, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/245, Loss=0.15313652604818345
Loss made of: CE 0.19519928097724915, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/245, Loss=0.14060842394828796
Loss made of: CE 0.11046004295349121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/245, Loss=0.14995240569114685
Loss made of: CE 0.1308063566684723, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/245, Loss=0.16229734495282172
Loss made of: CE 0.21131473779678345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/245, Loss=0.15437410473823548
Loss made of: CE 0.12408693879842758, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/245, Loss=0.16699089705944062
Loss made of: CE 0.15108174085617065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/245, Loss=0.1530307061970234
Loss made of: CE 0.1711081713438034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/245, Loss=0.17187410593032837
Loss made of: CE 0.19425193965435028, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/245, Loss=0.16449120938777922
Loss made of: CE 0.20412150025367737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/245, Loss=0.17775324136018752
Loss made of: CE 0.21464727818965912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/245, Loss=0.1606837436556816
Loss made of: CE 0.14010250568389893, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/245, Loss=0.191324283182621
Loss made of: CE 0.1387476921081543, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/245, Loss=0.18894360214471817
Loss made of: CE 0.17303746938705444, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/245, Loss=0.16887427270412445
Loss made of: CE 0.20654642581939697, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/245, Loss=0.2000352829694748
Loss made of: CE 0.2435460090637207, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/245, Loss=0.18408759087324142
Loss made of: CE 0.1885393112897873, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/245, Loss=0.17126131653785706
Loss made of: CE 0.13094675540924072, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/245, Loss=0.18616426438093187
Loss made of: CE 0.1667337864637375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/245, Loss=0.18257599472999572
Loss made of: CE 0.25903254747390747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 210/245, Loss=0.17028958722949028
Loss made of: CE 0.23239830136299133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 220/245, Loss=0.18306447863578795
Loss made of: CE 0.2772684097290039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 230/245, Loss=0.15846362113952636
Loss made of: CE 0.14293408393859863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 240/245, Loss=0.17263469249010086
Loss made of: CE 0.16531285643577576, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.16754737496376038, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.16754737496376038, Class Loss=0.16754737496376038, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009247
Epoch 2, Batch 10/245, Loss=0.14116077572107316
Loss made of: CE 0.14307084679603577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/245, Loss=0.1547111041843891
Loss made of: CE 0.171917125582695, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/245, Loss=0.14950127974152566
Loss made of: CE 0.12764161825180054, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/245, Loss=0.14291759207844734
Loss made of: CE 0.12362085282802582, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/245, Loss=0.13606915920972823
Loss made of: CE 0.14153866469860077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/245, Loss=0.16242059767246247
Loss made of: CE 0.1542433202266693, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/245, Loss=0.1433245487511158
Loss made of: CE 0.2225198745727539, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/245, Loss=0.16907671689987183
Loss made of: CE 0.2583872675895691, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/245, Loss=0.15462152361869813
Loss made of: CE 0.13676325976848602, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/245, Loss=0.15025173276662826
Loss made of: CE 0.14470118284225464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/245, Loss=0.17316581606864928
Loss made of: CE 0.12574023008346558, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/245, Loss=0.1419347643852234
Loss made of: CE 0.1608094871044159, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/245, Loss=0.16304637119174004
Loss made of: CE 0.12729178369045258, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/245, Loss=0.14193679317831992
Loss made of: CE 0.10806538164615631, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/245, Loss=0.15996105298399926
Loss made of: CE 0.1447848528623581, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/245, Loss=0.1621874324977398
Loss made of: CE 0.18676674365997314, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/245, Loss=0.17148371934890747
Loss made of: CE 0.19390878081321716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/245, Loss=0.16265389174222947
Loss made of: CE 0.16615015268325806, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/245, Loss=0.16266104876995086
Loss made of: CE 0.13646486401557922, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/245, Loss=0.14682016521692276
Loss made of: CE 0.15601858496665955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 210/245, Loss=0.15445475727319719
Loss made of: CE 0.1407642662525177, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 220/245, Loss=0.13662674352526666
Loss made of: CE 0.1628204882144928, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 230/245, Loss=0.14882088974118232
Loss made of: CE 0.17561137676239014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 240/245, Loss=0.13847742453217507
Loss made of: CE 0.22263619303703308, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.15393635630607605, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.15393635630607605, Class Loss=0.15393635630607605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.008487
Epoch 3, Batch 10/245, Loss=0.14434304237365722
Loss made of: CE 0.12514381110668182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/245, Loss=0.1369272477924824
Loss made of: CE 0.233859583735466, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/245, Loss=0.1395503304898739
Loss made of: CE 0.14823290705680847, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/245, Loss=0.12242581248283387
Loss made of: CE 0.11934765428304672, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/245, Loss=0.1307621493935585
Loss made of: CE 0.11092420667409897, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/245, Loss=0.1262269340455532
Loss made of: CE 0.10994105041027069, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/245, Loss=0.1279934324324131
Loss made of: CE 0.17005211114883423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/245, Loss=0.14310033544898032
Loss made of: CE 0.12372761964797974, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/245, Loss=0.13829140663146972
Loss made of: CE 0.15663142502307892, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/245, Loss=0.12665279135107993
Loss made of: CE 0.1242184042930603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/245, Loss=0.12425518557429313
Loss made of: CE 0.16979819536209106, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/245, Loss=0.1440598860383034
Loss made of: CE 0.12686088681221008, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/245, Loss=0.1542813576757908
Loss made of: CE 0.09933563321828842, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/245, Loss=0.1266355536878109
Loss made of: CE 0.12384721636772156, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/245, Loss=0.14931245297193527
Loss made of: CE 0.23219852149486542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/245, Loss=0.17326574325561522
Loss made of: CE 0.2284034788608551, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/245, Loss=0.15000054538249968
Loss made of: CE 0.1632181853055954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/245, Loss=0.14579565376043319
Loss made of: CE 0.17566311359405518, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/245, Loss=0.13686372339725494
Loss made of: CE 0.12734529376029968, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/245, Loss=0.12531272768974305
Loss made of: CE 0.13508163392543793, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 210/245, Loss=0.11857217997312545
Loss made of: CE 0.1054634377360344, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 220/245, Loss=0.1223160482943058
Loss made of: CE 0.13450363278388977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 230/245, Loss=0.1322885423898697
Loss made of: CE 0.0996628925204277, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 240/245, Loss=0.14319888278841972
Loss made of: CE 0.1656782180070877, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1363913118839264, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.1363913118839264, Class Loss=0.1363913118839264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007719
Epoch 4, Batch 10/245, Loss=0.11523289307951927
Loss made of: CE 0.1173424944281578, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/245, Loss=0.115448197722435
Loss made of: CE 0.13049694895744324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/245, Loss=0.10919839218258857
Loss made of: CE 0.10258156061172485, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/245, Loss=0.1183745339512825
Loss made of: CE 0.10193116962909698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/245, Loss=0.1242680735886097
Loss made of: CE 0.1334155648946762, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/245, Loss=0.1260470986366272
Loss made of: CE 0.19628319144248962, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/245, Loss=0.12398747056722641
Loss made of: CE 0.11583717912435532, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/245, Loss=0.11948161348700523
Loss made of: CE 0.09182555973529816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/245, Loss=0.11645253449678421
Loss made of: CE 0.15464745461940765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/245, Loss=0.13447313234210015
Loss made of: CE 0.21522049605846405, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/245, Loss=0.11106827110052109
Loss made of: CE 0.11045528948307037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/245, Loss=0.12875136584043503
Loss made of: CE 0.09922172129154205, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/245, Loss=0.12425675243139267
Loss made of: CE 0.16066086292266846, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/245, Loss=0.10648670122027397
Loss made of: CE 0.12762191891670227, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/245, Loss=0.11595901027321816
Loss made of: CE 0.10959945619106293, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/245, Loss=0.12255840748548508
Loss made of: CE 0.11415575444698334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/245, Loss=0.11314265578985214
Loss made of: CE 0.09560219943523407, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/245, Loss=0.11268863976001739
Loss made of: CE 0.14628294110298157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/245, Loss=0.11003738939762116
Loss made of: CE 0.12055498361587524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/245, Loss=0.10982904359698295
Loss made of: CE 0.1314230114221573, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 210/245, Loss=0.10849981047213078
Loss made of: CE 0.05861074849963188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 220/245, Loss=0.11235277503728866
Loss made of: CE 0.1071598157286644, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 230/245, Loss=0.12361710667610168
Loss made of: CE 0.10529179871082306, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 240/245, Loss=0.10918773040175438
Loss made of: CE 0.11544470489025116, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1171034574508667, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.1171034574508667, Class Loss=0.1171034574508667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/245, Loss=0.1092243731021881
Loss made of: CE 0.07229392230510712, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/245, Loss=0.10122845694422722
Loss made of: CE 0.09106241911649704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/245, Loss=0.10474954321980476
Loss made of: CE 0.15227310359477997, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/245, Loss=0.10525486543774605
Loss made of: CE 0.11124476045370102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/245, Loss=0.10476489290595055
Loss made of: CE 0.11711962521076202, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/245, Loss=0.10335772037506104
Loss made of: CE 0.11507143080234528, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/245, Loss=0.0981607936322689
Loss made of: CE 0.09689273685216904, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/245, Loss=0.10892309918999672
Loss made of: CE 0.08369992673397064, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/245, Loss=0.10289020575582981
Loss made of: CE 0.05961240455508232, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/245, Loss=0.10240739211440086
Loss made of: CE 0.09730847924947739, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/245, Loss=0.11490574702620507
Loss made of: CE 0.16155043244361877, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/245, Loss=0.10041835531592369
Loss made of: CE 0.07679662108421326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/245, Loss=0.10550816506147384
Loss made of: CE 0.09256206452846527, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/245, Loss=0.10907281488180161
Loss made of: CE 0.08689852803945541, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/245, Loss=0.10248576998710632
Loss made of: CE 0.08436493575572968, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/245, Loss=0.09293334260582924
Loss made of: CE 0.11090545356273651, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/245, Loss=0.09394960850477219
Loss made of: CE 0.10503213107585907, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/245, Loss=0.09937945753335953
Loss made of: CE 0.10155312716960907, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/245, Loss=0.10141742751002311
Loss made of: CE 0.10467539727687836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/245, Loss=0.10662937089800835
Loss made of: CE 0.1389865130186081, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 210/245, Loss=0.1110247939825058
Loss made of: CE 0.09966737031936646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 220/245, Loss=0.10037919357419015
Loss made of: CE 0.11163982003927231, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 230/245, Loss=0.10827082768082619
Loss made of: CE 0.1098044142127037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 240/245, Loss=0.11130796223878861
Loss made of: CE 0.1593930721282959, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10409090667963028, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.10409090667963028, Class Loss=0.10409090667963028, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006156
Epoch 6, Batch 10/245, Loss=0.10626352056860924
Loss made of: CE 0.1249491423368454, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/245, Loss=0.10545210614800453
Loss made of: CE 0.1327669322490692, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/245, Loss=0.09749666303396225
Loss made of: CE 0.09167073667049408, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/245, Loss=0.10093743205070496
Loss made of: CE 0.11972801387310028, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/245, Loss=0.10063055381178856
Loss made of: CE 0.12254898250102997, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/245, Loss=0.11372059136629105
Loss made of: CE 0.12170674651861191, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/245, Loss=0.09500081911683082
Loss made of: CE 0.07343096286058426, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/245, Loss=0.10258050188422203
Loss made of: CE 0.09018345922231674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/245, Loss=0.09754543155431747
Loss made of: CE 0.11963158845901489, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/245, Loss=0.09223922565579415
Loss made of: CE 0.09288975596427917, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/245, Loss=0.09276796579360962
Loss made of: CE 0.09567403048276901, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/245, Loss=0.10167084559798241
Loss made of: CE 0.1083308607339859, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/245, Loss=0.11078094094991683
Loss made of: CE 0.09886269271373749, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/245, Loss=0.10371545031666755
Loss made of: CE 0.1277981698513031, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/245, Loss=0.09655738100409508
Loss made of: CE 0.11545910686254501, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/245, Loss=0.09607139304280281
Loss made of: CE 0.07521624863147736, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/245, Loss=0.09296047389507293
Loss made of: CE 0.10850980132818222, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/245, Loss=0.09581985250115395
Loss made of: CE 0.11591856181621552, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/245, Loss=0.10141579061746597
Loss made of: CE 0.0880332738161087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/245, Loss=0.09640894681215287
Loss made of: CE 0.09927621483802795, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 210/245, Loss=0.09201404303312302
Loss made of: CE 0.09079998731613159, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 220/245, Loss=0.09553832188248634
Loss made of: CE 0.097709059715271, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 230/245, Loss=0.10047115311026573
Loss made of: CE 0.14173784852027893, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 240/245, Loss=0.09560108482837677
Loss made of: CE 0.07488226890563965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09917452186346054, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.09917452186346054, Class Loss=0.09917452186346054, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/157, Loss=0.13875873014330864
Loss made of: CE 0.13072516024112701, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=0.14564373046159745
Loss made of: CE 0.14022812247276306, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.12929153814911842
Loss made of: CE 0.15691165626049042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.12403127625584602
Loss made of: CE 0.08801227062940598, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.13538787066936492
Loss made of: CE 0.13604502379894257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.13766054809093475
Loss made of: CE 0.1294218748807907, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.1302945353090763
Loss made of: CE 0.19916462898254395, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.14099055528640747
Loss made of: CE 0.1797369122505188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.15093424171209335
Loss made of: CE 0.18353509902954102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.15059827715158464
Loss made of: CE 0.16998913884162903, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.15918125733733177
Loss made of: CE 0.13483399152755737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.18977188616991042
Loss made of: CE 0.13608741760253906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.16417013481259346
Loss made of: CE 0.1743946522474289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.1625036545097828
Loss made of: CE 0.12333296239376068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.15219676569104196
Loss made of: CE 0.1288169026374817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.14750105142593384, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.14750105142593384, Class Loss=0.14750105142593384, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009247
Epoch 2, Batch 10/157, Loss=0.15041205808520317
Loss made of: CE 0.13516521453857422, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.1332542836666107
Loss made of: CE 0.1266755908727646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.1455938257277012
Loss made of: CE 0.12408441305160522, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.12981846630573274
Loss made of: CE 0.1350623369216919, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.1279823087155819
Loss made of: CE 0.1335332989692688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.1453156515955925
Loss made of: CE 0.11904221773147583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.1487705923616886
Loss made of: CE 0.12008558213710785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.15674395486712456
Loss made of: CE 0.17084041237831116, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.14228649735450744
Loss made of: CE 0.14425311982631683, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.1397022821009159
Loss made of: CE 0.1383044719696045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.1420125424861908
Loss made of: CE 0.12427011132240295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.12979551926255226
Loss made of: CE 0.10984779894351959, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.1529337137937546
Loss made of: CE 0.2298857867717743, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.12483532056212425
Loss made of: CE 0.13395726680755615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.1500783458352089
Loss made of: CE 0.15057490766048431, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.14096355438232422, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.14096355438232422, Class Loss=0.14096355438232422, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.008487
Epoch 3, Batch 10/157, Loss=0.13605811446905136
Loss made of: CE 0.13334205746650696, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.12373396158218383
Loss made of: CE 0.12453384697437286, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.1156096763908863
Loss made of: CE 0.14792758226394653, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.13545327559113501
Loss made of: CE 0.08486919105052948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.11532404348254204
Loss made of: CE 0.12398263067007065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.1266076534986496
Loss made of: CE 0.14388877153396606, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.15403404459357262
Loss made of: CE 0.1461019366979599, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.12257533371448517
Loss made of: CE 0.11220912635326385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.1322113662958145
Loss made of: CE 0.11107128113508224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.11417154222726822
Loss made of: CE 0.10128958523273468, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.12711975052952768
Loss made of: CE 0.105864517390728, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.1429366022348404
Loss made of: CE 0.12695825099945068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.11675173342227936
Loss made of: CE 0.1509387195110321, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.11737131327390671
Loss made of: CE 0.13571511209011078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.11103080064058304
Loss made of: CE 0.10657969117164612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.12623752653598785, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.12623752653598785, Class Loss=0.12623752653598785, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007719
Epoch 4, Batch 10/157, Loss=0.11269511058926582
Loss made of: CE 0.10387492179870605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.10660290047526359
Loss made of: CE 0.12379635870456696, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.10118893310427665
Loss made of: CE 0.1294432133436203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.11972582340240479
Loss made of: CE 0.10825499147176743, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.11954485923051834
Loss made of: CE 0.16141295433044434, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.10656284317374229
Loss made of: CE 0.12117402255535126, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.10094099938869476
Loss made of: CE 0.11467979848384857, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.1094543144106865
Loss made of: CE 0.10726012289524078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.10033332221210003
Loss made of: CE 0.1368408501148224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.10894338935613632
Loss made of: CE 0.11104269325733185, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.103950996696949
Loss made of: CE 0.09385949373245239, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.11858937069773674
Loss made of: CE 0.12181540578603745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.10925793796777725
Loss made of: CE 0.13633495569229126, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.11575791016221046
Loss made of: CE 0.08879401534795761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.1132110595703125
Loss made of: CE 0.11096988618373871, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.11018949002027512, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.11018949002027512, Class Loss=0.11018949002027512, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/157, Loss=0.10011135637760163
Loss made of: CE 0.08952890336513519, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.10017304867506027
Loss made of: CE 0.09637259691953659, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.10612086206674576
Loss made of: CE 0.09791503846645355, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.1105075478553772
Loss made of: CE 0.11863865703344345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.10336784049868583
Loss made of: CE 0.09472900629043579, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.1082479901611805
Loss made of: CE 0.13704323768615723, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.11262483671307563
Loss made of: CE 0.12392920255661011, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.10137533918023109
Loss made of: CE 0.128086119890213, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.10513113588094711
Loss made of: CE 0.08668643236160278, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.10507208928465843
Loss made of: CE 0.09031631052494049, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.10598677769303322
Loss made of: CE 0.07763337343931198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.10041339173913003
Loss made of: CE 0.08945151418447495, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.09999404624104499
Loss made of: CE 0.10761655867099762, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.09596881493926049
Loss made of: CE 0.11427357792854309, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.10418266952037811
Loss made of: CE 0.09462439268827438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10352771729230881, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.10352771729230881, Class Loss=0.10352771729230881, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006156
Epoch 6, Batch 10/157, Loss=0.10236273929476739
Loss made of: CE 0.09144499152898788, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.09785118028521538
Loss made of: CE 0.10788798332214355, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.09171223789453506
Loss made of: CE 0.10006856918334961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.09834172055125237
Loss made of: CE 0.08028671145439148, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.10032841265201568
Loss made of: CE 0.095301054418087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.0976097673177719
Loss made of: CE 0.08992105722427368, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.09600578472018242
Loss made of: CE 0.09002965688705444, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.10191364362835884
Loss made of: CE 0.11665382236242294, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.09662770628929138
Loss made of: CE 0.08082708716392517, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.1013267233967781
Loss made of: CE 0.08065938949584961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.10062933415174484
Loss made of: CE 0.07943399250507355, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.09824989810585975
Loss made of: CE 0.12969183921813965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.09212770983576775
Loss made of: CE 0.07311508059501648, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.09421941936016083
Loss made of: CE 0.07252737879753113, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.1019620344042778
Loss made of: CE 0.07225249707698822, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09736625850200653, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.09736625850200653, Class Loss=0.09736625850200653, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/244, Loss=0.1120372049510479
Loss made of: CE 0.1385928988456726, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/244, Loss=0.12430263459682464
Loss made of: CE 0.1498120278120041, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/244, Loss=0.12884681224822997
Loss made of: CE 0.113456591963768, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/244, Loss=0.1342100203037262
Loss made of: CE 0.14526033401489258, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/244, Loss=0.11723117977380752
Loss made of: CE 0.14299780130386353, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/244, Loss=0.10989411547780037
Loss made of: CE 0.09973631799221039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/244, Loss=0.14284065887331962
Loss made of: CE 0.11348363012075424, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/244, Loss=0.1590864583849907
Loss made of: CE 0.2195245325565338, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/244, Loss=0.13104025274515152
Loss made of: CE 0.15790238976478577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/244, Loss=0.13097488060593604
Loss made of: CE 0.20202568173408508, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/244, Loss=0.12903714030981064
Loss made of: CE 0.13062672317028046, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/244, Loss=0.12982095554471015
Loss made of: CE 0.11472763121128082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/244, Loss=0.1876378819346428
Loss made of: CE 0.14914266765117645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/244, Loss=0.1526124194264412
Loss made of: CE 0.1701861321926117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/244, Loss=0.13022690936923026
Loss made of: CE 0.14563629031181335, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/244, Loss=0.15630836561322212
Loss made of: CE 0.19042792916297913, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/244, Loss=0.1490723729133606
Loss made of: CE 0.13154669106006622, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/244, Loss=0.12290456742048264
Loss made of: CE 0.09438522905111313, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/244, Loss=0.1445980042219162
Loss made of: CE 0.12098740041255951, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/244, Loss=0.150953658670187
Loss made of: CE 0.12022501230239868, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 210/244, Loss=0.14758222177624702
Loss made of: CE 0.10224087536334991, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 220/244, Loss=0.14637842774391174
Loss made of: CE 0.14127278327941895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 230/244, Loss=0.16904557943344117
Loss made of: CE 0.11758343875408173, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 240/244, Loss=0.16726532950997353
Loss made of: CE 0.1343441754579544, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1407189816236496, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.1407189816236496, Class Loss=0.1407189816236496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009247
Epoch 2, Batch 10/244, Loss=0.12345437332987785
Loss made of: CE 0.14783073961734772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/244, Loss=0.12657815366983413
Loss made of: CE 0.15757271647453308, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/244, Loss=0.13002875000238417
Loss made of: CE 0.12749119102954865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/244, Loss=0.1382370263338089
Loss made of: CE 0.1244770735502243, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/244, Loss=0.14170007780194283
Loss made of: CE 0.15312808752059937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/244, Loss=0.11837750598788262
Loss made of: CE 0.12175844609737396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/244, Loss=0.12210789993405342
Loss made of: CE 0.12524592876434326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/244, Loss=0.12134220823645592
Loss made of: CE 0.10314668715000153, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/244, Loss=0.13855951204895972
Loss made of: CE 0.17222927510738373, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/244, Loss=0.14918968454003334
Loss made of: CE 0.17605724930763245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/244, Loss=0.13212823048233985
Loss made of: CE 0.1253848671913147, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/244, Loss=0.13474268093705177
Loss made of: CE 0.1588287055492401, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/244, Loss=0.13516145423054696
Loss made of: CE 0.12530109286308289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/244, Loss=0.1320302590727806
Loss made of: CE 0.1594453752040863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/244, Loss=0.1270620808005333
Loss made of: CE 0.10272212326526642, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/244, Loss=0.12188121601939202
Loss made of: CE 0.11951464414596558, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/244, Loss=0.1296979084610939
Loss made of: CE 0.11778829991817474, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/244, Loss=0.14206337854266166
Loss made of: CE 0.21000199019908905, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/244, Loss=0.12643100693821907
Loss made of: CE 0.1430133879184723, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/244, Loss=0.10579395219683647
Loss made of: CE 0.10084140300750732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 210/244, Loss=0.11648245230317116
Loss made of: CE 0.11625146120786667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 220/244, Loss=0.1265338532626629
Loss made of: CE 0.14174021780490875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 230/244, Loss=0.12614893838763236
Loss made of: CE 0.10665854811668396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 240/244, Loss=0.12043918371200561
Loss made of: CE 0.16916224360466003, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.12851601839065552, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.12851601839065552, Class Loss=0.12851601839065552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.008487
Epoch 3, Batch 10/244, Loss=0.10716476142406464
Loss made of: CE 0.0887819454073906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/244, Loss=0.11747303307056427
Loss made of: CE 0.08374406397342682, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/244, Loss=0.11531890258193016
Loss made of: CE 0.1429540514945984, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/244, Loss=0.11225111484527588
Loss made of: CE 0.11506402492523193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/244, Loss=0.11998193636536598
Loss made of: CE 0.10187700390815735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/244, Loss=0.11792414337396621
Loss made of: CE 0.09409874677658081, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/244, Loss=0.11135789677500725
Loss made of: CE 0.11148564517498016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/244, Loss=0.12568386569619178
Loss made of: CE 0.15172499418258667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/244, Loss=0.1134637400507927
Loss made of: CE 0.11850146949291229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/244, Loss=0.10792394280433655
Loss made of: CE 0.1197909340262413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/244, Loss=0.12052795439958572
Loss made of: CE 0.11408521234989166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/244, Loss=0.11860738173127175
Loss made of: CE 0.10908131301403046, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/244, Loss=0.1008455790579319
Loss made of: CE 0.1066867932677269, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/244, Loss=0.10559352040290833
Loss made of: CE 0.11780643463134766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/244, Loss=0.10672629624605179
Loss made of: CE 0.10518160462379456, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/244, Loss=0.12953739538788794
Loss made of: CE 0.16323280334472656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/244, Loss=0.11280799806118011
Loss made of: CE 0.14405277371406555, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/244, Loss=0.1133449912071228
Loss made of: CE 0.10616695135831833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/244, Loss=0.12145181223750115
Loss made of: CE 0.10817550122737885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/244, Loss=0.1144581250846386
Loss made of: CE 0.12125294655561447, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 210/244, Loss=0.10938369482755661
Loss made of: CE 0.10707416385412216, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 220/244, Loss=0.11926592215895652
Loss made of: CE 0.14651569724082947, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 230/244, Loss=0.12031403779983521
Loss made of: CE 0.12426301836967468, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 240/244, Loss=0.12452965453267098
Loss made of: CE 0.09289431571960449, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1153627559542656, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.1153627559542656, Class Loss=0.1153627559542656, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007719
Epoch 4, Batch 10/244, Loss=0.11519989147782325
Loss made of: CE 0.10143207013607025, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/244, Loss=0.11746113449335098
Loss made of: CE 0.1139112114906311, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/244, Loss=0.11368459016084671
Loss made of: CE 0.08936341106891632, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/244, Loss=0.10851487740874291
Loss made of: CE 0.09617062658071518, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/244, Loss=0.1059907041490078
Loss made of: CE 0.08100980520248413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/244, Loss=0.10941625013947487
Loss made of: CE 0.11265799403190613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/244, Loss=0.11606286838650703
Loss made of: CE 0.10242889821529388, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/244, Loss=0.10187570378184319
Loss made of: CE 0.0922468900680542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/244, Loss=0.10473041087388993
Loss made of: CE 0.11087621748447418, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/244, Loss=0.09815173670649528
Loss made of: CE 0.08056327700614929, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/244, Loss=0.10719653144478798
Loss made of: CE 0.13876989483833313, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/244, Loss=0.10429862067103386
Loss made of: CE 0.09472683072090149, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/244, Loss=0.10867784768342972
Loss made of: CE 0.08400899171829224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/244, Loss=0.10306455045938492
Loss made of: CE 0.10115097463130951, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/244, Loss=0.10878777503967285
Loss made of: CE 0.09032969176769257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/244, Loss=0.10397948250174523
Loss made of: CE 0.10111334174871445, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/244, Loss=0.10523050427436828
Loss made of: CE 0.16232424974441528, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/244, Loss=0.1025187574326992
Loss made of: CE 0.10638775676488876, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/244, Loss=0.10810087025165557
Loss made of: CE 0.07617863267660141, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/244, Loss=0.12089922055602073
Loss made of: CE 0.11869792640209198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 210/244, Loss=0.11184303984045982
Loss made of: CE 0.09571605920791626, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 220/244, Loss=0.10985205247998238
Loss made of: CE 0.11860649287700653, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 230/244, Loss=0.10142024382948875
Loss made of: CE 0.07471926510334015, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 240/244, Loss=0.09827609211206437
Loss made of: CE 0.09165084362030029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10780423879623413, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.10780423879623413, Class Loss=0.10780423879623413, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/244, Loss=0.0963868647813797
Loss made of: CE 0.10476651042699814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/244, Loss=0.10292604491114617
Loss made of: CE 0.11524741351604462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/244, Loss=0.10556155890226364
Loss made of: CE 0.1470639407634735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/244, Loss=0.09170480594038963
Loss made of: CE 0.07839936017990112, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/244, Loss=0.09995992407202721
Loss made of: CE 0.09322434663772583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/244, Loss=0.0850016012787819
Loss made of: CE 0.09054230898618698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/244, Loss=0.09676112756133079
Loss made of: CE 0.12974633276462555, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/244, Loss=0.09668297246098519
Loss made of: CE 0.12200585007667542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/244, Loss=0.11501318290829658
Loss made of: CE 0.12513723969459534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/244, Loss=0.09356479048728943
Loss made of: CE 0.09546639770269394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/244, Loss=0.09130922183394433
Loss made of: CE 0.08906575292348862, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/244, Loss=0.1006684236228466
Loss made of: CE 0.12784861028194427, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/244, Loss=0.10378825441002845
Loss made of: CE 0.09905308485031128, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/244, Loss=0.08704883083701134
Loss made of: CE 0.09537170827388763, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/244, Loss=0.09775509387254715
Loss made of: CE 0.10069034993648529, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/244, Loss=0.09792496711015701
Loss made of: CE 0.12577998638153076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/244, Loss=0.09607205912470818
Loss made of: CE 0.08524374663829803, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/244, Loss=0.10371315255761146
Loss made of: CE 0.12806877493858337, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/244, Loss=0.11286024004220963
Loss made of: CE 0.0936652272939682, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/244, Loss=0.10405096188187599
Loss made of: CE 0.10802826285362244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 210/244, Loss=0.10051456913352012
Loss made of: CE 0.09240414947271347, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 220/244, Loss=0.11485013216733933
Loss made of: CE 0.1154850646853447, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 230/244, Loss=0.10114125087857247
Loss made of: CE 0.08169451355934143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 240/244, Loss=0.09697119891643524
Loss made of: CE 0.11006655544042587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09956088662147522, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.09956088662147522, Class Loss=0.09956088662147522, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006156
Epoch 6, Batch 10/244, Loss=0.1015821598470211
Loss made of: CE 0.12907643616199493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/244, Loss=0.09571589305996894
Loss made of: CE 0.09173206984996796, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/244, Loss=0.08811301663517952
Loss made of: CE 0.08847447484731674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/244, Loss=0.09399637058377266
Loss made of: CE 0.08998171985149384, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/244, Loss=0.08818377479910851
Loss made of: CE 0.07010679692029953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/244, Loss=0.10286415815353393
Loss made of: CE 0.10334271192550659, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/244, Loss=0.09752567932009697
Loss made of: CE 0.11377429962158203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/244, Loss=0.0932304598391056
Loss made of: CE 0.09180925786495209, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/244, Loss=0.08554333001375199
Loss made of: CE 0.0869017094373703, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/244, Loss=0.09137112870812417
Loss made of: CE 0.09605126082897186, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/244, Loss=0.09324341416358947
Loss made of: CE 0.1565297245979309, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/244, Loss=0.10632408931851386
Loss made of: CE 0.09584280103445053, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/244, Loss=0.0949502557516098
Loss made of: CE 0.10476692020893097, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/244, Loss=0.09481715634465218
Loss made of: CE 0.08496379852294922, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/244, Loss=0.10024544522166252
Loss made of: CE 0.079056017100811, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/244, Loss=0.0951429009437561
Loss made of: CE 0.0868634507060051, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/244, Loss=0.09678022414445878
Loss made of: CE 0.08418034762144089, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/244, Loss=0.09295547157526016
Loss made of: CE 0.07404856383800507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/244, Loss=0.10053029581904412
Loss made of: CE 0.07363954186439514, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/244, Loss=0.09229754358530044
Loss made of: CE 0.08894418179988861, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 210/244, Loss=0.09201623424887657
Loss made of: CE 0.0825631320476532, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 220/244, Loss=0.08603614196181297
Loss made of: CE 0.09644033759832382, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 230/244, Loss=0.0946577437222004
Loss made of: CE 0.08985139429569244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 240/244, Loss=0.11015813425183296
Loss made of: CE 0.12081348896026611, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09569065272808075, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.09569065272808075, Class Loss=0.09569065272808075, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.14760234951972961, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.949664
Mean Acc: 0.878675
FreqW Acc: 0.909167
Mean IoU: 0.782498
Class IoU:
	class 0: 0.94241697
	class 1: 0.89007956
	class 2: 0.41645557
	class 3: 0.8903515
	class 4: 0.6830739
	class 5: 0.81471944
	class 6: 0.93967545
	class 7: 0.89886343
	class 8: 0.92742777
	class 9: 0.43004265
	class 10: 0.70883644
	class 11: 0.60070944
	class 12: 0.89467126
	class 13: 0.7562198
	class 14: 0.8538376
	class 15: 0.8725846
Class Acc:
	class 0: 0.9711566
	class 1: 0.9742519
	class 2: 0.9054747
	class 3: 0.93241405
	class 4: 0.83080655
	class 5: 0.90445125
	class 6: 0.9613649
	class 7: 0.9437066
	class 8: 0.96634305
	class 9: 0.5832883
	class 10: 0.72753304
	class 11: 0.6262863
	class 12: 0.960707
	class 13: 0.9321446
	class 14: 0.9132388
	class 15: 0.9256269

federated global round: 4, step: 0
select part of clients to conduct local training
[7, 1, 6, 8]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.004384
Epoch 1, Batch 10/157, Loss=0.11124511510133743
Loss made of: CE 0.10017502307891846, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/157, Loss=0.1061310552060604
Loss made of: CE 0.08407105505466461, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/157, Loss=0.09991317465901375
Loss made of: CE 0.12809664011001587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/157, Loss=0.0847000613808632
Loss made of: CE 0.06755252182483673, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/157, Loss=0.09607712179422379
Loss made of: CE 0.06956659257411957, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/157, Loss=0.1007729597389698
Loss made of: CE 0.11148776859045029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/157, Loss=0.10234907865524293
Loss made of: CE 0.11652319878339767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/157, Loss=0.08972871825098991
Loss made of: CE 0.09246240556240082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/157, Loss=0.09194538667798043
Loss made of: CE 0.11155494302511215, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/157, Loss=0.09701749384403228
Loss made of: CE 0.09463121742010117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/157, Loss=0.08972142115235329
Loss made of: CE 0.07477154582738876, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/157, Loss=0.09665995761752129
Loss made of: CE 0.09935279935598373, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/157, Loss=0.08983240127563477
Loss made of: CE 0.09169507026672363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/157, Loss=0.09821469262242317
Loss made of: CE 0.07290562987327576, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/157, Loss=0.08691791221499442
Loss made of: CE 0.08141222596168518, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.09630602598190308, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.09630602598190308, Class Loss=0.09630602598190308, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.003720
Epoch 2, Batch 10/157, Loss=0.0847485639154911
Loss made of: CE 0.0830933153629303, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/157, Loss=0.09563175439834595
Loss made of: CE 0.08011460304260254, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/157, Loss=0.0930566780269146
Loss made of: CE 0.1033652052283287, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/157, Loss=0.08703798949718475
Loss made of: CE 0.0702652707695961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/157, Loss=0.08448793068528175
Loss made of: CE 0.06830762326717377, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/157, Loss=0.09367580190300942
Loss made of: CE 0.10872030258178711, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/157, Loss=0.0894848346710205
Loss made of: CE 0.07985398918390274, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/157, Loss=0.0897158220410347
Loss made of: CE 0.07947497069835663, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/157, Loss=0.09215057827532291
Loss made of: CE 0.08823039382696152, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/157, Loss=0.08790708258748055
Loss made of: CE 0.08023025840520859, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/157, Loss=0.07795239835977555
Loss made of: CE 0.09814181178808212, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/157, Loss=0.08176476024091243
Loss made of: CE 0.09019701182842255, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/157, Loss=0.0873168408870697
Loss made of: CE 0.08823919296264648, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/157, Loss=0.07841919474303723
Loss made of: CE 0.06974830478429794, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/157, Loss=0.09595725983381272
Loss made of: CE 0.08896563947200775, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08841906487941742, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.08841906487941742, Class Loss=0.08841906487941742, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.003043
Epoch 3, Batch 10/157, Loss=0.08384162560105324
Loss made of: CE 0.08903201669454575, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/157, Loss=0.0808823473751545
Loss made of: CE 0.07873673737049103, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/157, Loss=0.08353288620710372
Loss made of: CE 0.07984679937362671, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/157, Loss=0.08782612979412079
Loss made of: CE 0.08560363948345184, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/157, Loss=0.08902579322457313
Loss made of: CE 0.09374147653579712, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/157, Loss=0.08481486886739731
Loss made of: CE 0.09824808686971664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/157, Loss=0.08973517566919327
Loss made of: CE 0.07024583220481873, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/157, Loss=0.08141626566648483
Loss made of: CE 0.0801483690738678, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/157, Loss=0.07227351069450379
Loss made of: CE 0.08565285056829453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/157, Loss=0.08524308055639267
Loss made of: CE 0.08691607415676117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/157, Loss=0.08974563144147396
Loss made of: CE 0.08341600745916367, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/157, Loss=0.07878393903374672
Loss made of: CE 0.08873205631971359, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/157, Loss=0.08944455236196518
Loss made of: CE 0.10534676164388657, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/157, Loss=0.09599850550293923
Loss made of: CE 0.0864490196108818, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/157, Loss=0.07743134275078774
Loss made of: CE 0.07379698753356934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08500944823026657, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.08500944823026657, Class Loss=0.08500944823026657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.002349
Epoch 4, Batch 10/157, Loss=0.08391335085034371
Loss made of: CE 0.08888725936412811, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/157, Loss=0.08315836861729622
Loss made of: CE 0.06297114491462708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/157, Loss=0.0899846725165844
Loss made of: CE 0.10991258919239044, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/157, Loss=0.0783356100320816
Loss made of: CE 0.07119543850421906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/157, Loss=0.08384560272097588
Loss made of: CE 0.09049275517463684, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/157, Loss=0.08530179038643837
Loss made of: CE 0.10083805024623871, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/157, Loss=0.08051543608307839
Loss made of: CE 0.07598437368869781, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/157, Loss=0.09254656881093978
Loss made of: CE 0.08564399927854538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/157, Loss=0.08009520173072815
Loss made of: CE 0.07486571371555328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/157, Loss=0.09145808145403862
Loss made of: CE 0.10639448463916779, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/157, Loss=0.08537530452013016
Loss made of: CE 0.07434715330600739, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/157, Loss=0.07532201670110225
Loss made of: CE 0.05996750295162201, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/157, Loss=0.08432383760809899
Loss made of: CE 0.07145243883132935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/157, Loss=0.08814436420798302
Loss made of: CE 0.06963808834552765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/157, Loss=0.0806200735270977
Loss made of: CE 0.08638857305049896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08369278907775879, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.08369278907775879, Class Loss=0.08369278907775879, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.001631
Epoch 5, Batch 10/157, Loss=0.0752323441207409
Loss made of: CE 0.07844811677932739, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/157, Loss=0.07896273136138916
Loss made of: CE 0.07539665699005127, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/157, Loss=0.09190071523189544
Loss made of: CE 0.16593602299690247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/157, Loss=0.0778774581849575
Loss made of: CE 0.09334089607000351, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/157, Loss=0.08334949389100074
Loss made of: CE 0.06588160991668701, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/157, Loss=0.08332181423902511
Loss made of: CE 0.08912684768438339, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/157, Loss=0.07493858262896538
Loss made of: CE 0.05648175999522209, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/157, Loss=0.08377577885985374
Loss made of: CE 0.06875031441450119, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/157, Loss=0.0816728699952364
Loss made of: CE 0.07559587061405182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/157, Loss=0.09273511916399002
Loss made of: CE 0.09417346119880676, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/157, Loss=0.0799013327807188
Loss made of: CE 0.061694081872701645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/157, Loss=0.0841705858707428
Loss made of: CE 0.10082771629095078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/157, Loss=0.09020032584667206
Loss made of: CE 0.07753689587116241, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/157, Loss=0.08386767581105233
Loss made of: CE 0.08946695178747177, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/157, Loss=0.08589935079216957
Loss made of: CE 0.07125282287597656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08321841061115265, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.08321841061115265, Class Loss=0.08321841061115265, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000874
Epoch 6, Batch 10/157, Loss=0.0808662824332714
Loss made of: CE 0.06571415066719055, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/157, Loss=0.08288774490356446
Loss made of: CE 0.08721836656332016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/157, Loss=0.08232691995799542
Loss made of: CE 0.09759071469306946, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/157, Loss=0.07581542059779167
Loss made of: CE 0.0762130469083786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/157, Loss=0.08004702031612396
Loss made of: CE 0.07458373159170151, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/157, Loss=0.08012366071343421
Loss made of: CE 0.07076823711395264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/157, Loss=0.08366727977991104
Loss made of: CE 0.07233448326587677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/157, Loss=0.07376912087202073
Loss made of: CE 0.07261516153812408, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/157, Loss=0.08147400990128517
Loss made of: CE 0.07744384557008743, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/157, Loss=0.08242441602051258
Loss made of: CE 0.060243915766477585, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/157, Loss=0.07828682735562324
Loss made of: CE 0.07272150367498398, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/157, Loss=0.08184107914566993
Loss made of: CE 0.05204185098409653, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/157, Loss=0.08218538910150527
Loss made of: CE 0.10289230942726135, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/157, Loss=0.08836323879659176
Loss made of: CE 0.09666578471660614, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/157, Loss=0.08262583389878272
Loss made of: CE 0.08653246611356735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08144592493772507, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.08144592493772507, Class Loss=0.08144592493772507, Reg Loss=0.0
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/164, Loss=0.09706376716494561
Loss made of: CE 0.1080915629863739, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/164, Loss=0.10038905665278434
Loss made of: CE 0.08203238248825073, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/164, Loss=0.11178088113665581
Loss made of: CE 0.10544105619192123, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/164, Loss=0.12519069910049438
Loss made of: CE 0.07247728109359741, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/164, Loss=0.10248928517103195
Loss made of: CE 0.15837550163269043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/164, Loss=0.10265098363161088
Loss made of: CE 0.09625820815563202, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/164, Loss=0.0985662318766117
Loss made of: CE 0.07955716550350189, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/164, Loss=0.10664987340569496
Loss made of: CE 0.11075600981712341, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/164, Loss=0.11776330322027206
Loss made of: CE 0.09893783181905746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/164, Loss=0.10059922263026237
Loss made of: CE 0.0842999517917633, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/164, Loss=0.0968307763338089
Loss made of: CE 0.09456335008144379, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/164, Loss=0.11698434650897979
Loss made of: CE 0.09509995579719543, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/164, Loss=0.1043216459453106
Loss made of: CE 0.0911412239074707, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/164, Loss=0.10661334767937661
Loss made of: CE 0.11765508353710175, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/164, Loss=0.10539533272385597
Loss made of: CE 0.11375030875205994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/164, Loss=0.10628122761845589
Loss made of: CE 0.09918171167373657, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1060066670179367, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.1060066670179367, Class Loss=0.1060066670179367, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.006551
Epoch 2, Batch 10/164, Loss=0.09496684446930885
Loss made of: CE 0.0954710841178894, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/164, Loss=0.09743235036730766
Loss made of: CE 0.13121727108955383, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/164, Loss=0.1026931069791317
Loss made of: CE 0.10727718472480774, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/164, Loss=0.10096675753593445
Loss made of: CE 0.08193456381559372, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/164, Loss=0.09527924656867981
Loss made of: CE 0.10323916375637054, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/164, Loss=0.10159117802977562
Loss made of: CE 0.09205693006515503, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/164, Loss=0.09663139954209328
Loss made of: CE 0.0764634907245636, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/164, Loss=0.09786362200975418
Loss made of: CE 0.09041251242160797, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/164, Loss=0.09963051825761796
Loss made of: CE 0.10881054401397705, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/164, Loss=0.09483342841267586
Loss made of: CE 0.08073855936527252, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/164, Loss=0.09522010497748852
Loss made of: CE 0.09028029441833496, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/164, Loss=0.0964625507593155
Loss made of: CE 0.09217624366283417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/164, Loss=0.10809745937585831
Loss made of: CE 0.1344401240348816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/164, Loss=0.09094395861029625
Loss made of: CE 0.06523029506206512, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/164, Loss=0.10011401921510696
Loss made of: CE 0.08819831907749176, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/164, Loss=0.09599342420697213
Loss made of: CE 0.08055037260055542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09819793701171875, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.09819793701171875, Class Loss=0.09819793701171875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.005359
Epoch 3, Batch 10/164, Loss=0.09012778773903847
Loss made of: CE 0.08308550715446472, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/164, Loss=0.10189995393157006
Loss made of: CE 0.08859777450561523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/164, Loss=0.09312283918261528
Loss made of: CE 0.1212521344423294, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/164, Loss=0.08834656104445457
Loss made of: CE 0.087366484105587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/164, Loss=0.098982422798872
Loss made of: CE 0.10289788246154785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/164, Loss=0.09369168058037758
Loss made of: CE 0.09567757695913315, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/164, Loss=0.09243696928024292
Loss made of: CE 0.11161403357982635, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/164, Loss=0.09969058111310006
Loss made of: CE 0.11689978092908859, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/164, Loss=0.09038820639252662
Loss made of: CE 0.06386236101388931, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/164, Loss=0.0936246581375599
Loss made of: CE 0.10019642114639282, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/164, Loss=0.08929830640554429
Loss made of: CE 0.07829339802265167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/164, Loss=0.084769719094038
Loss made of: CE 0.10143925994634628, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/164, Loss=0.10234183892607689
Loss made of: CE 0.10780538618564606, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/164, Loss=0.09558671563863755
Loss made of: CE 0.09283754229545593, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/164, Loss=0.0853513978421688
Loss made of: CE 0.08139944076538086, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/164, Loss=0.09189575985074043
Loss made of: CE 0.09063754975795746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09286754578351974, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.09286754578351974, Class Loss=0.09286754578351974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.004136
Epoch 4, Batch 10/164, Loss=0.09289899095892906
Loss made of: CE 0.10450991243124008, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/164, Loss=0.09703879207372665
Loss made of: CE 0.07885389775037766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/164, Loss=0.0875308770686388
Loss made of: CE 0.10703971236944199, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/164, Loss=0.09275078848004341
Loss made of: CE 0.077889584004879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/164, Loss=0.09141636863350869
Loss made of: CE 0.08875622600317001, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/164, Loss=0.08875637128949165
Loss made of: CE 0.07471200823783875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/164, Loss=0.099922876060009
Loss made of: CE 0.10859300941228867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/164, Loss=0.09520339220762253
Loss made of: CE 0.1141132265329361, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/164, Loss=0.09393958076834678
Loss made of: CE 0.08619137853384018, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/164, Loss=0.0902905933558941
Loss made of: CE 0.09044935554265976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/164, Loss=0.08179152235388756
Loss made of: CE 0.0886022076010704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/164, Loss=0.08750011771917343
Loss made of: CE 0.1009683832526207, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/164, Loss=0.08652832433581352
Loss made of: CE 0.08111199736595154, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/164, Loss=0.09463886246085167
Loss made of: CE 0.07805168628692627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/164, Loss=0.08912184685468674
Loss made of: CE 0.07319415360689163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/164, Loss=0.0864132098853588
Loss made of: CE 0.07688067853450775, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09104178100824356, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.09104178100824356, Class Loss=0.09104178100824356, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.002872
Epoch 5, Batch 10/164, Loss=0.08811800964176655
Loss made of: CE 0.11100248992443085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/164, Loss=0.0918094277381897
Loss made of: CE 0.08587634563446045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/164, Loss=0.09207386672496795
Loss made of: CE 0.11201219260692596, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/164, Loss=0.07717622593045234
Loss made of: CE 0.09043560922145844, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/164, Loss=0.08453850597143173
Loss made of: CE 0.06381790339946747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/164, Loss=0.08754754215478897
Loss made of: CE 0.09397634118795395, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/164, Loss=0.08552202582359314
Loss made of: CE 0.09108734130859375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/164, Loss=0.08328011482954026
Loss made of: CE 0.082469142973423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/164, Loss=0.08289354033768177
Loss made of: CE 0.07516570389270782, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/164, Loss=0.08074308037757874
Loss made of: CE 0.07127609848976135, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/164, Loss=0.08969377651810646
Loss made of: CE 0.10399121046066284, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/164, Loss=0.08335537239909172
Loss made of: CE 0.09268541634082794, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/164, Loss=0.08810720443725586
Loss made of: CE 0.08045117557048798, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/164, Loss=0.08727112933993339
Loss made of: CE 0.08942142128944397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/164, Loss=0.08987168744206428
Loss made of: CE 0.08769679069519043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/164, Loss=0.07943043410778046
Loss made of: CE 0.08836856484413147, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08579377830028534, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.08579377830028534, Class Loss=0.08579377830028534, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.001539
Epoch 6, Batch 10/164, Loss=0.07911784127354622
Loss made of: CE 0.07802975922822952, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/164, Loss=0.08445645347237588
Loss made of: CE 0.06851111352443695, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/164, Loss=0.08996311575174332
Loss made of: CE 0.0776040330529213, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/164, Loss=0.08256204202771186
Loss made of: CE 0.07714863866567612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/164, Loss=0.08789677396416665
Loss made of: CE 0.09849868714809418, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/164, Loss=0.08488908559083938
Loss made of: CE 0.08640570938587189, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/164, Loss=0.08305735029280185
Loss made of: CE 0.059527214616537094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/164, Loss=0.09809702411293983
Loss made of: CE 0.09257730096578598, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/164, Loss=0.08284165039658546
Loss made of: CE 0.06518527120351791, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/164, Loss=0.07886201068758965
Loss made of: CE 0.08007724583148956, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/164, Loss=0.0882288634777069
Loss made of: CE 0.08327306061983109, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/164, Loss=0.08297279626131057
Loss made of: CE 0.06824621558189392, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/164, Loss=0.08573711328208447
Loss made of: CE 0.1071062684059143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/164, Loss=0.08507260158658028
Loss made of: CE 0.07904600352048874, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/164, Loss=0.0824048526585102
Loss made of: CE 0.0723099410533905, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/164, Loss=0.08854112401604652
Loss made of: CE 0.08216991275548935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08525697141885757, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.08525697141885757, Class Loss=0.08525697141885757, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.005679
Epoch 1, Batch 10/198, Loss=0.11164039000868797
Loss made of: CE 0.08311295509338379, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/198, Loss=0.13209879472851754
Loss made of: CE 0.1315581053495407, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/198, Loss=0.11398907825350761
Loss made of: CE 0.13130971789360046, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/198, Loss=0.11914198100566864
Loss made of: CE 0.11752618849277496, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/198, Loss=0.11000606939196586
Loss made of: CE 0.10441663861274719, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/198, Loss=0.11523393094539643
Loss made of: CE 0.08968549966812134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/198, Loss=0.10944563001394272
Loss made of: CE 0.09343080222606659, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/198, Loss=0.10553575232625008
Loss made of: CE 0.10091719031333923, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/198, Loss=0.11184853911399842
Loss made of: CE 0.08408555388450623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/198, Loss=0.11218760907649994
Loss made of: CE 0.11889360845088959, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/198, Loss=0.10070758312940598
Loss made of: CE 0.10266517102718353, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/198, Loss=0.10559427961707116
Loss made of: CE 0.0976443886756897, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/198, Loss=0.10658021941781044
Loss made of: CE 0.13115210831165314, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/198, Loss=0.10247446894645691
Loss made of: CE 0.09904786199331284, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/198, Loss=0.1116442896425724
Loss made of: CE 0.14484789967536926, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/198, Loss=0.11564567908644677
Loss made of: CE 0.08835828304290771, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/198, Loss=0.10547683537006378
Loss made of: CE 0.10678981989622116, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/198, Loss=0.10026680529117585
Loss made of: CE 0.1023302674293518, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/198, Loss=0.10285284221172333
Loss made of: CE 0.12537042796611786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.10999133437871933, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.10999133437871933, Class Loss=0.10999133437871933, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.004820
Epoch 2, Batch 10/198, Loss=0.09780489951372147
Loss made of: CE 0.08518484234809875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/198, Loss=0.09619502499699592
Loss made of: CE 0.07688043266534805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/198, Loss=0.09271494001150131
Loss made of: CE 0.06599011272192001, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/198, Loss=0.09567195475101471
Loss made of: CE 0.08730604499578476, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/198, Loss=0.10058483555912971
Loss made of: CE 0.10614198446273804, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/198, Loss=0.10448766425251961
Loss made of: CE 0.10175460577011108, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/198, Loss=0.10508456528186798
Loss made of: CE 0.0885823667049408, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/198, Loss=0.096807611733675
Loss made of: CE 0.08777346462011337, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/198, Loss=0.10854975134134293
Loss made of: CE 0.1318131685256958, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/198, Loss=0.10585313439369201
Loss made of: CE 0.10516133159399033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/198, Loss=0.09617379829287528
Loss made of: CE 0.08263593912124634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/198, Loss=0.09909379109740257
Loss made of: CE 0.10854403674602509, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/198, Loss=0.10067654401063919
Loss made of: CE 0.12490777671337128, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/198, Loss=0.10350430905818939
Loss made of: CE 0.12291242927312851, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/198, Loss=0.10706275850534439
Loss made of: CE 0.09149233996868134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/198, Loss=0.1049720048904419
Loss made of: CE 0.10000702738761902, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/198, Loss=0.10752195343375207
Loss made of: CE 0.10581310838460922, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/198, Loss=0.10389688313007354
Loss made of: CE 0.13473984599113464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/198, Loss=0.11006252467632294
Loss made of: CE 0.12996560335159302, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10185126215219498, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.10185126215219498, Class Loss=0.10185126215219498, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.003943
Epoch 3, Batch 10/198, Loss=0.09902169704437255
Loss made of: CE 0.10238369554281235, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/198, Loss=0.09518794119358062
Loss made of: CE 0.10417553037405014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/198, Loss=0.09355385899543762
Loss made of: CE 0.08811189979314804, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/198, Loss=0.10144203528761864
Loss made of: CE 0.10499047487974167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/198, Loss=0.0891443021595478
Loss made of: CE 0.07446303963661194, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/198, Loss=0.09578092470765114
Loss made of: CE 0.08704188466072083, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/198, Loss=0.10341806262731552
Loss made of: CE 0.10989266633987427, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/198, Loss=0.10137358978390694
Loss made of: CE 0.10776674002408981, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/198, Loss=0.10004064068198204
Loss made of: CE 0.10198399424552917, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/198, Loss=0.09799747690558433
Loss made of: CE 0.08642217516899109, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/198, Loss=0.0944289281964302
Loss made of: CE 0.06824098527431488, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/198, Loss=0.0861177995800972
Loss made of: CE 0.08792602270841599, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/198, Loss=0.10452947467565536
Loss made of: CE 0.1370076984167099, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/198, Loss=0.10200292021036148
Loss made of: CE 0.08840517699718475, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/198, Loss=0.11200719326734543
Loss made of: CE 0.1451045572757721, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/198, Loss=0.10496403872966767
Loss made of: CE 0.10316808521747589, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/198, Loss=0.09266654476523399
Loss made of: CE 0.07695566117763519, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/198, Loss=0.09570745155215263
Loss made of: CE 0.07910744845867157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/198, Loss=0.09599973931908608
Loss made of: CE 0.11066281795501709, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09820427745580673, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.09820427745580673, Class Loss=0.09820427745580673, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.003043
Epoch 4, Batch 10/198, Loss=0.09730735570192336
Loss made of: CE 0.07626720517873764, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/198, Loss=0.10002444386482238
Loss made of: CE 0.12272785604000092, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/198, Loss=0.09572576060891151
Loss made of: CE 0.08253213763237, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/198, Loss=0.08905047029256821
Loss made of: CE 0.09503455460071564, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/198, Loss=0.10079384371638297
Loss made of: CE 0.10318166017532349, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/198, Loss=0.09022568166255951
Loss made of: CE 0.08806245028972626, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/198, Loss=0.09780913740396499
Loss made of: CE 0.17073485255241394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/198, Loss=0.09384748861193656
Loss made of: CE 0.07771909981966019, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/198, Loss=0.0803861353546381
Loss made of: CE 0.09357991069555283, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/198, Loss=0.0983828216791153
Loss made of: CE 0.12262865155935287, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/198, Loss=0.09427629858255386
Loss made of: CE 0.10631813853979111, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/198, Loss=0.08886748552322388
Loss made of: CE 0.10266267508268356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/198, Loss=0.0990820948034525
Loss made of: CE 0.11143980920314789, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/198, Loss=0.0983795940876007
Loss made of: CE 0.10542559623718262, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/198, Loss=0.09359070211648941
Loss made of: CE 0.11730009317398071, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/198, Loss=0.10492119714617729
Loss made of: CE 0.0956231877207756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/198, Loss=0.0899977594614029
Loss made of: CE 0.09295471757650375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/198, Loss=0.09218405932188034
Loss made of: CE 0.09167411923408508, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/198, Loss=0.09282643646001816
Loss made of: CE 0.10707338899374008, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09438229352235794, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.09438229352235794, Class Loss=0.09438229352235794, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.002113
Epoch 5, Batch 10/198, Loss=0.09544832780957221
Loss made of: CE 0.10955439507961273, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/198, Loss=0.09216173812747001
Loss made of: CE 0.13709084689617157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/198, Loss=0.09262977726757526
Loss made of: CE 0.08962541073560715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/198, Loss=0.08928616344928741
Loss made of: CE 0.0959138497710228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/198, Loss=0.08669162467122078
Loss made of: CE 0.09508828818798065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/198, Loss=0.09231388494372368
Loss made of: CE 0.0834139809012413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/198, Loss=0.09480390623211861
Loss made of: CE 0.08274509012699127, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/198, Loss=0.08908522725105286
Loss made of: CE 0.06774011254310608, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/198, Loss=0.09393983110785484
Loss made of: CE 0.09385224431753159, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/198, Loss=0.09694114699959755
Loss made of: CE 0.11281715333461761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/198, Loss=0.10036220028996468
Loss made of: CE 0.09487728774547577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/198, Loss=0.09524046033620834
Loss made of: CE 0.10972490161657333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/198, Loss=0.09398469924926758
Loss made of: CE 0.10175126791000366, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/198, Loss=0.09381881579756737
Loss made of: CE 0.09523577988147736, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/198, Loss=0.09296016693115235
Loss made of: CE 0.10237937420606613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/198, Loss=0.09116163402795792
Loss made of: CE 0.0762888640165329, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/198, Loss=0.09292693510651588
Loss made of: CE 0.08306139707565308, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/198, Loss=0.09724813178181649
Loss made of: CE 0.10387544333934784, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/198, Loss=0.09235113933682441
Loss made of: CE 0.06597759574651718, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09305767714977264, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.09305767714977264, Class Loss=0.09305767714977264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.001132
Epoch 6, Batch 10/198, Loss=0.08825860396027566
Loss made of: CE 0.08674856275320053, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/198, Loss=0.09240316823124886
Loss made of: CE 0.09724761545658112, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/198, Loss=0.09203568175435066
Loss made of: CE 0.10349196195602417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/198, Loss=0.10153040140867234
Loss made of: CE 0.08363236486911774, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/198, Loss=0.0903707578778267
Loss made of: CE 0.07930172234773636, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/198, Loss=0.0915591299533844
Loss made of: CE 0.08616166561841965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/198, Loss=0.08793372884392739
Loss made of: CE 0.07799626886844635, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/198, Loss=0.09131842479109764
Loss made of: CE 0.10699343681335449, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/198, Loss=0.08375711441040039
Loss made of: CE 0.10221676528453827, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/198, Loss=0.08376281708478928
Loss made of: CE 0.05858885496854782, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/198, Loss=0.09235649555921555
Loss made of: CE 0.10399571061134338, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/198, Loss=0.09610712751746178
Loss made of: CE 0.08629485964775085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/198, Loss=0.0975148320198059
Loss made of: CE 0.09174570441246033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/198, Loss=0.10191749483346939
Loss made of: CE 0.10194765776395798, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/198, Loss=0.09786144942045212
Loss made of: CE 0.1255308985710144, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/198, Loss=0.08901154659688473
Loss made of: CE 0.060751866549253464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/198, Loss=0.10248640850186348
Loss made of: CE 0.08321605622768402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/198, Loss=0.08629503846168518
Loss made of: CE 0.07532557100057602, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/198, Loss=0.08564775958657264
Loss made of: CE 0.08307597041130066, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09206301718950272, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.09206301718950272, Class Loss=0.09206301718950272, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006943
Epoch 1, Batch 10/229, Loss=0.09891953095793724
Loss made of: CE 0.09512197971343994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/229, Loss=0.10808612033724785
Loss made of: CE 0.14810004830360413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/229, Loss=0.10880391895771027
Loss made of: CE 0.09705077111721039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/229, Loss=0.10525751188397407
Loss made of: CE 0.0862039253115654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/229, Loss=0.09886654391884804
Loss made of: CE 0.1290857046842575, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/229, Loss=0.10136894062161446
Loss made of: CE 0.1188824400305748, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/229, Loss=0.11494507491588593
Loss made of: CE 0.0933975875377655, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/229, Loss=0.10150781869888306
Loss made of: CE 0.09678834676742554, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/229, Loss=0.10823667421936989
Loss made of: CE 0.10179281234741211, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/229, Loss=0.10272862836718559
Loss made of: CE 0.09443410485982895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 110/229, Loss=0.10753819942474366
Loss made of: CE 0.10815935581922531, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 120/229, Loss=0.09572435989975929
Loss made of: CE 0.09026592969894409, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 130/229, Loss=0.10016820579767227
Loss made of: CE 0.13573798537254333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 140/229, Loss=0.11038596034049988
Loss made of: CE 0.07757329940795898, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 150/229, Loss=0.09649435132741928
Loss made of: CE 0.09290143102407455, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 160/229, Loss=0.10876928493380547
Loss made of: CE 0.08239869028329849, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 170/229, Loss=0.11140548512339592
Loss made of: CE 0.12930142879486084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 180/229, Loss=0.09865038469433784
Loss made of: CE 0.0841023325920105, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 190/229, Loss=0.10747438669204712
Loss made of: CE 0.0967748761177063, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 200/229, Loss=0.10299944505095482
Loss made of: CE 0.09012189507484436, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 210/229, Loss=0.10086801573634148
Loss made of: CE 0.09899457544088364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 220/229, Loss=0.11877184361219406
Loss made of: CE 0.18138182163238525, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.10562692582607269, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.10562692582607269, Class Loss=0.10562692582607269, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.005892
Epoch 2, Batch 10/229, Loss=0.10262295976281166
Loss made of: CE 0.09155336022377014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/229, Loss=0.09941253438591957
Loss made of: CE 0.11081171035766602, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/229, Loss=0.09568367972970009
Loss made of: CE 0.07365050166845322, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/229, Loss=0.1069275476038456
Loss made of: CE 0.10730214416980743, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/229, Loss=0.09186492338776589
Loss made of: CE 0.09943803399801254, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/229, Loss=0.10126607492566109
Loss made of: CE 0.07959713786840439, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/229, Loss=0.10210005715489387
Loss made of: CE 0.10000087320804596, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/229, Loss=0.09622604921460151
Loss made of: CE 0.09087194502353668, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/229, Loss=0.10268344283103943
Loss made of: CE 0.07684361934661865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/229, Loss=0.09681807011365891
Loss made of: CE 0.13020282983779907, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 110/229, Loss=0.09062490426003933
Loss made of: CE 0.1031871885061264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 120/229, Loss=0.09537969082593918
Loss made of: CE 0.09933745115995407, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 130/229, Loss=0.10519737526774406
Loss made of: CE 0.10575193166732788, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 140/229, Loss=0.0922017939388752
Loss made of: CE 0.09743750095367432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 150/229, Loss=0.0967933289706707
Loss made of: CE 0.09465616941452026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 160/229, Loss=0.09828477874398231
Loss made of: CE 0.12142124772071838, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 170/229, Loss=0.0903071079403162
Loss made of: CE 0.10417811572551727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 180/229, Loss=0.09842849746346474
Loss made of: CE 0.10057582706212997, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 190/229, Loss=0.09171673879027367
Loss made of: CE 0.12324982136487961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 200/229, Loss=0.09571064561605454
Loss made of: CE 0.10623260587453842, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 210/229, Loss=0.1041757807135582
Loss made of: CE 0.12866345047950745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 220/229, Loss=0.1038332037627697
Loss made of: CE 0.1082748994231224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.0977899581193924, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.0977899581193924, Class Loss=0.0977899581193924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.004820
Epoch 3, Batch 10/229, Loss=0.08479747027158738
Loss made of: CE 0.08853954076766968, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/229, Loss=0.08510137274861336
Loss made of: CE 0.10899707674980164, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/229, Loss=0.09030310958623886
Loss made of: CE 0.08653680980205536, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/229, Loss=0.09360159114003182
Loss made of: CE 0.0939977616071701, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/229, Loss=0.09168680757284164
Loss made of: CE 0.08515018224716187, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/229, Loss=0.0953639917075634
Loss made of: CE 0.11894392967224121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/229, Loss=0.090485318005085
Loss made of: CE 0.10290059447288513, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/229, Loss=0.09641861170530319
Loss made of: CE 0.07976266741752625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/229, Loss=0.08842801973223686
Loss made of: CE 0.1224280521273613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/229, Loss=0.09020617678761482
Loss made of: CE 0.08503739535808563, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 110/229, Loss=0.10121257379651069
Loss made of: CE 0.10691076517105103, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 120/229, Loss=0.08813108280301094
Loss made of: CE 0.09276683628559113, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 130/229, Loss=0.09981769919395447
Loss made of: CE 0.12021908164024353, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 140/229, Loss=0.0974015474319458
Loss made of: CE 0.10022841393947601, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 150/229, Loss=0.09728300496935845
Loss made of: CE 0.07349646091461182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 160/229, Loss=0.10109159052371978
Loss made of: CE 0.08000838756561279, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 170/229, Loss=0.09227789863944054
Loss made of: CE 0.07893085479736328, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 180/229, Loss=0.08829521611332894
Loss made of: CE 0.07139503210783005, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 190/229, Loss=0.08794217184185982
Loss made of: CE 0.0722295343875885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 200/229, Loss=0.09193072468042374
Loss made of: CE 0.08808152377605438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 210/229, Loss=0.08364082500338554
Loss made of: CE 0.07589050382375717, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 220/229, Loss=0.09678790718317032
Loss made of: CE 0.08991395682096481, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09252548217773438, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.09252548217773438, Class Loss=0.09252548217773438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.003720
Epoch 4, Batch 10/229, Loss=0.0882496640086174
Loss made of: CE 0.1045496016740799, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/229, Loss=0.09590066000819206
Loss made of: CE 0.14524200558662415, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/229, Loss=0.08917666673660278
Loss made of: CE 0.10344751179218292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/229, Loss=0.08755003362894058
Loss made of: CE 0.0967949852347374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/229, Loss=0.09117805734276771
Loss made of: CE 0.08542011678218842, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/229, Loss=0.08172156065702438
Loss made of: CE 0.07693232595920563, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/229, Loss=0.0930127464234829
Loss made of: CE 0.08548103272914886, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/229, Loss=0.09184810146689415
Loss made of: CE 0.08931203931570053, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/229, Loss=0.091511569917202
Loss made of: CE 0.08355572819709778, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/229, Loss=0.08221904151141643
Loss made of: CE 0.10213534533977509, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 110/229, Loss=0.09102767519652843
Loss made of: CE 0.08973502367734909, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 120/229, Loss=0.09429921954870224
Loss made of: CE 0.10617180913686752, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 130/229, Loss=0.08870093077421189
Loss made of: CE 0.09624280035495758, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 140/229, Loss=0.08260966166853904
Loss made of: CE 0.05832409858703613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 150/229, Loss=0.09104891791939736
Loss made of: CE 0.12334936857223511, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 160/229, Loss=0.0914809986948967
Loss made of: CE 0.09258773177862167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 170/229, Loss=0.08056226745247841
Loss made of: CE 0.07569914311170578, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 180/229, Loss=0.08944384679198265
Loss made of: CE 0.08819779008626938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 190/229, Loss=0.08081405609846115
Loss made of: CE 0.08307970315217972, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 200/229, Loss=0.1006255742162466
Loss made of: CE 0.09157596528530121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 210/229, Loss=0.09088659957051277
Loss made of: CE 0.12160949409008026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 220/229, Loss=0.08363914713263512
Loss made of: CE 0.07922579348087311, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08884943276643753, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.08884943276643753, Class Loss=0.08884943276643753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.002583
Epoch 5, Batch 10/229, Loss=0.08829878866672516
Loss made of: CE 0.10820408165454865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/229, Loss=0.08856798708438873
Loss made of: CE 0.06312292069196701, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/229, Loss=0.09205593392252923
Loss made of: CE 0.06432920694351196, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/229, Loss=0.0938730351626873
Loss made of: CE 0.11060428619384766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/229, Loss=0.08253605887293816
Loss made of: CE 0.09979528188705444, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/229, Loss=0.09015230163931846
Loss made of: CE 0.08003382384777069, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/229, Loss=0.08229699805378914
Loss made of: CE 0.060897283256053925, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/229, Loss=0.07827037796378136
Loss made of: CE 0.07686060667037964, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/229, Loss=0.08471294455230235
Loss made of: CE 0.09042124450206757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/229, Loss=0.08472384810447693
Loss made of: CE 0.07803582400083542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 110/229, Loss=0.08320420682430267
Loss made of: CE 0.0793013796210289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 120/229, Loss=0.08885896950960159
Loss made of: CE 0.10844191908836365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 130/229, Loss=0.08804044090211391
Loss made of: CE 0.07801598310470581, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 140/229, Loss=0.0861198902130127
Loss made of: CE 0.1134786605834961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 150/229, Loss=0.0796737514436245
Loss made of: CE 0.08039608597755432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 160/229, Loss=0.08033826947212219
Loss made of: CE 0.08541231602430344, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 170/229, Loss=0.08358858525753021
Loss made of: CE 0.07373733818531036, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 180/229, Loss=0.08704592287540436
Loss made of: CE 0.10180255025625229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 190/229, Loss=0.08818452060222626
Loss made of: CE 0.10194242000579834, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 200/229, Loss=0.08984484449028969
Loss made of: CE 0.11413547396659851, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 210/229, Loss=0.09122549816966057
Loss made of: CE 0.08785802125930786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 220/229, Loss=0.08647047653794289
Loss made of: CE 0.09398790448904037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08676967769861221, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.08676967769861221, Class Loss=0.08676967769861221, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.001384
Epoch 6, Batch 10/229, Loss=0.07670255899429321
Loss made of: CE 0.10715395212173462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/229, Loss=0.09524394124746323
Loss made of: CE 0.12998023629188538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/229, Loss=0.07839637026190757
Loss made of: CE 0.07426810264587402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/229, Loss=0.08071990907192231
Loss made of: CE 0.07470671832561493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/229, Loss=0.08570817559957504
Loss made of: CE 0.07184099406003952, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/229, Loss=0.08212962597608567
Loss made of: CE 0.08612455427646637, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/229, Loss=0.08417465463280678
Loss made of: CE 0.07220876961946487, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/229, Loss=0.08540435880422592
Loss made of: CE 0.07988283038139343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/229, Loss=0.08498891070485115
Loss made of: CE 0.08224155753850937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/229, Loss=0.08682554066181183
Loss made of: CE 0.09105680882930756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 110/229, Loss=0.08058330938220024
Loss made of: CE 0.07340793311595917, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 120/229, Loss=0.08216724917292595
Loss made of: CE 0.07755263149738312, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 130/229, Loss=0.08754362016916276
Loss made of: CE 0.11789830774068832, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 140/229, Loss=0.0932743489742279
Loss made of: CE 0.08143676817417145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 150/229, Loss=0.08453823551535607
Loss made of: CE 0.08165900409221649, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 160/229, Loss=0.09030548930168152
Loss made of: CE 0.0711463987827301, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 170/229, Loss=0.085775126516819
Loss made of: CE 0.078087717294693, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 180/229, Loss=0.08345378115773201
Loss made of: CE 0.08735664933919907, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 190/229, Loss=0.08797482848167419
Loss made of: CE 0.1154680848121643, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 200/229, Loss=0.08648377731442451
Loss made of: CE 0.11394201219081879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 210/229, Loss=0.08135316036641597
Loss made of: CE 0.07523614168167114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 220/229, Loss=0.08702606186270714
Loss made of: CE 0.06330036371946335, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08514275401830673, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.08514275401830673, Class Loss=0.08514275401830673, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.14576540887355804, Reg Loss=0.0 (without scaling)

Total samples: 1240.000000
Overall Acc: 0.951786
Mean Acc: 0.881543
FreqW Acc: 0.912116
Mean IoU: 0.794392
Class IoU:
	class 0: 0.94357705
	class 1: 0.90530145
	class 2: 0.42136124
	class 3: 0.89726245
	class 4: 0.69015
	class 5: 0.8226801
	class 6: 0.9389834
	class 7: 0.91357034
	class 8: 0.9198369
	class 9: 0.4394113
	class 10: 0.77789533
	class 11: 0.6194928
	class 12: 0.8648829
	class 13: 0.8079485
	class 14: 0.87708694
	class 15: 0.8708394
Class Acc:
	class 0: 0.97334474
	class 1: 0.96827084
	class 2: 0.9023548
	class 3: 0.9370718
	class 4: 0.82420397
	class 5: 0.91378945
	class 6: 0.9611253
	class 7: 0.943438
	class 8: 0.95630234
	class 9: 0.543615
	class 10: 0.81308925
	class 11: 0.6501438
	class 12: 0.959766
	class 13: 0.89861745
	class 14: 0.93769276
	class 15: 0.9218551

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[3, 6, 12, 0]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.6412233710289001, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.6412233710289001, Class Loss=0.6412233710289001, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.5322420001029968, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.5322420001029968, Class Loss=0.5322420001029968, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.5137231349945068, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.5137231349945068, Class Loss=0.5137231349945068, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5458629727363586, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.5458629727363586, Class Loss=0.5458629727363586, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.700692355632782, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.700692355632782, Class Loss=0.700692355632782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6312942504882812, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.6312942504882812, Class Loss=0.6312942504882812, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.6661399602890015, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.6661399602890015, Class Loss=0.6661399602890015, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.5268603563308716, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.5268603563308716, Class Loss=0.5268603563308716, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.51572585105896, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.51572585105896, Class Loss=0.51572585105896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.555176854133606, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.555176854133606, Class Loss=0.555176854133606, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6093631386756897, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.6093631386756897, Class Loss=0.6093631386756897, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6587038040161133, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.6587038040161133, Class Loss=0.6587038040161133, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.6690873503684998, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.6690873503684998, Class Loss=0.6690873503684998, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.5329669713973999, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.5329669713973999, Class Loss=0.5329669713973999, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.5339845418930054, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.5339845418930054, Class Loss=0.5339845418930054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5371752977371216, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.5371752977371216, Class Loss=0.5371752977371216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.7161180973052979, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.7161180973052979, Class Loss=0.7161180973052979, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6909277439117432, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.6909277439117432, Class Loss=0.6909277439117432, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.6220937371253967, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.6220937371253967, Class Loss=0.6220937371253967, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.5507999658584595, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.5507999658584595, Class Loss=0.5507999658584595, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.523082435131073, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.523082435131073, Class Loss=0.523082435131073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5578756332397461, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.5578756332397461, Class Loss=0.5578756332397461, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.8151836395263672, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.8151836395263672, Class Loss=0.8151836395263672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=1.1080306768417358, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=1.1080306768417358, Class Loss=1.1080306768417358, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.29782289266586304, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.931042
Mean Acc: 0.858311
FreqW Acc: 0.878242
Mean IoU: 0.708861
Class IoU:
	class 0: 0.9181505
	class 1: 0.79189694
	class 2: 0.37929666
	class 3: 0.8248035
	class 4: 0.6335711
	class 5: 0.75503623
	class 6: 0.9190165
	class 7: 0.899159
	class 8: 0.88039947
	class 9: 0.45987672
	class 10: 0.71481085
	class 11: 0.6174264
	class 12: 0.8228721
	class 13: 0.7431684
	class 14: 0.83933353
	class 15: 0.8518227
	class 16: 0.0
Class Acc:
	class 0: 0.94098353
	class 1: 0.99171543
	class 2: 0.93096864
	class 3: 0.9788138
	class 4: 0.8657382
	class 5: 0.9478697
	class 6: 0.9908373
	class 7: 0.95873153
	class 8: 0.9672511
	class 9: 0.6729752
	class 10: 0.77274483
	class 11: 0.7250315
	class 12: 0.9769415
	class 13: 0.9572162
	class 14: 0.97085077
	class 15: 0.9426127
	class 16: 0.0

federated global round: 6, step: 1
select part of clients to conduct local training
[0, 2, 8, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.8580734729766846, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.8580734729766846, Class Loss=0.8580734729766846, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Class Loss=0.8840703964233398, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.8840703964233398, Class Loss=0.8840703964233398, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.8216389417648315, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.8216389417648315, Class Loss=0.8216389417648315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Class Loss=0.7676901817321777, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.7676901817321777, Class Loss=0.7676901817321777, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=1.0775506496429443, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=1.0775506496429443, Class Loss=1.0775506496429443, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Class Loss=0.6697906255722046, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.6697906255722046, Class Loss=0.6697906255722046, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.4444084167480469, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.4444084167480469, Class Loss=0.4444084167480469, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.4836280941963196, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.4836280941963196, Class Loss=0.4836280941963196, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.4977562129497528, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.4977562129497528, Class Loss=0.4977562129497528, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.5390642881393433, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.5390642881393433, Class Loss=0.5390642881393433, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.5881651043891907, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5881651043891907, Class Loss=0.5881651043891907, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6023495197296143, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.6023495197296143, Class Loss=0.6023495197296143, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.45973697304725647, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.45973697304725647, Class Loss=0.45973697304725647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.48171794414520264, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.48171794414520264, Class Loss=0.48171794414520264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.5100250244140625, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5100250244140625, Class Loss=0.5100250244140625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.5226768851280212, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5226768851280212, Class Loss=0.5226768851280212, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.6045176982879639, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6045176982879639, Class Loss=0.6045176982879639, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6084023714065552, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6084023714065552, Class Loss=0.6084023714065552, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.4461986720561981, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.4461986720561981, Class Loss=0.4461986720561981, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.4752045273780823, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.4752045273780823, Class Loss=0.4752045273780823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.4908919930458069, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.4908919930458069, Class Loss=0.4908919930458069, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.5372841954231262, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5372841954231262, Class Loss=0.5372841954231262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.5866706967353821, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5866706967353821, Class Loss=0.5866706967353821, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.5813027620315552, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5813027620315552, Class Loss=0.5813027620315552, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.27104610204696655, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.933056
Mean Acc: 0.858001
FreqW Acc: 0.881471
Mean IoU: 0.713769
Class IoU:
	class 0: 0.92053163
	class 1: 0.7904466
	class 2: 0.39524314
	class 3: 0.8494872
	class 4: 0.637925
	class 5: 0.7581036
	class 6: 0.9154286
	class 7: 0.89917773
	class 8: 0.89411217
	class 9: 0.44633025
	class 10: 0.72634715
	class 11: 0.6000951
	class 12: 0.837633
	class 13: 0.7569125
	class 14: 0.85257155
	class 15: 0.8537252
	class 16: 0.0
Class Acc:
	class 0: 0.9442851
	class 1: 0.992255
	class 2: 0.9426978
	class 3: 0.9745942
	class 4: 0.8687425
	class 5: 0.94480866
	class 6: 0.9931662
	class 7: 0.95867616
	class 8: 0.9653421
	class 9: 0.64663565
	class 10: 0.784758
	class 11: 0.72995603
	class 12: 0.9778135
	class 13: 0.95545727
	class 14: 0.9678006
	class 15: 0.9390346
	class 16: 0.0

federated global round: 7, step: 1
select part of clients to conduct local training
[9, 13, 1, 11]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.40417465567588806, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.40417465567588806, Class Loss=0.40417465567588806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.4265463650226593, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.4265463650226593, Class Loss=0.4265463650226593, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.455281525850296, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.455281525850296, Class Loss=0.455281525850296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.47478827834129333, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.47478827834129333, Class Loss=0.47478827834129333, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5244801044464111, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5244801044464111, Class Loss=0.5244801044464111, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.506727933883667, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.506727933883667, Class Loss=0.506727933883667, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.41311296820640564, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.41311296820640564, Class Loss=0.41311296820640564, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.45147764682769775, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.45147764682769775, Class Loss=0.45147764682769775, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.46177420020103455, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.46177420020103455, Class Loss=0.46177420020103455, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.5187754034996033, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5187754034996033, Class Loss=0.5187754034996033, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5423489212989807, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5423489212989807, Class Loss=0.5423489212989807, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.5226290225982666, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5226290225982666, Class Loss=0.5226290225982666, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.36483296751976013, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.36483296751976013, Class Loss=0.36483296751976013, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.39212116599082947, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.39212116599082947, Class Loss=0.39212116599082947, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.4343273639678955, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.4343273639678955, Class Loss=0.4343273639678955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.44027793407440186, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.44027793407440186, Class Loss=0.44027793407440186, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.699838399887085, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.699838399887085, Class Loss=0.699838399887085, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.8055086731910706, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.8055086731910706, Class Loss=0.8055086731910706, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.36320921778678894, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.36320921778678894, Class Loss=0.36320921778678894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.41406992077827454, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.41406992077827454, Class Loss=0.41406992077827454, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.43767181038856506, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.43767181038856506, Class Loss=0.43767181038856506, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.551773190498352, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.551773190498352, Class Loss=0.551773190498352, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.4773663580417633, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.4773663580417633, Class Loss=0.4773663580417633, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Class Loss=0.5562923550605774, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5562923550605774, Class Loss=0.5562923550605774, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.24919992685317993, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.937164
Mean Acc: 0.856431
FreqW Acc: 0.888320
Mean IoU: 0.726273
Class IoU:
	class 0: 0.92666346
	class 1: 0.8189998
	class 2: 0.4017111
	class 3: 0.87770665
	class 4: 0.65382344
	class 5: 0.773921
	class 6: 0.9276841
	class 7: 0.9082316
	class 8: 0.8934914
	class 9: 0.44603047
	class 10: 0.70927453
	class 11: 0.6101061
	class 12: 0.8334789
	class 13: 0.750167
	class 14: 0.8555193
	class 15: 0.86467195
	class 16: 0.0951562
Class Acc:
	class 0: 0.9526419
	class 1: 0.99052
	class 2: 0.93598783
	class 3: 0.9587436
	class 4: 0.8488031
	class 5: 0.9389806
	class 6: 0.98781675
	class 7: 0.95520025
	class 8: 0.9600086
	class 9: 0.6192547
	class 10: 0.7540024
	class 11: 0.72665966
	class 12: 0.9725992
	class 13: 0.96005565
	class 14: 0.9688233
	class 15: 0.92855704
	class 16: 0.10068162

federated global round: 8, step: 1
select part of clients to conduct local training
[8, 5, 2, 9]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.564111053943634, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.564111053943634, Class Loss=0.564111053943634, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Class Loss=0.5670172572135925, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.5670172572135925, Class Loss=0.5670172572135925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Class Loss=0.52426677942276, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.52426677942276, Class Loss=0.52426677942276, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Class Loss=0.5409546494483948, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5409546494483948, Class Loss=0.5409546494483948, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Class Loss=0.5148683786392212, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.5148683786392212, Class Loss=0.5148683786392212, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Class Loss=0.5174406170845032, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.5174406170845032, Class Loss=0.5174406170845032, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3222771883010864, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.3222771883010864, Class Loss=0.3222771883010864, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.3639332950115204, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3639332950115204, Class Loss=0.3639332950115204, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.38439497351646423, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.38439497351646423, Class Loss=0.38439497351646423, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.45223385095596313, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.45223385095596313, Class Loss=0.45223385095596313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5230445265769958, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.5230445265769958, Class Loss=0.5230445265769958, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.6971864700317383, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.6971864700317383, Class Loss=0.6971864700317383, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.5521594882011414, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.5521594882011414, Class Loss=0.5521594882011414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.6037415862083435, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.6037415862083435, Class Loss=0.6037415862083435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Class Loss=0.5556088089942932, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.5556088089942932, Class Loss=0.5556088089942932, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Class Loss=0.5488554835319519, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.5488554835319519, Class Loss=0.5488554835319519, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Class Loss=0.5721491575241089, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5721491575241089, Class Loss=0.5721491575241089, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Class Loss=0.6057610511779785, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.6057610511779785, Class Loss=0.6057610511779785, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.5473676323890686, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5473676323890686, Class Loss=0.5473676323890686, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.5271579027175903, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.5271579027175903, Class Loss=0.5271579027175903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.5427654981613159, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5427654981613159, Class Loss=0.5427654981613159, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.5233919620513916, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.5233919620513916, Class Loss=0.5233919620513916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.5472833514213562, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5472833514213562, Class Loss=0.5472833514213562, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.5027235746383667, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.5027235746383667, Class Loss=0.5027235746383667, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.24525973200798035, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.936784
Mean Acc: 0.864431
FreqW Acc: 0.888271
Mean IoU: 0.732565
Class IoU:
	class 0: 0.9254376
	class 1: 0.8253205
	class 2: 0.4064414
	class 3: 0.8801243
	class 4: 0.6593911
	class 5: 0.7609553
	class 6: 0.9272187
	class 7: 0.8944585
	class 8: 0.89588517
	class 9: 0.4454709
	class 10: 0.7411428
	class 11: 0.6009229
	class 12: 0.8327532
	class 13: 0.7816112
	class 14: 0.8533649
	class 15: 0.86067903
	class 16: 0.16242824
Class Acc:
	class 0: 0.94986415
	class 1: 0.9904708
	class 2: 0.9292406
	class 3: 0.96756643
	class 4: 0.8553124
	class 5: 0.9470424
	class 6: 0.9893156
	class 7: 0.96035737
	class 8: 0.95957035
	class 9: 0.615881
	class 10: 0.7940707
	class 11: 0.7205617
	class 12: 0.97442186
	class 13: 0.9492674
	class 14: 0.9588094
	class 15: 0.9324964
	class 16: 0.20107165

federated global round: 9, step: 1
select part of clients to conduct local training
[8, 9, 12, 4]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000414
Epoch 1, Class Loss=0.5230153799057007, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.5230153799057007, Class Loss=0.5230153799057007, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000351
Epoch 2, Class Loss=0.5181915163993835, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.5181915163993835, Class Loss=0.5181915163993835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000287
Epoch 3, Class Loss=0.5323246717453003, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5323246717453003, Class Loss=0.5323246717453003, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000222
Epoch 4, Class Loss=0.5107976198196411, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5107976198196411, Class Loss=0.5107976198196411, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000154
Epoch 5, Class Loss=0.5140420794487, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.5140420794487, Class Loss=0.5140420794487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000082
Epoch 6, Class Loss=0.5043383836746216, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.5043383836746216, Class Loss=0.5043383836746216, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000372
Epoch 1, Class Loss=0.5040270686149597, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5040270686149597, Class Loss=0.5040270686149597, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.525105357170105, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.525105357170105, Class Loss=0.525105357170105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000258
Epoch 3, Class Loss=0.5088699460029602, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5088699460029602, Class Loss=0.5088699460029602, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000199
Epoch 4, Class Loss=0.525219202041626, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.525219202041626, Class Loss=0.525219202041626, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000138
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.530812680721283, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.530812680721283, Class Loss=0.530812680721283, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.4894725978374481, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.4894725978374481, Class Loss=0.4894725978374481, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.5991069674491882, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.5991069674491882, Class Loss=0.5991069674491882, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Class Loss=0.6057478189468384, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.6057478189468384, Class Loss=0.6057478189468384, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Class Loss=0.5512169599533081, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.5512169599533081, Class Loss=0.5512169599533081, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Class Loss=0.5371857285499573, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.5371857285499573, Class Loss=0.5371857285499573, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Class Loss=0.5120665431022644, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5120665431022644, Class Loss=0.5120665431022644, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Class Loss=0.5123504996299744, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.5123504996299744, Class Loss=0.5123504996299744, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.5259063839912415, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.5259063839912415, Class Loss=0.5259063839912415, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Class Loss=0.5200148820877075, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.5200148820877075, Class Loss=0.5200148820877075, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.4943844974040985, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.4943844974040985, Class Loss=0.4943844974040985, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Class Loss=0.5061376094818115, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5061376094818115, Class Loss=0.5061376094818115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Class Loss=0.5057897567749023, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5057897567749023, Class Loss=0.5057897567749023, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.6062734127044678, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6062734127044678, Class Loss=0.6062734127044678, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.23084913194179535, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.939273
Mean Acc: 0.861552
FreqW Acc: 0.891984
Mean IoU: 0.738719
Class IoU:
	class 0: 0.9282451
	class 1: 0.8361011
	class 2: 0.40946713
	class 3: 0.8938052
	class 4: 0.66119134
	class 5: 0.7682955
	class 6: 0.9302684
	class 7: 0.90752685
	class 8: 0.89876574
	class 9: 0.44486615
	class 10: 0.7567776
	class 11: 0.60229725
	class 12: 0.84206396
	class 13: 0.7931769
	class 14: 0.8619133
	class 15: 0.8618134
	class 16: 0.16164494
Class Acc:
	class 0: 0.95436066
	class 1: 0.9876846
	class 2: 0.9236014
	class 3: 0.9591017
	class 4: 0.8428355
	class 5: 0.9422464
	class 6: 0.9894429
	class 7: 0.95483047
	class 8: 0.959088
	class 9: 0.6144149
	class 10: 0.8061066
	class 11: 0.71217215
	class 12: 0.9725305
	class 13: 0.948104
	class 14: 0.9625127
	class 15: 0.92972153
	class 16: 0.18763582

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[14, 6, 0, 16]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.532436728477478, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.532436728477478, Class Loss=0.532436728477478, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.9014698266983032, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.9014698266983032, Class Loss=0.9014698266983032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=1.0780677795410156, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=1.0780677795410156, Class Loss=1.0780677795410156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=1.1826627254486084, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=1.1826627254486084, Class Loss=1.1826627254486084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=1.16875422000885, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=1.16875422000885, Class Loss=1.16875422000885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=1.0893434286117554, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=1.0893434286117554, Class Loss=1.0893434286117554, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.5713179111480713, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.5713179111480713, Class Loss=0.5713179111480713, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=1.1262153387069702, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=1.1262153387069702, Class Loss=1.1262153387069702, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=1.2022989988327026, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=1.2022989988327026, Class Loss=1.2022989988327026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=1.2739287614822388, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=1.2739287614822388, Class Loss=1.2739287614822388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=1.2829428911209106, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=1.2829428911209106, Class Loss=1.2829428911209106, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=1.119468092918396, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=1.119468092918396, Class Loss=1.119468092918396, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.6736965775489807, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.6736965775489807, Class Loss=0.6736965775489807, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.825711727142334, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.825711727142334, Class Loss=0.825711727142334, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=1.1140706539154053, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=1.1140706539154053, Class Loss=1.1140706539154053, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=1.101618766784668, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=1.101618766784668, Class Loss=1.101618766784668, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.9930947422981262, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.9930947422981262, Class Loss=0.9930947422981262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.9221639633178711, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.9221639633178711, Class Loss=0.9221639633178711, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.5905713438987732, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.5905713438987732, Class Loss=0.5905713438987732, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.7782982587814331, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.7782982587814331, Class Loss=0.7782982587814331, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.8856926560401917, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.8856926560401917, Class Loss=0.8856926560401917, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.9037023186683655, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.9037023186683655, Class Loss=0.9037023186683655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.9473877549171448, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.9473877549171448, Class Loss=0.9473877549171448, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=1.3726125955581665, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=1.3726125955581665, Class Loss=1.3726125955581665, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.529077410697937, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.899993
Mean Acc: 0.753964
FreqW Acc: 0.833436
Mean IoU: 0.621569
Class IoU:
	class 0: 0.8930202
	class 1: 0.871927
	class 2: 0.3931173
	class 3: 0.8126295
	class 4: 0.61938065
	class 5: 0.7647748
	class 6: 0.9229071
	class 7: 0.83651006
	class 8: 0.69929224
	class 9: 0.3845709
	class 10: 0.54373264
	class 11: 0.520201
	class 12: 0.7202305
	class 13: 0.45944175
	class 14: 0.82480854
	class 15: 0.8158795
	class 16: 0.10486531
	class 17: 0.0009504544
Class Acc:
	class 0: 0.9409095
	class 1: 0.95026726
	class 2: 0.9269385
	class 3: 0.8613049
	class 4: 0.7940764
	class 5: 0.8976091
	class 6: 0.97255945
	class 7: 0.9609682
	class 8: 0.70849043
	class 9: 0.5310015
	class 10: 0.8967572
	class 11: 0.54510367
	class 12: 0.8938496
	class 13: 0.4916876
	class 14: 0.95741683
	class 15: 0.9170537
	class 16: 0.3243973
	class 17: 0.000952143

federated global round: 11, step: 2
select part of clients to conduct local training
[0, 12, 10, 17]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.9783270359039307, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.9783270359039307, Class Loss=0.9783270359039307, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Class Loss=0.7970227599143982, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.7970227599143982, Class Loss=0.7970227599143982, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Class Loss=0.8423572182655334, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.8423572182655334, Class Loss=0.8423572182655334, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Class Loss=0.7640682458877563, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.7640682458877563, Class Loss=0.7640682458877563, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.8034642934799194, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.8034642934799194, Class Loss=0.8034642934799194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Class Loss=0.853325366973877, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.853325366973877, Class Loss=0.853325366973877, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.36412256956100464, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.36412256956100464, Class Loss=0.36412256956100464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.6116610169410706, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.6116610169410706, Class Loss=0.6116610169410706, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.7565580606460571, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.7565580606460571, Class Loss=0.7565580606460571, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.7451972961425781, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.7451972961425781, Class Loss=0.7451972961425781, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7447529435157776, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.7447529435157776, Class Loss=0.7447529435157776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7174084186553955, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.7174084186553955, Class Loss=0.7174084186553955, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3603774905204773, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.3603774905204773, Class Loss=0.3603774905204773, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.5739178657531738, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.5739178657531738, Class Loss=0.5739178657531738, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.737865149974823, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.737865149974823, Class Loss=0.737865149974823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.7655253410339355, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.7655253410339355, Class Loss=0.7655253410339355, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7285146713256836, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.7285146713256836, Class Loss=0.7285146713256836, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6853843331336975, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6853843331336975, Class Loss=0.6853843331336975, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.3329552412033081, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.3329552412033081, Class Loss=0.3329552412033081, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.5635347366333008, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.5635347366333008, Class Loss=0.5635347366333008, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.688795268535614, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.688795268535614, Class Loss=0.688795268535614, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.7044756412506104, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.7044756412506104, Class Loss=0.7044756412506104, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7771511077880859, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.7771511077880859, Class Loss=0.7771511077880859, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7202684879302979, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.7202684879302979, Class Loss=0.7202684879302979, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4211445152759552, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.921085
Mean Acc: 0.807831
FreqW Acc: 0.864779
Mean IoU: 0.681385
Class IoU:
	class 0: 0.912899
	class 1: 0.85104316
	class 2: 0.396076
	class 3: 0.8552983
	class 4: 0.5855691
	class 5: 0.76851195
	class 6: 0.91410077
	class 7: 0.87353617
	class 8: 0.8542975
	class 9: 0.4099717
	class 10: 0.5684989
	class 11: 0.5616521
	class 12: 0.79602766
	class 13: 0.6967043
	class 14: 0.8296699
	class 15: 0.822063
	class 16: 0.07039414
	class 17: 0.4986076
Class Acc:
	class 0: 0.94852406
	class 1: 0.94260937
	class 2: 0.92545414
	class 3: 0.9037999
	class 4: 0.82846475
	class 5: 0.92673916
	class 6: 0.98296684
	class 7: 0.95446116
	class 8: 0.8951092
	class 9: 0.5768506
	class 10: 0.6087303
	class 11: 0.62229913
	class 12: 0.89032865
	class 13: 0.9089678
	class 14: 0.92472756
	class 15: 0.93708456
	class 16: 0.13383189
	class 17: 0.6300137

federated global round: 12, step: 2
select part of clients to conduct local training
[14, 8, 16, 3]
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.7086671590805054, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.7086671590805054, Class Loss=0.7086671590805054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.7227675914764404, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.7227675914764404, Class Loss=0.7227675914764404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Class Loss=0.7403729557991028, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.7403729557991028, Class Loss=0.7403729557991028, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Class Loss=0.6302450895309448, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.6302450895309448, Class Loss=0.6302450895309448, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Class Loss=0.5995123982429504, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5995123982429504, Class Loss=0.5995123982429504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Class Loss=0.5278047323226929, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5278047323226929, Class Loss=0.5278047323226929, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.25912153720855713, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.25912153720855713, Class Loss=0.25912153720855713, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.43842849135398865, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.43842849135398865, Class Loss=0.43842849135398865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.5404202938079834, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5404202938079834, Class Loss=0.5404202938079834, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.6037617325782776, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.6037617325782776, Class Loss=0.6037617325782776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6280646324157715, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6280646324157715, Class Loss=0.6280646324157715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6117613911628723, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6117613911628723, Class Loss=0.6117613911628723, Reg Loss=0.0
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.6738933324813843, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.6738933324813843, Class Loss=0.6738933324813843, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Class Loss=0.6078845262527466, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.6078845262527466, Class Loss=0.6078845262527466, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Class Loss=0.6386640667915344, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.6386640667915344, Class Loss=0.6386640667915344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.6848149299621582, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.6848149299621582, Class Loss=0.6848149299621582, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Class Loss=0.7428848147392273, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.7428848147392273, Class Loss=0.7428848147392273, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Class Loss=0.6778086423873901, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.6778086423873901, Class Loss=0.6778086423873901, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.32092341780662537, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.32092341780662537, Class Loss=0.32092341780662537, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.4480520188808441, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.4480520188808441, Class Loss=0.4480520188808441, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.5414042472839355, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.5414042472839355, Class Loss=0.5414042472839355, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.594956636428833, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.594956636428833, Class Loss=0.594956636428833, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6246984004974365, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.6246984004974365, Class Loss=0.6246984004974365, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6571618914604187, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.6571618914604187, Class Loss=0.6571618914604187, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3694877326488495, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.923396
Mean Acc: 0.801917
FreqW Acc: 0.866936
Mean IoU: 0.678374
Class IoU:
	class 0: 0.9179408
	class 1: 0.85497844
	class 2: 0.40749323
	class 3: 0.8549103
	class 4: 0.5925007
	class 5: 0.77901703
	class 6: 0.9128599
	class 7: 0.8829019
	class 8: 0.74880564
	class 9: 0.40367416
	class 10: 0.6185632
	class 11: 0.55695
	class 12: 0.78739446
	class 13: 0.7584787
	class 14: 0.8269219
	class 15: 0.83622926
	class 16: 0.03149795
	class 17: 0.4396225
Class Acc:
	class 0: 0.9582939
	class 1: 0.9399319
	class 2: 0.92319286
	class 3: 0.901071
	class 4: 0.84888303
	class 5: 0.92167217
	class 6: 0.9834115
	class 7: 0.9469731
	class 8: 0.7555564
	class 9: 0.5662465
	class 10: 0.68546635
	class 11: 0.6163471
	class 12: 0.8506656
	class 13: 0.849473
	class 14: 0.9006332
	class 15: 0.93410444
	class 16: 0.03766901
	class 17: 0.8149119

federated global round: 13, step: 2
select part of clients to conduct local training
[17, 13, 11, 7]
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.6146036982536316, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.6146036982536316, Class Loss=0.6146036982536316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Class Loss=0.6798248887062073, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.6798248887062073, Class Loss=0.6798248887062073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Class Loss=0.5753170251846313, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.5753170251846313, Class Loss=0.5753170251846313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.5375815033912659, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.5375815033912659, Class Loss=0.5375815033912659, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Class Loss=0.5476018190383911, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.5476018190383911, Class Loss=0.5476018190383911, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Class Loss=0.5223015546798706, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.5223015546798706, Class Loss=0.5223015546798706, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.23150762915611267, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.23150762915611267, Class Loss=0.23150762915611267, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.36816170811653137, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.36816170811653137, Class Loss=0.36816170811653137, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.4444486200809479, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.4444486200809479, Class Loss=0.4444486200809479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.5310211181640625, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5310211181640625, Class Loss=0.5310211181640625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5598723888397217, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5598723888397217, Class Loss=0.5598723888397217, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.5572183132171631, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5572183132171631, Class Loss=0.5572183132171631, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.18531867861747742, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.18531867861747742, Class Loss=0.18531867861747742, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.4020666480064392, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.4020666480064392, Class Loss=0.4020666480064392, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.4690318703651428, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4690318703651428, Class Loss=0.4690318703651428, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.5055370330810547, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5055370330810547, Class Loss=0.5055370330810547, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5132047533988953, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5132047533988953, Class Loss=0.5132047533988953, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.5583635568618774, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5583635568618774, Class Loss=0.5583635568618774, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.2148735225200653, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.2148735225200653, Class Loss=0.2148735225200653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.4074676036834717, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.4074676036834717, Class Loss=0.4074676036834717, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.4646275043487549, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.4646275043487549, Class Loss=0.4646275043487549, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.5421828031539917, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.5421828031539917, Class Loss=0.5421828031539917, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5555931329727173, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.5555931329727173, Class Loss=0.5555931329727173, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.5594749450683594, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.5594749450683594, Class Loss=0.5594749450683594, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3213377892971039, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.928689
Mean Acc: 0.813862
FreqW Acc: 0.874630
Mean IoU: 0.690189
Class IoU:
	class 0: 0.92153686
	class 1: 0.84687483
	class 2: 0.40794298
	class 3: 0.8548229
	class 4: 0.58541995
	class 5: 0.7769486
	class 6: 0.9199201
	class 7: 0.88615686
	class 8: 0.8477903
	class 9: 0.40563288
	class 10: 0.63962126
	class 11: 0.5706112
	class 12: 0.81583965
	class 13: 0.7641349
	class 14: 0.82582104
	class 15: 0.8396235
	class 16: 0.014984369
	class 17: 0.49971274
Class Acc:
	class 0: 0.9582166
	class 1: 0.95736647
	class 2: 0.9275649
	class 3: 0.911188
	class 4: 0.8619279
	class 5: 0.91900206
	class 6: 0.98369575
	class 7: 0.9469325
	class 8: 0.8670168
	class 9: 0.56503177
	class 10: 0.6989303
	class 11: 0.6465175
	class 12: 0.88599116
	class 13: 0.88679147
	class 14: 0.9031519
	class 15: 0.9325811
	class 16: 0.016102945
	class 17: 0.7815107

federated global round: 14, step: 2
select part of clients to conduct local training
[17, 4, 5, 2]
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000414
Epoch 1, Class Loss=0.5476769804954529, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.5476769804954529, Class Loss=0.5476769804954529, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000351
Epoch 2, Class Loss=0.5280992984771729, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.5280992984771729, Class Loss=0.5280992984771729, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000287
Epoch 3, Class Loss=0.5214669108390808, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.5214669108390808, Class Loss=0.5214669108390808, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000222
Epoch 4, Class Loss=0.556999921798706, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.556999921798706, Class Loss=0.556999921798706, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000154
Epoch 5, Class Loss=0.5892940759658813, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.5892940759658813, Class Loss=0.5892940759658813, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000082
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Class Loss=0.5523413419723511, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.5523413419723511, Class Loss=0.5523413419723511, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.2064121663570404, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.2064121663570404, Class Loss=0.2064121663570404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.34831470251083374, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.34831470251083374, Class Loss=0.34831470251083374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.42963510751724243, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.42963510751724243, Class Loss=0.42963510751724243, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5002481937408447, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5002481937408447, Class Loss=0.5002481937408447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.5354045033454895, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5354045033454895, Class Loss=0.5354045033454895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.5246953964233398, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5246953964233398, Class Loss=0.5246953964233398, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.20569205284118652, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.20569205284118652, Class Loss=0.20569205284118652, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.3507543206214905, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3507543206214905, Class Loss=0.3507543206214905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.4691378176212311, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.4691378176212311, Class Loss=0.4691378176212311, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.4903883934020996, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.4903883934020996, Class Loss=0.4903883934020996, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.5469457507133484, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.5469457507133484, Class Loss=0.5469457507133484, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.5700374245643616, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.5700374245643616, Class Loss=0.5700374245643616, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.21075627207756042, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.21075627207756042, Class Loss=0.21075627207756042, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.3756842613220215, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.3756842613220215, Class Loss=0.3756842613220215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.4471377432346344, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.4471377432346344, Class Loss=0.4471377432346344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5028215050697327, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.5028215050697327, Class Loss=0.5028215050697327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.5175126791000366, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5175126791000366, Class Loss=0.5175126791000366, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.5266704559326172, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.5266704559326172, Class Loss=0.5266704559326172, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3091500401496887, Reg Loss=0.0 (without scaling)

Total samples: 1329.000000
Overall Acc: 0.929384
Mean Acc: 0.809909
FreqW Acc: 0.875254
Mean IoU: 0.690630
Class IoU:
	class 0: 0.9224466
	class 1: 0.85304785
	class 2: 0.40700388
	class 3: 0.8574524
	class 4: 0.59412646
	class 5: 0.7769715
	class 6: 0.91938084
	class 7: 0.8889038
	class 8: 0.8481641
	class 9: 0.40778685
	class 10: 0.62419474
	class 11: 0.5719743
	class 12: 0.80273354
	class 13: 0.76683533
	class 14: 0.82451737
	class 15: 0.84291804
	class 16: 0.010184878
	class 17: 0.51270384
Class Acc:
	class 0: 0.96071917
	class 1: 0.95410496
	class 2: 0.92781574
	class 3: 0.9101636
	class 4: 0.8589462
	class 5: 0.9223683
	class 6: 0.9827888
	class 7: 0.9461365
	class 8: 0.8666773
	class 9: 0.5675018
	class 10: 0.6752293
	class 11: 0.63554204
	class 12: 0.88728
	class 13: 0.8613615
	class 14: 0.89837354
	class 15: 0.93185705
	class 16: 0.010697202
	class 17: 0.78079456

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[17, 21, 10, 5]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.28223758935928345, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.28223758935928345, Class Loss=0.28223758935928345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.37935230135917664, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.37935230135917664, Class Loss=0.37935230135917664, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6861885190010071, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.6861885190010071, Class Loss=0.6861885190010071, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.8214902877807617, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.8214902877807617, Class Loss=0.8214902877807617, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.839515209197998, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.839515209197998, Class Loss=0.839515209197998, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.8719987869262695, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.8719987869262695, Class Loss=0.8719987869262695, Reg Loss=0.0
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.26595333218574524, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.26595333218574524, Class Loss=0.26595333218574524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.4132530987262726, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.4132530987262726, Class Loss=0.4132530987262726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6917768716812134, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.6917768716812134, Class Loss=0.6917768716812134, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.8040890097618103, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.8040890097618103, Class Loss=0.8040890097618103, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.8248583674430847, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.8248583674430847, Class Loss=0.8248583674430847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.8345049023628235, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.8345049023628235, Class Loss=0.8345049023628235, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.23394261300563812, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.23394261300563812, Class Loss=0.23394261300563812, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.4023912847042084, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.4023912847042084, Class Loss=0.4023912847042084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6665889024734497, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.6665889024734497, Class Loss=0.6665889024734497, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.7873082160949707, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.7873082160949707, Class Loss=0.7873082160949707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.8614619374275208, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.8614619374275208, Class Loss=0.8614619374275208, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.8537845611572266, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.8537845611572266, Class Loss=0.8537845611572266, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.2678031325340271, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.2678031325340271, Class Loss=0.2678031325340271, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.38360828161239624, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.38360828161239624, Class Loss=0.38360828161239624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6764611005783081, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.6764611005783081, Class Loss=0.6764611005783081, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.7926762104034424, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.7926762104034424, Class Loss=0.7926762104034424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.8118412494659424, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.8118412494659424, Class Loss=0.8118412494659424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.8341173529624939, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.8341173529624939, Class Loss=0.8341173529624939, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5247132778167725, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.910648
Mean Acc: 0.788021
FreqW Acc: 0.844822
Mean IoU: 0.644041
Class IoU:
	class 0: 0.901655
	class 1: 0.8299434
	class 2: 0.38071796
	class 3: 0.8452955
	class 4: 0.5785965
	class 5: 0.7398578
	class 6: 0.89611274
	class 7: 0.87243855
	class 8: 0.8691386
	class 9: 0.41789958
	class 10: 0.6319013
	class 11: 0.556321
	class 12: 0.81873137
	class 13: 0.71445537
	class 14: 0.7893266
	class 15: 0.84369224
	class 16: 0.15603326
	class 17: 0.38628927
	class 18: 0.008380386
Class Acc:
	class 0: 0.944836
	class 1: 0.9614063
	class 2: 0.94997114
	class 3: 0.94881296
	class 4: 0.85088015
	class 5: 0.9392994
	class 6: 0.9916571
	class 7: 0.9476771
	class 8: 0.9308267
	class 9: 0.5935951
	class 10: 0.84832925
	class 11: 0.7143349
	class 12: 0.9488728
	class 13: 0.8004964
	class 14: 0.94485086
	class 15: 0.9347608
	class 16: 0.21449947
	class 17: 0.4987092
	class 18: 0.008583115

federated global round: 16, step: 3
select part of clients to conduct local training
[19, 11, 9, 1]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.12440486252307892, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.12440486252307892, Class Loss=0.12440486252307892, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.28093236684799194, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.28093236684799194, Class Loss=0.28093236684799194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.49266132712364197, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.49266132712364197, Class Loss=0.49266132712364197, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.6234415769577026, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.6234415769577026, Class Loss=0.6234415769577026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.774071455001831, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.774071455001831, Class Loss=0.774071455001831, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7546414732933044, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.7546414732933044, Class Loss=0.7546414732933044, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.12102434039115906, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.12102434039115906, Class Loss=0.12102434039115906, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.30079391598701477, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.30079391598701477, Class Loss=0.30079391598701477, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.4825945198535919, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4825945198535919, Class Loss=0.4825945198535919, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.6481706500053406, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.6481706500053406, Class Loss=0.6481706500053406, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7141354084014893, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.7141354084014893, Class Loss=0.7141354084014893, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7506384253501892, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.7506384253501892, Class Loss=0.7506384253501892, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.12159086763858795, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.12159086763858795, Class Loss=0.12159086763858795, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.2871953547000885, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.2871953547000885, Class Loss=0.2871953547000885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.4797278940677643, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.4797278940677643, Class Loss=0.4797278940677643, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.6144723892211914, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.6144723892211914, Class Loss=0.6144723892211914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7027943730354309, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.7027943730354309, Class Loss=0.7027943730354309, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7347537279129028, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.7347537279129028, Class Loss=0.7347537279129028, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.11538221687078476, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.11538221687078476, Class Loss=0.11538221687078476, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.30472347140312195, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.30472347140312195, Class Loss=0.30472347140312195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.523820161819458, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.523820161819458, Class Loss=0.523820161819458, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.6226664781570435, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.6226664781570435, Class Loss=0.6226664781570435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7336020469665527, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.7336020469665527, Class Loss=0.7336020469665527, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7549104690551758, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.7549104690551758, Class Loss=0.7549104690551758, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.48329025506973267, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.900933
Mean Acc: 0.799958
FreqW Acc: 0.838578
Mean IoU: 0.647393
Class IoU:
	class 0: 0.8893985
	class 1: 0.81972206
	class 2: 0.37166327
	class 3: 0.8352979
	class 4: 0.584821
	class 5: 0.72768605
	class 6: 0.9021769
	class 7: 0.8827582
	class 8: 0.87497324
	class 9: 0.41365454
	class 10: 0.6119113
	class 11: 0.5443719
	class 12: 0.834422
	class 13: 0.7219415
	class 14: 0.8021203
	class 15: 0.84353995
	class 16: 0.116857715
	class 17: 0.37251145
	class 18: 0.15062985
Class Acc:
	class 0: 0.9232662
	class 1: 0.964696
	class 2: 0.95169485
	class 3: 0.9554474
	class 4: 0.856491
	class 5: 0.93442184
	class 6: 0.9886233
	class 7: 0.94546026
	class 8: 0.94419736
	class 9: 0.60856444
	class 10: 0.86639655
	class 11: 0.7279805
	class 12: 0.94980085
	class 13: 0.8153149
	class 14: 0.9458859
	class 15: 0.93985486
	class 16: 0.1392287
	class 17: 0.43249387
	class 18: 0.30937344

federated global round: 17, step: 3
select part of clients to conduct local training
[8, 5, 21, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.10028006881475449, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.10028006881475449, Class Loss=0.10028006881475449, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.2442227154970169, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.2442227154970169, Class Loss=0.2442227154970169, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.42097848653793335, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.42097848653793335, Class Loss=0.42097848653793335, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.5496083498001099, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5496083498001099, Class Loss=0.5496083498001099, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6405096650123596, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6405096650123596, Class Loss=0.6405096650123596, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6876028776168823, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6876028776168823, Class Loss=0.6876028776168823, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.7578931450843811, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.7578931450843811, Class Loss=0.7578931450843811, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Class Loss=0.7373986840248108, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.7373986840248108, Class Loss=0.7373986840248108, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Class Loss=0.7165776491165161, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.7165776491165161, Class Loss=0.7165776491165161, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Class Loss=0.7177174687385559, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.7177174687385559, Class Loss=0.7177174687385559, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Class Loss=0.6792352199554443, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.6792352199554443, Class Loss=0.6792352199554443, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Class Loss=0.7007830739021301, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.7007830739021301, Class Loss=0.7007830739021301, Reg Loss=0.0
Current Client Index:  21
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.8048535585403442, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.8048535585403442, Class Loss=0.8048535585403442, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Class Loss=0.7418937087059021, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.7418937087059021, Class Loss=0.7418937087059021, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Class Loss=0.7218388915061951, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.7218388915061951, Class Loss=0.7218388915061951, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Class Loss=0.7382074594497681, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.7382074594497681, Class Loss=0.7382074594497681, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Class Loss=0.681500256061554, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.681500256061554, Class Loss=0.681500256061554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Class Loss=0.674676775932312, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.674676775932312, Class Loss=0.674676775932312, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.0924844741821289, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.0924844741821289, Class Loss=0.0924844741821289, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.25378209352493286, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.25378209352493286, Class Loss=0.25378209352493286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.4194640517234802, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.4194640517234802, Class Loss=0.4194640517234802, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.561398983001709, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.561398983001709, Class Loss=0.561398983001709, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6409261226654053, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.6409261226654053, Class Loss=0.6409261226654053, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.697089433670044, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.697089433670044, Class Loss=0.697089433670044, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4456911087036133, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.898763
Mean Acc: 0.796320
FreqW Acc: 0.836907
Mean IoU: 0.637221
Class IoU:
	class 0: 0.8889981
	class 1: 0.8249863
	class 2: 0.3801241
	class 3: 0.7693074
	class 4: 0.5898838
	class 5: 0.73015314
	class 6: 0.90761405
	class 7: 0.8756838
	class 8: 0.8697455
	class 9: 0.41281688
	class 10: 0.5930592
	class 11: 0.555893
	class 12: 0.8317997
	class 13: 0.7294889
	class 14: 0.80660504
	class 15: 0.8491663
	class 16: 0.04391979
	class 17: 0.25024977
	class 18: 0.19769344
Class Acc:
	class 0: 0.919519
	class 1: 0.9697116
	class 2: 0.9495189
	class 3: 0.974516
	class 4: 0.8730154
	class 5: 0.93840295
	class 6: 0.9876259
	class 7: 0.9498849
	class 8: 0.9306107
	class 9: 0.6023357
	class 10: 0.8744683
	class 11: 0.70331824
	class 12: 0.9516297
	class 13: 0.82070893
	class 14: 0.94899493
	class 15: 0.93765956
	class 16: 0.044892974
	class 17: 0.26828277
	class 18: 0.4849843

federated global round: 18, step: 3
select part of clients to conduct local training
[14, 4, 11, 21]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.07541931420564651, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.07541931420564651, Class Loss=0.07541931420564651, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.22028957307338715, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.22028957307338715, Class Loss=0.22028957307338715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.3935558795928955, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.3935558795928955, Class Loss=0.3935558795928955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.49800822138786316, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.49800822138786316, Class Loss=0.49800822138786316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5924729704856873, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5924729704856873, Class Loss=0.5924729704856873, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.6637503504753113, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.6637503504753113, Class Loss=0.6637503504753113, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.09846826642751694, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.09846826642751694, Class Loss=0.09846826642751694, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.21539172530174255, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.21539172530174255, Class Loss=0.21539172530174255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.3619813919067383, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.3619813919067383, Class Loss=0.3619813919067383, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.4956483542919159, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.4956483542919159, Class Loss=0.4956483542919159, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5707213282585144, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5707213282585144, Class Loss=0.5707213282585144, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.6367417573928833, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6367417573928833, Class Loss=0.6367417573928833, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.6975159049034119, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.6975159049034119, Class Loss=0.6975159049034119, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Class Loss=0.6747432351112366, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.6747432351112366, Class Loss=0.6747432351112366, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Class Loss=0.6715811491012573, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.6715811491012573, Class Loss=0.6715811491012573, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Class Loss=0.6619725823402405, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.6619725823402405, Class Loss=0.6619725823402405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Class Loss=0.6245996356010437, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.6245996356010437, Class Loss=0.6245996356010437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Class Loss=0.6176848411560059, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.6176848411560059, Class Loss=0.6176848411560059, Reg Loss=0.0
Current Client Index:  21
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Class Loss=0.7085239887237549, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.7085239887237549, Class Loss=0.7085239887237549, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Class Loss=0.6680052280426025, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.6680052280426025, Class Loss=0.6680052280426025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Class Loss=0.6635898351669312, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.6635898351669312, Class Loss=0.6635898351669312, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Class Loss=0.7269615530967712, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.7269615530967712, Class Loss=0.7269615530967712, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Class Loss=0.6290807127952576, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.6290807127952576, Class Loss=0.6290807127952576, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Class Loss=0.6354849934577942, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.6354849934577942, Class Loss=0.6354849934577942, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.41259604692459106, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.900653
Mean Acc: 0.790836
FreqW Acc: 0.840083
Mean IoU: 0.640651
Class IoU:
	class 0: 0.8920819
	class 1: 0.84480226
	class 2: 0.3905222
	class 3: 0.8180484
	class 4: 0.60494775
	class 5: 0.7364803
	class 6: 0.9119385
	class 7: 0.8792943
	class 8: 0.8717828
	class 9: 0.41588336
	class 10: 0.60404664
	class 11: 0.5550092
	class 12: 0.82878286
	class 13: 0.736586
	class 14: 0.8130933
	class 15: 0.85028726
	class 16: 0.021069383
	class 17: 0.18549845
	class 18: 0.212211
Class Acc:
	class 0: 0.9231918
	class 1: 0.96098834
	class 2: 0.94039714
	class 3: 0.96362805
	class 4: 0.86722136
	class 5: 0.9257301
	class 6: 0.98636293
	class 7: 0.9444559
	class 8: 0.93078005
	class 9: 0.5953286
	class 10: 0.86783814
	class 11: 0.7043404
	class 12: 0.94734377
	class 13: 0.8239078
	class 14: 0.94135946
	class 15: 0.939214
	class 16: 0.02111457
	class 17: 0.19287986
	class 18: 0.5498047

federated global round: 19, step: 3
select part of clients to conduct local training
[15, 12, 21, 16]
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.08235147595405579, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.08235147595405579, Class Loss=0.08235147595405579, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.19180965423583984, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.19180965423583984, Class Loss=0.19180965423583984, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.30866652727127075, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.30866652727127075, Class Loss=0.30866652727127075, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.4347064793109894, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.4347064793109894, Class Loss=0.4347064793109894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.5477871894836426, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.5477871894836426, Class Loss=0.5477871894836426, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.6292210221290588, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.6292210221290588, Class Loss=0.6292210221290588, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.07515706866979599, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.07515706866979599, Class Loss=0.07515706866979599, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.18828734755516052, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.18828734755516052, Class Loss=0.18828734755516052, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.3381190896034241, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.3381190896034241, Class Loss=0.3381190896034241, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.4496297836303711, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.4496297836303711, Class Loss=0.4496297836303711, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.5341877341270447, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5341877341270447, Class Loss=0.5341877341270447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.6415737867355347, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.6415737867355347, Class Loss=0.6415737867355347, Reg Loss=0.0
Current Client Index:  21
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000304
Epoch 1, Class Loss=0.6707427501678467, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.6707427501678467, Class Loss=0.6707427501678467, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000258
Epoch 2, Class Loss=0.6344648599624634, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.6344648599624634, Class Loss=0.6344648599624634, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000211
Epoch 3, Class Loss=0.6311216950416565, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.6311216950416565, Class Loss=0.6311216950416565, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000163
Epoch 4, Class Loss=0.672334611415863, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.672334611415863, Class Loss=0.672334611415863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000113
Epoch 5, Class Loss=0.6110295057296753, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.6110295057296753, Class Loss=0.6110295057296753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000061
Epoch 6, Class Loss=0.623022198677063, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.623022198677063, Class Loss=0.623022198677063, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.08828762173652649, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.08828762173652649, Class Loss=0.08828762173652649, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.18506327271461487, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.18506327271461487, Class Loss=0.18506327271461487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.3425115942955017, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.3425115942955017, Class Loss=0.3425115942955017, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.44090497493743896, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.44090497493743896, Class Loss=0.44090497493743896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.5589911341667175, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.5589911341667175, Class Loss=0.5589911341667175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.6242703795433044, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.6242703795433044, Class Loss=0.6242703795433044, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4398896098136902, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.893144
Mean Acc: 0.797116
FreqW Acc: 0.832355
Mean IoU: 0.636457
Class IoU:
	class 0: 0.8831501
	class 1: 0.8264848
	class 2: 0.37591794
	class 3: 0.84802663
	class 4: 0.59059745
	class 5: 0.7250299
	class 6: 0.90470237
	class 7: 0.8745872
	class 8: 0.86985105
	class 9: 0.41356137
	class 10: 0.6051564
	class 11: 0.5540125
	class 12: 0.8153768
	class 13: 0.73370874
	class 14: 0.808962
	class 15: 0.8412867
	class 16: 0.024069171
	class 17: 0.19274215
	class 18: 0.20546438
Class Acc:
	class 0: 0.9092405
	class 1: 0.9689615
	class 2: 0.9485986
	class 3: 0.9415871
	class 4: 0.877855
	class 5: 0.9373632
	class 6: 0.98803174
	class 7: 0.9473459
	class 8: 0.93530864
	class 9: 0.5987409
	class 10: 0.8637126
	class 11: 0.7119884
	class 12: 0.9479458
	class 13: 0.8288084
	class 14: 0.9415382
	class 15: 0.944705
	class 16: 0.024251537
	class 17: 0.1999702
	class 18: 0.62925696

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[9, 12, 24, 8]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.05180402100086212, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.05180402100086212, Class Loss=0.05180402100086212, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.26562967896461487, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.26562967896461487, Class Loss=0.26562967896461487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6242327094078064, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.6242327094078064, Class Loss=0.6242327094078064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.9047532677650452, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.9047532677650452, Class Loss=0.9047532677650452, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.9348040819168091, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.9348040819168091, Class Loss=0.9348040819168091, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.8681164383888245, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.8681164383888245, Class Loss=0.8681164383888245, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.04792783409357071, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.04792783409357071, Class Loss=0.04792783409357071, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.2771555185317993, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.2771555185317993, Class Loss=0.2771555185317993, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6991431713104248, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.6991431713104248, Class Loss=0.6991431713104248, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.9294244050979614, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.9294244050979614, Class Loss=0.9294244050979614, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.924990177154541, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.924990177154541, Class Loss=0.924990177154541, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.8572262525558472, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.8572262525558472, Class Loss=0.8572262525558472, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.045060910284519196, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.045060910284519196, Class Loss=0.045060910284519196, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.29350507259368896, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.29350507259368896, Class Loss=0.29350507259368896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.7270351648330688, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.7270351648330688, Class Loss=0.7270351648330688, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.9269136190414429, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.9269136190414429, Class Loss=0.9269136190414429, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.9323641657829285, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.9323641657829285, Class Loss=0.9323641657829285, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.8587096333503723, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.8587096333503723, Class Loss=0.8587096333503723, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.042523663491010666, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.042523663491010666, Class Loss=0.042523663491010666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.25681638717651367, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.25681638717651367, Class Loss=0.25681638717651367, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6698064208030701, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.6698064208030701, Class Loss=0.6698064208030701, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.9319466352462769, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.9319466352462769, Class Loss=0.9319466352462769, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.9556683897972107, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.9556683897972107, Class Loss=0.9556683897972107, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.8831719160079956, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.8831719160079956, Class Loss=0.8831719160079956, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.6525607109069824, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.887679
Mean Acc: 0.707706
FreqW Acc: 0.811315
Mean IoU: 0.585795
Class IoU:
	class 0: 0.8818489
	class 1: 0.7766394
	class 2: 0.391986
	class 3: 0.80691034
	class 4: 0.57563007
	class 5: 0.70783323
	class 6: 0.83651716
	class 7: 0.8302043
	class 8: 0.80471945
	class 9: 0.28604567
	class 10: 0.6019673
	class 11: 0.51440006
	class 12: 0.71220595
	class 13: 0.66467434
	class 14: 0.78442806
	class 15: 0.8155842
	class 16: 0.0
	class 17: 0.15860078
	class 18: 0.038529236
	class 19: 0.52717215
Class Acc:
	class 0: 0.9439718
	class 1: 0.8493354
	class 2: 0.9355184
	class 3: 0.87203604
	class 4: 0.7293822
	class 5: 0.8667146
	class 6: 0.91585004
	class 7: 0.9349117
	class 8: 0.83831304
	class 9: 0.570742
	class 10: 0.74395245
	class 11: 0.6539578
	class 12: 0.90784615
	class 13: 0.74746907
	class 14: 0.9051361
	class 15: 0.9296367
	class 16: 0.0
	class 17: 0.16221319
	class 18: 0.061999556
	class 19: 0.5851407

federated global round: 21, step: 4
select part of clients to conduct local training
[5, 8, 0, 6]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.028702689334750175, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.028702689334750175, Class Loss=0.028702689334750175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.16738522052764893, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.16738522052764893, Class Loss=0.16738522052764893, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.4301019012928009, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.4301019012928009, Class Loss=0.4301019012928009, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.6319857239723206, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.6319857239723206, Class Loss=0.6319857239723206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.7298539280891418, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.7298539280891418, Class Loss=0.7298539280891418, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7200208306312561, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.7200208306312561, Class Loss=0.7200208306312561, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.8775173425674438, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.8775173425674438, Class Loss=0.8775173425674438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Class Loss=0.7757244110107422, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.7757244110107422, Class Loss=0.7757244110107422, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Class Loss=0.6835519671440125, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.6835519671440125, Class Loss=0.6835519671440125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Class Loss=0.6423041224479675, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.6423041224479675, Class Loss=0.6423041224479675, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.598846435546875, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.598846435546875, Class Loss=0.598846435546875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Class Loss=0.5878340601921082, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.5878340601921082, Class Loss=0.5878340601921082, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.029535433277487755, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.029535433277487755, Class Loss=0.029535433277487755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.15446142852306366, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.15446142852306366, Class Loss=0.15446142852306366, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.4075700342655182, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.4075700342655182, Class Loss=0.4075700342655182, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.6179401874542236, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.6179401874542236, Class Loss=0.6179401874542236, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.6929644346237183, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.6929644346237183, Class Loss=0.6929644346237183, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7029711008071899, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.7029711008071899, Class Loss=0.7029711008071899, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.03044651262462139, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.03044651262462139, Class Loss=0.03044651262462139, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.1606871336698532, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.1606871336698532, Class Loss=0.1606871336698532, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.4041120707988739, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4041120707988739, Class Loss=0.4041120707988739, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.6111600995063782, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.6111600995063782, Class Loss=0.6111600995063782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7053140997886658, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.7053140997886658, Class Loss=0.7053140997886658, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.684792160987854, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.684792160987854, Class Loss=0.684792160987854, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5361697673797607, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.894408
Mean Acc: 0.708766
FreqW Acc: 0.818616
Mean IoU: 0.590368
Class IoU:
	class 0: 0.8901387
	class 1: 0.80223227
	class 2: 0.38828087
	class 3: 0.8112629
	class 4: 0.58519113
	class 5: 0.71696645
	class 6: 0.8591503
	class 7: 0.85139924
	class 8: 0.80203635
	class 9: 0.3014008
	class 10: 0.61014843
	class 11: 0.54120547
	class 12: 0.7430691
	class 13: 0.6695003
	class 14: 0.8021193
	class 15: 0.82020706
	class 16: 0.0
	class 17: 0.11172286
	class 18: 0.018265113
	class 19: 0.48306778
Class Acc:
	class 0: 0.95179224
	class 1: 0.8540538
	class 2: 0.93719196
	class 3: 0.8518136
	class 4: 0.7439107
	class 5: 0.8612795
	class 6: 0.9180345
	class 7: 0.94011295
	class 8: 0.82424676
	class 9: 0.5347731
	class 10: 0.75596404
	class 11: 0.6351635
	class 12: 0.90568674
	class 13: 0.7575776
	class 14: 0.8989955
	class 15: 0.9313656
	class 16: 0.0
	class 17: 0.11295824
	class 18: 0.021092767
	class 19: 0.7393114

federated global round: 22, step: 4
select part of clients to conduct local training
[8, 25, 22, 5]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Epoch 1, Class Loss=0.6334670186042786, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.6334670186042786, Class Loss=0.6334670186042786, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000600
Epoch 2, Class Loss=0.6367908716201782, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.6367908716201782, Class Loss=0.6367908716201782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Class Loss=0.5854191780090332, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5854191780090332, Class Loss=0.5854191780090332, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5518666505813599, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5518666505813599, Class Loss=0.5518666505813599, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000504
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.5378129482269287, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.5378129482269287, Class Loss=0.5378129482269287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000471
Epoch 6, Class Loss=0.5329003930091858, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.5329003930091858, Class Loss=0.5329003930091858, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.021580753847956657, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.021580753847956657, Class Loss=0.021580753847956657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.11510225385427475, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.11510225385427475, Class Loss=0.11510225385427475, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.321022093296051, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.321022093296051, Class Loss=0.321022093296051, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.5015671849250793, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.5015671849250793, Class Loss=0.5015671849250793, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5879971385002136, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.5879971385002136, Class Loss=0.5879971385002136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.5873246788978577, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.5873246788978577, Class Loss=0.5873246788978577, Reg Loss=0.0
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.022858012467622757, Reg Loss=0.0
Clinet index 22, End of Epoch 1/6, Average Loss=0.022858012467622757, Class Loss=0.022858012467622757, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.11291017383337021, Reg Loss=0.0
Clinet index 22, End of Epoch 2/6, Average Loss=0.11291017383337021, Class Loss=0.11291017383337021, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.3155812919139862, Reg Loss=0.0
Clinet index 22, End of Epoch 3/6, Average Loss=0.3155812919139862, Class Loss=0.3155812919139862, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.5009173154830933, Reg Loss=0.0
Clinet index 22, End of Epoch 4/6, Average Loss=0.5009173154830933, Class Loss=0.5009173154830933, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.594698429107666, Reg Loss=0.0
Clinet index 22, End of Epoch 5/6, Average Loss=0.594698429107666, Class Loss=0.594698429107666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6017303466796875, Reg Loss=0.0
Clinet index 22, End of Epoch 6/6, Average Loss=0.6017303466796875, Class Loss=0.6017303466796875, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.6615733504295349, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.6615733504295349, Class Loss=0.6615733504295349, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Class Loss=0.6419110894203186, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.6419110894203186, Class Loss=0.6419110894203186, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.623180627822876, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.623180627822876, Class Loss=0.623180627822876, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Class Loss=0.592258870601654, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.592258870601654, Class Loss=0.592258870601654, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Class Loss=0.536920964717865, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.536920964717865, Class Loss=0.536920964717865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Class Loss=0.5429043769836426, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.5429043769836426, Class Loss=0.5429043769836426, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4583512544631958, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.897533
Mean Acc: 0.698319
FreqW Acc: 0.820358
Mean IoU: 0.591473
Class IoU:
	class 0: 0.89202094
	class 1: 0.8098481
	class 2: 0.41098565
	class 3: 0.8147608
	class 4: 0.59994316
	class 5: 0.7194453
	class 6: 0.86746186
	class 7: 0.8510939
	class 8: 0.76391536
	class 9: 0.3200806
	class 10: 0.6206852
	class 11: 0.5319666
	class 12: 0.7661549
	class 13: 0.6873537
	class 14: 0.8019859
	class 15: 0.8316449
	class 16: 0.0
	class 17: 0.051169787
	class 18: 0.006072298
	class 19: 0.4828679
Class Acc:
	class 0: 0.960691
	class 1: 0.8482104
	class 2: 0.91650796
	class 3: 0.85597646
	class 4: 0.75374126
	class 5: 0.84072095
	class 6: 0.9214451
	class 7: 0.94099766
	class 8: 0.77827585
	class 9: 0.4944524
	class 10: 0.77251947
	class 11: 0.5984271
	class 12: 0.8964415
	class 13: 0.7635375
	class 14: 0.8805958
	class 15: 0.9289314
	class 16: 0.0
	class 17: 0.051413313
	class 18: 0.0062641376
	class 19: 0.7572339

federated global round: 23, step: 4
select part of clients to conduct local training
[3, 19, 1, 11]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.018400348722934723, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.018400348722934723, Class Loss=0.018400348722934723, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.10710550099611282, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.10710550099611282, Class Loss=0.10710550099611282, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.2578996419906616, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.2578996419906616, Class Loss=0.2578996419906616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.42678433656692505, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.42678433656692505, Class Loss=0.42678433656692505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5209285020828247, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.5209285020828247, Class Loss=0.5209285020828247, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.521172285079956, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.521172285079956, Class Loss=0.521172285079956, Reg Loss=0.0
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.01856452226638794, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.01856452226638794, Class Loss=0.01856452226638794, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.11450124531984329, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.11450124531984329, Class Loss=0.11450124531984329, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.27389368414878845, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.27389368414878845, Class Loss=0.27389368414878845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.43695396184921265, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.43695396184921265, Class Loss=0.43695396184921265, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5105065703392029, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.5105065703392029, Class Loss=0.5105065703392029, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.5373694896697998, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.5373694896697998, Class Loss=0.5373694896697998, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.026249080896377563, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.026249080896377563, Class Loss=0.026249080896377563, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.09314880520105362, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.09314880520105362, Class Loss=0.09314880520105362, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.26203471422195435, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.26203471422195435, Class Loss=0.26203471422195435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.4060570001602173, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.4060570001602173, Class Loss=0.4060570001602173, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5205605626106262, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.5205605626106262, Class Loss=0.5205605626106262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.5446661114692688, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.5446661114692688, Class Loss=0.5446661114692688, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.023384420201182365, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.023384420201182365, Class Loss=0.023384420201182365, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.10424499958753586, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.10424499958753586, Class Loss=0.10424499958753586, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.2802913188934326, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.2802913188934326, Class Loss=0.2802913188934326, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.4227544367313385, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4227544367313385, Class Loss=0.4227544367313385, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5174335241317749, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5174335241317749, Class Loss=0.5174335241317749, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.5348348617553711, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5348348617553711, Class Loss=0.5348348617553711, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4531422257423401, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.896557
Mean Acc: 0.712690
FreqW Acc: 0.821336
Mean IoU: 0.595170
Class IoU:
	class 0: 0.8926091
	class 1: 0.8369065
	class 2: 0.4075816
	class 3: 0.8244906
	class 4: 0.61104584
	class 5: 0.7164413
	class 6: 0.8768294
	class 7: 0.8583715
	class 8: 0.7709772
	class 9: 0.32046083
	class 10: 0.61936146
	class 11: 0.5484201
	class 12: 0.7711733
	class 13: 0.69490284
	class 14: 0.80854875
	class 15: 0.82696366
	class 16: 4.655684e-05
	class 17: 0.06571512
	class 18: 0.007823837
	class 19: 0.4447361
Class Acc:
	class 0: 0.9531398
	class 1: 0.89322716
	class 2: 0.9264869
	class 3: 0.86914575
	class 4: 0.7830989
	class 5: 0.85940266
	class 6: 0.94428664
	class 7: 0.9447885
	class 8: 0.7858488
	class 9: 0.5012183
	class 10: 0.7878968
	class 11: 0.62804115
	class 12: 0.9053706
	class 13: 0.7814799
	class 14: 0.89615506
	class 15: 0.9344742
	class 16: 4.6560694e-05
	class 17: 0.0661178
	class 18: 0.00813655
	class 19: 0.7854457

federated global round: 24, step: 4
select part of clients to conduct local training
[4, 13, 18, 9]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.019944876432418823, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.019944876432418823, Class Loss=0.019944876432418823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.10284950584173203, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.10284950584173203, Class Loss=0.10284950584173203, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.2554342448711395, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.2554342448711395, Class Loss=0.2554342448711395, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.4118053615093231, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.4118053615093231, Class Loss=0.4118053615093231, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.49720054864883423, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.49720054864883423, Class Loss=0.49720054864883423, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.5538799166679382, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5538799166679382, Class Loss=0.5538799166679382, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.020281879231333733, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.020281879231333733, Class Loss=0.020281879231333733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.10275956243276596, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.10275956243276596, Class Loss=0.10275956243276596, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.2695210576057434, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.2695210576057434, Class Loss=0.2695210576057434, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.4066178798675537, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.4066178798675537, Class Loss=0.4066178798675537, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.4935309886932373, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.4935309886932373, Class Loss=0.4935309886932373, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.556067168712616, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.556067168712616, Class Loss=0.556067168712616, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.01493301335722208, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.01493301335722208, Class Loss=0.01493301335722208, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.09116276353597641, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.09116276353597641, Class Loss=0.09116276353597641, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.24870926141738892, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.24870926141738892, Class Loss=0.24870926141738892, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.4079934060573578, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.4079934060573578, Class Loss=0.4079934060573578, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.4970296025276184, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.4970296025276184, Class Loss=0.4970296025276184, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.5391049981117249, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.5391049981117249, Class Loss=0.5391049981117249, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.5281070470809937, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5281070470809937, Class Loss=0.5281070470809937, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Class Loss=0.5458829998970032, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.5458829998970032, Class Loss=0.5458829998970032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Class Loss=0.5301593542098999, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5301593542098999, Class Loss=0.5301593542098999, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Class Loss=0.5115727782249451, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.5115727782249451, Class Loss=0.5115727782249451, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Class Loss=0.4982723593711853, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.4982723593711853, Class Loss=0.4982723593711853, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Class Loss=0.49310654401779175, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.49310654401779175, Class Loss=0.49310654401779175, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4386371970176697, Reg Loss=0.0 (without scaling)

Total samples: 1421.000000
Overall Acc: 0.897099
Mean Acc: 0.715309
FreqW Acc: 0.821445
Mean IoU: 0.596757
Class IoU:
	class 0: 0.89234793
	class 1: 0.8413036
	class 2: 0.40271237
	class 3: 0.82535774
	class 4: 0.6226027
	class 5: 0.7238512
	class 6: 0.87010175
	class 7: 0.86039716
	class 8: 0.7543335
	class 9: 0.32374105
	class 10: 0.6116705
	class 11: 0.5521999
	class 12: 0.78202033
	class 13: 0.6944243
	class 14: 0.8142319
	class 15: 0.82699436
	class 16: 6.1130675e-05
	class 17: 0.0663203
	class 18: 0.008250915
	class 19: 0.46221206
Class Acc:
	class 0: 0.95328385
	class 1: 0.89606196
	class 2: 0.92537385
	class 3: 0.876901
	class 4: 0.79583263
	class 5: 0.87115216
	class 6: 0.95401835
	class 7: 0.9506332
	class 8: 0.76696634
	class 9: 0.5005725
	class 10: 0.81699663
	class 11: 0.6265046
	class 12: 0.9074289
	class 13: 0.76765084
	class 14: 0.9070878
	class 15: 0.9353228
	class 16: 6.11403e-05
	class 17: 0.06665693
	class 18: 0.008572769
	class 19: 0.77909666

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 25, step: 5
select part of clients to conduct local training
[29, 3, 8, 19]
Current Client Index:  29
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.04847414046525955, Reg Loss=0.0
Clinet index 29, End of Epoch 1/6, Average Loss=0.04847414046525955, Class Loss=0.04847414046525955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.15421134233474731, Reg Loss=0.0
Clinet index 29, End of Epoch 2/6, Average Loss=0.15421134233474731, Class Loss=0.15421134233474731, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.31228700280189514, Reg Loss=0.0
Clinet index 29, End of Epoch 3/6, Average Loss=0.31228700280189514, Class Loss=0.31228700280189514, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5205073356628418, Reg Loss=0.0
Clinet index 29, End of Epoch 4/6, Average Loss=0.5205073356628418, Class Loss=0.5205073356628418, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6596078276634216, Reg Loss=0.0
Clinet index 29, End of Epoch 5/6, Average Loss=0.6596078276634216, Class Loss=0.6596078276634216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.7223569750785828, Reg Loss=0.0
Clinet index 29, End of Epoch 6/6, Average Loss=0.7223569750785828, Class Loss=0.7223569750785828, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.05053847283124924, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.05053847283124924, Class Loss=0.05053847283124924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.1747659146785736, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.1747659146785736, Class Loss=0.1747659146785736, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.31300169229507446, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.31300169229507446, Class Loss=0.31300169229507446, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5035075545310974, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.5035075545310974, Class Loss=0.5035075545310974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6474649906158447, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.6474649906158447, Class Loss=0.6474649906158447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.7405977845191956, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.7405977845191956, Class Loss=0.7405977845191956, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.044538360089063644, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.044538360089063644, Class Loss=0.044538360089063644, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.1257086545228958, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.1257086545228958, Class Loss=0.1257086545228958, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.2894212007522583, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.2894212007522583, Class Loss=0.2894212007522583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.512322187423706, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.512322187423706, Class Loss=0.512322187423706, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.7001871466636658, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.7001871466636658, Class Loss=0.7001871466636658, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.7611205577850342, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.7611205577850342, Class Loss=0.7611205577850342, Reg Loss=0.0
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.05932904779911041, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.05932904779911041, Class Loss=0.05932904779911041, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.1776740401983261, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.1776740401983261, Class Loss=0.1776740401983261, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.3309647738933563, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.3309647738933563, Class Loss=0.3309647738933563, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5276554822921753, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.5276554822921753, Class Loss=0.5276554822921753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6918132901191711, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.6918132901191711, Class Loss=0.6918132901191711, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.744622528553009, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.744622528553009, Class Loss=0.744622528553009, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6266987323760986, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.882844
Mean Acc: 0.699818
FreqW Acc: 0.802390
Mean IoU: 0.564573
Class IoU:
	class 0: 0.87599707
	class 1: 0.8100347
	class 2: 0.3775322
	class 3: 0.8233665
	class 4: 0.59166205
	class 5: 0.70194423
	class 6: 0.88274455
	class 7: 0.8133236
	class 8: 0.78401834
	class 9: 0.34445852
	class 10: 0.59145325
	class 11: 0.5596891
	class 12: 0.7616856
	class 13: 0.6741686
	class 14: 0.780924
	class 15: 0.8152435
	class 16: 0.028145801
	class 17: 0.20877182
	class 18: 0.053442746
	class 19: 0.37742975
	class 20: 0.0
Class Acc:
	class 0: 0.936625
	class 1: 0.9309278
	class 2: 0.9334134
	class 3: 0.88594747
	class 4: 0.8590399
	class 5: 0.8984308
	class 6: 0.9538766
	class 7: 0.9463264
	class 8: 0.80370855
	class 9: 0.5442476
	class 10: 0.8444799
	class 11: 0.6391246
	class 12: 0.92827857
	class 13: 0.8087559
	class 14: 0.9115669
	class 15: 0.94363886
	class 16: 0.029420244
	class 17: 0.23084125
	class 18: 0.07150797
	class 19: 0.59602344
	class 20: 0.0

federated global round: 26, step: 5
select part of clients to conduct local training
[4, 0, 29, 20]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.03936025872826576, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.03936025872826576, Class Loss=0.03936025872826576, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.12008557468652725, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.12008557468652725, Class Loss=0.12008557468652725, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.2254665493965149, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.2254665493965149, Class Loss=0.2254665493965149, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.3991210162639618, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.3991210162639618, Class Loss=0.3991210162639618, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.5491511225700378, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5491511225700378, Class Loss=0.5491511225700378, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.662600576877594, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.662600576877594, Class Loss=0.662600576877594, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.04009118676185608, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.04009118676185608, Class Loss=0.04009118676185608, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.1040547713637352, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.1040547713637352, Class Loss=0.1040547713637352, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.23883135616779327, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.23883135616779327, Class Loss=0.23883135616779327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.3823460638523102, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3823460638523102, Class Loss=0.3823460638523102, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.5156921148300171, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.5156921148300171, Class Loss=0.5156921148300171, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6339651942253113, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.6339651942253113, Class Loss=0.6339651942253113, Reg Loss=0.0
Current Client Index:  29
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Class Loss=0.7961595058441162, Reg Loss=0.0
Clinet index 29, End of Epoch 1/6, Average Loss=0.7961595058441162, Class Loss=0.7961595058441162, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Class Loss=0.7637368440628052, Reg Loss=0.0
Clinet index 29, End of Epoch 2/6, Average Loss=0.7637368440628052, Class Loss=0.7637368440628052, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Class Loss=0.7168614268302917, Reg Loss=0.0
Clinet index 29, End of Epoch 3/6, Average Loss=0.7168614268302917, Class Loss=0.7168614268302917, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Class Loss=0.6749647855758667, Reg Loss=0.0
Clinet index 29, End of Epoch 4/6, Average Loss=0.6749647855758667, Class Loss=0.6749647855758667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.6447086334228516, Reg Loss=0.0
Clinet index 29, End of Epoch 5/6, Average Loss=0.6447086334228516, Class Loss=0.6447086334228516, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Class Loss=0.6459320783615112, Reg Loss=0.0
Clinet index 29, End of Epoch 6/6, Average Loss=0.6459320783615112, Class Loss=0.6459320783615112, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.03724803403019905, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.03724803403019905, Class Loss=0.03724803403019905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.10891693085432053, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.10891693085432053, Class Loss=0.10891693085432053, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.24301324784755707, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.24301324784755707, Class Loss=0.24301324784755707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.4063449800014496, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.4063449800014496, Class Loss=0.4063449800014496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.5679075717926025, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.5679075717926025, Class Loss=0.5679075717926025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6618809103965759, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.6618809103965759, Class Loss=0.6618809103965759, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5591297149658203, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.887500
Mean Acc: 0.686179
FreqW Acc: 0.804065
Mean IoU: 0.559548
Class IoU:
	class 0: 0.8814702
	class 1: 0.8148276
	class 2: 0.3765448
	class 3: 0.81817573
	class 4: 0.6012391
	class 5: 0.70801795
	class 6: 0.8565174
	class 7: 0.8076048
	class 8: 0.80714035
	class 9: 0.35670352
	class 10: 0.58268577
	class 11: 0.5608223
	class 12: 0.7027315
	class 13: 0.6758702
	class 14: 0.78404874
	class 15: 0.81140846
	class 16: 0.01784487
	class 17: 0.16163751
	class 18: 0.02517257
	class 19: 0.40003464
	class 20: 0.0
Class Acc:
	class 0: 0.9484274
	class 1: 0.9199144
	class 2: 0.9367046
	class 3: 0.8934464
	class 4: 0.84218806
	class 5: 0.9126439
	class 6: 0.9614732
	class 7: 0.9507715
	class 8: 0.8344098
	class 9: 0.5184517
	class 10: 0.7255078
	class 11: 0.63071007
	class 12: 0.9406573
	class 13: 0.82096446
	class 14: 0.9078604
	class 15: 0.9469636
	class 16: 0.018378304
	class 17: 0.17365637
	class 18: 0.02797521
	class 19: 0.4986551
	class 20: 0.0

federated global round: 27, step: 5
select part of clients to conduct local training
[13, 12, 20, 15]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.0435493104159832, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.0435493104159832, Class Loss=0.0435493104159832, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.10730508714914322, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.10730508714914322, Class Loss=0.10730508714914322, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.2171323299407959, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.2171323299407959, Class Loss=0.2171323299407959, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.37327539920806885, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.37327539920806885, Class Loss=0.37327539920806885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5411721467971802, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5411721467971802, Class Loss=0.5411721467971802, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6451318860054016, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.6451318860054016, Class Loss=0.6451318860054016, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.045220255851745605, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.045220255851745605, Class Loss=0.045220255851745605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.10781033337116241, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.10781033337116241, Class Loss=0.10781033337116241, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.20270118117332458, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.20270118117332458, Class Loss=0.20270118117332458, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.3459291458129883, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.3459291458129883, Class Loss=0.3459291458129883, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5177175402641296, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5177175402641296, Class Loss=0.5177175402641296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6247419714927673, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.6247419714927673, Class Loss=0.6247419714927673, Reg Loss=0.0
Current Client Index:  20
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Class Loss=0.7372055053710938, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.7372055053710938, Class Loss=0.7372055053710938, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Class Loss=0.6700036525726318, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.6700036525726318, Class Loss=0.6700036525726318, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.6541732549667358, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.6541732549667358, Class Loss=0.6541732549667358, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Class Loss=0.6142151951789856, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.6142151951789856, Class Loss=0.6142151951789856, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Class Loss=0.6208699345588684, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.6208699345588684, Class Loss=0.6208699345588684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Class Loss=0.6301172375679016, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.6301172375679016, Class Loss=0.6301172375679016, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.03059452399611473, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.03059452399611473, Class Loss=0.03059452399611473, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.08736129850149155, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.08736129850149155, Class Loss=0.08736129850149155, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.20795480906963348, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.20795480906963348, Class Loss=0.20795480906963348, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.3380888104438782, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.3380888104438782, Class Loss=0.3380888104438782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.50662761926651, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.50662761926651, Class Loss=0.50662761926651, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.62114018201828, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.62114018201828, Class Loss=0.62114018201828, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5497140884399414, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.888977
Mean Acc: 0.691227
FreqW Acc: 0.806592
Mean IoU: 0.561422
Class IoU:
	class 0: 0.8843011
	class 1: 0.8101939
	class 2: 0.3753543
	class 3: 0.7998966
	class 4: 0.60273373
	class 5: 0.71051025
	class 6: 0.8347552
	class 7: 0.79950964
	class 8: 0.8267878
	class 9: 0.35295364
	class 10: 0.59005225
	class 11: 0.5641439
	class 12: 0.711731
	class 13: 0.6928297
	class 14: 0.786264
	class 15: 0.8104141
	class 16: 0.026138479
	class 17: 0.1482462
	class 18: 0.025265545
	class 19: 0.40838328
	class 20: 0.029400356
Class Acc:
	class 0: 0.94800705
	class 1: 0.9309016
	class 2: 0.9341874
	class 3: 0.9074306
	class 4: 0.81426793
	class 5: 0.92080736
	class 6: 0.96682733
	class 7: 0.9533712
	class 8: 0.86019075
	class 9: 0.5450982
	class 10: 0.7689452
	class 11: 0.64190274
	class 12: 0.942537
	class 13: 0.80369276
	class 14: 0.88761824
	class 15: 0.94827414
	class 16: 0.026814727
	class 17: 0.16076978
	class 18: 0.027417287
	class 19: 0.4973005
	class 20: 0.029400356

federated global round: 28, step: 5
select part of clients to conduct local training
[6, 29, 13, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.03284268453717232, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.03284268453717232, Class Loss=0.03284268453717232, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.09423837065696716, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.09423837065696716, Class Loss=0.09423837065696716, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.18801438808441162, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.18801438808441162, Class Loss=0.18801438808441162, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.33235350251197815, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.33235350251197815, Class Loss=0.33235350251197815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.4957254230976105, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.4957254230976105, Class Loss=0.4957254230976105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.6160872578620911, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.6160872578620911, Class Loss=0.6160872578620911, Reg Loss=0.0
Current Client Index:  29
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Epoch 1, Class Loss=0.6812137365341187, Reg Loss=0.0
Clinet index 29, End of Epoch 1/6, Average Loss=0.6812137365341187, Class Loss=0.6812137365341187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000584
Epoch 2, Class Loss=0.6587343811988831, Reg Loss=0.0
Clinet index 29, End of Epoch 2/6, Average Loss=0.6587343811988831, Class Loss=0.6587343811988831, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Class Loss=0.6341772675514221, Reg Loss=0.0
Clinet index 29, End of Epoch 3/6, Average Loss=0.6341772675514221, Class Loss=0.6341772675514221, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000487
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.5976130962371826, Reg Loss=0.0
Clinet index 29, End of Epoch 4/6, Average Loss=0.5976130962371826, Class Loss=0.5976130962371826, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000438
Epoch 5, Class Loss=0.5891735553741455, Reg Loss=0.0
Clinet index 29, End of Epoch 5/6, Average Loss=0.5891735553741455, Class Loss=0.5891735553741455, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000389
Epoch 6, Class Loss=0.5731126070022583, Reg Loss=0.0
Clinet index 29, End of Epoch 6/6, Average Loss=0.5731126070022583, Class Loss=0.5731126070022583, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.6957207918167114, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.6957207918167114, Class Loss=0.6957207918167114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Epoch 2, Class Loss=0.695081889629364, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.695081889629364, Class Loss=0.695081889629364, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.6655183434486389, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.6655183434486389, Class Loss=0.6655183434486389, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.6413421630859375, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.6413421630859375, Class Loss=0.6413421630859375, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.644301176071167, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.644301176071167, Class Loss=0.644301176071167, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.614045262336731, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.614045262336731, Class Loss=0.614045262336731, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.04002382233738899, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.04002382233738899, Class Loss=0.04002382233738899, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.11118890345096588, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.11118890345096588, Class Loss=0.11118890345096588, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.2180713266134262, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.2180713266134262, Class Loss=0.2180713266134262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.32608121633529663, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.32608121633529663, Class Loss=0.32608121633529663, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.5008872151374817, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5008872151374817, Class Loss=0.5008872151374817, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.5982112884521484, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.5982112884521484, Class Loss=0.5982112884521484, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5156095623970032, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.890885
Mean Acc: 0.687084
FreqW Acc: 0.807992
Mean IoU: 0.564911
Class IoU:
	class 0: 0.88617545
	class 1: 0.8205588
	class 2: 0.37341583
	class 3: 0.7990988
	class 4: 0.60083485
	class 5: 0.7143167
	class 6: 0.82771677
	class 7: 0.8126916
	class 8: 0.81910527
	class 9: 0.35180044
	class 10: 0.5895272
	class 11: 0.56333387
	class 12: 0.7128992
	class 13: 0.7089223
	class 14: 0.7928566
	class 15: 0.80577993
	class 16: 0.014379482
	class 17: 0.09247178
	class 18: 0.012744938
	class 19: 0.39674285
	class 20: 0.16775943
Class Acc:
	class 0: 0.95357764
	class 1: 0.9046992
	class 2: 0.9366293
	class 3: 0.897599
	class 4: 0.81476027
	class 5: 0.91511405
	class 6: 0.96770555
	class 7: 0.95147324
	class 8: 0.851056
	class 9: 0.52573645
	class 10: 0.76837707
	class 11: 0.6318302
	class 12: 0.93356586
	class 13: 0.80517375
	class 14: 0.8760728
	class 15: 0.94944817
	class 16: 0.014594661
	class 17: 0.096190825
	class 18: 0.013138304
	class 19: 0.45142028
	class 20: 0.17059448

federated global round: 29, step: 5
select part of clients to conduct local training
[6, 16, 13, 22]
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Class Loss=0.6500399112701416, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.6500399112701416, Class Loss=0.6500399112701416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Class Loss=0.6377065777778625, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6377065777778625, Class Loss=0.6377065777778625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Class Loss=0.617808997631073, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.617808997631073, Class Loss=0.617808997631073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Class Loss=0.6034553050994873, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.6034553050994873, Class Loss=0.6034553050994873, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Class Loss=0.6108981370925903, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.6108981370925903, Class Loss=0.6108981370925903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Class Loss=0.6031810641288757, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.6031810641288757, Class Loss=0.6031810641288757, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.03525717929005623, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.03525717929005623, Class Loss=0.03525717929005623, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.08336987346410751, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.08336987346410751, Class Loss=0.08336987346410751, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.1766752153635025, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.1766752153635025, Class Loss=0.1766752153635025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.2930518388748169, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.2930518388748169, Class Loss=0.2930518388748169, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.4362238347530365, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.4362238347530365, Class Loss=0.4362238347530365, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.5617163777351379, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.5617163777351379, Class Loss=0.5617163777351379, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000372
Epoch 1, Class Loss=0.6469919681549072, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.6469919681549072, Class Loss=0.6469919681549072, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.6531965732574463, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.6531965732574463, Class Loss=0.6531965732574463, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000258
Epoch 3, Class Loss=0.63105708360672, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.63105708360672, Class Loss=0.63105708360672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000199
Epoch 4, Class Loss=0.6354916095733643, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.6354916095733643, Class Loss=0.6354916095733643, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000138
Epoch 5, Class Loss=0.6599498987197876, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.6599498987197876, Class Loss=0.6599498987197876, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.626247763633728, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.626247763633728, Class Loss=0.626247763633728, Reg Loss=0.0
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.0359162874519825, Reg Loss=0.0
Clinet index 22, End of Epoch 1/6, Average Loss=0.0359162874519825, Class Loss=0.0359162874519825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.09108056873083115, Reg Loss=0.0
Clinet index 22, End of Epoch 2/6, Average Loss=0.09108056873083115, Class Loss=0.09108056873083115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.19627666473388672, Reg Loss=0.0
Clinet index 22, End of Epoch 3/6, Average Loss=0.19627666473388672, Class Loss=0.19627666473388672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.3171379268169403, Reg Loss=0.0
Clinet index 22, End of Epoch 4/6, Average Loss=0.3171379268169403, Class Loss=0.3171379268169403, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.468360960483551, Reg Loss=0.0
Clinet index 22, End of Epoch 5/6, Average Loss=0.468360960483551, Class Loss=0.468360960483551, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.5876044631004333, Reg Loss=0.0
Clinet index 22, End of Epoch 6/6, Average Loss=0.5876044631004333, Class Loss=0.5876044631004333, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5200256109237671, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.890366
Mean Acc: 0.694881
FreqW Acc: 0.808178
Mean IoU: 0.566304
Class IoU:
	class 0: 0.886723
	class 1: 0.819252
	class 2: 0.37319127
	class 3: 0.790547
	class 4: 0.6049052
	class 5: 0.71309566
	class 6: 0.81908023
	class 7: 0.7967677
	class 8: 0.81944484
	class 9: 0.3463279
	class 10: 0.58289886
	class 11: 0.56975394
	class 12: 0.7033276
	class 13: 0.7044073
	class 14: 0.79381603
	class 15: 0.8053698
	class 16: 0.017807253
	class 17: 0.08988386
	class 18: 0.0152593
	class 19: 0.4050014
	class 20: 0.23551326
Class Acc:
	class 0: 0.95030266
	class 1: 0.9175746
	class 2: 0.9371031
	class 3: 0.90212065
	class 4: 0.82484853
	class 5: 0.92139477
	class 6: 0.9674226
	class 7: 0.95608145
	class 8: 0.8515674
	class 9: 0.53574586
	class 10: 0.7703483
	class 11: 0.6508398
	class 12: 0.937897
	class 13: 0.80490327
	class 14: 0.87563825
	class 15: 0.9489776
	class 16: 0.018108817
	class 17: 0.09361465
	class 18: 0.015795121
	class 19: 0.4647726
	class 20: 0.24744353

voc_15-1_OURS On GPUs 2
Run in 74515s
