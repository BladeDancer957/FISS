nohup: ignoring input
25
kvoc_4-4_OURS On GPUs 0\Writing in results/seed_2023-ov/2023-03-06_voc_4-4_OURS.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/28, Loss=0.8663020312786103
Loss made of: CE 0.5369400382041931, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.41057601273059846
Loss made of: CE 0.3509715795516968, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5437301993370056, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.5437301993370056, Class Loss=0.5437301993370056, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/28, Loss=0.22661364674568177
Loss made of: CE 0.19912753999233246, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.1972617283463478
Loss made of: CE 0.1613975465297699, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2165270745754242, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.2165270745754242, Class Loss=0.2165270745754242, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/28, Loss=0.16771278530359268
Loss made of: CE 0.23664146661758423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.16961756646633147
Loss made of: CE 0.22370722889900208, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.16907812654972076, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.16907812654972076, Class Loss=0.16907812654972076, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/28, Loss=0.1549431100487709
Loss made of: CE 0.1289367377758026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.14656640589237213
Loss made of: CE 0.12054626643657684, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.15311279892921448, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.15311279892921448, Class Loss=0.15311279892921448, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/28, Loss=0.13748960718512535
Loss made of: CE 0.12136095017194748, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.1384456366300583
Loss made of: CE 0.11093831062316895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.13061949610710144, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.13061949610710144, Class Loss=0.13061949610710144, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/28, Loss=0.11482855305075645
Loss made of: CE 0.1025361567735672, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.12344053983688355
Loss made of: CE 0.10838006436824799, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11702744662761688, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.11702744662761688, Class Loss=0.11702744662761688, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/32, Loss=0.867270040512085
Loss made of: CE 0.7929067015647888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/32, Loss=0.41153696179389954
Loss made of: CE 0.321002721786499, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/32, Loss=0.28472654819488524
Loss made of: CE 0.21716201305389404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.501393735408783, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.501393735408783, Class Loss=0.501393735408783, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/32, Loss=0.20143622308969497
Loss made of: CE 0.31142687797546387, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/32, Loss=0.17011396288871766
Loss made of: CE 0.15438657999038696, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/32, Loss=0.17770883142948152
Loss made of: CE 0.1505936086177826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.18203340470790863, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.18203340470790863, Class Loss=0.18203340470790863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/32, Loss=0.15090100914239885
Loss made of: CE 0.12643581628799438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/32, Loss=0.1475618563592434
Loss made of: CE 0.1431894600391388, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/32, Loss=0.1410754382610321
Loss made of: CE 0.1591675728559494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1449986845254898, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.1449986845254898, Class Loss=0.1449986845254898, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/32, Loss=0.1297619879245758
Loss made of: CE 0.09855403751134872, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/32, Loss=0.12024844363331795
Loss made of: CE 0.0946955606341362, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/32, Loss=0.12380976304411888
Loss made of: CE 0.09909725189208984, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.12440594285726547, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.12440594285726547, Class Loss=0.12440594285726547, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/32, Loss=0.10961266383528709
Loss made of: CE 0.10085655748844147, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/32, Loss=0.12434668242931365
Loss made of: CE 0.10858114808797836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/32, Loss=0.09689163416624069
Loss made of: CE 0.0840967521071434, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.11275075376033783, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.11275075376033783, Class Loss=0.11275075376033783, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/32, Loss=0.10556697994470596
Loss made of: CE 0.08229917287826538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/32, Loss=0.10448077917099
Loss made of: CE 0.10248923301696777, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/32, Loss=0.10062486976385117
Loss made of: CE 0.10904698818922043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10303729772567749, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.10303729772567749, Class Loss=0.10303729772567749, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/29, Loss=0.8246585130691528
Loss made of: CE 0.6051774024963379, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.4430232733488083
Loss made of: CE 0.35592231154441833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5231749415397644, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5231749415397644, Class Loss=0.5231749415397644, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/29, Loss=0.21397947818040847
Loss made of: CE 0.19089588522911072, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.19743913114070893
Loss made of: CE 0.1800454556941986, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2031259834766388, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.2031259834766388, Class Loss=0.2031259834766388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/29, Loss=0.1872761517763138
Loss made of: CE 0.1512836217880249, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.15981591492891312
Loss made of: CE 0.15575477480888367, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.17049792408943176, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.17049792408943176, Class Loss=0.17049792408943176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/29, Loss=0.13489765748381616
Loss made of: CE 0.12494949996471405, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.13624921962618827
Loss made of: CE 0.12051510810852051, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.14005139470100403, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.14005139470100403, Class Loss=0.14005139470100403, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/29, Loss=0.1296745330095291
Loss made of: CE 0.14583268761634827, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.12666610330343248
Loss made of: CE 0.10426966845989227, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1287604421377182, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.1287604421377182, Class Loss=0.1287604421377182, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/29, Loss=0.12712323516607285
Loss made of: CE 0.14849987626075745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.11322255060076714
Loss made of: CE 0.10545149445533752, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11909579485654831, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.11909579485654831, Class Loss=0.11909579485654831, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/29, Loss=0.817145836353302
Loss made of: CE 0.5701463222503662, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.436820450425148
Loss made of: CE 0.309579074382782, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5241144895553589, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.5241144895553589, Class Loss=0.5241144895553589, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/29, Loss=0.2309108406305313
Loss made of: CE 0.206918403506279, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.19459144324064254
Loss made of: CE 0.2907916307449341, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.20642630755901337, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.20642630755901337, Class Loss=0.20642630755901337, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/29, Loss=0.16253620982170106
Loss made of: CE 0.12930890917778015, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.16192494332790375
Loss made of: CE 0.14780187606811523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.16142159700393677, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.16142159700393677, Class Loss=0.16142159700393677, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/29, Loss=0.16034301817417146
Loss made of: CE 0.13005422055721283, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.138018586486578
Loss made of: CE 0.11368304491043091, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.14239154756069183, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.14239154756069183, Class Loss=0.14239154756069183, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/29, Loss=0.1322089895606041
Loss made of: CE 0.15194380283355713, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.13456796631217002
Loss made of: CE 0.12312853336334229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1309502273797989, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.1309502273797989, Class Loss=0.1309502273797989, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/29, Loss=0.11873260214924812
Loss made of: CE 0.10067993402481079, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.11385773122310638
Loss made of: CE 0.10576626658439636, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11777501553297043, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.11777501553297043, Class Loss=0.11777501553297043, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3247489929199219, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.902883
Mean Acc: 0.381211
FreqW Acc: 0.815981
Mean IoU: 0.342988
Class IoU:
	class 0: 0.8995233
	class 1: 0.0
	class 2: 0.0
	class 3: 0.8154157
	class 4: 0.0
Class Acc:
	class 0: 0.99484336
	class 1: 0.0
	class 2: 0.0
	class 3: 0.91121376
	class 4: 0.0

federated global round: 1, step: 0
select part of clients to conduct local training
[6, 0, 7, 2]
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/28, Loss=0.3611298218369484
Loss made of: CE 0.24517330527305603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.17307310923933983
Loss made of: CE 0.19095563888549805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2354675829410553, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.2354675829410553, Class Loss=0.2354675829410553, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/28, Loss=0.13002014830708503
Loss made of: CE 0.10867427289485931, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.12215315252542495
Loss made of: CE 0.11156158894300461, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.12870848178863525, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.12870848178863525, Class Loss=0.12870848178863525, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/28, Loss=0.11827360615134239
Loss made of: CE 0.16951410472393036, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.11960596442222596
Loss made of: CE 0.1495613008737564, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1173987090587616, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.1173987090587616, Class Loss=0.1173987090587616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/28, Loss=0.11147593334317207
Loss made of: CE 0.10546588897705078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.10257574543356895
Loss made of: CE 0.07653998583555222, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10939602553844452, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.10939602553844452, Class Loss=0.10939602553844452, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/28, Loss=0.10341078490018844
Loss made of: CE 0.08139998465776443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.10232693403959274
Loss made of: CE 0.08363203704357147, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10012053698301315, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.10012053698301315, Class Loss=0.10012053698301315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/28, Loss=0.09149739742279053
Loss made of: CE 0.09150165319442749, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.09492977857589721
Loss made of: CE 0.09296800941228867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09270889312028885, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.09270889312028885, Class Loss=0.09270889312028885, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/26, Loss=0.7301344633102417
Loss made of: CE 0.33263087272644043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=0.32226577401161194
Loss made of: CE 0.2862430214881897, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.45554831624031067, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.45554831624031067, Class Loss=0.45554831624031067, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/26, Loss=0.20656847655773164
Loss made of: CE 0.14432960748672485, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=0.18065005093812941
Loss made of: CE 0.2345302551984787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.19922040402889252, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.19922040402889252, Class Loss=0.19922040402889252, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/26, Loss=0.15801823809742926
Loss made of: CE 0.12156526744365692, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=0.1464879184961319
Loss made of: CE 0.17393039166927338, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.14604538679122925, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.14604538679122925, Class Loss=0.14604538679122925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/26, Loss=0.12522919699549676
Loss made of: CE 0.16287356615066528, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=0.1400642067193985
Loss made of: CE 0.1363140046596527, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.13272897899150848, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.13272897899150848, Class Loss=0.13272897899150848, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/26, Loss=0.11125451326370239
Loss made of: CE 0.09762550890445709, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=0.1183368407189846
Loss made of: CE 0.11252759397029877, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.11719903349876404, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.11719903349876404, Class Loss=0.11719903349876404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/26, Loss=0.10536282882094383
Loss made of: CE 0.11906664073467255, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=0.09834862202405929
Loss made of: CE 0.10320918262004852, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10239654034376144, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.10239654034376144, Class Loss=0.10239654034376144, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/32, Loss=0.3107754483819008
Loss made of: CE 0.2392442524433136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/32, Loss=0.14768208265304567
Loss made of: CE 0.16470330953598022, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/32, Loss=0.14275060296058656
Loss made of: CE 0.110884889960289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.19502568244934082, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.19502568244934082, Class Loss=0.19502568244934082, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/32, Loss=0.12590067833662033
Loss made of: CE 0.2245321273803711, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/32, Loss=0.10391460284590721
Loss made of: CE 0.1107315719127655, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/32, Loss=0.11708252802491188
Loss made of: CE 0.09660541266202927, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.11548339575529099, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.11548339575529099, Class Loss=0.11548339575529099, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/32, Loss=0.10004960224032403
Loss made of: CE 0.0824378952383995, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/32, Loss=0.10244938954710961
Loss made of: CE 0.10787724703550339, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/32, Loss=0.09335760399699211
Loss made of: CE 0.07944509387016296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09725693613290787, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.09725693613290787, Class Loss=0.09725693613290787, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/32, Loss=0.09188788011670113
Loss made of: CE 0.06896263360977173, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/32, Loss=0.08488776981830597
Loss made of: CE 0.07665181905031204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/32, Loss=0.08696065619587898
Loss made of: CE 0.07270258665084839, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08892987668514252, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.08892987668514252, Class Loss=0.08892987668514252, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/32, Loss=0.08513782098889351
Loss made of: CE 0.08960908651351929, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/32, Loss=0.08583677932620049
Loss made of: CE 0.07894593477249146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/32, Loss=0.07340486496686935
Loss made of: CE 0.07160386443138123, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0851084366440773, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.0851084366440773, Class Loss=0.0851084366440773, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/32, Loss=0.0829793594777584
Loss made of: CE 0.07923128455877304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/32, Loss=0.07851898595690728
Loss made of: CE 0.07993683218955994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/32, Loss=0.07770198918879032
Loss made of: CE 0.0924496278166771, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07973369210958481, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.07973369210958481, Class Loss=0.07973369210958481, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/29, Loss=0.1849059984087944
Loss made of: CE 0.12549327313899994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.14396359771490097
Loss made of: CE 0.1144934892654419, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.15773990750312805, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.15773990750312805, Class Loss=0.15773990750312805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/29, Loss=0.11829549893736839
Loss made of: CE 0.11821603775024414, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.11865847483277321
Loss made of: CE 0.16068699955940247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.11912725120782852, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.11912725120782852, Class Loss=0.11912725120782852, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/29, Loss=0.10777206793427467
Loss made of: CE 0.0831703394651413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.11472256928682327
Loss made of: CE 0.11683420836925507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.11274145543575287, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.11274145543575287, Class Loss=0.11274145543575287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/29, Loss=0.11409810557961464
Loss made of: CE 0.09392215311527252, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.10243874341249466
Loss made of: CE 0.08289182186126709, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10490802675485611, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.10490802675485611, Class Loss=0.10490802675485611, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/29, Loss=0.09677930548787117
Loss made of: CE 0.12308396399021149, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.10612524822354316
Loss made of: CE 0.12150945514440536, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10113383829593658, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.10113383829593658, Class Loss=0.10113383829593658, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/29, Loss=0.09145224690437317
Loss made of: CE 0.0792853832244873, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.08711297959089279
Loss made of: CE 0.08931790292263031, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09356231242418289, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.09356231242418289, Class Loss=0.09356231242418289, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.15202204883098602, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.936222
Mean Acc: 0.560019
FreqW Acc: 0.877894
Mean IoU: 0.540114
Class IoU:
	class 0: 0.93180156
	class 1: 0.5838196
	class 2: 0.022213213
	class 3: 0.7217591
	class 4: 0.440978
Class Acc:
	class 0: 0.997739
	class 1: 0.59523124
	class 2: 0.022618411
	class 3: 0.73056346
	class 4: 0.45394194

federated global round: 2, step: 0
select part of clients to conduct local training
[8, 5, 3, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/28, Loss=0.15333928540349007
Loss made of: CE 0.16642872989177704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.12566960155963897
Loss made of: CE 0.14048463106155396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1352432668209076, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.1352432668209076, Class Loss=0.1352432668209076, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/28, Loss=0.09353632777929306
Loss made of: CE 0.06303101778030396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.09604738056659698
Loss made of: CE 0.10318716615438461, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09939447045326233, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.09939447045326233, Class Loss=0.09939447045326233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/28, Loss=0.08323138505220413
Loss made of: CE 0.08362901210784912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.09248731806874275
Loss made of: CE 0.07915537059307098, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08841636031866074, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.08841636031866074, Class Loss=0.08841636031866074, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/28, Loss=0.0732813335955143
Loss made of: CE 0.07845251262187958, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.08034627996385098
Loss made of: CE 0.08853316307067871, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08235766738653183, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.08235766738653183, Class Loss=0.08235766738653183, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/28, Loss=0.0771861407905817
Loss made of: CE 0.07638706266880035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.07255858220160008
Loss made of: CE 0.09036695957183838, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07669928669929504, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.07669928669929504, Class Loss=0.07669928669929504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/28, Loss=0.07149163223803043
Loss made of: CE 0.0769926905632019, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.06645851992070675
Loss made of: CE 0.06913689523935318, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07150702178478241, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.07150702178478241, Class Loss=0.07150702178478241, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/26, Loss=0.24421111941337587
Loss made of: CE 0.1927361935377121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=0.1805311217904091
Loss made of: CE 0.16532331705093384, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.20005890727043152, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.20005890727043152, Class Loss=0.20005890727043152, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/26, Loss=0.13723599091172217
Loss made of: CE 0.12713681161403656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=0.12504292652010918
Loss made of: CE 0.10639988631010056, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.13214337825775146, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.13214337825775146, Class Loss=0.13214337825775146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/26, Loss=0.10743961110711098
Loss made of: CE 0.07995534688234329, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=0.10913163498044014
Loss made of: CE 0.10404231399297714, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10966223478317261, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.10966223478317261, Class Loss=0.10966223478317261, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/26, Loss=0.09880390241742135
Loss made of: CE 0.1022755354642868, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=0.10360556095838547
Loss made of: CE 0.13211590051651, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09653551131486893, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.09653551131486893, Class Loss=0.09653551131486893, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/26, Loss=0.09224678166210651
Loss made of: CE 0.09266763925552368, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=0.08186692968010903
Loss made of: CE 0.10396864265203476, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08823147416114807, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.08823147416114807, Class Loss=0.08823147416114807, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/26, Loss=0.08525474593043328
Loss made of: CE 0.06929265707731247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=0.08876891732215882
Loss made of: CE 0.08947046846151352, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08575713634490967, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.08575713634490967, Class Loss=0.08575713634490967, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/28, Loss=0.1429074577987194
Loss made of: CE 0.13752877712249756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.12669869065284728
Loss made of: CE 0.1361355185508728, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.13714246451854706, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.13714246451854706, Class Loss=0.13714246451854706, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/28, Loss=0.12716506123542787
Loss made of: CE 0.13606029748916626, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.10372283831238746
Loss made of: CE 0.09273716807365417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1151331439614296, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.1151331439614296, Class Loss=0.1151331439614296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/28, Loss=0.09397876560688019
Loss made of: CE 0.07275742292404175, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.09586624950170516
Loss made of: CE 0.08200912177562714, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09578647464513779, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.09578647464513779, Class Loss=0.09578647464513779, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/28, Loss=0.0902278833091259
Loss made of: CE 0.09345053136348724, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.08479321375489235
Loss made of: CE 0.08639882504940033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08767134696245193, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.08767134696245193, Class Loss=0.08767134696245193, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/28, Loss=0.07366666533052921
Loss made of: CE 0.0718962773680687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.07005477473139762
Loss made of: CE 0.07816345989704132, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07641972601413727, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.07641972601413727, Class Loss=0.07641972601413727, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/28, Loss=0.06764209643006325
Loss made of: CE 0.05966012179851532, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.07757126241922378
Loss made of: CE 0.06960321962833405, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0738152414560318, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.0738152414560318, Class Loss=0.0738152414560318, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/32, Loss=0.14331470653414727
Loss made of: CE 0.12642008066177368, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/32, Loss=0.09660207629203796
Loss made of: CE 0.09950666129589081, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/32, Loss=0.09606296941637993
Loss made of: CE 0.08072328567504883, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.11014777421951294, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.11014777421951294, Class Loss=0.11014777421951294, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.005998
Epoch 2, Batch 10/32, Loss=0.08555891513824462
Loss made of: CE 0.10977138578891754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/32, Loss=0.07618874907493592
Loss made of: CE 0.07905469089746475, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/32, Loss=0.09097576439380646
Loss made of: CE 0.07670361548662186, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08399087935686111, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.08399087935686111, Class Loss=0.08399087935686111, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.005679
Epoch 3, Batch 10/32, Loss=0.07391185015439987
Loss made of: CE 0.06394290924072266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/32, Loss=0.08380479589104653
Loss made of: CE 0.08941914141178131, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/32, Loss=0.0738897480070591
Loss made of: CE 0.08035992830991745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07665203511714935, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.07665203511714935, Class Loss=0.07665203511714935, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/32, Loss=0.07394994050264359
Loss made of: CE 0.057436615228652954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/32, Loss=0.06963629871606827
Loss made of: CE 0.06884148716926575, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/32, Loss=0.07106708958745003
Loss made of: CE 0.06437252461910248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.0718323141336441, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.0718323141336441, Class Loss=0.0718323141336441, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.005036
Epoch 5, Batch 10/32, Loss=0.06961316801607609
Loss made of: CE 0.06952115893363953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/32, Loss=0.07434237822890281
Loss made of: CE 0.07825326919555664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/32, Loss=0.0643932119011879
Loss made of: CE 0.06146668642759323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07218862324953079, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.07218862324953079, Class Loss=0.07218862324953079, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.004711
Epoch 6, Batch 10/32, Loss=0.07196039594709873
Loss made of: CE 0.0575086884200573, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/32, Loss=0.06590338088572026
Loss made of: CE 0.07002885639667511, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/32, Loss=0.06628645062446595
Loss made of: CE 0.0859159454703331, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06861266493797302, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.06861266493797302, Class Loss=0.06861266493797302, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.12073038518428802, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.955407
Mean Acc: 0.699707
FreqW Acc: 0.916276
Mean IoU: 0.640293
Class IoU:
	class 0: 0.9546252
	class 1: 0.7315079
	class 2: 0.006856091
	class 3: 0.8694152
	class 4: 0.63905954
Class Acc:
	class 0: 0.9903437
	class 1: 0.7554636
	class 2: 0.0068647936
	class 3: 0.89385873
	class 4: 0.8520023

federated global round: 3, step: 0
select part of clients to conduct local training
[5, 2, 0, 3]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006943
Epoch 1, Batch 10/26, Loss=0.15717020854353905
Loss made of: CE 0.1111927255988121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=0.12530628815293313
Loss made of: CE 0.11790617555379868, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1336936503648758, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.1336936503648758, Class Loss=0.1336936503648758, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.006420
Epoch 2, Batch 10/26, Loss=0.10325658693909645
Loss made of: CE 0.09441548585891724, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=0.09553380012512207
Loss made of: CE 0.0970509871840477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10044863075017929, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.10044863075017929, Class Loss=0.10044863075017929, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.005892
Epoch 3, Batch 10/26, Loss=0.08724615052342415
Loss made of: CE 0.0777008906006813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=0.08987159356474876
Loss made of: CE 0.08793331682682037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08932577818632126, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.08932577818632126, Class Loss=0.08932577818632126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/26, Loss=0.08373631164431572
Loss made of: CE 0.08379113674163818, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=0.08813634850084781
Loss made of: CE 0.10542099922895432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08321601152420044, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.08321601152420044, Class Loss=0.08321601152420044, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.004820
Epoch 5, Batch 10/26, Loss=0.08333125971257686
Loss made of: CE 0.07786472141742706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=0.07258912622928619
Loss made of: CE 0.09317386150360107, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07750561833381653, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.07750561833381653, Class Loss=0.07750561833381653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.004274
Epoch 6, Batch 10/26, Loss=0.0774437826126814
Loss made of: CE 0.057600151747465134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=0.07633646316826344
Loss made of: CE 0.07140384614467621, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07638980448246002, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.07638980448246002, Class Loss=0.07638980448246002, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/29, Loss=0.23132720738649368
Loss made of: CE 0.13473573327064514, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.1368789479136467
Loss made of: CE 0.09962725639343262, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.16400659084320068, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.16400659084320068, Class Loss=0.16400659084320068, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.005839
Epoch 2, Batch 10/29, Loss=0.09249508827924728
Loss made of: CE 0.1037621945142746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.09488494247198105
Loss made of: CE 0.13225895166397095, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09434226155281067, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.09434226155281067, Class Loss=0.09434226155281067, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.005359
Epoch 3, Batch 10/29, Loss=0.08278183788061141
Loss made of: CE 0.06417746841907501, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.09045365899801254
Loss made of: CE 0.09399394690990448, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08629314601421356, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.08629314601421356, Class Loss=0.08629314601421356, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.004874
Epoch 4, Batch 10/29, Loss=0.08737062886357308
Loss made of: CE 0.06909333914518356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.0827941469848156
Loss made of: CE 0.0656985342502594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08431430160999298, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.08431430160999298, Class Loss=0.08431430160999298, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.004384
Epoch 5, Batch 10/29, Loss=0.08042186498641968
Loss made of: CE 0.11180424690246582, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.08561302199959755
Loss made of: CE 0.0818522721529007, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0823538601398468, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.0823538601398468, Class Loss=0.0823538601398468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.003887
Epoch 6, Batch 10/29, Loss=0.07528401613235473
Loss made of: CE 0.06161252409219742, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.07402072288095951
Loss made of: CE 0.08294568210840225, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07824806123971939, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.07824806123971939, Class Loss=0.07824806123971939, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/26, Loss=0.20123596489429474
Loss made of: CE 0.0987808108329773, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=0.1320113644003868
Loss made of: CE 0.15254484117031097, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.15339241921901703, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.15339241921901703, Class Loss=0.15339241921901703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007137
Epoch 2, Batch 10/26, Loss=0.10736717581748963
Loss made of: CE 0.08348153531551361, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=0.10135126486420631
Loss made of: CE 0.12258567661046982, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.11160137504339218, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.11160137504339218, Class Loss=0.11160137504339218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.006551
Epoch 3, Batch 10/26, Loss=0.09903557151556015
Loss made of: CE 0.07017781585454941, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=0.10438581481575966
Loss made of: CE 0.15276899933815002, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09745819866657257, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.09745819866657257, Class Loss=0.09745819866657257, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005958
Epoch 4, Batch 10/26, Loss=0.08387554809451103
Loss made of: CE 0.1062638983130455, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=0.09662167951464654
Loss made of: CE 0.06544091552495956, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09087434411048889, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.09087434411048889, Class Loss=0.09087434411048889, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.005359
Epoch 5, Batch 10/26, Loss=0.0827207863330841
Loss made of: CE 0.06655272096395493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=0.08422873839735985
Loss made of: CE 0.09069465100765228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08374794572591782, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.08374794572591782, Class Loss=0.08374794572591782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.004752
Epoch 6, Batch 10/26, Loss=0.08448282107710839
Loss made of: CE 0.0937400758266449, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=0.07684797495603561
Loss made of: CE 0.08561588823795319, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07872197777032852, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.07872197777032852, Class Loss=0.07872197777032852, Reg Loss=0.0
Current Client Index:  3
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006943
Epoch 1, Batch 10/28, Loss=0.07885207161307335
Loss made of: CE 0.0969357043504715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.08608484193682671
Loss made of: CE 0.10878317058086395, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.08630044758319855, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.08630044758319855, Class Loss=0.08630044758319855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.006420
Epoch 2, Batch 10/28, Loss=0.09231768660247326
Loss made of: CE 0.10155384987592697, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.07459882125258446
Loss made of: CE 0.07751716673374176, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08185016363859177, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.08185016363859177, Class Loss=0.08185016363859177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.005892
Epoch 3, Batch 10/28, Loss=0.0761566136032343
Loss made of: CE 0.051403116434812546, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.07093263044953346
Loss made of: CE 0.0638420581817627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07492704689502716, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.07492704689502716, Class Loss=0.07492704689502716, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/28, Loss=0.07171233519911765
Loss made of: CE 0.07075681537389755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.07237707339227199
Loss made of: CE 0.07363525032997131, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07272466272115707, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.07272466272115707, Class Loss=0.07272466272115707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.004820
Epoch 5, Batch 10/28, Loss=0.06346325539052486
Loss made of: CE 0.05700439214706421, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.057969691976904866
Loss made of: CE 0.06342922151088715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0655394047498703, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.0655394047498703, Class Loss=0.0655394047498703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.004274
Epoch 6, Batch 10/28, Loss=0.05784988962113857
Loss made of: CE 0.05020012706518173, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.06745155043900013
Loss made of: CE 0.05367943271994591, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06300237774848938, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.06300237774848938, Class Loss=0.06300237774848938, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.10575830191373825, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.956891
Mean Acc: 0.732085
FreqW Acc: 0.920333
Mean IoU: 0.679619
Class IoU:
	class 0: 0.95351756
	class 1: 0.7930849
	class 2: 0.107902095
	class 3: 0.85033923
	class 4: 0.6932525
Class Acc:
	class 0: 0.987459
	class 1: 0.83533967
	class 2: 0.12692198
	class 3: 0.8663724
	class 4: 0.8443324

federated global round: 4, step: 0
select part of clients to conduct local training
[3, 6, 1, 7]
Current Client Index:  3
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.003720
Epoch 1, Batch 10/28, Loss=0.08950907364487648
Loss made of: CE 0.09092911332845688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.0803380236029625
Loss made of: CE 0.07803892344236374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.08508797734975815, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.08508797734975815, Class Loss=0.08508797734975815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.003157
Epoch 2, Batch 10/28, Loss=0.0806118331849575
Loss made of: CE 0.0835539847612381, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.06707087568938733
Loss made of: CE 0.07148973643779755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.07291984558105469, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.07291984558105469, Class Loss=0.07291984558105469, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.002583
Epoch 3, Batch 10/28, Loss=0.07010724171996116
Loss made of: CE 0.0503554493188858, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.06968324407935142
Loss made of: CE 0.06275559961795807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07046322524547577, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.07046322524547577, Class Loss=0.07046322524547577, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.001994
Epoch 4, Batch 10/28, Loss=0.0692621611058712
Loss made of: CE 0.06885868310928345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.06932464875280857
Loss made of: CE 0.07322829961776733, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.06784122437238693, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.06784122437238693, Class Loss=0.06784122437238693, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.001384
Epoch 5, Batch 10/28, Loss=0.0647128164768219
Loss made of: CE 0.061220936477184296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.05813412368297577
Loss made of: CE 0.06416952610015869, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.06454917043447495, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.06454917043447495, Class Loss=0.06454917043447495, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000742
Epoch 6, Batch 10/28, Loss=0.05914861559867859
Loss made of: CE 0.05091395974159241, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.06984694711863995
Loss made of: CE 0.06034420058131218, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0650857612490654, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.0650857612490654, Class Loss=0.0650857612490654, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/28, Loss=0.11485807672142982
Loss made of: CE 0.07247228175401688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.0789459504187107
Loss made of: CE 0.09163784980773926, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.09080267697572708, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.09080267697572708, Class Loss=0.09080267697572708, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.005359
Epoch 2, Batch 10/28, Loss=0.07262698151171207
Loss made of: CE 0.06000947207212448, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.06681544221937656
Loss made of: CE 0.05981290340423584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.07138342410326004, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.07138342410326004, Class Loss=0.07138342410326004, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.004384
Epoch 3, Batch 10/28, Loss=0.06928319782018662
Loss made of: CE 0.09615233540534973, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.07161805927753448
Loss made of: CE 0.0636989176273346, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.06957080215215683, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.06957080215215683, Class Loss=0.06957080215215683, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.003384
Epoch 4, Batch 10/28, Loss=0.06679836958646775
Loss made of: CE 0.061895303428173065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.06332903727889061
Loss made of: CE 0.05201460421085358, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.0676957368850708, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.0676957368850708, Class Loss=0.0676957368850708, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.002349
Epoch 5, Batch 10/28, Loss=0.06641104854643345
Loss made of: CE 0.05236738920211792, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.06761747524142266
Loss made of: CE 0.045865412801504135, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0657842755317688, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.0657842755317688, Class Loss=0.0657842755317688, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.001259
Epoch 6, Batch 10/28, Loss=0.060699852928519246
Loss made of: CE 0.06101323291659355, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.06615627147257327
Loss made of: CE 0.0593026801943779, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06280386447906494, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.06280386447906494, Class Loss=0.06280386447906494, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/29, Loss=0.16508651673793792
Loss made of: CE 0.1288292109966278, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.1273845262825489
Loss made of: CE 0.1346113681793213, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.13638803362846375, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.13638803362846375, Class Loss=0.13638803362846375, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.008487
Epoch 2, Batch 10/29, Loss=0.10177416354417801
Loss made of: CE 0.09876003116369247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.08922976851463318
Loss made of: CE 0.07309668511152267, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09401101619005203, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.09401101619005203, Class Loss=0.09401101619005203, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.006943
Epoch 3, Batch 10/29, Loss=0.08486575447022915
Loss made of: CE 0.07179924100637436, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.09603538140654563
Loss made of: CE 0.0902901291847229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08859887719154358, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.08859887719154358, Class Loss=0.08859887719154358, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/29, Loss=0.07780056893825531
Loss made of: CE 0.07472476363182068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.08806034736335278
Loss made of: CE 0.09454404562711716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.0830073431134224, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.0830073431134224, Class Loss=0.0830073431134224, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.003720
Epoch 5, Batch 10/29, Loss=0.08202661350369453
Loss made of: CE 0.08031036704778671, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.08474823087453842
Loss made of: CE 0.08681263774633408, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08081641793251038, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.08081641793251038, Class Loss=0.08081641793251038, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.001994
Epoch 6, Batch 10/29, Loss=0.07662470005452633
Loss made of: CE 0.07275401055812836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.0701990183442831
Loss made of: CE 0.07404114305973053, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07551009953022003, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.07551009953022003, Class Loss=0.07551009953022003, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.004384
Epoch 1, Batch 10/32, Loss=0.1281033605337143
Loss made of: CE 0.11095421016216278, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/32, Loss=0.07314561791718006
Loss made of: CE 0.07503383606672287, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/32, Loss=0.06655217818915844
Loss made of: CE 0.059093475341796875, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.08774589002132416, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.08774589002132416, Class Loss=0.08774589002132416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.003720
Epoch 2, Batch 10/32, Loss=0.06579519361257553
Loss made of: CE 0.07447125762701035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/32, Loss=0.0642109677195549
Loss made of: CE 0.06940974295139313, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/32, Loss=0.06549291759729385
Loss made of: CE 0.059628814458847046, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.06521780043840408, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.06521780043840408, Class Loss=0.06521780043840408, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.003043
Epoch 3, Batch 10/32, Loss=0.0599687110632658
Loss made of: CE 0.0589764341711998, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/32, Loss=0.06689781919121743
Loss made of: CE 0.06476962566375732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/32, Loss=0.05773853249847889
Loss made of: CE 0.05496799200773239, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.061053480952978134, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.061053480952978134, Class Loss=0.061053480952978134, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.002349
Epoch 4, Batch 10/32, Loss=0.06265476830303669
Loss made of: CE 0.04966290295124054, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/32, Loss=0.05871128924190998
Loss made of: CE 0.05441541224718094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/32, Loss=0.05663549043238163
Loss made of: CE 0.05298545956611633, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.05945388227701187, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.05945388227701187, Class Loss=0.05945388227701187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.001631
Epoch 5, Batch 10/32, Loss=0.05800753682851791
Loss made of: CE 0.0548267588019371, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/32, Loss=0.06293247193098069
Loss made of: CE 0.06549727916717529, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/32, Loss=0.052263444289565086
Loss made of: CE 0.05322536826133728, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0605572909116745, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.0605572909116745, Class Loss=0.0605572909116745, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000874
Epoch 6, Batch 10/32, Loss=0.05999092347919941
Loss made of: CE 0.04638109728693962, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/32, Loss=0.05644233636558056
Loss made of: CE 0.055851832032203674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/32, Loss=0.055732112377882004
Loss made of: CE 0.07082051783800125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0577971450984478, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.0577971450984478, Class Loss=0.0577971450984478, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.09878692775964737, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.957758
Mean Acc: 0.761803
FreqW Acc: 0.923271
Mean IoU: 0.703998
Class IoU:
	class 0: 0.95394605
	class 1: 0.76182854
	class 2: 0.20548439
	class 3: 0.8919018
	class 4: 0.7068279
Class Acc:
	class 0: 0.9848608
	class 1: 0.77408487
	class 2: 0.2774787
	class 3: 0.9518583
	class 4: 0.8207329

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[0, 12, 6, 10]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/52, Loss=8.813620853424073
Loss made of: CE 1.1119388341903687, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.495026588439941 EntMin 0.0
Epoch 1, Batch 20/52, Loss=7.119751024246216
Loss made of: CE 0.8114356994628906, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.264117240905762 EntMin 0.0
Epoch 1, Batch 30/52, Loss=6.640964990854263
Loss made of: CE 0.7553552389144897, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5998663902282715 EntMin 0.0
Epoch 1, Batch 40/52, Loss=6.013932728767395
Loss made of: CE 0.5519757270812988, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.036245822906494 EntMin 0.0
Epoch 1, Batch 50/52, Loss=5.895457291603089
Loss made of: CE 0.5745660662651062, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.872379302978516 EntMin 0.0
Epoch 1, Class Loss=0.8353390097618103, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.8353390097618103, Class Loss=0.8353390097618103, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/52, Loss=5.501963353157043
Loss made of: CE 0.5872688293457031, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.667014122009277 EntMin 0.0
Epoch 2, Batch 20/52, Loss=5.3205064833164215
Loss made of: CE 0.5974539518356323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.803963661193848 EntMin 0.0
Epoch 2, Batch 30/52, Loss=5.36940943300724
Loss made of: CE 0.43178248405456543, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.721846580505371 EntMin 0.0
Epoch 2, Batch 40/52, Loss=5.1731780707836155
Loss made of: CE 0.41292327642440796, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.299727439880371 EntMin 0.0
Epoch 2, Batch 50/52, Loss=5.005418309569359
Loss made of: CE 0.49471521377563477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.843623638153076 EntMin 0.0
Epoch 2, Class Loss=0.5422772765159607, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.5422772765159607, Class Loss=0.5422772765159607, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/52, Loss=4.946286600828171
Loss made of: CE 0.3745395541191101, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.603297710418701 EntMin 0.0
Epoch 3, Batch 20/52, Loss=4.842501425743103
Loss made of: CE 0.40390321612358093, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1965012550354 EntMin 0.0
Epoch 3, Batch 30/52, Loss=4.81677727997303
Loss made of: CE 0.4747132658958435, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7892842292785645 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.807233527302742
Loss made of: CE 0.3399428725242615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.050895690917969 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.775654798746109
Loss made of: CE 0.3584590554237366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.281868934631348 EntMin 0.0
Epoch 3, Class Loss=0.4290705919265747, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.4290705919265747, Class Loss=0.4290705919265747, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/52, Loss=4.83421428501606
Loss made of: CE 0.5262283682823181, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.174344062805176 EntMin 0.0
Epoch 4, Batch 20/52, Loss=4.529488968849182
Loss made of: CE 0.3584958016872406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.079983234405518 EntMin 0.0
Epoch 4, Batch 30/52, Loss=4.579247422516346
Loss made of: CE 0.2423357218503952, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9100208282470703 EntMin 0.0
Epoch 4, Batch 40/52, Loss=4.513334316015244
Loss made of: CE 0.30833888053894043, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8624587059020996 EntMin 0.0
Epoch 4, Batch 50/52, Loss=4.2885268300771715
Loss made of: CE 0.32372337579727173, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.711585521697998 EntMin 0.0
Epoch 4, Class Loss=0.3608623147010803, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3608623147010803, Class Loss=0.3608623147010803, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/52, Loss=4.450271052122116
Loss made of: CE 0.3473518490791321, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.875410795211792 EntMin 0.0
Epoch 5, Batch 20/52, Loss=4.398672813177109
Loss made of: CE 0.2806797921657562, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.897736072540283 EntMin 0.0
Epoch 5, Batch 30/52, Loss=4.3726768612861635
Loss made of: CE 0.38720521330833435, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.022536277770996 EntMin 0.0
Epoch 5, Batch 40/52, Loss=4.35059403181076
Loss made of: CE 0.359151691198349, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.449504375457764 EntMin 0.0
Epoch 5, Batch 50/52, Loss=4.3149216145277025
Loss made of: CE 0.32075971364974976, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.724575996398926 EntMin 0.0
Epoch 5, Class Loss=0.3449447453022003, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.3449447453022003, Class Loss=0.3449447453022003, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/52, Loss=4.2164975821971895
Loss made of: CE 0.24555936455726624, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6478075981140137 EntMin 0.0
Epoch 6, Batch 20/52, Loss=4.340985649824143
Loss made of: CE 0.27945226430892944, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5647358894348145 EntMin 0.0
Epoch 6, Batch 30/52, Loss=4.472050404548645
Loss made of: CE 0.4915319085121155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9928030967712402 EntMin 0.0
Epoch 6, Batch 40/52, Loss=4.085498553514481
Loss made of: CE 0.36246222257614136, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5972514152526855 EntMin 0.0
Epoch 6, Batch 50/52, Loss=4.293331179022789
Loss made of: CE 0.4331992566585541, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.089247226715088 EntMin 0.0
Epoch 6, Class Loss=0.3321790397167206, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.3321790397167206, Class Loss=0.3321790397167206, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=9.619283056259155
Loss made of: CE 0.9855461120605469, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.122368812561035 EntMin 0.0
Epoch 1, Batch 20/40, Loss=8.468348890542984
Loss made of: CE 0.6486449241638184, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.452095985412598 EntMin 0.0
Epoch 1, Batch 30/40, Loss=7.504340392351151
Loss made of: CE 0.5890582799911499, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.364404678344727 EntMin 0.0
Epoch 1, Batch 40/40, Loss=7.024247318506241
Loss made of: CE 0.46838945150375366, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.325498580932617 EntMin 0.0
Epoch 1, Class Loss=0.8035699725151062, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.8035699725151062, Class Loss=0.8035699725151062, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/40, Loss=6.805718576908111
Loss made of: CE 0.2758358120918274, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.58943510055542 EntMin 0.0
Epoch 2, Batch 20/40, Loss=6.340044778585434
Loss made of: CE 0.506449818611145, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2615838050842285 EntMin 0.0
Epoch 2, Batch 30/40, Loss=6.116792386770248
Loss made of: CE 0.4878736436367035, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.464430809020996 EntMin 0.0
Epoch 2, Batch 40/40, Loss=6.242943185567856
Loss made of: CE 0.45592010021209717, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.032101631164551 EntMin 0.0
Epoch 2, Class Loss=0.4978488087654114, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.4978488087654114, Class Loss=0.4978488087654114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/40, Loss=5.903031584620476
Loss made of: CE 0.3683607578277588, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.316448211669922 EntMin 0.0
Epoch 3, Batch 20/40, Loss=5.62613697052002
Loss made of: CE 0.4968838095664978, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.210671424865723 EntMin 0.0
Epoch 3, Batch 30/40, Loss=5.656175306439399
Loss made of: CE 0.3323483467102051, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.50944709777832 EntMin 0.0
Epoch 3, Batch 40/40, Loss=5.5491091281175615
Loss made of: CE 0.25335249304771423, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.055615425109863 EntMin 0.0
Epoch 3, Class Loss=0.4141392409801483, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.4141392409801483, Class Loss=0.4141392409801483, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/40, Loss=5.403052341938019
Loss made of: CE 0.41088324785232544, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.554047107696533 EntMin 0.0
Epoch 4, Batch 20/40, Loss=5.489075693488121
Loss made of: CE 0.21437716484069824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.683132171630859 EntMin 0.0
Epoch 4, Batch 30/40, Loss=5.358172944188118
Loss made of: CE 0.3596683740615845, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.09822940826416 EntMin 0.0
Epoch 4, Batch 40/40, Loss=5.491375353932381
Loss made of: CE 0.34600362181663513, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.168972969055176 EntMin 0.0
Epoch 4, Class Loss=0.36272117495536804, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.36272117495536804, Class Loss=0.36272117495536804, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/40, Loss=5.34223250746727
Loss made of: CE 0.318231463432312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.749142646789551 EntMin 0.0
Epoch 5, Batch 20/40, Loss=5.126037520170212
Loss made of: CE 0.4743156433105469, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.260575771331787 EntMin 0.0
Epoch 5, Batch 30/40, Loss=5.269506192207336
Loss made of: CE 0.303632915019989, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.552989482879639 EntMin 0.0
Epoch 5, Batch 40/40, Loss=5.21963646709919
Loss made of: CE 0.34342825412750244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.654191017150879 EntMin 0.0
Epoch 5, Class Loss=0.3489673435688019, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.3489673435688019, Class Loss=0.3489673435688019, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/40, Loss=5.031142070889473
Loss made of: CE 0.2692820429801941, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.648186683654785 EntMin 0.0
Epoch 6, Batch 20/40, Loss=5.249328690767288
Loss made of: CE 0.32044392824172974, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.360158920288086 EntMin 0.0
Epoch 6, Batch 30/40, Loss=5.165318948030472
Loss made of: CE 0.3320326507091522, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.405837059020996 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.961192891001701
Loss made of: CE 0.24772903323173523, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.556589126586914 EntMin 0.0
Epoch 6, Class Loss=0.32497352361679077, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.32497352361679077, Class Loss=0.32497352361679077, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=10.130135047435761
Loss made of: CE 1.1685354709625244, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.305137634277344 EntMin 0.0
Epoch 1, Batch 20/34, Loss=8.421655744314194
Loss made of: CE 0.7628116607666016, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.527923583984375 EntMin 0.0
Epoch 1, Batch 30/34, Loss=7.9421001136302944
Loss made of: CE 0.7961756587028503, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.771012783050537 EntMin 0.0
Epoch 1, Class Loss=0.9847039580345154, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.9847039580345154, Class Loss=0.9847039580345154, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/34, Loss=7.160568678379059
Loss made of: CE 0.6513628363609314, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.403237819671631 EntMin 0.0
Epoch 2, Batch 20/34, Loss=6.639585357904434
Loss made of: CE 0.6428551077842712, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.733071327209473 EntMin 0.0
Epoch 2, Batch 30/34, Loss=6.260920131206513
Loss made of: CE 0.5248932838439941, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.652496337890625 EntMin 0.0
Epoch 2, Class Loss=0.6199215054512024, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6199215054512024, Class Loss=0.6199215054512024, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/34, Loss=5.9419596016407015
Loss made of: CE 0.5264142751693726, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.115811824798584 EntMin 0.0
Epoch 3, Batch 20/34, Loss=5.936120843887329
Loss made of: CE 0.4873175024986267, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.138288497924805 EntMin 0.0
Epoch 3, Batch 30/34, Loss=5.909289455413818
Loss made of: CE 0.45685920119285583, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.277614593505859 EntMin 0.0
Epoch 3, Class Loss=0.4809925854206085, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4809925854206085, Class Loss=0.4809925854206085, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/34, Loss=5.579055586457253
Loss made of: CE 0.4214840829372406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9839372634887695 EntMin 0.0
Epoch 4, Batch 20/34, Loss=5.649280679225922
Loss made of: CE 0.38928091526031494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.789682388305664 EntMin 0.0
Epoch 4, Batch 30/34, Loss=5.638465800881386
Loss made of: CE 0.504305899143219, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.801968574523926 EntMin 0.0
Epoch 4, Class Loss=0.4031718373298645, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4031718373298645, Class Loss=0.4031718373298645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/34, Loss=5.317493960261345
Loss made of: CE 0.3312509059906006, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.725069999694824 EntMin 0.0
Epoch 5, Batch 20/34, Loss=5.394763574004173
Loss made of: CE 0.427837610244751, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.261910438537598 EntMin 0.0
Epoch 5, Batch 30/34, Loss=5.157836619019508
Loss made of: CE 0.3779751658439636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.716787338256836 EntMin 0.0
Epoch 5, Class Loss=0.3671039044857025, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.3671039044857025, Class Loss=0.3671039044857025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/34, Loss=5.217267698049545
Loss made of: CE 0.3608413636684418, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.122236728668213 EntMin 0.0
Epoch 6, Batch 20/34, Loss=5.265291172266006
Loss made of: CE 0.4065515398979187, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.819739818572998 EntMin 0.0
Epoch 6, Batch 30/34, Loss=5.275933349132538
Loss made of: CE 0.30741116404533386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.504445552825928 EntMin 0.0
Epoch 6, Class Loss=0.34526270627975464, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.34526270627975464, Class Loss=0.34526270627975464, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=9.226743006706238
Loss made of: CE 1.1980223655700684, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.511418342590332 EntMin 0.0
Epoch 1, Batch 20/33, Loss=8.056860303878784
Loss made of: CE 1.0446720123291016, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.304590225219727 EntMin 0.0
Epoch 1, Batch 30/33, Loss=6.867711865901947
Loss made of: CE 0.7782827615737915, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.165507793426514 EntMin 0.0
Epoch 1, Class Loss=0.9864934682846069, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.9864934682846069, Class Loss=0.9864934682846069, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/33, Loss=6.254217910766601
Loss made of: CE 0.6921706199645996, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.071103572845459 EntMin 0.0
Epoch 2, Batch 20/33, Loss=5.970818692445755
Loss made of: CE 0.8653100728988647, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.729533672332764 EntMin 0.0
Epoch 2, Batch 30/33, Loss=5.533191025257111
Loss made of: CE 0.59708571434021, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.910067558288574 EntMin 0.0
Epoch 2, Class Loss=0.6766274571418762, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.6766274571418762, Class Loss=0.6766274571418762, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/33, Loss=5.27680167555809
Loss made of: CE 0.5151937007904053, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.465028762817383 EntMin 0.0
Epoch 3, Batch 20/33, Loss=5.203414452075958
Loss made of: CE 0.49557262659072876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.399480819702148 EntMin 0.0
Epoch 3, Batch 30/33, Loss=5.029303738474846
Loss made of: CE 0.47142308950424194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.283821105957031 EntMin 0.0
Epoch 3, Class Loss=0.5274466276168823, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5274466276168823, Class Loss=0.5274466276168823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/33, Loss=4.76332640349865
Loss made of: CE 0.4304274916648865, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9933829307556152 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.8406429708004
Loss made of: CE 0.38217172026634216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2773261070251465 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.684686076641083
Loss made of: CE 0.31775227189064026, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.990157127380371 EntMin 0.0
Epoch 4, Class Loss=0.428211510181427, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.428211510181427, Class Loss=0.428211510181427, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/33, Loss=4.609659495949745
Loss made of: CE 0.44918912649154663, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1401262283325195 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.56322971880436
Loss made of: CE 0.2939557731151581, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8880205154418945 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.673324218392372
Loss made of: CE 0.3890039324760437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9972429275512695 EntMin 0.0
Epoch 5, Class Loss=0.37784239649772644, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.37784239649772644, Class Loss=0.37784239649772644, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/33, Loss=4.424328482151031
Loss made of: CE 0.4036242961883545, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0781755447387695 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.543323284387588
Loss made of: CE 0.34454700350761414, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.99176025390625 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.348332792520523
Loss made of: CE 0.31865936517715454, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.383386611938477 EntMin 0.0
Epoch 6, Class Loss=0.34467774629592896, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.34467774629592896, Class Loss=0.34467774629592896, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.41054412722587585, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.839769
Mean Acc: 0.506402
FreqW Acc: 0.717137
Mean IoU: 0.373373
Class IoU:
	class 0: 0.8470877
	class 1: 0.8076669
	class 2: 0.31151953
	class 3: 0.50530624
	class 4: 0.5316257
	class 5: 0.0
	class 6: 0.0024947373
	class 7: 0.0002508002
	class 8: 0.35440916
Class Acc:
	class 0: 0.98159736
	class 1: 0.8699377
	class 2: 0.53413653
	class 3: 0.9643196
	class 4: 0.84632
	class 5: 0.0
	class 6: 0.0024947373
	class 7: 0.0002508002
	class 8: 0.3585573

federated global round: 6, step: 1
select part of clients to conduct local training
[11, 5, 8, 12]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/43, Loss=5.15062182545662
Loss made of: CE 0.3745223879814148, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.692275047302246 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.741844674944877
Loss made of: CE 0.5951957702636719, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9969375133514404 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.603065377473831
Loss made of: CE 0.3485296666622162, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.699518203735352 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.461491802334786
Loss made of: CE 0.40421175956726074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7298378944396973 EntMin 0.0
Epoch 1, Class Loss=0.4250466525554657, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.4250466525554657, Class Loss=0.4250466525554657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/43, Loss=4.369192206859589
Loss made of: CE 0.35800009965896606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8031115531921387 EntMin 0.0
Epoch 2, Batch 20/43, Loss=4.2247398644685745
Loss made of: CE 0.3531806468963623, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9796512126922607 EntMin 0.0
Epoch 2, Batch 30/43, Loss=4.2219654187560085
Loss made of: CE 0.4187510013580322, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.276919841766357 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.29541552066803
Loss made of: CE 0.4027429521083832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.039584159851074 EntMin 0.0
Epoch 2, Class Loss=0.3495863974094391, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.3495863974094391, Class Loss=0.3495863974094391, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/43, Loss=3.984977212548256
Loss made of: CE 0.369473397731781, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6273648738861084 EntMin 0.0
Epoch 3, Batch 20/43, Loss=4.123837427794934
Loss made of: CE 0.2795819640159607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1540727615356445 EntMin 0.0
Epoch 3, Batch 30/43, Loss=4.161713679134846
Loss made of: CE 0.39673489332199097, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7370991706848145 EntMin 0.0
Epoch 3, Batch 40/43, Loss=4.064064730703831
Loss made of: CE 0.2546122074127197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8262288570404053 EntMin 0.0
Epoch 3, Class Loss=0.32292646169662476, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.32292646169662476, Class Loss=0.32292646169662476, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/43, Loss=3.9618995428085326
Loss made of: CE 0.3771657943725586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.059918403625488 EntMin 0.0
Epoch 4, Batch 20/43, Loss=3.979872314631939
Loss made of: CE 0.2885425090789795, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.542534828186035 EntMin 0.0
Epoch 4, Batch 30/43, Loss=3.9301586374640465
Loss made of: CE 0.3265199363231659, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.523540496826172 EntMin 0.0
Epoch 4, Batch 40/43, Loss=4.006060732901096
Loss made of: CE 0.29497969150543213, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5015904903411865 EntMin 0.0
Epoch 4, Class Loss=0.308541476726532, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.308541476726532, Class Loss=0.308541476726532, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/43, Loss=3.978987714648247
Loss made of: CE 0.27393150329589844, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9531164169311523 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.9247970446944236
Loss made of: CE 0.3592243194580078, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9440135955810547 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.9847068950533866
Loss made of: CE 0.30322757363319397, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.727639675140381 EntMin 0.0
Epoch 5, Batch 40/43, Loss=3.9992180556058883
Loss made of: CE 0.3528396487236023, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3025240898132324 EntMin 0.0
Epoch 5, Class Loss=0.307559072971344, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.307559072971344, Class Loss=0.307559072971344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/43, Loss=3.875090481340885
Loss made of: CE 0.33971118927001953, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2663745880126953 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.9410903230309486
Loss made of: CE 0.26263123750686646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5886340141296387 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.880264188349247
Loss made of: CE 0.24970678985118866, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3436479568481445 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.750668856501579
Loss made of: CE 0.3067697286605835, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4497146606445312 EntMin 0.0
Epoch 6, Class Loss=0.29028552770614624, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.29028552770614624, Class Loss=0.29028552770614624, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=5.6402411490678785
Loss made of: CE 0.5311921834945679, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9998650550842285 EntMin 0.0
Epoch 1, Batch 20/34, Loss=5.545323523879051
Loss made of: CE 0.37340664863586426, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2378950119018555 EntMin 0.0
Epoch 1, Batch 30/34, Loss=5.384297889471054
Loss made of: CE 0.2650798261165619, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.846402168273926 EntMin 0.0
Epoch 1, Class Loss=0.42191627621650696, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.42191627621650696, Class Loss=0.42191627621650696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/34, Loss=5.179756501317025
Loss made of: CE 0.31271225214004517, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.668615341186523 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.915715953707695
Loss made of: CE 0.3040946125984192, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7831501960754395 EntMin 0.0
Epoch 2, Batch 30/34, Loss=5.217599262297154
Loss made of: CE 0.3747968077659607, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.707046031951904 EntMin 0.0
Epoch 2, Class Loss=0.3089759945869446, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3089759945869446, Class Loss=0.3089759945869446, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/34, Loss=4.78865859657526
Loss made of: CE 0.3054927587509155, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.561487197875977 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.873668587207794
Loss made of: CE 0.30644580721855164, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.268156051635742 EntMin 0.0
Epoch 3, Batch 30/34, Loss=5.05289439111948
Loss made of: CE 0.3113578259944916, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1249847412109375 EntMin 0.0
Epoch 3, Class Loss=0.29034167528152466, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.29034167528152466, Class Loss=0.29034167528152466, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/34, Loss=4.9095894515514376
Loss made of: CE 0.2786649167537689, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.57422399520874 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.924752980470657
Loss made of: CE 0.32038581371307373, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.747786998748779 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.754707060754299
Loss made of: CE 0.26764702796936035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0125732421875 EntMin 0.0
Epoch 4, Class Loss=0.28976884484291077, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.28976884484291077, Class Loss=0.28976884484291077, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/34, Loss=4.719404529035091
Loss made of: CE 0.2734682559967041, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.80244779586792 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.783936274051666
Loss made of: CE 0.2945091724395752, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.287139415740967 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.7264882862567905
Loss made of: CE 0.31594544649124146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.682889938354492 EntMin 0.0
Epoch 5, Class Loss=0.2734220623970032, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.2734220623970032, Class Loss=0.2734220623970032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/34, Loss=4.872229102253914
Loss made of: CE 0.31693822145462036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.402913570404053 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.65792124569416
Loss made of: CE 0.23956340551376343, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.596836566925049 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.640516629815101
Loss made of: CE 0.5725675821304321, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.979940891265869 EntMin 0.0
Epoch 6, Class Loss=0.28763607144355774, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.28763607144355774, Class Loss=0.28763607144355774, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=5.740746304392815
Loss made of: CE 0.42073172330856323, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.418614864349365 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.56600469648838
Loss made of: CE 0.2901727855205536, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.438750267028809 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.596324071288109
Loss made of: CE 0.34255051612854004, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.08000373840332 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.219454365968704
Loss made of: CE 0.21792027354240417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.555398941040039 EntMin 0.0
Epoch 1, Class Loss=0.36137014627456665, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.36137014627456665, Class Loss=0.36137014627456665, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/40, Loss=5.255133780837059
Loss made of: CE 0.37675756216049194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.515376091003418 EntMin 0.0
Epoch 2, Batch 20/40, Loss=4.931104953587055
Loss made of: CE 0.16885046660900116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3603057861328125 EntMin 0.0
Epoch 2, Batch 30/40, Loss=5.158986055850983
Loss made of: CE 0.3926028609275818, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.039682388305664 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.1131344705820085
Loss made of: CE 0.3137143850326538, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.909355640411377 EntMin 0.0
Epoch 2, Class Loss=0.3190097510814667, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.3190097510814667, Class Loss=0.3190097510814667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/40, Loss=4.969115498661995
Loss made of: CE 0.2532070279121399, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.781156063079834 EntMin 0.0
Epoch 3, Batch 20/40, Loss=4.803218641877175
Loss made of: CE 0.2850973606109619, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.630558490753174 EntMin 0.0
Epoch 3, Batch 30/40, Loss=4.900020964443684
Loss made of: CE 0.3052518367767334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.220808982849121 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.933495263755321
Loss made of: CE 0.2717679738998413, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.116921424865723 EntMin 0.0
Epoch 3, Class Loss=0.2954167127609253, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.2954167127609253, Class Loss=0.2954167127609253, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/40, Loss=4.921487593650818
Loss made of: CE 0.40024781227111816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7749786376953125 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.745578764379024
Loss made of: CE 0.22235839068889618, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.334657669067383 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.825432734191418
Loss made of: CE 0.2680423855781555, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760263442993164 EntMin 0.0
Epoch 4, Batch 40/40, Loss=4.827179177105426
Loss made of: CE 0.3040495812892914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2310686111450195 EntMin 0.0
Epoch 4, Class Loss=0.2953616678714752, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.2953616678714752, Class Loss=0.2953616678714752, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/40, Loss=4.758893360197544
Loss made of: CE 0.3316164016723633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.282840728759766 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.789093014597893
Loss made of: CE 0.3231608271598816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.055689811706543 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.551333059370518
Loss made of: CE 0.30800506472587585, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4002275466918945 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.739803442358971
Loss made of: CE 0.23407647013664246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611478328704834 EntMin 0.0
Epoch 5, Class Loss=0.2811781167984009, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.2811781167984009, Class Loss=0.2811781167984009, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/40, Loss=4.461708459258079
Loss made of: CE 0.23244115710258484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.456748008728027 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.619999073445797
Loss made of: CE 0.2638779282569885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.222993850708008 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.594667974114418
Loss made of: CE 0.2948250472545624, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.245285987854004 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.61310933381319
Loss made of: CE 0.29389819502830505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.313488483428955 EntMin 0.0
Epoch 6, Class Loss=0.27778494358062744, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.27778494358062744, Class Loss=0.27778494358062744, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/40, Loss=5.818319401144981
Loss made of: CE 0.47606849670410156, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.367870330810547 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.70479491353035
Loss made of: CE 0.4544791579246521, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.135302543640137 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.558336591720581
Loss made of: CE 0.5419086217880249, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.951673984527588 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.436854857206344
Loss made of: CE 0.36721962690353394, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.017308235168457 EntMin 0.0
Epoch 1, Class Loss=0.48650574684143066, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.48650574684143066, Class Loss=0.48650574684143066, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/40, Loss=5.418506413698196
Loss made of: CE 0.21670708060264587, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.435338020324707 EntMin 0.0
Epoch 2, Batch 20/40, Loss=5.205585685372353
Loss made of: CE 0.3284725546836853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.298165321350098 EntMin 0.0
Epoch 2, Batch 30/40, Loss=5.187811115384102
Loss made of: CE 0.4559426009654999, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.563210487365723 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.360271528363228
Loss made of: CE 0.3670535981655121, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.285569190979004 EntMin 0.0
Epoch 2, Class Loss=0.3874414563179016, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.3874414563179016, Class Loss=0.3874414563179016, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/40, Loss=5.138967621326446
Loss made of: CE 0.2903541028499603, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.438895225524902 EntMin 0.0
Epoch 3, Batch 20/40, Loss=5.085327523946762
Loss made of: CE 0.44800710678100586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.692631721496582 EntMin 0.0
Epoch 3, Batch 30/40, Loss=5.033193716406823
Loss made of: CE 0.2722993493080139, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6346821784973145 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.9333365768194195
Loss made of: CE 0.25285837054252625, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.309762001037598 EntMin 0.0
Epoch 3, Class Loss=0.36287710070610046, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.36287710070610046, Class Loss=0.36287710070610046, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/40, Loss=4.7375855296850204
Loss made of: CE 0.35868510603904724, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.175478458404541 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.892815917730331
Loss made of: CE 0.18155944347381592, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.067239761352539 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.869187688827514
Loss made of: CE 0.3445434272289276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.588863372802734 EntMin 0.0
Epoch 4, Batch 40/40, Loss=5.058735111355782
Loss made of: CE 0.3210226893424988, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.955946922302246 EntMin 0.0
Epoch 4, Class Loss=0.3348042964935303, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.3348042964935303, Class Loss=0.3348042964935303, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/40, Loss=4.9039111495018
Loss made of: CE 0.31612539291381836, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.462250709533691 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.6628784701228145
Loss made of: CE 0.3927036225795746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.731427192687988 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.795702514052391
Loss made of: CE 0.24598419666290283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0621843338012695 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.847025209665299
Loss made of: CE 0.31674492359161377, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2821149826049805 EntMin 0.0
Epoch 5, Class Loss=0.31724536418914795, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.31724536418914795, Class Loss=0.31724536418914795, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/40, Loss=4.6405019327998165
Loss made of: CE 0.2636447548866272, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427688121795654 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.789386869966984
Loss made of: CE 0.3032042384147644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.813087463378906 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.81349401473999
Loss made of: CE 0.2977641522884369, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.005880355834961 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.629843981564045
Loss made of: CE 0.224354550242424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.428406238555908 EntMin 0.0
Epoch 6, Class Loss=0.2926451861858368, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.2926451861858368, Class Loss=0.2926451861858368, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.29713213443756104, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.884675
Mean Acc: 0.609796
FreqW Acc: 0.791021
Mean IoU: 0.506324
Class IoU:
	class 0: 0.8777754
	class 1: 0.80694413
	class 2: 0.30511716
	class 3: 0.7049256
	class 4: 0.58100724
	class 5: 0.0087063
	class 6: 0.45170563
	class 7: 0.08052736
	class 8: 0.74020666
Class Acc:
	class 0: 0.9777592
	class 1: 0.8573243
	class 2: 0.53169465
	class 3: 0.94870347
	class 4: 0.81530815
	class 5: 0.00871005
	class 6: 0.45612425
	class 7: 0.080738254
	class 8: 0.81180197

federated global round: 7, step: 1
select part of clients to conduct local training
[0, 6, 13, 5]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/52, Loss=4.494483783841133
Loss made of: CE 0.6402725577354431, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.022002220153809 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.181888589262963
Loss made of: CE 0.35316747426986694, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.502079486846924 EntMin 0.0
Epoch 1, Batch 30/52, Loss=4.1679947674274445
Loss made of: CE 0.375365674495697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7127647399902344 EntMin 0.0
Epoch 1, Batch 40/52, Loss=4.229051148891449
Loss made of: CE 0.34348106384277344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.062376976013184 EntMin 0.0
Epoch 1, Batch 50/52, Loss=4.103914791345597
Loss made of: CE 0.24695688486099243, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.504824161529541 EntMin 0.0
Epoch 1, Class Loss=0.3966173231601715, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.3966173231601715, Class Loss=0.3966173231601715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/52, Loss=4.059839867055416
Loss made of: CE 0.3886149823665619, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5234880447387695 EntMin 0.0
Epoch 2, Batch 20/52, Loss=4.026182354986668
Loss made of: CE 0.32712554931640625, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.44415545463562 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.202903202176094
Loss made of: CE 0.41590091586112976, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.06156063079834 EntMin 0.0
Epoch 2, Batch 40/52, Loss=4.162107127904892
Loss made of: CE 0.296642005443573, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3807952404022217 EntMin 0.0
Epoch 2, Batch 50/52, Loss=4.054839193820953
Loss made of: CE 0.336430162191391, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.132656574249268 EntMin 0.0
Epoch 2, Class Loss=0.32346922159194946, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.32346922159194946, Class Loss=0.32346922159194946, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/52, Loss=4.024877053499222
Loss made of: CE 0.26457273960113525, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.960282325744629 EntMin 0.0
Epoch 3, Batch 20/52, Loss=4.027031610906124
Loss made of: CE 0.2814304828643799, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.716245651245117 EntMin 0.0
Epoch 3, Batch 30/52, Loss=4.112574726343155
Loss made of: CE 0.26009851694107056, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.373595952987671 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.064443451166153
Loss made of: CE 0.29824209213256836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6463112831115723 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.062805087864399
Loss made of: CE 0.30659371614456177, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5839385986328125 EntMin 0.0
Epoch 3, Class Loss=0.30504435300827026, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.30504435300827026, Class Loss=0.30504435300827026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/52, Loss=4.2168461859226225
Loss made of: CE 0.29973527789115906, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.652462959289551 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.890114836394787
Loss made of: CE 0.23811660706996918, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4591574668884277 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.937680958211422
Loss made of: CE 0.24707405269145966, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3767802715301514 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.9400079622864723
Loss made of: CE 0.23590587079524994, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4431283473968506 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.7939697265625
Loss made of: CE 0.2874252200126648, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3303189277648926 EntMin 0.0
Epoch 4, Class Loss=0.28660324215888977, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.28660324215888977, Class Loss=0.28660324215888977, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/52, Loss=3.9144318044185638
Loss made of: CE 0.2589169442653656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5237059593200684 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.860794831812382
Loss made of: CE 0.2357550859451294, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.467517614364624 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.844745767116547
Loss made of: CE 0.2940021753311157, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6368234157562256 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.859862418472767
Loss made of: CE 0.3540214002132416, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8996806144714355 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.817269913852215
Loss made of: CE 0.28004151582717896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.347519636154175 EntMin 0.0
Epoch 5, Class Loss=0.27450355887413025, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.27450355887413025, Class Loss=0.27450355887413025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/52, Loss=3.717463527619839
Loss made of: CE 0.2709355354309082, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3240180015563965 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.921211740374565
Loss made of: CE 0.22365933656692505, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.252873182296753 EntMin 0.0
Epoch 6, Batch 30/52, Loss=4.017089562118054
Loss made of: CE 0.29213568568229675, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4867331981658936 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.6345074847340584
Loss made of: CE 0.293959379196167, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.386962652206421 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.8417440965771674
Loss made of: CE 0.30295294523239136, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8248279094696045 EntMin 0.0
Epoch 6, Class Loss=0.2754664421081543, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.2754664421081543, Class Loss=0.2754664421081543, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/34, Loss=5.313364687561989
Loss made of: CE 0.5361323356628418, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.468597412109375 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.816588008403778
Loss made of: CE 0.3087860345840454, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.059142112731934 EntMin 0.0
Epoch 1, Batch 30/34, Loss=5.063754969835282
Loss made of: CE 0.3397768437862396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.462071895599365 EntMin 0.0
Epoch 1, Class Loss=0.4377824366092682, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.4377824366092682, Class Loss=0.4377824366092682, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/34, Loss=4.895193532109261
Loss made of: CE 0.28470370173454285, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5949859619140625 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.584026035666466
Loss made of: CE 0.3379000425338745, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.159944534301758 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.588016137480736
Loss made of: CE 0.30090129375457764, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.553938865661621 EntMin 0.0
Epoch 2, Class Loss=0.31974318623542786, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.31974318623542786, Class Loss=0.31974318623542786, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/34, Loss=4.457113364338875
Loss made of: CE 0.3009861409664154, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.011308670043945 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.4872540310025215
Loss made of: CE 0.24874107539653778, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.973637580871582 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.595556250214576
Loss made of: CE 0.29547953605651855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.157926082611084 EntMin 0.0
Epoch 3, Class Loss=0.29854756593704224, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.29854756593704224, Class Loss=0.29854756593704224, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/34, Loss=4.53523126244545
Loss made of: CE 0.36535489559173584, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.338634490966797 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.57457945048809
Loss made of: CE 0.28617939352989197, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.105398178100586 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.6147680029273035
Loss made of: CE 0.3902484178543091, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.876482963562012 EntMin 0.0
Epoch 4, Class Loss=0.30511900782585144, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.30511900782585144, Class Loss=0.30511900782585144, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/34, Loss=4.362300525605678
Loss made of: CE 0.2710714340209961, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.791975736618042 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.578449197113514
Loss made of: CE 0.3773082196712494, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.528623580932617 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.28410924077034
Loss made of: CE 0.2922217547893524, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.048256874084473 EntMin 0.0
Epoch 5, Class Loss=0.2993389964103699, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.2993389964103699, Class Loss=0.2993389964103699, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/34, Loss=4.340598060190677
Loss made of: CE 0.35495349764823914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.154993057250977 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.3358397603034975
Loss made of: CE 0.3233643174171448, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.107067108154297 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.4447087660431865
Loss made of: CE 0.26134932041168213, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7550783157348633 EntMin 0.0
Epoch 6, Class Loss=0.28202512860298157, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.28202512860298157, Class Loss=0.28202512860298157, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=6.481941312551498
Loss made of: CE 0.49800628423690796, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.028181552886963 EntMin 0.0
Epoch 1, Batch 20/33, Loss=5.464082357287407
Loss made of: CE 0.5447899103164673, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.303376197814941 EntMin 0.0
Epoch 1, Batch 30/33, Loss=4.950522790849209
Loss made of: CE 0.31067556142807007, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.29457950592041 EntMin 0.0
Epoch 1, Class Loss=0.5520163774490356, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5520163774490356, Class Loss=0.5520163774490356, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/33, Loss=4.764136376976967
Loss made of: CE 0.3906305730342865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.250195503234863 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.578225299715996
Loss made of: CE 0.2661585211753845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.056788921356201 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.433997878432274
Loss made of: CE 0.32335197925567627, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.313506126403809 EntMin 0.0
Epoch 2, Class Loss=0.3183452785015106, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.3183452785015106, Class Loss=0.3183452785015106, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/33, Loss=4.417371986806392
Loss made of: CE 0.33496373891830444, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.706738471984863 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.314507606625557
Loss made of: CE 0.24309441447257996, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0295515060424805 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.208157517015934
Loss made of: CE 0.2549647092819214, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8849568367004395 EntMin 0.0
Epoch 3, Class Loss=0.28137415647506714, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.28137415647506714, Class Loss=0.28137415647506714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/33, Loss=4.1867214605212215
Loss made of: CE 0.2440289705991745, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.828202724456787 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.21031978726387
Loss made of: CE 0.29787808656692505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.102336406707764 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.09844920784235
Loss made of: CE 0.23129801452159882, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6532535552978516 EntMin 0.0
Epoch 4, Class Loss=0.27075985074043274, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.27075985074043274, Class Loss=0.27075985074043274, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/33, Loss=4.200122901797295
Loss made of: CE 0.2935398519039154, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6645236015319824 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.04457633793354
Loss made of: CE 0.2623020112514496, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6690216064453125 EntMin 0.0
Epoch 5, Batch 30/33, Loss=3.9909937217831613
Loss made of: CE 0.2882642447948456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.876803398132324 EntMin 0.0
Epoch 5, Class Loss=0.26984313130378723, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.26984313130378723, Class Loss=0.26984313130378723, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/33, Loss=4.132726788520813
Loss made of: CE 0.332444965839386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.099972724914551 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.061009211838245
Loss made of: CE 0.22073757648468018, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.603489398956299 EntMin 0.0
Epoch 6, Batch 30/33, Loss=3.929837080836296
Loss made of: CE 0.2893776595592499, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9421043395996094 EntMin 0.0
Epoch 6, Class Loss=0.27642083168029785, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.27642083168029785, Class Loss=0.27642083168029785, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/34, Loss=5.339737993478775
Loss made of: CE 0.5451129674911499, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.592998504638672 EntMin 0.0
Epoch 1, Batch 20/34, Loss=5.0112255871295925
Loss made of: CE 0.34645217657089233, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.524877548217773 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.90236888229847
Loss made of: CE 0.3231174349784851, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.47196102142334 EntMin 0.0
Epoch 1, Class Loss=0.42229944467544556, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.42229944467544556, Class Loss=0.42229944467544556, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/34, Loss=4.83922929763794
Loss made of: CE 0.3398643136024475, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.198198318481445 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.629336127638817
Loss made of: CE 0.3070622682571411, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5553107261657715 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.963929545879364
Loss made of: CE 0.4008779525756836, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.185838222503662 EntMin 0.0
Epoch 2, Class Loss=0.3358948230743408, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3358948230743408, Class Loss=0.3358948230743408, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/34, Loss=4.439992448687553
Loss made of: CE 0.2737519145011902, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202776908874512 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.54127299785614
Loss made of: CE 0.3077980875968933, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0581135749816895 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.670076590776444
Loss made of: CE 0.2945046126842499, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.951450824737549 EntMin 0.0
Epoch 3, Class Loss=0.2946699261665344, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.2946699261665344, Class Loss=0.2946699261665344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/34, Loss=4.538185866177082
Loss made of: CE 0.2880251109600067, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.236427307128906 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.6108438819646835
Loss made of: CE 0.30920499563217163, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303805828094482 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.475032065808773
Loss made of: CE 0.27962833642959595, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6829023361206055 EntMin 0.0
Epoch 4, Class Loss=0.2968502640724182, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.2968502640724182, Class Loss=0.2968502640724182, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/34, Loss=4.521102604269982
Loss made of: CE 0.2519932687282562, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3276686668396 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.491861152648926
Loss made of: CE 0.3046753704547882, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.086928844451904 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.5187640860676765
Loss made of: CE 0.2942557632923126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.506544589996338 EntMin 0.0
Epoch 5, Class Loss=0.28332754969596863, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.28332754969596863, Class Loss=0.28332754969596863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/34, Loss=4.567109972238541
Loss made of: CE 0.3278127610683441, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0920844078063965 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.394160087406635
Loss made of: CE 0.23813307285308838, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.039423942565918 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.407062143087387
Loss made of: CE 0.38270673155784607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5323357582092285 EntMin 0.0
Epoch 6, Class Loss=0.27970266342163086, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.27970266342163086, Class Loss=0.27970266342163086, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.25135982036590576, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.923604
Mean Acc: 0.732874
FreqW Acc: 0.861881
Mean IoU: 0.623738
Class IoU:
	class 0: 0.9189772
	class 1: 0.82576835
	class 2: 0.30787122
	class 3: 0.72496456
	class 4: 0.6260979
	class 5: 0.0
	class 6: 0.8400153
	class 7: 0.6060248
	class 8: 0.76392215
Class Acc:
	class 0: 0.9725545
	class 1: 0.8895747
	class 2: 0.54429036
	class 3: 0.935436
	class 4: 0.86062676
	class 5: 0.0
	class 6: 0.9207934
	class 7: 0.6355097
	class 8: 0.8370811

federated global round: 8, step: 1
select part of clients to conduct local training
[4, 11, 9, 6]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=5.503127521276474
Loss made of: CE 0.46630772948265076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.699584007263184 EntMin 0.0
Epoch 1, Batch 20/25, Loss=5.0680005013942715
Loss made of: CE 0.30790457129478455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.436236381530762 EntMin 0.0
Epoch 1, Class Loss=0.3929073214530945, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.3929073214530945, Class Loss=0.3929073214530945, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/25, Loss=4.74479208290577
Loss made of: CE 0.3183881640434265, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8936030864715576 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.611410154402256
Loss made of: CE 0.42820239067077637, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.616593837738037 EntMin 0.0
Epoch 2, Class Loss=0.31565240025520325, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.31565240025520325, Class Loss=0.31565240025520325, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/25, Loss=4.466911381483078
Loss made of: CE 0.3207992613315582, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.316328048706055 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.298035117983818
Loss made of: CE 0.3240875005722046, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.021883964538574 EntMin 0.0
Epoch 3, Class Loss=0.2971997857093811, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.2971997857093811, Class Loss=0.2971997857093811, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/25, Loss=4.389658689498901
Loss made of: CE 0.3145739734172821, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.066148281097412 EntMin 0.0
Epoch 4, Batch 20/25, Loss=4.202547177672386
Loss made of: CE 0.18597130477428436, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195306777954102 EntMin 0.0
Epoch 4, Class Loss=0.2760429084300995, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.2760429084300995, Class Loss=0.2760429084300995, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/25, Loss=4.2552544385194775
Loss made of: CE 0.24111518263816833, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.842439651489258 EntMin 0.0
Epoch 5, Batch 20/25, Loss=4.3915916085243225
Loss made of: CE 0.4713677167892456, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.891798496246338 EntMin 0.0
Epoch 5, Class Loss=0.28387460112571716, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.28387460112571716, Class Loss=0.28387460112571716, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/25, Loss=4.145306421816349
Loss made of: CE 0.22799190878868103, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.804269790649414 EntMin 0.0
Epoch 6, Batch 20/25, Loss=4.197492276132107
Loss made of: CE 0.3143591284751892, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.292889595031738 EntMin 0.0
Epoch 6, Class Loss=0.2580977976322174, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.2580977976322174, Class Loss=0.2580977976322174, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/43, Loss=5.14593291580677
Loss made of: CE 0.5042569041252136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.717785835266113 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.631797704100609
Loss made of: CE 0.5434806942939758, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9788835048675537 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.391651737689972
Loss made of: CE 0.3353516459465027, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.306993007659912 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.193093493580818
Loss made of: CE 0.38801276683807373, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4769253730773926 EntMin 0.0
Epoch 1, Class Loss=0.4473740756511688, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.4473740756511688, Class Loss=0.4473740756511688, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/43, Loss=4.167760773003101
Loss made of: CE 0.3258715271949768, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6733860969543457 EntMin 0.0
Epoch 2, Batch 20/43, Loss=4.042423942685128
Loss made of: CE 0.31094449758529663, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7247769832611084 EntMin 0.0
Epoch 2, Batch 30/43, Loss=4.064373967051506
Loss made of: CE 0.44164547324180603, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.235383987426758 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.108606821298599
Loss made of: CE 0.3414883613586426, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7948367595672607 EntMin 0.0
Epoch 2, Class Loss=0.33036693930625916, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.33036693930625916, Class Loss=0.33036693930625916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/43, Loss=3.84394171833992
Loss made of: CE 0.3155732750892639, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3885440826416016 EntMin 0.0
Epoch 3, Batch 20/43, Loss=4.002484038472176
Loss made of: CE 0.3088560998439789, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.938666820526123 EntMin 0.0
Epoch 3, Batch 30/43, Loss=3.9378930807113646
Loss made of: CE 0.3192761540412903, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.546177625656128 EntMin 0.0
Epoch 3, Batch 40/43, Loss=3.876587228477001
Loss made of: CE 0.2498333901166916, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.598479747772217 EntMin 0.0
Epoch 3, Class Loss=0.3071991801261902, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.3071991801261902, Class Loss=0.3071991801261902, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/43, Loss=3.8108625277876853
Loss made of: CE 0.31783923506736755, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.86283016204834 EntMin 0.0
Epoch 4, Batch 20/43, Loss=3.7795154958963395
Loss made of: CE 0.2926407754421234, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.426837682723999 EntMin 0.0
Epoch 4, Batch 30/43, Loss=3.799026882648468
Loss made of: CE 0.3189266324043274, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.445211887359619 EntMin 0.0
Epoch 4, Batch 40/43, Loss=3.7964800760149955
Loss made of: CE 0.2801252007484436, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.249878406524658 EntMin 0.0
Epoch 4, Class Loss=0.2838689982891083, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.2838689982891083, Class Loss=0.2838689982891083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/43, Loss=3.739979679882526
Loss made of: CE 0.24694335460662842, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.641047239303589 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.702061803638935
Loss made of: CE 0.2849142551422119, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7317657470703125 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.806639015674591
Loss made of: CE 0.24305972456932068, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6594343185424805 EntMin 0.0
Epoch 5, Batch 40/43, Loss=3.8470767334103586
Loss made of: CE 0.2649915814399719, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.317666530609131 EntMin 0.0
Epoch 5, Class Loss=0.26719459891319275, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.26719459891319275, Class Loss=0.26719459891319275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/43, Loss=3.709837666153908
Loss made of: CE 0.2820453643798828, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2624411582946777 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.758741517364979
Loss made of: CE 0.24733206629753113, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.464219093322754 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.699606017768383
Loss made of: CE 0.22782489657402039, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.288928508758545 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.5658817142248154
Loss made of: CE 0.28300243616104126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4166364669799805 EntMin 0.0
Epoch 6, Class Loss=0.2515040934085846, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.2515040934085846, Class Loss=0.2515040934085846, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/52, Loss=3.769247391819954
Loss made of: CE 0.24425452947616577, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7043709754943848 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.128810293972492
Loss made of: CE 0.21065960824489594, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6920862197875977 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.975292593240738
Loss made of: CE 0.1922125518321991, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.77646541595459 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.7185271739959718
Loss made of: CE 0.2240770310163498, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.439774513244629 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.873691885173321
Loss made of: CE 0.20487236976623535, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8626575469970703 EntMin 0.0
Epoch 1, Class Loss=0.227571040391922, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.227571040391922, Class Loss=0.227571040391922, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/52, Loss=3.9184701427817346
Loss made of: CE 0.2397153377532959, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.576808452606201 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.938599444925785
Loss made of: CE 0.2623603343963623, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.516091823577881 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.0576222613453865
Loss made of: CE 0.1763942837715149, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3823134899139404 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.9151662960648537
Loss made of: CE 0.23021471500396729, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5055904388427734 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.8560869082808495
Loss made of: CE 0.17519745230674744, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.165770530700684 EntMin 0.0
Epoch 2, Class Loss=0.24675412476062775, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.24675412476062775, Class Loss=0.24675412476062775, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/52, Loss=3.9720169246196746
Loss made of: CE 0.22164976596832275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4895331859588623 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.8845622077584268
Loss made of: CE 0.36189162731170654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.602893829345703 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.8545537695288656
Loss made of: CE 0.23317542672157288, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.554877996444702 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.9625517085194586
Loss made of: CE 0.24168738722801208, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8498854637145996 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.0009484648704525
Loss made of: CE 0.2103690505027771, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.662388324737549 EntMin 0.0
Epoch 3, Class Loss=0.2465333491563797, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.2465333491563797, Class Loss=0.2465333491563797, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/52, Loss=3.9202328085899354
Loss made of: CE 0.21713408827781677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5019092559814453 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.8432126194238663
Loss made of: CE 0.3309924602508545, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.586282253265381 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.8391398772597314
Loss made of: CE 0.26711779832839966, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.66123104095459 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.9308162704110146
Loss made of: CE 0.2376522272825241, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.811972141265869 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.729005794227123
Loss made of: CE 0.24122916162014008, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6521224975585938 EntMin 0.0
Epoch 4, Class Loss=0.25123661756515503, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.25123661756515503, Class Loss=0.25123661756515503, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/52, Loss=3.6560023039579392
Loss made of: CE 0.25671088695526123, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4335689544677734 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.9043479204177856
Loss made of: CE 0.26669618487358093, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.92850399017334 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.6182409331202505
Loss made of: CE 0.24386775493621826, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5972065925598145 EntMin 0.0
Epoch 5, Batch 40/52, Loss=4.0889767616987225
Loss made of: CE 0.23206353187561035, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8390908241271973 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.962507776916027
Loss made of: CE 0.27994173765182495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.075909614562988 EntMin 0.0
Epoch 5, Class Loss=0.2516101896762848, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.2516101896762848, Class Loss=0.2516101896762848, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/52, Loss=3.802008847892284
Loss made of: CE 0.2158403843641281, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.490386724472046 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.7593336150050165
Loss made of: CE 0.25967586040496826, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.823533058166504 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.749231094121933
Loss made of: CE 0.23585832118988037, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.450965404510498 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.9279281169176103
Loss made of: CE 0.375076025724411, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.643010139465332 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.828838515281677
Loss made of: CE 0.20207473635673523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.774837017059326 EntMin 0.0
Epoch 6, Class Loss=0.25858473777770996, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.25858473777770996, Class Loss=0.25858473777770996, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/34, Loss=4.91008947789669
Loss made of: CE 0.3910444378852844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2204999923706055 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.4847240671515465
Loss made of: CE 0.24533657729625702, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.612837314605713 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.734177342057228
Loss made of: CE 0.294647216796875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.007706165313721 EntMin 0.0
Epoch 1, Class Loss=0.35522302985191345, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.35522302985191345, Class Loss=0.35522302985191345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/34, Loss=4.526346786320209
Loss made of: CE 0.2966255843639374, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.248486042022705 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.28951229006052
Loss made of: CE 0.25178882479667664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.889781951904297 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.383169341087341
Loss made of: CE 0.23043987154960632, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.326964378356934 EntMin 0.0
Epoch 2, Class Loss=0.28252625465393066, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.28252625465393066, Class Loss=0.28252625465393066, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/34, Loss=4.147115908563137
Loss made of: CE 0.25038793683052063, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7306265830993652 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.266469007730484
Loss made of: CE 0.2394593358039856, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.81638765335083 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.424359576404095
Loss made of: CE 0.26271483302116394, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.978955030441284 EntMin 0.0
Epoch 3, Class Loss=0.2859991788864136, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.2859991788864136, Class Loss=0.2859991788864136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/34, Loss=4.273536287248135
Loss made of: CE 0.2347659319639206, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.868764877319336 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.266218844056129
Loss made of: CE 0.33912068605422974, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.894468307495117 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.360258305072785
Loss made of: CE 0.38496264815330505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.517262935638428 EntMin 0.0
Epoch 4, Class Loss=0.29264751076698303, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.29264751076698303, Class Loss=0.29264751076698303, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/34, Loss=4.126582832634449
Loss made of: CE 0.23674029111862183, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4861152172088623 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.396207235753536
Loss made of: CE 0.4012889266014099, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.174821853637695 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.063318155705929
Loss made of: CE 0.24065980315208435, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.78898286819458 EntMin 0.0
Epoch 5, Class Loss=0.27430522441864014, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.27430522441864014, Class Loss=0.27430522441864014, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/34, Loss=4.256721338629722
Loss made of: CE 0.3234330713748932, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.010918140411377 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.191024108231067
Loss made of: CE 0.3008778989315033, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.858200550079346 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.285295525193215
Loss made of: CE 0.23479974269866943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3830642700195312 EntMin 0.0
Epoch 6, Class Loss=0.26915356516838074, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.26915356516838074, Class Loss=0.26915356516838074, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.21959188580513, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.930870
Mean Acc: 0.768947
FreqW Acc: 0.876029
Mean IoU: 0.653028
Class IoU:
	class 0: 0.9256647
	class 1: 0.8242373
	class 2: 0.33144927
	class 3: 0.70290834
	class 4: 0.62873334
	class 5: 0.05389682
	class 6: 0.88494766
	class 7: 0.733508
	class 8: 0.79190314
Class Acc:
	class 0: 0.9700191
	class 1: 0.87476134
	class 2: 0.6129168
	class 3: 0.95683044
	class 4: 0.8555479
	class 5: 0.05431852
	class 6: 0.92219615
	class 7: 0.8290136
	class 8: 0.8449209

federated global round: 9, step: 1
select part of clients to conduct local training
[5, 0, 9, 2]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/34, Loss=5.018346604704857
Loss made of: CE 0.403827428817749, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.234336853027344 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.918958839774132
Loss made of: CE 0.40010809898376465, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.43132209777832 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.8512783035635945
Loss made of: CE 0.5373671650886536, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.357100486755371 EntMin 0.0
Epoch 1, Class Loss=0.41838258504867554, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.41838258504867554, Class Loss=0.41838258504867554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/34, Loss=4.624191531538964
Loss made of: CE 0.30402904748916626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.075014114379883 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.244231343269348
Loss made of: CE 0.3125211000442505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.092179298400879 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.612549088895321
Loss made of: CE 0.45783698558807373, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.101759433746338 EntMin 0.0
Epoch 2, Class Loss=0.31460461020469666, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.31460461020469666, Class Loss=0.31460461020469666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/34, Loss=4.123185946047306
Loss made of: CE 0.25840890407562256, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9761648178100586 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.21453747600317
Loss made of: CE 0.3111130893230438, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.848401069641113 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.527362330257892
Loss made of: CE 0.44094014167785645, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0737996101379395 EntMin 0.0
Epoch 3, Class Loss=0.2817028760910034, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.2817028760910034, Class Loss=0.2817028760910034, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/34, Loss=4.268781846761703
Loss made of: CE 0.2618653178215027, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.94452166557312 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.423494319617748
Loss made of: CE 0.33279794454574585, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.097328186035156 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.209701350331306
Loss made of: CE 0.26313361525535583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4565377235412598 EntMin 0.0
Epoch 4, Class Loss=0.2860802710056305, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.2860802710056305, Class Loss=0.2860802710056305, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/34, Loss=4.20163891762495
Loss made of: CE 0.24879586696624756, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7796058654785156 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.31101136058569
Loss made of: CE 0.26142770051956177, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.00904655456543 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.253221744298935
Loss made of: CE 0.30890876054763794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478289604187012 EntMin 0.0
Epoch 5, Class Loss=0.2753414213657379, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.2753414213657379, Class Loss=0.2753414213657379, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/34, Loss=4.483030821382999
Loss made of: CE 0.28063368797302246, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7502055168151855 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.129320986568928
Loss made of: CE 0.23246443271636963, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.040614128112793 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.134466661512851
Loss made of: CE 0.34014639258384705, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4986772537231445 EntMin 0.0
Epoch 6, Class Loss=0.2747713029384613, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.2747713029384613, Class Loss=0.2747713029384613, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/52, Loss=4.0914844393730165
Loss made of: CE 0.4453539252281189, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4061098098754883 EntMin 0.0
Epoch 1, Batch 20/52, Loss=3.5538440957665443
Loss made of: CE 0.29184576869010925, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1495208740234375 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.7896824911236764
Loss made of: CE 0.3404528796672821, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3365025520324707 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.785859651863575
Loss made of: CE 0.3303718566894531, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.711552143096924 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.8267363578081133
Loss made of: CE 0.23542039096355438, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.169553518295288 EntMin 0.0
Epoch 1, Class Loss=0.30933070182800293, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.30933070182800293, Class Loss=0.30933070182800293, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/52, Loss=3.8420103669166563
Loss made of: CE 0.33938083052635193, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.368237257003784 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.6758206978440287
Loss made of: CE 0.2745039463043213, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1108977794647217 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.9149351164698603
Loss made of: CE 0.37402623891830444, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5841987133026123 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.785669331252575
Loss made of: CE 0.25071483850479126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2151448726654053 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.78223185390234
Loss made of: CE 0.3651720881462097, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9819886684417725 EntMin 0.0
Epoch 2, Class Loss=0.27948155999183655, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.27948155999183655, Class Loss=0.27948155999183655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/52, Loss=3.6652142614126206
Loss made of: CE 0.21967193484306335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3289601802825928 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.825548940896988
Loss made of: CE 0.25553297996520996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.514535903930664 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.838699994981289
Loss made of: CE 0.326615571975708, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.202573537826538 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.8014578461647033
Loss made of: CE 0.27206191420555115, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5004501342773438 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.7543579548597337
Loss made of: CE 0.2831815481185913, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.372654676437378 EntMin 0.0
Epoch 3, Class Loss=0.2846390902996063, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.2846390902996063, Class Loss=0.2846390902996063, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/52, Loss=3.9213630825281145
Loss made of: CE 0.3272229731082916, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4266529083251953 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.615850454568863
Loss made of: CE 0.1975845992565155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.266510486602783 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.681484007835388
Loss made of: CE 0.18123172223567963, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1647815704345703 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.671514257788658
Loss made of: CE 0.22905302047729492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1903419494628906 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.514388954639435
Loss made of: CE 0.24572119116783142, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.229635000228882 EntMin 0.0
Epoch 4, Class Loss=0.25938159227371216, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.25938159227371216, Class Loss=0.25938159227371216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/52, Loss=3.6738698050379752
Loss made of: CE 0.22534260153770447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2954587936401367 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.6072454705834387
Loss made of: CE 0.2126328945159912, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3187170028686523 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.5693491026759148
Loss made of: CE 0.26074180006980896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3252086639404297 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.5814228385686873
Loss made of: CE 0.27089494466781616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5605173110961914 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.5782080605626105
Loss made of: CE 0.2605297565460205, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1987404823303223 EntMin 0.0
Epoch 5, Class Loss=0.2561233341693878, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.2561233341693878, Class Loss=0.2561233341693878, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/52, Loss=3.5039343506097795
Loss made of: CE 0.24212169647216797, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.106574058532715 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.614475394785404
Loss made of: CE 0.21387074887752533, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0889079570770264 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.8171452224254607
Loss made of: CE 0.3020988404750824, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3515775203704834 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.394503317773342
Loss made of: CE 0.25562116503715515, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1184282302856445 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.577518865466118
Loss made of: CE 0.280617892742157, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4520883560180664 EntMin 0.0
Epoch 6, Class Loss=0.25579923391342163, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.25579923391342163, Class Loss=0.25579923391342163, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/52, Loss=3.8728049248456955
Loss made of: CE 0.34861689805984497, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.420555830001831 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.279862427711487
Loss made of: CE 0.35186439752578735, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8900365829467773 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.9867197573184967
Loss made of: CE 0.3209630250930786, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7351949214935303 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.6695639818906782
Loss made of: CE 0.3127579689025879, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2629783153533936 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.8112716287374497
Loss made of: CE 0.28168368339538574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.924588680267334 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.315719872713089, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.315719872713089, Class Loss=0.315719872713089, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/52, Loss=3.8675425231456755
Loss made of: CE 0.32027873396873474, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.548764228820801 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.8758046209812163
Loss made of: CE 0.2978912591934204, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4621028900146484 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.040407571196556
Loss made of: CE 0.2854123115539551, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.433506727218628 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.8331941649317742
Loss made of: CE 0.2517359256744385, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.338441848754883 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.797468386590481
Loss made of: CE 0.23113664984703064, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9093127250671387 EntMin 0.0
Epoch 2, Class Loss=0.2999960482120514, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.2999960482120514, Class Loss=0.2999960482120514, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/52, Loss=3.840247705578804
Loss made of: CE 0.29681700468063354, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5350539684295654 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.7921585515141487
Loss made of: CE 0.4238503873348236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.475962162017822 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.7055980443954466
Loss made of: CE 0.30025696754455566, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.419278860092163 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.8727864220738413
Loss made of: CE 0.2712918221950531, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7486538887023926 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.924751581251621
Loss made of: CE 0.23213033378124237, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6161794662475586 EntMin 0.0
Epoch 3, Class Loss=0.28470560908317566, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.28470560908317566, Class Loss=0.28470560908317566, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/52, Loss=3.7677926421165466
Loss made of: CE 0.29437023401260376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.461449146270752 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.736468490958214
Loss made of: CE 0.3169490098953247, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6049726009368896 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.723963312804699
Loss made of: CE 0.29622602462768555, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.405703544616699 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.8090698033571244
Loss made of: CE 0.2561890482902527, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.442323684692383 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.575853334367275
Loss made of: CE 0.2036178708076477, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.616602659225464 EntMin 0.0
Epoch 4, Class Loss=0.27307868003845215, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.27307868003845215, Class Loss=0.27307868003845215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/52, Loss=3.5425027698278426
Loss made of: CE 0.26676344871520996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.302743911743164 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.7490762755274774
Loss made of: CE 0.2364259660243988, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5342767238616943 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.4555978268384933
Loss made of: CE 0.29372188448905945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.182136058807373 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.938953918218613
Loss made of: CE 0.2409558743238449, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6695401668548584 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.8727102667093276
Loss made of: CE 0.3265833258628845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.271969318389893 EntMin 0.0
Epoch 5, Class Loss=0.26604387164115906, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.26604387164115906, Class Loss=0.26604387164115906, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/52, Loss=3.6478432327508927
Loss made of: CE 0.18192355334758759, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3765788078308105 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.7108472123742104
Loss made of: CE 0.22333964705467224, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9832799434661865 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.62650503218174
Loss made of: CE 0.2021259069442749, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.319772243499756 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.8468921005725862
Loss made of: CE 0.38879263401031494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.802567481994629 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.70874720364809
Loss made of: CE 0.19860467314720154, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7914962768554688 EntMin 0.0
Epoch 6, Class Loss=0.26530057191848755, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.26530057191848755, Class Loss=0.26530057191848755, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=4.787009432911873
Loss made of: CE 0.3179178237915039, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.230264186859131 EntMin 0.0
Epoch 1, Batch 20/25, Loss=4.424735799431801
Loss made of: CE 0.34634900093078613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.408252716064453 EntMin 0.0
Epoch 1, Class Loss=0.28522124886512756, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.28522124886512756, Class Loss=0.28522124886512756, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/25, Loss=4.278606897592544
Loss made of: CE 0.23146399855613708, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.688666343688965 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.2446048393845555
Loss made of: CE 0.18372642993927002, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.054784297943115 EntMin 0.0
Epoch 2, Class Loss=0.25306886434555054, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.25306886434555054, Class Loss=0.25306886434555054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/25, Loss=4.116451236605644
Loss made of: CE 0.20740237832069397, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9816060066223145 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.32633421421051
Loss made of: CE 0.20967911183834076, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7148427963256836 EntMin 0.0
Epoch 3, Class Loss=0.24601520597934723, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.24601520597934723, Class Loss=0.24601520597934723, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/25, Loss=4.163628178834915
Loss made of: CE 0.2243114709854126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5358827114105225 EntMin 0.0
Epoch 4, Batch 20/25, Loss=4.069789756834507
Loss made of: CE 0.1932644546031952, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.387558937072754 EntMin 0.0
Epoch 4, Class Loss=0.23617608845233917, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.23617608845233917, Class Loss=0.23617608845233917, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/25, Loss=3.9947667747735975
Loss made of: CE 0.1787281334400177, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4946627616882324 EntMin 0.0
Epoch 5, Batch 20/25, Loss=4.024647283554077
Loss made of: CE 0.2863716185092926, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.573647975921631 EntMin 0.0
Epoch 5, Class Loss=0.24107219278812408, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.24107219278812408, Class Loss=0.24107219278812408, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/25, Loss=3.9478550091385842
Loss made of: CE 0.3288875222206116, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.657742500305176 EntMin 0.0
Epoch 6, Batch 20/25, Loss=3.9938954919576646
Loss made of: CE 0.25011569261550903, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.711120128631592 EntMin 0.0
Epoch 6, Class Loss=0.25116801261901855, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.25116801261901855, Class Loss=0.25116801261901855, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.21037578582763672, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.933449
Mean Acc: 0.781234
FreqW Acc: 0.880617
Mean IoU: 0.665680
Class IoU:
	class 0: 0.92807746
	class 1: 0.8297785
	class 2: 0.32730317
	class 3: 0.7135831
	class 4: 0.6494581
	class 5: 0.11571544
	class 6: 0.88889474
	class 7: 0.7346772
	class 8: 0.8036347
Class Acc:
	class 0: 0.9677803
	class 1: 0.8852939
	class 2: 0.58362424
	class 3: 0.9469803
	class 4: 0.865105
	class 5: 0.117003165
	class 6: 0.92617637
	class 7: 0.8628583
	class 8: 0.8762832

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[0, 15, 1, 4]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=7.441583561897278
Loss made of: CE 0.570807695388794, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.698881149291992 EntMin 0.0
Epoch 1, Class Loss=0.6634573340415955, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.6634573340415955, Class Loss=0.6634573340415955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.013056004047394
Loss made of: CE 0.7443242073059082, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.237018585205078 EntMin 0.0
Epoch 2, Class Loss=0.7332018613815308, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.7332018613815308, Class Loss=0.7332018613815308, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.602279853820801
Loss made of: CE 0.756245493888855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5903449058532715 EntMin 0.0
Epoch 3, Class Loss=0.7668576836585999, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.7668576836585999, Class Loss=0.7668576836585999, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.704655903577804
Loss made of: CE 0.8204299807548523, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3998918533325195 EntMin 0.0
Epoch 4, Class Loss=0.8007635474205017, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.8007635474205017, Class Loss=0.8007635474205017, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.281064611673355
Loss made of: CE 0.8307831287384033, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6388020515441895 EntMin 0.0
Epoch 5, Class Loss=0.7658832669258118, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.7658832669258118, Class Loss=0.7658832669258118, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.3054361820220945
Loss made of: CE 0.7209250330924988, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.329847812652588 EntMin 0.0
Epoch 6, Class Loss=0.7291591763496399, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.7291591763496399, Class Loss=0.7291591763496399, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=7.373405086994171
Loss made of: CE 0.6265459060668945, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.560173988342285 EntMin 0.0
Epoch 1, Class Loss=0.6898269057273865, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.6898269057273865, Class Loss=0.6898269057273865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.187699669599533
Loss made of: CE 0.717841625213623, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.610786437988281 EntMin 0.0
Epoch 2, Class Loss=0.7290632724761963, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.7290632724761963, Class Loss=0.7290632724761963, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.702538847923279
Loss made of: CE 0.9231623411178589, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.842646598815918 EntMin 0.0
Epoch 3, Class Loss=0.7786680459976196, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.7786680459976196, Class Loss=0.7786680459976196, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.577079278230667
Loss made of: CE 0.8933728933334351, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8409647941589355 EntMin 0.0
Epoch 4, Class Loss=0.7821491956710815, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.7821491956710815, Class Loss=0.7821491956710815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.359592622518539
Loss made of: CE 0.6446553468704224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.659450531005859 EntMin 0.0
Epoch 5, Class Loss=0.7791610360145569, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.7791610360145569, Class Loss=0.7791610360145569, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.241106677055359
Loss made of: CE 0.7057957649230957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.045905590057373 EntMin 0.0
Epoch 6, Class Loss=0.7526174783706665, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.7526174783706665, Class Loss=0.7526174783706665, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=8.711085867881774
Loss made of: CE 0.5463541746139526, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.794512748718262 EntMin 0.0
Epoch 1, Batch 20/35, Loss=7.1392260730266575
Loss made of: CE 0.49903494119644165, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.076837539672852 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.653128704428672
Loss made of: CE 0.39654573798179626, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.223628520965576 EntMin 0.0
Epoch 1, Class Loss=0.5425804257392883, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.5425804257392883, Class Loss=0.5425804257392883, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/35, Loss=6.336973381042481
Loss made of: CE 0.5663987398147583, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.730876922607422 EntMin 0.0
Epoch 2, Batch 20/35, Loss=6.446399450302124
Loss made of: CE 0.7473706007003784, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.99077033996582 EntMin 0.0
Epoch 2, Batch 30/35, Loss=6.00001779794693
Loss made of: CE 0.3254801630973816, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.769084453582764 EntMin 0.0
Epoch 2, Class Loss=0.5485423803329468, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.5485423803329468, Class Loss=0.5485423803329468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/35, Loss=5.847638964653015
Loss made of: CE 0.501948356628418, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.049603462219238 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.967836400866508
Loss made of: CE 0.44626396894454956, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.422452449798584 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.699363470077515
Loss made of: CE 0.43865740299224854, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.385341644287109 EntMin 0.0
Epoch 3, Class Loss=0.5162986516952515, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.5162986516952515, Class Loss=0.5162986516952515, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/35, Loss=5.679783234000206
Loss made of: CE 0.45744967460632324, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.133728981018066 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.453887364268303
Loss made of: CE 0.46173375844955444, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.104487895965576 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.650039923191071
Loss made of: CE 0.4732414484024048, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1633453369140625 EntMin 0.0
Epoch 4, Class Loss=0.48190104961395264, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.48190104961395264, Class Loss=0.48190104961395264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/35, Loss=5.550642684102058
Loss made of: CE 0.3912546634674072, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.729503154754639 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.65207672715187
Loss made of: CE 0.5082509517669678, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.22855281829834 EntMin 0.0
Epoch 5, Batch 30/35, Loss=5.141551184654236
Loss made of: CE 0.33110612630844116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.716912269592285 EntMin 0.0
Epoch 5, Class Loss=0.44521841406822205, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.44521841406822205, Class Loss=0.44521841406822205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/35, Loss=5.249762624502182
Loss made of: CE 0.34100431203842163, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.735060691833496 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.414238151907921
Loss made of: CE 0.36459586024284363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.436313152313232 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.536364653706551
Loss made of: CE 0.5358767509460449, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464914321899414 EntMin 0.0
Epoch 6, Class Loss=0.44210007786750793, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.44210007786750793, Class Loss=0.44210007786750793, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=7.3998908162117
Loss made of: CE 0.7020058035850525, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.313657760620117 EntMin 0.0
Epoch 1, Class Loss=0.669101893901825, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.669101893901825, Class Loss=0.669101893901825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.248696655035019
Loss made of: CE 0.6350617408752441, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.968939781188965 EntMin 0.0
Epoch 2, Class Loss=0.7193224430084229, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.7193224430084229, Class Loss=0.7193224430084229, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.828291928768158
Loss made of: CE 0.7560948133468628, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.412001609802246 EntMin 0.0
Epoch 3, Class Loss=0.789631724357605, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.789631724357605, Class Loss=0.789631724357605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.536255478858948
Loss made of: CE 0.8399430513381958, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.546238899230957 EntMin 0.0
Epoch 4, Class Loss=0.7990188598632812, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.7990188598632812, Class Loss=0.7990188598632812, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.3287213742733
Loss made of: CE 0.7492343783378601, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.255084037780762 EntMin 0.0
Epoch 5, Class Loss=0.7605118155479431, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.7605118155479431, Class Loss=0.7605118155479431, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.347179859876633
Loss made of: CE 0.6705192923545837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4214067459106445 EntMin 0.0
Epoch 6, Class Loss=0.7484409213066101, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.7484409213066101, Class Loss=0.7484409213066101, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5569255352020264, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.861724
Mean Acc: 0.515568
FreqW Acc: 0.755162
Mean IoU: 0.412079
Class IoU:
	class 0: 0.8681604
	class 1: 0.7512135
	class 2: 0.29317167
	class 3: 0.4625926
	class 4: 0.5885103
	class 5: 0.17917061
	class 6: 0.8538731
	class 7: 0.73495543
	class 8: 0.6253795
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.97903675
	class 1: 0.77119076
	class 2: 0.4668167
	class 3: 0.9360545
	class 4: 0.83454657
	class 5: 0.18199453
	class 6: 0.8931203
	class 7: 0.79303145
	class 8: 0.84659
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[1, 13, 11, 12]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/35, Loss=7.479334861040115
Loss made of: CE 1.1619210243225098, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.665477275848389 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.51005916595459
Loss made of: CE 0.7832698822021484, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.211398601531982 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.218635541200638
Loss made of: CE 0.7931656837463379, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.602473258972168 EntMin 0.0
Epoch 1, Class Loss=0.900133490562439, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.900133490562439, Class Loss=0.900133490562439, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/35, Loss=5.890722155570984
Loss made of: CE 0.5490602850914001, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.243983745574951 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.990496242046357
Loss made of: CE 0.6589179635047913, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.752849102020264 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.63002202808857
Loss made of: CE 0.42463892698287964, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1728105545043945 EntMin 0.0
Epoch 2, Class Loss=0.5726760625839233, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.5726760625839233, Class Loss=0.5726760625839233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/35, Loss=5.47702165544033
Loss made of: CE 0.519274890422821, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.86443567276001 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.687216892838478
Loss made of: CE 0.39818474650382996, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.26484489440918 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.522047907114029
Loss made of: CE 0.4268887937068939, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.334460735321045 EntMin 0.0
Epoch 3, Class Loss=0.46610915660858154, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.46610915660858154, Class Loss=0.46610915660858154, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/35, Loss=5.408270412683487
Loss made of: CE 0.3804803490638733, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.039202690124512 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.156667560338974
Loss made of: CE 0.38297614455223083, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.580663681030273 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.346075123548507
Loss made of: CE 0.40356048941612244, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.022838115692139 EntMin 0.0
Epoch 4, Class Loss=0.4198889434337616, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.4198889434337616, Class Loss=0.4198889434337616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/35, Loss=5.2371559798717495
Loss made of: CE 0.37805765867233276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.601367950439453 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.430973663926125
Loss made of: CE 0.48731404542922974, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.072114944458008 EntMin 0.0
Epoch 5, Batch 30/35, Loss=5.0243162482976915
Loss made of: CE 0.28136324882507324, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.602971076965332 EntMin 0.0
Epoch 5, Class Loss=0.3830350637435913, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.3830350637435913, Class Loss=0.3830350637435913, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/35, Loss=5.007974922657013
Loss made of: CE 0.2931950092315674, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.844786643981934 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.216736900806427
Loss made of: CE 0.32665905356407166, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.528168678283691 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.343160727620125
Loss made of: CE 0.43800637125968933, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.472451210021973 EntMin 0.0
Epoch 6, Class Loss=0.3701796531677246, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.3701796531677246, Class Loss=0.3701796531677246, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.74973087310791
Loss made of: CE 0.378429114818573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5602312088012695 EntMin 0.0
Epoch 1, Class Loss=0.37485554814338684, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.37485554814338684, Class Loss=0.37485554814338684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/19, Loss=5.0399639993906025
Loss made of: CE 0.49569281935691833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.874337196350098 EntMin 0.0
Epoch 2, Class Loss=0.5012036561965942, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.5012036561965942, Class Loss=0.5012036561965942, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/19, Loss=4.886750200390816
Loss made of: CE 0.5359089374542236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.316513538360596 EntMin 0.0
Epoch 3, Class Loss=0.5500515699386597, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.5500515699386597, Class Loss=0.5500515699386597, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/19, Loss=4.986383634805679
Loss made of: CE 0.6888031959533691, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304762363433838 EntMin 0.0
Epoch 4, Class Loss=0.5803857445716858, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5803857445716858, Class Loss=0.5803857445716858, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/19, Loss=4.988599747419357
Loss made of: CE 0.4929787516593933, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7786788940429688 EntMin 0.0
Epoch 5, Class Loss=0.5659316182136536, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5659316182136536, Class Loss=0.5659316182136536, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/19, Loss=4.853998494148255
Loss made of: CE 0.5738428235054016, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332950592041016 EntMin 0.0
Epoch 6, Class Loss=0.5625157356262207, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5625157356262207, Class Loss=0.5625157356262207, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=5.046605780720711
Loss made of: CE 0.4760189950466156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7837724685668945 EntMin 0.0
Epoch 1, Batch 20/33, Loss=4.965098252892494
Loss made of: CE 0.3346949815750122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.833206653594971 EntMin 0.0
Epoch 1, Batch 30/33, Loss=4.911592695116997
Loss made of: CE 0.2386060655117035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.462714195251465 EntMin 0.0
Epoch 1, Class Loss=0.36326488852500916, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.36326488852500916, Class Loss=0.36326488852500916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=5.122607499361038
Loss made of: CE 0.5387183427810669, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1297430992126465 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.765944829583168
Loss made of: CE 0.39705610275268555, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.960660696029663 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.770805060863495
Loss made of: CE 0.3908050060272217, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.863720893859863 EntMin 0.0
Epoch 2, Class Loss=0.4554365873336792, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.4554365873336792, Class Loss=0.4554365873336792, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.891409656405449
Loss made of: CE 0.4470452666282654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.281764984130859 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.986517605185509
Loss made of: CE 0.5846996307373047, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.619019985198975 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.76928023993969
Loss made of: CE 0.6147825717926025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.738990306854248 EntMin 0.0
Epoch 3, Class Loss=0.5198179483413696, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.5198179483413696, Class Loss=0.5198179483413696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.758829471468926
Loss made of: CE 0.5165718793869019, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.083805084228516 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.901793047785759
Loss made of: CE 0.45175549387931824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.146932125091553 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.7331960797309875
Loss made of: CE 0.5310692191123962, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.817931890487671 EntMin 0.0
Epoch 4, Class Loss=0.5359016060829163, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5359016060829163, Class Loss=0.5359016060829163, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.740532711148262
Loss made of: CE 0.4858091473579407, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.915156841278076 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.94637850522995
Loss made of: CE 0.47947463393211365, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9528920650482178 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.665594899654389
Loss made of: CE 0.5966611504554749, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0823211669921875 EntMin 0.0
Epoch 5, Class Loss=0.5428339242935181, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5428339242935181, Class Loss=0.5428339242935181, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.622825875878334
Loss made of: CE 0.5828606486320496, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.046778678894043 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.695731607079506
Loss made of: CE 0.45286545157432556, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.436058759689331 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.6269484728574755
Loss made of: CE 0.48101723194122314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1802496910095215 EntMin 0.0
Epoch 6, Class Loss=0.5489062070846558, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5489062070846558, Class Loss=0.5489062070846558, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=5.123588168621064
Loss made of: CE 0.448820561170578, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.298748970031738 EntMin 0.0
Epoch 1, Batch 20/33, Loss=4.8424065798521045
Loss made of: CE 0.43311116099357605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.479135036468506 EntMin 0.0
Epoch 1, Batch 30/33, Loss=4.729248769581318
Loss made of: CE 0.2583601474761963, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6169538497924805 EntMin 0.0
Epoch 1, Class Loss=0.35890892148017883, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.35890892148017883, Class Loss=0.35890892148017883, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=4.840553483366966
Loss made of: CE 0.3315197825431824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.442542552947998 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.998402491211891
Loss made of: CE 0.47178447246551514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.51214599609375 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.740587347745896
Loss made of: CE 0.4694162607192993, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.896074056625366 EntMin 0.0
Epoch 2, Class Loss=0.4598175883293152, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.4598175883293152, Class Loss=0.4598175883293152, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.92009556889534
Loss made of: CE 0.6641907691955566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.100720405578613 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.808570754528046
Loss made of: CE 0.5211814641952515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2625532150268555 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.6762060701847075
Loss made of: CE 0.5273370742797852, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9611687660217285 EntMin 0.0
Epoch 3, Class Loss=0.522005021572113, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.522005021572113, Class Loss=0.522005021572113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.8040672719478605
Loss made of: CE 0.48486584424972534, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.183100700378418 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.6868678569793705
Loss made of: CE 0.5875714421272278, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.038374900817871 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.617225840687752
Loss made of: CE 0.5695271492004395, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.754236698150635 EntMin 0.0
Epoch 4, Class Loss=0.5312972068786621, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.5312972068786621, Class Loss=0.5312972068786621, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.711704641580582
Loss made of: CE 0.5207092761993408, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.688305377960205 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.521338614821434
Loss made of: CE 0.6552129983901978, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.822603702545166 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.610296079516411
Loss made of: CE 0.6351951956748962, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.033768177032471 EntMin 0.0
Epoch 5, Class Loss=0.5328502655029297, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5328502655029297, Class Loss=0.5328502655029297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.585744535923004
Loss made of: CE 0.6243194937705994, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.219877243041992 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.621803253889084
Loss made of: CE 0.5162842273712158, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.751415729522705 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.806248894333839
Loss made of: CE 0.48628315329551697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6837387084960938 EntMin 0.0
Epoch 6, Class Loss=0.5504410266876221, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.5504410266876221, Class Loss=0.5504410266876221, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.44845932722091675, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.874191
Mean Acc: 0.581754
FreqW Acc: 0.779739
Mean IoU: 0.457374
Class IoU:
	class 0: 0.885642
	class 1: 0.73420024
	class 2: 0.34094435
	class 3: 0.4452539
	class 4: 0.5971809
	class 5: 0.15653996
	class 6: 0.80714923
	class 7: 0.73936117
	class 8: 0.6508521
	class 9: 0.0
	class 10: 0.5887367
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.9759167
	class 1: 0.7523214
	class 2: 0.6433927
	class 3: 0.9495865
	class 4: 0.84093994
	class 5: 0.15791401
	class 6: 0.82411885
	class 7: 0.80031973
	class 8: 0.85692465
	class 9: 0.0
	class 10: 0.7613662
	class 11: 0.0
	class 12: 0.0

federated global round: 12, step: 2
select part of clients to conduct local training
[0, 16, 7, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.8780934572219845
Loss made of: CE 0.7117206454277039, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.703961372375488 EntMin 0.0
Epoch 1, Class Loss=0.6432815194129944, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.6432815194129944, Class Loss=0.6432815194129944, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.735904055833816
Loss made of: CE 0.623770534992218, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.235766887664795 EntMin 0.0
Epoch 2, Class Loss=0.5856748819351196, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.5856748819351196, Class Loss=0.5856748819351196, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.633382028341293
Loss made of: CE 0.5836197733879089, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6832809448242188 EntMin 0.0
Epoch 3, Class Loss=0.5514542460441589, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.5514542460441589, Class Loss=0.5514542460441589, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.666919231414795
Loss made of: CE 0.7010999917984009, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.795646667480469 EntMin 0.0
Epoch 4, Class Loss=0.5401163101196289, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.5401163101196289, Class Loss=0.5401163101196289, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.487436094880104
Loss made of: CE 0.513037919998169, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.093387126922607 EntMin 0.0
Epoch 5, Class Loss=0.4907524883747101, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.4907524883747101, Class Loss=0.4907524883747101, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.601552578806877
Loss made of: CE 0.5056926608085632, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7408626079559326 EntMin 0.0
Epoch 6, Class Loss=0.4912308156490326, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.4912308156490326, Class Loss=0.4912308156490326, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/42, Loss=5.342753249406814
Loss made of: CE 0.3578764796257019, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.142716407775879 EntMin 0.0
Epoch 1, Batch 20/42, Loss=5.0713807314634325
Loss made of: CE 0.27045556902885437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.863651275634766 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.242146891355515
Loss made of: CE 0.3892791271209717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7045698165893555 EntMin 0.0
Epoch 1, Batch 40/42, Loss=4.936689023673535
Loss made of: CE 0.30406564474105835, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2813944816589355 EntMin 0.0
Epoch 1, Class Loss=0.3596850037574768, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.3596850037574768, Class Loss=0.3596850037574768, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/42, Loss=5.0763437986373905
Loss made of: CE 0.26602286100387573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3758673667907715 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.884597510099411
Loss made of: CE 0.307829886674881, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.87877893447876 EntMin 0.0
Epoch 2, Batch 30/42, Loss=4.791048228740692
Loss made of: CE 0.27659639716148376, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.727328300476074 EntMin 0.0
Epoch 2, Batch 40/42, Loss=4.789483854174614
Loss made of: CE 0.3944091498851776, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.947673797607422 EntMin 0.0
Epoch 2, Class Loss=0.3435252010822296, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.3435252010822296, Class Loss=0.3435252010822296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/42, Loss=4.749588340520859
Loss made of: CE 0.40349656343460083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.080682277679443 EntMin 0.0
Epoch 3, Batch 20/42, Loss=5.109193035960198
Loss made of: CE 0.2822890877723694, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.89598274230957 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.73082409799099
Loss made of: CE 0.2273242175579071, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.606665134429932 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.689987459778786
Loss made of: CE 0.21795690059661865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.799392223358154 EntMin 0.0
Epoch 3, Class Loss=0.3496760129928589, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.3496760129928589, Class Loss=0.3496760129928589, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/42, Loss=4.922808986902237
Loss made of: CE 0.3718501031398773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4479241371154785 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.740429550409317
Loss made of: CE 0.40408191084861755, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.397059440612793 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.8061250269413
Loss made of: CE 0.36190611124038696, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.146677494049072 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.8506234139204025
Loss made of: CE 0.36209389567375183, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.314330577850342 EntMin 0.0
Epoch 4, Class Loss=0.38327857851982117, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.38327857851982117, Class Loss=0.38327857851982117, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/42, Loss=4.54661306142807
Loss made of: CE 0.35079845786094666, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9509449005126953 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.743963178992272
Loss made of: CE 0.4005318582057953, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.232892990112305 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.832443702220917
Loss made of: CE 0.4633769392967224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645978927612305 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.746747136116028
Loss made of: CE 0.3870989978313446, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.060449600219727 EntMin 0.0
Epoch 5, Class Loss=0.37537911534309387, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.37537911534309387, Class Loss=0.37537911534309387, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/42, Loss=4.7903873205184935
Loss made of: CE 0.35870689153671265, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.001728534698486 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.604609337449074
Loss made of: CE 0.4182586371898651, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.079471588134766 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.794456577301025
Loss made of: CE 0.4683955907821655, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.330796718597412 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.797493794560433
Loss made of: CE 0.40344905853271484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.700168609619141 EntMin 0.0
Epoch 6, Class Loss=0.39442184567451477, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.39442184567451477, Class Loss=0.39442184567451477, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/31, Loss=5.412215112149715
Loss made of: CE 0.3708842694759369, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.617087364196777 EntMin 0.0
Epoch 1, Batch 20/31, Loss=5.101029008626938
Loss made of: CE 0.32660114765167236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.96690559387207 EntMin 0.0
Epoch 1, Batch 30/31, Loss=5.069780090451241
Loss made of: CE 0.35520467162132263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.447978973388672 EntMin 0.0
Epoch 1, Class Loss=0.343735009431839, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.343735009431839, Class Loss=0.343735009431839, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/31, Loss=4.885254454612732
Loss made of: CE 0.4598156809806824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.327542304992676 EntMin 0.0
Epoch 2, Batch 20/31, Loss=4.665548226237297
Loss made of: CE 0.44075220823287964, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.250814914703369 EntMin 0.0
Epoch 2, Batch 30/31, Loss=5.072640585899353
Loss made of: CE 0.3275161385536194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.04658317565918 EntMin 0.0
Epoch 2, Class Loss=0.41002386808395386, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.41002386808395386, Class Loss=0.41002386808395386, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/31, Loss=4.8438844829797745
Loss made of: CE 0.4806945323944092, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9448089599609375 EntMin 0.0
Epoch 3, Batch 20/31, Loss=4.701464274525643
Loss made of: CE 0.47648924589157104, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8537745475769043 EntMin 0.0
Epoch 3, Batch 30/31, Loss=4.725379911065102
Loss made of: CE 0.2979382276535034, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.774707794189453 EntMin 0.0
Epoch 3, Class Loss=0.452844500541687, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.452844500541687, Class Loss=0.452844500541687, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/31, Loss=4.606577378511429
Loss made of: CE 0.6369643211364746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.62376594543457 EntMin 0.0
Epoch 4, Batch 20/31, Loss=4.715437182784081
Loss made of: CE 0.4763501286506653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.02421760559082 EntMin 0.0
Epoch 4, Batch 30/31, Loss=4.735337802767754
Loss made of: CE 0.6193704605102539, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.189594268798828 EntMin 0.0
Epoch 4, Class Loss=0.4849976599216461, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4849976599216461, Class Loss=0.4849976599216461, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/31, Loss=4.651766937971115
Loss made of: CE 0.502541184425354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3093767166137695 EntMin 0.0
Epoch 5, Batch 20/31, Loss=4.708814382553101
Loss made of: CE 0.48354649543762207, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8932552337646484 EntMin 0.0
Epoch 5, Batch 30/31, Loss=4.911049383878708
Loss made of: CE 0.37380480766296387, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9344239234924316 EntMin 0.0
Epoch 5, Class Loss=0.4980488419532776, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4980488419532776, Class Loss=0.4980488419532776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/31, Loss=4.59561884701252
Loss made of: CE 0.5888410806655884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.045797348022461 EntMin 0.0
Epoch 6, Batch 20/31, Loss=4.706047016382217
Loss made of: CE 0.4753803610801697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9301652908325195 EntMin 0.0
Epoch 6, Batch 30/31, Loss=4.566629081964493
Loss made of: CE 0.5574984550476074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5619969367980957 EntMin 0.0
Epoch 6, Class Loss=0.4999546408653259, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.4999546408653259, Class Loss=0.4999546408653259, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.961423563957214
Loss made of: CE 0.6540462970733643, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.390166759490967 EntMin 0.0
Epoch 1, Class Loss=0.6656203269958496, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.6656203269958496, Class Loss=0.6656203269958496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.920278173685074
Loss made of: CE 0.521082878112793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.163521766662598 EntMin 0.0
Epoch 2, Class Loss=0.5965482592582703, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.5965482592582703, Class Loss=0.5965482592582703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.759006851911545
Loss made of: CE 0.4646376967430115, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5929677486419678 EntMin 0.0
Epoch 3, Class Loss=0.5503169894218445, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.5503169894218445, Class Loss=0.5503169894218445, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.630312734842301
Loss made of: CE 0.4903533160686493, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.839351177215576 EntMin 0.0
Epoch 4, Class Loss=0.5185683369636536, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5185683369636536, Class Loss=0.5185683369636536, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.569929441809654
Loss made of: CE 0.5090210437774658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9483394622802734 EntMin 0.0
Epoch 5, Class Loss=0.49545684456825256, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.49545684456825256, Class Loss=0.49545684456825256, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.583471363782882
Loss made of: CE 0.4819374978542328, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8928706645965576 EntMin 0.0
Epoch 6, Class Loss=0.49586114287376404, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.49586114287376404, Class Loss=0.49586114287376404, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.41453802585601807, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.873491
Mean Acc: 0.601323
FreqW Acc: 0.784089
Mean IoU: 0.463963
Class IoU:
	class 0: 0.8910914
	class 1: 0.7264126
	class 2: 0.35006472
	class 3: 0.42059803
	class 4: 0.54386806
	class 5: 0.16707692
	class 6: 0.7792283
	class 7: 0.73250127
	class 8: 0.6184052
	class 9: 0.00016490054
	class 10: 0.49097297
	class 11: 0.30989948
	class 12: 0.0012403742
Class Acc:
	class 0: 0.97092277
	class 1: 0.74619067
	class 2: 0.6745903
	class 3: 0.9562672
	class 4: 0.88081414
	class 5: 0.16904305
	class 6: 0.79237705
	class 7: 0.8173607
	class 8: 0.8739396
	class 9: 0.00016490511
	class 10: 0.5241357
	class 11: 0.4101497
	class 12: 0.0012403905

federated global round: 13, step: 2
select part of clients to conduct local training
[14, 13, 0, 1]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.274023456871509
Loss made of: CE 0.2465389370918274, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7619080543518066 EntMin 0.0
Epoch 1, Class Loss=0.2350115031003952, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.2350115031003952, Class Loss=0.2350115031003952, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=4.333618788421154
Loss made of: CE 0.34746038913726807, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6866652965545654 EntMin 0.0
Epoch 2, Class Loss=0.3095478415489197, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.3095478415489197, Class Loss=0.3095478415489197, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=4.475834983587265
Loss made of: CE 0.3028119206428528, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.274108409881592 EntMin 0.0
Epoch 3, Class Loss=0.3692139983177185, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.3692139983177185, Class Loss=0.3692139983177185, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=4.403341591358185
Loss made of: CE 0.3419743478298187, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.731415271759033 EntMin 0.0
Epoch 4, Class Loss=0.40739795565605164, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.40739795565605164, Class Loss=0.40739795565605164, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=4.346669697761536
Loss made of: CE 0.3607618808746338, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8876452445983887 EntMin 0.0
Epoch 5, Class Loss=0.42359665036201477, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.42359665036201477, Class Loss=0.42359665036201477, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=4.413662475347519
Loss made of: CE 0.36169955134391785, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.806426763534546 EntMin 0.0
Epoch 6, Class Loss=0.4442759156227112, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.4442759156227112, Class Loss=0.4442759156227112, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/19, Loss=4.556716087460518
Loss made of: CE 0.5135620832443237, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.014169216156006 EntMin 0.0
Epoch 1, Class Loss=0.5316370129585266, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5316370129585266, Class Loss=0.5316370129585266, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/19, Loss=4.550118413567543
Loss made of: CE 0.5192966461181641, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.372595310211182 EntMin 0.0
Epoch 2, Class Loss=0.4953278601169586, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.4953278601169586, Class Loss=0.4953278601169586, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/19, Loss=4.46837942302227
Loss made of: CE 0.3830993175506592, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9967715740203857 EntMin 0.0
Epoch 3, Class Loss=0.4704124629497528, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.4704124629497528, Class Loss=0.4704124629497528, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/19, Loss=4.503408366441727
Loss made of: CE 0.5384725332260132, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8321104049682617 EntMin 0.0
Epoch 4, Class Loss=0.469108521938324, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.469108521938324, Class Loss=0.469108521938324, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/19, Loss=4.5382873058319095
Loss made of: CE 0.37427982687950134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.435445785522461 EntMin 0.0
Epoch 5, Class Loss=0.45489802956581116, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.45489802956581116, Class Loss=0.45489802956581116, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/19, Loss=4.398143470287323
Loss made of: CE 0.44697508215904236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.041268348693848 EntMin 0.0
Epoch 6, Class Loss=0.4470333456993103, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.4470333456993103, Class Loss=0.4470333456993103, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.594181329011917
Loss made of: CE 0.507104754447937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.236204147338867 EntMin 0.0
Epoch 1, Class Loss=0.542336106300354, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.542336106300354, Class Loss=0.542336106300354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/19, Loss=4.386089876294136
Loss made of: CE 0.5788509845733643, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035360336303711 EntMin 0.0
Epoch 2, Class Loss=0.49799758195877075, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.49799758195877075, Class Loss=0.49799758195877075, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/19, Loss=4.462315267324447
Loss made of: CE 0.53534334897995, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7359836101531982 EntMin 0.0
Epoch 3, Class Loss=0.47617632150650024, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.47617632150650024, Class Loss=0.47617632150650024, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/19, Loss=4.466030487418175
Loss made of: CE 0.5939545631408691, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.72530460357666 EntMin 0.0
Epoch 4, Class Loss=0.4808643162250519, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.4808643162250519, Class Loss=0.4808643162250519, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/19, Loss=4.2790692538023
Loss made of: CE 0.5160991549491882, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.849419355392456 EntMin 0.0
Epoch 5, Class Loss=0.46211954951286316, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.46211954951286316, Class Loss=0.46211954951286316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/19, Loss=4.423687526583672
Loss made of: CE 0.44564124941825867, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.743025302886963 EntMin 0.0
Epoch 6, Class Loss=0.4611658751964569, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.4611658751964569, Class Loss=0.4611658751964569, Reg Loss=0.0
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/35, Loss=7.751841121912003
Loss made of: CE 1.1301907300949097, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.174150466918945 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.634365493059159
Loss made of: CE 0.629737377166748, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.998393535614014 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.188421046733856
Loss made of: CE 0.6477227210998535, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.576568603515625 EntMin 0.0
Epoch 1, Class Loss=0.7979165315628052, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.7979165315628052, Class Loss=0.7979165315628052, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000584
Epoch 2, Batch 10/35, Loss=5.795316988229752
Loss made of: CE 0.41655367612838745, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.233196258544922 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.763288938999176
Loss made of: CE 0.46026477217674255, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.600639343261719 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.536866798996925
Loss made of: CE 0.3651140630245209, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.077702522277832 EntMin 0.0
Epoch 2, Class Loss=0.46178239583969116, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.46178239583969116, Class Loss=0.46178239583969116, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/35, Loss=5.271158242225647
Loss made of: CE 0.4049442410469055, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.823339462280273 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.573657724261284
Loss made of: CE 0.3608686625957489, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3818769454956055 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.347688058018685
Loss made of: CE 0.3766601085662842, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.196542739868164 EntMin 0.0
Epoch 3, Class Loss=0.38575315475463867, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.38575315475463867, Class Loss=0.38575315475463867, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000487
Epoch 4, Batch 10/35, Loss=5.218306410312652
Loss made of: CE 0.30177924036979675, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.931157112121582 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.107784372568131
Loss made of: CE 0.3249211609363556, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.604617118835449 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.3498987555503845
Loss made of: CE 0.29700934886932373, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.985188007354736 EntMin 0.0
Epoch 4, Class Loss=0.3487735688686371, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.3487735688686371, Class Loss=0.3487735688686371, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000438
Epoch 5, Batch 10/35, Loss=5.160742059350014
Loss made of: CE 0.3306249976158142, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.618830680847168 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.248076486587524
Loss made of: CE 0.3306270241737366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.912017345428467 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.874182498455047
Loss made of: CE 0.26135993003845215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.528792858123779 EntMin 0.0
Epoch 5, Class Loss=0.3374650180339813, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.3374650180339813, Class Loss=0.3374650180339813, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000389
Epoch 6, Batch 10/35, Loss=4.944025075435638
Loss made of: CE 0.2580106258392334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.430482864379883 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.158311991393566
Loss made of: CE 0.3069615960121155, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.163496017456055 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.220412075519562
Loss made of: CE 0.34475192427635193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.38591194152832 EntMin 0.0
Epoch 6, Class Loss=0.34971997141838074, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.34971997141838074, Class Loss=0.34971997141838074, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3806421756744385, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.878725
Mean Acc: 0.616836
FreqW Acc: 0.791477
Mean IoU: 0.476297
Class IoU:
	class 0: 0.8959093
	class 1: 0.7177777
	class 2: 0.3516647
	class 3: 0.4570232
	class 4: 0.5889603
	class 5: 0.16195111
	class 6: 0.7455058
	class 7: 0.72240824
	class 8: 0.65352404
	class 9: 0.0028964404
	class 10: 0.55662704
	class 11: 0.28766382
	class 12: 0.04995325
Class Acc:
	class 0: 0.97206193
	class 1: 0.73649895
	class 2: 0.68886316
	class 3: 0.9588909
	class 4: 0.86036825
	class 5: 0.16368595
	class 6: 0.75672066
	class 7: 0.8060942
	class 8: 0.8528596
	class 9: 0.0028991797
	class 10: 0.8124979
	class 11: 0.35740995
	class 12: 0.050021533

federated global round: 14, step: 2
select part of clients to conduct local training
[16, 9, 4, 0]
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/42, Loss=5.494494223594666
Loss made of: CE 0.7058398127555847, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.177146911621094 EntMin 0.0
Epoch 1, Batch 20/42, Loss=5.179305690526962
Loss made of: CE 0.528749942779541, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.60346794128418 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.288070821762085
Loss made of: CE 0.5865062475204468, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.497803688049316 EntMin 0.0
Epoch 1, Batch 40/42, Loss=4.742574366927147
Loss made of: CE 0.39173638820648193, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.901793956756592 EntMin 0.0
Epoch 1, Class Loss=0.6052983999252319, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.6052983999252319, Class Loss=0.6052983999252319, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/42, Loss=5.057456442713738
Loss made of: CE 0.3220166563987732, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363475799560547 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.78553232550621
Loss made of: CE 0.4254175126552582, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.906817436218262 EntMin 0.0
Epoch 2, Batch 30/42, Loss=4.657806661725044
Loss made of: CE 0.34737706184387207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7267961502075195 EntMin 0.0
Epoch 2, Batch 40/42, Loss=4.6835375934839245
Loss made of: CE 0.4589185118675232, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.957432746887207 EntMin 0.0
Epoch 2, Class Loss=0.4271565079689026, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.4271565079689026, Class Loss=0.4271565079689026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/42, Loss=4.644775110483169
Loss made of: CE 0.3806607723236084, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.917823314666748 EntMin 0.0
Epoch 3, Batch 20/42, Loss=4.895822140574455
Loss made of: CE 0.3783075213432312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.591151714324951 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.648886907100677
Loss made of: CE 0.34791892766952515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.440168380737305 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.468190401792526
Loss made of: CE 0.3082364797592163, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.366252899169922 EntMin 0.0
Epoch 3, Class Loss=0.38953685760498047, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.38953685760498047, Class Loss=0.38953685760498047, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/42, Loss=4.713340133428574
Loss made of: CE 0.35582780838012695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.051985740661621 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.498114085197448
Loss made of: CE 0.41050347685813904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.209440231323242 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.566923150420189
Loss made of: CE 0.3227344751358032, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.959663152694702 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.635713657736778
Loss made of: CE 0.3818715512752533, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.171881198883057 EntMin 0.0
Epoch 4, Class Loss=0.39127805829048157, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.39127805829048157, Class Loss=0.39127805829048157, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/42, Loss=4.349681505560875
Loss made of: CE 0.37927210330963135, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9125630855560303 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.6306552439928055
Loss made of: CE 0.3513343036174774, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.839017391204834 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.640639770030975
Loss made of: CE 0.4360911548137665, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.528929710388184 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.528898590803147
Loss made of: CE 0.41173475980758667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.163607120513916 EntMin 0.0
Epoch 5, Class Loss=0.3803790807723999, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3803790807723999, Class Loss=0.3803790807723999, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/42, Loss=4.56845520734787
Loss made of: CE 0.34610217809677124, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.638840675354004 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.381483921408654
Loss made of: CE 0.3761593699455261, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.883281707763672 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.670181542634964
Loss made of: CE 0.4703138470649719, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.263631343841553 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.730686846375465
Loss made of: CE 0.4348500967025757, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.747434616088867 EntMin 0.0
Epoch 6, Class Loss=0.39316365122795105, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.39316365122795105, Class Loss=0.39316365122795105, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=6.359302705526352
Loss made of: CE 0.44200778007507324, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.450603008270264 EntMin 0.0
Epoch 1, Batch 20/35, Loss=5.858371604979038
Loss made of: CE 0.20847293734550476, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.100863456726074 EntMin 0.0
Epoch 1, Batch 30/35, Loss=5.402763144671917
Loss made of: CE 0.19207358360290527, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.909979820251465 EntMin 0.0
Epoch 1, Class Loss=0.26085859537124634, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.26085859537124634, Class Loss=0.26085859537124634, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/35, Loss=5.535865902900696
Loss made of: CE 0.33635932207107544, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5279459953308105 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.315281388163567
Loss made of: CE 0.19390511512756348, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.804713249206543 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.184883476793766
Loss made of: CE 0.2003318965435028, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.385770320892334 EntMin 0.0
Epoch 2, Class Loss=0.24651971459388733, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.24651971459388733, Class Loss=0.24651971459388733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/35, Loss=5.202572877705097
Loss made of: CE 0.22064588963985443, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.65500020980835 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.253919386863709
Loss made of: CE 0.2486949861049652, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.73625373840332 EntMin 0.0
Epoch 3, Batch 30/35, Loss=4.990153500437737
Loss made of: CE 0.2365202009677887, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.484267234802246 EntMin 0.0
Epoch 3, Class Loss=0.2685745358467102, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.2685745358467102, Class Loss=0.2685745358467102, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/35, Loss=5.1666681438684465
Loss made of: CE 0.25660574436187744, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.24688196182251 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.2079460069537165
Loss made of: CE 0.2891746759414673, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.218258857727051 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.100489644706249
Loss made of: CE 0.29330694675445557, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.336888313293457 EntMin 0.0
Epoch 4, Class Loss=0.303495317697525, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.303495317697525, Class Loss=0.303495317697525, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/35, Loss=4.837737429141998
Loss made of: CE 0.2845911979675293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.519055366516113 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.187492145597934
Loss made of: CE 0.24678441882133484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.198098182678223 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.770621359348297
Loss made of: CE 0.5313000679016113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.913003921508789 EntMin 0.0
Epoch 5, Class Loss=0.3257063329219818, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.3257063329219818, Class Loss=0.3257063329219818, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/35, Loss=5.109854498505593
Loss made of: CE 0.37342214584350586, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.132028102874756 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.698159247636795
Loss made of: CE 0.3891019821166992, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.815869331359863 EntMin 0.0
Epoch 6, Batch 30/35, Loss=4.979514190554619
Loss made of: CE 0.2862653434276581, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.997030735015869 EntMin 0.0
Epoch 6, Class Loss=0.3293754458427429, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3293754458427429, Class Loss=0.3293754458427429, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.457847276329995
Loss made of: CE 0.4341030716896057, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.136844635009766 EntMin 0.0
Epoch 1, Class Loss=0.48910441994667053, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.48910441994667053, Class Loss=0.48910441994667053, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/19, Loss=4.543195652961731
Loss made of: CE 0.4408068060874939, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.063776016235352 EntMin 0.0
Epoch 2, Class Loss=0.4678736627101898, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.4678736627101898, Class Loss=0.4678736627101898, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/19, Loss=4.363528800010681
Loss made of: CE 0.3882475793361664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4691715240478516 EntMin 0.0
Epoch 3, Class Loss=0.4588298201560974, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.4588298201560974, Class Loss=0.4588298201560974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/19, Loss=4.301503348350525
Loss made of: CE 0.44660770893096924, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6916167736053467 EntMin 0.0
Epoch 4, Class Loss=0.45080164074897766, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.45080164074897766, Class Loss=0.45080164074897766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/19, Loss=4.245310166478157
Loss made of: CE 0.4963495433330536, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.599229574203491 EntMin 0.0
Epoch 5, Class Loss=0.44678056240081787, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.44678056240081787, Class Loss=0.44678056240081787, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/19, Loss=4.350607630610466
Loss made of: CE 0.4326295554637909, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7778306007385254 EntMin 0.0
Epoch 6, Class Loss=0.45040637254714966, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.45040637254714966, Class Loss=0.45040637254714966, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000304
Epoch 1, Batch 10/19, Loss=4.467759954929352
Loss made of: CE 0.5324091911315918, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.378763198852539 EntMin 0.0
Epoch 1, Class Loss=0.5004112720489502, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.5004112720489502, Class Loss=0.5004112720489502, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000258
Epoch 2, Batch 10/19, Loss=4.307673865556717
Loss made of: CE 0.49561256170272827, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.941767692565918 EntMin 0.0
Epoch 2, Class Loss=0.46626001596450806, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.46626001596450806, Class Loss=0.46626001596450806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000211
Epoch 3, Batch 10/19, Loss=4.276860901713372
Loss made of: CE 0.4970642924308777, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5602989196777344 EntMin 0.0
Epoch 3, Class Loss=0.46035823225975037, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.46035823225975037, Class Loss=0.46035823225975037, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000163
Epoch 4, Batch 10/19, Loss=4.34364086985588
Loss made of: CE 0.6355111002922058, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5346198081970215 EntMin 0.0
Epoch 4, Class Loss=0.4670630097389221, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.4670630097389221, Class Loss=0.4670630097389221, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000113
Epoch 5, Batch 10/19, Loss=4.209101414680481
Loss made of: CE 0.455674409866333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8894715309143066 EntMin 0.0
Epoch 5, Class Loss=0.4582030773162842, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.4582030773162842, Class Loss=0.4582030773162842, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000061
Epoch 6, Batch 10/19, Loss=4.355669379234314
Loss made of: CE 0.48210328817367554, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.647738218307495 EntMin 0.0
Epoch 6, Class Loss=0.457004189491272, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.457004189491272, Class Loss=0.457004189491272, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3534746766090393, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.897063
Mean Acc: 0.657465
FreqW Acc: 0.822082
Mean IoU: 0.536036
Class IoU:
	class 0: 0.9026443
	class 1: 0.72723776
	class 2: 0.3509915
	class 3: 0.49672094
	class 4: 0.5685974
	class 5: 0.1685993
	class 6: 0.75741637
	class 7: 0.72951233
	class 8: 0.7088836
	class 9: 0.0069534928
	class 10: 0.67849886
	class 11: 0.30431724
	class 12: 0.56810087
Class Acc:
	class 0: 0.9698526
	class 1: 0.75246733
	class 2: 0.68111825
	class 3: 0.94932175
	class 4: 0.87573653
	class 5: 0.17075793
	class 6: 0.7693058
	class 7: 0.8157552
	class 8: 0.75170404
	class 9: 0.0069735893
	class 10: 0.7007714
	class 11: 0.3819559
	class 12: 0.72132266

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[11, 6, 10, 7]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=6.808316589891911
Loss made of: CE 0.46087655425071716, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.929113388061523 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.696919474005699
Loss made of: CE 0.35000142455101013, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.177255153656006 EntMin 0.0
Epoch 1, Batch 30/102, Loss=5.858220504224301
Loss made of: CE 0.16213493049144745, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.348276138305664 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.646493470668792
Loss made of: CE 0.19076834619045258, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.788684368133545 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.544392259418965
Loss made of: CE 0.3002139925956726, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.609355926513672 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.341928453743458
Loss made of: CE 0.2104916274547577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.336264610290527 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.063842503726482
Loss made of: CE 0.3114185929298401, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.612520694732666 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.88446244969964
Loss made of: CE 0.26998084783554077, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.61778450012207 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.916023196280003
Loss made of: CE 0.11476556956768036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.063963890075684 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.946540132164955
Loss made of: CE 0.17576733231544495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.627367973327637 EntMin 0.0
Epoch 1, Class Loss=0.23133908212184906, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.23133908212184906, Class Loss=0.23133908212184906, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=4.894940572977066
Loss made of: CE 0.343565434217453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.230241775512695 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.8875433772802355
Loss made of: CE 0.30651119351387024, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.165154933929443 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.9362415507435795
Loss made of: CE 0.2980425953865051, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.759097576141357 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.944983665645123
Loss made of: CE 0.30943676829338074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.067055702209473 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.821439614892006
Loss made of: CE 0.24113202095031738, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389843940734863 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.93877934217453
Loss made of: CE 0.34659117460250854, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.23326301574707 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.4385945975780485
Loss made of: CE 0.20065920054912567, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.972884178161621 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.739186108112335
Loss made of: CE 0.18479196727275848, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611248016357422 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.9646345436573025
Loss made of: CE 0.40056341886520386, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.85429573059082 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.868760745227337
Loss made of: CE 0.23563441634178162, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.82865047454834 EntMin 0.0
Epoch 2, Class Loss=0.29773008823394775, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.29773008823394775, Class Loss=0.29773008823394775, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=4.699503669142723
Loss made of: CE 0.337309330701828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.348742961883545 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.740860396623612
Loss made of: CE 0.4685792922973633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.021507263183594 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.885170048475265
Loss made of: CE 0.3373428285121918, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.355779647827148 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.680624243617058
Loss made of: CE 0.2984452247619629, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.130984783172607 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.86518132686615
Loss made of: CE 0.3557155132293701, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.896641254425049 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.594943284988403
Loss made of: CE 0.3173210620880127, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.147147178649902 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.526075750589371
Loss made of: CE 0.3123985230922699, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.487740516662598 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.819215765595436
Loss made of: CE 0.24684980511665344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.689326286315918 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.853335377573967
Loss made of: CE 0.32535791397094727, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6901919841766357 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 100/102, Loss=4.6558003425598145
Loss made of: CE 0.25696849822998047, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7791128158569336 EntMin 0.0
Epoch 3, Class Loss=0.33642587065696716, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.33642587065696716, Class Loss=0.33642587065696716, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.547978559136391
Loss made of: CE 0.37888282537460327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0492634773254395 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.865102392435074
Loss made of: CE 0.3506055474281311, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.201409339904785 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.673659986257553
Loss made of: CE 0.44369301199913025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.285128593444824 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.614359712600708
Loss made of: CE 0.3670675456523895, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6784720420837402 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.745093661546707
Loss made of: CE 0.35420966148376465, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.025924205780029 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.91700213253498
Loss made of: CE 0.48331186175346375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.795552730560303 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.658567905426025
Loss made of: CE 0.4397454261779785, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.439022064208984 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.811298578977585
Loss made of: CE 0.4613914489746094, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.824606895446777 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.809593152999878
Loss made of: CE 0.30664321780204773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.49131441116333 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.701673471927643
Loss made of: CE 0.4274325370788574, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.469483375549316 EntMin 0.0
Epoch 4, Class Loss=0.38167935609817505, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.38167935609817505, Class Loss=0.38167935609817505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=4.892941316962242
Loss made of: CE 0.4252493381500244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7789220809936523 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.880080768465996
Loss made of: CE 0.43815821409225464, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.179123401641846 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.804787439107895
Loss made of: CE 0.3753097653388977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.237695693969727 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.598919788002968
Loss made of: CE 0.4304603040218353, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128458023071289 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.710320147871971
Loss made of: CE 0.39509114623069763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.739551067352295 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.424380558729172
Loss made of: CE 0.5069000124931335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.919597625732422 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.51678563952446
Loss made of: CE 0.45147770643234253, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9553732872009277 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.626926451921463
Loss made of: CE 0.39409393072128296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.602634429931641 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.652066159248352
Loss made of: CE 0.3692915439605713, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.615161418914795 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.729094743728638
Loss made of: CE 0.3951690196990967, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.64923095703125 EntMin 0.0
Epoch 5, Class Loss=0.42311808466911316, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.42311808466911316, Class Loss=0.42311808466911316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.79608114361763
Loss made of: CE 0.43657809495925903, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.301263332366943 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.406954967975617
Loss made of: CE 0.40769094228744507, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.011358261108398 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.311085373163223
Loss made of: CE 0.4015223979949951, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9573514461517334 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.869209188222885
Loss made of: CE 0.4890556335449219, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.833590030670166 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.405455231666565
Loss made of: CE 0.41226792335510254, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.72965407371521 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.582670640945435
Loss made of: CE 0.5692936182022095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.039735794067383 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.432561555504799
Loss made of: CE 0.42978882789611816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5076823234558105 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.518022838234901
Loss made of: CE 0.38784775137901306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9821276664733887 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.5623581320047375
Loss made of: CE 0.42444103956222534, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9939496517181396 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.681316360831261
Loss made of: CE 0.4516032636165619, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.759493827819824 EntMin 0.0
Epoch 6, Class Loss=0.43387651443481445, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.43387651443481445, Class Loss=0.43387651443481445, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=6.532911986112595
Loss made of: CE 0.2675250768661499, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.460153579711914 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.541338224709034
Loss made of: CE 0.15737617015838623, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.082916259765625 EntMin 0.0
Epoch 1, Class Loss=0.20567195117473602, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.20567195117473602, Class Loss=0.20567195117473602, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=5.337381058931351
Loss made of: CE 0.42185741662979126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.91279411315918 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.9609455347061155
Loss made of: CE 0.3279194235801697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.536362171173096 EntMin 0.0
Epoch 2, Class Loss=0.4257968068122864, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.4257968068122864, Class Loss=0.4257968068122864, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=4.9791632771492
Loss made of: CE 0.5754830837249756, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.476161956787109 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.877209296822548
Loss made of: CE 0.678377628326416, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9984750747680664 EntMin 0.0
Epoch 3, Class Loss=0.5643695592880249, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.5643695592880249, Class Loss=0.5643695592880249, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.774925720691681
Loss made of: CE 0.5899143218994141, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.378685474395752 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.734104245901108
Loss made of: CE 0.6848532557487488, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.484174728393555 EntMin 0.0
Epoch 4, Class Loss=0.6621701121330261, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.6621701121330261, Class Loss=0.6621701121330261, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=4.785317713022232
Loss made of: CE 0.6892387270927429, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.124320030212402 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.704797029495239
Loss made of: CE 0.6391874551773071, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698692321777344 EntMin 0.0
Epoch 5, Class Loss=0.702867865562439, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.702867865562439, Class Loss=0.702867865562439, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.6583851158618925
Loss made of: CE 0.8804416656494141, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.105939865112305 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.689864987134934
Loss made of: CE 0.649628758430481, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.412630796432495 EntMin 0.0
Epoch 6, Class Loss=0.7163538932800293, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.7163538932800293, Class Loss=0.7163538932800293, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=6.722677391767502
Loss made of: CE 0.1891886591911316, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.795621395111084 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.6062628597021105
Loss made of: CE 0.30263659358024597, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.285118103027344 EntMin 0.0
Epoch 1, Class Loss=0.21254962682724, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.21254962682724, Class Loss=0.21254962682724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=5.193040215969086
Loss made of: CE 0.3776586949825287, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.323428630828857 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.88988775908947
Loss made of: CE 0.3522791564464569, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.766530513763428 EntMin 0.0
Epoch 2, Class Loss=0.4280414283275604, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.4280414283275604, Class Loss=0.4280414283275604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=4.832068455219269
Loss made of: CE 0.522892951965332, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.829789638519287 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.953818273544312
Loss made of: CE 0.5560371279716492, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.376594066619873 EntMin 0.0
Epoch 3, Class Loss=0.5865606665611267, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5865606665611267, Class Loss=0.5865606665611267, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.74469792842865
Loss made of: CE 0.6499085426330566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.172874450683594 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.797622004151345
Loss made of: CE 0.6733044385910034, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.772674322128296 EntMin 0.0
Epoch 4, Class Loss=0.6542398929595947, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.6542398929595947, Class Loss=0.6542398929595947, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=4.964626139402389
Loss made of: CE 0.7224206924438477, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.862137794494629 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.7197735667228695
Loss made of: CE 0.7263026237487793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5299391746521 EntMin 0.0
Epoch 5, Class Loss=0.7086175680160522, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.7086175680160522, Class Loss=0.7086175680160522, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.707751095294952
Loss made of: CE 0.6097532510757446, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.934872627258301 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.670810848474503
Loss made of: CE 0.653290867805481, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0476250648498535 EntMin 0.0
Epoch 6, Class Loss=0.7226912379264832, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.7226912379264832, Class Loss=0.7226912379264832, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=6.859126833081246
Loss made of: CE 0.18573980033397675, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.478808403015137 EntMin 0.0
Epoch 1, Batch 20/102, Loss=6.239907890558243
Loss made of: CE 0.3189142942428589, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.458907604217529 EntMin 0.0
Epoch 1, Batch 30/102, Loss=5.785648138821125
Loss made of: CE 0.2700173854827881, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.600059509277344 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.3617534071207045
Loss made of: CE 0.20404301583766937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.547070503234863 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.386903411149978
Loss made of: CE 0.28127458691596985, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.368210792541504 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.254915674030781
Loss made of: CE 0.2606916129589081, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.312352180480957 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.233724670112133
Loss made of: CE 0.17897503077983856, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.563855171203613 EntMin 0.0
Epoch 1, Batch 80/102, Loss=5.014857226610184
Loss made of: CE 0.19335207343101501, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.203210830688477 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.492106773704291
Loss made of: CE 0.2036862075328827, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.038193702697754 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.5156139329075815
Loss made of: CE 0.24185721576213837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.359274864196777 EntMin 0.0
Epoch 1, Class Loss=0.23311585187911987, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.23311585187911987, Class Loss=0.23311585187911987, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=5.203973016142845
Loss made of: CE 0.24291536211967468, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.132046699523926 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.875190001726151
Loss made of: CE 0.28396064043045044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.929784774780273 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.9337292775511745
Loss made of: CE 0.27539560198783875, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.617833137512207 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.923550596833229
Loss made of: CE 0.25108832120895386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.643573760986328 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.573237189650536
Loss made of: CE 0.2709522247314453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.179411888122559 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.768518111109733
Loss made of: CE 0.3337133824825287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.789127349853516 EntMin 0.0
Epoch 2, Batch 70/102, Loss=5.224102933704853
Loss made of: CE 0.387652724981308, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.26202392578125 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.667309023439884
Loss made of: CE 0.21063992381095886, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7138051986694336 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.543691883981228
Loss made of: CE 0.2754461169242859, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363630294799805 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.931737507879734
Loss made of: CE 0.27644437551498413, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.819116592407227 EntMin 0.0
Epoch 2, Class Loss=0.2969059348106384, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.2969059348106384, Class Loss=0.2969059348106384, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=5.088899958133697
Loss made of: CE 0.31588053703308105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.083148002624512 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.893868863582611
Loss made of: CE 0.38949280977249146, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.524087905883789 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.672221788764
Loss made of: CE 0.37050601840019226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291283130645752 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.865134808421135
Loss made of: CE 0.2892768085002899, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.921415328979492 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.670452490448952
Loss made of: CE 0.30240458250045776, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.702836036682129 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.554787865281105
Loss made of: CE 0.2649211585521698, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.062650680541992 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.819141775369644
Loss made of: CE 0.25840747356414795, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8996217250823975 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 80/102, Loss=4.513896609842777
Loss made of: CE 0.22323238849639893, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9946885108947754 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.644056418538094
Loss made of: CE 0.3120948076248169, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.043220520019531 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.4005141913890835
Loss made of: CE 0.2711949944496155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9246110916137695 EntMin 0.0
Epoch 3, Class Loss=0.3393782079219818, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3393782079219818, Class Loss=0.3393782079219818, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.896622732281685
Loss made of: CE 0.3921155631542206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.109992980957031 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.54435071349144
Loss made of: CE 0.4323387145996094, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1472578048706055 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.706421339511872
Loss made of: CE 0.2871038317680359, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.767580986022949 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.701446235179901
Loss made of: CE 0.36673301458358765, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.284455299377441 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.662447401881218
Loss made of: CE 0.44456422328948975, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.017611026763916 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.771928671002388
Loss made of: CE 0.38552841544151306, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664976119995117 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.565846157073975
Loss made of: CE 0.3361278474330902, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9093027114868164 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.756332457065582
Loss made of: CE 0.36827629804611206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2515869140625 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.734049320220947
Loss made of: CE 0.3967815041542053, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.608184814453125 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.867213456332683
Loss made of: CE 0.24984924495220184, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.03751802444458 EntMin 0.0
Epoch 4, Class Loss=0.3829585611820221, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.3829585611820221, Class Loss=0.3829585611820221, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=4.482754284143448
Loss made of: CE 0.4061731696128845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.759298324584961 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.956637570261956
Loss made of: CE 0.40287238359451294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.329229354858398 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.847725832462311
Loss made of: CE 0.39642423391342163, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2772722244262695 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.634566611051559
Loss made of: CE 0.4730339050292969, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8508708477020264 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.565608215332031
Loss made of: CE 0.470306396484375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.765195369720459 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.670664247870445
Loss made of: CE 0.4429264962673187, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6603705883026123 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.65301676094532
Loss made of: CE 0.34109753370285034, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.268185615539551 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.7899030148983
Loss made of: CE 0.3301044702529907, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202493667602539 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.577602225542068
Loss made of: CE 0.5882114171981812, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9861388206481934 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.540243402123451
Loss made of: CE 0.4291049838066101, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.805144786834717 EntMin 0.0
Epoch 5, Class Loss=0.42360740900039673, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.42360740900039673, Class Loss=0.42360740900039673, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.618684259057045
Loss made of: CE 0.4539080858230591, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3559350967407227 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.874932193756104
Loss made of: CE 0.4188363254070282, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.07991361618042 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.280127888917923
Loss made of: CE 0.4722083508968353, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3697824478149414 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.629120928049088
Loss made of: CE 0.40217170119285583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.958670139312744 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.73170328438282
Loss made of: CE 0.5436708331108093, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.216637134552002 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.697155651450157
Loss made of: CE 0.40364593267440796, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.828808546066284 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.57603911459446
Loss made of: CE 0.4554738998413086, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9926223754882812 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.485123899579048
Loss made of: CE 0.4246893525123596, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.055361747741699 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.718676152825355
Loss made of: CE 0.4245271682739258, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.345865249633789 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.600636133551598
Loss made of: CE 0.4568168520927429, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.897691249847412 EntMin 0.0
Epoch 6, Class Loss=0.43578967452049255, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.43578967452049255, Class Loss=0.43578967452049255, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.581415593624115, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.827199
Mean Acc: 0.555888
FreqW Acc: 0.703231
Mean IoU: 0.425915
Class IoU:
	class 0: 0.82981825
	class 1: 0.6770977
	class 2: 0.33170372
	class 3: 0.50314945
	class 4: 0.41568494
	class 5: 0.19377635
	class 6: 0.80309325
	class 7: 0.7539521
	class 8: 0.70498025
	class 9: 0.0013639992
	class 10: 0.47081867
	class 11: 0.33166105
	class 12: 0.53772634
	class 13: 0.0
	class 14: 0.589838
	class 15: 0.09588756
	class 16: 0.0
Class Acc:
	class 0: 0.96891016
	class 1: 0.69333816
	class 2: 0.7720836
	class 3: 0.82817286
	class 4: 0.88622636
	class 5: 0.19568229
	class 6: 0.8272218
	class 7: 0.8407704
	class 8: 0.7822061
	class 9: 0.0013648871
	class 10: 0.7174166
	class 11: 0.49441907
	class 12: 0.6988394
	class 13: 0.0
	class 14: 0.64702326
	class 15: 0.096415915
	class 16: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[7, 11, 16, 18]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=5.027294379472733
Loss made of: CE 0.656425416469574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6562840938568115 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.0351334571838375
Loss made of: CE 0.6069096922874451, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.029767990112305 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.831689500808716
Loss made of: CE 0.4796813428401947, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.27353572845459 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.98529925942421
Loss made of: CE 0.5593340396881104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.105982780456543 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.96313624382019
Loss made of: CE 0.5833898782730103, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.41121244430542 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.968536627292633
Loss made of: CE 0.489854097366333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8965461254119873 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.860149765014649
Loss made of: CE 0.5504758954048157, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.275732517242432 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.850944900512696
Loss made of: CE 0.5225637555122375, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8538949489593506 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.374881887435913
Loss made of: CE 0.46605920791625977, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5428426265716553 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.463634443283081
Loss made of: CE 0.5756202936172485, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9453160762786865 EntMin 0.0
Epoch 1, Class Loss=0.5505442023277283, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.5505442023277283, Class Loss=0.5505442023277283, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.9341538667678835
Loss made of: CE 0.4717252254486084, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.735891103744507 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.566508108377457
Loss made of: CE 0.4785788953304291, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.381386756896973 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.727412974834442
Loss made of: CE 0.49270594120025635, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.173946380615234 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.655769342184067
Loss made of: CE 0.4681801199913025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.364627838134766 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.472975015640259
Loss made of: CE 0.4253133535385132, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.738664388656616 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.606331580877304
Loss made of: CE 0.46941107511520386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.007726192474365 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.986326619982719
Loss made of: CE 0.5389624834060669, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.253371238708496 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.636766594648361
Loss made of: CE 0.38967248797416687, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.61964750289917 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 90/102, Loss=4.468343511223793
Loss made of: CE 0.3927555978298187, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8988113403320312 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.754077681899071
Loss made of: CE 0.46576711535453796, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.401922225952148 EntMin 0.0
Epoch 2, Class Loss=0.48024609684944153, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.48024609684944153, Class Loss=0.48024609684944153, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.989488592743873
Loss made of: CE 0.4939965009689331, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9102437496185303 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.629564854502678
Loss made of: CE 0.47001975774765015, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.414527416229248 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.487693324685097
Loss made of: CE 0.5027651786804199, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.10626220703125 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.706858086585998
Loss made of: CE 0.35434409976005554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.72044563293457 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.58684593141079
Loss made of: CE 0.4830079972743988, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.509983539581299 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.352940201759338
Loss made of: CE 0.4745853841304779, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.74723482131958 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.6039017051458355
Loss made of: CE 0.5155431628227234, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.558753728866577 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.379678231477738
Loss made of: CE 0.5062360167503357, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.783088445663452 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.452366524934769
Loss made of: CE 0.45126456022262573, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7665317058563232 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.2762835085391995
Loss made of: CE 0.3769969940185547, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5548923015594482 EntMin 0.0
Epoch 3, Class Loss=0.4617144763469696, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.4617144763469696, Class Loss=0.4617144763469696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.896094673871994
Loss made of: CE 0.41328322887420654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0865888595581055 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.398853662610054
Loss made of: CE 0.44361209869384766, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8668031692504883 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.4499781966209415
Loss made of: CE 0.3998832106590271, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4534966945648193 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.426977255940438
Loss made of: CE 0.4173301160335541, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.034599304199219 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.4406298220157625
Loss made of: CE 0.4587235152721405, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.711252212524414 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.717672038078308
Loss made of: CE 0.506980836391449, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.717852592468262 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.329254505038262
Loss made of: CE 0.44603246450424194, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8398547172546387 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.51344703733921
Loss made of: CE 0.4617190361022949, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.947267532348633 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.5492450386285785
Loss made of: CE 0.49288496375083923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.386627197265625 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.876488456130028
Loss made of: CE 0.4360417127609253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.005451202392578 EntMin 0.0
Epoch 4, Class Loss=0.4556962847709656, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4556962847709656, Class Loss=0.4556962847709656, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.199891000986099
Loss made of: CE 0.46948665380477905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.16183614730835 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.684654861688614
Loss made of: CE 0.4187731146812439, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.311068534851074 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.633611384034157
Loss made of: CE 0.5053022503852844, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.949801206588745 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.44082932472229
Loss made of: CE 0.4661155641078949, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.520883560180664 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.298162734508514
Loss made of: CE 0.5472512245178223, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.028692245483398 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.452595344185829
Loss made of: CE 0.4235764741897583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4953413009643555 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.450586116313934
Loss made of: CE 0.38990268111228943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.041675567626953 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.661429113149643
Loss made of: CE 0.3865777850151062, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.046257019042969 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.343085485696792
Loss made of: CE 0.5148768424987793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8251044750213623 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.323917752504348
Loss made of: CE 0.46969303488731384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.423416614532471 EntMin 0.0
Epoch 5, Class Loss=0.43817004561424255, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.43817004561424255, Class Loss=0.43817004561424255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.421053439378738
Loss made of: CE 0.44690561294555664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2912745475769043 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.753081864118576
Loss made of: CE 0.4248943328857422, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6949386596679688 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.118568047881126
Loss made of: CE 0.4682736396789551, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4203224182128906 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.449224826693535
Loss made of: CE 0.4160948693752289, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6991066932678223 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.481623554229737
Loss made of: CE 0.4275757670402527, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.006839275360107 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.453255522251129
Loss made of: CE 0.4258699417114258, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.652695417404175 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.48218283355236
Loss made of: CE 0.4138529300689697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.968538761138916 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.232285019755364
Loss made of: CE 0.41685718297958374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.923435926437378 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.35030061006546
Loss made of: CE 0.42488956451416016, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6207807064056396 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.373243793845177
Loss made of: CE 0.4948883056640625, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.596364736557007 EntMin 0.0
Epoch 6, Class Loss=0.43451011180877686, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.43451011180877686, Class Loss=0.43451011180877686, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=5.05350091457367
Loss made of: CE 0.7130064964294434, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.459266185760498 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.673404705524445
Loss made of: CE 0.5686307549476624, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.904839515686035 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.7984546512365345
Loss made of: CE 0.47691354155540466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.750972747802734 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.1643689453601835
Loss made of: CE 0.520854115486145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.759478569030762 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.106142356991768
Loss made of: CE 0.49502283334732056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.161378383636475 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.109136453270912
Loss made of: CE 0.485340416431427, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7069482803344727 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.851916640996933
Loss made of: CE 0.5185389518737793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.048641204833984 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.716711539030075
Loss made of: CE 0.5007818341255188, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.766732215881348 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.6536329001188275
Loss made of: CE 0.6282548904418945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.657121419906616 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.674985530972481
Loss made of: CE 0.540892481803894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.147935390472412 EntMin 0.0
Epoch 1, Class Loss=0.5501475930213928, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.5501475930213928, Class Loss=0.5501475930213928, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.5814397752285005
Loss made of: CE 0.45903700590133667, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7631583213806152 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.55348656475544
Loss made of: CE 0.44020605087280273, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.912726879119873 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.529528257250786
Loss made of: CE 0.5162718892097473, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.068572044372559 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.763789770007134
Loss made of: CE 0.5341639518737793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8721091747283936 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.670557126402855
Loss made of: CE 0.45778441429138184, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.010717868804932 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.7311795353889465
Loss made of: CE 0.6034960746765137, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.864002227783203 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.281242460012436
Loss made of: CE 0.4181346297264099, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6141889095306396 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.59676276743412
Loss made of: CE 0.5452346801757812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.292820453643799 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 90/102, Loss=4.783411031961441
Loss made of: CE 0.5195885896682739, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.335334300994873 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.658732098340988
Loss made of: CE 0.4701819121837616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.514407634735107 EntMin 0.0
Epoch 2, Class Loss=0.48023200035095215, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.48023200035095215, Class Loss=0.48023200035095215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.650305581092835
Loss made of: CE 0.49483522772789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.003664016723633 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.591412279009819
Loss made of: CE 0.5232386589050293, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.688201427459717 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.608775997161866
Loss made of: CE 0.4197195768356323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.723548889160156 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.620942476391792
Loss made of: CE 0.40147972106933594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.187795639038086 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.7141214340925215
Loss made of: CE 0.5083849430084229, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.684282064437866 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.532844197750092
Loss made of: CE 0.42860549688339233, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.080055236816406 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.497631025314331
Loss made of: CE 0.37330636382102966, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.139524459838867 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.703212556242943
Loss made of: CE 0.35548529028892517, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.78047513961792 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.71210788488388
Loss made of: CE 0.3935766816139221, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5330562591552734 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.5940176486969
Loss made of: CE 0.3991091847419739, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.615248441696167 EntMin 0.0
Epoch 3, Class Loss=0.4572669267654419, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4572669267654419, Class Loss=0.4572669267654419, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.414142417907715
Loss made of: CE 0.4399157166481018, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7724101543426514 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.662748703360558
Loss made of: CE 0.4043765068054199, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.313876628875732 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.587912771105766
Loss made of: CE 0.5081784725189209, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.127795219421387 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.55283936560154
Loss made of: CE 0.3932841718196869, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8443667888641357 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.688882523775101
Loss made of: CE 0.42849868535995483, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0262298583984375 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.794843485951423
Loss made of: CE 0.4234437346458435, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.486972808837891 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.545251363515854
Loss made of: CE 0.5524091720581055, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.829704999923706 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.645718276500702
Loss made of: CE 0.5940053462982178, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.151252269744873 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.606593725085259
Loss made of: CE 0.359391450881958, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.257240295410156 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.7114082217216495
Loss made of: CE 0.4797848165035248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.976889610290527 EntMin 0.0
Epoch 4, Class Loss=0.45141786336898804, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.45141786336898804, Class Loss=0.45141786336898804, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.758486816287041
Loss made of: CE 0.46252140402793884, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7730443477630615 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.698959183692932
Loss made of: CE 0.46739625930786133, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.078423500061035 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.496553221344948
Loss made of: CE 0.36200398206710815, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.951754570007324 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.344351056218147
Loss made of: CE 0.4476214647293091, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.866485595703125 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.54047489464283
Loss made of: CE 0.42684242129325867, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.708037376403809 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.325207528471947
Loss made of: CE 0.5282540321350098, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.84643816947937 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.288161051273346
Loss made of: CE 0.47701072692871094, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.591907262802124 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.309716674685478
Loss made of: CE 0.35275858640670776, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.311040878295898 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.4174488365650175
Loss made of: CE 0.4380471408367157, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.497494697570801 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.483341389894486
Loss made of: CE 0.42735904455184937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.601045608520508 EntMin 0.0
Epoch 5, Class Loss=0.4361989498138428, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.4361989498138428, Class Loss=0.4361989498138428, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.603765964508057
Loss made of: CE 0.44150158762931824, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9243006706237793 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.18335389494896
Loss made of: CE 0.40911364555358887, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7538962364196777 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.187884369492531
Loss made of: CE 0.4429682195186615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.012299060821533 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.667711538076401
Loss made of: CE 0.4693039357662201, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.953364849090576 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.230795165896415
Loss made of: CE 0.37606415152549744, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5937612056732178 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.465002304315567
Loss made of: CE 0.5145583748817444, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8128373622894287 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.200584030151367
Loss made of: CE 0.412781298160553, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.844191551208496 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.406559038162231
Loss made of: CE 0.3865297734737396, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6673989295959473 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.373688092827797
Loss made of: CE 0.4041440486907959, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7340474128723145 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.43733666241169
Loss made of: CE 0.4861795902252197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6600160598754883 EntMin 0.0
Epoch 6, Class Loss=0.43127793073654175, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.43127793073654175, Class Loss=0.43127793073654175, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/105, Loss=4.817458467930555
Loss made of: CE 0.2658807933330536, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.972683906555176 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.332593141496181
Loss made of: CE 0.09809380769729614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.324484825134277 EntMin 0.0
Epoch 1, Batch 30/105, Loss=4.182124916836619
Loss made of: CE 0.1147240623831749, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.517154693603516 EntMin 0.0
Epoch 1, Batch 40/105, Loss=4.487443327903748
Loss made of: CE 0.07949109375476837, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6987149715423584 EntMin 0.0
Epoch 1, Batch 50/105, Loss=4.662819562107325
Loss made of: CE 0.12931856513023376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.808562755584717 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.319144193828106
Loss made of: CE 0.14899489283561707, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.398740768432617 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.34452221095562
Loss made of: CE 0.11646474897861481, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.856740951538086 EntMin 0.0
Epoch 1, Batch 80/105, Loss=4.576447223499417
Loss made of: CE 0.08507180213928223, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6926140785217285 EntMin 0.0
Epoch 1, Batch 90/105, Loss=4.571717166155577
Loss made of: CE 0.09445004910230637, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.349567413330078 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.2141423262655735
Loss made of: CE 0.06588592380285263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0581207275390625 EntMin 0.0
Epoch 1, Class Loss=0.11097776889801025, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.11097776889801025, Class Loss=0.11097776889801025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/105, Loss=4.281578524410724
Loss made of: CE 0.2078637331724167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.025097846984863 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.344984567165374
Loss made of: CE 0.19168274104595184, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.027324676513672 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.47470443546772
Loss made of: CE 0.17539207637310028, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.595643997192383 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.386074008047581
Loss made of: CE 0.18756726384162903, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.176476001739502 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.206644308567047
Loss made of: CE 0.14806918799877167, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6922078132629395 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 60/105, Loss=4.376142005622387
Loss made of: CE 0.1310281902551651, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8299262523651123 EntMin 0.0
Epoch 2, Batch 70/105, Loss=4.616026087105274
Loss made of: CE 0.16145960986614227, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.880571126937866 EntMin 0.0
Epoch 2, Batch 80/105, Loss=4.370802141726017
Loss made of: CE 0.1766844242811203, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.015096187591553 EntMin 0.0
Epoch 2, Batch 90/105, Loss=4.537860479205847
Loss made of: CE 0.1127174124121666, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.579461097717285 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.384336511790752
Loss made of: CE 0.17475539445877075, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.163650035858154 EntMin 0.0
Epoch 2, Class Loss=0.19586485624313354, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.19586485624313354, Class Loss=0.19586485624313354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/105, Loss=4.243778170645237
Loss made of: CE 0.23123595118522644, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8625874519348145 EntMin 0.0
Epoch 3, Batch 20/105, Loss=4.5409047335386274
Loss made of: CE 0.2612079679965973, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.351191520690918 EntMin 0.0
Epoch 3, Batch 30/105, Loss=4.4019874960184096
Loss made of: CE 0.21135364472866058, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.528250217437744 EntMin 0.0
Epoch 3, Batch 40/105, Loss=4.5069937139749525
Loss made of: CE 0.2603214979171753, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0102667808532715 EntMin 0.0
Epoch 3, Batch 50/105, Loss=4.332132829725742
Loss made of: CE 0.23659440875053406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.870495557785034 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.385613904893399
Loss made of: CE 0.24422404170036316, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7873780727386475 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.407538212835789
Loss made of: CE 0.26222917437553406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8730621337890625 EntMin 0.0
Epoch 3, Batch 80/105, Loss=4.4739450216293335
Loss made of: CE 0.28863072395324707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.240150451660156 EntMin 0.0
Epoch 3, Batch 90/105, Loss=4.377342760562897
Loss made of: CE 0.19728486239910126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.232231140136719 EntMin 0.0
Epoch 3, Batch 100/105, Loss=4.295502844452858
Loss made of: CE 0.23593056201934814, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7210693359375 EntMin 0.0
Epoch 3, Class Loss=0.25141724944114685, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.25141724944114685, Class Loss=0.25141724944114685, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/105, Loss=4.2337008029222485
Loss made of: CE 0.3833279609680176, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.871166944503784 EntMin 0.0
Epoch 4, Batch 20/105, Loss=4.341575688123703
Loss made of: CE 0.2859412431716919, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7695252895355225 EntMin 0.0
Epoch 4, Batch 30/105, Loss=4.245315216481686
Loss made of: CE 0.27989351749420166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8985276222229004 EntMin 0.0
Epoch 4, Batch 40/105, Loss=4.620526364445686
Loss made of: CE 0.26689088344573975, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7145166397094727 EntMin 0.0
Epoch 4, Batch 50/105, Loss=4.331114777922631
Loss made of: CE 0.3154633641242981, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.755659818649292 EntMin 0.0
Epoch 4, Batch 60/105, Loss=4.506794303655624
Loss made of: CE 0.3134855628013611, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.969196081161499 EntMin 0.0
Epoch 4, Batch 70/105, Loss=4.456950871646404
Loss made of: CE 0.3256591558456421, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.541645050048828 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.311502197384835
Loss made of: CE 0.3500990867614746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.110286712646484 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.535167908668518
Loss made of: CE 0.3575276732444763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.484406471252441 EntMin 0.0
Epoch 4, Batch 100/105, Loss=4.427866259217263
Loss made of: CE 0.2712819278240204, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8581008911132812 EntMin 0.0
Epoch 4, Class Loss=0.33099961280822754, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.33099961280822754, Class Loss=0.33099961280822754, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/105, Loss=4.788690033555031
Loss made of: CE 0.42133161425590515, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9770705699920654 EntMin 0.0
Epoch 5, Batch 20/105, Loss=4.484821000695229
Loss made of: CE 0.34137505292892456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.868173599243164 EntMin 0.0
Epoch 5, Batch 30/105, Loss=4.3228987693786625
Loss made of: CE 0.4039008617401123, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.554142713546753 EntMin 0.0
Epoch 5, Batch 40/105, Loss=4.97029776275158
Loss made of: CE 0.40329819917678833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.576952934265137 EntMin 0.0
Epoch 5, Batch 50/105, Loss=4.294776138663292
Loss made of: CE 0.436504602432251, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.264971733093262 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.518807718157769
Loss made of: CE 0.3801121115684509, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9531562328338623 EntMin 0.0
Epoch 5, Batch 70/105, Loss=4.388701367378235
Loss made of: CE 0.4001200199127197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5920209884643555 EntMin 0.0
Epoch 5, Batch 80/105, Loss=4.49537867307663
Loss made of: CE 0.33288872241973877, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.505824565887451 EntMin 0.0
Epoch 5, Batch 90/105, Loss=4.394092684984207
Loss made of: CE 0.4131929874420166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9031572341918945 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.354384744167328
Loss made of: CE 0.3979420065879822, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807943344116211 EntMin 0.0
Epoch 5, Class Loss=0.3853853940963745, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3853853940963745, Class Loss=0.3853853940963745, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/105, Loss=4.287941780686379
Loss made of: CE 0.41671034693717957, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7157437801361084 EntMin 0.0
Epoch 6, Batch 20/105, Loss=4.768537005782127
Loss made of: CE 0.6500613689422607, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.707248210906982 EntMin 0.0
Epoch 6, Batch 30/105, Loss=4.395200777053833
Loss made of: CE 0.47959253191947937, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5898005962371826 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.409896779060364
Loss made of: CE 0.3394457697868347, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.473947048187256 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.42458153963089
Loss made of: CE 0.3220330774784088, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.702897548675537 EntMin 0.0
Epoch 6, Batch 60/105, Loss=4.6784968912601474
Loss made of: CE 0.41786670684814453, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9357800483703613 EntMin 0.0
Epoch 6, Batch 70/105, Loss=4.221958237886429
Loss made of: CE 0.383678674697876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.137866020202637 EntMin 0.0
Epoch 6, Batch 80/105, Loss=4.839014041423797
Loss made of: CE 0.3748103976249695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.352090835571289 EntMin 0.0
Epoch 6, Batch 90/105, Loss=4.340160661935807
Loss made of: CE 0.44030866026878357, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4305801391601562 EntMin 0.0
Epoch 6, Batch 100/105, Loss=4.382853552699089
Loss made of: CE 0.32698628306388855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.015279769897461 EntMin 0.0
Epoch 6, Class Loss=0.4235631823539734, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.4235631823539734, Class Loss=0.4235631823539734, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=4.120984194800258
Loss made of: CE 0.11310900747776031, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4697656631469727 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.003572206199169
Loss made of: CE 0.15343114733695984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.976352691650391 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.36635655760765
Loss made of: CE 0.09976363182067871, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.685028076171875 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.292895082384348
Loss made of: CE 0.08671657741069794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.297153472900391 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.145862112566829
Loss made of: CE 0.07992273569107056, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9559266567230225 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.642508126795292
Loss made of: CE 0.17639988660812378, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.036277770996094 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.501521438360214
Loss made of: CE 0.1278005689382553, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.615392208099365 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.2352855823934075
Loss made of: CE 0.09118162840604782, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.983489513397217 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.338316377624869
Loss made of: CE 0.07018810510635376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.691225051879883 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.565247092396021
Loss made of: CE 0.07147485017776489, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.255075454711914 EntMin 0.0
Epoch 1, Class Loss=0.10831806808710098, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.10831806808710098, Class Loss=0.10831806808710098, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/102, Loss=4.1249224379658695
Loss made of: CE 0.17935264110565186, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.596892833709717 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.390398416668177
Loss made of: CE 0.1139262318611145, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9643492698669434 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.392694333940744
Loss made of: CE 0.2050187885761261, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.261251449584961 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.450565564632416
Loss made of: CE 0.2558825612068176, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3121771812438965 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.2713635802268985
Loss made of: CE 0.1834040731191635, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.635253429412842 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.584250501543283
Loss made of: CE 0.10815253108739853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.745424270629883 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 70/102, Loss=4.67323298305273
Loss made of: CE 0.19521313905715942, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5593478679656982 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.359673428535461
Loss made of: CE 0.20662175118923187, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.71507453918457 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.162571348249912
Loss made of: CE 0.2464209645986557, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.438487529754639 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.2550663441419605
Loss made of: CE 0.12752659618854523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9524927139282227 EntMin 0.0
Epoch 2, Class Loss=0.18371880054473877, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.18371880054473877, Class Loss=0.18371880054473877, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/102, Loss=4.465671189129353
Loss made of: CE 0.19782096147537231, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7989413738250732 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.746554501354694
Loss made of: CE 0.30773696303367615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.765548229217529 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.213791766762734
Loss made of: CE 0.23729035258293152, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.99703311920166 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.540344768762589
Loss made of: CE 0.17032119631767273, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.995330572128296 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.508657333254814
Loss made of: CE 0.2545786201953888, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.387604713439941 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.204021395742894
Loss made of: CE 0.22925975918769836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.587392568588257 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.54332682788372
Loss made of: CE 0.17162832617759705, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.145808219909668 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.338615994155407
Loss made of: CE 0.25862500071525574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0085296630859375 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.234272930026054
Loss made of: CE 0.20190416276454926, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.861661434173584 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.256938077509403
Loss made of: CE 0.18785086274147034, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.778676748275757 EntMin 0.0
Epoch 3, Class Loss=0.23885679244995117, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.23885679244995117, Class Loss=0.23885679244995117, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/102, Loss=4.487812495231628
Loss made of: CE 0.3052244782447815, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.815613269805908 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.615923079848289
Loss made of: CE 0.30741751194000244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1142258644104 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.573885142803192
Loss made of: CE 0.3384988307952881, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9812867641448975 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.190865139663219
Loss made of: CE 0.3362264335155487, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.105047702789307 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.5266099780797955
Loss made of: CE 0.35658153891563416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.190727233886719 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.331655961275101
Loss made of: CE 0.3167937397956848, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5757322311401367 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.317285713553429
Loss made of: CE 0.28986719250679016, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.38014030456543 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.233249741792679
Loss made of: CE 0.27775150537490845, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6802871227264404 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.72011744081974
Loss made of: CE 0.3283950686454773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.413983345031738 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.251559181511402
Loss made of: CE 0.29859742522239685, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7817485332489014 EntMin 0.0
Epoch 4, Class Loss=0.315640926361084, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.315640926361084, Class Loss=0.315640926361084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/102, Loss=4.213304540514946
Loss made of: CE 0.347785085439682, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6558120250701904 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.497435116767884
Loss made of: CE 0.3402897119522095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.718083381652832 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.36714893579483
Loss made of: CE 0.3423907160758972, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7781131267547607 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.291110301017762
Loss made of: CE 0.2823931574821472, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.301599502563477 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.368922764062882
Loss made of: CE 0.3491997718811035, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.632855176925659 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.126532018184662
Loss made of: CE 0.3391041159629822, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.884786605834961 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.139211040735245
Loss made of: CE 0.43679356575012207, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7285192012786865 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.145866969227791
Loss made of: CE 0.3341771364212036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.371167182922363 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.725172272324562
Loss made of: CE 0.36846840381622314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.306846618652344 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.227982753515244
Loss made of: CE 0.3893923759460449, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9803056716918945 EntMin 0.0
Epoch 5, Class Loss=0.36146026849746704, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.36146026849746704, Class Loss=0.36146026849746704, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/102, Loss=4.4014177322387695
Loss made of: CE 0.4125804305076599, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9886062145233154 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.576788681745529
Loss made of: CE 0.40988850593566895, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9978275299072266 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.537016636133194
Loss made of: CE 0.34817084670066833, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8854689598083496 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.292495793104171
Loss made of: CE 0.4490499496459961, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8307645320892334 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.65755384862423
Loss made of: CE 0.33481502532958984, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.873525619506836 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.456864064931869
Loss made of: CE 0.380887508392334, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.574398040771484 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.52919924557209
Loss made of: CE 0.3962124288082123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.097208023071289 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.393988552689552
Loss made of: CE 0.5135366916656494, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.769892692565918 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.445710361003876
Loss made of: CE 0.4423682689666748, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.822540283203125 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.333194079995155
Loss made of: CE 0.4415402114391327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.654043197631836 EntMin 0.0
Epoch 6, Class Loss=0.40718820691108704, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.40718820691108704, Class Loss=0.40718820691108704, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.463410884141922, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.869756
Mean Acc: 0.605242
FreqW Acc: 0.781374
Mean IoU: 0.463265
Class IoU:
	class 0: 0.8845011
	class 1: 0.7346955
	class 2: 0.34134758
	class 3: 0.44558862
	class 4: 0.43200785
	class 5: 0.14286454
	class 6: 0.80144703
	class 7: 0.7568578
	class 8: 0.71340144
	class 9: 0.0
	class 10: 0.45821318
	class 11: 0.30619287
	class 12: 0.5261301
	class 13: 0.0
	class 14: 0.6680929
	class 15: 0.664169
	class 16: 0.0
Class Acc:
	class 0: 0.9577162
	class 1: 0.7655487
	class 2: 0.78804934
	class 3: 0.93938595
	class 4: 0.9085556
	class 5: 0.14356884
	class 6: 0.8243845
	class 7: 0.86334616
	class 8: 0.7836652
	class 9: 0.0
	class 10: 0.6537344
	class 11: 0.43423158
	class 12: 0.67630816
	class 13: 0.0
	class 14: 0.75064355
	class 15: 0.7999776
	class 16: 0.0

federated global round: 17, step: 3
select part of clients to conduct local training
[5, 3, 17, 11]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=3.7352712132036685
Loss made of: CE 0.06671597808599472, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.23893141746521 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.047854826413095
Loss made of: CE 0.03459709882736206, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.654357433319092 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.759933869540691
Loss made of: CE 0.07588265836238861, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.489506244659424 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.808279123902321
Loss made of: CE 0.06597771495580673, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.061736106872559 EntMin 0.0
Epoch 1, Batch 50/102, Loss=3.9039495401084423
Loss made of: CE 0.08270259946584702, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.583355665206909 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.22656847126782
Loss made of: CE 0.08265694230794907, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581544399261475 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 70/102, Loss=4.563217793777585
Loss made of: CE 0.06909192353487015, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0927276611328125 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.1939863711595535
Loss made of: CE 0.07857034355401993, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.355889320373535 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.9326499734073876
Loss made of: CE 0.0511385016143322, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9605259895324707 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.7742934145033358
Loss made of: CE 0.0732748880982399, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3417937755584717 EntMin 0.0
Epoch 1, Class Loss=0.06622825562953949, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.06622825562953949, Class Loss=0.06622825562953949, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.320902800559997
Loss made of: CE 0.14922016859054565, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9364981651306152 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.022796231508255
Loss made of: CE 0.11143726110458374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3452582359313965 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.036882238835096
Loss made of: CE 0.08468722552061081, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.717404842376709 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.149520678073168
Loss made of: CE 0.15737715363502502, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0058393478393555 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.377069465816021
Loss made of: CE 0.12126635015010834, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.236393928527832 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.187533922493458
Loss made of: CE 0.1659454107284546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.783491611480713 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.9814603410661222
Loss made of: CE 0.1447640210390091, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7548770904541016 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.248798574879766
Loss made of: CE 0.2198164463043213, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.125664234161377 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.061946564167738
Loss made of: CE 0.13214856386184692, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.25820255279541 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.429444734752178
Loss made of: CE 0.13685032725334167, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9608447551727295 EntMin 0.0
Epoch 2, Class Loss=0.13546833395957947, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.13546833395957947, Class Loss=0.13546833395957947, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.063164255023002
Loss made of: CE 0.19802360236644745, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.713766574859619 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.150149711966515
Loss made of: CE 0.19182643294334412, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.157045364379883 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.052226098626852
Loss made of: CE 0.20475971698760986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.561984062194824 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.421657793223858
Loss made of: CE 0.193282350897789, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8460681438446045 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.131807915866375
Loss made of: CE 0.2521336078643799, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4559874534606934 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.0682484120130535
Loss made of: CE 0.21094268560409546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.072535514831543 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.109116853773594
Loss made of: CE 0.1375676542520523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.796553611755371 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.529866030812263
Loss made of: CE 0.13352113962173462, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.737050771713257 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.235694755613804
Loss made of: CE 0.1831173300743103, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6869029998779297 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.9274419002234935
Loss made of: CE 0.19278943538665771, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8370614051818848 EntMin 0.0
Epoch 3, Class Loss=0.19076305627822876, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.19076305627822876, Class Loss=0.19076305627822876, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.2246724799275395
Loss made of: CE 0.22685959935188293, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.750967025756836 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.996174669265747
Loss made of: CE 0.258881151676178, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.67077898979187 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.591401575505733
Loss made of: CE 0.2884597182273865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.383065223693848 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.246239447593689
Loss made of: CE 0.22665134072303772, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.919853687286377 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.1658837527036665
Loss made of: CE 0.23718887567520142, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8584089279174805 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.2211535051465034
Loss made of: CE 0.31757211685180664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.651392936706543 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.166852144896984
Loss made of: CE 0.31787973642349243, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.355841636657715 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.086102302372455
Loss made of: CE 0.2960216999053955, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.979555130004883 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.2310116469860075
Loss made of: CE 0.33362457156181335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8384032249450684 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.386579382419586
Loss made of: CE 0.36303576827049255, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9481942653656006 EntMin 0.0
Epoch 4, Class Loss=0.2740136384963989, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.2740136384963989, Class Loss=0.2740136384963989, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.15859072804451
Loss made of: CE 0.34844690561294556, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.856602430343628 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.060024240612984
Loss made of: CE 0.27733832597732544, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.979034423828125 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.980133965611458
Loss made of: CE 0.3057251572608948, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7640905380249023 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.261295181512833
Loss made of: CE 0.38234004378318787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5443830490112305 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.311427441239357
Loss made of: CE 0.3255540728569031, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.712596893310547 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.241435492038727
Loss made of: CE 0.3520646095275879, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.435663938522339 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.250777232646942
Loss made of: CE 0.2596489191055298, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.700108528137207 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.643979835510254
Loss made of: CE 0.3884524703025818, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8283135890960693 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.090895846486092
Loss made of: CE 0.3719957768917084, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8434176445007324 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.304890652000904
Loss made of: CE 0.3128458559513092, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7218825817108154 EntMin 0.0
Epoch 5, Class Loss=0.3282621502876282, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.3282621502876282, Class Loss=0.3282621502876282, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.161871400475502
Loss made of: CE 0.3261190354824066, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.674407958984375 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.1162531733512875
Loss made of: CE 0.38723188638687134, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.244511604309082 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.055599921941758
Loss made of: CE 0.38293594121932983, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.491309642791748 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.365110000967979
Loss made of: CE 0.38721299171447754, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.503279685974121 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.344579803943634
Loss made of: CE 0.46269530057907104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.970148086547852 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.175326099991798
Loss made of: CE 0.37219294905662537, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.570995330810547 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.046814668178558
Loss made of: CE 0.3813505172729492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6499881744384766 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.043286469578743
Loss made of: CE 0.3450816869735718, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.859344482421875 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.454762133955955
Loss made of: CE 0.43489235639572144, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.900270938873291 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.491848808526993
Loss made of: CE 0.3836809992790222, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.465509414672852 EntMin 0.0
Epoch 6, Class Loss=0.37879833579063416, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.37879833579063416, Class Loss=0.37879833579063416, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=3.9610912816599013
Loss made of: CE 0.0717725157737732, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.006626605987549 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.8625381756573915
Loss made of: CE 0.03994932770729065, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.313601016998291 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.001992242783308
Loss made of: CE 0.05509595572948456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4738166332244873 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.056108703836799
Loss made of: CE 0.17182707786560059, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.403960704803467 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.140253875032068
Loss made of: CE 0.08630724251270294, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.068954944610596 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.082832326367497
Loss made of: CE 0.047040533274412155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5484066009521484 EntMin 0.0
Epoch 1, Batch 70/102, Loss=3.981022865511477
Loss made of: CE 0.0784674733877182, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.66749382019043 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.060155729204416
Loss made of: CE 0.07776130735874176, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4434306621551514 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.27864770591259
Loss made of: CE 0.07045261561870575, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8909049034118652 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.094732487946748
Loss made of: CE 0.061045870184898376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5654830932617188 EntMin 0.0
Epoch 1, Class Loss=0.0672765001654625, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.0672765001654625, Class Loss=0.0672765001654625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.137217751890421
Loss made of: CE 0.16780602931976318, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9886395931243896 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.9866411343216894
Loss made of: CE 0.18411670625209808, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.799367904663086 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.365776035934687
Loss made of: CE 0.16270264983177185, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7238357067108154 EntMin 0.0
Epoch 2, Batch 40/102, Loss=3.9778105899691583
Loss made of: CE 0.09984689205884933, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.581676483154297 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.085200610011816
Loss made of: CE 0.11132272332906723, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2958877086639404 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.060848319530487
Loss made of: CE 0.1340378224849701, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.667107582092285 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.103211253136396
Loss made of: CE 0.12063050270080566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.147381782531738 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.083543644100428
Loss made of: CE 0.11994479596614838, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.139080047607422 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.15883804783225
Loss made of: CE 0.10140561312437057, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6736466884613037 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.256734556704759
Loss made of: CE 0.1354426145553589, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.991919994354248 EntMin 0.0
Epoch 2, Class Loss=0.1350906640291214, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.1350906640291214, Class Loss=0.1350906640291214, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.35275200009346
Loss made of: CE 0.173130065202713, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7482786178588867 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.010578732192516
Loss made of: CE 0.20719820261001587, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.79840087890625 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.266710077226162
Loss made of: CE 0.25180962681770325, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.083656311035156 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.0950117945671085
Loss made of: CE 0.19709639251232147, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.172560691833496 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.517019633948803
Loss made of: CE 0.24232648313045502, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.25321102142334 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.017996275424958
Loss made of: CE 0.2329324185848236, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.431664228439331 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.2165009275078775
Loss made of: CE 0.1349843591451645, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5641140937805176 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.363516944646835
Loss made of: CE 0.18048128485679626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2197160720825195 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.106107278168201
Loss made of: CE 0.159213587641716, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5261707305908203 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.304606001079082
Loss made of: CE 0.2694893777370453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.815134048461914 EntMin 0.0
Epoch 3, Class Loss=0.19778820872306824, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.19778820872306824, Class Loss=0.19778820872306824, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.065844506025314
Loss made of: CE 0.2699918746948242, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6731393337249756 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.440243905782699
Loss made of: CE 0.2912891209125519, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6668076515197754 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.213403607904911
Loss made of: CE 0.2884027361869812, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.732595205307007 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.522138991951943
Loss made of: CE 0.27525418996810913, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4591033458709717 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.100207014381885
Loss made of: CE 0.23595285415649414, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.479733943939209 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.221016696095466
Loss made of: CE 0.36994147300720215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.016719818115234 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.480783899128437
Loss made of: CE 0.31888678669929504, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9271578788757324 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.494677925109864
Loss made of: CE 0.21436700224876404, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7971205711364746 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.194697663187981
Loss made of: CE 0.256858229637146, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.446084976196289 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.22225616723299
Loss made of: CE 0.2623269557952881, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7858543395996094 EntMin 0.0
Epoch 4, Class Loss=0.2713797092437744, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.2713797092437744, Class Loss=0.2713797092437744, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.2717271745204926
Loss made of: CE 0.34887704253196716, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5369296073913574 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.301910004019737
Loss made of: CE 0.373685359954834, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.738722324371338 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.109777200222015
Loss made of: CE 0.3302765488624573, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4985320568084717 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.3775867253541945
Loss made of: CE 0.3811308741569519, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7985541820526123 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.390530925989151
Loss made of: CE 0.27248072624206543, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8140764236450195 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.107975277304649
Loss made of: CE 0.26710665225982666, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.448190689086914 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.540075609087944
Loss made of: CE 0.3326875567436218, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.95112681388855 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.6077736765146255
Loss made of: CE 0.3234904706478119, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.764995098114014 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.0548203647136685
Loss made of: CE 0.27933233976364136, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.916313648223877 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.110914307832718
Loss made of: CE 0.3251391053199768, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5571160316467285 EntMin 0.0
Epoch 5, Class Loss=0.335636705160141, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.335636705160141, Class Loss=0.335636705160141, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.215290784835815
Loss made of: CE 0.3491020202636719, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7732951641082764 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.048243507742882
Loss made of: CE 0.33655041456222534, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.445760726928711 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.324977371096611
Loss made of: CE 0.337298721075058, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8107285499572754 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.988676595687866
Loss made of: CE 0.35392189025878906, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.341704845428467 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.287223386764526
Loss made of: CE 0.5080435276031494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.064998626708984 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.24514085650444
Loss made of: CE 0.3571353852748871, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.447722911834717 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.024162957072258
Loss made of: CE 0.4269854724407196, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5375592708587646 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.178734061121941
Loss made of: CE 0.3872971534729004, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.500852108001709 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.341938102245331
Loss made of: CE 0.4113805294036865, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5523271560668945 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.385951969027519
Loss made of: CE 0.2787027657032013, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8892250061035156 EntMin 0.0
Epoch 6, Class Loss=0.3747439384460449, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.3747439384460449, Class Loss=0.3747439384460449, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=5.361178982257843
Loss made of: CE 0.22114866971969604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698819160461426 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.85039746016264
Loss made of: CE 0.1528671681880951, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.536510467529297 EntMin 0.0
Epoch 1, Class Loss=0.1955486387014389, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.1955486387014389, Class Loss=0.1955486387014389, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/23, Loss=4.760068556666374
Loss made of: CE 0.4897882342338562, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.218016147613525 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.502720262110233
Loss made of: CE 0.24058255553245544, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2808003425598145 EntMin 0.0
Epoch 2, Class Loss=0.34563934803009033, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.34563934803009033, Class Loss=0.34563934803009033, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/23, Loss=4.484180349111557
Loss made of: CE 0.36089497804641724, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.107729911804199 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.62234226167202
Loss made of: CE 0.5738487243652344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421168327331543 EntMin 0.0
Epoch 3, Class Loss=0.432742178440094, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.432742178440094, Class Loss=0.432742178440094, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/23, Loss=4.608669945597649
Loss made of: CE 0.5700864791870117, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7979981899261475 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.448787569999695
Loss made of: CE 0.3404289186000824, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9379725456237793 EntMin 0.0
Epoch 4, Class Loss=0.44491684436798096, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.44491684436798096, Class Loss=0.44491684436798096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/23, Loss=4.522945982217789
Loss made of: CE 0.4137108325958252, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9121809005737305 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.304095742106438
Loss made of: CE 0.3694015443325043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0256524085998535 EntMin 0.0
Epoch 5, Class Loss=0.4783482551574707, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.4783482551574707, Class Loss=0.4783482551574707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/23, Loss=4.49933286011219
Loss made of: CE 0.5120469331741333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9572372436523438 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.341655573248863
Loss made of: CE 0.5039215087890625, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.215287208557129 EntMin 0.0
Epoch 6, Class Loss=0.47463053464889526, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.47463053464889526, Class Loss=0.47463053464889526, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/102, Loss=4.109792962670326
Loss made of: CE 0.4410420060157776, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.391061782836914 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.143076580762863
Loss made of: CE 0.41885289549827576, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8393425941467285 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 30/102, Loss=4.273043072223663
Loss made of: CE 0.3775356709957123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.810091972351074 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.580577546358109
Loss made of: CE 0.3674350678920746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.096931457519531 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.395389774441719
Loss made of: CE 0.3712565302848816, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.722522258758545 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.539486467838287
Loss made of: CE 0.410157173871994, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.554762601852417 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.179474958777428
Loss made of: CE 0.385114848613739, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.807732343673706 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.300572898983956
Loss made of: CE 0.3811870813369751, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.254599571228027 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.173089337348938
Loss made of: CE 0.48763927817344666, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.30479097366333 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.176695689558983
Loss made of: CE 0.49879369139671326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.586963653564453 EntMin 0.0
Epoch 1, Class Loss=0.43172726035118103, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.43172726035118103, Class Loss=0.43172726035118103, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000600
Epoch 2, Batch 10/102, Loss=4.082408192753792
Loss made of: CE 0.35532301664352417, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3931806087493896 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.144498202204704
Loss made of: CE 0.3945276439189911, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6134347915649414 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.202884182333946
Loss made of: CE 0.518328845500946, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.76694917678833 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.247943761944771
Loss made of: CE 0.3981209695339203, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.541066884994507 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.200457501411438
Loss made of: CE 0.36820870637893677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.732064962387085 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.440139520168304
Loss made of: CE 0.6321557760238647, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7514522075653076 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.9278518348932265
Loss made of: CE 0.3516468405723572, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3532376289367676 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.224828022718429
Loss made of: CE 0.472634881734848, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9818410873413086 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.338076940178871
Loss made of: CE 0.5165766477584839, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.613008499145508 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.38221121430397
Loss made of: CE 0.41032466292381287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.008147239685059 EntMin 0.0
Epoch 2, Class Loss=0.4221881628036499, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.4221881628036499, Class Loss=0.4221881628036499, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/102, Loss=4.213625973463058
Loss made of: CE 0.4216296076774597, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8321692943573 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.164268881082535
Loss made of: CE 0.49214959144592285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.59603214263916 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.172067081928253
Loss made of: CE 0.4036588668823242, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1228413581848145 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.258966335654259
Loss made of: CE 0.35014206171035767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9379756450653076 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.412839975953102
Loss made of: CE 0.4473867416381836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5285985469818115 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.20645312666893
Loss made of: CE 0.4355033338069916, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.643120527267456 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.2317462235689165
Loss made of: CE 0.34883272647857666, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.109190940856934 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.3854441285133365
Loss made of: CE 0.3080943822860718, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8834147453308105 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.285558542609214
Loss made of: CE 0.3188609778881073, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4087114334106445 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.272776567935944
Loss made of: CE 0.39221256971359253, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.322171926498413 EntMin 0.0
Epoch 3, Class Loss=0.41787654161453247, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.41787654161453247, Class Loss=0.41787654161453247, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/102, Loss=4.097964525222778
Loss made of: CE 0.39774006605148315, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3996334075927734 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.346236896514893
Loss made of: CE 0.3397558629512787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.849209785461426 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.249714687466621
Loss made of: CE 0.5063656568527222, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9347732067108154 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.146140736341477
Loss made of: CE 0.37620052695274353, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4704203605651855 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.338384953141213
Loss made of: CE 0.37732475996017456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.779071569442749 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.343728587031364
Loss made of: CE 0.3891453146934509, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9831480979919434 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.282194811105728
Loss made of: CE 0.6255051493644714, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9705162048339844 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.375032335519791
Loss made of: CE 0.5573375225067139, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.619242191314697 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.2372412264347075
Loss made of: CE 0.3797941207885742, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.762037992477417 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.347916749119759
Loss made of: CE 0.44200366735458374, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6128435134887695 EntMin 0.0
Epoch 4, Class Loss=0.4238404333591461, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4238404333591461, Class Loss=0.4238404333591461, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000504
Epoch 5, Batch 10/102, Loss=4.420828646421432
Loss made of: CE 0.3938673436641693, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5632104873657227 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.325053468346596
Loss made of: CE 0.4513927400112152, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.694847822189331 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.1674149423837665
Loss made of: CE 0.34784016013145447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.388124465942383 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.018003129959107
Loss made of: CE 0.4327382445335388, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7732245922088623 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.245364505052566
Loss made of: CE 0.4197031259536743, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.379727363586426 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.8682293027639387
Loss made of: CE 0.47602543234825134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.710514783859253 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.070978319644928
Loss made of: CE 0.4947585463523865, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6395175457000732 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.113543406128883
Loss made of: CE 0.32714200019836426, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.140111923217773 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.21073559820652
Loss made of: CE 0.46983033418655396, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.439528465270996 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.237729680538178
Loss made of: CE 0.4410344064235687, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.99865984916687 EntMin 0.0
Epoch 5, Class Loss=0.40300631523132324, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.40300631523132324, Class Loss=0.40300631523132324, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000471
Epoch 6, Batch 10/102, Loss=4.356650027632713
Loss made of: CE 0.37915319204330444, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6900949478149414 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.99695286154747
Loss made of: CE 0.43564772605895996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5953853130340576 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.031389603018761
Loss made of: CE 0.43686866760253906, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.827178716659546 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.334611898660659
Loss made of: CE 0.4235641360282898, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.348138809204102 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.05470789372921
Loss made of: CE 0.38115328550338745, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.449000835418701 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.144366219639778
Loss made of: CE 0.600960373878479, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.737788677215576 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.9548137575387954
Loss made of: CE 0.39595746994018555, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6183056831359863 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.044620329141617
Loss made of: CE 0.38858819007873535, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4101359844207764 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.139084374904632
Loss made of: CE 0.36836156249046326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4608075618743896 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.249242946505547
Loss made of: CE 0.5072497725486755, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.538630962371826 EntMin 0.0
Epoch 6, Class Loss=0.4058873951435089, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.4058873951435089, Class Loss=0.4058873951435089, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4346655011177063, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.872580
Mean Acc: 0.619276
FreqW Acc: 0.786635
Mean IoU: 0.474425
Class IoU:
	class 0: 0.8874144
	class 1: 0.740089
	class 2: 0.33796299
	class 3: 0.49311596
	class 4: 0.4059375
	class 5: 0.16072218
	class 6: 0.8095859
	class 7: 0.7722099
	class 8: 0.71641064
	class 9: 0.0
	class 10: 0.46705776
	class 11: 0.30611956
	class 12: 0.5474204
	class 13: 0.06901263
	class 14: 0.6903912
	class 15: 0.66178244
	class 16: 0.0
Class Acc:
	class 0: 0.9552417
	class 1: 0.7871015
	class 2: 0.7989536
	class 3: 0.92159116
	class 4: 0.9175121
	class 5: 0.16221748
	class 6: 0.83187664
	class 7: 0.85780895
	class 8: 0.7831157
	class 9: 0.0
	class 10: 0.6665884
	class 11: 0.4549861
	class 12: 0.72075695
	class 13: 0.069461875
	class 14: 0.79022354
	class 15: 0.8102572
	class 16: 0.0

federated global round: 18, step: 3
select part of clients to conduct local training
[9, 11, 13, 10]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.1568901017308235
Loss made of: CE 0.07861994206905365, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2015414237976074 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.021718098223209
Loss made of: CE 0.04685521870851517, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.296435832977295 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.988205024972558
Loss made of: CE 0.05609409138560295, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8678488731384277 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.8402701947838067
Loss made of: CE 0.06588099896907806, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.379849910736084 EntMin 0.0
Epoch 1, Batch 50/102, Loss=3.976820010319352
Loss made of: CE 0.11820435523986816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.069486618041992 EntMin 0.0
Epoch 1, Batch 60/102, Loss=3.8628887495025994
Loss made of: CE 0.07124456763267517, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1385273933410645 EntMin 0.0
Epoch 1, Batch 70/102, Loss=3.86826281324029
Loss made of: CE 0.055131129920482635, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5613489151000977 EntMin 0.0
Epoch 1, Batch 80/102, Loss=3.9865609100088477
Loss made of: CE 0.05068681761622429, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.84768009185791 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.878445854038
Loss made of: CE 0.046316325664520264, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5679821968078613 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.7195147667080164
Loss made of: CE 0.06296060979366302, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5276095867156982 EntMin 0.0
Epoch 1, Class Loss=0.059402018785476685, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.059402018785476685, Class Loss=0.059402018785476685, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/102, Loss=3.8182845056056975
Loss made of: CE 0.09673639386892319, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.448704481124878 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.174253576248884
Loss made of: CE 0.12376471608877182, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6283092498779297 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.044535196572542
Loss made of: CE 0.21196675300598145, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8759403228759766 EntMin 0.0
Epoch 2, Batch 40/102, Loss=3.999144713580608
Loss made of: CE 0.16757676005363464, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8105132579803467 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.3091249704360965
Loss made of: CE 0.09799307584762573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.486268043518066 EntMin 0.0
Epoch 2, Batch 60/102, Loss=3.963638312369585
Loss made of: CE 0.08260709047317505, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.562440872192383 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.959919871389866
Loss made of: CE 0.11940645426511765, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.350349426269531 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.038211382180452
Loss made of: CE 0.12231293320655823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.600325107574463 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.039411424845457
Loss made of: CE 0.11028216779232025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9282312393188477 EntMin 0.0
Epoch 2, Batch 100/102, Loss=3.6904247403144836
Loss made of: CE 0.10382835566997528, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.55996036529541 EntMin 0.0
Epoch 2, Class Loss=0.1259903758764267, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.1259903758764267, Class Loss=0.1259903758764267, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/102, Loss=4.08630797713995
Loss made of: CE 0.180450439453125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1841511726379395 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.199927473068238
Loss made of: CE 0.2449551522731781, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.515021800994873 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.9344756484031675
Loss made of: CE 0.1717337816953659, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1034088134765625 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.083330225944519
Loss made of: CE 0.15087421238422394, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4960169792175293 EntMin 0.0
Epoch 3, Batch 50/102, Loss=3.8392343178391455
Loss made of: CE 0.1999680995941162, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.426405429840088 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.075325854122639
Loss made of: CE 0.2074548453092575, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9792442321777344 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.20058656334877
Loss made of: CE 0.2137027382850647, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1835618019104 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.003796224296093
Loss made of: CE 0.15117911994457245, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.506194591522217 EntMin 0.0
Epoch 3, Batch 90/102, Loss=3.8488986864686012
Loss made of: CE 0.1825726479291916, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5779366493225098 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.833037681877613
Loss made of: CE 0.17060017585754395, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7445428371429443 EntMin 0.0
Epoch 3, Class Loss=0.1777459979057312, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.1777459979057312, Class Loss=0.1777459979057312, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/102, Loss=4.177788962423802
Loss made of: CE 0.30712080001831055, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358499526977539 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.38060152977705
Loss made of: CE 0.23495641350746155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.872230291366577 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.017305833101273
Loss made of: CE 0.2767143249511719, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8862760066986084 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.272638417780399
Loss made of: CE 0.24602007865905762, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7348217964172363 EntMin 0.0
Epoch 4, Batch 50/102, Loss=3.988267970085144
Loss made of: CE 0.18086592853069305, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7656102180480957 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.11588805615902
Loss made of: CE 0.19401466846466064, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3232831954956055 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.221416833996773
Loss made of: CE 0.2871994078159332, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.830580711364746 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.118670748174191
Loss made of: CE 0.239990696310997, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.244533538818359 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.172379794716835
Loss made of: CE 0.1943889558315277, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4253158569335938 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.2067498281598095
Loss made of: CE 0.27478593587875366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.305905818939209 EntMin 0.0
Epoch 4, Class Loss=0.2634972035884857, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.2634972035884857, Class Loss=0.2634972035884857, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.111563166975975
Loss made of: CE 0.3017955422401428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.150679588317871 EntMin 0.0
Epoch 5, Batch 20/102, Loss=3.9133395135402678
Loss made of: CE 0.317778617143631, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5152902603149414 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.957827538251877
Loss made of: CE 0.26953691244125366, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7580535411834717 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.317307263612747
Loss made of: CE 0.2688983380794525, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4566173553466797 EntMin 0.0
Epoch 5, Batch 50/102, Loss=3.991890789568424
Loss made of: CE 0.23797355592250824, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.782794952392578 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.9956552624702453
Loss made of: CE 0.32903751730918884, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7520713806152344 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 70/102, Loss=4.188547149300575
Loss made of: CE 0.3703112006187439, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8193678855895996 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.2579816550016405
Loss made of: CE 0.244817852973938, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8105335235595703 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.105996206402779
Loss made of: CE 0.48361772298812866, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3974099159240723 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.066067039966583
Loss made of: CE 0.3743802011013031, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4072465896606445 EntMin 0.0
Epoch 5, Class Loss=0.3208500146865845, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.3208500146865845, Class Loss=0.3208500146865845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/102, Loss=4.1453770130872725
Loss made of: CE 0.4312470853328705, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7597312927246094 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.9244021117687224
Loss made of: CE 0.3600359857082367, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3509597778320312 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.055925443768501
Loss made of: CE 0.40432700514793396, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3361692428588867 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.300259503722191
Loss made of: CE 0.28487667441368103, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4743714332580566 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.173387715220452
Loss made of: CE 0.360212504863739, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9747190475463867 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.296650639176368
Loss made of: CE 0.31766176223754883, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.612722396850586 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.19304213821888
Loss made of: CE 0.36821770668029785, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.21303129196167 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.182563701272011
Loss made of: CE 0.37666282057762146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.336684703826904 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.135908579826355
Loss made of: CE 0.34711676836013794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0196123123168945 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.9356731295585634
Loss made of: CE 0.3104234039783478, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3032212257385254 EntMin 0.0
Epoch 6, Class Loss=0.3623102605342865, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3623102605342865, Class Loss=0.3623102605342865, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000438
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.009578648209572
Loss made of: CE 0.45547497272491455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202901363372803 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.9596170872449874
Loss made of: CE 0.38851016759872437, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6734187602996826 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.963659185171127
Loss made of: CE 0.34192466735839844, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.940824270248413 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.2782437145709995
Loss made of: CE 0.3699692189693451, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9343700408935547 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.255286100506782
Loss made of: CE 0.3378191888332367, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.510331630706787 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.3226366430521015
Loss made of: CE 0.3465951085090637, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.299880027770996 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.093678092956543
Loss made of: CE 0.35170191526412964, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5829999446868896 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.084888133406639
Loss made of: CE 0.3620963990688324, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6988425254821777 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.9996571809053423
Loss made of: CE 0.46965834498405457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.271221160888672 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 100/102, Loss=4.040133145451546
Loss made of: CE 0.46522602438926697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.762850761413574 EntMin 0.0
Epoch 1, Class Loss=0.4044925570487976, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.4044925570487976, Class Loss=0.4044925570487976, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000405
Epoch 2, Batch 10/102, Loss=4.000242155790329
Loss made of: CE 0.33771002292633057, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.323012113571167 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.958754000067711
Loss made of: CE 0.37717458605766296, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.436739921569824 EntMin 0.0
Epoch 2, Batch 30/102, Loss=3.9245249807834623
Loss made of: CE 0.4401378035545349, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4228599071502686 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.1402492225170135
Loss made of: CE 0.3736359477043152, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3329315185546875 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.094317623972893
Loss made of: CE 0.3595639169216156, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.620779037475586 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.235213941335678
Loss made of: CE 0.556399941444397, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6380348205566406 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.798177608847618
Loss made of: CE 0.38104018568992615, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2826528549194336 EntMin 0.0
Epoch 2, Batch 80/102, Loss=3.9903093189001084
Loss made of: CE 0.4539795517921448, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7544009685516357 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.075360676646232
Loss made of: CE 0.4641258716583252, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.198326587677002 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.164541003108025
Loss made of: CE 0.3875124454498291, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.009194374084473 EntMin 0.0
Epoch 2, Class Loss=0.40004462003707886, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.40004462003707886, Class Loss=0.40004462003707886, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/102, Loss=4.011482626199722
Loss made of: CE 0.39347466826438904, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6642072200775146 EntMin 0.0
Epoch 3, Batch 20/102, Loss=3.9086999714374544
Loss made of: CE 0.4105680584907532, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1813666820526123 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.049267446994781
Loss made of: CE 0.40791457891464233, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.197152614593506 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.1308888524770735
Loss made of: CE 0.35377955436706543, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6418867111206055 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.08085922896862
Loss made of: CE 0.43386149406433105, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3850278854370117 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.911698654294014
Loss made of: CE 0.42672857642173767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3477208614349365 EntMin 0.0
Epoch 3, Batch 70/102, Loss=3.906438961625099
Loss made of: CE 0.3364242911338806, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.729851722717285 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.046031314134598
Loss made of: CE 0.30589473247528076, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7230794429779053 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.192036175727845
Loss made of: CE 0.331356018781662, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.317793369293213 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.9380105793476106
Loss made of: CE 0.34429240226745605, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1281771659851074 EntMin 0.0
Epoch 3, Class Loss=0.3976590633392334, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.3976590633392334, Class Loss=0.3976590633392334, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=3.976859709620476
Loss made of: CE 0.36900460720062256, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.360905647277832 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.163838088512421
Loss made of: CE 0.34100040793418884, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6763885021209717 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.065705326199532
Loss made of: CE 0.4298962950706482, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7033658027648926 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.9666834592819216
Loss made of: CE 0.37334927916526794, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2237091064453125 EntMin 0.0
Epoch 4, Batch 50/102, Loss=3.984168013930321
Loss made of: CE 0.4280589520931244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.56231951713562 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.153836676478386
Loss made of: CE 0.3961702287197113, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.966008186340332 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.011852315068245
Loss made of: CE 0.540081262588501, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.449141025543213 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.161206558346748
Loss made of: CE 0.448610782623291, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.620753765106201 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.141137012839318
Loss made of: CE 0.3794722557067871, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.570655345916748 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.125552904605866
Loss made of: CE 0.45519381761550903, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.575786113739014 EntMin 0.0
Epoch 4, Class Loss=0.4005521535873413, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4005521535873413, Class Loss=0.4005521535873413, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/102, Loss=4.163191947340965
Loss made of: CE 0.42767852544784546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.199650764465332 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.133442929387092
Loss made of: CE 0.4882185459136963, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.447719097137451 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.082267311215401
Loss made of: CE 0.3304310441017151, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3303065299987793 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.857894849777222
Loss made of: CE 0.40377897024154663, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5021729469299316 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.162767460942268
Loss made of: CE 0.4328097701072693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332690715789795 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.744555249810219
Loss made of: CE 0.5275856852531433, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.509542942047119 EntMin 0.0
Epoch 5, Batch 70/102, Loss=3.8742345184087754
Loss made of: CE 0.43649813532829285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3473708629608154 EntMin 0.0
Epoch 5, Batch 80/102, Loss=3.971679562330246
Loss made of: CE 0.29425808787345886, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8773913383483887 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.955707001686096
Loss made of: CE 0.4356478452682495, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.251303195953369 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.9294406533241273
Loss made of: CE 0.4407205283641815, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6616458892822266 EntMin 0.0
Epoch 5, Class Loss=0.39150965213775635, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.39150965213775635, Class Loss=0.39150965213775635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000270
Epoch 6, Batch 10/102, Loss=4.0665263235569
Loss made of: CE 0.35634931921958923, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4343059062957764 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.824639078974724
Loss made of: CE 0.43225836753845215, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.343787670135498 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.739892598986626
Loss made of: CE 0.3734687268733978, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.731043815612793 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.225536414980889
Loss made of: CE 0.4587952792644501, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.774904251098633 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.8253927141427995
Loss made of: CE 0.3694801330566406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.264247417449951 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.033973553776741
Loss made of: CE 0.5155367851257324, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.645446538925171 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.7592343151569367
Loss made of: CE 0.41620808839797974, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3370442390441895 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.9234826892614363
Loss made of: CE 0.38380166888237, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.314643621444702 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.9667806029319763
Loss made of: CE 0.43761515617370605, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3173511028289795 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.06958721280098
Loss made of: CE 0.4103749692440033, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.432347059249878 EntMin 0.0
Epoch 6, Class Loss=0.39157769083976746, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.39157769083976746, Class Loss=0.39157769083976746, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=4.603616596013308
Loss made of: CE 0.11736579239368439, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.641551971435547 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.541662780195475
Loss made of: CE 0.09599139541387558, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.215554237365723 EntMin 0.0
Epoch 1, Class Loss=0.14625190198421478, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.14625190198421478, Class Loss=0.14625190198421478, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/23, Loss=4.361128399521112
Loss made of: CE 0.3109527826309204, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.383500576019287 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.372059224545955
Loss made of: CE 0.272874116897583, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.010608673095703 EntMin 0.0
Epoch 2, Class Loss=0.2563481032848358, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.2563481032848358, Class Loss=0.2563481032848358, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/23, Loss=4.302328199148178
Loss made of: CE 0.5377223491668701, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.816495895385742 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.331538270413875
Loss made of: CE 0.35091540217399597, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.622745037078857 EntMin 0.0
Epoch 3, Class Loss=0.3484412431716919, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.3484412431716919, Class Loss=0.3484412431716919, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/23, Loss=4.146873331069946
Loss made of: CE 0.3803943991661072, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9132890701293945 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.431433820724488
Loss made of: CE 0.3720632493495941, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6722002029418945 EntMin 0.0
Epoch 4, Class Loss=0.39769911766052246, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.39769911766052246, Class Loss=0.39769911766052246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/23, Loss=4.249889546632767
Loss made of: CE 0.3596042990684509, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.723928928375244 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.1062221139669415
Loss made of: CE 0.3751993179321289, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3461928367614746 EntMin 0.0
Epoch 5, Class Loss=0.44334620237350464, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.44334620237350464, Class Loss=0.44334620237350464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/23, Loss=4.262179043889046
Loss made of: CE 0.6109669208526611, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.226641654968262 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.132141238451004
Loss made of: CE 0.4126688838005066, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.824408531188965 EntMin 0.0
Epoch 6, Class Loss=0.4679419696331024, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.4679419696331024, Class Loss=0.4679419696331024, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/23, Loss=5.393060809373855
Loss made of: CE 0.72939532995224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3716139793396 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.6427307516336445
Loss made of: CE 0.6574721336364746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0952019691467285 EntMin 0.0
Epoch 1, Class Loss=0.6978128552436829, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.6978128552436829, Class Loss=0.6978128552436829, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/23, Loss=4.581419640779496
Loss made of: CE 0.6723417043685913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.396113872528076 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.2104164272546765
Loss made of: CE 0.5563746690750122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084314346313477 EntMin 0.0
Epoch 2, Class Loss=0.5632859468460083, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.5632859468460083, Class Loss=0.5632859468460083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/23, Loss=4.199389630556107
Loss made of: CE 0.4903665781021118, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.931556224822998 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.244495350122452
Loss made of: CE 0.42857250571250916, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.751801013946533 EntMin 0.0
Epoch 3, Class Loss=0.514258086681366, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.514258086681366, Class Loss=0.514258086681366, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/23, Loss=4.106950065493583
Loss made of: CE 0.4061974883079529, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7655370235443115 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.208314552903175
Loss made of: CE 0.4986855983734131, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.46999454498291 EntMin 0.0
Epoch 4, Class Loss=0.47340095043182373, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.47340095043182373, Class Loss=0.47340095043182373, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/23, Loss=4.299075418710709
Loss made of: CE 0.4665374755859375, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5390470027923584 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.030171269178391
Loss made of: CE 0.5856685042381287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.096680641174316 EntMin 0.0
Epoch 5, Class Loss=0.45673391222953796, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.45673391222953796, Class Loss=0.45673391222953796, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/23, Loss=4.058036935329437
Loss made of: CE 0.4048703610897064, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.698688507080078 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.072211745381355
Loss made of: CE 0.45093485713005066, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.750880718231201 EntMin 0.0
Epoch 6, Class Loss=0.446376770734787, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.446376770734787, Class Loss=0.446376770734787, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4236403703689575, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.877110
Mean Acc: 0.638455
FreqW Acc: 0.794503
Mean IoU: 0.495316
Class IoU:
	class 0: 0.8892998
	class 1: 0.73931164
	class 2: 0.34310964
	class 3: 0.44820142
	class 4: 0.41654113
	class 5: 0.18847948
	class 6: 0.8067896
	class 7: 0.7769343
	class 8: 0.70579386
	class 9: 0.0
	class 10: 0.5156883
	class 11: 0.30726197
	class 12: 0.55907005
	class 13: 0.3403429
	class 14: 0.69920135
	class 15: 0.6843471
	class 16: 0.0
Class Acc:
	class 0: 0.9546275
	class 1: 0.778482
	class 2: 0.79210526
	class 3: 0.9461147
	class 4: 0.9095473
	class 5: 0.19111498
	class 6: 0.8291453
	class 7: 0.852559
	class 8: 0.764933
	class 9: 0.0
	class 10: 0.64728445
	class 11: 0.45027918
	class 12: 0.7165286
	class 13: 0.37957782
	class 14: 0.82361376
	class 15: 0.8178283
	class 16: 0.0

federated global round: 19, step: 3
select part of clients to conduct local training
[4, 7, 17, 15]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/105, Loss=4.137793924286962
Loss made of: CE 0.1223745048046112, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.764161109924316 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.026074755191803
Loss made of: CE 0.08788969367742538, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8498587608337402 EntMin 0.0
Epoch 1, Batch 30/105, Loss=3.836804522946477
Loss made of: CE 0.08083218336105347, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.278739929199219 EntMin 0.0
Epoch 1, Batch 40/105, Loss=3.7719465117901563
Loss made of: CE 0.099283367395401, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.54158353805542 EntMin 0.0
Epoch 1, Batch 50/105, Loss=3.6635803397744895
Loss made of: CE 0.0751197338104248, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.272434711456299 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.245607284083962
Loss made of: CE 0.07351118326187134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8571834564208984 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.002890181727707
Loss made of: CE 0.03056083433330059, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4314534664154053 EntMin 0.0
Epoch 1, Batch 80/105, Loss=3.765210283920169
Loss made of: CE 0.04655073210597038, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.587611198425293 EntMin 0.0
Epoch 1, Batch 90/105, Loss=4.019867917522788
Loss made of: CE 0.03173726424574852, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.591001510620117 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.015448653697968
Loss made of: CE 0.06713895499706268, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.002919673919678 EntMin 0.0
Epoch 1, Class Loss=0.06777443736791611, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.06777443736791611, Class Loss=0.06777443736791611, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/105, Loss=3.9052326932549475
Loss made of: CE 0.1284339427947998, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.329407215118408 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.21767920255661
Loss made of: CE 0.07383420318365097, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.017297267913818 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.1317674532532696
Loss made of: CE 0.08080624043941498, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.209992408752441 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.023642798513174
Loss made of: CE 0.11312901228666306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.959792137145996 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.105762027949095
Loss made of: CE 0.15518569946289062, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.578174114227295 EntMin 0.0
Epoch 2, Batch 60/105, Loss=4.101579857617617
Loss made of: CE 0.1043427586555481, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6675679683685303 EntMin 0.0
Epoch 2, Batch 70/105, Loss=3.9112093918025495
Loss made of: CE 0.12009715288877487, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7757833003997803 EntMin 0.0
Epoch 2, Batch 80/105, Loss=3.9134969182312487
Loss made of: CE 0.10330472886562347, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.447155475616455 EntMin 0.0
Epoch 2, Batch 90/105, Loss=3.884294997155666
Loss made of: CE 0.12626847624778748, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4507088661193848 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.1704840861260895
Loss made of: CE 0.15061159431934357, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.98386812210083 EntMin 0.0
Epoch 2, Class Loss=0.1351124346256256, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.1351124346256256, Class Loss=0.1351124346256256, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/105, Loss=4.129467083513736
Loss made of: CE 0.2579021155834198, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7517871856689453 EntMin 0.0
Epoch 3, Batch 20/105, Loss=3.770836766064167
Loss made of: CE 0.21550697088241577, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.374859571456909 EntMin 0.0
Epoch 3, Batch 30/105, Loss=3.9189893707633017
Loss made of: CE 0.17541949450969696, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.238051414489746 EntMin 0.0
Epoch 3, Batch 40/105, Loss=4.0632409930229185
Loss made of: CE 0.19459711015224457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9473559856414795 EntMin 0.0
Epoch 3, Batch 50/105, Loss=3.8548718824982644
Loss made of: CE 0.2090444564819336, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3070523738861084 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.092092742025852
Loss made of: CE 0.21198713779449463, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4135589599609375 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.031909616291523
Loss made of: CE 0.24074816703796387, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9425103664398193 EntMin 0.0
Epoch 3, Batch 80/105, Loss=3.8995398208498955
Loss made of: CE 0.15024112164974213, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1216158866882324 EntMin 0.0
Epoch 3, Batch 90/105, Loss=3.9009929656982423
Loss made of: CE 0.2524285316467285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.973684310913086 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 100/105, Loss=4.1033188149333
Loss made of: CE 0.1299571692943573, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4944350719451904 EntMin 0.0
Epoch 3, Class Loss=0.20097336173057556, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.20097336173057556, Class Loss=0.20097336173057556, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/105, Loss=3.9897065699100493
Loss made of: CE 0.2592623829841614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.281160354614258 EntMin 0.0
Epoch 4, Batch 20/105, Loss=4.05662125647068
Loss made of: CE 0.27501440048217773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.491302490234375 EntMin 0.0
Epoch 4, Batch 30/105, Loss=3.9848830118775367
Loss made of: CE 0.2607685327529907, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8142776489257812 EntMin 0.0
Epoch 4, Batch 40/105, Loss=3.831152854859829
Loss made of: CE 0.295572966337204, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.836531639099121 EntMin 0.0
Epoch 4, Batch 50/105, Loss=3.9296844080090523
Loss made of: CE 0.2337709367275238, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.414641857147217 EntMin 0.0
Epoch 4, Batch 60/105, Loss=3.912754476070404
Loss made of: CE 0.3151465654373169, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.066322326660156 EntMin 0.0
Epoch 4, Batch 70/105, Loss=3.7876061707735063
Loss made of: CE 0.25643110275268555, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.246917486190796 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.126917999982834
Loss made of: CE 0.2359476089477539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.131743907928467 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.102084857225418
Loss made of: CE 0.24201606214046478, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2249484062194824 EntMin 0.0
Epoch 4, Batch 100/105, Loss=3.724750965833664
Loss made of: CE 0.2567994296550751, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0211589336395264 EntMin 0.0
Epoch 4, Class Loss=0.253690630197525, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.253690630197525, Class Loss=0.253690630197525, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/105, Loss=3.90266675055027
Loss made of: CE 0.30228906869888306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4907093048095703 EntMin 0.0
Epoch 5, Batch 20/105, Loss=4.046987034380436
Loss made of: CE 0.4347083568572998, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.374529838562012 EntMin 0.0
Epoch 5, Batch 30/105, Loss=3.825274407863617
Loss made of: CE 0.3798345923423767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5190236568450928 EntMin 0.0
Epoch 5, Batch 40/105, Loss=4.128806313872337
Loss made of: CE 0.31265050172805786, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.506730556488037 EntMin 0.0
Epoch 5, Batch 50/105, Loss=3.746160611510277
Loss made of: CE 0.2855992913246155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.253417730331421 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.179167771339417
Loss made of: CE 0.32761019468307495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.008398056030273 EntMin 0.0
Epoch 5, Batch 70/105, Loss=3.955616797506809
Loss made of: CE 0.3377823233604431, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.922511577606201 EntMin 0.0
Epoch 5, Batch 80/105, Loss=3.925446444749832
Loss made of: CE 0.2832379937171936, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.622892141342163 EntMin 0.0
Epoch 5, Batch 90/105, Loss=3.9998926639556887
Loss made of: CE 0.2749132215976715, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.933727979660034 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.073837965726852
Loss made of: CE 0.3352421224117279, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.208669662475586 EntMin 0.0
Epoch 5, Class Loss=0.32834768295288086, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.32834768295288086, Class Loss=0.32834768295288086, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/105, Loss=4.243652206659317
Loss made of: CE 0.32147330045700073, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.256803035736084 EntMin 0.0
Epoch 6, Batch 20/105, Loss=3.8787961095571517
Loss made of: CE 0.3280707001686096, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.361623525619507 EntMin 0.0
Epoch 6, Batch 30/105, Loss=3.789479100704193
Loss made of: CE 0.3652154803276062, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8826651573181152 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.062738558650016
Loss made of: CE 0.3027859926223755, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.08160924911499 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.1708078622818
Loss made of: CE 0.4213656187057495, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.538698196411133 EntMin 0.0
Epoch 6, Batch 60/105, Loss=3.8859635561704637
Loss made of: CE 0.34541162848472595, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.885985851287842 EntMin 0.0
Epoch 6, Batch 70/105, Loss=3.8857109576463698
Loss made of: CE 0.3986576497554779, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.280336380004883 EntMin 0.0
Epoch 6, Batch 80/105, Loss=3.970625728368759
Loss made of: CE 0.3105252683162689, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.770578384399414 EntMin 0.0
Epoch 6, Batch 90/105, Loss=3.761704206466675
Loss made of: CE 0.406206339597702, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.544231414794922 EntMin 0.0
Epoch 6, Batch 100/105, Loss=3.7779444754123688
Loss made of: CE 0.3301522731781006, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3450918197631836 EntMin 0.0
Epoch 6, Class Loss=0.3708014488220215, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.3708014488220215, Class Loss=0.3708014488220215, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.093083962798119
Loss made of: CE 0.42101413011550903, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.362093687057495 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.1529114812612535
Loss made of: CE 0.4934633672237396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.075499534606934 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.159137284755706
Loss made of: CE 0.338448166847229, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.684687376022339 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.1458685785532
Loss made of: CE 0.43643057346343994, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6617372035980225 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 50/102, Loss=4.271519419550896
Loss made of: CE 0.42287778854370117, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.225849151611328 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.186277943849563
Loss made of: CE 0.30911070108413696, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.551204204559326 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.24339334666729
Loss made of: CE 0.39618098735809326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8294265270233154 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.233660537004471
Loss made of: CE 0.39342769980430603, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.454191207885742 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.793599972128868
Loss made of: CE 0.325939416885376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.170231342315674 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.897377386689186
Loss made of: CE 0.4922068119049072, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4089365005493164 EntMin 0.0
Epoch 1, Class Loss=0.41220054030418396, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.41220054030418396, Class Loss=0.41220054030418396, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/102, Loss=4.220697525143623
Loss made of: CE 0.3529110550880432, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3024730682373047 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.058691528439522
Loss made of: CE 0.38137906789779663, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9262263774871826 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.039760708808899
Loss made of: CE 0.3873752951622009, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.486270904541016 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.056542176008224
Loss made of: CE 0.3724932372570038, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7769429683685303 EntMin 0.0
Epoch 2, Batch 50/102, Loss=3.9117344975471497
Loss made of: CE 0.3461322784423828, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4276692867279053 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.016407823562622
Loss made of: CE 0.35313257575035095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6359148025512695 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.35862580537796
Loss made of: CE 0.4454697370529175, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.349775314331055 EntMin 0.0
Epoch 2, Batch 80/102, Loss=3.893393775820732
Loss made of: CE 0.33765602111816406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3070993423461914 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.969108760356903
Loss made of: CE 0.3820936679840088, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2964329719543457 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.281547898054123
Loss made of: CE 0.4014330804347992, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.236809730529785 EntMin 0.0
Epoch 2, Class Loss=0.4044592082500458, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.4044592082500458, Class Loss=0.4044592082500458, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/102, Loss=4.380946624279022
Loss made of: CE 0.3760031461715698, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.574561595916748 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.065276816487312
Loss made of: CE 0.4652453362941742, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0270323753356934 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.9629382640123367
Loss made of: CE 0.4139198064804077, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6245055198669434 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.009889221191406
Loss made of: CE 0.35684800148010254, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.724687099456787 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.061994808912277
Loss made of: CE 0.4035618305206299, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.080446720123291 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.8433775335550306
Loss made of: CE 0.40426620841026306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2814369201660156 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.0842189460992815
Loss made of: CE 0.45176786184310913, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3831899166107178 EntMin 0.0
Epoch 3, Batch 80/102, Loss=3.8882768511772157
Loss made of: CE 0.44127923250198364, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4105873107910156 EntMin 0.0
Epoch 3, Batch 90/102, Loss=3.9879461765289306
Loss made of: CE 0.40688556432724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2521047592163086 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.772747242450714
Loss made of: CE 0.35060226917266846, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.312190532684326 EntMin 0.0
Epoch 3, Class Loss=0.3968268632888794, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3968268632888794, Class Loss=0.3968268632888794, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=4.140738096833229
Loss made of: CE 0.35189908742904663, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7539925575256348 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.9257910788059234
Loss made of: CE 0.4031613767147064, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.629777669906616 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.070885464549065
Loss made of: CE 0.3632916212081909, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.136826515197754 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.988362154364586
Loss made of: CE 0.3202301859855652, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4766159057617188 EntMin 0.0
Epoch 4, Batch 50/102, Loss=3.896966627240181
Loss made of: CE 0.4039077162742615, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3237199783325195 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.096050554513932
Loss made of: CE 0.41048091650009155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7711119651794434 EntMin 0.0
Epoch 4, Batch 70/102, Loss=3.9315691769123076
Loss made of: CE 0.3877955675125122, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5689144134521484 EntMin 0.0
Epoch 4, Batch 80/102, Loss=3.9708257585763933
Loss made of: CE 0.4153560996055603, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.435791015625 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.063517886400223
Loss made of: CE 0.45364856719970703, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7921805381774902 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.2212263405323025
Loss made of: CE 0.413752019405365, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4580752849578857 EntMin 0.0
Epoch 4, Class Loss=0.4033737778663635, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4033737778663635, Class Loss=0.4033737778663635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/102, Loss=3.750362882018089
Loss made of: CE 0.4128350615501404, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.436619520187378 EntMin 0.0
Epoch 5, Batch 20/102, Loss=3.9819267272949217
Loss made of: CE 0.4179205298423767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.526705503463745 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.132537943124771
Loss made of: CE 0.3731761872768402, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3749217987060547 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.978885120153427
Loss made of: CE 0.392814964056015, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2137928009033203 EntMin 0.0
Epoch 5, Batch 50/102, Loss=3.8717811733484266
Loss made of: CE 0.429634153842926, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8418655395507812 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.8738108307123182
Loss made of: CE 0.3650898337364197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0236105918884277 EntMin 0.0
Epoch 5, Batch 70/102, Loss=3.9810318410396577
Loss made of: CE 0.3728935122489929, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7881598472595215 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.0479600697755815
Loss made of: CE 0.3516070544719696, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.624472141265869 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.937512055039406
Loss made of: CE 0.507629930973053, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5050315856933594 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.8991565644741057
Loss made of: CE 0.38629645109176636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.295122146606445 EntMin 0.0
Epoch 5, Class Loss=0.3905712366104126, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.3905712366104126, Class Loss=0.3905712366104126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/102, Loss=3.9301646023988726
Loss made of: CE 0.39742183685302734, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0363292694091797 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.130197665095329
Loss made of: CE 0.3666107654571533, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.039775848388672 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.613488695025444
Loss made of: CE 0.34617647528648376, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.9135098457336426 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.8835044264793397
Loss made of: CE 0.3300434648990631, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0674519538879395 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.980113059282303
Loss made of: CE 0.4788580536842346, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5220658779144287 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.8440959841012954
Loss made of: CE 0.36415159702301025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3198070526123047 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.830232006311417
Loss made of: CE 0.3933131694793701, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5483012199401855 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.7914749801158907
Loss made of: CE 0.4208002984523773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.670309543609619 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.8874296694993973
Loss made of: CE 0.4276880621910095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.412830114364624 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.844995805621147
Loss made of: CE 0.41037434339523315, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0216078758239746 EntMin 0.0
Epoch 6, Class Loss=0.3873404860496521, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.3873404860496521, Class Loss=0.3873404860496521, Reg Loss=0.0
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/23, Loss=5.100832015275955
Loss made of: CE 0.5716133117675781, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.226000785827637 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.602823358774185
Loss made of: CE 0.5872931480407715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.06451416015625 EntMin 0.0
Epoch 1, Class Loss=0.629825234413147, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.629825234413147, Class Loss=0.629825234413147, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/23, Loss=4.564630165696144
Loss made of: CE 0.6156815886497498, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1168951988220215 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.438681107759476
Loss made of: CE 0.3956215977668762, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8543100357055664 EntMin 0.0
Epoch 2, Class Loss=0.5419328212738037, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.5419328212738037, Class Loss=0.5419328212738037, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/23, Loss=4.385771504044532
Loss made of: CE 0.3561217188835144, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6843514442443848 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.498045763373375
Loss made of: CE 0.5597788095474243, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.214273929595947 EntMin 0.0
Epoch 3, Class Loss=0.4835965633392334, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.4835965633392334, Class Loss=0.4835965633392334, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/23, Loss=4.429873958230019
Loss made of: CE 0.6454236507415771, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4975099563598633 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.338453340530395
Loss made of: CE 0.3590153157711029, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.750001907348633 EntMin 0.0
Epoch 4, Class Loss=0.47364336252212524, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.47364336252212524, Class Loss=0.47364336252212524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/23, Loss=4.357885292172432
Loss made of: CE 0.4170805811882019, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.981616735458374 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.167427471280098
Loss made of: CE 0.34708285331726074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8862195014953613 EntMin 0.0
Epoch 5, Class Loss=0.4649697542190552, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.4649697542190552, Class Loss=0.4649697542190552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/23, Loss=4.269532683491707
Loss made of: CE 0.4484807848930359, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7524125576019287 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.206422632932663
Loss made of: CE 0.4890279769897461, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.316186428070068 EntMin 0.0
Epoch 6, Class Loss=0.43760383129119873, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.43760383129119873, Class Loss=0.43760383129119873, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/103, Loss=3.604813647270203
Loss made of: CE 0.08381378650665283, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.491227865219116 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/103, Loss=3.8099964290857313
Loss made of: CE 0.05461964011192322, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6960608959198 EntMin 0.0
Epoch 1, Batch 30/103, Loss=3.740006397664547
Loss made of: CE 0.0818556621670723, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.760727882385254 EntMin 0.0
Epoch 1, Batch 40/103, Loss=3.9635626431554556
Loss made of: CE 0.05282127857208252, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.370392322540283 EntMin 0.0
Epoch 1, Batch 50/103, Loss=4.066133949533105
Loss made of: CE 0.09559007734060287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5393805503845215 EntMin 0.0
Epoch 1, Batch 60/103, Loss=3.88030289337039
Loss made of: CE 0.06093970686197281, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1950201988220215 EntMin 0.0
Epoch 1, Batch 70/103, Loss=3.759169503673911
Loss made of: CE 0.04365678131580353, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.697530746459961 EntMin 0.0
Epoch 1, Batch 80/103, Loss=4.085485130921006
Loss made of: CE 0.11048563569784164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.22075891494751 EntMin 0.0
Epoch 1, Batch 90/103, Loss=3.6685749534517527
Loss made of: CE 0.06468039751052856, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.097834348678589 EntMin 0.0
Epoch 1, Batch 100/103, Loss=3.899741180241108
Loss made of: CE 0.053371042013168335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6142568588256836 EntMin 0.0
Epoch 1, Class Loss=0.0637814849615097, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.0637814849615097, Class Loss=0.0637814849615097, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/103, Loss=3.880187647789717
Loss made of: CE 0.09150569885969162, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.413394927978516 EntMin 0.0
Epoch 2, Batch 20/103, Loss=4.174013838917017
Loss made of: CE 0.1398199200630188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6102943420410156 EntMin 0.0
Epoch 2, Batch 30/103, Loss=3.8812830060720445
Loss made of: CE 0.09930788725614548, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0889434814453125 EntMin 0.0
Epoch 2, Batch 40/103, Loss=3.9717593863606453
Loss made of: CE 0.15593591332435608, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0231218338012695 EntMin 0.0
Epoch 2, Batch 50/103, Loss=3.739363870769739
Loss made of: CE 0.1058264821767807, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.258073568344116 EntMin 0.0
Epoch 2, Batch 60/103, Loss=4.206854915618896
Loss made of: CE 0.09322743862867355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.301766395568848 EntMin 0.0
Epoch 2, Batch 70/103, Loss=3.863768608868122
Loss made of: CE 0.13057392835617065, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.843139171600342 EntMin 0.0
Epoch 2, Batch 80/103, Loss=3.976230423897505
Loss made of: CE 0.15118715167045593, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.286159515380859 EntMin 0.0
Epoch 2, Batch 90/103, Loss=3.9968152716755867
Loss made of: CE 0.15784963965415955, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.443167209625244 EntMin 0.0
Epoch 2, Batch 100/103, Loss=4.214815738797188
Loss made of: CE 0.10766085237264633, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8310632705688477 EntMin 0.0
Epoch 2, Class Loss=0.12490236759185791, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.12490236759185791, Class Loss=0.12490236759185791, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/103, Loss=4.045924493670464
Loss made of: CE 0.20428328216075897, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3509862422943115 EntMin 0.0
Epoch 3, Batch 20/103, Loss=3.9488191388547422
Loss made of: CE 0.18055859208106995, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5963058471679688 EntMin 0.0
Epoch 3, Batch 30/103, Loss=3.9756993137300016
Loss made of: CE 0.19099543988704681, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4662275314331055 EntMin 0.0
Epoch 3, Batch 40/103, Loss=3.991989305615425
Loss made of: CE 0.12365663051605225, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.790825366973877 EntMin 0.0
Epoch 3, Batch 50/103, Loss=4.194904761016369
Loss made of: CE 0.24184322357177734, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.696723937988281 EntMin 0.0
Epoch 3, Batch 60/103, Loss=3.975139120221138
Loss made of: CE 0.14272865653038025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4276795387268066 EntMin 0.0
Epoch 3, Batch 70/103, Loss=4.165571239590645
Loss made of: CE 0.1585453450679779, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.34380578994751 EntMin 0.0
Epoch 3, Batch 80/103, Loss=3.8368754640221594
Loss made of: CE 0.15883079171180725, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6555957794189453 EntMin 0.0
Epoch 3, Batch 90/103, Loss=3.9493505366146566
Loss made of: CE 0.16329547762870789, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4680068492889404 EntMin 0.0
Epoch 3, Batch 100/103, Loss=3.8841282084584234
Loss made of: CE 0.23865985870361328, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.063019752502441 EntMin 0.0
Epoch 3, Class Loss=0.17718537151813507, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.17718537151813507, Class Loss=0.17718537151813507, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/103, Loss=4.004091839492321
Loss made of: CE 0.2618366479873657, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3149561882019043 EntMin 0.0
Epoch 4, Batch 20/103, Loss=3.9889509066939355
Loss made of: CE 0.28926223516464233, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.564424514770508 EntMin 0.0
Epoch 4, Batch 30/103, Loss=3.963029311597347
Loss made of: CE 0.2943764925003052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.417215824127197 EntMin 0.0
Epoch 4, Batch 40/103, Loss=3.7638323098421096
Loss made of: CE 0.24663925170898438, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.631258964538574 EntMin 0.0
Epoch 4, Batch 50/103, Loss=4.156719870865345
Loss made of: CE 0.24086475372314453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.056036949157715 EntMin 0.0
Epoch 4, Batch 60/103, Loss=3.89178211838007
Loss made of: CE 0.20850853621959686, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4530885219573975 EntMin 0.0
Epoch 4, Batch 70/103, Loss=3.8999023720622064
Loss made of: CE 0.2787325978279114, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7212204933166504 EntMin 0.0
Epoch 4, Batch 80/103, Loss=3.9575673073530195
Loss made of: CE 0.23797091841697693, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.398895740509033 EntMin 0.0
Epoch 4, Batch 90/103, Loss=3.827724041044712
Loss made of: CE 0.21567842364311218, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.534147024154663 EntMin 0.0
Epoch 4, Batch 100/103, Loss=4.120148369669915
Loss made of: CE 0.29294532537460327, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5067434310913086 EntMin 0.0
Epoch 4, Class Loss=0.25498032569885254, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.25498032569885254, Class Loss=0.25498032569885254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/103, Loss=3.986479805409908
Loss made of: CE 0.268343061208725, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.63147234916687 EntMin 0.0
Epoch 5, Batch 20/103, Loss=4.013164287805557
Loss made of: CE 0.3543817400932312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.600859642028809 EntMin 0.0
Epoch 5, Batch 30/103, Loss=3.8108691543340685
Loss made of: CE 0.3740244507789612, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.99052095413208 EntMin 0.0
Epoch 5, Batch 40/103, Loss=4.008621805906296
Loss made of: CE 0.28151631355285645, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.045121669769287 EntMin 0.0
Epoch 5, Batch 50/103, Loss=4.029522357881069
Loss made of: CE 0.24787738919258118, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3447694778442383 EntMin 0.0
Epoch 5, Batch 60/103, Loss=3.9357463628053666
Loss made of: CE 0.39507073163986206, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.739813804626465 EntMin 0.0
Epoch 5, Batch 70/103, Loss=3.8918223172426223
Loss made of: CE 0.3378273844718933, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5481650829315186 EntMin 0.0
Epoch 5, Batch 80/103, Loss=4.132177752256394
Loss made of: CE 0.2742394804954529, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.424842357635498 EntMin 0.0
Epoch 5, Batch 90/103, Loss=3.959589201211929
Loss made of: CE 0.3510459065437317, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.552708148956299 EntMin 0.0
Epoch 5, Batch 100/103, Loss=4.048578500747681
Loss made of: CE 0.2356705367565155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.446434497833252 EntMin 0.0
Epoch 5, Class Loss=0.32087403535842896, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.32087403535842896, Class Loss=0.32087403535842896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/103, Loss=3.879932764172554
Loss made of: CE 0.3546150326728821, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2373275756835938 EntMin 0.0
Epoch 6, Batch 20/103, Loss=3.9449211776256563
Loss made of: CE 0.32356274127960205, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0713577270507812 EntMin 0.0
Epoch 6, Batch 30/103, Loss=3.8911782085895537
Loss made of: CE 0.33545875549316406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4719302654266357 EntMin 0.0
Epoch 6, Batch 40/103, Loss=3.830014857649803
Loss made of: CE 0.3021964430809021, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1558260917663574 EntMin 0.0
Epoch 6, Batch 50/103, Loss=3.9165474355220793
Loss made of: CE 0.30977219343185425, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.470724582672119 EntMin 0.0
Epoch 6, Batch 60/103, Loss=3.8899106055498125
Loss made of: CE 0.3373645544052124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304592609405518 EntMin 0.0
Epoch 6, Batch 70/103, Loss=4.187785828113556
Loss made of: CE 0.31325721740722656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5094351768493652 EntMin 0.0
Epoch 6, Batch 80/103, Loss=3.9984975516796113
Loss made of: CE 0.4210180640220642, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4986140727996826 EntMin 0.0
Epoch 6, Batch 90/103, Loss=4.081351256370544
Loss made of: CE 0.3303242325782776, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.339250087738037 EntMin 0.0
Epoch 6, Batch 100/103, Loss=3.945707356929779
Loss made of: CE 0.32625454664230347, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6385531425476074 EntMin 0.0
Epoch 6, Class Loss=0.3600861728191376, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.3600861728191376, Class Loss=0.3600861728191376, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4219929277896881, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.877107
Mean Acc: 0.646245
FreqW Acc: 0.796027
Mean IoU: 0.495912
Class IoU:
	class 0: 0.8901239
	class 1: 0.7418905
	class 2: 0.33317766
	class 3: 0.47106346
	class 4: 0.41036606
	class 5: 0.19304927
	class 6: 0.8104499
	class 7: 0.77291477
	class 8: 0.7176302
	class 9: 0.0
	class 10: 0.37838224
	class 11: 0.305682
	class 12: 0.5737389
	class 13: 0.44043487
	class 14: 0.6988757
	class 15: 0.69272476
	class 16: 0.0
Class Acc:
	class 0: 0.950826
	class 1: 0.7824022
	class 2: 0.81303364
	class 3: 0.93461746
	class 4: 0.91969913
	class 5: 0.19566692
	class 6: 0.83546406
	class 7: 0.85755557
	class 8: 0.77769893
	class 9: 0.0
	class 10: 0.39988056
	class 11: 0.47310928
	class 12: 0.73853683
	class 13: 0.6629361
	class 14: 0.81679434
	class 15: 0.8279505
	class 16: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[19, 23, 1, 8]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=9.012309038639069
Loss made of: CE 0.9102061986923218, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.9069366455078125 EntMin 0.0
Epoch 1, Batch 20/24, Loss=7.772242164611816
Loss made of: CE 0.5803154110908508, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9467058181762695 EntMin 0.0
Epoch 1, Class Loss=0.7922963500022888, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.7922963500022888, Class Loss=0.7922963500022888, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=7.6299335896968845
Loss made of: CE 0.8043603897094727, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.16722297668457 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.9139619171619415
Loss made of: CE 0.742163896560669, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.472520351409912 EntMin 0.0
Epoch 2, Class Loss=0.7713907957077026, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.7713907957077026, Class Loss=0.7713907957077026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=6.4333640992641445
Loss made of: CE 0.6949964761734009, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6504411697387695 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 20/24, Loss=6.270977509021759
Loss made of: CE 0.5974063873291016, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.815771102905273 EntMin 0.0
Epoch 3, Class Loss=0.714911162853241, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.714911162853241, Class Loss=0.714911162853241, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=6.065915989875793
Loss made of: CE 0.723164975643158, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.422380447387695 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.41520082950592
Loss made of: CE 0.6374667882919312, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.394814491271973 EntMin 0.0
Epoch 4, Class Loss=0.7085202932357788, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.7085202932357788, Class Loss=0.7085202932357788, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=5.811489665508271
Loss made of: CE 0.730066180229187, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0308380126953125 EntMin 0.0
Epoch 5, Batch 20/24, Loss=6.018111842870712
Loss made of: CE 0.5973809957504272, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.588363170623779 EntMin 0.0
Epoch 5, Class Loss=0.6801055669784546, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.6801055669784546, Class Loss=0.6801055669784546, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=5.969836497306824
Loss made of: CE 0.6713223457336426, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.988637924194336 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.500133538246155
Loss made of: CE 0.6530442237854004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6082658767700195 EntMin 0.0
Epoch 6, Class Loss=0.6651706099510193, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.6651706099510193, Class Loss=0.6651706099510193, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=7.988635230064392
Loss made of: CE 0.5727389454841614, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.654144287109375 EntMin 0.0
Epoch 1, Batch 20/21, Loss=7.043379563093185
Loss made of: CE 0.616625964641571, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.951398849487305 EntMin 0.0
Epoch 1, Class Loss=0.5555190443992615, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.5555190443992615, Class Loss=0.5555190443992615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/21, Loss=6.4587438106536865
Loss made of: CE 0.6315850615501404, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.267549514770508 EntMin 0.0
Epoch 2, Batch 20/21, Loss=6.023444432020187
Loss made of: CE 0.5551031827926636, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.844146728515625 EntMin 0.0
Epoch 2, Class Loss=0.6274478435516357, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.6274478435516357, Class Loss=0.6274478435516357, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/21, Loss=6.440032088756562
Loss made of: CE 0.5375875234603882, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.952322483062744 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.973168593645096
Loss made of: CE 0.6282212734222412, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.626458168029785 EntMin 0.0
Epoch 3, Class Loss=0.6771071553230286, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.6771071553230286, Class Loss=0.6771071553230286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/21, Loss=6.148164784908294
Loss made of: CE 0.8225116729736328, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.289271354675293 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.685971534252166
Loss made of: CE 0.5613731741905212, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0268473625183105 EntMin 0.0
Epoch 4, Class Loss=0.6926060914993286, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.6926060914993286, Class Loss=0.6926060914993286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/21, Loss=5.893808525800705
Loss made of: CE 0.7248935103416443, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.150191783905029 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.739260488748551
Loss made of: CE 0.7321954965591431, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.729934215545654 EntMin 0.0
Epoch 5, Class Loss=0.7135127782821655, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.7135127782821655, Class Loss=0.7135127782821655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/21, Loss=5.950814282894134
Loss made of: CE 0.6704058647155762, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.218782424926758 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.5215246975421906
Loss made of: CE 0.7559347152709961, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.495850563049316 EntMin 0.0
Epoch 6, Class Loss=0.7069918513298035, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.7069918513298035, Class Loss=0.7069918513298035, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=10.096055281162261
Loss made of: CE 0.9121010303497314, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.771390914916992 EntMin 0.0
Epoch 1, Class Loss=0.8876422047615051, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.8876422047615051, Class Loss=0.8876422047615051, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/19, Loss=8.117586052417755
Loss made of: CE 0.8513766527175903, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.316793441772461 EntMin 0.0
Epoch 2, Class Loss=0.8688313364982605, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.8688313364982605, Class Loss=0.8688313364982605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=7.449443131685257
Loss made of: CE 0.8769339323043823, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.876584053039551 EntMin 0.0
Epoch 3, Class Loss=0.7979978322982788, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.7979978322982788, Class Loss=0.7979978322982788, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=7.046763044595719
Loss made of: CE 0.6722189784049988, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.495655059814453 EntMin 0.0
Epoch 4, Class Loss=0.7267217636108398, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.7267217636108398, Class Loss=0.7267217636108398, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=6.670214724540711
Loss made of: CE 0.6483756303787231, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0432868003845215 EntMin 0.0
Epoch 5, Class Loss=0.6459320783615112, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.6459320783615112, Class Loss=0.6459320783615112, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=6.404626369476318
Loss made of: CE 0.5817322134971619, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.337460994720459 EntMin 0.0
Epoch 6, Class Loss=0.5939647555351257, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.5939647555351257, Class Loss=0.5939647555351257, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=8.893602114915847
Loss made of: CE 0.3357476592063904, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.272151947021484 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=7.454177072644233
Loss made of: CE 0.3194914758205414, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.026238441467285 EntMin 0.0
Epoch 1, Class Loss=0.49409541487693787, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.49409541487693787, Class Loss=0.49409541487693787, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=6.9056556046009065
Loss made of: CE 0.5542912483215332, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.562951564788818 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.561047691106796
Loss made of: CE 0.5604011416435242, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.298388481140137 EntMin 0.0
Epoch 2, Class Loss=0.5693761110305786, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.5693761110305786, Class Loss=0.5693761110305786, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=6.70800411105156
Loss made of: CE 0.7123743295669556, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.813186168670654 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.258772769570351
Loss made of: CE 0.4894278943538666, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.134971618652344 EntMin 0.0
Epoch 3, Class Loss=0.6219580173492432, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.6219580173492432, Class Loss=0.6219580173492432, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=6.22819162607193
Loss made of: CE 0.7492538690567017, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.88763952255249 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.243536603450775
Loss made of: CE 0.5422475337982178, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.031585216522217 EntMin 0.0
Epoch 4, Class Loss=0.6718757152557373, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.6718757152557373, Class Loss=0.6718757152557373, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=6.363669109344483
Loss made of: CE 0.9478386640548706, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.141714096069336 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.966257536411286
Loss made of: CE 0.6126251220703125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.790450096130371 EntMin 0.0
Epoch 5, Class Loss=0.7091169357299805, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.7091169357299805, Class Loss=0.7091169357299805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=6.182331830263138
Loss made of: CE 0.7590551376342773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.961569786071777 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.883160364627838
Loss made of: CE 0.6690999865531921, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.252345561981201 EntMin 0.0
Epoch 6, Class Loss=0.7351710796356201, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.7351710796356201, Class Loss=0.7351710796356201, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.6452590823173523, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.833252
Mean Acc: 0.469057
FreqW Acc: 0.715501
Mean IoU: 0.358596
Class IoU:
	class 0: 0.8384592
	class 1: 0.4761487
	class 2: 0.32871243
	class 3: 0.49307695
	class 4: 0.3154507
	class 5: 0.046914246
	class 6: 0.67857057
	class 7: 0.5853234
	class 8: 0.6993012
	class 9: 0.0
	class 10: 0.3746925
	class 11: 0.3379299
	class 12: 0.5709221
	class 13: 0.41030702
	class 14: 0.6595838
	class 15: 0.71512574
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0
Class Acc:
	class 0: 0.96958154
	class 1: 0.48388532
	class 2: 0.64745724
	class 3: 0.8807743
	class 4: 0.8825528
	class 5: 0.046920635
	class 6: 0.7305035
	class 7: 0.59719086
	class 8: 0.77490956
	class 9: 0.0
	class 10: 0.45620358
	class 11: 0.5248551
	class 12: 0.6928346
	class 13: 0.60035634
	class 14: 0.77482444
	class 15: 0.78735304
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[23, 14, 4, 11]
Current Client Index:  23
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=6.527158880233765
Loss made of: CE 0.9639022350311279, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8902812004089355 EntMin 0.0
Epoch 1, Batch 20/21, Loss=6.220229136943817
Loss made of: CE 0.8991227149963379, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.296599864959717 EntMin 0.0
Epoch 1, Class Loss=0.9725589752197266, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.9725589752197266, Class Loss=0.9725589752197266, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/21, Loss=5.729935950040817
Loss made of: CE 0.805755615234375, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.17098331451416 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.656158173084259
Loss made of: CE 0.7473578453063965, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.158116817474365 EntMin 0.0
Epoch 2, Class Loss=0.8424375653266907, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.8424375653266907, Class Loss=0.8424375653266907, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/21, Loss=5.940550655126572
Loss made of: CE 0.7343591451644897, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.885636329650879 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.534121376276016
Loss made of: CE 0.8268710374832153, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.218506813049316 EntMin 0.0
Epoch 3, Class Loss=0.7472447156906128, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.7472447156906128, Class Loss=0.7472447156906128, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/21, Loss=5.850051087141037
Loss made of: CE 0.7067829966545105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.888494491577148 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.183085161447525
Loss made of: CE 0.6024190187454224, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.052318572998047 EntMin 0.0
Epoch 4, Class Loss=0.6832083463668823, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.6832083463668823, Class Loss=0.6832083463668823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=5.611218279600143
Loss made of: CE 0.661109209060669, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.93365478515625 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.262703451514244
Loss made of: CE 0.6906808614730835, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.37863826751709 EntMin 0.0
Epoch 5, Class Loss=0.6440311074256897, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.6440311074256897, Class Loss=0.6440311074256897, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/21, Loss=5.428155952692032
Loss made of: CE 0.5669407248497009, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.399423122406006 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.299723947048188
Loss made of: CE 0.6759775876998901, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.321071624755859 EntMin 0.0
Epoch 6, Class Loss=0.6215341091156006, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.6215341091156006, Class Loss=0.6215341091156006, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=6.897145789861679
Loss made of: CE 0.4740307927131653, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.665462017059326 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/26, Loss=6.272035837173462
Loss made of: CE 0.3704586923122406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8125529289245605 EntMin 0.0
Epoch 1, Class Loss=0.4539737105369568, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.4539737105369568, Class Loss=0.4539737105369568, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=6.407479625940323
Loss made of: CE 0.48547595739364624, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.739469528198242 EntMin 0.0
Epoch 2, Batch 20/26, Loss=6.00075760781765
Loss made of: CE 0.7084447741508484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.966017723083496 EntMin 0.0
Epoch 2, Class Loss=0.48634248971939087, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.48634248971939087, Class Loss=0.48634248971939087, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=5.929527008533478
Loss made of: CE 0.522533118724823, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.34852933883667 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.62898580133915
Loss made of: CE 0.40044164657592773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.688381195068359 EntMin 0.0
Epoch 3, Class Loss=0.4970894753932953, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.4970894753932953, Class Loss=0.4970894753932953, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.489308834075928
Loss made of: CE 0.5341389179229736, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.357846736907959 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.440161362290382
Loss made of: CE 0.5280463695526123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.659632205963135 EntMin 0.0
Epoch 4, Class Loss=0.5148289799690247, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.5148289799690247, Class Loss=0.5148289799690247, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.432051900029182
Loss made of: CE 0.7584787607192993, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.892133712768555 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.449993994832039
Loss made of: CE 0.6265474557876587, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.248935222625732 EntMin 0.0
Epoch 5, Class Loss=0.5663062930107117, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5663062930107117, Class Loss=0.5663062930107117, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.523746544122696
Loss made of: CE 0.618679940700531, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.689323425292969 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.403933817148209
Loss made of: CE 0.4914647936820984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.896991729736328 EntMin 0.0
Epoch 6, Class Loss=0.5921025276184082, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5921025276184082, Class Loss=0.5921025276184082, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=6.502775084972382
Loss made of: CE 0.4214974343776703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.90103816986084 EntMin 0.0
Epoch 1, Batch 20/26, Loss=6.582504320144653
Loss made of: CE 0.43939775228500366, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.527655601501465 EntMin 0.0
Epoch 1, Class Loss=0.4437152147293091, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.4437152147293091, Class Loss=0.4437152147293091, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=5.8022760331630705
Loss made of: CE 0.5167743563652039, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581906318664551 EntMin 0.0
Epoch 2, Batch 20/26, Loss=6.011674448847771
Loss made of: CE 0.4881945848464966, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.52495002746582 EntMin 0.0
Epoch 2, Class Loss=0.4980294406414032, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.4980294406414032, Class Loss=0.4980294406414032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=5.83715868294239
Loss made of: CE 0.4672677218914032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.855434417724609 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.601023310422898
Loss made of: CE 0.4766918122768402, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.950128078460693 EntMin 0.0
Epoch 3, Class Loss=0.4995008707046509, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.4995008707046509, Class Loss=0.4995008707046509, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.479039165377617
Loss made of: CE 0.4873879849910736, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.509316921234131 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.329137825965882
Loss made of: CE 0.5005953311920166, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.065688133239746 EntMin 0.0
Epoch 4, Class Loss=0.5539363026618958, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5539363026618958, Class Loss=0.5539363026618958, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.462472578883171
Loss made of: CE 0.4959707260131836, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.360936641693115 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.5661461532115935
Loss made of: CE 0.5118613243103027, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.764836311340332 EntMin 0.0
Epoch 5, Class Loss=0.560871422290802, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.560871422290802, Class Loss=0.560871422290802, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.750577929615974
Loss made of: CE 0.4705648422241211, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7586894035339355 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.2746506243944165
Loss made of: CE 0.5858702659606934, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.341732501983643 EntMin 0.0
Epoch 6, Class Loss=0.5697323679924011, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5697323679924011, Class Loss=0.5697323679924011, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=6.411570917069912
Loss made of: CE 0.4330691695213318, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.356681823730469 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=6.232582339644432
Loss made of: CE 0.48275625705718994, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.04189395904541 EntMin 0.0
Epoch 1, Class Loss=0.3527182340621948, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.3527182340621948, Class Loss=0.3527182340621948, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/24, Loss=5.981874543428421
Loss made of: CE 0.3997827470302582, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.121947288513184 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.063285148143768
Loss made of: CE 0.3139164447784424, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.047691822052002 EntMin 0.0
Epoch 2, Class Loss=0.42564648389816284, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.42564648389816284, Class Loss=0.42564648389816284, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/24, Loss=5.790379801392556
Loss made of: CE 0.5207518339157104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.768558502197266 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.022189298272133
Loss made of: CE 0.3283710181713104, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.441254615783691 EntMin 0.0
Epoch 3, Class Loss=0.4966704845428467, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4966704845428467, Class Loss=0.4966704845428467, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/24, Loss=5.5479391425848
Loss made of: CE 0.5746099948883057, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.660971641540527 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.056072491407394
Loss made of: CE 0.6246719360351562, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.278278350830078 EntMin 0.0
Epoch 4, Class Loss=0.5804453492164612, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5804453492164612, Class Loss=0.5804453492164612, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/24, Loss=5.83367753624916
Loss made of: CE 0.6398219466209412, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.815125465393066 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.754681235551834
Loss made of: CE 0.5078845024108887, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.345816612243652 EntMin 0.0
Epoch 5, Class Loss=0.6146671175956726, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.6146671175956726, Class Loss=0.6146671175956726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/24, Loss=5.554226303100586
Loss made of: CE 0.665040135383606, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.390150547027588 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.706396371126175
Loss made of: CE 0.6957553625106812, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.088109493255615 EntMin 0.0
Epoch 6, Class Loss=0.636225700378418, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.636225700378418, Class Loss=0.636225700378418, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5810924172401428, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.833260
Mean Acc: 0.470944
FreqW Acc: 0.715594
Mean IoU: 0.366278
Class IoU:
	class 0: 0.83771455
	class 1: 0.57414985
	class 2: 0.3299192
	class 3: 0.5078748
	class 4: 0.31952515
	class 5: 0.04803483
	class 6: 0.45975035
	class 7: 0.6606759
	class 8: 0.71019906
	class 9: 0.0
	class 10: 0.33666608
	class 11: 0.32430092
	class 12: 0.58360887
	class 13: 0.39056253
	class 14: 0.66082686
	class 15: 0.70187896
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.16477841
	class 20: 0.081379704
Class Acc:
	class 0: 0.97193664
	class 1: 0.5881943
	class 2: 0.6073815
	class 3: 0.8248157
	class 4: 0.8901056
	class 5: 0.04803602
	class 6: 0.4679454
	class 7: 0.6855307
	class 8: 0.7950076
	class 9: 0.0
	class 10: 0.40356043
	class 11: 0.51740146
	class 12: 0.7230601
	class 13: 0.5994786
	class 14: 0.76569164
	class 15: 0.75078475
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.167977
	class 20: 0.08291645

federated global round: 22, step: 4
select part of clients to conduct local training
[0, 8, 16, 14]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.188400329649449
Loss made of: CE 0.3320237398147583, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.331621170043945 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.831296077370643
Loss made of: CE 0.3699038028717041, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.793920516967773 EntMin 0.0
Epoch 1, Class Loss=0.2933037579059601, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.2933037579059601, Class Loss=0.2933037579059601, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/24, Loss=5.568216119706631
Loss made of: CE 0.34157595038414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.841521263122559 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.346710839867592
Loss made of: CE 0.3051533102989197, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.348527431488037 EntMin 0.0
Epoch 2, Class Loss=0.3430044651031494, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.3430044651031494, Class Loss=0.3430044651031494, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/24, Loss=5.463934475183487
Loss made of: CE 0.4532284140586853, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.173164367675781 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.920934829115867
Loss made of: CE 0.4108341336250305, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.934988021850586 EntMin 0.0
Epoch 3, Class Loss=0.40267759561538696, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.40267759561538696, Class Loss=0.40267759561538696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/24, Loss=5.270133450627327
Loss made of: CE 0.5577336549758911, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.09725284576416 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.740800657868386
Loss made of: CE 0.5509395599365234, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7086100578308105 EntMin 0.0
Epoch 4, Class Loss=0.47775590419769287, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.47775590419769287, Class Loss=0.47775590419769287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/24, Loss=5.825508332252502
Loss made of: CE 0.5952579975128174, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.613156318664551 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.510861113667488
Loss made of: CE 0.5904948711395264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.994788646697998 EntMin 0.0
Epoch 5, Class Loss=0.5416626334190369, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.5416626334190369, Class Loss=0.5416626334190369, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/24, Loss=5.532403641939164
Loss made of: CE 0.6245864629745483, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.303197860717773 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.459972324967384
Loss made of: CE 0.45307597517967224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.013938903808594 EntMin 0.0
Epoch 6, Class Loss=0.582832932472229, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.582832932472229, Class Loss=0.582832932472229, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/24, Loss=6.698368966579437
Loss made of: CE 0.969759464263916, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0774407386779785 EntMin 0.0
Epoch 1, Batch 20/24, Loss=6.001381540298462
Loss made of: CE 0.7120606303215027, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.434825420379639 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.8716050386428833, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.8716050386428833, Class Loss=0.8716050386428833, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/24, Loss=5.953858172893524
Loss made of: CE 0.6643686294555664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.791796684265137 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.769821131229401
Loss made of: CE 0.6902467608451843, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.263437271118164 EntMin 0.0
Epoch 2, Class Loss=0.7741644382476807, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.7741644382476807, Class Loss=0.7741644382476807, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/24, Loss=5.775203782320022
Loss made of: CE 0.6476576328277588, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8656744956970215 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.734170401096344
Loss made of: CE 0.7439980506896973, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.850527286529541 EntMin 0.0
Epoch 3, Class Loss=0.6895405054092407, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.6895405054092407, Class Loss=0.6895405054092407, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/24, Loss=5.6667442440986635
Loss made of: CE 0.6987271308898926, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.340486526489258 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.669440889358521
Loss made of: CE 0.6210951209068298, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.373137950897217 EntMin 0.0
Epoch 4, Class Loss=0.6681951880455017, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.6681951880455017, Class Loss=0.6681951880455017, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/24, Loss=5.815978789329529
Loss made of: CE 0.7735562324523926, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.055234432220459 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.340932589769364
Loss made of: CE 0.622063159942627, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.614284992218018 EntMin 0.0
Epoch 5, Class Loss=0.6304733753204346, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6304733753204346, Class Loss=0.6304733753204346, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/24, Loss=5.576106464862823
Loss made of: CE 0.6737629771232605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.661054611206055 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.334694361686706
Loss made of: CE 0.571164071559906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.638174057006836 EntMin 0.0
Epoch 6, Class Loss=0.6245104074478149, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6245104074478149, Class Loss=0.6245104074478149, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=8.227361214160918
Loss made of: CE 0.4131165146827698, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.272982597351074 EntMin 0.0
Epoch 1, Class Loss=0.4404568374156952, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.4404568374156952, Class Loss=0.4404568374156952, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/19, Loss=6.776450505852699
Loss made of: CE 0.474723756313324, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.350287914276123 EntMin 0.0
Epoch 2, Class Loss=0.4858347773551941, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.4858347773551941, Class Loss=0.4858347773551941, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/19, Loss=6.145695757865906
Loss made of: CE 0.5450167655944824, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.325162887573242 EntMin 0.0
Epoch 3, Class Loss=0.49061715602874756, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.49061715602874756, Class Loss=0.49061715602874756, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/19, Loss=5.952752813696861
Loss made of: CE 0.5762552618980408, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.511556625366211 EntMin 0.0
Epoch 4, Class Loss=0.4693889021873474, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.4693889021873474, Class Loss=0.4693889021873474, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/19, Loss=5.873700699210167
Loss made of: CE 0.3805437982082367, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.766222953796387 EntMin 0.0
Epoch 5, Class Loss=0.47434118390083313, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.47434118390083313, Class Loss=0.47434118390083313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/19, Loss=5.471883645653724
Loss made of: CE 0.41115057468414307, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3643598556518555 EntMin 0.0
Epoch 6, Class Loss=0.4536314606666565, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.4536314606666565, Class Loss=0.4536314606666565, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=6.463039654493332
Loss made of: CE 0.7271871566772461, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.787355899810791 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.738002121448517
Loss made of: CE 0.6644525527954102, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.192852973937988 EntMin 0.0
Epoch 1, Class Loss=0.7335343360900879, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.7335343360900879, Class Loss=0.7335343360900879, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/26, Loss=5.991084665060043
Loss made of: CE 0.7727245688438416, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.295751571655273 EntMin 0.0
Epoch 2, Batch 20/26, Loss=5.64701110124588
Loss made of: CE 0.5743275880813599, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.625123977661133 EntMin 0.0
Epoch 2, Class Loss=0.6470447182655334, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.6470447182655334, Class Loss=0.6470447182655334, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/26, Loss=5.594710314273835
Loss made of: CE 0.7339868545532227, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.797624588012695 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.43934782743454
Loss made of: CE 0.5452370643615723, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.410942077636719 EntMin 0.0
Epoch 3, Class Loss=0.5919530391693115, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.5919530391693115, Class Loss=0.5919530391693115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/26, Loss=5.224911147356034
Loss made of: CE 0.5828946828842163, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.191363334655762 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.226743000745773
Loss made of: CE 0.6265231966972351, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.345314025878906 EntMin 0.0
Epoch 4, Class Loss=0.5526836514472961, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.5526836514472961, Class Loss=0.5526836514472961, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/26, Loss=5.031768280267715
Loss made of: CE 0.6685158014297485, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.491929531097412 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.242135685682297
Loss made of: CE 0.6044023036956787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.801607131958008 EntMin 0.0
Epoch 5, Class Loss=0.5447914004325867, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5447914004325867, Class Loss=0.5447914004325867, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/26, Loss=5.187149560451507
Loss made of: CE 0.5054259300231934, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.283328533172607 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.114608654379845
Loss made of: CE 0.40811166167259216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807827949523926 EntMin 0.0
Epoch 6, Class Loss=0.5294846296310425, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5294846296310425, Class Loss=0.5294846296310425, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.551142692565918, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.843620
Mean Acc: 0.506423
FreqW Acc: 0.735599
Mean IoU: 0.394487
Class IoU:
	class 0: 0.8523087
	class 1: 0.5714322
	class 2: 0.3351751
	class 3: 0.49774146
	class 4: 0.31080005
	class 5: 0.043331448
	class 6: 0.62328464
	class 7: 0.69274235
	class 8: 0.7182293
	class 9: 0.0
	class 10: 0.34979373
	class 11: 0.2998622
	class 12: 0.59197235
	class 13: 0.3992508
	class 14: 0.656504
	class 15: 0.7072363
	class 16: 0.0
	class 17: 0.0
	class 18: 0.017021019
	class 19: 0.35623866
	class 20: 0.26130012
Class Acc:
	class 0: 0.96914995
	class 1: 0.5840466
	class 2: 0.67108697
	class 3: 0.87188846
	class 4: 0.89757895
	class 5: 0.043337222
	class 6: 0.6421907
	class 7: 0.70877415
	class 8: 0.80320305
	class 9: 0.0
	class 10: 0.42949438
	class 11: 0.4628991
	class 12: 0.76170695
	class 13: 0.58782196
	class 14: 0.7433608
	class 15: 0.7589251
	class 16: 0.0
	class 17: 0.0
	class 18: 0.017248433
	class 19: 0.40382552
	class 20: 0.27834088

federated global round: 23, step: 4
select part of clients to conduct local training
[12, 10, 9, 6]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=5.237856462597847
Loss made of: CE 0.22417184710502625, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.89284086227417 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/26, Loss=4.973784948885441
Loss made of: CE 0.21164970099925995, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.104963779449463 EntMin 0.0
Epoch 1, Class Loss=0.27715906500816345, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.27715906500816345, Class Loss=0.27715906500816345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/26, Loss=4.874898192286492
Loss made of: CE 0.28136497735977173, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9461381435394287 EntMin 0.0
Epoch 2, Batch 20/26, Loss=5.053519786894322
Loss made of: CE 0.21694859862327576, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.673931121826172 EntMin 0.0
Epoch 2, Class Loss=0.3051985204219818, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.3051985204219818, Class Loss=0.3051985204219818, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/26, Loss=5.182631433010101
Loss made of: CE 0.31549423933029175, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.561406135559082 EntMin 0.0
Epoch 3, Batch 20/26, Loss=4.936743211746216
Loss made of: CE 0.3710063397884369, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.187636852264404 EntMin 0.0
Epoch 3, Class Loss=0.349520742893219, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.349520742893219, Class Loss=0.349520742893219, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/26, Loss=4.885707086324691
Loss made of: CE 0.4142952859401703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.747823238372803 EntMin 0.0
Epoch 4, Batch 20/26, Loss=4.920112562179566
Loss made of: CE 0.46801990270614624, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332413673400879 EntMin 0.0
Epoch 4, Class Loss=0.4217528700828552, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.4217528700828552, Class Loss=0.4217528700828552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/26, Loss=4.779597359895706
Loss made of: CE 0.3650282323360443, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.144084930419922 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.062348708510399
Loss made of: CE 0.4813760221004486, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664430618286133 EntMin 0.0
Epoch 5, Class Loss=0.4441109299659729, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.4441109299659729, Class Loss=0.4441109299659729, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/26, Loss=4.737621289491654
Loss made of: CE 0.4818527102470398, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.526338577270508 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.68351748585701
Loss made of: CE 0.4548754096031189, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0890984535217285 EntMin 0.0
Epoch 6, Class Loss=0.46930068731307983, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.46930068731307983, Class Loss=0.46930068731307983, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=6.62373094856739
Loss made of: CE 0.289192795753479, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.899718761444092 EntMin 0.0
Epoch 1, Class Loss=0.3573009669780731, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.3573009669780731, Class Loss=0.3573009669780731, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=5.948780435323715
Loss made of: CE 0.3195372223854065, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.427013397216797 EntMin 0.0
Epoch 2, Class Loss=0.45151203870773315, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.45151203870773315, Class Loss=0.45151203870773315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=5.431181231141091
Loss made of: CE 0.4553132951259613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.721627235412598 EntMin 0.0
Epoch 3, Class Loss=0.49927762150764465, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.49927762150764465, Class Loss=0.49927762150764465, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=5.732623451948166
Loss made of: CE 0.5458164215087891, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.235749244689941 EntMin 0.0
Epoch 4, Class Loss=0.5832605957984924, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.5832605957984924, Class Loss=0.5832605957984924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=5.61999973654747
Loss made of: CE 0.5843166708946228, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.545331001281738 EntMin 0.0
Epoch 5, Class Loss=0.6112029552459717, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.6112029552459717, Class Loss=0.6112029552459717, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=5.2404504954814914
Loss made of: CE 0.6529645919799805, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.311042785644531 EntMin 0.0
Epoch 6, Class Loss=0.6326377987861633, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6326377987861633, Class Loss=0.6326377987861633, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=5.314380373060703
Loss made of: CE 0.21587291359901428, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.037500381469727 EntMin 0.0
Epoch 1, Batch 20/21, Loss=4.93094577640295
Loss made of: CE 0.2459043711423874, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.677783012390137 EntMin 0.0
Epoch 1, Class Loss=0.22557440400123596, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.22557440400123596, Class Loss=0.22557440400123596, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/21, Loss=4.859127071499825
Loss made of: CE 0.35557910799980164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.503958225250244 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.063869521021843
Loss made of: CE 0.24844180047512054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.071081638336182 EntMin 0.0
Epoch 2, Class Loss=0.3382994532585144, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.3382994532585144, Class Loss=0.3382994532585144, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/21, Loss=5.251594150066376
Loss made of: CE 0.41543763875961304, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.19045352935791 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.05961209833622
Loss made of: CE 0.43398675322532654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.680880546569824 EntMin 0.0
Epoch 3, Class Loss=0.42378994822502136, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.42378994822502136, Class Loss=0.42378994822502136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/21, Loss=4.801209279894829
Loss made of: CE 0.4400506019592285, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.38850212097168 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.123854193091392
Loss made of: CE 0.3833204209804535, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9618349075317383 EntMin 0.0
Epoch 4, Class Loss=0.457497239112854, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.457497239112854, Class Loss=0.457497239112854, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=4.733288407325745
Loss made of: CE 0.3817729949951172, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.42582893371582 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.87900162935257
Loss made of: CE 0.40744978189468384, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7936806678771973 EntMin 0.0
Epoch 5, Class Loss=0.47643357515335083, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.47643357515335083, Class Loss=0.47643357515335083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/21, Loss=5.164774203300476
Loss made of: CE 0.4717729091644287, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.461154460906982 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.990602457523346
Loss made of: CE 0.480617493391037, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.821072578430176 EntMin 0.0
Epoch 6, Class Loss=0.5113600492477417, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.5113600492477417, Class Loss=0.5113600492477417, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=5.7284907720983025
Loss made of: CE 0.22282426059246063, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.02631139755249 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.519875180721283
Loss made of: CE 0.2597932815551758, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.822760105133057 EntMin 0.0
Epoch 1, Class Loss=0.2504653334617615, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.2504653334617615, Class Loss=0.2504653334617615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/24, Loss=5.290001478791237
Loss made of: CE 0.3637547492980957, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.717179775238037 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.386255759000778
Loss made of: CE 0.2879849076271057, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7648725509643555 EntMin 0.0
Epoch 2, Class Loss=0.3020055890083313, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3020055890083313, Class Loss=0.3020055890083313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/24, Loss=5.366500176489353
Loss made of: CE 0.3367696702480316, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6867265701293945 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.118055507540703
Loss made of: CE 0.33342182636260986, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.720860481262207 EntMin 0.0
Epoch 3, Class Loss=0.35715848207473755, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.35715848207473755, Class Loss=0.35715848207473755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/24, Loss=5.293861150741577
Loss made of: CE 0.4183988571166992, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7395782470703125 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.119283759593964
Loss made of: CE 0.46687787771224976, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8141937255859375 EntMin 0.0
Epoch 4, Class Loss=0.4454270303249359, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4454270303249359, Class Loss=0.4454270303249359, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/24, Loss=5.2808404743671415
Loss made of: CE 0.6929110288619995, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.641477584838867 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.578900167346001
Loss made of: CE 0.4050937592983246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.349491119384766 EntMin 0.0
Epoch 5, Class Loss=0.5049697160720825, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.5049697160720825, Class Loss=0.5049697160720825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/24, Loss=5.365806666016579
Loss made of: CE 0.5796701312065125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.531349182128906 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.263608375191689
Loss made of: CE 0.5573036670684814, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.059028625488281 EntMin 0.0
Epoch 6, Class Loss=0.5480875372886658, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5480875372886658, Class Loss=0.5480875372886658, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5404204726219177, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.847703
Mean Acc: 0.544851
FreqW Acc: 0.748023
Mean IoU: 0.415942
Class IoU:
	class 0: 0.8623852
	class 1: 0.63409305
	class 2: 0.34171623
	class 3: 0.45858577
	class 4: 0.2935381
	class 5: 0.06213998
	class 6: 0.6152942
	class 7: 0.7155613
	class 8: 0.7330502
	class 9: 0.0
	class 10: 0.28971702
	class 11: 0.29928234
	class 12: 0.59685344
	class 13: 0.3934909
	class 14: 0.6625926
	class 15: 0.7176054
	class 16: 0.0
	class 17: 0.03938798
	class 18: 0.221695
	class 19: 0.33929414
	class 20: 0.45850572
Class Acc:
	class 0: 0.9599637
	class 1: 0.655846
	class 2: 0.7033718
	class 3: 0.9017721
	class 4: 0.91183597
	class 5: 0.0621664
	class 6: 0.6357785
	class 7: 0.7392898
	class 8: 0.8243022
	class 9: 0.0
	class 10: 0.34514576
	class 11: 0.49523187
	class 12: 0.79893106
	class 13: 0.63217914
	class 14: 0.78319114
	class 15: 0.77919304
	class 16: 0.0
	class 17: 0.039451472
	class 18: 0.25645587
	class 19: 0.3718455
	class 20: 0.5459238

federated global round: 24, step: 4
select part of clients to conduct local training
[18, 24, 25, 10]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=6.044915556907654
Loss made of: CE 0.30153369903564453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.998992443084717 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.2816944122314453, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.2816944122314453, Class Loss=0.2816944122314453, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/19, Loss=5.452052673697471
Loss made of: CE 0.38228926062583923, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.02087926864624 EntMin 0.0
Epoch 2, Class Loss=0.35211271047592163, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.35211271047592163, Class Loss=0.35211271047592163, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/19, Loss=5.431263858079911
Loss made of: CE 0.4421916902065277, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.586874961853027 EntMin 0.0
Epoch 3, Class Loss=0.4382508099079132, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.4382508099079132, Class Loss=0.4382508099079132, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/19, Loss=5.374841430783272
Loss made of: CE 0.43796128034591675, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.097548961639404 EntMin 0.0
Epoch 4, Class Loss=0.5109103322029114, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.5109103322029114, Class Loss=0.5109103322029114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/19, Loss=5.4538016527891156
Loss made of: CE 0.48812398314476013, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.892202377319336 EntMin 0.0
Epoch 5, Class Loss=0.5605835318565369, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.5605835318565369, Class Loss=0.5605835318565369, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/19, Loss=5.1128279864788055
Loss made of: CE 0.6114629507064819, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4326958656311035 EntMin 0.0
Epoch 6, Class Loss=0.5802620649337769, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.5802620649337769, Class Loss=0.5802620649337769, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=4.636355866491795
Loss made of: CE 0.17202997207641602, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7829790115356445 EntMin 0.0
Epoch 1, Batch 20/21, Loss=4.978862231969833
Loss made of: CE 0.2112307846546173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.835662841796875 EntMin 0.0
Epoch 1, Class Loss=0.20288340747356415, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.20288340747356415, Class Loss=0.20288340747356415, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/21, Loss=4.7758466601371765
Loss made of: CE 0.29114776849746704, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2442474365234375 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.223191523551941
Loss made of: CE 0.3862394690513611, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.08359956741333 EntMin 0.0
Epoch 2, Class Loss=0.30886009335517883, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.30886009335517883, Class Loss=0.30886009335517883, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/21, Loss=4.9914949730038645
Loss made of: CE 0.3864433467388153, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.407659530639648 EntMin 0.0
Epoch 3, Batch 20/21, Loss=4.712653511762619
Loss made of: CE 0.3418901860713959, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590749740600586 EntMin 0.0
Epoch 3, Class Loss=0.35687369108200073, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.35687369108200073, Class Loss=0.35687369108200073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/21, Loss=4.668549382686615
Loss made of: CE 0.4776667058467865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.640380859375 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.738227730989456
Loss made of: CE 0.4283101558685303, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.51050329208374 EntMin 0.0
Epoch 4, Class Loss=0.423378050327301, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.423378050327301, Class Loss=0.423378050327301, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/21, Loss=4.76295328438282
Loss made of: CE 0.5114721059799194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.225393295288086 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.61462684571743
Loss made of: CE 0.38598641753196716, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.081236839294434 EntMin 0.0
Epoch 5, Class Loss=0.4607442319393158, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.4607442319393158, Class Loss=0.4607442319393158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/21, Loss=5.042681539058686
Loss made of: CE 0.751091480255127, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.275506973266602 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.409501364827156
Loss made of: CE 0.4845917224884033, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.703599452972412 EntMin 0.0
Epoch 6, Class Loss=0.5076406002044678, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.5076406002044678, Class Loss=0.5076406002044678, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=5.118042396008969
Loss made of: CE 0.16211357712745667, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.437246322631836 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=4.927692501246929
Loss made of: CE 0.19114693999290466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.846386909484863 EntMin 0.0
Epoch 1, Class Loss=0.177723690867424, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.177723690867424, Class Loss=0.177723690867424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/24, Loss=5.197227960824966
Loss made of: CE 0.2527769207954407, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7976789474487305 EntMin 0.0
Epoch 2, Batch 20/24, Loss=4.895368406176567
Loss made of: CE 0.2493896335363388, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.799278259277344 EntMin 0.0
Epoch 2, Class Loss=0.2637670040130615, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.2637670040130615, Class Loss=0.2637670040130615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/24, Loss=5.273143470287323
Loss made of: CE 0.35311299562454224, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.108893394470215 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.7774579465389255
Loss made of: CE 0.23838314414024353, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.218823432922363 EntMin 0.0
Epoch 3, Class Loss=0.32188504934310913, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.32188504934310913, Class Loss=0.32188504934310913, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/24, Loss=5.306161305308342
Loss made of: CE 0.45274779200553894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.465068817138672 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.080189624428749
Loss made of: CE 0.4961446523666382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.809808731079102 EntMin 0.0
Epoch 4, Class Loss=0.4512452483177185, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.4512452483177185, Class Loss=0.4512452483177185, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/24, Loss=5.016041481494904
Loss made of: CE 0.5447198152542114, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1779608726501465 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.080135229229927
Loss made of: CE 0.5720365047454834, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.345008373260498 EntMin 0.0
Epoch 5, Class Loss=0.4966287612915039, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.4966287612915039, Class Loss=0.4966287612915039, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/24, Loss=4.781399947404862
Loss made of: CE 0.6612774133682251, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5728559494018555 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.966381016373634
Loss made of: CE 0.6245524287223816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.825318336486816 EntMin 0.0
Epoch 6, Class Loss=0.5634072422981262, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.5634072422981262, Class Loss=0.5634072422981262, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=6.562165033817291
Loss made of: CE 0.9842301607131958, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.583980560302734 EntMin 0.0
Epoch 1, Class Loss=0.9332272410392761, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.9332272410392761, Class Loss=0.9332272410392761, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/19, Loss=5.929151606559754
Loss made of: CE 0.6311944723129272, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.214494705200195 EntMin 0.0
Epoch 2, Class Loss=0.8163444995880127, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.8163444995880127, Class Loss=0.8163444995880127, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/19, Loss=5.408890295028686
Loss made of: CE 0.6610519886016846, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.229704856872559 EntMin 0.0
Epoch 3, Class Loss=0.7530534863471985, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.7530534863471985, Class Loss=0.7530534863471985, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/19, Loss=5.838859117031097
Loss made of: CE 0.785355806350708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.237538814544678 EntMin 0.0
Epoch 4, Class Loss=0.7028893232345581, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.7028893232345581, Class Loss=0.7028893232345581, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/19, Loss=5.678447645902634
Loss made of: CE 0.6751713752746582, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.403226375579834 EntMin 0.0
Epoch 5, Class Loss=0.6945950388908386, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.6945950388908386, Class Loss=0.6945950388908386, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/19, Loss=5.258846014738083
Loss made of: CE 0.7096979022026062, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.706606388092041 EntMin 0.0
Epoch 6, Class Loss=0.6844692230224609, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6844692230224609, Class Loss=0.6844692230224609, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5537234544754028, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.848725
Mean Acc: 0.580019
FreqW Acc: 0.752804
Mean IoU: 0.438720
Class IoU:
	class 0: 0.86288714
	class 1: 0.66151756
	class 2: 0.3403095
	class 3: 0.48345804
	class 4: 0.28027302
	class 5: 0.06852963
	class 6: 0.6468531
	class 7: 0.75558007
	class 8: 0.74081117
	class 9: 0.0
	class 10: 0.2602571
	class 11: 0.29884538
	class 12: 0.6174227
	class 13: 0.40062663
	class 14: 0.660414
	class 15: 0.70398325
	class 16: 0.0
	class 17: 0.40392014
	class 18: 0.2743735
	class 19: 0.26827884
	class 20: 0.48477775
Class Acc:
	class 0: 0.9524296
	class 1: 0.6972466
	class 2: 0.71623206
	class 3: 0.9043459
	class 4: 0.9180069
	class 5: 0.06859811
	class 6: 0.6733827
	class 7: 0.8001926
	class 8: 0.82355493
	class 9: 0.0
	class 10: 0.28082144
	class 11: 0.5129406
	class 12: 0.78888464
	class 13: 0.63153183
	class 14: 0.7907656
	class 15: 0.7605523
	class 16: 0.0
	class 17: 0.55256236
	class 18: 0.40112883
	class 19: 0.27808663
	class 20: 0.62912965

voc_4-4_OURS On GPUs 0
Run in 108759s
