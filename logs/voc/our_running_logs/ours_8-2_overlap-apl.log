nohup: ignoring input
35
kvoc_8-2_OURS-APL On GPUs 2\Writing in results/seed_2023-ov/2023-03-19_voc_8-2_OURS-APL.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[4, 3, 1, 2]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[0, 4, 7, 2]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  4
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[1, 9, 3, 8]
Current Client Index:  1
Current Client Index:  9
Current Client Index:  3
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[5, 9, 0, 4]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Current Client Index:  0
Current Client Index:  4
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.11884414404630661, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.959771
Mean Acc: 0.898867
FreqW Acc: 0.927997
Mean IoU: 0.803600
Class IoU:
	class 0: 0.9505860659032771
	class 1: 0.9002121543757099
	class 2: 0.3911996045853578
	class 3: 0.7886755539422873
	class 4: 0.7227948650191602
	class 5: 0.777020491034379
	class 6: 0.9452273228283382
	class 7: 0.8680230685934628
	class 8: 0.8886579855388657
Class Acc:
	class 0: 0.9742031520387592
	class 1: 0.9505188705401268
	class 2: 0.8333169681087177
	class 3: 0.7954920085182839
	class 4: 0.8649138231019398
	class 5: 0.839701567241524
	class 6: 0.9766569231687022
	class 7: 0.9440467027150332
	class 8: 0.910953836504121

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[5, 11, 7, 1]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/27, Loss=10.596162688732147
Loss made of: CE 0.8512040972709656, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.822399139404297 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/27, Loss=9.003220158815385
Loss made of: CE 0.6100150346755981, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.8856201171875 EntMin 0.0
Epoch 1, Class Loss=0.8666507005691528, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.8666507005691528, Class Loss=0.8666507005691528, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=8.428918504714966
Loss made of: CE 0.5167340040206909, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.843615531921387 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.9490090876817705
Loss made of: CE 0.42670905590057373, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.153018951416016 EntMin 0.0
Epoch 2, Class Loss=0.5509548783302307, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.5509548783302307, Class Loss=0.5509548783302307, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=7.573406395316124
Loss made of: CE 0.5395047664642334, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.753362655639648 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 20/27, Loss=7.253846991062164
Loss made of: CE 0.4445740580558777, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.723142623901367 EntMin 0.0
Epoch 3, Class Loss=0.5136517882347107, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.5136517882347107, Class Loss=0.5136517882347107, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=7.026173707842827
Loss made of: CE 0.398318886756897, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.736842155456543 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.637381973862648
Loss made of: CE 0.4759455621242523, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0276336669921875 EntMin 0.0
Epoch 4, Class Loss=0.4661952257156372, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.4661952257156372, Class Loss=0.4661952257156372, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.588910591602326
Loss made of: CE 0.3784962296485901, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.228403091430664 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.6241917431354524
Loss made of: CE 0.4716523289680481, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.056224822998047 EntMin 0.0
Epoch 5, Class Loss=0.4522587060928345, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.4522587060928345, Class Loss=0.4522587060928345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=6.687758469581604
Loss made of: CE 0.4433395266532898, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.055832862854004 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.405479851365089
Loss made of: CE 0.44045162200927734, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.826356887817383 EntMin 0.0
Epoch 6, Class Loss=0.42883628606796265, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.42883628606796265, Class Loss=0.42883628606796265, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.591576099395752, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=1.591576099395752, Class Loss=1.591576099395752, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=1.0467456579208374, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=1.0467456579208374, Class Loss=1.0467456579208374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.79463791847229, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.79463791847229, Class Loss=0.79463791847229, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.6846766471862793, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.6846766471862793, Class Loss=0.6846766471862793, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6263571381568909, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.6263571381568909, Class Loss=0.6263571381568909, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.5816681385040283, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5816681385040283, Class Loss=0.5816681385040283, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.633033108711242
Loss made of: CE 0.8068221807479858, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.164512634277344 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/27, Loss=8.892697155475616
Loss made of: CE 0.5999027490615845, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.951845169067383 EntMin 0.0
Epoch 1, Class Loss=0.8787402510643005, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.8787402510643005, Class Loss=0.8787402510643005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=8.471909743547439
Loss made of: CE 0.8153808116912842, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.305583953857422 EntMin 0.0
Epoch 2, Batch 20/27, Loss=8.185288205742836
Loss made of: CE 0.5307425260543823, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.6564764976501465 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.674454391002655, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.674454391002655, Class Loss=0.674454391002655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=7.4051195830106735
Loss made of: CE 0.5385119318962097, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.75526762008667 EntMin 0.0
Epoch 3, Batch 20/27, Loss=7.185769018530846
Loss made of: CE 0.4555957317352295, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.386302471160889 EntMin 0.0
Epoch 3, Class Loss=0.5284605026245117, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.5284605026245117, Class Loss=0.5284605026245117, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.9606493532657625
Loss made of: CE 0.4867370128631592, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.623986721038818 EntMin 0.0
Epoch 4, Batch 20/27, Loss=7.098797565698623
Loss made of: CE 0.8797027468681335, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.458281993865967 EntMin 0.0
Epoch 4, Class Loss=0.5169014930725098, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.5169014930725098, Class Loss=0.5169014930725098, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.987384629249573
Loss made of: CE 0.5049294829368591, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0025954246521 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.501140621304512
Loss made of: CE 0.37000101804733276, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.016633033752441 EntMin 0.0
Epoch 5, Class Loss=0.47059717774391174, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.47059717774391174, Class Loss=0.47059717774391174, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=6.515011948347092
Loss made of: CE 0.49171024560928345, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.91745662689209 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.698750057816506
Loss made of: CE 0.5592036247253418, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.27823543548584 EntMin 0.0
Epoch 6, Class Loss=0.479361355304718, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.479361355304718, Class Loss=0.479361355304718, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.492633128166199
Loss made of: CE 0.7622343897819519, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.388337135314941 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/27, Loss=9.134883630275727
Loss made of: CE 0.550560712814331, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.416959762573242 EntMin 0.0
Epoch 1, Class Loss=0.8724491000175476, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.8724491000175476, Class Loss=0.8724491000175476, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=8.392283362150192
Loss made of: CE 1.505284309387207, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.529195785522461 EntMin 0.0
Epoch 2, Batch 20/27, Loss=8.28490595817566
Loss made of: CE 0.5470895767211914, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.525972366333008 EntMin 0.0
Epoch 2, Class Loss=0.6227993965148926, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.6227993965148926, Class Loss=0.6227993965148926, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=7.386102992296219
Loss made of: CE 0.448819637298584, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.18193244934082 EntMin 0.0
Epoch 3, Batch 20/27, Loss=7.649557596445083
Loss made of: CE 0.5374529361724854, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.142101287841797 EntMin 0.0
Epoch 3, Class Loss=0.5420622229576111, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.5420622229576111, Class Loss=0.5420622229576111, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=7.630593192577362
Loss made of: CE 0.488523006439209, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.066619873046875 EntMin 0.0
Epoch 4, Batch 20/27, Loss=7.448439186811447
Loss made of: CE 0.4387197494506836, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.803562164306641 EntMin 0.0
Epoch 4, Class Loss=0.4914640784263611, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.4914640784263611, Class Loss=0.4914640784263611, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=7.0250944793224335
Loss made of: CE 0.4389011561870575, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.506572246551514 EntMin 0.0
Epoch 5, Batch 20/27, Loss=7.088580095767975
Loss made of: CE 0.34220796823501587, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.719700813293457 EntMin 0.0
Epoch 5, Class Loss=0.4695890545845032, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4695890545845032, Class Loss=0.4695890545845032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=6.97350215613842
Loss made of: CE 0.39778026938438416, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.454166412353516 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.951867404580116
Loss made of: CE 0.6398476362228394, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.563119888305664 EntMin 0.0
Epoch 6, Class Loss=0.45426782965660095, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.45426782965660095, Class Loss=0.45426782965660095, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.325797438621521, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923010
Mean Acc: 0.692424
FreqW Acc: 0.857926
Mean IoU: 0.624789
Class IoU:
	class 0: 0.9101033
	class 1: 0.80944407
	class 2: 0.35885373
	class 3: 0.77598363
	class 4: 0.6680538
	class 5: 0.741469
	class 6: 0.9126577
	class 7: 0.818587
	class 8: 0.87753177
	class 9: 0.0
	class 10: 0.0
Class Acc:
	class 0: 0.9822626
	class 1: 0.82832783
	class 2: 0.7845001
	class 3: 0.7856843
	class 4: 0.7520126
	class 5: 0.7757021
	class 6: 0.93112165
	class 7: 0.84724694
	class 8: 0.9298087
	class 9: 0.0
	class 10: 0.0

federated global round: 6, step: 1
select part of clients to conduct local training
[1, 6, 7, 3]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=6.344752106070518
Loss made of: CE 0.3747299313545227, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.782273769378662 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/27, Loss=6.252326247096062
Loss made of: CE 0.46487855911254883, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7919416427612305 EntMin 0.0
Epoch 1, Class Loss=0.4453486502170563, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.4453486502170563, Class Loss=0.4453486502170563, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=6.083432415127755
Loss made of: CE 0.46146154403686523, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8086371421813965 EntMin 0.0
Epoch 2, Batch 20/27, Loss=6.433733698725701
Loss made of: CE 0.4399469494819641, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.10188627243042 EntMin 0.0
Epoch 2, Class Loss=0.44356653094291687, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.44356653094291687, Class Loss=0.44356653094291687, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=6.111215680837631
Loss made of: CE 0.4187849164009094, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.794477462768555 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.1845129281282425
Loss made of: CE 0.461540549993515, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.56532096862793 EntMin 0.0
Epoch 3, Class Loss=0.4476531445980072, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.4476531445980072, Class Loss=0.4476531445980072, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=6.125194534659386
Loss made of: CE 0.4285312592983246, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7154035568237305 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.109030306339264
Loss made of: CE 0.3899739682674408, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.739329814910889 EntMin 0.0
Epoch 4, Class Loss=0.411455899477005, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.411455899477005, Class Loss=0.411455899477005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.9343794047832485
Loss made of: CE 0.4082365036010742, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5273566246032715 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.099085634946823
Loss made of: CE 0.36090680956840515, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.804572582244873 EntMin 0.0
Epoch 5, Class Loss=0.4037353992462158, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4037353992462158, Class Loss=0.4037353992462158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=5.88867275416851
Loss made of: CE 0.3766332268714905, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.377409934997559 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 20/27, Loss=5.855440676212311
Loss made of: CE 0.4134972095489502, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.450314998626709 EntMin 0.0
Epoch 6, Class Loss=0.40472376346588135, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.40472376346588135, Class Loss=0.40472376346588135, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.4225512623786924
Loss made of: CE 0.45173484086990356, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.676506996154785 EntMin 0.0
Epoch 1, Batch 20/27, Loss=6.189860251545906
Loss made of: CE 0.3721393942832947, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8388214111328125 EntMin 0.0
Epoch 1, Class Loss=0.47605812549591064, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.47605812549591064, Class Loss=0.47605812549591064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/27, Loss=6.180436655879021
Loss made of: CE 0.40233469009399414, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.879892349243164 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/27, Loss=6.211065649986267
Loss made of: CE 0.5463166236877441, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.747361183166504 EntMin 0.0
Epoch 2, Class Loss=0.46398428082466125, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.46398428082466125, Class Loss=0.46398428082466125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/27, Loss=5.971716991066932
Loss made of: CE 0.45193013548851013, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.463113307952881 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.9801042705774305
Loss made of: CE 0.46982163190841675, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.24592399597168 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.44135212898254395, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.44135212898254395, Class Loss=0.44135212898254395, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/27, Loss=5.794487622380257
Loss made of: CE 0.46119317412376404, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.352503776550293 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.899135583639145
Loss made of: CE 0.3535109758377075, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.454684734344482 EntMin 0.0
Epoch 4, Class Loss=0.4214092195034027, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4214092195034027, Class Loss=0.4214092195034027, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/27, Loss=6.162411558628082
Loss made of: CE 0.44477176666259766, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.303713798522949 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.154260993003845
Loss made of: CE 0.2839207053184509, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.71351957321167 EntMin 0.0
Epoch 5, Class Loss=0.5033313632011414, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.5033313632011414, Class Loss=0.5033313632011414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/27, Loss=6.177226153016091
Loss made of: CE 0.36652833223342896, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.879158973693848 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.050646224617958
Loss made of: CE 0.34699660539627075, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.173937797546387 EntMin 0.0
Epoch 6, Class Loss=0.3963734805583954, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.3963734805583954, Class Loss=0.3963734805583954, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=6.475161868333816
Loss made of: CE 0.48192358016967773, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.080061912536621 EntMin 0.0
Epoch 1, Batch 20/27, Loss=6.1906024515628815
Loss made of: CE 0.5123812556266785, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.677544593811035 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4595157504081726, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.4595157504081726, Class Loss=0.4595157504081726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/27, Loss=6.358015525341034
Loss made of: CE 0.6229669451713562, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.377884864807129 EntMin 0.0
Epoch 2, Batch 20/27, Loss=6.1991858124732975
Loss made of: CE 0.47016429901123047, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.228450775146484 EntMin 0.0
Epoch 2, Class Loss=0.45482054352760315, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.45482054352760315, Class Loss=0.45482054352760315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=5.988347533345222
Loss made of: CE 0.4783831834793091, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.636815547943115 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.045130762457847
Loss made of: CE 0.45454734563827515, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.415693283081055 EntMin 0.0
Epoch 3, Class Loss=0.4429042637348175, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.4429042637348175, Class Loss=0.4429042637348175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=6.063737294077873
Loss made of: CE 0.38127297163009644, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.616795539855957 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.207697135210037
Loss made of: CE 0.5975856781005859, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.708674907684326 EntMin 0.0
Epoch 4, Class Loss=0.4384031295776367, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4384031295776367, Class Loss=0.4384031295776367, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=6.353699785470963
Loss made of: CE 0.5553097724914551, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.484216690063477 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.918879133462906
Loss made of: CE 0.35713809728622437, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.671119213104248 EntMin 0.0
Epoch 5, Class Loss=0.4438461661338806, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4438461661338806, Class Loss=0.4438461661338806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=6.06805811226368
Loss made of: CE 0.4516032636165619, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.460944175720215 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.1773680537939075
Loss made of: CE 0.3242672085762024, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.796086311340332 EntMin 0.0
Epoch 6, Class Loss=0.4228845238685608, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.4228845238685608, Class Loss=0.4228845238685608, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.2377073764801025, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=1.2377073764801025, Class Loss=1.2377073764801025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=1.1545379161834717, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=1.1545379161834717, Class Loss=1.1545379161834717, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.960545539855957, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.960545539855957, Class Loss=0.960545539855957, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.8328799605369568, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.8328799605369568, Class Loss=0.8328799605369568, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7639559507369995, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.7639559507369995, Class Loss=0.7639559507369995, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6465144157409668, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.6465144157409668, Class Loss=0.6465144157409668, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3125704824924469, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.924592
Mean Acc: 0.719701
FreqW Acc: 0.861390
Mean IoU: 0.638195
Class IoU:
	class 0: 0.9121232
	class 1: 0.86449385
	class 2: 0.36410138
	class 3: 0.830214
	class 4: 0.6595928
	class 5: 0.7915737
	class 6: 0.8738899
	class 7: 0.845214
	class 8: 0.87123376
	class 9: 0.007712933
	class 10: 0.0
Class Acc:
	class 0: 0.9777449
	class 1: 0.8943458
	class 2: 0.8363344
	class 3: 0.84621245
	class 4: 0.73876697
	class 5: 0.87951356
	class 6: 0.8830604
	class 7: 0.8978132
	class 8: 0.95519924
	class 9: 0.0077203233
	class 10: 0.0

federated global round: 7, step: 1
select part of clients to conduct local training
[13, 8, 0, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.214053988456726, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=1.214053988456726, Class Loss=1.214053988456726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.15360689163208, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=1.15360689163208, Class Loss=1.15360689163208, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.9101502895355225, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.9101502895355225, Class Loss=0.9101502895355225, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.7759329676628113, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.7759329676628113, Class Loss=0.7759329676628113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6568278074264526, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.6568278074264526, Class Loss=0.6568278074264526, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6059525012969971, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.6059525012969971, Class Loss=0.6059525012969971, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2977045774459839, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.2977045774459839, Class Loss=1.2977045774459839, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.1269373893737793, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=1.1269373893737793, Class Loss=1.1269373893737793, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.9975157976150513, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.9975157976150513, Class Loss=0.9975157976150513, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.9045884013175964, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.9045884013175964, Class Loss=0.9045884013175964, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.7418293356895447, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.7418293356895447, Class Loss=0.7418293356895447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6153382658958435, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6153382658958435, Class Loss=0.6153382658958435, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.2327156066894531, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=1.2327156066894531, Class Loss=1.2327156066894531, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.1913361549377441, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=1.1913361549377441, Class Loss=1.1913361549377441, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=1.019639015197754, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=1.019639015197754, Class Loss=1.019639015197754, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.8487843871116638, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.8487843871116638, Class Loss=0.8487843871116638, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6969143748283386, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.6969143748283386, Class Loss=0.6969143748283386, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6457430124282837, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.6457430124282837, Class Loss=0.6457430124282837, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=6.032693678140641
Loss made of: CE 0.3590599000453949, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.649477005004883 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.898716184496879
Loss made of: CE 0.32504481077194214, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.21285343170166 EntMin 0.0
Epoch 1, Class Loss=0.4165729880332947, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.4165729880332947, Class Loss=0.4165729880332947, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/27, Loss=5.644594776630401
Loss made of: CE 0.38271474838256836, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.318896293640137 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.603281486034393
Loss made of: CE 0.39421194791793823, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1854119300842285 EntMin 0.0
Epoch 2, Class Loss=0.39214232563972473, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.39214232563972473, Class Loss=0.39214232563972473, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/27, Loss=5.8635118991136554
Loss made of: CE 0.4256978929042816, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.545625686645508 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.799117729067802
Loss made of: CE 0.32029128074645996, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.478766441345215 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.40647900104522705, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.40647900104522705, Class Loss=0.40647900104522705, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/27, Loss=5.8560212582349775
Loss made of: CE 0.33272644877433777, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.671355724334717 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.726533889770508
Loss made of: CE 0.38359031081199646, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.134801864624023 EntMin 0.0
Epoch 4, Class Loss=0.3925837576389313, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.3925837576389313, Class Loss=0.3925837576389313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/27, Loss=5.78176509141922
Loss made of: CE 0.3722165822982788, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.441686153411865 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.672901058197022
Loss made of: CE 0.3620929718017578, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.070732116699219 EntMin 0.0
Epoch 5, Class Loss=0.39635276794433594, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.39635276794433594, Class Loss=0.39635276794433594, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/27, Loss=5.704336115717888
Loss made of: CE 0.41461944580078125, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.171118259429932 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.56312255859375
Loss made of: CE 0.3371405005455017, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.979495048522949 EntMin 0.0
Epoch 6, Class Loss=0.36690279841423035, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.36690279841423035, Class Loss=0.36690279841423035, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2765415608882904, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.924067
Mean Acc: 0.731137
FreqW Acc: 0.861920
Mean IoU: 0.639350
Class IoU:
	class 0: 0.9119558
	class 1: 0.8736566
	class 2: 0.35120794
	class 3: 0.8530826
	class 4: 0.66506696
	class 5: 0.76149243
	class 6: 0.90528643
	class 7: 0.83939505
	class 8: 0.86312777
	class 9: 0.008580006
	class 10: 0.0
Class Acc:
	class 0: 0.97598016
	class 1: 0.913808
	class 2: 0.9069189
	class 3: 0.88406724
	class 4: 0.78710824
	class 5: 0.8409201
	class 6: 0.9254638
	class 7: 0.8752149
	class 8: 0.92429173
	class 9: 0.008727755
	class 10: 0.0

federated global round: 8, step: 1
select part of clients to conduct local training
[12, 9, 4, 13]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.839250937104225
Loss made of: CE 0.5817484855651855, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.651604652404785 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.877191320061684
Loss made of: CE 0.39729833602905273, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.279006004333496 EntMin 0.0
Epoch 1, Class Loss=0.4544365406036377, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.4544365406036377, Class Loss=0.4544365406036377, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.723173639178276
Loss made of: CE 0.40456461906433105, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.381914138793945 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/27, Loss=5.936817148327828
Loss made of: CE 0.492647260427475, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.841513633728027 EntMin 0.0
Epoch 2, Class Loss=0.40427881479263306, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.40427881479263306, Class Loss=0.40427881479263306, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=5.762843182682991
Loss made of: CE 0.40500354766845703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.083712577819824 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.885962864756584
Loss made of: CE 0.35607051849365234, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.154177665710449 EntMin 0.0
Epoch 3, Class Loss=0.39358994364738464, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.39358994364738464, Class Loss=0.39358994364738464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.945869049429893
Loss made of: CE 0.34243059158325195, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.334529399871826 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.6370835334062575
Loss made of: CE 0.38921138644218445, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.296233177185059 EntMin 0.0
Epoch 4, Class Loss=0.4103517234325409, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.4103517234325409, Class Loss=0.4103517234325409, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.8869151949882506
Loss made of: CE 0.3579370677471161, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.692814826965332 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.87889797091484
Loss made of: CE 0.32823795080184937, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.635317802429199 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Class Loss=0.3649134337902069, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.3649134337902069, Class Loss=0.3649134337902069, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.531035095453262
Loss made of: CE 0.47908255457878113, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.087654113769531 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.525507947802543
Loss made of: CE 0.39437898993492126, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.397078990936279 EntMin 0.0
Epoch 6, Class Loss=0.36989113688468933, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.36989113688468933, Class Loss=0.36989113688468933, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.7026593089103699, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.7026593089103699, Class Loss=0.7026593089103699, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.6211817264556885, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.6211817264556885, Class Loss=0.6211817264556885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.5385728478431702, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5385728478431702, Class Loss=0.5385728478431702, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.5132642388343811, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.5132642388343811, Class Loss=0.5132642388343811, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.4788808822631836, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.4788808822631836, Class Loss=0.4788808822631836, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.4500441253185272, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.4500441253185272, Class Loss=0.4500441253185272, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.791819819808007
Loss made of: CE 0.5232725143432617, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.886055946350098 EntMin 0.0
Epoch 1, Batch 20/27, Loss=6.041388982534409
Loss made of: CE 0.47094663977622986, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.545102596282959 EntMin 0.0
Epoch 1, Class Loss=0.4733079671859741, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.4733079671859741, Class Loss=0.4733079671859741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.807878914475441
Loss made of: CE 0.3740999698638916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9430646896362305 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.816700264811516
Loss made of: CE 0.435452401638031, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.998311519622803 EntMin 0.0
Epoch 2, Class Loss=0.41204965114593506, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.41204965114593506, Class Loss=0.41204965114593506, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/27, Loss=5.510781770944595
Loss made of: CE 0.4169034957885742, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.074085235595703 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.729677340388298
Loss made of: CE 0.4106752872467041, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.064416885375977 EntMin 0.0
Epoch 3, Class Loss=0.38772860169410706, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.38772860169410706, Class Loss=0.38772860169410706, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.650943726301193
Loss made of: CE 0.4271215796470642, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.63243293762207 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.854932016134262
Loss made of: CE 0.34361064434051514, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.439340114593506 EntMin 0.0
Epoch 4, Class Loss=0.387341171503067, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.387341171503067, Class Loss=0.387341171503067, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 10/27, Loss=5.656069126725197
Loss made of: CE 0.38402098417282104, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.491466045379639 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.722367745637894
Loss made of: CE 0.39110320806503296, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.472116470336914 EntMin 0.0
Epoch 5, Class Loss=0.40757352113723755, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.40757352113723755, Class Loss=0.40757352113723755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.563351386785508
Loss made of: CE 0.46862250566482544, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.217620849609375 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.533601754903794
Loss made of: CE 0.40767204761505127, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.803767681121826 EntMin 0.0
Epoch 6, Class Loss=0.3985857367515564, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.3985857367515564, Class Loss=0.3985857367515564, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.6705430746078491, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.6705430746078491, Class Loss=0.6705430746078491, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Epoch 2, Class Loss=0.6338040828704834, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.6338040828704834, Class Loss=0.6338040828704834, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.5705256462097168, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.5705256462097168, Class Loss=0.5705256462097168, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5439481735229492, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5439481735229492, Class Loss=0.5439481735229492, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.5062226057052612, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5062226057052612, Class Loss=0.5062226057052612, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.5046355724334717, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5046355724334717, Class Loss=0.5046355724334717, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.24959315359592438, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923583
Mean Acc: 0.712230
FreqW Acc: 0.860207
Mean IoU: 0.631337
Class IoU:
	class 0: 0.9118436
	class 1: 0.82716256
	class 2: 0.3612303
	class 3: 0.7998607
	class 4: 0.6568468
	class 5: 0.74957466
	class 6: 0.8833359
	class 7: 0.8454167
	class 8: 0.87622696
	class 9: 0.03321255
	class 10: 0.0
Class Acc:
	class 0: 0.9791244
	class 1: 0.8470171
	class 2: 0.8710998
	class 3: 0.8130524
	class 4: 0.76764524
	class 5: 0.7937078
	class 6: 0.8928066
	class 7: 0.8868538
	class 8: 0.94927406
	class 9: 0.033953737
	class 10: 0.0

federated global round: 9, step: 1
select part of clients to conduct local training
[4, 6, 13, 12]
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=5.9719310134649275
Loss made of: CE 0.4223039150238037, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.304988384246826 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.627783653140068
Loss made of: CE 0.4910810589790344, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.268141746520996 EntMin 0.0
Epoch 1, Class Loss=0.4296526312828064, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.4296526312828064, Class Loss=0.4296526312828064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.411839798092842
Loss made of: CE 0.36321204900741577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.901268482208252 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/27, Loss=5.517103612422943
Loss made of: CE 0.4237555265426636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.615312099456787 EntMin 0.0
Epoch 2, Class Loss=0.408975213766098, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.408975213766098, Class Loss=0.408975213766098, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=5.205299612879753
Loss made of: CE 0.4463096857070923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.719560146331787 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.488127222657203
Loss made of: CE 0.4532431960105896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8190717697143555 EntMin 0.0
Epoch 3, Class Loss=0.40217894315719604, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.40217894315719604, Class Loss=0.40217894315719604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.282010924816132
Loss made of: CE 0.5588378310203552, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.23488187789917 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.399670678377151
Loss made of: CE 0.37857699394226074, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.011980056762695 EntMin 0.0
Epoch 4, Class Loss=0.4089515209197998, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.4089515209197998, Class Loss=0.4089515209197998, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.305154684185982
Loss made of: CE 0.37376415729522705, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.921609878540039 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.303329792618752
Loss made of: CE 0.3863085210323334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.919099807739258 EntMin 0.0
Epoch 5, Class Loss=0.3905904293060303, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.3905904293060303, Class Loss=0.3905904293060303, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=5.3306323200464245
Loss made of: CE 0.47040891647338867, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.902256011962891 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.20021239221096
Loss made of: CE 0.43336519598960876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.552648544311523 EntMin 0.0
Epoch 6, Class Loss=0.39229947328567505, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.39229947328567505, Class Loss=0.39229947328567505, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/27, Loss=6.009843903779983
Loss made of: CE 0.4988703727722168, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.089196681976318 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.647315472364426
Loss made of: CE 0.40473541617393494, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2396440505981445 EntMin 0.0
Epoch 1, Class Loss=0.4663567543029785, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.4663567543029785, Class Loss=0.4663567543029785, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/27, Loss=5.490165117383003
Loss made of: CE 0.41425561904907227, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.325603485107422 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.645011875033378
Loss made of: CE 0.5538292527198792, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.258297443389893 EntMin 0.0
Epoch 2, Class Loss=0.43095022439956665, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.43095022439956665, Class Loss=0.43095022439956665, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/27, Loss=5.396943759918213
Loss made of: CE 0.44228267669677734, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.984918594360352 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.44589883685112
Loss made of: CE 0.48227760195732117, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.747283458709717 EntMin 0.0
Epoch 3, Class Loss=0.41794756054878235, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.41794756054878235, Class Loss=0.41794756054878235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/27, Loss=5.344441425800324
Loss made of: CE 0.38879096508026123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7732672691345215 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.457960152626038
Loss made of: CE 0.3592579960823059, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.996382713317871 EntMin 0.0
Epoch 4, Class Loss=0.40262067317962646, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.40262067317962646, Class Loss=0.40262067317962646, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/27, Loss=5.392926180362702
Loss made of: CE 0.36377257108688354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.646228790283203 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.3069754302501675
Loss made of: CE 0.2910080552101135, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.049396514892578 EntMin 0.0
Epoch 5, Class Loss=0.39740148186683655, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.39740148186683655, Class Loss=0.39740148186683655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/27, Loss=5.288950601220131
Loss made of: CE 0.34274789690971375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.821097373962402 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.15948723256588
Loss made of: CE 0.36243516206741333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.575657844543457 EntMin 0.0
Epoch 6, Class Loss=0.3979024887084961, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.3979024887084961, Class Loss=0.3979024887084961, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000372
Epoch 1, Class Loss=0.7375799417495728, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.7375799417495728, Class Loss=0.7375799417495728, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.6773614287376404, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.6773614287376404, Class Loss=0.6773614287376404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000258
Epoch 3, Class Loss=0.6385365724563599, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.6385365724563599, Class Loss=0.6385365724563599, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000199
Epoch 4, Class Loss=0.6408752202987671, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.6408752202987671, Class Loss=0.6408752202987671, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000138
Epoch 5, Class Loss=0.6224657893180847, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.6224657893180847, Class Loss=0.6224657893180847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.6395466327667236, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.6395466327667236, Class Loss=0.6395466327667236, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=6.050149574875832
Loss made of: CE 0.5220106244087219, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.04248046875 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.532960361242294
Loss made of: CE 0.40282052755355835, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.937217712402344 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.4188017249107361, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.4188017249107361, Class Loss=0.4188017249107361, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.420678019523621
Loss made of: CE 0.42026135325431824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.907898426055908 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.641102141141891
Loss made of: CE 0.49816077947616577, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.343548774719238 EntMin 0.0
Epoch 2, Class Loss=0.3966781795024872, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.3966781795024872, Class Loss=0.3966781795024872, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=5.440201392769813
Loss made of: CE 0.42228788137435913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.903179168701172 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.413485425710678
Loss made of: CE 0.34778761863708496, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.624558448791504 EntMin 0.0
Epoch 3, Class Loss=0.38716334104537964, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.38716334104537964, Class Loss=0.38716334104537964, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.310112851858139
Loss made of: CE 0.3439086377620697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.977819919586182 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.124723055958748
Loss made of: CE 0.39286109805107117, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.95127534866333 EntMin 0.0
Epoch 4, Class Loss=0.3811827003955841, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.3811827003955841, Class Loss=0.3811827003955841, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.372030931711197
Loss made of: CE 0.35500407218933105, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.135631561279297 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.3313837260007855
Loss made of: CE 0.32428455352783203, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.053159713745117 EntMin 0.0
Epoch 5, Class Loss=0.3751325309276581, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.3751325309276581, Class Loss=0.3751325309276581, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=5.2335301607847216
Loss made of: CE 0.46633219718933105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.781951427459717 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.118342119455337
Loss made of: CE 0.4105573892593384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.869863986968994 EntMin 0.0
Epoch 6, Class Loss=0.3765183687210083, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.3765183687210083, Class Loss=0.3765183687210083, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2505341172218323, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.926552
Mean Acc: 0.731970
FreqW Acc: 0.866077
Mean IoU: 0.650541
Class IoU:
	class 0: 0.91443515
	class 1: 0.8642301
	class 2: 0.3690879
	class 3: 0.7924145
	class 4: 0.68118787
	class 5: 0.7807352
	class 6: 0.9012297
	class 7: 0.85427636
	class 8: 0.88692415
	class 9: 0.111434825
	class 10: 0.0
Class Acc:
	class 0: 0.9781829
	class 1: 0.88957304
	class 2: 0.86634153
	class 3: 0.80290097
	class 4: 0.80284715
	class 5: 0.83747137
	class 6: 0.91252816
	class 7: 0.8909122
	class 8: 0.94951004
	class 9: 0.12140102
	class 10: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[10, 14, 16, 7]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=12.725709545612336
Loss made of: CE 1.0631331205368042, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.29714584350586 EntMin 0.0
Epoch 1, Batch 20/29, Loss=10.848430335521698
Loss made of: CE 0.8706066012382507, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.312189102172852 EntMin 0.0
Epoch 1, Class Loss=1.0674364566802979, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=1.0674364566802979, Class Loss=1.0674364566802979, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=9.564268213510513
Loss made of: CE 0.6391425728797913, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.1432466506958 EntMin 0.0
Epoch 2, Batch 20/29, Loss=9.181103402376175
Loss made of: CE 0.5542121529579163, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.152130126953125 EntMin 0.0
Epoch 2, Class Loss=0.6532153487205505, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.6532153487205505, Class Loss=0.6532153487205505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.874846291542053
Loss made of: CE 0.5645081400871277, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.6841459274292 EntMin 0.0
Epoch 3, Batch 20/29, Loss=8.204564890265464
Loss made of: CE 0.43382176756858826, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.160360336303711 EntMin 0.0
Epoch 3, Class Loss=0.5253392457962036, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5253392457962036, Class Loss=0.5253392457962036, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=8.199760919809341
Loss made of: CE 0.47348734736442566, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.153932571411133 EntMin 0.0
Epoch 4, Batch 20/29, Loss=8.118586328625678
Loss made of: CE 0.4534926414489746, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.466283798217773 EntMin 0.0
Epoch 4, Class Loss=0.44961458444595337, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.44961458444595337, Class Loss=0.44961458444595337, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.750037100911141
Loss made of: CE 0.40297186374664307, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.724732398986816 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.883733981847763
Loss made of: CE 0.37436598539352417, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.254465103149414 EntMin 0.0
Epoch 5, Class Loss=0.4240656793117523, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.4240656793117523, Class Loss=0.4240656793117523, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.466516438126564
Loss made of: CE 0.3767317533493042, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.44104528427124 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.5158594965934755
Loss made of: CE 0.450885534286499, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.605000972747803 EntMin 0.0
Epoch 6, Class Loss=0.41891419887542725, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.41891419887542725, Class Loss=0.41891419887542725, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=7.51895534992218
Loss made of: CE 0.7582694292068481, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.253796577453613 EntMin 0.0
Epoch 1, Class Loss=0.822053849697113, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.822053849697113, Class Loss=0.822053849697113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.6526428163051605
Loss made of: CE 0.5679786205291748, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.89027214050293 EntMin 0.0
Epoch 2, Class Loss=0.634962260723114, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.634962260723114, Class Loss=0.634962260723114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=6.115151047706604
Loss made of: CE 0.5238257646560669, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.037252426147461 EntMin 0.0
Epoch 3, Class Loss=0.5603004097938538, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.5603004097938538, Class Loss=0.5603004097938538, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.789137244224548
Loss made of: CE 0.5728360414505005, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.567864894866943 EntMin 0.0
Epoch 4, Class Loss=0.5372312664985657, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.5372312664985657, Class Loss=0.5372312664985657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.622894370555878
Loss made of: CE 0.5053915977478027, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.805019378662109 EntMin 0.0
Epoch 5, Class Loss=0.5120407342910767, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5120407342910767, Class Loss=0.5120407342910767, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.425886654853821
Loss made of: CE 0.5199344158172607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8854546546936035 EntMin 0.0
Epoch 6, Class Loss=0.4883238971233368, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.4883238971233368, Class Loss=0.4883238971233368, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=13.186517024040223
Loss made of: CE 1.1714797019958496, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.039458274841309 EntMin 0.0
Epoch 1, Batch 20/29, Loss=11.068560516834259
Loss made of: CE 0.9579841494560242, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.843414306640625 EntMin 0.0
Epoch 1, Class Loss=1.1377055644989014, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=1.1377055644989014, Class Loss=1.1377055644989014, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=9.531394612789153
Loss made of: CE 0.6765429377555847, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.716242790222168 EntMin 0.0
Epoch 2, Batch 20/29, Loss=9.421032255887985
Loss made of: CE 0.5950227975845337, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.780587196350098 EntMin 0.0
Epoch 2, Class Loss=0.6546323299407959, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.6546323299407959, Class Loss=0.6546323299407959, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.941572552919387
Loss made of: CE 0.5781869888305664, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.03265380859375 EntMin 0.0
Epoch 3, Batch 20/29, Loss=8.161312994360923
Loss made of: CE 0.4379398822784424, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.283913612365723 EntMin 0.0
Epoch 3, Class Loss=0.4916815459728241, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.4916815459728241, Class Loss=0.4916815459728241, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=8.196838945150375
Loss made of: CE 0.43727827072143555, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.217151641845703 EntMin 0.0
Epoch 4, Batch 20/29, Loss=8.21158232986927
Loss made of: CE 0.4002778232097626, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.924407005310059 EntMin 0.0
Epoch 4, Class Loss=0.4538087248802185, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.4538087248802185, Class Loss=0.4538087248802185, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.717755764722824
Loss made of: CE 0.45671936869621277, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.425603866577148 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.97311686873436
Loss made of: CE 0.32864975929260254, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.238681793212891 EntMin 0.0
Epoch 5, Class Loss=0.44671180844306946, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.44671180844306946, Class Loss=0.44671180844306946, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.54100561439991
Loss made of: CE 0.38498520851135254, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.470401763916016 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.614803516864777
Loss made of: CE 0.31182408332824707, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.677358627319336 EntMin 0.0
Epoch 6, Class Loss=0.43207037448883057, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.43207037448883057, Class Loss=0.43207037448883057, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=7.38887055516243
Loss made of: CE 0.8017446398735046, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.95819616317749 EntMin 0.0
Epoch 1, Class Loss=0.8177469372749329, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.8177469372749329, Class Loss=0.8177469372749329, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.563858890533448
Loss made of: CE 0.5214093923568726, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.406944274902344 EntMin 0.0
Epoch 2, Class Loss=0.6265765428543091, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.6265765428543091, Class Loss=0.6265765428543091, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=6.07773243188858
Loss made of: CE 0.4885815978050232, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.717408657073975 EntMin 0.0
Epoch 3, Class Loss=0.5660150647163391, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.5660150647163391, Class Loss=0.5660150647163391, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.960093781352043
Loss made of: CE 0.4984498620033264, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.064545631408691 EntMin 0.0
Epoch 4, Class Loss=0.5212236642837524, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.5212236642837524, Class Loss=0.5212236642837524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.774191996455192
Loss made of: CE 0.5746029615402222, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.795989036560059 EntMin 0.0
Epoch 5, Class Loss=0.5102200508117676, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.5102200508117676, Class Loss=0.5102200508117676, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.629199722409249
Loss made of: CE 0.5239753127098083, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.655978202819824 EntMin 0.0
Epoch 6, Class Loss=0.4899701178073883, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.4899701178073883, Class Loss=0.4899701178073883, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.39434510469436646, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.878806
Mean Acc: 0.596948
FreqW Acc: 0.779029
Mean IoU: 0.516990
Class IoU:
	class 0: 0.86792445
	class 1: 0.79111105
	class 2: 0.3588042
	class 3: 0.764938
	class 4: 0.6729467
	class 5: 0.7647988
	class 6: 0.8753313
	class 7: 0.84323674
	class 8: 0.7770754
	class 9: 0.004649155
	class 10: 0.0
	class 11: 0.0
	class 12: 5.2495234e-05
Class Acc:
	class 0: 0.9815582
	class 1: 0.8050805
	class 2: 0.8477475
	class 3: 0.7767933
	class 4: 0.7828312
	class 5: 0.83273065
	class 6: 0.88613266
	class 7: 0.88297737
	class 8: 0.95970845
	class 9: 0.004708314
	class 10: 0.0
	class 11: 0.0
	class 12: 5.2495234e-05

federated global round: 11, step: 2
select part of clients to conduct local training
[10, 13, 6, 1]
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/29, Loss=9.367687338590622
Loss made of: CE 0.5911220908164978, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.826404571533203 EntMin 0.0
Epoch 1, Batch 20/29, Loss=8.322726517915726
Loss made of: CE 0.5687537789344788, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.514289379119873 EntMin 0.0
Epoch 1, Class Loss=0.7011968493461609, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.7011968493461609, Class Loss=0.7011968493461609, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/29, Loss=8.12074597477913
Loss made of: CE 0.48200225830078125, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.613476753234863 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.917532461881637
Loss made of: CE 0.4206319749355316, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.123570442199707 EntMin 0.0
Epoch 2, Class Loss=0.4900980293750763, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.4900980293750763, Class Loss=0.4900980293750763, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/29, Loss=7.706235826015472
Loss made of: CE 0.42054545879364014, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.583720684051514 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.307871428132057
Loss made of: CE 0.3459583520889282, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.524595260620117 EntMin 0.0
Epoch 3, Class Loss=0.4282229244709015, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.4282229244709015, Class Loss=0.4282229244709015, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/29, Loss=7.445242395997047
Loss made of: CE 0.45811545848846436, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.39840841293335 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 20/29, Loss=7.3946764975786206
Loss made of: CE 0.34814757108688354, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.920770645141602 EntMin 0.0
Epoch 4, Class Loss=0.4105214476585388, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.4105214476585388, Class Loss=0.4105214476585388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=7.470532029867172
Loss made of: CE 0.38754069805145264, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.435778617858887 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.357832935452461
Loss made of: CE 0.37972837686538696, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.722479820251465 EntMin 0.0
Epoch 5, Class Loss=0.4220634400844574, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.4220634400844574, Class Loss=0.4220634400844574, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/29, Loss=7.147539111971855
Loss made of: CE 0.3299419581890106, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.874312400817871 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.940734922885895
Loss made of: CE 0.3659675121307373, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.196897506713867 EntMin 0.0
Epoch 6, Class Loss=0.39397871494293213, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.39397871494293213, Class Loss=0.39397871494293213, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=9.721097207069397
Loss made of: CE 0.5956196784973145, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.379196166992188 EntMin 0.0
Epoch 1, Batch 20/29, Loss=8.595780843496323
Loss made of: CE 0.5093668103218079, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.959949493408203 EntMin 0.0
Epoch 1, Class Loss=0.6961541771888733, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.6961541771888733, Class Loss=0.6961541771888733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/29, Loss=8.41462088227272
Loss made of: CE 0.4604793190956116, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.858544826507568 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.769444680213928
Loss made of: CE 0.4071345329284668, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.39638614654541 EntMin 0.0
Epoch 2, Class Loss=0.47946053743362427, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.47946053743362427, Class Loss=0.47946053743362427, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/29, Loss=7.56640633046627
Loss made of: CE 0.43872278928756714, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.227651119232178 EntMin 0.0
Epoch 3, Batch 20/29, Loss=8.142545560002327
Loss made of: CE 0.4123518466949463, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.2157111167907715 EntMin 0.0
Epoch 3, Class Loss=0.4225291609764099, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.4225291609764099, Class Loss=0.4225291609764099, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/29, Loss=7.645836746692657
Loss made of: CE 0.4011942744255066, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.3336005210876465 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.692913317680359
Loss made of: CE 0.35790687799453735, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0582804679870605 EntMin 0.0
Epoch 4, Class Loss=0.4062994718551636, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.4062994718551636, Class Loss=0.4062994718551636, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/29, Loss=7.29250285923481
Loss made of: CE 0.471796452999115, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.10536003112793 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 20/29, Loss=7.435160344839096
Loss made of: CE 0.419468492269516, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.273231029510498 EntMin 0.0
Epoch 5, Class Loss=0.4038409888744354, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.4038409888744354, Class Loss=0.4038409888744354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/29, Loss=7.138982000946998
Loss made of: CE 0.3096160888671875, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.812707901000977 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.442705339193344
Loss made of: CE 0.43783140182495117, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.200501918792725 EntMin 0.0
Epoch 6, Class Loss=0.4182929992675781, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.4182929992675781, Class Loss=0.4182929992675781, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=7.076590365171432
Loss made of: CE 1.0735008716583252, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.609918117523193 EntMin 0.0
Epoch 1, Class Loss=0.7543756365776062, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.7543756365776062, Class Loss=0.7543756365776062, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=6.192008072137833
Loss made of: CE 0.6118366718292236, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.452521800994873 EntMin 0.0
Epoch 2, Class Loss=0.6507840752601624, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6507840752601624, Class Loss=0.6507840752601624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=5.761496430635452
Loss made of: CE 0.41743361949920654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.653817176818848 EntMin 0.0
Epoch 3, Class Loss=0.5490067601203918, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.5490067601203918, Class Loss=0.5490067601203918, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.586172100901604
Loss made of: CE 0.551210343837738, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.207952499389648 EntMin 0.0
Epoch 4, Class Loss=0.4939349889755249, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4939349889755249, Class Loss=0.4939349889755249, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.420699778199196
Loss made of: CE 0.4571259021759033, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734879016876221 EntMin 0.0
Epoch 5, Class Loss=0.45345574617385864, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.45345574617385864, Class Loss=0.45345574617385864, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.218821132183075
Loss made of: CE 0.41590702533721924, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.183330535888672 EntMin 0.0
Epoch 6, Class Loss=0.4239714741706848, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.4239714741706848, Class Loss=0.4239714741706848, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=7.017392361164093
Loss made of: CE 0.7335753440856934, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.566928386688232 EntMin 0.0
Epoch 1, Class Loss=0.7716856002807617, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.7716856002807617, Class Loss=0.7716856002807617, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=6.243387985229492
Loss made of: CE 0.5385630130767822, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.226056098937988 EntMin 0.0
Epoch 2, Class Loss=0.6397011280059814, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.6397011280059814, Class Loss=0.6397011280059814, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/13, Loss=5.568049797415734
Loss made of: CE 0.5666618347167969, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.540346622467041 EntMin 0.0
Epoch 3, Class Loss=0.5467272996902466, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.5467272996902466, Class Loss=0.5467272996902466, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.540443187952041
Loss made of: CE 0.44464313983917236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.72145938873291 EntMin 0.0
Epoch 4, Class Loss=0.4883038401603699, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.4883038401603699, Class Loss=0.4883038401603699, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.326828742027283
Loss made of: CE 0.39349040389060974, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.661567687988281 EntMin 0.0
Epoch 5, Class Loss=0.43111711740493774, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.43111711740493774, Class Loss=0.43111711740493774, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.123376405239105
Loss made of: CE 0.4539540708065033, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9177021980285645 EntMin 0.0
Epoch 6, Class Loss=0.4236404597759247, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.4236404597759247, Class Loss=0.4236404597759247, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3539585769176483, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.890203
Mean Acc: 0.617303
FreqW Acc: 0.799592
Mean IoU: 0.541408
Class IoU:
	class 0: 0.8813234
	class 1: 0.7745588
	class 2: 0.3537843
	class 3: 0.80864036
	class 4: 0.6831762
	class 5: 0.7798225
	class 6: 0.84859663
	class 7: 0.85185266
	class 8: 0.78297347
	class 9: 0.0014068125
	class 10: 0.0
	class 11: 0.0
	class 12: 0.27217525
Class Acc:
	class 0: 0.98275703
	class 1: 0.7874416
	class 2: 0.76472133
	class 3: 0.8292523
	class 4: 0.8065679
	class 5: 0.8641641
	class 6: 0.8570664
	class 7: 0.8877036
	class 8: 0.95701873
	class 9: 0.0014086047
	class 10: 0.0
	class 11: 0.0
	class 12: 0.2868418

federated global round: 12, step: 2
select part of clients to conduct local training
[11, 0, 8, 14]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.915421742200851
Loss made of: CE 0.7610613703727722, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.836453437805176 EntMin 0.0
Epoch 1, Class Loss=0.6993541717529297, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.6993541717529297, Class Loss=0.6993541717529297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.120564818382263
Loss made of: CE 0.5686591863632202, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.917993068695068 EntMin 0.0
Epoch 2, Class Loss=0.5769808888435364, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.5769808888435364, Class Loss=0.5769808888435364, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.4993976891040806
Loss made of: CE 0.4668286442756653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.449918746948242 EntMin 0.0
Epoch 3, Class Loss=0.4753812849521637, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4753812849521637, Class Loss=0.4753812849521637, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.425570791959762
Loss made of: CE 0.43405818939208984, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.033766746520996 EntMin 0.0
Epoch 4, Class Loss=0.4157569110393524, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4157569110393524, Class Loss=0.4157569110393524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.5060089260339735
Loss made of: CE 0.38639020919799805, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.72791051864624 EntMin 0.0
Epoch 5, Class Loss=0.39423590898513794, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.39423590898513794, Class Loss=0.39423590898513794, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.091823333501816
Loss made of: CE 0.32993417978286743, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1616716384887695 EntMin 0.0
Epoch 6, Class Loss=0.37867918610572815, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.37867918610572815, Class Loss=0.37867918610572815, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.5656268119812
Loss made of: CE 0.6944303512573242, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.526607513427734 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.6975322514772415
Loss made of: CE 0.4850810468196869, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.113249778747559 EntMin 0.0
Epoch 1, Class Loss=0.5548721551895142, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.5548721551895142, Class Loss=0.5548721551895142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=7.407801985740662
Loss made of: CE 0.36486363410949707, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.738483428955078 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.596267637610436
Loss made of: CE 0.4457591474056244, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.287257194519043 EntMin 0.0
Epoch 2, Class Loss=0.431545227766037, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.431545227766037, Class Loss=0.431545227766037, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=7.398752450942993
Loss made of: CE 0.3501836657524109, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.293658256530762 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.044751596450806
Loss made of: CE 0.4036317467689514, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.499722957611084 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.4323038160800934, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.4323038160800934, Class Loss=0.4323038160800934, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=7.123157951235771
Loss made of: CE 0.4978727698326111, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.143649578094482 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.000982296466828
Loss made of: CE 0.40533536672592163, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4619574546813965 EntMin 0.0
Epoch 4, Class Loss=0.40945279598236084, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.40945279598236084, Class Loss=0.40945279598236084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=6.867753604054451
Loss made of: CE 0.33068954944610596, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.356559753417969 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.02475320994854
Loss made of: CE 0.35515546798706055, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1134257316589355 EntMin 0.0
Epoch 5, Class Loss=0.38515931367874146, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.38515931367874146, Class Loss=0.38515931367874146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=6.910541996359825
Loss made of: CE 0.4542703628540039, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.167243480682373 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.932843524217605
Loss made of: CE 0.376779168844223, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.557240962982178 EntMin 0.0
Epoch 6, Class Loss=0.394992858171463, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.394992858171463, Class Loss=0.394992858171463, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.784932941198349
Loss made of: CE 0.6618068218231201, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.044486999511719 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.810032767057419
Loss made of: CE 0.3959593176841736, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.797197341918945 EntMin 0.0
Epoch 1, Class Loss=0.5496440529823303, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.5496440529823303, Class Loss=0.5496440529823303, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=7.2347139656543735
Loss made of: CE 0.4428013265132904, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.431978225708008 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.61110797226429
Loss made of: CE 0.440626859664917, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.145505428314209 EntMin 0.0
Epoch 2, Class Loss=0.444295197725296, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.444295197725296, Class Loss=0.444295197725296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=7.647466462850571
Loss made of: CE 0.6379338502883911, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.579354286193848 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.503759589791298
Loss made of: CE 0.4010138213634491, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.523078918457031 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.45323672890663147, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.45323672890663147, Class Loss=0.45323672890663147, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=6.942092311382294
Loss made of: CE 0.38835352659225464, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.539775371551514 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.394128733873368
Loss made of: CE 0.40726837515830994, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.351851940155029 EntMin 0.0
Epoch 4, Class Loss=0.4033430218696594, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.4033430218696594, Class Loss=0.4033430218696594, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=7.238543167710304
Loss made of: CE 0.5918232202529907, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.560294151306152 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.950520759820938
Loss made of: CE 0.44174009561538696, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.946006774902344 EntMin 0.0
Epoch 5, Class Loss=0.4156002402305603, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.4156002402305603, Class Loss=0.4156002402305603, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=7.147204595804214
Loss made of: CE 0.4126283526420593, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.288333415985107 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.044418984651566
Loss made of: CE 0.33562517166137695, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.651645660400391 EntMin 0.0
Epoch 6, Class Loss=0.38511404395103455, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.38511404395103455, Class Loss=0.38511404395103455, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.897335746884346
Loss made of: CE 0.7620961666107178, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.407830238342285 EntMin 0.0
Epoch 1, Class Loss=0.7114366888999939, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.7114366888999939, Class Loss=0.7114366888999939, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/13, Loss=6.092275643348694
Loss made of: CE 0.5538907647132874, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.973850250244141 EntMin 0.0
Epoch 2, Class Loss=0.6133015155792236, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.6133015155792236, Class Loss=0.6133015155792236, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/13, Loss=5.688050004839897
Loss made of: CE 0.5065273642539978, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.764832496643066 EntMin 0.0
Epoch 3, Class Loss=0.5134025812149048, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.5134025812149048, Class Loss=0.5134025812149048, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/13, Loss=5.3427796989679335
Loss made of: CE 0.4700300693511963, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.287282466888428 EntMin 0.0
Epoch 4, Class Loss=0.4530271291732788, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.4530271291732788, Class Loss=0.4530271291732788, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/13, Loss=5.255561402440071
Loss made of: CE 0.49462294578552246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.609482765197754 EntMin 0.0
Epoch 5, Class Loss=0.42855411767959595, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.42855411767959595, Class Loss=0.42855411767959595, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/13, Loss=5.148719555139541
Loss made of: CE 0.44013330340385437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.440847873687744 EntMin 0.0
Epoch 6, Class Loss=0.40741169452667236, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.40741169452667236, Class Loss=0.40741169452667236, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.338337779045105, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.899885
Mean Acc: 0.638571
FreqW Acc: 0.818236
Mean IoU: 0.558001
Class IoU:
	class 0: 0.8956221
	class 1: 0.76233476
	class 2: 0.36562747
	class 3: 0.805937
	class 4: 0.6816766
	class 5: 0.77242494
	class 6: 0.82413507
	class 7: 0.85476243
	class 8: 0.80385613
	class 9: 0.0019872081
	class 10: 0.0
	class 11: 0.0
	class 12: 0.48564464
Class Acc:
	class 0: 0.98207575
	class 1: 0.7720879
	class 2: 0.8004716
	class 3: 0.8244815
	class 4: 0.7980987
	class 5: 0.8498009
	class 6: 0.83095145
	class 7: 0.89969146
	class 8: 0.9572816
	class 9: 0.0019885048
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5844983

federated global round: 13, step: 2
select part of clients to conduct local training
[13, 5, 15, 7]
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/29, Loss=8.236003065109253
Loss made of: CE 0.4349157214164734, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.683669090270996 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.520837885141373
Loss made of: CE 0.39967530965805054, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.365108489990234 EntMin 0.0
Epoch 1, Class Loss=0.5191360116004944, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5191360116004944, Class Loss=0.5191360116004944, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/29, Loss=7.534479033946991
Loss made of: CE 0.5686283707618713, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.849384307861328 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.940384301543236
Loss made of: CE 0.38199496269226074, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.472846984863281 EntMin 0.0
Epoch 2, Class Loss=0.4382394254207611, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.4382394254207611, Class Loss=0.4382394254207611, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/29, Loss=6.862950187921524
Loss made of: CE 0.3343336284160614, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.551365852355957 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.351653131842613
Loss made of: CE 0.36584025621414185, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.257184028625488 EntMin 0.0
Epoch 3, Class Loss=0.3975382149219513, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.3975382149219513, Class Loss=0.3975382149219513, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/29, Loss=6.972556123137474
Loss made of: CE 0.3366968333721161, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.733241558074951 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.054624471068382
Loss made of: CE 0.3589664399623871, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.357758522033691 EntMin 0.0
Epoch 4, Class Loss=0.39530494809150696, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.39530494809150696, Class Loss=0.39530494809150696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/29, Loss=6.820122337341308
Loss made of: CE 0.37086808681488037, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.362911224365234 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.728950792551041
Loss made of: CE 0.38836178183555603, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.593853950500488 EntMin 0.0
Epoch 5, Class Loss=0.3813559412956238, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.3813559412956238, Class Loss=0.3813559412956238, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/29, Loss=6.62858627140522
Loss made of: CE 0.3219712972640991, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.493782997131348 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.889814493060112
Loss made of: CE 0.3913407325744629, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.456935882568359 EntMin 0.0
Epoch 6, Class Loss=0.3626590073108673, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.3626590073108673, Class Loss=0.3626590073108673, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.592370635271072
Loss made of: CE 0.41119420528411865, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.083991527557373 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.680286493897438
Loss made of: CE 0.4522239565849304, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.05919075012207 EntMin 0.0
Epoch 1, Class Loss=0.49462854862213135, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.49462854862213135, Class Loss=0.49462854862213135, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/29, Loss=7.682183030247688
Loss made of: CE 0.4476870894432068, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.01386022567749 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/29, Loss=7.303801330924034
Loss made of: CE 0.33061522245407104, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.955997467041016 EntMin 0.0
Epoch 2, Class Loss=0.42683348059654236, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.42683348059654236, Class Loss=0.42683348059654236, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/29, Loss=7.2443889558315275
Loss made of: CE 0.3462160527706146, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.847379207611084 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.890582105517387
Loss made of: CE 0.3246333599090576, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4311347007751465 EntMin 0.0
Epoch 3, Class Loss=0.3888581097126007, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.3888581097126007, Class Loss=0.3888581097126007, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/29, Loss=6.857881346344948
Loss made of: CE 0.345537394285202, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.728026866912842 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.921149814128876
Loss made of: CE 0.40971583127975464, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1277313232421875 EntMin 0.0
Epoch 4, Class Loss=0.38107365369796753, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.38107365369796753, Class Loss=0.38107365369796753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=6.71291092634201
Loss made of: CE 0.43003976345062256, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1630706787109375 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.925532296299934
Loss made of: CE 0.36526939272880554, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.072690486907959 EntMin 0.0
Epoch 5, Class Loss=0.3969481289386749, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.3969481289386749, Class Loss=0.3969481289386749, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/29, Loss=6.796277040243149
Loss made of: CE 0.480130672454834, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.97000789642334 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.730235859751701
Loss made of: CE 0.29588252305984497, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.142972946166992 EntMin 0.0
Epoch 6, Class Loss=0.37341558933258057, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.37341558933258057, Class Loss=0.37341558933258057, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.74016455411911
Loss made of: CE 0.5898339152336121, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.808144569396973 EntMin 0.0
Epoch 1, Class Loss=0.5990703701972961, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.5990703701972961, Class Loss=0.5990703701972961, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.971588438749313
Loss made of: CE 0.5365535020828247, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6362457275390625 EntMin 0.0
Epoch 2, Class Loss=0.48236238956451416, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.48236238956451416, Class Loss=0.48236238956451416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.582225584983826
Loss made of: CE 0.38611432909965515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.507829666137695 EntMin 0.0
Epoch 3, Class Loss=0.41778528690338135, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.41778528690338135, Class Loss=0.41778528690338135, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.198496425151825
Loss made of: CE 0.4212684631347656, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.031778335571289 EntMin 0.0
Epoch 4, Class Loss=0.3737536072731018, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.3737536072731018, Class Loss=0.3737536072731018, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.218137407302857
Loss made of: CE 0.3232155442237854, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.908663749694824 EntMin 0.0
Epoch 5, Class Loss=0.35499608516693115, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.35499608516693115, Class Loss=0.35499608516693115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=4.929754793643951
Loss made of: CE 0.3617329001426697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.413034439086914 EntMin 0.0
Epoch 6, Class Loss=0.3483121693134308, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.3483121693134308, Class Loss=0.3483121693134308, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.954202580451965
Loss made of: CE 0.5864631533622742, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.328427314758301 EntMin 0.0
Epoch 1, Class Loss=0.6604170203208923, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.6604170203208923, Class Loss=0.6604170203208923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/13, Loss=6.048456013202667
Loss made of: CE 0.444476455450058, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.112606048583984 EntMin 0.0
Epoch 2, Class Loss=0.5383841395378113, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.5383841395378113, Class Loss=0.5383841395378113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.538465538620949
Loss made of: CE 0.39549607038497925, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.609249114990234 EntMin 0.0
Epoch 3, Class Loss=0.45690301060676575, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.45690301060676575, Class Loss=0.45690301060676575, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/13, Loss=5.5633754581213
Loss made of: CE 0.38684961199760437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.861996650695801 EntMin 0.0
Epoch 4, Class Loss=0.41221383213996887, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.41221383213996887, Class Loss=0.41221383213996887, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/13, Loss=5.435563266277313
Loss made of: CE 0.4764637351036072, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.890463829040527 EntMin 0.0
Epoch 5, Class Loss=0.3956625759601593, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.3956625759601593, Class Loss=0.3956625759601593, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/13, Loss=5.315517669916153
Loss made of: CE 0.4224720001220703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.592836380004883 EntMin 0.0
Epoch 6, Class Loss=0.36869436502456665, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.36869436502456665, Class Loss=0.36869436502456665, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3362191319465637, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.900571
Mean Acc: 0.627207
FreqW Acc: 0.819074
Mean IoU: 0.554033
Class IoU:
	class 0: 0.89605665
	class 1: 0.74078935
	class 2: 0.36037692
	class 3: 0.76353824
	class 4: 0.6677264
	class 5: 0.76671
	class 6: 0.77812946
	class 7: 0.8545819
	class 8: 0.8296
	class 9: 0.0017845597
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5431364
Class Acc:
	class 0: 0.98390275
	class 1: 0.74858975
	class 2: 0.75801855
	class 3: 0.7736832
	class 4: 0.7633941
	class 5: 0.82633114
	class 6: 0.782725
	class 7: 0.88940644
	class 8: 0.9453489
	class 9: 0.0017850255
	class 10: 0.0
	class 11: 0.0
	class 12: 0.68051314

federated global round: 14, step: 2
select part of clients to conduct local training
[17, 3, 12, 16]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=7.8111591130495075
Loss made of: CE 0.39131802320480347, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.994366645812988 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.833407533168793
Loss made of: CE 0.4122469425201416, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.983758926391602 EntMin 0.0
Epoch 1, Class Loss=0.5133724808692932, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.5133724808692932, Class Loss=0.5133724808692932, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=7.218569549918175
Loss made of: CE 0.4359928369522095, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.923356056213379 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.9598711967468265
Loss made of: CE 0.4085845351219177, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.569213390350342 EntMin 0.0
Epoch 2, Class Loss=0.4194563925266266, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.4194563925266266, Class Loss=0.4194563925266266, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=6.570525678992271
Loss made of: CE 0.43925294280052185, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.796581268310547 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.025625744462014
Loss made of: CE 0.441360205411911, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1282477378845215 EntMin 0.0
Epoch 3, Class Loss=0.4041445255279541, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.4041445255279541, Class Loss=0.4041445255279541, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=6.643156504631042
Loss made of: CE 0.4317881464958191, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3954291343688965 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.73413265645504
Loss made of: CE 0.564834475517273, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.324857234954834 EntMin 0.0
Epoch 4, Class Loss=0.3984334468841553, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.3984334468841553, Class Loss=0.3984334468841553, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.741940924525261
Loss made of: CE 0.2855897545814514, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.689263820648193 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.562211328744889
Loss made of: CE 0.3227042555809021, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.839616775512695 EntMin 0.0
Epoch 5, Class Loss=0.39079535007476807, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.39079535007476807, Class Loss=0.39079535007476807, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.688622868061065
Loss made of: CE 0.3960144519805908, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.155248641967773 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.500721895694733
Loss made of: CE 0.4840738773345947, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.451005458831787 EntMin 0.0
Epoch 6, Class Loss=0.38631024956703186, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.38631024956703186, Class Loss=0.38631024956703186, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=8.024890485405923
Loss made of: CE 0.4627532958984375, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.640097618103027 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.192577964067459
Loss made of: CE 0.5221525430679321, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.490291595458984 EntMin 0.0
Epoch 1, Class Loss=0.48511001467704773, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.48511001467704773, Class Loss=0.48511001467704773, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=7.02072728574276
Loss made of: CE 0.37535572052001953, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.378653526306152 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.023294883966446
Loss made of: CE 0.4312606751918793, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.785962104797363 EntMin 0.0
Epoch 2, Class Loss=0.41552019119262695, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.41552019119262695, Class Loss=0.41552019119262695, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=7.035112261772156
Loss made of: CE 0.506900429725647, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.940288066864014 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.100547149777412
Loss made of: CE 0.3865315914154053, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.901705741882324 EntMin 0.0
Epoch 3, Class Loss=0.427519291639328, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.427519291639328, Class Loss=0.427519291639328, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=7.12531995177269
Loss made of: CE 0.38145893812179565, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.244778633117676 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.641538533568382
Loss made of: CE 0.3404427468776703, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.021135330200195 EntMin 0.0
Epoch 4, Class Loss=0.4047718644142151, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.4047718644142151, Class Loss=0.4047718644142151, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.650588124990463
Loss made of: CE 0.3295883536338806, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.499497413635254 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.71596283018589
Loss made of: CE 0.340125173330307, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.810802459716797 EntMin 0.0
Epoch 5, Class Loss=0.3664577603340149, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.3664577603340149, Class Loss=0.3664577603340149, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.606175369024276
Loss made of: CE 0.35777395963668823, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.056042671203613 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.401777473092079
Loss made of: CE 0.3894328773021698, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.567201137542725 EntMin 0.0
Epoch 6, Class Loss=0.39432865381240845, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.39432865381240845, Class Loss=0.39432865381240845, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.646240919828415
Loss made of: CE 0.4465565085411072, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.114112854003906 EntMin 0.0
Epoch 1, Class Loss=0.5712733268737793, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.5712733268737793, Class Loss=0.5712733268737793, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.851827299594879
Loss made of: CE 0.5069104433059692, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.033071517944336 EntMin 0.0
Epoch 2, Class Loss=0.4662739932537079, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.4662739932537079, Class Loss=0.4662739932537079, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/13, Loss=5.577652224898339
Loss made of: CE 0.3984405994415283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.951652526855469 EntMin 0.0
Epoch 3, Class Loss=0.420963317155838, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.420963317155838, Class Loss=0.420963317155838, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.292087364196777
Loss made of: CE 0.3567085862159729, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.992599010467529 EntMin 0.0
Epoch 4, Class Loss=0.3953244984149933, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.3953244984149933, Class Loss=0.3953244984149933, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.250001764297485
Loss made of: CE 0.4720757007598877, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.856050491333008 EntMin 0.0
Epoch 5, Class Loss=0.3915385901927948, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.3915385901927948, Class Loss=0.3915385901927948, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.229529565572738
Loss made of: CE 0.38383060693740845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.694820404052734 EntMin 0.0
Epoch 6, Class Loss=0.3770037591457367, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.3770037591457367, Class Loss=0.3770037591457367, Reg Loss=0.0
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=8.039519533514977
Loss made of: CE 0.5285788774490356, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.334787368774414 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.232026898860932
Loss made of: CE 0.45334696769714355, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.384860992431641 EntMin 0.0
Epoch 1, Class Loss=0.5042749643325806, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.5042749643325806, Class Loss=0.5042749643325806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/29, Loss=6.883890047669411
Loss made of: CE 0.3920738399028778, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.276143550872803 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.0942294776439665
Loss made of: CE 0.34668177366256714, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.991057395935059 EntMin 0.0
Epoch 2, Class Loss=0.4242440462112427, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.4242440462112427, Class Loss=0.4242440462112427, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/29, Loss=6.986007434129715
Loss made of: CE 0.4861707091331482, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.947391510009766 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.693531504273414
Loss made of: CE 0.31471264362335205, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.829780101776123 EntMin 0.0
Epoch 3, Class Loss=0.4087871015071869, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.4087871015071869, Class Loss=0.4087871015071869, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/29, Loss=6.713545259833336
Loss made of: CE 0.3555940091609955, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.922127723693848 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.764394670724869
Loss made of: CE 0.3271086513996124, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.533412933349609 EntMin 0.0
Epoch 4, Class Loss=0.4004993140697479, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.4004993140697479, Class Loss=0.4004993140697479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/29, Loss=6.350540202856064
Loss made of: CE 0.43399330973625183, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.017714500427246 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.700348043441773
Loss made of: CE 0.2766255736351013, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5422892570495605 EntMin 0.0
Epoch 5, Class Loss=0.3880458176136017, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3880458176136017, Class Loss=0.3880458176136017, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/29, Loss=6.366919609904289
Loss made of: CE 0.3421038091182709, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.492614269256592 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.527664551138878
Loss made of: CE 0.32120978832244873, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.442541122436523 EntMin 0.0
Epoch 6, Class Loss=0.383733332157135, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.383733332157135, Class Loss=0.383733332157135, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.34214016795158386, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.901812
Mean Acc: 0.618495
FreqW Acc: 0.822551
Mean IoU: 0.545391
Class IoU:
	class 0: 0.9020616
	class 1: 0.69332
	class 2: 0.3544465
	class 3: 0.7262768
	class 4: 0.6334033
	class 5: 0.7665492
	class 6: 0.7470345
	class 7: 0.84879965
	class 8: 0.8411299
	class 9: 0.00023916062
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5768253
Class Acc:
	class 0: 0.98482984
	class 1: 0.69833744
	class 2: 0.7187112
	class 3: 0.7340353
	class 4: 0.7138228
	class 5: 0.82804453
	class 6: 0.7504043
	class 7: 0.8791273
	class 8: 0.90004414
	class 9: 0.00023916062
	class 10: 0.0
	class 11: 0.0
	class 12: 0.8328334

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[13, 9, 6, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=10.001298832893372
Loss made of: CE 1.0120182037353516, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.936916351318359 EntMin 0.0
Epoch 1, Class Loss=1.245991826057434, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=1.245991826057434, Class Loss=1.245991826057434, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=8.263710767030716
Loss made of: CE 0.7016892433166504, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.026039123535156 EntMin 0.0
Epoch 2, Class Loss=0.8148579597473145, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.8148579597473145, Class Loss=0.8148579597473145, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.556275743246078
Loss made of: CE 0.6745154857635498, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.097707748413086 EntMin 0.0
Epoch 3, Class Loss=0.6227964162826538, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.6227964162826538, Class Loss=0.6227964162826538, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/11, Loss=7.061717432737351
Loss made of: CE 0.5550875663757324, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.485996246337891 EntMin 0.0
Epoch 4, Class Loss=0.5244374871253967, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5244374871253967, Class Loss=0.5244374871253967, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/11, Loss=6.757722795009613
Loss made of: CE 0.46968063712120056, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.398545265197754 EntMin 0.0
Epoch 5, Class Loss=0.4620654582977295, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.4620654582977295, Class Loss=0.4620654582977295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.542034360766411
Loss made of: CE 0.37253308296203613, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.997954368591309 EntMin 0.0
Epoch 6, Class Loss=0.4150625169277191, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.4150625169277191, Class Loss=0.4150625169277191, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=12.470163583755493
Loss made of: CE 1.116783857345581, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.868515014648438 EntMin 0.0
Epoch 1, Class Loss=1.2956396341323853, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=1.2956396341323853, Class Loss=1.2956396341323853, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=10.170137202739715
Loss made of: CE 0.7747477293014526, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.537633895874023 EntMin 0.0
Epoch 2, Class Loss=0.8932167887687683, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.8932167887687683, Class Loss=0.8932167887687683, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=9.039026582241059
Loss made of: CE 0.6372484564781189, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.142101287841797 EntMin 0.0
Epoch 3, Class Loss=0.6828386783599854, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.6828386783599854, Class Loss=0.6828386783599854, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.30726342201233
Loss made of: CE 0.4973083734512329, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.892762184143066 EntMin 0.0
Epoch 4, Class Loss=0.5403454899787903, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.5403454899787903, Class Loss=0.5403454899787903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.13422895371914
Loss made of: CE 0.46703040599823, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.962374687194824 EntMin 0.0
Epoch 5, Class Loss=0.466528981924057, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.466528981924057, Class Loss=0.466528981924057, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=7.759418618679047
Loss made of: CE 0.37126749753952026, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.283555030822754 EntMin 0.0
Epoch 6, Class Loss=0.4306304454803467, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.4306304454803467, Class Loss=0.4306304454803467, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=10.158472418785095
Loss made of: CE 1.0590986013412476, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.042610168457031 EntMin 0.0
Epoch 1, Class Loss=1.2631573677062988, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=1.2631573677062988, Class Loss=1.2631573677062988, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=8.434502053260804
Loss made of: CE 0.7875375747680664, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.449761867523193 EntMin 0.0
Epoch 2, Class Loss=0.8096842765808105, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.8096842765808105, Class Loss=0.8096842765808105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.622142910957336
Loss made of: CE 0.6390078067779541, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.069501876831055 EntMin 0.0
Epoch 3, Class Loss=0.6344190239906311, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.6344190239906311, Class Loss=0.6344190239906311, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/11, Loss=7.124515613913536
Loss made of: CE 0.5558525323867798, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.2071661949157715 EntMin 0.0
Epoch 4, Class Loss=0.5425872802734375, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.5425872802734375, Class Loss=0.5425872802734375, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/11, Loss=6.825444695353508
Loss made of: CE 0.45161378383636475, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.619245529174805 EntMin 0.0
Epoch 5, Class Loss=0.48276829719543457, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.48276829719543457, Class Loss=0.48276829719543457, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.692476373910904
Loss made of: CE 0.360099732875824, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.635934829711914 EntMin 0.0
Epoch 6, Class Loss=0.42180776596069336, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.42180776596069336, Class Loss=0.42180776596069336, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=12.369952118396759
Loss made of: CE 1.1343152523040771, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.480806350708008 EntMin 0.0
Epoch 1, Class Loss=1.3260877132415771, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=1.3260877132415771, Class Loss=1.3260877132415771, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=9.991619843244553
Loss made of: CE 0.7787474989891052, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.038004875183105 EntMin 0.0
Epoch 2, Class Loss=0.8631762862205505, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.8631762862205505, Class Loss=0.8631762862205505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=8.9440563082695
Loss made of: CE 0.6332474946975708, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.177139282226562 EntMin 0.0
Epoch 3, Class Loss=0.6680960655212402, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.6680960655212402, Class Loss=0.6680960655212402, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.447977268695832
Loss made of: CE 0.5173315405845642, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.567105770111084 EntMin 0.0
Epoch 4, Class Loss=0.5493278503417969, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.5493278503417969, Class Loss=0.5493278503417969, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.104445466399193
Loss made of: CE 0.4397967457771301, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.724453926086426 EntMin 0.0
Epoch 5, Class Loss=0.4687380790710449, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.4687380790710449, Class Loss=0.4687380790710449, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=7.672672706842422
Loss made of: CE 0.44858425855636597, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.204582214355469 EntMin 0.0
Epoch 6, Class Loss=0.4284719228744507, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.4284719228744507, Class Loss=0.4284719228744507, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5138610005378723, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.841609
Mean Acc: 0.421639
FreqW Acc: 0.716268
Mean IoU: 0.372128
Class IoU:
	class 0: 0.8333533
	class 1: 0.53337383
	class 2: 0.3150636
	class 3: 0.6091586
	class 4: 0.57710403
	class 5: 0.60883677
	class 6: 0.39968628
	class 7: 0.7831996
	class 8: 0.7306363
	class 9: 9.15268e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.19126955
	class 13: 0.0
	class 14: 0.00015104786
Class Acc:
	class 0: 0.9907688
	class 1: 0.5347818
	class 2: 0.6814852
	class 3: 0.61401963
	class 4: 0.62920594
	class 5: 0.6309254
	class 6: 0.4002804
	class 7: 0.8205558
	class 8: 0.8048635
	class 9: 9.161395e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.21746139
	class 13: 0.0
	class 14: 0.00015104786

federated global round: 16, step: 3
select part of clients to conduct local training
[18, 5, 20, 0]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.960062557458878
Loss made of: CE 0.6905009746551514, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.435365200042725 EntMin 0.0
Epoch 1, Class Loss=0.8047380447387695, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.8047380447387695, Class Loss=0.8047380447387695, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=8.189790979027748
Loss made of: CE 0.5642722249031067, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.677718162536621 EntMin 0.0
Epoch 2, Class Loss=0.5914120674133301, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.5914120674133301, Class Loss=0.5914120674133301, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.6395340979099275
Loss made of: CE 0.43437668681144714, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.139019966125488 EntMin 0.0
Epoch 3, Class Loss=0.46041029691696167, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.46041029691696167, Class Loss=0.46041029691696167, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.378109246492386
Loss made of: CE 0.3396095633506775, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.475115776062012 EntMin 0.0
Epoch 4, Class Loss=0.4128193259239197, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.4128193259239197, Class Loss=0.4128193259239197, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.176630198955536
Loss made of: CE 0.35517793893814087, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.77940559387207 EntMin 0.0
Epoch 5, Class Loss=0.38990291953086853, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.38990291953086853, Class Loss=0.38990291953086853, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.007399272918701
Loss made of: CE 0.3445071578025818, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.424245834350586 EntMin 0.0
Epoch 6, Class Loss=0.3763851821422577, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.3763851821422577, Class Loss=0.3763851821422577, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/12, Loss=8.81227240562439
Loss made of: CE 0.7650668025016785, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.518735885620117 EntMin 0.0
Epoch 1, Class Loss=0.7777054309844971, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.7777054309844971, Class Loss=0.7777054309844971, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=8.072434329986573
Loss made of: CE 0.6016446948051453, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.792187690734863 EntMin 0.0
Epoch 2, Class Loss=0.5956295728683472, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.5956295728683472, Class Loss=0.5956295728683472, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=7.758159667253494
Loss made of: CE 0.476593017578125, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.493207931518555 EntMin 0.0
Epoch 3, Class Loss=0.4821251332759857, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.4821251332759857, Class Loss=0.4821251332759857, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/12, Loss=7.50044202208519
Loss made of: CE 0.4430871605873108, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0158586502075195 EntMin 0.0
Epoch 4, Class Loss=0.4277660846710205, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.4277660846710205, Class Loss=0.4277660846710205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.373103469610214
Loss made of: CE 0.3721712529659271, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.33546257019043 EntMin 0.0
Epoch 5, Class Loss=0.3921326994895935, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.3921326994895935, Class Loss=0.3921326994895935, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=7.15916448533535
Loss made of: CE 0.40742379426956177, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.67173957824707 EntMin 0.0
Epoch 6, Class Loss=0.38264304399490356, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.38264304399490356, Class Loss=0.38264304399490356, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.868594986200332
Loss made of: CE 0.9060174822807312, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.276382446289062 EntMin 0.0
Epoch 1, Class Loss=0.8045313358306885, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.8045313358306885, Class Loss=0.8045313358306885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=8.099620562791824
Loss made of: CE 0.5686432719230652, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.129465103149414 EntMin 0.0
Epoch 2, Class Loss=0.5802444219589233, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.5802444219589233, Class Loss=0.5802444219589233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.795127826929092
Loss made of: CE 0.503161609172821, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.574380874633789 EntMin 0.0
Epoch 3, Class Loss=0.4576653242111206, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.4576653242111206, Class Loss=0.4576653242111206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.4335704177618025
Loss made of: CE 0.3543606400489807, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.964170455932617 EntMin 0.0
Epoch 4, Class Loss=0.397286981344223, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.397286981344223, Class Loss=0.397286981344223, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.246453812718391
Loss made of: CE 0.3547411859035492, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0033392906188965 EntMin 0.0
Epoch 5, Class Loss=0.3832356631755829, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.3832356631755829, Class Loss=0.3832356631755829, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=6.982243460416794
Loss made of: CE 0.40288591384887695, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.77082633972168 EntMin 0.0
Epoch 6, Class Loss=0.3639201819896698, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.3639201819896698, Class Loss=0.3639201819896698, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=7.8887771487236025
Loss made of: CE 0.7317025065422058, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.878453254699707 EntMin 0.0
Epoch 1, Class Loss=0.737551212310791, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.737551212310791, Class Loss=0.737551212310791, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/11, Loss=7.066100406646728
Loss made of: CE 0.508257269859314, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8841047286987305 EntMin 0.0
Epoch 2, Class Loss=0.5869764089584351, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.5869764089584351, Class Loss=0.5869764089584351, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/11, Loss=6.6746503710746765
Loss made of: CE 0.42670512199401855, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.477714538574219 EntMin 0.0
Epoch 3, Class Loss=0.4779922068119049, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.4779922068119049, Class Loss=0.4779922068119049, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/11, Loss=6.555170714855194
Loss made of: CE 0.4065950810909271, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.723254680633545 EntMin 0.0
Epoch 4, Class Loss=0.4089534282684326, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.4089534282684326, Class Loss=0.4089534282684326, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/11, Loss=6.334945696592331
Loss made of: CE 0.382170170545578, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.350869178771973 EntMin 0.0
Epoch 5, Class Loss=0.3746766746044159, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.3746766746044159, Class Loss=0.3746766746044159, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/11, Loss=6.155869552493096
Loss made of: CE 0.34448128938674927, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.742856979370117 EntMin 0.0
Epoch 6, Class Loss=0.34041064977645874, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.34041064977645874, Class Loss=0.34041064977645874, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.48293134570121765, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.853649
Mean Acc: 0.471016
FreqW Acc: 0.735862
Mean IoU: 0.421671
Class IoU:
	class 0: 0.842513
	class 1: 0.6252659
	class 2: 0.32660627
	class 3: 0.58312124
	class 4: 0.54090524
	class 5: 0.620801
	class 6: 0.5504549
	class 7: 0.80910033
	class 8: 0.7068218
	class 9: 6.10739e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.15690574
	class 13: 0.0
	class 14: 0.56250155
Class Acc:
	class 0: 0.9902122
	class 1: 0.62816733
	class 2: 0.68342894
	class 3: 0.58620214
	class 4: 0.5764504
	class 5: 0.64401513
	class 6: 0.55195236
	class 7: 0.84657377
	class 8: 0.76230305
	class 9: 6.107597e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.16693266
	class 13: 0.0
	class 14: 0.6289479

federated global round: 17, step: 3
select part of clients to conduct local training
[1, 16, 6, 21]
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.670062851905822
Loss made of: CE 0.8027890920639038, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.193553924560547 EntMin 0.0
Epoch 1, Class Loss=0.8968143463134766, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.8968143463134766, Class Loss=0.8968143463134766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=7.066016364097595
Loss made of: CE 0.5273208618164062, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.499600887298584 EntMin 0.0
Epoch 2, Class Loss=0.6329284906387329, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.6329284906387329, Class Loss=0.6329284906387329, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.589694061875344
Loss made of: CE 0.3990832269191742, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4518632888793945 EntMin 0.0
Epoch 3, Class Loss=0.4462430775165558, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.4462430775165558, Class Loss=0.4462430775165558, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.286798045039177
Loss made of: CE 0.31499022245407104, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.613317489624023 EntMin 0.0
Epoch 4, Class Loss=0.35947543382644653, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.35947543382644653, Class Loss=0.35947543382644653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.062684714794159
Loss made of: CE 0.3471258580684662, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.363334655761719 EntMin 0.0
Epoch 5, Class Loss=0.3290766477584839, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.3290766477584839, Class Loss=0.3290766477584839, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=5.963522112369537
Loss made of: CE 0.31203362345695496, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.987372875213623 EntMin 0.0
Epoch 6, Class Loss=0.3110615015029907, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.3110615015029907, Class Loss=0.3110615015029907, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.5424454331398
Loss made of: CE 0.7305033206939697, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.449246406555176 EntMin 0.0
Epoch 1, Class Loss=0.8922274708747864, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.8922274708747864, Class Loss=0.8922274708747864, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=7.028334492444992
Loss made of: CE 0.5504302978515625, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.772908687591553 EntMin 0.0
Epoch 2, Class Loss=0.6189921498298645, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.6189921498298645, Class Loss=0.6189921498298645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.602975010871887
Loss made of: CE 0.42057132720947266, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.72659969329834 EntMin 0.0
Epoch 3, Class Loss=0.44580429792404175, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.44580429792404175, Class Loss=0.44580429792404175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.398982518911362
Loss made of: CE 0.3201051950454712, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.473531246185303 EntMin 0.0
Epoch 4, Class Loss=0.36609935760498047, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.36609935760498047, Class Loss=0.36609935760498047, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.171525636315346
Loss made of: CE 0.2942662239074707, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.588772773742676 EntMin 0.0
Epoch 5, Class Loss=0.3290393352508545, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3290393352508545, Class Loss=0.3290393352508545, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=5.909889727830887
Loss made of: CE 0.29596149921417236, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.163806915283203 EntMin 0.0
Epoch 6, Class Loss=0.30266663432121277, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.30266663432121277, Class Loss=0.30266663432121277, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.828719127178193
Loss made of: CE 0.849380612373352, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.910840034484863 EntMin 0.0
Epoch 1, Class Loss=0.9209071397781372, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.9209071397781372, Class Loss=0.9209071397781372, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/11, Loss=7.181393051147461
Loss made of: CE 0.7337948679924011, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.694713115692139 EntMin 0.0
Epoch 2, Class Loss=0.6646486520767212, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6646486520767212, Class Loss=0.6646486520767212, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/11, Loss=6.709602653980255
Loss made of: CE 0.4850805401802063, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3884806632995605 EntMin 0.0
Epoch 3, Class Loss=0.4977777898311615, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4977777898311615, Class Loss=0.4977777898311615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/11, Loss=6.37464599609375
Loss made of: CE 0.42831477522850037, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.749666213989258 EntMin 0.0
Epoch 4, Class Loss=0.4058908224105835, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4058908224105835, Class Loss=0.4058908224105835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/11, Loss=6.229180449247361
Loss made of: CE 0.3492583930492401, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.078588008880615 EntMin 0.0
Epoch 5, Class Loss=0.35185250639915466, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.35185250639915466, Class Loss=0.35185250639915466, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/11, Loss=6.067291903495788
Loss made of: CE 0.28030461072921753, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.255824089050293 EntMin 0.0
Epoch 6, Class Loss=0.32432910799980164, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.32432910799980164, Class Loss=0.32432910799980164, Reg Loss=0.0
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=7.388734447956085
Loss made of: CE 0.38664811849594116, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.109713554382324 EntMin 0.0
Epoch 1, Class Loss=0.46260255575180054, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.46260255575180054, Class Loss=0.46260255575180054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=7.090869459509849
Loss made of: CE 0.4497377276420593, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.993579387664795 EntMin 0.0
Epoch 2, Class Loss=0.4054623544216156, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.4054623544216156, Class Loss=0.4054623544216156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=7.071935224533081
Loss made of: CE 0.36207088828086853, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.56262731552124 EntMin 0.0
Epoch 3, Class Loss=0.3794039487838745, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.3794039487838745, Class Loss=0.3794039487838745, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=6.980576086044311
Loss made of: CE 0.34593233466148376, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.909213066101074 EntMin 0.0
Epoch 4, Class Loss=0.35791271924972534, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.35791271924972534, Class Loss=0.35791271924972534, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=6.625884175300598
Loss made of: CE 0.3568093180656433, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.472516059875488 EntMin 0.0
Epoch 5, Class Loss=0.3556517958641052, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.3556517958641052, Class Loss=0.3556517958641052, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=6.8316761493682865
Loss made of: CE 0.3525223135948181, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.198465347290039 EntMin 0.0
Epoch 6, Class Loss=0.3430287837982178, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.3430287837982178, Class Loss=0.3430287837982178, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.46865415573120117, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.849111
Mean Acc: 0.454890
FreqW Acc: 0.739200
Mean IoU: 0.394491
Class IoU:
	class 0: 0.85796756
	class 1: 0.60208094
	class 2: 0.3259575
	class 3: 0.6401861
	class 4: 0.53443694
	class 5: 0.6002534
	class 6: 0.51910084
	class 7: 0.82016784
	class 8: 0.660691
	class 9: 0.00021825465
	class 10: 0.0
	class 11: 0.0
	class 12: 0.13109191
	class 13: 0.22514519
	class 14: 6.66492e-05
Class Acc:
	class 0: 0.9911267
	class 1: 0.60416025
	class 2: 0.6474802
	class 3: 0.6440454
	class 4: 0.5691561
	class 5: 0.61672914
	class 6: 0.5202991
	class 7: 0.862142
	class 8: 0.68841344
	class 9: 0.00021826621
	class 10: 0.0
	class 11: 0.0
	class 12: 0.13621391
	class 13: 0.5433025
	class 14: 6.665149e-05

federated global round: 18, step: 3
select part of clients to conduct local training
[8, 2, 17, 14]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=6.091097608208656
Loss made of: CE 0.32412514090538025, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.401546001434326 EntMin 0.0
Epoch 1, Class Loss=0.3727763593196869, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.3727763593196869, Class Loss=0.3727763593196869, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/11, Loss=5.828697228431702
Loss made of: CE 0.2936476469039917, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.256486892700195 EntMin 0.0
Epoch 2, Class Loss=0.33547207713127136, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.33547207713127136, Class Loss=0.33547207713127136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.8377579867839815
Loss made of: CE 0.29309824109077454, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.467437267303467 EntMin 0.0
Epoch 3, Class Loss=0.31313708424568176, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.31313708424568176, Class Loss=0.31313708424568176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.610947333276272
Loss made of: CE 0.23837558925151825, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.18480110168457 EntMin 0.0
Epoch 4, Class Loss=0.28910374641418457, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.28910374641418457, Class Loss=0.28910374641418457, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.598440244793892
Loss made of: CE 0.3083598017692566, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.595208644866943 EntMin 0.0
Epoch 5, Class Loss=0.29718682169914246, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.29718682169914246, Class Loss=0.29718682169914246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.487722858786583
Loss made of: CE 0.25125807523727417, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.695630073547363 EntMin 0.0
Epoch 6, Class Loss=0.27086982131004333, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.27086982131004333, Class Loss=0.27086982131004333, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=6.395591226220131
Loss made of: CE 0.34042876958847046, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.541224956512451 EntMin 0.0
Epoch 1, Class Loss=0.38527289032936096, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.38527289032936096, Class Loss=0.38527289032936096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=6.069929540157318
Loss made of: CE 0.31707531213760376, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.56937313079834 EntMin 0.0
Epoch 2, Class Loss=0.3640434741973877, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.3640434741973877, Class Loss=0.3640434741973877, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.939284592866898
Loss made of: CE 0.31860214471817017, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.101598262786865 EntMin 0.0
Epoch 3, Class Loss=0.3197892904281616, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.3197892904281616, Class Loss=0.3197892904281616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.876661384105683
Loss made of: CE 0.2855377197265625, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.495659828186035 EntMin 0.0
Epoch 4, Class Loss=0.29134219884872437, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.29134219884872437, Class Loss=0.29134219884872437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.739180794358253
Loss made of: CE 0.2644845247268677, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.520515441894531 EntMin 0.0
Epoch 5, Class Loss=0.28256916999816895, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.28256916999816895, Class Loss=0.28256916999816895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.610149019956589
Loss made of: CE 0.29406994581222534, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.847560405731201 EntMin 0.0
Epoch 6, Class Loss=0.27293121814727783, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.27293121814727783, Class Loss=0.27293121814727783, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.962585043907165
Loss made of: CE 0.7944216132164001, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.105015754699707 EntMin 0.0
Epoch 1, Class Loss=0.8082561492919922, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.8082561492919922, Class Loss=0.8082561492919922, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=7.607248023152351
Loss made of: CE 0.45455431938171387, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.198533058166504 EntMin 0.0
Epoch 2, Class Loss=0.532252311706543, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.532252311706543, Class Loss=0.532252311706543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=7.26381501853466
Loss made of: CE 0.3392221927642822, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.484635829925537 EntMin 0.0
Epoch 3, Class Loss=0.422156423330307, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.422156423330307, Class Loss=0.422156423330307, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=7.133515605330468
Loss made of: CE 0.32353073358535767, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.496176719665527 EntMin 0.0
Epoch 4, Class Loss=0.40014752745628357, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.40014752745628357, Class Loss=0.40014752745628357, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=6.739973106980324
Loss made of: CE 0.34201329946517944, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.169361114501953 EntMin 0.0
Epoch 5, Class Loss=0.375581830739975, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.375581830739975, Class Loss=0.375581830739975, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=6.841514673829079
Loss made of: CE 0.337258905172348, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.30061674118042 EntMin 0.0
Epoch 6, Class Loss=0.36735618114471436, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.36735618114471436, Class Loss=0.36735618114471436, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=6.365689668059349
Loss made of: CE 0.36432093381881714, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0981268882751465 EntMin 0.0
Epoch 1, Class Loss=0.3759271204471588, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.3759271204471588, Class Loss=0.3759271204471588, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=6.105959203839302
Loss made of: CE 0.3483341336250305, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.813113212585449 EntMin 0.0
Epoch 2, Class Loss=0.3424716889858246, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.3424716889858246, Class Loss=0.3424716889858246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=6.049522590637207
Loss made of: CE 0.3102467656135559, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.739853858947754 EntMin 0.0
Epoch 3, Class Loss=0.3307367265224457, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.3307367265224457, Class Loss=0.3307367265224457, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/11, Loss=5.91178286075592
Loss made of: CE 0.28391212224960327, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.172750473022461 EntMin 0.0
Epoch 4, Class Loss=0.31416478753089905, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.31416478753089905, Class Loss=0.31416478753089905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.715023033320904
Loss made of: CE 0.2455264776945114, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.066291332244873 EntMin 0.0
Epoch 5, Class Loss=0.2810623049736023, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.2810623049736023, Class Loss=0.2810623049736023, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.794315779209137
Loss made of: CE 0.2590169906616211, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.091012001037598 EntMin 0.0
Epoch 6, Class Loss=0.2757355272769928, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.2757355272769928, Class Loss=0.2757355272769928, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.46734219789505005, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.849650
Mean Acc: 0.461698
FreqW Acc: 0.743996
Mean IoU: 0.393598
Class IoU:
	class 0: 0.86578256
	class 1: 0.59228474
	class 2: 0.3275618
	class 3: 0.6631358
	class 4: 0.50813276
	class 5: 0.6357099
	class 6: 0.5346338
	class 7: 0.8205818
	class 8: 0.66241944
	class 9: 2.764436e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0733989
	class 13: 0.22029692
	class 14: 0.0
Class Acc:
	class 0: 0.9906533
	class 1: 0.59380114
	class 2: 0.67215556
	class 3: 0.6678836
	class 4: 0.5339622
	class 5: 0.6586348
	class 6: 0.53588206
	class 7: 0.86982876
	class 8: 0.69689083
	class 9: 2.764491e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.07415921
	class 13: 0.6315871
	class 14: 0.0

federated global round: 19, step: 3
select part of clients to conduct local training
[1, 9, 8, 0]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=5.932194569706917
Loss made of: CE 0.27338743209838867, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.437767028808594 EntMin 0.0
Epoch 1, Class Loss=0.33197107911109924, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.33197107911109924, Class Loss=0.33197107911109924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/11, Loss=5.631333526968956
Loss made of: CE 0.31643927097320557, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.491573333740234 EntMin 0.0
Epoch 2, Class Loss=0.3152887225151062, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.3152887225151062, Class Loss=0.3152887225151062, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/11, Loss=5.605411607027054
Loss made of: CE 0.27254819869995117, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.75485897064209 EntMin 0.0
Epoch 3, Class Loss=0.30694451928138733, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.30694451928138733, Class Loss=0.30694451928138733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/11, Loss=5.558860355615616
Loss made of: CE 0.2885242700576782, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.875391960144043 EntMin 0.0
Epoch 4, Class Loss=0.2971329689025879, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.2971329689025879, Class Loss=0.2971329689025879, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/11, Loss=5.593258664011955
Loss made of: CE 0.31160932779312134, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.904154300689697 EntMin 0.0
Epoch 5, Class Loss=0.2847020626068115, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.2847020626068115, Class Loss=0.2847020626068115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/11, Loss=5.421951276063919
Loss made of: CE 0.28010010719299316, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.531229019165039 EntMin 0.0
Epoch 6, Class Loss=0.28324782848358154, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.28324782848358154, Class Loss=0.28324782848358154, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.829285305738448
Loss made of: CE 0.8543157577514648, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.365547180175781 EntMin 0.0
Epoch 1, Class Loss=0.8471419215202332, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.8471419215202332, Class Loss=0.8471419215202332, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/12, Loss=7.578129023313522
Loss made of: CE 0.5822501182556152, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.704490661621094 EntMin 0.0
Epoch 2, Class Loss=0.5929077863693237, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.5929077863693237, Class Loss=0.5929077863693237, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/12, Loss=7.088607513904572
Loss made of: CE 0.4661576747894287, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.745342254638672 EntMin 0.0
Epoch 3, Class Loss=0.4378023147583008, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.4378023147583008, Class Loss=0.4378023147583008, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/12, Loss=6.763544276356697
Loss made of: CE 0.30808576941490173, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.643804550170898 EntMin 0.0
Epoch 4, Class Loss=0.39174795150756836, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.39174795150756836, Class Loss=0.39174795150756836, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/12, Loss=7.0387961834669115
Loss made of: CE 0.42997586727142334, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.441020965576172 EntMin 0.0
Epoch 5, Class Loss=0.38322746753692627, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.38322746753692627, Class Loss=0.38322746753692627, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/12, Loss=6.877408826351166
Loss made of: CE 0.3539581000804901, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6769609451293945 EntMin 0.0
Epoch 6, Class Loss=0.39266401529312134, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.39266401529312134, Class Loss=0.39266401529312134, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=5.754750180244446
Loss made of: CE 0.2898924946784973, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.906919479370117 EntMin 0.0
Epoch 1, Class Loss=0.33357352018356323, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.33357352018356323, Class Loss=0.33357352018356323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/11, Loss=5.410379493236542
Loss made of: CE 0.29714223742485046, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.802445888519287 EntMin 0.0
Epoch 2, Class Loss=0.3118452727794647, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.3118452727794647, Class Loss=0.3118452727794647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/11, Loss=5.39484969675541
Loss made of: CE 0.2994014620780945, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.153439044952393 EntMin 0.0
Epoch 3, Class Loss=0.3008081018924713, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.3008081018924713, Class Loss=0.3008081018924713, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/11, Loss=5.336511477828026
Loss made of: CE 0.2554110586643219, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.048168182373047 EntMin 0.0
Epoch 4, Class Loss=0.2868626117706299, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.2868626117706299, Class Loss=0.2868626117706299, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/11, Loss=5.397485083341598
Loss made of: CE 0.2760867476463318, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.487237453460693 EntMin 0.0
Epoch 5, Class Loss=0.29485926032066345, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.29485926032066345, Class Loss=0.29485926032066345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/11, Loss=5.36476793885231
Loss made of: CE 0.2763272225856781, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.329028129577637 EntMin 0.0
Epoch 6, Class Loss=0.2868349254131317, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.2868349254131317, Class Loss=0.2868349254131317, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/11, Loss=5.79381737112999
Loss made of: CE 0.35071414709091187, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.486721992492676 EntMin 0.0
Epoch 1, Class Loss=0.34334245324134827, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.34334245324134827, Class Loss=0.34334245324134827, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/11, Loss=5.661539626121521
Loss made of: CE 0.28804731369018555, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.762796878814697 EntMin 0.0
Epoch 2, Class Loss=0.31108352541923523, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.31108352541923523, Class Loss=0.31108352541923523, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/11, Loss=5.66361328959465
Loss made of: CE 0.28377145528793335, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.622675895690918 EntMin 0.0
Epoch 3, Class Loss=0.31035715341567993, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.31035715341567993, Class Loss=0.31035715341567993, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/11, Loss=5.613666781783104
Loss made of: CE 0.289469450712204, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9612274169921875 EntMin 0.0
Epoch 4, Class Loss=0.2945226728916168, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.2945226728916168, Class Loss=0.2945226728916168, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/11, Loss=5.499475267529488
Loss made of: CE 0.28767916560173035, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.670427322387695 EntMin 0.0
Epoch 5, Class Loss=0.2840072810649872, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.2840072810649872, Class Loss=0.2840072810649872, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/11, Loss=5.540158922970295
Loss made of: CE 0.24683253467082977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.989367485046387 EntMin 0.0
Epoch 6, Class Loss=0.2835717499256134, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.2835717499256134, Class Loss=0.2835717499256134, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.46555495262145996, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.850563
Mean Acc: 0.469089
FreqW Acc: 0.750695
Mean IoU: 0.395337
Class IoU:
	class 0: 0.87413394
	class 1: 0.6109727
	class 2: 0.33656868
	class 3: 0.62598926
	class 4: 0.5295389
	class 5: 0.61577195
	class 6: 0.5622332
	class 7: 0.82741857
	class 8: 0.67661345
	class 9: 2.2500999e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.054234076
	class 13: 0.20928854
	class 14: 0.0072628157
Class Acc:
	class 0: 0.9896248
	class 1: 0.612856
	class 2: 0.6993933
	class 3: 0.62930435
	class 4: 0.5610035
	class 5: 0.63467985
	class 6: 0.5636368
	class 7: 0.87089837
	class 8: 0.7075695
	class 9: 2.2501672e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.054435596
	class 13: 0.7056345
	class 14: 0.0072682584

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[11, 2, 15, 6]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=9.826114070415496
Loss made of: CE 1.0976788997650146, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.182246208190918 EntMin 0.0
Epoch 1, Class Loss=1.2507182359695435, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=1.2507182359695435, Class Loss=1.2507182359695435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.783889532089233
Loss made of: CE 0.7988057136535645, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.278855323791504 EntMin 0.0
Epoch 2, Class Loss=0.8525092005729675, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.8525092005729675, Class Loss=0.8525092005729675, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.856048101186753
Loss made of: CE 0.7041424512863159, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8705854415893555 EntMin 0.0
Epoch 3, Class Loss=0.7362411022186279, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.7362411022186279, Class Loss=0.7362411022186279, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.4575917541980745
Loss made of: CE 0.7285852432250977, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.726259231567383 EntMin 0.0
Epoch 4, Class Loss=0.7319480180740356, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.7319480180740356, Class Loss=0.7319480180740356, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.247783231735229
Loss made of: CE 0.6738840341567993, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.504052639007568 EntMin 0.0
Epoch 5, Class Loss=0.7020645141601562, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.7020645141601562, Class Loss=0.7020645141601562, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.131566601991653
Loss made of: CE 0.7394207715988159, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.516304969787598 EntMin 0.0
Epoch 6, Class Loss=0.6820582151412964, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.6820582151412964, Class Loss=0.6820582151412964, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=9.835697209835052
Loss made of: CE 1.1319255828857422, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.461757659912109 EntMin 0.0
Epoch 1, Class Loss=1.2685418128967285, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=1.2685418128967285, Class Loss=1.2685418128967285, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.6744143724441525
Loss made of: CE 0.7012516260147095, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.214357376098633 EntMin 0.0
Epoch 2, Class Loss=0.8407047986984253, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.8407047986984253, Class Loss=0.8407047986984253, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.853325116634369
Loss made of: CE 0.6797899007797241, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.292365550994873 EntMin 0.0
Epoch 3, Class Loss=0.7438330054283142, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.7438330054283142, Class Loss=0.7438330054283142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.5731659710407255
Loss made of: CE 0.7389745712280273, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.697686195373535 EntMin 0.0
Epoch 4, Class Loss=0.716498076915741, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.716498076915741, Class Loss=0.716498076915741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.180110305547714
Loss made of: CE 0.7774269580841064, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.624793529510498 EntMin 0.0
Epoch 5, Class Loss=0.6928627490997314, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.6928627490997314, Class Loss=0.6928627490997314, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.195491868257522
Loss made of: CE 0.7164365649223328, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.270351409912109 EntMin 0.0
Epoch 6, Class Loss=0.6659575700759888, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.6659575700759888, Class Loss=0.6659575700759888, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=9.661042010784149
Loss made of: CE 1.0678768157958984, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.170637130737305 EntMin 0.0
Epoch 1, Class Loss=1.2214128971099854, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=1.2214128971099854, Class Loss=1.2214128971099854, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.610607171058655
Loss made of: CE 0.8493744730949402, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.220889091491699 EntMin 0.0
Epoch 2, Class Loss=0.8051953315734863, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.8051953315734863, Class Loss=0.8051953315734863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.867778241634369
Loss made of: CE 0.7618042230606079, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.845426082611084 EntMin 0.0
Epoch 3, Class Loss=0.7459993362426758, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.7459993362426758, Class Loss=0.7459993362426758, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.643849128484726
Loss made of: CE 0.5516735315322876, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.955857276916504 EntMin 0.0
Epoch 4, Class Loss=0.7184468507766724, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.7184468507766724, Class Loss=0.7184468507766724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.287760239839554
Loss made of: CE 0.8132742643356323, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.856182098388672 EntMin 0.0
Epoch 5, Class Loss=0.6840912699699402, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.6840912699699402, Class Loss=0.6840912699699402, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.14032673239708
Loss made of: CE 0.6331717371940613, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.879863262176514 EntMin 0.0
Epoch 6, Class Loss=0.6620265245437622, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.6620265245437622, Class Loss=0.6620265245437622, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=9.106966841220856
Loss made of: CE 1.0748003721237183, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.891544342041016 EntMin 0.0
Epoch 1, Batch 20/97, Loss=7.861871385574341
Loss made of: CE 0.804808497428894, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.768987655639648 EntMin 0.0
Epoch 1, Batch 30/97, Loss=6.783546882867813
Loss made of: CE 0.6244359016418457, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.039371013641357 EntMin 0.0
Epoch 1, Batch 40/97, Loss=6.498341697454452
Loss made of: CE 0.6207462549209595, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.565082550048828 EntMin 0.0
Epoch 1, Batch 50/97, Loss=6.406040579080582
Loss made of: CE 0.6373097896575928, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.981947898864746 EntMin 0.0
Epoch 1, Batch 60/97, Loss=6.271174842119217
Loss made of: CE 0.6660041809082031, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.417738914489746 EntMin 0.0
Epoch 1, Batch 70/97, Loss=6.193158227205276
Loss made of: CE 0.6572617888450623, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.345151901245117 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.81569350361824
Loss made of: CE 0.5719709396362305, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.517415523529053 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.792343473434448
Loss made of: CE 0.5106146335601807, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.235588073730469 EntMin 0.0
Epoch 1, Class Loss=0.6992143988609314, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.6992143988609314, Class Loss=0.6992143988609314, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/97, Loss=5.84497344493866
Loss made of: CE 0.48858532309532166, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.6535139083862305 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.7904801845550535
Loss made of: CE 0.5883756875991821, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.976497173309326 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.479310530424118
Loss made of: CE 0.4276934862136841, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.972626686096191 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.64875536262989
Loss made of: CE 0.47639068961143494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.540257453918457 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.4463762670755385
Loss made of: CE 0.4370236396789551, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.748194694519043 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.5414438307285305
Loss made of: CE 0.5346765518188477, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.218436241149902 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.527219250798225
Loss made of: CE 0.36903953552246094, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.146057605743408 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.683148586750031
Loss made of: CE 0.4912240505218506, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.461747169494629 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.665653264522552
Loss made of: CE 0.416707843542099, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.303009033203125 EntMin 0.0
Epoch 2, Class Loss=0.46500226855278015, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.46500226855278015, Class Loss=0.46500226855278015, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/97, Loss=5.387589693069458
Loss made of: CE 0.40089136362075806, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.796828269958496 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.388475066423416
Loss made of: CE 0.43105801939964294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.562342166900635 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.705785384774208
Loss made of: CE 0.32982954382896423, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.381608963012695 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.538247826695442
Loss made of: CE 0.4396342933177948, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.498518943786621 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.6104174047708515
Loss made of: CE 0.42077717185020447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8360161781311035 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.557405117154121
Loss made of: CE 0.5103071928024292, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6757307052612305 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.283554741740227
Loss made of: CE 0.37680917978286743, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645190238952637 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.562087723612786
Loss made of: CE 0.340448796749115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.87678861618042 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.554231914877891
Loss made of: CE 0.31982317566871643, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.00764274597168 EntMin 0.0
Epoch 3, Class Loss=0.4126597046852112, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4126597046852112, Class Loss=0.4126597046852112, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/97, Loss=5.32690717279911
Loss made of: CE 0.3851768374443054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.769069671630859 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.332114380598068
Loss made of: CE 0.3794071674346924, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.640458583831787 EntMin 0.0
Epoch 4, Batch 30/97, Loss=5.155318704247475
Loss made of: CE 0.3293362557888031, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.492917537689209 EntMin 0.0
Epoch 4, Batch 40/97, Loss=5.253265020251274
Loss made of: CE 0.402588427066803, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.896398544311523 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.257138141989708
Loss made of: CE 0.3653545081615448, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9872050285339355 EntMin 0.0
Epoch 4, Batch 60/97, Loss=5.116499334573746
Loss made of: CE 0.3837071359157562, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8714094161987305 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.350258538126946
Loss made of: CE 0.3702538013458252, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.544593811035156 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.431569629907608
Loss made of: CE 0.30677103996276855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8410491943359375 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.3016094952821735
Loss made of: CE 0.556517481803894, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.292636871337891 EntMin 0.0
Epoch 4, Class Loss=0.39040759205818176, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.39040759205818176, Class Loss=0.39040759205818176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/97, Loss=5.317237788438797
Loss made of: CE 0.32311078906059265, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.312536239624023 EntMin 0.0
Epoch 5, Batch 20/97, Loss=5.168908059597015
Loss made of: CE 0.3339385986328125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.647830963134766 EntMin 0.0
Epoch 5, Batch 30/97, Loss=5.190680068731308
Loss made of: CE 0.3822663724422455, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.212737083435059 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.216562792658806
Loss made of: CE 0.4475415349006653, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3214569091796875 EntMin 0.0
Epoch 5, Batch 50/97, Loss=5.340302157402038
Loss made of: CE 0.3847073018550873, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.063995361328125 EntMin 0.0
Epoch 5, Batch 60/97, Loss=5.411778056621552
Loss made of: CE 0.3003469407558441, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.465656280517578 EntMin 0.0
Epoch 5, Batch 70/97, Loss=5.107519006729126
Loss made of: CE 0.3003208637237549, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421270370483398 EntMin 0.0
Epoch 5, Batch 80/97, Loss=5.241877886652946
Loss made of: CE 0.3087272047996521, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.292036533355713 EntMin 0.0
Epoch 5, Batch 90/97, Loss=5.226787543296814
Loss made of: CE 0.3358379304409027, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.366694927215576 EntMin 0.0
Epoch 5, Class Loss=0.38442182540893555, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.38442182540893555, Class Loss=0.38442182540893555, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/97, Loss=5.269533407688141
Loss made of: CE 0.2937782406806946, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.350968837738037 EntMin 0.0
Epoch 6, Batch 20/97, Loss=5.416152501106263
Loss made of: CE 0.25721973180770874, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.412875652313232 EntMin 0.0
Epoch 6, Batch 30/97, Loss=5.119322738051414
Loss made of: CE 0.35169777274131775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.752352714538574 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.269288125634193
Loss made of: CE 0.3918308913707733, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.100151062011719 EntMin 0.0
Epoch 6, Batch 50/97, Loss=5.088197869062424
Loss made of: CE 0.3159205913543701, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.507308006286621 EntMin 0.0
Epoch 6, Batch 60/97, Loss=5.199767747521401
Loss made of: CE 0.3692473769187927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.952478408813477 EntMin 0.0
Epoch 6, Batch 70/97, Loss=5.164353343844414
Loss made of: CE 0.730320930480957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.346563339233398 EntMin 0.0
Epoch 6, Batch 80/97, Loss=5.094039824604988
Loss made of: CE 0.3420085906982422, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.090767860412598 EntMin 0.0
Epoch 6, Batch 90/97, Loss=5.364077842235565
Loss made of: CE 0.5912556052207947, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.073822021484375 EntMin 0.0
Epoch 6, Class Loss=0.3847907781600952, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.3847907781600952, Class Loss=0.3847907781600952, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6335666179656982, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.785487
Mean Acc: 0.400375
FreqW Acc: 0.633566
Mean IoU: 0.334487
Class IoU:
	class 0: 0.7871044
	class 1: 0.6139444
	class 2: 0.35402206
	class 3: 0.48973787
	class 4: 0.54884046
	class 5: 0.6132155
	class 6: 0.46895328
	class 7: 0.81724125
	class 8: 0.7341494
	class 9: 9.417891e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.05822118
	class 13: 0.20075347
	class 14: 0.0
	class 15: 0.0
	class 16: 0.0
Class Acc:
	class 0: 0.9901125
	class 1: 0.6171182
	class 2: 0.8092357
	class 3: 0.49117842
	class 4: 0.58425087
	class 5: 0.6422462
	class 6: 0.4703971
	class 7: 0.86610395
	class 8: 0.76382893
	class 9: 9.418557e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.058660917
	class 13: 0.5131547
	class 14: 0.0
	class 15: 0.0
	class 16: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[20, 2, 24, 4]
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=7.216755574941635
Loss made of: CE 0.6616389751434326, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.804858684539795 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.782208347320557
Loss made of: CE 0.6887299418449402, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.861428260803223 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.921300917863846
Loss made of: CE 0.491380512714386, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.24311637878418 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.9385064333677295
Loss made of: CE 0.49441999197006226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.497966289520264 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.57024450302124
Loss made of: CE 0.5201570987701416, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.492819786071777 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.585925820469856
Loss made of: CE 0.42257052659988403, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.944607734680176 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.693483161926269
Loss made of: CE 0.45743346214294434, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.630982398986816 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.476699659228325
Loss made of: CE 0.4724157452583313, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.707272052764893 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/97, Loss=5.765632456541061
Loss made of: CE 0.4257476329803467, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0020036697387695 EntMin 0.0
Epoch 1, Class Loss=0.5333641767501831, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.5333641767501831, Class Loss=0.5333641767501831, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/97, Loss=5.3214911878108975
Loss made of: CE 0.4458250403404236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.939676284790039 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.324227657914162
Loss made of: CE 0.42013686895370483, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.011586666107178 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.28754577934742
Loss made of: CE 0.40596121549606323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9278693199157715 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.376143884658814
Loss made of: CE 0.3988511562347412, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.592988967895508 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.308265045285225
Loss made of: CE 0.37952885031700134, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5398945808410645 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.295940500497818
Loss made of: CE 0.429426372051239, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.951143264770508 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.693104991316796
Loss made of: CE 0.4532233476638794, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.32166051864624 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.22983270585537
Loss made of: CE 0.3925836980342865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.846412658691406 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.211296129226684
Loss made of: CE 0.40775904059410095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.263185501098633 EntMin 0.0
Epoch 2, Class Loss=0.4136817157268524, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.4136817157268524, Class Loss=0.4136817157268524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/97, Loss=5.266900220513344
Loss made of: CE 0.3729482889175415, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.821460723876953 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.489321327209472
Loss made of: CE 0.37987685203552246, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.296609878540039 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.237583902478218
Loss made of: CE 0.3188740909099579, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.766756057739258 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.254341733455658
Loss made of: CE 0.45086273550987244, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.776670455932617 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.1587739199399945
Loss made of: CE 0.30467844009399414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.504156589508057 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.46336759030819
Loss made of: CE 0.3147066831588745, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.652205467224121 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.028166171908379
Loss made of: CE 0.3338029086589813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.732489585876465 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.2134724766016
Loss made of: CE 0.3542684018611908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.370457172393799 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.457500466704369
Loss made of: CE 0.40941566228866577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.474882125854492 EntMin 0.0
Epoch 3, Class Loss=0.3905710279941559, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.3905710279941559, Class Loss=0.3905710279941559, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/97, Loss=5.458592948317528
Loss made of: CE 0.33476895093917847, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.802642345428467 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.219831272959709
Loss made of: CE 0.35959500074386597, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.739034175872803 EntMin 0.0
Epoch 4, Batch 30/97, Loss=5.184858804941177
Loss made of: CE 0.34076255559921265, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332101821899414 EntMin 0.0
Epoch 4, Batch 40/97, Loss=5.023536288738251
Loss made of: CE 0.37278565764427185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.697104454040527 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.238067609071732
Loss made of: CE 0.3598353862762451, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.609563827514648 EntMin 0.0
Epoch 4, Batch 60/97, Loss=5.0849680483341215
Loss made of: CE 0.41614824533462524, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.628204345703125 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.071416875720024
Loss made of: CE 0.4368894100189209, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0889105796813965 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.559143683314323
Loss made of: CE 0.38225239515304565, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.869538307189941 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.276996532082558
Loss made of: CE 0.36388522386550903, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.955706596374512 EntMin 0.0
Epoch 4, Class Loss=0.37509679794311523, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.37509679794311523, Class Loss=0.37509679794311523, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/97, Loss=5.177815937995911
Loss made of: CE 0.40574198961257935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.70871639251709 EntMin 0.0
Epoch 5, Batch 20/97, Loss=5.16074871122837
Loss made of: CE 0.25473830103874207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.845842361450195 EntMin 0.0
Epoch 5, Batch 30/97, Loss=5.277851977944374
Loss made of: CE 0.33952969312667847, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.072082996368408 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.106577527523041
Loss made of: CE 0.35414695739746094, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.65290641784668 EntMin 0.0
Epoch 5, Batch 50/97, Loss=5.209765583276749
Loss made of: CE 0.36139702796936035, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.307293891906738 EntMin 0.0
Epoch 5, Batch 60/97, Loss=5.175745809078217
Loss made of: CE 0.49231046438217163, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.844520092010498 EntMin 0.0
Epoch 5, Batch 70/97, Loss=5.11632983982563
Loss made of: CE 0.35818737745285034, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.322127819061279 EntMin 0.0
Epoch 5, Batch 80/97, Loss=5.065172457695008
Loss made of: CE 0.4666137099266052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.59700870513916 EntMin 0.0
Epoch 5, Batch 90/97, Loss=5.096081158518791
Loss made of: CE 0.382102906703949, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.372074604034424 EntMin 0.0
Epoch 5, Class Loss=0.3709324896335602, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.3709324896335602, Class Loss=0.3709324896335602, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/97, Loss=4.906091165542603
Loss made of: CE 0.3929758071899414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.392459869384766 EntMin 0.0
Epoch 6, Batch 20/97, Loss=5.286618539690972
Loss made of: CE 0.43619295954704285, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.820463180541992 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.912741759419442
Loss made of: CE 0.32136231660842896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.454048156738281 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.285820946097374
Loss made of: CE 0.36316153407096863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.863034725189209 EntMin 0.0
Epoch 6, Batch 50/97, Loss=5.195182022452355
Loss made of: CE 0.3180472254753113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4843668937683105 EntMin 0.0
Epoch 6, Batch 60/97, Loss=5.085867571830749
Loss made of: CE 0.33848971128463745, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7352070808410645 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.989348912239075
Loss made of: CE 0.4771055579185486, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.582516670227051 EntMin 0.0
Epoch 6, Batch 80/97, Loss=5.159917998313904
Loss made of: CE 0.4336031377315521, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.810605525970459 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.9047425329685215
Loss made of: CE 0.35508716106414795, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.381519794464111 EntMin 0.0
Epoch 6, Class Loss=0.3776664733886719, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.3776664733886719, Class Loss=0.3776664733886719, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.981607121229172
Loss made of: CE 0.9239203333854675, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.677181720733643 EntMin 0.0
Epoch 1, Class Loss=0.7548485994338989, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.7548485994338989, Class Loss=0.7548485994338989, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=5.78219318985939
Loss made of: CE 0.5974018573760986, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.60076379776001 EntMin 0.0
Epoch 2, Class Loss=0.685893714427948, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.685893714427948, Class Loss=0.685893714427948, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=5.628778356313705
Loss made of: CE 0.5962096452713013, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.181285858154297 EntMin 0.0
Epoch 3, Class Loss=0.6540836095809937, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.6540836095809937, Class Loss=0.6540836095809937, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 10/12, Loss=5.637161070108414
Loss made of: CE 0.6096125245094299, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.801391124725342 EntMin 0.0
Epoch 4, Class Loss=0.6218677759170532, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.6218677759170532, Class Loss=0.6218677759170532, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.382197815179825
Loss made of: CE 0.6298714876174927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.989198684692383 EntMin 0.0
Epoch 5, Class Loss=0.5706769227981567, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5706769227981567, Class Loss=0.5706769227981567, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=5.422555390000343
Loss made of: CE 0.5781136751174927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.697977066040039 EntMin 0.0
Epoch 6, Class Loss=0.5607524514198303, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.5607524514198303, Class Loss=0.5607524514198303, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.8218388736248015
Loss made of: CE 0.6722583174705505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.748465061187744 EntMin 0.0
Epoch 1, Class Loss=0.6770166158676147, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.6770166158676147, Class Loss=0.6770166158676147, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.706113249063492
Loss made of: CE 0.7082154154777527, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.601478099822998 EntMin 0.0
Epoch 2, Class Loss=0.6297999024391174, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.6297999024391174, Class Loss=0.6297999024391174, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.627464792132377
Loss made of: CE 0.5433156490325928, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.396275520324707 EntMin 0.0
Epoch 3, Class Loss=0.5887961387634277, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.5887961387634277, Class Loss=0.5887961387634277, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=5.611919343471527
Loss made of: CE 0.5006536245346069, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.306610584259033 EntMin 0.0
Epoch 4, Class Loss=0.5757200121879578, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.5757200121879578, Class Loss=0.5757200121879578, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.582742393016815
Loss made of: CE 0.4556920528411865, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.01624870300293 EntMin 0.0
Epoch 5, Class Loss=0.5563998222351074, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.5563998222351074, Class Loss=0.5563998222351074, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.400035339593887
Loss made of: CE 0.5755126476287842, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.816510200500488 EntMin 0.0
Epoch 6, Class Loss=0.537796676158905, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.537796676158905, Class Loss=0.537796676158905, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.92016630768776
Loss made of: CE 0.5230637788772583, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.200185775756836 EntMin 0.0
Epoch 1, Class Loss=0.7023011445999146, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.7023011445999146, Class Loss=0.7023011445999146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.923071318864823
Loss made of: CE 0.7100052237510681, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0800395011901855 EntMin 0.0
Epoch 2, Class Loss=0.6685128211975098, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.6685128211975098, Class Loss=0.6685128211975098, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.8407539367675785
Loss made of: CE 0.7460908889770508, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.163461685180664 EntMin 0.0
Epoch 3, Class Loss=0.6140646934509277, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.6140646934509277, Class Loss=0.6140646934509277, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=5.574207812547684
Loss made of: CE 0.7646592855453491, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.07394552230835 EntMin 0.0
Epoch 4, Class Loss=0.5797404646873474, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5797404646873474, Class Loss=0.5797404646873474, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.566068017482758
Loss made of: CE 0.6034388542175293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8858723640441895 EntMin 0.0
Epoch 5, Class Loss=0.57585608959198, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.57585608959198, Class Loss=0.57585608959198, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.421616500616073
Loss made of: CE 0.5458298921585083, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.164670944213867 EntMin 0.0
Epoch 6, Class Loss=0.5439640283584595, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5439640283584595, Class Loss=0.5439640283584595, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5756625533103943, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.799051
Mean Acc: 0.431313
FreqW Acc: 0.657839
Mean IoU: 0.358342
Class IoU:
	class 0: 0.80426717
	class 1: 0.6752478
	class 2: 0.33641258
	class 3: 0.6160659
	class 4: 0.54795116
	class 5: 0.67710155
	class 6: 0.47977516
	class 7: 0.8214638
	class 8: 0.7304524
	class 9: 8.357763e-06
	class 10: 0.0
	class 11: 0.0
	class 12: 0.06857766
	class 13: 0.21040238
	class 14: 0.0
	class 15: 0.1240897
	class 16: 0.0
Class Acc:
	class 0: 0.9895222
	class 1: 0.6799369
	class 2: 0.823958
	class 3: 0.61991256
	class 4: 0.5858849
	class 5: 0.718678
	class 6: 0.48114005
	class 7: 0.8775388
	class 8: 0.762558
	class 9: 8.357763e-06
	class 10: 0.0
	class 11: 0.0
	class 12: 0.06899921
	class 13: 0.6000069
	class 14: 0.0
	class 15: 0.1241834
	class 16: 0.0

federated global round: 22, step: 4
select part of clients to conduct local training
[21, 0, 25, 23]
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=6.254608592391014
Loss made of: CE 0.6452063322067261, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.27935791015625 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.692099687457085
Loss made of: CE 0.47196027636528015, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.051019668579102 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.519362136721611
Loss made of: CE 0.42405247688293457, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.04014253616333 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.594436934590339
Loss made of: CE 0.4823596477508545, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.458065032958984 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.348121637105942
Loss made of: CE 0.39040908217430115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.493692398071289 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.333664971590042
Loss made of: CE 0.3933975100517273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.620719909667969 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.2785394310951235
Loss made of: CE 0.4206700325012207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.602254867553711 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.126759958267212
Loss made of: CE 0.3787485957145691, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.712206840515137 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.154983818531036
Loss made of: CE 0.34606286883354187, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.309403419494629 EntMin 0.0
Epoch 1, Class Loss=0.45697444677352905, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.45697444677352905, Class Loss=0.45697444677352905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=5.096961170434952
Loss made of: CE 0.38361310958862305, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.795485496520996 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.153415268659591
Loss made of: CE 0.33581575751304626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4454755783081055 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.248616996407509
Loss made of: CE 0.3683851659297943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.628274917602539 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.2625915765762326
Loss made of: CE 0.34641823172569275, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.253636360168457 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.113863596320153
Loss made of: CE 0.3556249141693115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4445319175720215 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.136388915777206
Loss made of: CE 0.3658255934715271, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.14902400970459 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.086804082989692
Loss made of: CE 0.3120515048503876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.536226749420166 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.246545574069023
Loss made of: CE 0.39269623160362244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6239495277404785 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.950437539815903
Loss made of: CE 0.39227479696273804, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.124095916748047 EntMin 0.0
Epoch 2, Class Loss=0.38160601258277893, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.38160601258277893, Class Loss=0.38160601258277893, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=5.046238377690315
Loss made of: CE 0.5198283195495605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7812018394470215 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.181505563855171
Loss made of: CE 0.37121570110321045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.544466018676758 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.058962780237198
Loss made of: CE 0.4049862325191498, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.269110202789307 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.950031352043152
Loss made of: CE 0.3241213858127594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.589258193969727 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.982914918661118
Loss made of: CE 0.3860663175582886, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.328113555908203 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.146789020299911
Loss made of: CE 0.3850715160369873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.586433410644531 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.1021935731172565
Loss made of: CE 0.4237311780452728, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.683398246765137 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.120405489206314
Loss made of: CE 0.3973648250102997, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.606855869293213 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.941578376293182
Loss made of: CE 0.3600301444530487, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.182031631469727 EntMin 0.0
Epoch 3, Class Loss=0.37122881412506104, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.37122881412506104, Class Loss=0.37122881412506104, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=5.061827519536019
Loss made of: CE 0.33435583114624023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478394508361816 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.025376760959626
Loss made of: CE 0.39844441413879395, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.277958393096924 EntMin 0.0
Epoch 4, Batch 30/97, Loss=5.131264311075211
Loss made of: CE 0.3547554910182953, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.777822494506836 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.97460036277771
Loss made of: CE 0.43401360511779785, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.874418258666992 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.064744773507118
Loss made of: CE 0.31366077065467834, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.909527778625488 EntMin 0.0
Epoch 4, Batch 60/97, Loss=5.1381251275539395
Loss made of: CE 0.42519718408584595, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.233275890350342 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.083204165101051
Loss made of: CE 0.34529909491539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.587708473205566 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.179339560866356
Loss made of: CE 0.30989864468574524, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5538787841796875 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.307019317150116
Loss made of: CE 0.42948731780052185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.674936294555664 EntMin 0.0
Epoch 4, Class Loss=0.36521095037460327, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.36521095037460327, Class Loss=0.36521095037460327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=5.057983455061913
Loss made of: CE 0.32535117864608765, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.943601608276367 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.987556648254395
Loss made of: CE 0.41994667053222656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.984428882598877 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.925251817703247
Loss made of: CE 0.44810816645622253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.633007526397705 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.1525552600622175
Loss made of: CE 0.3667551875114441, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.822149753570557 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.8802240014076235
Loss made of: CE 0.31350937485694885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.649929046630859 EntMin 0.0
Epoch 5, Batch 60/97, Loss=5.224388435482979
Loss made of: CE 0.360691636800766, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.647985458374023 EntMin 0.0
Epoch 5, Batch 70/97, Loss=5.177601793408394
Loss made of: CE 0.31000816822052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6218581199646 EntMin 0.0
Epoch 5, Batch 80/97, Loss=5.132735186815262
Loss made of: CE 0.4468313157558441, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1889753341674805 EntMin 0.0
Epoch 5, Batch 90/97, Loss=5.125733438134193
Loss made of: CE 0.3839516043663025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.829527854919434 EntMin 0.0
Epoch 5, Class Loss=0.37234020233154297, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.37234020233154297, Class Loss=0.37234020233154297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.851010423898697
Loss made of: CE 0.32111847400665283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.721741199493408 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.8856352180242535
Loss made of: CE 0.324160099029541, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7968363761901855 EntMin 0.0
Epoch 6, Batch 30/97, Loss=5.111357980966568
Loss made of: CE 0.4133809804916382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.182562828063965 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.014360174536705
Loss made of: CE 0.36776965856552124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.465259552001953 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.825913909077644
Loss made of: CE 0.3535783290863037, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4285888671875 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.862220567464829
Loss made of: CE 0.3219877779483795, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.894215106964111 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.812757495045662
Loss made of: CE 0.34287965297698975, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.144678115844727 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.7465989798307415
Loss made of: CE 0.3385009467601776, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.362334251403809 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.694745108485222
Loss made of: CE 0.3646813631057739, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.110284328460693 EntMin 0.0
Epoch 6, Class Loss=0.3572264015674591, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.3572264015674591, Class Loss=0.3572264015674591, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=6.065567064285278
Loss made of: CE 0.7374711036682129, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.339280128479004 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.538556948304176
Loss made of: CE 0.4540260434150696, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.465789794921875 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.414869114756584
Loss made of: CE 0.46320998668670654, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.06907320022583 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.652403390407562
Loss made of: CE 0.44099920988082886, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.259002208709717 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.340840479731559
Loss made of: CE 0.5891731977462769, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.73246955871582 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.098382180929184
Loss made of: CE 0.3824997544288635, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.132174491882324 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.299270567297936
Loss made of: CE 0.5018822550773621, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.263154983520508 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.210000962018967
Loss made of: CE 0.4215583801269531, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.622997283935547 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.455397218465805
Loss made of: CE 0.34584328532218933, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.203471660614014 EntMin 0.0
Epoch 1, Class Loss=0.45586031675338745, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.45586031675338745, Class Loss=0.45586031675338745, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=5.275182458758354
Loss made of: CE 0.35988086462020874, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.713433742523193 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.216463926434517
Loss made of: CE 0.3796059787273407, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.574963092803955 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.301984316110611
Loss made of: CE 0.3960440158843994, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.148317813873291 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.954907345771789
Loss made of: CE 0.34468889236450195, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.617672920227051 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.189113563299179
Loss made of: CE 0.32062584161758423, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.456976890563965 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.162009593844414
Loss made of: CE 0.4057583808898926, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.980903148651123 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.249457514286041
Loss made of: CE 0.4202288091182709, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.922581672668457 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.16702247262001
Loss made of: CE 0.3169178366661072, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.061070919036865 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.172755393385887
Loss made of: CE 0.3600301146507263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.855590343475342 EntMin 0.0
Epoch 2, Class Loss=0.38462918996810913, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.38462918996810913, Class Loss=0.38462918996810913, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=5.034829363226891
Loss made of: CE 0.4390225112438202, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3660712242126465 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.9797438204288484
Loss made of: CE 0.297497034072876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481088161468506 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.304456031322479
Loss made of: CE 0.4780551791191101, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.942296028137207 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.3207140386104586
Loss made of: CE 0.389240562915802, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0885725021362305 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.313545134663582
Loss made of: CE 0.3935839533805847, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.339217185974121 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.286151006817818
Loss made of: CE 0.35182008147239685, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.996360778808594 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.320443975925445
Loss made of: CE 0.38980555534362793, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.263629913330078 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.22477505505085
Loss made of: CE 0.35254019498825073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.262905597686768 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.217295596003533
Loss made of: CE 0.33125677704811096, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.538390636444092 EntMin 0.0
Epoch 3, Class Loss=0.3815285563468933, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3815285563468933, Class Loss=0.3815285563468933, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.981791952252388
Loss made of: CE 0.32430022954940796, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.407969951629639 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.095074540376663
Loss made of: CE 0.38036561012268066, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.905647277832031 EntMin 0.0
Epoch 4, Batch 30/97, Loss=5.464867451786995
Loss made of: CE 0.4097837805747986, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.328357219696045 EntMin 0.0
Epoch 4, Batch 40/97, Loss=5.0842903643846515
Loss made of: CE 0.3445751368999481, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.065727233886719 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.973496729135514
Loss made of: CE 0.3755728602409363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.981104850769043 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.931209436058998
Loss made of: CE 0.38542765378952026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.550022125244141 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.1073196023702625
Loss made of: CE 0.31010493636131287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.385321617126465 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.922554048895836
Loss made of: CE 0.32589003443717957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128190994262695 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.246745926141739
Loss made of: CE 0.36022377014160156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.513097286224365 EntMin 0.0
Epoch 4, Class Loss=0.37494662404060364, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.37494662404060364, Class Loss=0.37494662404060364, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.829241690039635
Loss made of: CE 0.302662193775177, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.173558235168457 EntMin 0.0
Epoch 5, Batch 20/97, Loss=5.0802903980016705
Loss made of: CE 0.3555181920528412, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5486249923706055 EntMin 0.0
Epoch 5, Batch 30/97, Loss=5.064161801338196
Loss made of: CE 0.39063093066215515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.492504119873047 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.1613204151391985
Loss made of: CE 0.33789992332458496, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.015771389007568 EntMin 0.0
Epoch 5, Batch 50/97, Loss=5.0658084690570835
Loss made of: CE 0.378483384847641, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500133991241455 EntMin 0.0
Epoch 5, Batch 60/97, Loss=5.015101429820061
Loss made of: CE 0.4268009662628174, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.167605876922607 EntMin 0.0
Epoch 5, Batch 70/97, Loss=5.101202207803726
Loss made of: CE 0.30734726786613464, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.22426176071167 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.7271686643362045
Loss made of: CE 0.359435498714447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.396801471710205 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.753241688013077
Loss made of: CE 0.38965702056884766, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.238602638244629 EntMin 0.0
Epoch 5, Class Loss=0.36399585008621216, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.36399585008621216, Class Loss=0.36399585008621216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.811288928985595
Loss made of: CE 0.28928911685943604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.316400527954102 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.982252004742622
Loss made of: CE 0.35232624411582947, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.119942665100098 EntMin 0.0
Epoch 6, Batch 30/97, Loss=5.179599431157112
Loss made of: CE 0.3607785403728485, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.18318510055542 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.280768275260925
Loss made of: CE 0.35890424251556396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.980754852294922 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.939524629712105
Loss made of: CE 0.3486318290233612, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251705169677734 EntMin 0.0
Epoch 6, Batch 60/97, Loss=5.196367222070694
Loss made of: CE 0.319735050201416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.328255653381348 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.91602072417736
Loss made of: CE 0.3603024482727051, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.532642364501953 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.946952319145202
Loss made of: CE 0.32024186849594116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.524556636810303 EntMin 0.0
Epoch 6, Batch 90/97, Loss=5.126054409146309
Loss made of: CE 0.35176846385002136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.399786949157715 EntMin 0.0
Epoch 6, Class Loss=0.3692525625228882, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.3692525625228882, Class Loss=0.3692525625228882, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.978471636772156
Loss made of: CE 0.6395244598388672, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.219832420349121 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.7560844123363495
Loss made of: CE 0.5346322059631348, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.097915172576904 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.505741432309151
Loss made of: CE 0.46156883239746094, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.298925399780273 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.366631805896759
Loss made of: CE 0.4683484435081482, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.732804775238037 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.356972014904022
Loss made of: CE 0.44001442193984985, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.790971755981445 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.2744334667921065
Loss made of: CE 0.36712348461151123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.706657886505127 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.427089312672615
Loss made of: CE 0.3832198977470398, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.23112154006958 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.297099995613098
Loss made of: CE 0.45206815004348755, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.009191989898682 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.440617921948433
Loss made of: CE 0.3105587959289551, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.88767147064209 EntMin 0.0
Epoch 1, Class Loss=0.46046122908592224, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.46046122908592224, Class Loss=0.46046122908592224, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.990597134828567
Loss made of: CE 0.404230535030365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.823840141296387 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.328256094455719
Loss made of: CE 0.34448954463005066, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.380989074707031 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.204005688428879
Loss made of: CE 0.44585275650024414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.299951553344727 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.359920674562455
Loss made of: CE 0.39602920413017273, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.326157569885254 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.2274197280406955
Loss made of: CE 0.42291325330734253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.531621932983398 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.386848655343056
Loss made of: CE 0.3620854616165161, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.843765735626221 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.184520331025124
Loss made of: CE 0.4622669219970703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.84500789642334 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.069981792569161
Loss made of: CE 0.31463199853897095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.418386459350586 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.118517437577248
Loss made of: CE 0.3424847722053528, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6238226890563965 EntMin 0.0
Epoch 2, Class Loss=0.39183980226516724, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.39183980226516724, Class Loss=0.39183980226516724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=5.005025911331177
Loss made of: CE 0.37257644534111023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.599557876586914 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.092763352394104
Loss made of: CE 0.45399221777915955, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.530550956726074 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.060735794901848
Loss made of: CE 0.420040488243103, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3777689933776855 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.261848962306976
Loss made of: CE 0.40908631682395935, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.434597015380859 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.326611298322677
Loss made of: CE 0.38959187269210815, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.63370418548584 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.232750403881073
Loss made of: CE 0.4334469735622406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.735217094421387 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.105055186152458
Loss made of: CE 0.5469550490379333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679028511047363 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.043223085999489
Loss made of: CE 0.36271461844444275, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.077498435974121 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.095145043730736
Loss made of: CE 0.33487996459007263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.788580894470215 EntMin 0.0
Epoch 3, Class Loss=0.37934747338294983, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.37934747338294983, Class Loss=0.37934747338294983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=5.000631782412529
Loss made of: CE 0.30872008204460144, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.243225574493408 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.049999868869781
Loss made of: CE 0.42071935534477234, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.614692211151123 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.88318358361721
Loss made of: CE 0.33266520500183105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.496744632720947 EntMin 0.0
Epoch 4, Batch 40/97, Loss=5.075485813617706
Loss made of: CE 0.38885611295700073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.754334449768066 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.301159608364105
Loss made of: CE 0.4926108121871948, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.670932769775391 EntMin 0.0
Epoch 4, Batch 60/97, Loss=5.563550981879234
Loss made of: CE 0.38996565341949463, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6532697677612305 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.169192209839821
Loss made of: CE 0.4187783896923065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.422152519226074 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.128003093600273
Loss made of: CE 0.5229079127311707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.309076309204102 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.064611008763313
Loss made of: CE 0.310924768447876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645698070526123 EntMin 0.0
Epoch 4, Class Loss=0.37666434049606323, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.37666434049606323, Class Loss=0.37666434049606323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.8602119326591495
Loss made of: CE 0.3617430627346039, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5178375244140625 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.790849184989929
Loss made of: CE 0.3277774751186371, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.44310998916626 EntMin 0.0
Epoch 5, Batch 30/97, Loss=5.3255594104528425
Loss made of: CE 0.3486858010292053, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.894519805908203 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.3397550761699675
Loss made of: CE 0.29618650674819946, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.034488677978516 EntMin 0.0
Epoch 5, Batch 50/97, Loss=5.109739845991134
Loss made of: CE 0.33357006311416626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.884217262268066 EntMin 0.0
Epoch 5, Batch 60/97, Loss=5.1681080758571625
Loss made of: CE 0.3512839674949646, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.498384475708008 EntMin 0.0
Epoch 5, Batch 70/97, Loss=5.098717865347862
Loss made of: CE 0.3515815734863281, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.406874656677246 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.935845506191254
Loss made of: CE 0.29029083251953125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.607822418212891 EntMin 0.0
Epoch 5, Batch 90/97, Loss=5.049151688814163
Loss made of: CE 0.4040302038192749, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.751162528991699 EntMin 0.0
Epoch 5, Class Loss=0.3689418435096741, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.3689418435096741, Class Loss=0.3689418435096741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.777558916807175
Loss made of: CE 0.3052816092967987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.486845970153809 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.927860537171364
Loss made of: CE 0.3556620478630066, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.216368675231934 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.963711932301521
Loss made of: CE 0.31386497616767883, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.020622253417969 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.830886793136597
Loss made of: CE 0.28073593974113464, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.528576374053955 EntMin 0.0
Epoch 6, Batch 50/97, Loss=5.005665048956871
Loss made of: CE 0.4518830180168152, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.913989067077637 EntMin 0.0
Epoch 6, Batch 60/97, Loss=5.104593631625176
Loss made of: CE 0.4659988582134247, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9380292892456055 EntMin 0.0
Epoch 6, Batch 70/97, Loss=5.059628710150719
Loss made of: CE 0.3520016372203827, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.501042366027832 EntMin 0.0
Epoch 6, Batch 80/97, Loss=5.012648487091065
Loss made of: CE 0.34550341963768005, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400869369506836 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.718232825398445
Loss made of: CE 0.3255455493927002, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.368926048278809 EntMin 0.0
Epoch 6, Class Loss=0.35840532183647156, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.35840532183647156, Class Loss=0.35840532183647156, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=6.50480634868145
Loss made of: CE 0.44768986105918884, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.416857719421387 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.322427225112915
Loss made of: CE 0.4433698058128357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.419301986694336 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.1947858989238735
Loss made of: CE 0.452620267868042, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664918899536133 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.459993517398834
Loss made of: CE 0.5683177709579468, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.575621604919434 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.307479444146156
Loss made of: CE 0.4396299123764038, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.98473596572876 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.097449526190758
Loss made of: CE 0.43303048610687256, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.00283145904541 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.194223383069039
Loss made of: CE 0.3804173469543457, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.143459320068359 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.260116067528725
Loss made of: CE 0.42568421363830566, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1984968185424805 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.4319293916225435
Loss made of: CE 0.3842541575431824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.258439064025879 EntMin 0.0
Epoch 1, Class Loss=0.46028682589530945, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.46028682589530945, Class Loss=0.46028682589530945, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=5.147846901416779
Loss made of: CE 0.3370843827724457, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.059276103973389 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.125479486584664
Loss made of: CE 0.40983420610427856, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.868932723999023 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.025856605172157
Loss made of: CE 0.38069090247154236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.634060859680176 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.155995857715607
Loss made of: CE 0.410134494304657, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.121640205383301 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.287191575765609
Loss made of: CE 0.36118966341018677, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.566720008850098 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.414616528153419
Loss made of: CE 0.4051027297973633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.711447238922119 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.232402804493904
Loss made of: CE 0.4349939525127411, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.800167083740234 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.257727372646332
Loss made of: CE 0.4045952260494232, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.950420379638672 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.394967186450958
Loss made of: CE 0.39052820205688477, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.53183650970459 EntMin 0.0
Epoch 2, Class Loss=0.3902687132358551, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.3902687132358551, Class Loss=0.3902687132358551, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=5.215645879507065
Loss made of: CE 0.4338979125022888, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.010446548461914 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.2821272850036625
Loss made of: CE 0.46721386909484863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.971039772033691 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.206923291087151
Loss made of: CE 0.3750132918357849, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.452999591827393 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.133239954710007
Loss made of: CE 0.4176056981086731, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7492289543151855 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.19650274515152
Loss made of: CE 0.35637742280960083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.80633020401001 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.005057781934738
Loss made of: CE 0.3655970096588135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.439847946166992 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.965757772326469
Loss made of: CE 0.3254433274269104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.629839897155762 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.904126951098442
Loss made of: CE 0.46112751960754395, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0230607986450195 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.238804438710213
Loss made of: CE 0.3452906310558319, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734320640563965 EntMin 0.0
Epoch 3, Class Loss=0.37473347783088684, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.37473347783088684, Class Loss=0.37473347783088684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=5.049498587846756
Loss made of: CE 0.38419151306152344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.812990665435791 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.097911435365677
Loss made of: CE 0.33353328704833984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.355372428894043 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.925380969047547
Loss made of: CE 0.31737834215164185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.618918418884277 EntMin 0.0
Epoch 4, Batch 40/97, Loss=5.1776853740215305
Loss made of: CE 0.35168886184692383, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.258190155029297 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.1026684731245044
Loss made of: CE 0.37189173698425293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.723297119140625 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.8649715185165405
Loss made of: CE 0.3803696632385254, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.602436065673828 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.086665758490563
Loss made of: CE 0.34422606229782104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.442593574523926 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.291529259085655
Loss made of: CE 0.29137933254241943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363327980041504 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.978392174839973
Loss made of: CE 0.3484835624694824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.753363132476807 EntMin 0.0
Epoch 4, Class Loss=0.3713887631893158, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.3713887631893158, Class Loss=0.3713887631893158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=5.082808232307434
Loss made of: CE 0.3246369957923889, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.544652938842773 EntMin 0.0
Epoch 5, Batch 20/97, Loss=5.064152052998542
Loss made of: CE 0.5325648784637451, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.723560333251953 EntMin 0.0
Epoch 5, Batch 30/97, Loss=5.111938419938087
Loss made of: CE 0.41112589836120605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.743434906005859 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.040101182460785
Loss made of: CE 0.3599594831466675, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.978841304779053 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.803025624155998
Loss made of: CE 0.3392329216003418, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.556517124176025 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.715381452441216
Loss made of: CE 0.2946053445339203, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.967071294784546 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.812094181776047
Loss made of: CE 0.34381064772605896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.311114311218262 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.825423365831375
Loss made of: CE 0.3530921936035156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.295081615447998 EntMin 0.0
Epoch 5, Batch 90/97, Loss=5.046060267090797
Loss made of: CE 0.38282737135887146, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.314400672912598 EntMin 0.0
Epoch 5, Class Loss=0.3624315857887268, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.3624315857887268, Class Loss=0.3624315857887268, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.849089893698692
Loss made of: CE 0.3134958744049072, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.287381172180176 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.9175446927547455
Loss made of: CE 0.33983278274536133, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3984174728393555 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.934011885523796
Loss made of: CE 0.4002543091773987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.542752742767334 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.790243539214134
Loss made of: CE 0.2719300389289856, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.987396240234375 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.983621254563332
Loss made of: CE 0.31676939129829407, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.328176498413086 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.906264275312424
Loss made of: CE 0.32322031259536743, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.633343696594238 EntMin 0.0
Epoch 6, Batch 70/97, Loss=5.01040917634964
Loss made of: CE 0.39536571502685547, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.229676723480225 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.943929648399353
Loss made of: CE 0.35368189215660095, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.153863430023193 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.9253921002149585
Loss made of: CE 0.38189074397087097, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.458873748779297 EntMin 0.0
Epoch 6, Class Loss=0.3641011118888855, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.3641011118888855, Class Loss=0.3641011118888855, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5400978326797485, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.842128
Mean Acc: 0.464170
FreqW Acc: 0.730856
Mean IoU: 0.392020
Class IoU:
	class 0: 0.85103595
	class 1: 0.68815327
	class 2: 0.31848356
	class 3: 0.5696997
	class 4: 0.5237806
	class 5: 0.6006971
	class 6: 0.5822149
	class 7: 0.8302986
	class 8: 0.7151192
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.07283476
	class 13: 0.22690062
	class 14: 0.0
	class 15: 0.6851233
	class 16: 0.0
Class Acc:
	class 0: 0.98732
	class 1: 0.6922159
	class 2: 0.76963663
	class 3: 0.5715742
	class 4: 0.5591197
	class 5: 0.6189776
	class 6: 0.58382535
	class 7: 0.88980573
	class 8: 0.74232507
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.07323063
	class 13: 0.6545863
	class 14: 0.0
	class 15: 0.7482691
	class 16: 0.0

federated global round: 23, step: 4
select part of clients to conduct local training
[17, 3, 5, 22]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.5455872297286986
Loss made of: CE 0.43768879771232605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.35015344619751 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/97, Loss=4.582782664895058
Loss made of: CE 0.36591196060180664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6994788646698 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.732689246535301
Loss made of: CE 0.3267040550708771, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.151700496673584 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.764470186829567
Loss made of: CE 0.3021734356880188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8649020195007324 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.798871624469757
Loss made of: CE 0.33303698897361755, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8907623291015625 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.793398308753967
Loss made of: CE 0.32765766978263855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.351400375366211 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.611465007066727
Loss made of: CE 0.25860992074012756, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.192739486694336 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.934577235579491
Loss made of: CE 0.3182342052459717, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.438371658325195 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.885518339276314
Loss made of: CE 0.33094826340675354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3797926902771 EntMin 0.0
Epoch 1, Class Loss=0.34296363592147827, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.34296363592147827, Class Loss=0.34296363592147827, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.796702754497528
Loss made of: CE 0.25929102301597595, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.095211982727051 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.633472833037376
Loss made of: CE 0.33215370774269104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4602460861206055 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.065100711584091
Loss made of: CE 0.46771273016929626, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.141949653625488 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.961164820194244
Loss made of: CE 0.3596068322658539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664551734924316 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.824269726872444
Loss made of: CE 0.35758739709854126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.627126216888428 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.99731967151165
Loss made of: CE 0.3311161696910858, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.754500389099121 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.121578270196915
Loss made of: CE 0.3654719293117523, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.817956924438477 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.954247549176216
Loss made of: CE 0.442847341299057, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.398309230804443 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.717007723450661
Loss made of: CE 0.2985907793045044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.199692249298096 EntMin 0.0
Epoch 2, Class Loss=0.35566049814224243, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.35566049814224243, Class Loss=0.35566049814224243, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.882298803329467
Loss made of: CE 0.3491438627243042, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.545766353607178 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.914503672719002
Loss made of: CE 0.3996017873287201, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.036140441894531 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.671025869250298
Loss made of: CE 0.3806643486022949, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.451864719390869 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.058481389284134
Loss made of: CE 0.4627797305583954, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6587138175964355 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.792727389931679
Loss made of: CE 0.743849515914917, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.559911727905273 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.220582374930382
Loss made of: CE 0.3838365972042084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9242329597473145 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.106768196821212
Loss made of: CE 0.3506372570991516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.770188331604004 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.020917907357216
Loss made of: CE 0.4598960280418396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.385577201843262 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.107050609588623
Loss made of: CE 0.3800690174102783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.562859058380127 EntMin 0.0
Epoch 3, Class Loss=0.3547341525554657, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.3547341525554657, Class Loss=0.3547341525554657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.925511649250984
Loss made of: CE 0.3780561089515686, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.560081481933594 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.3117254108190535
Loss made of: CE 0.6969539523124695, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.190463542938232 EntMin 0.0
Epoch 4, Batch 30/97, Loss=5.074088987708092
Loss made of: CE 0.33315932750701904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.240443229675293 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.92962307035923
Loss made of: CE 0.43819865584373474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.265525817871094 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.059521290659904
Loss made of: CE 0.32316944003105164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.55342435836792 EntMin 0.0
Epoch 4, Batch 60/97, Loss=5.086726924777031
Loss made of: CE 0.3193761706352234, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.389598846435547 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.8440558314323425
Loss made of: CE 0.41052281856536865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389196395874023 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.911583116650581
Loss made of: CE 0.3029857873916626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4713945388793945 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.6561066776514055
Loss made of: CE 0.2583303451538086, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.36204195022583 EntMin 0.0
Epoch 4, Class Loss=0.35528701543807983, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.35528701543807983, Class Loss=0.35528701543807983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.643759605288506
Loss made of: CE 0.3606269061565399, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.781525611877441 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.907694157958031
Loss made of: CE 0.41853201389312744, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.683574676513672 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.746967560052871
Loss made of: CE 0.330035924911499, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.364702224731445 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.906617546081543
Loss made of: CE 0.2999592423439026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.587316513061523 EntMin 0.0
Epoch 5, Batch 50/97, Loss=5.041469517350197
Loss made of: CE 0.32646337151527405, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.15505313873291 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.95702648460865
Loss made of: CE 0.39654025435447693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594907760620117 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.928080645203591
Loss made of: CE 0.5165669322013855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.775210857391357 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.850804591178894
Loss made of: CE 0.3675721287727356, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53585958480835 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.907309499382973
Loss made of: CE 0.30935320258140564, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.392736911773682 EntMin 0.0
Epoch 5, Class Loss=0.349435031414032, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.349435031414032, Class Loss=0.349435031414032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.8960230350494385
Loss made of: CE 0.33037877082824707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.767155647277832 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.710302194952964
Loss made of: CE 0.3458399176597595, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.133781433105469 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.730764997005463
Loss made of: CE 0.30396246910095215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.499827861785889 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.65489609092474
Loss made of: CE 0.31421321630477905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.080818176269531 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.883450770378113
Loss made of: CE 0.2730088233947754, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.602997303009033 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.7624637737870215
Loss made of: CE 0.22553293406963348, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.19411563873291 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.828055998682975
Loss made of: CE 0.33172300457954407, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8797454833984375 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.7569669231772425
Loss made of: CE 0.34651488065719604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1677141189575195 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.9566395580768585
Loss made of: CE 0.3506720960140228, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5068359375 EntMin 0.0
Epoch 6, Class Loss=0.34698835015296936, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.34698835015296936, Class Loss=0.34698835015296936, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=7.382969212532044
Loss made of: CE 0.9735358953475952, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.484409332275391 EntMin 0.0
Epoch 1, Class Loss=1.1719584465026855, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=1.1719584465026855, Class Loss=1.1719584465026855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=6.5037815272808075
Loss made of: CE 0.8774195909500122, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3280439376831055 EntMin 0.0
Epoch 2, Class Loss=0.9969958662986755, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.9969958662986755, Class Loss=0.9969958662986755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.931995221972466
Loss made of: CE 0.4410460889339447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.779251575469971 EntMin 0.0
Epoch 3, Class Loss=0.8506046533584595, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.8506046533584595, Class Loss=0.8506046533584595, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.896797055006028
Loss made of: CE 0.49301677942276, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.087969779968262 EntMin 0.0
Epoch 4, Class Loss=0.7298356890678406, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.7298356890678406, Class Loss=0.7298356890678406, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.516940236091614
Loss made of: CE 0.6042676568031311, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.80464506149292 EntMin 0.0
Epoch 5, Class Loss=0.6410878300666809, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.6410878300666809, Class Loss=0.6410878300666809, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.549268594384193
Loss made of: CE 0.432686984539032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.801385879516602 EntMin 0.0
Epoch 6, Class Loss=0.5748380422592163, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.5748380422592163, Class Loss=0.5748380422592163, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.427107262611389
Loss made of: CE 0.39466673135757446, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9349448680877686 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.717122933268547
Loss made of: CE 0.48253363370895386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.535346984863281 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 30/97, Loss=4.673502767086029
Loss made of: CE 0.32972824573516846, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590249538421631 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.763612234592438
Loss made of: CE 0.3600843846797943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.29079008102417 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.8465472787618635
Loss made of: CE 0.34585055708885193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.434372901916504 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.742014095187187
Loss made of: CE 0.40182971954345703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.075077056884766 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.861313900351524
Loss made of: CE 0.43187516927719116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.100215911865234 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.0644449353218075
Loss made of: CE 0.37640655040740967, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.377659797668457 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.9493398606777195
Loss made of: CE 0.4212214946746826, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.723954200744629 EntMin 0.0
Epoch 1, Class Loss=0.3577391505241394, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.3577391505241394, Class Loss=0.3577391505241394, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.883663216233254
Loss made of: CE 0.34951621294021606, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645843982696533 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.977531704306602
Loss made of: CE 0.3649384379386902, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.165593147277832 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.979299175739288
Loss made of: CE 0.4250245988368988, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.964873790740967 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.860150653123855
Loss made of: CE 0.37899503111839294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.353259086608887 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.050864455103874
Loss made of: CE 0.43764424324035645, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.072027206420898 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.033837795257568
Loss made of: CE 0.36294519901275635, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.410897254943848 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.904636150598526
Loss made of: CE 0.3179178833961487, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.867663383483887 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.988257122039795
Loss made of: CE 0.4533085525035858, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5019755363464355 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.049830064177513
Loss made of: CE 0.3255971074104309, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.026915550231934 EntMin 0.0
Epoch 2, Class Loss=0.3711112141609192, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3711112141609192, Class Loss=0.3711112141609192, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.979471433162689
Loss made of: CE 0.3169568181037903, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.376437664031982 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.954035791754722
Loss made of: CE 0.3431427478790283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.886838436126709 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.84885983467102
Loss made of: CE 0.29803240299224854, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.122066020965576 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.015045994520188
Loss made of: CE 0.32297953963279724, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.207769870758057 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.9446504563093185
Loss made of: CE 0.32351475954055786, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.465640544891357 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.8902287721633915
Loss made of: CE 0.5832165479660034, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.418558120727539 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.757854101061821
Loss made of: CE 0.29305070638656616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.166013717651367 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.7986119270324705
Loss made of: CE 0.3316650390625, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.241109848022461 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.801205658912659
Loss made of: CE 0.3242483139038086, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8133344650268555 EntMin 0.0
Epoch 3, Class Loss=0.35424959659576416, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.35424959659576416, Class Loss=0.35424959659576416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.937746575474739
Loss made of: CE 0.3977174162864685, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.570619583129883 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.739150750637054
Loss made of: CE 0.3453677296638489, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.231204032897949 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.834395569562912
Loss made of: CE 0.30737340450286865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.488059043884277 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.747347512841225
Loss made of: CE 0.3769989609718323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.125055313110352 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.6915099859237674
Loss made of: CE 0.3668921887874603, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358465194702148 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.839709338545799
Loss made of: CE 0.37016743421554565, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.761427402496338 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.769613790512085
Loss made of: CE 0.3623620271682739, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.638128280639648 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.690000775456428
Loss made of: CE 0.33762046694755554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.069965839385986 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.852985599637032
Loss made of: CE 0.31878662109375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.188999176025391 EntMin 0.0
Epoch 4, Class Loss=0.3475755751132965, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.3475755751132965, Class Loss=0.3475755751132965, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.963045704364776
Loss made of: CE 0.3622938096523285, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.585695266723633 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.88322468996048
Loss made of: CE 0.34221991896629333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.329860210418701 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.756366902589798
Loss made of: CE 0.4188154935836792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5042924880981445 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.616446170210838
Loss made of: CE 0.4367559850215912, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.528405666351318 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.765427654981613
Loss made of: CE 0.3624102473258972, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.161752223968506 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.942114716768264
Loss made of: CE 0.3426092565059662, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.481176376342773 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.870520308613777
Loss made of: CE 0.3056468665599823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8571038246154785 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.9176301717758175
Loss made of: CE 0.32532674074172974, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1004109382629395 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.787196081876755
Loss made of: CE 0.35026830434799194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.269064426422119 EntMin 0.0
Epoch 5, Class Loss=0.34698787331581116, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.34698787331581116, Class Loss=0.34698787331581116, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.707753658294678
Loss made of: CE 0.3902081847190857, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2583208084106445 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.584714597463607
Loss made of: CE 0.3026049733161926, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.89479923248291 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.572479695081711
Loss made of: CE 0.36267203092575073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.183280944824219 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.895415362715721
Loss made of: CE 0.39511817693710327, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.114112377166748 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.557169085741043
Loss made of: CE 0.33900997042655945, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.673496723175049 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.696464046835899
Loss made of: CE 0.36085304617881775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.836029529571533 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.524987941980362
Loss made of: CE 0.3459928035736084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.37869119644165 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.601608520746231
Loss made of: CE 0.367048442363739, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358218193054199 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.66206211745739
Loss made of: CE 0.31308138370513916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.058114051818848 EntMin 0.0
Epoch 6, Class Loss=0.35125431418418884, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.35125431418418884, Class Loss=0.35125431418418884, Reg Loss=0.0
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=7.410899668931961
Loss made of: CE 1.4415305852890015, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.841380596160889 EntMin 0.0
Epoch 1, Class Loss=1.1782909631729126, Reg Loss=0.0
Clinet index 22, End of Epoch 1/6, Average Loss=1.1782909631729126, Class Loss=1.1782909631729126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=6.315324604511261
Loss made of: CE 0.6475560069084167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9622697830200195 EntMin 0.0
Epoch 2, Class Loss=0.9438868761062622, Reg Loss=0.0
Clinet index 22, End of Epoch 2/6, Average Loss=0.9438868761062622, Class Loss=0.9438868761062622, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.8547838687896725
Loss made of: CE 0.9900100231170654, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.147408962249756 EntMin 0.0
Epoch 3, Class Loss=0.834602952003479, Reg Loss=0.0
Clinet index 22, End of Epoch 3/6, Average Loss=0.834602952003479, Class Loss=0.834602952003479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.7954763025045395
Loss made of: CE 0.6809234619140625, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.125348091125488 EntMin 0.0
Epoch 4, Class Loss=0.7304022312164307, Reg Loss=0.0
Clinet index 22, End of Epoch 4/6, Average Loss=0.7304022312164307, Class Loss=0.7304022312164307, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.442107620835304
Loss made of: CE 0.4252496361732483, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.358172416687012 EntMin 0.0
Epoch 5, Class Loss=0.6103896498680115, Reg Loss=0.0
Clinet index 22, End of Epoch 5/6, Average Loss=0.6103896498680115, Class Loss=0.6103896498680115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.240396562218666
Loss made of: CE 0.4967386722564697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.607767581939697 EntMin 0.0
Epoch 6, Class Loss=0.5482749342918396, Reg Loss=0.0
Clinet index 22, End of Epoch 6/6, Average Loss=0.5482749342918396, Class Loss=0.5482749342918396, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5122241973876953, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.847504
Mean Acc: 0.483283
FreqW Acc: 0.741793
Mean IoU: 0.407007
Class IoU:
	class 0: 0.8597301
	class 1: 0.7180286
	class 2: 0.33952007
	class 3: 0.5669897
	class 4: 0.5494164
	class 5: 0.6881993
	class 6: 0.56656915
	class 7: 0.8326221
	class 8: 0.77544826
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.10574382
	class 13: 0.21904367
	class 14: 0.0
	class 15: 0.6978112
	class 16: 0.0
Class Acc:
	class 0: 0.98603004
	class 1: 0.7237353
	class 2: 0.79065746
	class 3: 0.56905824
	class 4: 0.58850825
	class 5: 0.726429
	class 6: 0.56816375
	class 7: 0.8937154
	class 8: 0.8237923
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.106513806
	class 13: 0.6783531
	class 14: 0.0
	class 15: 0.76086086
	class 16: 0.0

federated global round: 24, step: 4
select part of clients to conduct local training
[16, 9, 12, 2]
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=4.947291746735573
Loss made of: CE 0.30913710594177246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.95591926574707 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.618572950363159
Loss made of: CE 0.33628207445144653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.309664726257324 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.66664653122425
Loss made of: CE 0.32761886715888977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.038055419921875 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.6622826099395756
Loss made of: CE 0.39822763204574585, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.507472515106201 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.634977608919144
Loss made of: CE 0.4508971571922302, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.889357566833496 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.005508425831795
Loss made of: CE 0.39818811416625977, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.486939430236816 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.817155578732491
Loss made of: CE 0.29504331946372986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.786616086959839 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.791230022907257
Loss made of: CE 0.37762922048568726, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.049561500549316 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.715059578418732
Loss made of: CE 0.4370715618133545, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6271538734436035 EntMin 0.0
Epoch 1, Class Loss=0.3591088354587555, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.3591088354587555, Class Loss=0.3591088354587555, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/97, Loss=4.881197413802147
Loss made of: CE 0.262434720993042, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.554028034210205 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.686987960338593
Loss made of: CE 0.32625406980514526, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.308300495147705 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.9009808003902435
Loss made of: CE 0.37055569887161255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.849767684936523 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.945497554540634
Loss made of: CE 0.3962854743003845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.699008941650391 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.656936037540436
Loss made of: CE 0.27035194635391235, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.510809898376465 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.666967850923538
Loss made of: CE 0.42813611030578613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.571331024169922 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.745673224329948
Loss made of: CE 0.43182671070098877, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.635882377624512 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.741074824333191
Loss made of: CE 0.3077901005744934, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.284587860107422 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.826644802093506
Loss made of: CE 0.38723549246788025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.895941734313965 EntMin 0.0
Epoch 2, Class Loss=0.3538963496685028, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.3538963496685028, Class Loss=0.3538963496685028, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/97, Loss=4.445245110988617
Loss made of: CE 0.33722037076950073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.014176368713379 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.6284485220909115
Loss made of: CE 0.3378797769546509, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.345592975616455 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.941255417466164
Loss made of: CE 0.36876964569091797, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.961076259613037 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.038759678602219
Loss made of: CE 0.3094330430030823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1953911781311035 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.7205641090869905
Loss made of: CE 0.26978036761283875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590921401977539 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.769648694992066
Loss made of: CE 0.3230086863040924, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4075236320495605 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.967590257525444
Loss made of: CE 0.6939266920089722, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.347175121307373 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.860524821281433
Loss made of: CE 0.37941834330558777, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.724304676055908 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.510136191546917
Loss made of: CE 0.27992701530456543, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.036190032958984 EntMin 0.0
Epoch 3, Class Loss=0.35284775495529175, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.35284775495529175, Class Loss=0.35284775495529175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/97, Loss=4.658040875196457
Loss made of: CE 0.4072159230709076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.766292095184326 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.498085954785347
Loss made of: CE 0.31171923875808716, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9637649059295654 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.620191958546639
Loss made of: CE 0.2885105609893799, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7769575119018555 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.571441438794136
Loss made of: CE 0.35735639929771423, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6341233253479 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.665799075365067
Loss made of: CE 0.2866622507572174, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.080350399017334 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.606040230393409
Loss made of: CE 0.2864208221435547, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.155848503112793 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.656031066179276
Loss made of: CE 0.3032335042953491, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.463161468505859 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.556837871670723
Loss made of: CE 0.3694142997264862, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.291943073272705 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.514463552832604
Loss made of: CE 0.3640286326408386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.43894100189209 EntMin 0.0
Epoch 4, Class Loss=0.34499382972717285, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.34499382972717285, Class Loss=0.34499382972717285, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/97, Loss=4.521788996458054
Loss made of: CE 0.3316490054130554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.512862205505371 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.6393166273832325
Loss made of: CE 0.42617663741111755, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8701629638671875 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.577255201339722
Loss made of: CE 0.46526092290878296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.409505844116211 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.678160828351975
Loss made of: CE 0.3874662518501282, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8411319255828857 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.534663832187652
Loss made of: CE 0.31291908025741577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1874589920043945 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.602861753106117
Loss made of: CE 0.33608314394950867, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2172346115112305 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.521571280062199
Loss made of: CE 0.2392105907201767, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.020550727844238 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.509627345204353
Loss made of: CE 0.35569095611572266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.241476058959961 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.662599977850914
Loss made of: CE 0.5065208077430725, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.740509033203125 EntMin 0.0
Epoch 5, Class Loss=0.3499825894832611, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3499825894832611, Class Loss=0.3499825894832611, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/97, Loss=4.528262278437614
Loss made of: CE 0.3237423300743103, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.840456962585449 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.470295438170433
Loss made of: CE 0.3505121171474457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.470897674560547 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.321282401680946
Loss made of: CE 0.2740188539028168, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.487546682357788 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.301638734340668
Loss made of: CE 0.3785852789878845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.346701622009277 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.282707799971104
Loss made of: CE 0.23944614827632904, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.559174060821533 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.373370763659477
Loss made of: CE 0.31335437297821045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.009019374847412 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.346522429585457
Loss made of: CE 0.3151445686817169, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1061272621154785 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.412730979919433
Loss made of: CE 0.29267001152038574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.933023452758789 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.652223664522171
Loss made of: CE 0.36460375785827637, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.064641952514648 EntMin 0.0
Epoch 6, Class Loss=0.343549907207489, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.343549907207489, Class Loss=0.343549907207489, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.003577166795731
Loss made of: CE 0.9339754581451416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.423905372619629 EntMin 0.0
Epoch 1, Class Loss=0.7809672355651855, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.7809672355651855, Class Loss=0.7809672355651855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.61969085931778
Loss made of: CE 0.73920077085495, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0883612632751465 EntMin 0.0
Epoch 2, Class Loss=0.6976760625839233, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.6976760625839233, Class Loss=0.6976760625839233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.3240772664546965
Loss made of: CE 0.49724388122558594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.757883548736572 EntMin 0.0
Epoch 3, Class Loss=0.6103843450546265, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.6103843450546265, Class Loss=0.6103843450546265, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=5.381410855054855
Loss made of: CE 0.5004077553749084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.621722221374512 EntMin 0.0
Epoch 4, Class Loss=0.5487963557243347, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.5487963557243347, Class Loss=0.5487963557243347, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 10/12, Loss=5.174676191806793
Loss made of: CE 0.399301141500473, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.554167747497559 EntMin 0.0
Epoch 5, Class Loss=0.48433828353881836, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.48433828353881836, Class Loss=0.48433828353881836, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=5.067377814650536
Loss made of: CE 0.3133091926574707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8826003074646 EntMin 0.0
Epoch 6, Class Loss=0.5037996768951416, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.5037996768951416, Class Loss=0.5037996768951416, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.815017527341842
Loss made of: CE 0.7055296897888184, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.170598983764648 EntMin 0.0
Epoch 1, Class Loss=0.7595579624176025, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.7595579624176025, Class Loss=0.7595579624176025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.51145356297493
Loss made of: CE 0.5861653089523315, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.893925666809082 EntMin 0.0
Epoch 2, Class Loss=0.6729540824890137, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.6729540824890137, Class Loss=0.6729540824890137, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.322151222825051
Loss made of: CE 0.6374300718307495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.609996795654297 EntMin 0.0
Epoch 3, Class Loss=0.6028919219970703, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.6028919219970703, Class Loss=0.6028919219970703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=5.133424541354179
Loss made of: CE 0.5826466679573059, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.466017723083496 EntMin 0.0
Epoch 4, Class Loss=0.5493170022964478, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.5493170022964478, Class Loss=0.5493170022964478, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=5.127660578489303
Loss made of: CE 0.5221260786056519, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.339033603668213 EntMin 0.0
Epoch 5, Class Loss=0.5113719701766968, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5113719701766968, Class Loss=0.5113719701766968, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.9206219732761385
Loss made of: CE 0.4322218894958496, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.703614234924316 EntMin 0.0
Epoch 6, Class Loss=0.4904322624206543, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.4904322624206543, Class Loss=0.4904322624206543, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.025937503576278
Loss made of: CE 0.9906139373779297, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.610996246337891 EntMin 0.0
Epoch 1, Class Loss=0.8439576029777527, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.8439576029777527, Class Loss=0.8439576029777527, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/12, Loss=5.598621374368667
Loss made of: CE 0.5423738956451416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.512256622314453 EntMin 0.0
Epoch 2, Class Loss=0.7503389120101929, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.7503389120101929, Class Loss=0.7503389120101929, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/12, Loss=5.421211594343186
Loss made of: CE 0.6431713104248047, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9403886795043945 EntMin 0.0
Epoch 3, Class Loss=0.7004108428955078, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.7004108428955078, Class Loss=0.7004108428955078, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/12, Loss=5.411283618211746
Loss made of: CE 0.6926829218864441, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.803244590759277 EntMin 0.0
Epoch 4, Class Loss=0.6331169009208679, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.6331169009208679, Class Loss=0.6331169009208679, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/12, Loss=5.205591684579849
Loss made of: CE 0.730161190032959, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.911609649658203 EntMin 0.0
Epoch 5, Class Loss=0.6191822290420532, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.6191822290420532, Class Loss=0.6191822290420532, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/12, Loss=5.298240441083908
Loss made of: CE 0.5520002245903015, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.507707118988037 EntMin 0.0
Epoch 6, Class Loss=0.5893993973731995, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.5893993973731995, Class Loss=0.5893993973731995, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.5027323961257935, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.850175
Mean Acc: 0.495207
FreqW Acc: 0.746719
Mean IoU: 0.415047
Class IoU:
	class 0: 0.8630893
	class 1: 0.72170883
	class 2: 0.33561936
	class 3: 0.6283937
	class 4: 0.5570977
	class 5: 0.6955309
	class 6: 0.61323035
	class 7: 0.8178924
	class 8: 0.78883773
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.11832187
	class 13: 0.21373908
	class 14: 0.0
	class 15: 0.6988938
	class 16: 0.0034450209
Class Acc:
	class 0: 0.98494244
	class 1: 0.72719735
	class 2: 0.81320924
	class 3: 0.63227516
	class 4: 0.6003791
	class 5: 0.7500533
	class 6: 0.6157844
	class 7: 0.90063775
	class 8: 0.85415876
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.1189627
	class 13: 0.65904486
	class 14: 0.0
	class 15: 0.75842994
	class 16: 0.0034450209

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 25, step: 5
select part of clients to conduct local training
[24, 7, 4, 12]
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=7.763239419460296
Loss made of: CE 0.8965961933135986, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.578467845916748 EntMin 0.0
Epoch 1, Class Loss=1.040871262550354, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=1.040871262550354, Class Loss=1.040871262550354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=6.370748686790466
Loss made of: CE 0.7967743873596191, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.695186138153076 EntMin 0.0
Epoch 2, Class Loss=0.822600781917572, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.822600781917572, Class Loss=0.822600781917572, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.044086307287216
Loss made of: CE 0.70455402135849, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.52376651763916 EntMin 0.0
Epoch 3, Class Loss=0.735724925994873, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.735724925994873, Class Loss=0.735724925994873, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.816290563344955
Loss made of: CE 0.7677788734436035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.838504314422607 EntMin 0.0
Epoch 4, Class Loss=0.7000762224197388, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.7000762224197388, Class Loss=0.7000762224197388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.467268806695938
Loss made of: CE 0.6680926084518433, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.444025039672852 EntMin 0.0
Epoch 5, Class Loss=0.6637449264526367, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.6637449264526367, Class Loss=0.6637449264526367, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.256440716981888
Loss made of: CE 0.617162823677063, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.670900344848633 EntMin 0.0
Epoch 6, Class Loss=0.6437098979949951, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.6437098979949951, Class Loss=0.6437098979949951, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.3259776830673218, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=1.3259776830673218, Class Loss=1.3259776830673218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=1.0306873321533203, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=1.0306873321533203, Class Loss=1.0306873321533203, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.818670928478241, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.818670928478241, Class Loss=0.818670928478241, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.7310389876365662, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.7310389876365662, Class Loss=0.7310389876365662, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6323440670967102, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.6323440670967102, Class Loss=0.6323440670967102, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.5693909525871277, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.5693909525871277, Class Loss=0.5693909525871277, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=7.515520858764648
Loss made of: CE 0.9937289953231812, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.894639492034912 EntMin 0.0
Epoch 1, Class Loss=1.0574345588684082, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=1.0574345588684082, Class Loss=1.0574345588684082, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=6.418018013238907
Loss made of: CE 0.7690035700798035, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.752170085906982 EntMin 0.0
Epoch 2, Class Loss=0.8598591685295105, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.8598591685295105, Class Loss=0.8598591685295105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.944572561979294
Loss made of: CE 0.742231547832489, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.928045272827148 EntMin 0.0
Epoch 3, Class Loss=0.7475391626358032, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.7475391626358032, Class Loss=0.7475391626358032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.768001693487167
Loss made of: CE 0.7271384596824646, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.304263591766357 EntMin 0.0
Epoch 4, Class Loss=0.6946179866790771, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.6946179866790771, Class Loss=0.6946179866790771, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.425370389223099
Loss made of: CE 0.7512492537498474, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.337203502655029 EntMin 0.0
Epoch 5, Class Loss=0.6704296469688416, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.6704296469688416, Class Loss=0.6704296469688416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.261339724063873
Loss made of: CE 0.6031227111816406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.609857559204102 EntMin 0.0
Epoch 6, Class Loss=0.6381499171257019, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6381499171257019, Class Loss=0.6381499171257019, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=7.724125874042511
Loss made of: CE 1.0139081478118896, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.094249725341797 EntMin 0.0
Epoch 1, Class Loss=1.0386486053466797, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=1.0386486053466797, Class Loss=1.0386486053466797, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=6.4873533010482785
Loss made of: CE 0.8082704544067383, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4973554611206055 EntMin 0.0
Epoch 2, Class Loss=0.828201413154602, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.828201413154602, Class Loss=0.828201413154602, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.912395411729813
Loss made of: CE 0.7865896224975586, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.179862022399902 EntMin 0.0
Epoch 3, Class Loss=0.7411522269248962, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.7411522269248962, Class Loss=0.7411522269248962, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.654643005132675
Loss made of: CE 0.6919199228286743, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.485071182250977 EntMin 0.0
Epoch 4, Class Loss=0.7029421329498291, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.7029421329498291, Class Loss=0.7029421329498291, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.316086024045944
Loss made of: CE 0.6058369874954224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481985092163086 EntMin 0.0
Epoch 5, Class Loss=0.6729406118392944, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.6729406118392944, Class Loss=0.6729406118392944, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.297849088907242
Loss made of: CE 0.6506513953208923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.46865701675415 EntMin 0.0
Epoch 6, Class Loss=0.6398451328277588, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.6398451328277588, Class Loss=0.6398451328277588, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7144891619682312, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.820524
Mean Acc: 0.398847
FreqW Acc: 0.696525
Mean IoU: 0.330399
Class IoU:
	class 0: 0.83000016
	class 1: 0.575251
	class 2: 0.3244608
	class 3: 0.46697247
	class 4: 0.51027143
	class 5: 0.5888246
	class 6: 0.37990105
	class 7: 0.80506164
	class 8: 0.7624823
	class 9: 6.429049e-07
	class 10: 0.0
	class 11: 0.0
	class 12: 0.111644045
	class 13: 0.19376875
	class 14: 0.0
	class 15: 0.6997999
	class 16: 0.02914602
	class 17: 0.0
	class 18: 0.0
Class Acc:
	class 0: 0.9866494
	class 1: 0.5783316
	class 2: 0.76501954
	class 3: 0.46827367
	class 4: 0.5519349
	class 5: 0.611572
	class 6: 0.3812115
	class 7: 0.89519376
	class 8: 0.8153271
	class 9: 6.429049e-07
	class 10: 0.0
	class 11: 0.0
	class 12: 0.11214975
	class 13: 0.63273716
	class 14: 0.0
	class 15: 0.75055104
	class 16: 0.029148405
	class 17: 0.0
	class 18: 0.0

federated global round: 26, step: 5
select part of clients to conduct local training
[6, 28, 20, 4]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.498811715841294
Loss made of: CE 0.7572674751281738, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6162519454956055 EntMin 0.0
Epoch 1, Class Loss=0.7217981219291687, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.7217981219291687, Class Loss=0.7217981219291687, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.2682387888431546
Loss made of: CE 0.6528148651123047, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.159348011016846 EntMin 0.0
Epoch 2, Class Loss=0.6800219416618347, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6800219416618347, Class Loss=0.6800219416618347, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.1548209369182585
Loss made of: CE 0.641130805015564, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.48931884765625 EntMin 0.0
Epoch 3, Class Loss=0.6607481241226196, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.6607481241226196, Class Loss=0.6607481241226196, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=4.904688900709152
Loss made of: CE 0.63838791847229, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.489526748657227 EntMin 0.0
Epoch 4, Class Loss=0.6221998929977417, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.6221998929977417, Class Loss=0.6221998929977417, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.133712339401245
Loss made of: CE 0.6806154251098633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.876461029052734 EntMin 0.0
Epoch 5, Class Loss=0.6179068088531494, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.6179068088531494, Class Loss=0.6179068088531494, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=4.789795476198196
Loss made of: CE 0.5499428510665894, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.903878927230835 EntMin 0.0
Epoch 6, Class Loss=0.5981840491294861, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5981840491294861, Class Loss=0.5981840491294861, Reg Loss=0.0
Current Client Index:  28
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.2893855571746826, Reg Loss=0.0
Clinet index 28, End of Epoch 1/6, Average Loss=1.2893855571746826, Class Loss=1.2893855571746826, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=1.110807180404663, Reg Loss=0.0
Clinet index 28, End of Epoch 2/6, Average Loss=1.110807180404663, Class Loss=1.110807180404663, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.944193959236145, Reg Loss=0.0
Clinet index 28, End of Epoch 3/6, Average Loss=0.944193959236145, Class Loss=0.944193959236145, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.7595131993293762, Reg Loss=0.0
Clinet index 28, End of Epoch 4/6, Average Loss=0.7595131993293762, Class Loss=0.7595131993293762, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.6444971561431885, Reg Loss=0.0
Clinet index 28, End of Epoch 5/6, Average Loss=0.6444971561431885, Class Loss=0.6444971561431885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.5613362193107605, Reg Loss=0.0
Clinet index 28, End of Epoch 6/6, Average Loss=0.5613362193107605, Class Loss=0.5613362193107605, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.2316797971725464, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=1.2316797971725464, Class Loss=1.2316797971725464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=1.1079771518707275, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=1.1079771518707275, Class Loss=1.1079771518707275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.8946459293365479, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.8946459293365479, Class Loss=0.8946459293365479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.7653846144676208, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.7653846144676208, Class Loss=0.7653846144676208, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.6551863551139832, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.6551863551139832, Class Loss=0.6551863551139832, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.57259601354599, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.57259601354599, Class Loss=0.57259601354599, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.389833104610443
Loss made of: CE 0.6699132323265076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.209992408752441 EntMin 0.0
Epoch 1, Class Loss=0.7111498713493347, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.7111498713493347, Class Loss=0.7111498713493347, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=5.190543442964554
Loss made of: CE 0.6649429202079773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698096752166748 EntMin 0.0
Epoch 2, Class Loss=0.6829665899276733, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.6829665899276733, Class Loss=0.6829665899276733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=5.083830773830414
Loss made of: CE 0.5823997855186462, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.217229843139648 EntMin 0.0
Epoch 3, Class Loss=0.6541551351547241, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.6541551351547241, Class Loss=0.6541551351547241, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=5.143392241001129
Loss made of: CE 0.6564576625823975, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.001683235168457 EntMin 0.0
Epoch 4, Class Loss=0.622869074344635, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.622869074344635, Class Loss=0.622869074344635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.960105735063553
Loss made of: CE 0.6869252920150757, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.847206115722656 EntMin 0.0
Epoch 5, Class Loss=0.6173406839370728, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.6173406839370728, Class Loss=0.6173406839370728, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=4.717603409290314
Loss made of: CE 0.5691460371017456, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53889274597168 EntMin 0.0
Epoch 6, Class Loss=0.5850579142570496, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5850579142570496, Class Loss=0.5850579142570496, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6797789335250854, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.811267
Mean Acc: 0.365714
FreqW Acc: 0.679571
Mean IoU: 0.302765
Class IoU:
	class 0: 0.81765604
	class 1: 0.5096503
	class 2: 0.31744274
	class 3: 0.31294835
	class 4: 0.48092094
	class 5: 0.48700014
	class 6: 0.32624847
	class 7: 0.7999127
	class 8: 0.73867404
	class 9: 1.4465319e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.07808612
	class 13: 0.19315726
	class 14: 0.0
	class 15: 0.6762465
	class 16: 0.014584538
	class 17: 0.0
	class 18: 0.0
Class Acc:
	class 0: 0.9890102
	class 1: 0.5117533
	class 2: 0.7359807
	class 3: 0.31323493
	class 4: 0.5143728
	class 5: 0.49685404
	class 6: 0.32709152
	class 7: 0.8811619
	class 8: 0.7803272
	class 9: 1.446536e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.078394555
	class 13: 0.59646153
	class 14: 0.0
	class 15: 0.709328
	class 16: 0.014584784
	class 17: 0.0
	class 18: 0.0

federated global round: 27, step: 5
select part of clients to conduct local training
[24, 8, 26, 0]
Current Client Index:  24
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.039483094215393
Loss made of: CE 0.7115305662155151, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.322296142578125 EntMin 0.0
Epoch 1, Class Loss=0.7330145835876465, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.7330145835876465, Class Loss=0.7330145835876465, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/12, Loss=5.238861364126206
Loss made of: CE 0.6902927160263062, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.666851997375488 EntMin 0.0
Epoch 2, Class Loss=0.6645005345344543, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.6645005345344543, Class Loss=0.6645005345344543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/12, Loss=5.130401301383972
Loss made of: CE 0.620500922203064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.247990608215332 EntMin 0.0
Epoch 3, Class Loss=0.6224460601806641, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.6224460601806641, Class Loss=0.6224460601806641, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/12, Loss=5.110852062702179
Loss made of: CE 0.6413951516151428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.265993595123291 EntMin 0.0
Epoch 4, Class Loss=0.6158877611160278, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.6158877611160278, Class Loss=0.6158877611160278, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/12, Loss=4.8591396629810335
Loss made of: CE 0.6224535703659058, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.100257396697998 EntMin 0.0
Epoch 5, Class Loss=0.5917417407035828, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.5917417407035828, Class Loss=0.5917417407035828, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/12, Loss=4.79076189994812
Loss made of: CE 0.5418805480003357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1266093254089355 EntMin 0.0
Epoch 6, Class Loss=0.5835318565368652, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.5835318565368652, Class Loss=0.5835318565368652, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.8838604092597961, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.8838604092597961, Class Loss=0.8838604092597961, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.8239910006523132, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.8239910006523132, Class Loss=0.8239910006523132, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.6981822848320007, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.6981822848320007, Class Loss=0.6981822848320007, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.5923771262168884, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5923771262168884, Class Loss=0.5923771262168884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5338298678398132, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.5338298678398132, Class Loss=0.5338298678398132, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.4674663841724396, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.4674663841724396, Class Loss=0.4674663841724396, Reg Loss=0.0
Current Client Index:  26
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.900384676456452
Loss made of: CE 0.6614428162574768, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.487736701965332 EntMin 0.0
Epoch 1, Class Loss=0.7210690975189209, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.7210690975189209, Class Loss=0.7210690975189209, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=5.2857187449932095
Loss made of: CE 0.7269247770309448, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.235659122467041 EntMin 0.0
Epoch 2, Class Loss=0.6635253429412842, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.6635253429412842, Class Loss=0.6635253429412842, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=5.218167793750763
Loss made of: CE 0.6191405653953552, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.736016273498535 EntMin 0.0
Epoch 3, Class Loss=0.6431448459625244, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.6431448459625244, Class Loss=0.6431448459625244, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=5.08455211520195
Loss made of: CE 0.5457491278648376, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.582911968231201 EntMin 0.0
Epoch 4, Class Loss=0.6063432097434998, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.6063432097434998, Class Loss=0.6063432097434998, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.72643626332283
Loss made of: CE 0.556920051574707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.846489906311035 EntMin 0.0
Epoch 5, Class Loss=0.5718024373054504, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.5718024373054504, Class Loss=0.5718024373054504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.824282509088516
Loss made of: CE 0.5538865327835083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18611478805542 EntMin 0.0
Epoch 6, Class Loss=0.5667694807052612, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.5667694807052612, Class Loss=0.5667694807052612, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.709557372331619
Loss made of: CE 0.682741105556488, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.392929553985596 EntMin 0.0
Epoch 1, Class Loss=0.7375983595848083, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.7375983595848083, Class Loss=0.7375983595848083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=5.09800534248352
Loss made of: CE 0.5653092861175537, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128658294677734 EntMin 0.0
Epoch 2, Class Loss=0.6584944128990173, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.6584944128990173, Class Loss=0.6584944128990173, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=5.0896123349666595
Loss made of: CE 0.65218186378479, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.370149612426758 EntMin 0.0
Epoch 3, Class Loss=0.637620210647583, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.637620210647583, Class Loss=0.637620210647583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=4.9186082005500795
Loss made of: CE 0.5728613138198853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.542966365814209 EntMin 0.0
Epoch 4, Class Loss=0.6041920185089111, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.6041920185089111, Class Loss=0.6041920185089111, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.849670380353928
Loss made of: CE 0.5981658697128296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.468455791473389 EntMin 0.0
Epoch 5, Class Loss=0.5860708951950073, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.5860708951950073, Class Loss=0.5860708951950073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.723777437210083
Loss made of: CE 0.5545026659965515, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7771124839782715 EntMin 0.0
Epoch 6, Class Loss=0.5633317828178406, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.5633317828178406, Class Loss=0.5633317828178406, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6619905829429626, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.820298
Mean Acc: 0.401644
FreqW Acc: 0.699136
Mean IoU: 0.327905
Class IoU:
	class 0: 0.8353015
	class 1: 0.5882246
	class 2: 0.3163524
	class 3: 0.388427
	class 4: 0.48475143
	class 5: 0.56242186
	class 6: 0.33292434
	class 7: 0.7948526
	class 8: 0.79154944
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.11858996
	class 13: 0.19611509
	class 14: 0.0
	class 15: 0.6747426
	class 16: 0.012604309
	class 17: 0.0
	class 18: 0.13333082
Class Acc:
	class 0: 0.9864994
	class 1: 0.591528
	class 2: 0.78008366
	class 3: 0.38921466
	class 4: 0.51855373
	class 5: 0.5786367
	class 6: 0.33382753
	class 7: 0.89726925
	class 8: 0.88506186
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.11914632
	class 13: 0.680342
	class 14: 0.0
	class 15: 0.70340514
	class 16: 0.012604309
	class 17: 0.0
	class 18: 0.1550617

federated global round: 28, step: 5
select part of clients to conduct local training
[8, 9, 16, 3]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.0363237857818604, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.0363237857818604, Class Loss=1.0363237857818604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.9664757251739502, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.9664757251739502, Class Loss=0.9664757251739502, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.8432782292366028, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.8432782292366028, Class Loss=0.8432782292366028, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.7224753499031067, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.7224753499031067, Class Loss=0.7224753499031067, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.6471672058105469, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6471672058105469, Class Loss=0.6471672058105469, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.5828936100006104, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.5828936100006104, Class Loss=0.5828936100006104, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.0875154733657837, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=1.0875154733657837, Class Loss=1.0875154733657837, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.9389088749885559, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.9389088749885559, Class Loss=0.9389088749885559, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.7461387515068054, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.7461387515068054, Class Loss=0.7461387515068054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.633615255355835, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.633615255355835, Class Loss=0.633615255355835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Class Loss=0.5115459561347961, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5115459561347961, Class Loss=0.5115459561347961, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.4941690266132355, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.4941690266132355, Class Loss=0.4941690266132355, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.025633949041366
Loss made of: CE 0.5543229579925537, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.654572486877441 EntMin 0.0
Epoch 1, Class Loss=0.6295875310897827, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.6295875310897827, Class Loss=0.6295875310897827, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=4.72820811867714
Loss made of: CE 0.5909829139709473, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.512201309204102 EntMin 0.0
Epoch 2, Class Loss=0.6024526953697205, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.6024526953697205, Class Loss=0.6024526953697205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.621938866376877
Loss made of: CE 0.5935149788856506, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.864928722381592 EntMin 0.0
Epoch 3, Class Loss=0.5685521364212036, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.5685521364212036, Class Loss=0.5685521364212036, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.549955523014068
Loss made of: CE 0.5605576038360596, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9882383346557617 EntMin 0.0
Epoch 4, Class Loss=0.5732036232948303, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.5732036232948303, Class Loss=0.5732036232948303, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.549795380234718
Loss made of: CE 0.4994862675666809, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.697166919708252 EntMin 0.0
Epoch 5, Class Loss=0.5465852618217468, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.5465852618217468, Class Loss=0.5465852618217468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.596874588727951
Loss made of: CE 0.5279058218002319, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1517653465271 EntMin 0.0
Epoch 6, Class Loss=0.5606354475021362, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.5606354475021362, Class Loss=0.5606354475021362, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.963589668273926
Loss made of: CE 0.5391838550567627, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.228393077850342 EntMin 0.0
Epoch 1, Class Loss=0.6191054582595825, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.6191054582595825, Class Loss=0.6191054582595825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=4.9582969665527346
Loss made of: CE 0.682285487651825, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.666943550109863 EntMin 0.0
Epoch 2, Class Loss=0.6133420467376709, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.6133420467376709, Class Loss=0.6133420467376709, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.881820464134217
Loss made of: CE 0.6371433138847351, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.696597099304199 EntMin 0.0
Epoch 3, Class Loss=0.5789406299591064, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.5789406299591064, Class Loss=0.5789406299591064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.816777896881104
Loss made of: CE 0.5534764528274536, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9392812252044678 EntMin 0.0
Epoch 4, Class Loss=0.568223237991333, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.568223237991333, Class Loss=0.568223237991333, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.766299217939377
Loss made of: CE 0.5359088182449341, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.436303615570068 EntMin 0.0
Epoch 5, Class Loss=0.5453851222991943, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.5453851222991943, Class Loss=0.5453851222991943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.660325872898102
Loss made of: CE 0.4847729802131653, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8354263305664062 EntMin 0.0
Epoch 6, Class Loss=0.5477029085159302, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.5477029085159302, Class Loss=0.5477029085159302, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6402449607849121, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.815130
Mean Acc: 0.378374
FreqW Acc: 0.687719
Mean IoU: 0.310777
Class IoU:
	class 0: 0.8260127
	class 1: 0.5361452
	class 2: 0.31424633
	class 3: 0.36262885
	class 4: 0.42989296
	class 5: 0.53518766
	class 6: 0.28995547
	class 7: 0.78460354
	class 8: 0.78390384
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.122455
	class 13: 0.19597404
	class 14: 0.0
	class 15: 0.6614809
	class 16: 0.004363066
	class 17: 0.0
	class 18: 0.05791728
Class Acc:
	class 0: 0.9886727
	class 1: 0.5385875
	class 2: 0.7405964
	class 3: 0.36334246
	class 4: 0.44772252
	class 5: 0.55117255
	class 6: 0.2905692
	class 7: 0.8939276
	class 8: 0.8612682
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.12310198
	class 13: 0.63543946
	class 14: 0.0
	class 15: 0.6880636
	class 16: 0.004363066
	class 17: 0.0
	class 18: 0.06228698

federated global round: 29, step: 5
select part of clients to conduct local training
[29, 27, 26, 18]
Current Client Index:  29
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.3918169975280765
Loss made of: CE 0.6779431700706482, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.076486587524414 EntMin 0.0
Epoch 1, Class Loss=0.665918231010437, Reg Loss=0.0
Clinet index 29, End of Epoch 1/6, Average Loss=0.665918231010437, Class Loss=0.665918231010437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.13129575252533
Loss made of: CE 0.5013324618339539, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9183034896850586 EntMin 0.0
Epoch 2, Class Loss=0.6129003167152405, Reg Loss=0.0
Clinet index 29, End of Epoch 2/6, Average Loss=0.6129003167152405, Class Loss=0.6129003167152405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.754369097948074
Loss made of: CE 0.5702922344207764, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.159847259521484 EntMin 0.0
Epoch 3, Class Loss=0.5897926688194275, Reg Loss=0.0
Clinet index 29, End of Epoch 3/6, Average Loss=0.5897926688194275, Class Loss=0.5897926688194275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.911215233802795
Loss made of: CE 0.6005657911300659, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.144357681274414 EntMin 0.0
Epoch 4, Class Loss=0.5842452049255371, Reg Loss=0.0
Clinet index 29, End of Epoch 4/6, Average Loss=0.5842452049255371, Class Loss=0.5842452049255371, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.733576864004135
Loss made of: CE 0.5157214999198914, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9974284172058105 EntMin 0.0
Epoch 5, Class Loss=0.5685466527938843, Reg Loss=0.0
Clinet index 29, End of Epoch 5/6, Average Loss=0.5685466527938843, Class Loss=0.5685466527938843, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.607923674583435
Loss made of: CE 0.5919365286827087, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6491057872772217 EntMin 0.0
Epoch 6, Class Loss=0.5623868107795715, Reg Loss=0.0
Clinet index 29, End of Epoch 6/6, Average Loss=0.5623868107795715, Class Loss=0.5623868107795715, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.8814235925674438, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.8814235925674438, Class Loss=0.8814235925674438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.7749003767967224, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.7749003767967224, Class Loss=0.7749003767967224, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.6058518886566162, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.6058518886566162, Class Loss=0.6058518886566162, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5475775003433228, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.5475775003433228, Class Loss=0.5475775003433228, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.5084547400474548, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.5084547400474548, Class Loss=0.5084547400474548, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.4777821898460388, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.4777821898460388, Class Loss=0.4777821898460388, Reg Loss=0.0
Current Client Index:  26
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.470828056335449
Loss made of: CE 0.5833730697631836, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.25786018371582 EntMin 0.0
Epoch 1, Class Loss=0.658383846282959, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.658383846282959, Class Loss=0.658383846282959, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/12, Loss=4.871026211977005
Loss made of: CE 0.7142278552055359, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.078004360198975 EntMin 0.0
Epoch 2, Class Loss=0.6230461001396179, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.6230461001396179, Class Loss=0.6230461001396179, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/12, Loss=4.863327294588089
Loss made of: CE 0.5748848915100098, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.731015205383301 EntMin 0.0
Epoch 3, Class Loss=0.6032640933990479, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.6032640933990479, Class Loss=0.6032640933990479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/12, Loss=4.672778308391571
Loss made of: CE 0.5010978579521179, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.054818153381348 EntMin 0.0
Epoch 4, Class Loss=0.5717980861663818, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.5717980861663818, Class Loss=0.5717980861663818, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/12, Loss=4.432485401630402
Loss made of: CE 0.5511994957923889, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.258430480957031 EntMin 0.0
Epoch 5, Class Loss=0.569860577583313, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.569860577583313, Class Loss=0.569860577583313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/12, Loss=4.549938386678695
Loss made of: CE 0.5354440212249756, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.322808265686035 EntMin 0.0
Epoch 6, Class Loss=0.5584436655044556, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.5584436655044556, Class Loss=0.5584436655044556, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.538179087638855
Loss made of: CE 0.5716598033905029, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.762402534484863 EntMin 0.0
Epoch 1, Class Loss=0.6713770627975464, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.6713770627975464, Class Loss=0.6713770627975464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=4.983708614110947
Loss made of: CE 0.5971630215644836, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.402682304382324 EntMin 0.0
Epoch 2, Class Loss=0.6130287051200867, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.6130287051200867, Class Loss=0.6130287051200867, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.818219363689423
Loss made of: CE 0.5315355062484741, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2487406730651855 EntMin 0.0
Epoch 3, Class Loss=0.5801882743835449, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.5801882743835449, Class Loss=0.5801882743835449, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.751091539859772
Loss made of: CE 0.5715817213058472, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.001593589782715 EntMin 0.0
Epoch 4, Class Loss=0.5715131759643555, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.5715131759643555, Class Loss=0.5715131759643555, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.675098240375519
Loss made of: CE 0.5276283025741577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.903488636016846 EntMin 0.0
Epoch 5, Class Loss=0.5537080764770508, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.5537080764770508, Class Loss=0.5537080764770508, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.5795293033123015
Loss made of: CE 0.5079059600830078, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.228394031524658 EntMin 0.0
Epoch 6, Class Loss=0.5427393913269043, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.5427393913269043, Class Loss=0.5427393913269043, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6518296599388123, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.820601
Mean Acc: 0.403971
FreqW Acc: 0.703537
Mean IoU: 0.329077
Class IoU:
	class 0: 0.84052837
	class 1: 0.57191914
	class 2: 0.31820115
	class 3: 0.4118919
	class 4: 0.44140068
	class 5: 0.58288914
	class 6: 0.28060424
	class 7: 0.79725695
	class 8: 0.8002292
	class 9: 5.7861384e-06
	class 10: 0.0
	class 11: 0.0
	class 12: 0.15014745
	class 13: 0.19574302
	class 14: 0.0
	class 15: 0.6772976
	class 16: 0.010018074
	class 17: 0.0
	class 18: 0.17433393
Class Acc:
	class 0: 0.98444945
	class 1: 0.57475096
	class 2: 0.7452802
	class 3: 0.41280168
	class 4: 0.4637822
	class 5: 0.60384
	class 6: 0.28129897
	class 7: 0.8869755
	class 8: 0.8918668
	class 9: 5.786144e-06
	class 10: 0.0
	class 11: 0.0
	class 12: 0.15123583
	class 13: 0.6809353
	class 14: 0.0
	class 15: 0.70657045
	class 16: 0.010018074
	class 17: 0.0
	class 18: 0.2816343

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 30, step: 6
select part of clients to conduct local training
[10, 30, 4, 33]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=14.455552423000336
Loss made of: CE 1.3574175834655762, LKD 0.0, LDE 0.0, LReg 0.0, POD 12.262165069580078 EntMin 0.0
Epoch 1, Class Loss=1.4772623777389526, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=1.4772623777389526, Class Loss=1.4772623777389526, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=12.16494635939598
Loss made of: CE 0.9750967025756836, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.461732864379883 EntMin 0.0
Epoch 2, Class Loss=1.0011792182922363, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=1.0011792182922363, Class Loss=1.0011792182922363, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=11.263290840387345
Loss made of: CE 0.6903605461120605, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.641166687011719 EntMin 0.0
Epoch 3, Class Loss=0.7260035276412964, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.7260035276412964, Class Loss=0.7260035276412964, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.562048149108886
Loss made of: CE 0.5302706956863403, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.05683422088623 EntMin 0.0
Epoch 4, Class Loss=0.57829749584198, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.57829749584198, Class Loss=0.57829749584198, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.208200600743293
Loss made of: CE 0.4637734293937683, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.306541442871094 EntMin 0.0
Epoch 5, Class Loss=0.49578118324279785, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.49578118324279785, Class Loss=0.49578118324279785, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.608721020817757
Loss made of: CE 0.40601596236228943, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.122538566589355 EntMin 0.0
Epoch 6, Class Loss=0.45414602756500244, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.45414602756500244, Class Loss=0.45414602756500244, Reg Loss=0.0
Current Client Index:  30
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=10.317284035682679
Loss made of: CE 1.1928693056106567, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.203987121582031 EntMin 0.0
Epoch 1, Class Loss=1.2533618211746216, Reg Loss=0.0
Clinet index 30, End of Epoch 1/6, Average Loss=1.2533618211746216, Class Loss=1.2533618211746216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=8.67284927368164
Loss made of: CE 0.7434980273246765, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.30567741394043 EntMin 0.0
Epoch 2, Class Loss=0.8314085602760315, Reg Loss=0.0
Clinet index 30, End of Epoch 2/6, Average Loss=0.8314085602760315, Class Loss=0.8314085602760315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=7.363985764980316
Loss made of: CE 0.7113238573074341, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.032127380371094 EntMin 0.0
Epoch 3, Class Loss=0.7107448577880859, Reg Loss=0.0
Clinet index 30, End of Epoch 3/6, Average Loss=0.7107448577880859, Class Loss=0.7107448577880859, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=7.062332981824875
Loss made of: CE 0.7641989588737488, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.547639846801758 EntMin 0.0
Epoch 4, Class Loss=0.6766588091850281, Reg Loss=0.0
Clinet index 30, End of Epoch 4/6, Average Loss=0.6766588091850281, Class Loss=0.6766588091850281, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=6.887964129447937
Loss made of: CE 0.7680261135101318, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.889927864074707 EntMin 0.0
Epoch 5, Class Loss=0.6352099776268005, Reg Loss=0.0
Clinet index 30, End of Epoch 5/6, Average Loss=0.6352099776268005, Class Loss=0.6352099776268005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=6.573729574680328
Loss made of: CE 0.7032825350761414, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.146374702453613 EntMin 0.0
Epoch 6, Class Loss=0.6191135048866272, Reg Loss=0.0
Clinet index 30, End of Epoch 6/6, Average Loss=0.6191135048866272, Class Loss=0.6191135048866272, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=14.124699056148529
Loss made of: CE 1.2774025201797485, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.201478004455566 EntMin 0.0
Epoch 1, Class Loss=1.437831163406372, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=1.437831163406372, Class Loss=1.437831163406372, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=11.693682497739792
Loss made of: CE 0.868916392326355, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.724085807800293 EntMin 0.0
Epoch 2, Class Loss=0.9643799662590027, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.9643799662590027, Class Loss=0.9643799662590027, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/12, Loss=11.097742068767548
Loss made of: CE 0.7562927007675171, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.449941635131836 EntMin 0.0
Epoch 3, Class Loss=0.7162873148918152, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.7162873148918152, Class Loss=0.7162873148918152, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.295104736089707
Loss made of: CE 0.5635112524032593, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.166171073913574 EntMin 0.0
Epoch 4, Class Loss=0.5895607471466064, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5895607471466064, Class Loss=0.5895607471466064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=9.919246956706047
Loss made of: CE 0.45450758934020996, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.69900131225586 EntMin 0.0
Epoch 5, Class Loss=0.4914020001888275, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.4914020001888275, Class Loss=0.4914020001888275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.198389384150506
Loss made of: CE 0.4220138490200043, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.650634765625 EntMin 0.0
Epoch 6, Class Loss=0.4571945071220398, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.4571945071220398, Class Loss=0.4571945071220398, Reg Loss=0.0
Current Client Index:  33
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=14.324589705467224
Loss made of: CE 1.2571839094161987, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.426352500915527 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=1.454222559928894, Reg Loss=0.0
Clinet index 33, End of Epoch 1/6, Average Loss=1.454222559928894, Class Loss=1.454222559928894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=12.381997340917588
Loss made of: CE 0.8681238889694214, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.698323249816895 EntMin 0.0
Epoch 2, Class Loss=0.9867331981658936, Reg Loss=0.0
Clinet index 33, End of Epoch 2/6, Average Loss=0.9867331981658936, Class Loss=0.9867331981658936, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 3, Batch 10/12, Loss=11.502631467580795
Loss made of: CE 0.6907447576522827, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.777841567993164 EntMin 0.0
Epoch 3, Class Loss=0.7259297370910645, Reg Loss=0.0
Clinet index 33, End of Epoch 3/6, Average Loss=0.7259297370910645, Class Loss=0.7259297370910645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.748290193080901
Loss made of: CE 0.5145947337150574, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.445908546447754 EntMin 0.0
Epoch 4, Class Loss=0.5873731374740601, Reg Loss=0.0
Clinet index 33, End of Epoch 4/6, Average Loss=0.5873731374740601, Class Loss=0.5873731374740601, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.41241075694561
Loss made of: CE 0.5238323211669922, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.397267818450928 EntMin 0.0
Epoch 5, Class Loss=0.5081115365028381, Reg Loss=0.0
Clinet index 33, End of Epoch 5/6, Average Loss=0.5081115365028381, Class Loss=0.5081115365028381, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.682919397950172
Loss made of: CE 0.432654470205307, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.342997550964355 EntMin 0.0
Epoch 6, Class Loss=0.4495062530040741, Reg Loss=0.0
Clinet index 33, End of Epoch 6/6, Average Loss=0.4495062530040741, Class Loss=0.4495062530040741, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7706246972084045, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.786266
Mean Acc: 0.291893
FreqW Acc: 0.637156
Mean IoU: 0.242667
Class IoU:
	class 0: 0.7892503
	class 1: 0.42763948
	class 2: 0.2931684
	class 3: 0.10704476
	class 4: 0.23856674
	class 5: 0.44355044
	class 6: 0.43623564
	class 7: 0.7728936
	class 8: 0.5381582
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.028428035
	class 13: 0.19522782
	class 14: 0.0
	class 15: 0.61781555
	class 16: 0.0
	class 17: 0.0
	class 18: 1.6364558e-05
	class 19: 0.20800851
	class 20: 0.0
Class Acc:
	class 0: 0.99238425
	class 1: 0.4286209
	class 2: 0.6196784
	class 3: 0.10704924
	class 4: 0.2409122
	class 5: 0.45038664
	class 6: 0.44001827
	class 7: 0.84112483
	class 8: 0.5515623
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.028441308
	class 13: 0.5335878
	class 14: 0.0
	class 15: 0.63531506
	class 16: 0.0
	class 17: 0.0
	class 18: 1.6365273e-05
	class 19: 0.26065013
	class 20: 0.0

federated global round: 31, step: 6
select part of clients to conduct local training
[9, 17, 14, 1]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=9.306490063667297
Loss made of: CE 1.0670212507247925, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.266135215759277 EntMin 0.0
Epoch 1, Class Loss=1.022412896156311, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=1.022412896156311, Class Loss=1.022412896156311, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=7.860233622789383
Loss made of: CE 0.6249943375587463, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.094264984130859 EntMin 0.0
Epoch 2, Class Loss=0.8387913107872009, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.8387913107872009, Class Loss=0.8387913107872009, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=6.9306934684515
Loss made of: CE 0.6929778456687927, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.494327545166016 EntMin 0.0
Epoch 3, Class Loss=0.7167065143585205, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.7167065143585205, Class Loss=0.7167065143585205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.5131310522556305
Loss made of: CE 0.6157181859016418, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.05823278427124 EntMin 0.0
Epoch 4, Class Loss=0.6679337024688721, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.6679337024688721, Class Loss=0.6679337024688721, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=6.186847817897797
Loss made of: CE 0.5388315320014954, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.462159633636475 EntMin 0.0
Epoch 5, Class Loss=0.5948053598403931, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5948053598403931, Class Loss=0.5948053598403931, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=6.118675720691681
Loss made of: CE 0.4607527256011963, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.329530715942383 EntMin 0.0
Epoch 6, Class Loss=0.5812938213348389, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.5812938213348389, Class Loss=0.5812938213348389, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=9.778633922338486
Loss made of: CE 0.47923827171325684, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.61923599243164 EntMin 0.0
Epoch 1, Class Loss=0.575168251991272, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.575168251991272, Class Loss=0.575168251991272, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=9.711031356453896
Loss made of: CE 0.43902426958084106, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.504291534423828 EntMin 0.0
Epoch 2, Class Loss=0.474099338054657, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.474099338054657, Class Loss=0.474099338054657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/12, Loss=8.951586109399795
Loss made of: CE 0.42548155784606934, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.363628387451172 EntMin 0.0
Epoch 3, Class Loss=0.4187200963497162, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.4187200963497162, Class Loss=0.4187200963497162, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.685410723090172
Loss made of: CE 0.4235425889492035, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.215434551239014 EntMin 0.0
Epoch 4, Class Loss=0.4031335115432739, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.4031335115432739, Class Loss=0.4031335115432739, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.130322754383087
Loss made of: CE 0.41062530875205994, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.9498701095581055 EntMin 0.0
Epoch 5, Class Loss=0.3794560134410858, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.3794560134410858, Class Loss=0.3794560134410858, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.789735823869705
Loss made of: CE 0.3376201093196869, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.305192947387695 EntMin 0.0
Epoch 6, Class Loss=0.3687983453273773, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.3687983453273773, Class Loss=0.3687983453273773, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=9.409548330307008
Loss made of: CE 1.0635133981704712, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.143009185791016 EntMin 0.0
Epoch 1, Class Loss=0.9969099760055542, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.9969099760055542, Class Loss=0.9969099760055542, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=7.759045648574829
Loss made of: CE 0.6682096719741821, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.888514518737793 EntMin 0.0
Epoch 2, Class Loss=0.8403699994087219, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.8403699994087219, Class Loss=0.8403699994087219, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=7.056116074323654
Loss made of: CE 0.6467222571372986, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.068143844604492 EntMin 0.0
Epoch 3, Class Loss=0.7461404204368591, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.7461404204368591, Class Loss=0.7461404204368591, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.64324209690094
Loss made of: CE 0.5426964163780212, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.317025184631348 EntMin 0.0
Epoch 4, Class Loss=0.6712676286697388, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.6712676286697388, Class Loss=0.6712676286697388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=6.162455463409424
Loss made of: CE 0.5556011199951172, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.162692070007324 EntMin 0.0
Epoch 5, Class Loss=0.6100788116455078, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.6100788116455078, Class Loss=0.6100788116455078, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=6.141940593719482
Loss made of: CE 0.5851569771766663, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.013623237609863 EntMin 0.0
Epoch 6, Class Loss=0.5561427474021912, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5561427474021912, Class Loss=0.5561427474021912, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=9.90069957971573
Loss made of: CE 0.5385790467262268, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.067449569702148 EntMin 0.0
Epoch 1, Class Loss=0.5851885080337524, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.5851885080337524, Class Loss=0.5851885080337524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=9.20466915667057
Loss made of: CE 0.5211004614830017, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.28653621673584 EntMin 0.0
Epoch 2, Class Loss=0.4975622296333313, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.4975622296333313, Class Loss=0.4975622296333313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=8.747700282931328
Loss made of: CE 0.44722509384155273, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.786979675292969 EntMin 0.0
Epoch 3, Class Loss=0.44711726903915405, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.44711726903915405, Class Loss=0.44711726903915405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.643193173408509
Loss made of: CE 0.34184178709983826, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.317653656005859 EntMin 0.0
Epoch 4, Class Loss=0.40348029136657715, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.40348029136657715, Class Loss=0.40348029136657715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.394460329413414
Loss made of: CE 0.4143269956111908, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.411740779876709 EntMin 0.0
Epoch 5, Class Loss=0.37915050983428955, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.37915050983428955, Class Loss=0.37915050983428955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.968209388852119
Loss made of: CE 0.4916834831237793, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.661664962768555 EntMin 0.0
Epoch 6, Class Loss=0.368632972240448, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.368632972240448, Class Loss=0.368632972240448, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7078729867935181, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.789939
Mean Acc: 0.299232
FreqW Acc: 0.639602
Mean IoU: 0.253793
Class IoU:
	class 0: 0.786785
	class 1: 0.4567355
	class 2: 0.28252992
	class 3: 0.14402868
	class 4: 0.3033704
	class 5: 0.486318
	class 6: 0.5180494
	class 7: 0.76693815
	class 8: 0.6133303
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.043078758
	class 13: 0.19404525
	class 14: 0.0
	class 15: 0.6269182
	class 16: 7.0546507e-06
	class 17: 0.0
	class 18: 0.004276977
	class 19: 0.10324971
	class 20: 0.0
Class Acc:
	class 0: 0.99270964
	class 1: 0.4583252
	class 2: 0.58211774
	class 3: 0.1440369
	class 4: 0.31038198
	class 5: 0.4963213
	class 6: 0.5271687
	class 7: 0.8608202
	class 8: 0.64627665
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.04309532
	class 13: 0.4620747
	class 14: 0.0
	class 15: 0.6458467
	class 16: 7.0546507e-06
	class 17: 0.0
	class 18: 0.0042901468
	class 19: 0.11039941
	class 20: 0.0

federated global round: 32, step: 6
select part of clients to conduct local training
[3, 8, 27, 7]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=9.319401502609253
Loss made of: CE 0.7066725492477417, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.135374069213867 EntMin 0.0
Epoch 1, Class Loss=0.7027574777603149, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.7027574777603149, Class Loss=0.7027574777603149, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=8.395024782419204
Loss made of: CE 0.522423267364502, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.496389389038086 EntMin 0.0
Epoch 2, Class Loss=0.5131176710128784, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.5131176710128784, Class Loss=0.5131176710128784, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=7.983283305168152
Loss made of: CE 0.42382532358169556, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.917376518249512 EntMin 0.0
Epoch 3, Class Loss=0.43711957335472107, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.43711957335472107, Class Loss=0.43711957335472107, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=7.6868881493806835
Loss made of: CE 0.38431161642074585, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.750657081604004 EntMin 0.0
Epoch 4, Class Loss=0.4094991683959961, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.4094991683959961, Class Loss=0.4094991683959961, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=7.357166600227356
Loss made of: CE 0.38271230459213257, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.617331504821777 EntMin 0.0
Epoch 5, Class Loss=0.3969491422176361, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.3969491422176361, Class Loss=0.3969491422176361, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=7.148170670866966
Loss made of: CE 0.36218422651290894, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.233840465545654 EntMin 0.0
Epoch 6, Class Loss=0.37454861402511597, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.37454861402511597, Class Loss=0.37454861402511597, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=7.374294626712799
Loss made of: CE 0.6235160827636719, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.537569046020508 EntMin 0.0
Epoch 1, Class Loss=0.8092368245124817, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.8092368245124817, Class Loss=0.8092368245124817, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.480829870700836
Loss made of: CE 0.6954853534698486, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.145130634307861 EntMin 0.0
Epoch 2, Class Loss=0.6879253387451172, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.6879253387451172, Class Loss=0.6879253387451172, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=6.283762222528457
Loss made of: CE 0.6029579639434814, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.364405632019043 EntMin 0.0
Epoch 3, Class Loss=0.6140826344490051, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.6140826344490051, Class Loss=0.6140826344490051, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.956574338674545
Loss made of: CE 0.6765207052230835, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.886824607849121 EntMin 0.0
Epoch 4, Class Loss=0.5595340728759766, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5595340728759766, Class Loss=0.5595340728759766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.831901159882546
Loss made of: CE 0.49537500739097595, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.03523063659668 EntMin 0.0
Epoch 5, Class Loss=0.5418853759765625, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.5418853759765625, Class Loss=0.5418853759765625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.58687804043293
Loss made of: CE 0.5622869729995728, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.197106838226318 EntMin 0.0
Epoch 6, Class Loss=0.5123569369316101, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.5123569369316101, Class Loss=0.5123569369316101, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=7.274087363481522
Loss made of: CE 0.8015798330307007, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.289453983306885 EntMin 0.0
Epoch 1, Class Loss=0.8533008694648743, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.8533008694648743, Class Loss=0.8533008694648743, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.303271454572678
Loss made of: CE 0.8891925811767578, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.612499237060547 EntMin 0.0
Epoch 2, Class Loss=0.7203640937805176, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.7203640937805176, Class Loss=0.7203640937805176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=6.153585538268089
Loss made of: CE 0.6048870086669922, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.588011741638184 EntMin 0.0
Epoch 3, Class Loss=0.5917094349861145, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.5917094349861145, Class Loss=0.5917094349861145, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.726804053783416
Loss made of: CE 0.5069806575775146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4240641593933105 EntMin 0.0
Epoch 4, Class Loss=0.5397213697433472, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.5397213697433472, Class Loss=0.5397213697433472, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.636423116922378
Loss made of: CE 0.5373150706291199, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.720767021179199 EntMin 0.0
Epoch 5, Class Loss=0.5459293127059937, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.5459293127059937, Class Loss=0.5459293127059937, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.6044295728206635
Loss made of: CE 0.53113853931427, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.680652618408203 EntMin 0.0
Epoch 6, Class Loss=0.5139759182929993, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.5139759182929993, Class Loss=0.5139759182929993, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=7.6133187353611
Loss made of: CE 0.7996988296508789, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.310164928436279 EntMin 0.0
Epoch 1, Class Loss=0.8116995096206665, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.8116995096206665, Class Loss=0.8116995096206665, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.829003995656967
Loss made of: CE 0.7142028212547302, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.441442012786865 EntMin 0.0
Epoch 2, Class Loss=0.7304672598838806, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.7304672598838806, Class Loss=0.7304672598838806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=6.092362666130066
Loss made of: CE 0.538486659526825, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.922276496887207 EntMin 0.0
Epoch 3, Class Loss=0.5974683165550232, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.5974683165550232, Class Loss=0.5974683165550232, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=6.23273895084858
Loss made of: CE 0.48897966742515564, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.179549217224121 EntMin 0.0
Epoch 4, Class Loss=0.5755171775817871, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.5755171775817871, Class Loss=0.5755171775817871, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.822263994812966
Loss made of: CE 0.5848361253738403, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.013152122497559 EntMin 0.0
Epoch 5, Class Loss=0.5499950647354126, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.5499950647354126, Class Loss=0.5499950647354126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.733356055617333
Loss made of: CE 0.5649121999740601, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.938534736633301 EntMin 0.0
Epoch 6, Class Loss=0.5115340352058411, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.5115340352058411, Class Loss=0.5115340352058411, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.695691704750061, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.796854
Mean Acc: 0.313191
FreqW Acc: 0.651971
Mean IoU: 0.265764
Class IoU:
	class 0: 0.7966866
	class 1: 0.47530055
	class 2: 0.27958643
	class 3: 0.22352795
	class 4: 0.28516924
	class 5: 0.49972114
	class 6: 0.5471257
	class 7: 0.7470563
	class 8: 0.6877875
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.051892698
	class 13: 0.18306841
	class 14: 0.0
	class 15: 0.67465407
	class 16: 0.00037906988
	class 17: 0.0
	class 18: 0.035007
	class 19: 1.1453127e-05
	class 20: 0.09406353
Class Acc:
	class 0: 0.99191904
	class 1: 0.47702762
	class 2: 0.5554204
	class 3: 0.22356018
	class 4: 0.28989843
	class 5: 0.5104354
	class 6: 0.5603999
	class 7: 0.84983706
	class 8: 0.7457979
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.05190853
	class 13: 0.47215605
	class 14: 0.0
	class 15: 0.70669967
	class 16: 0.00037906988
	class 17: 0.0
	class 18: 0.035621934
	class 19: 1.1453127e-05
	class 20: 0.10593251

federated global round: 33, step: 6
select part of clients to conduct local training
[6, 20, 13, 10]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=5.7797431468963625
Loss made of: CE 0.5092715620994568, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.840475082397461 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.5733177661895752, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.5733177661895752, Class Loss=0.5733177661895752, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.8426207333803175
Loss made of: CE 0.4879727363586426, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.247247695922852 EntMin 0.0
Epoch 2, Class Loss=0.5367868542671204, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.5367868542671204, Class Loss=0.5367868542671204, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.643726739287376
Loss made of: CE 0.5549290180206299, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6873555183410645 EntMin 0.0
Epoch 3, Class Loss=0.5262641906738281, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.5262641906738281, Class Loss=0.5262641906738281, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.697369095683098
Loss made of: CE 0.5382225513458252, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.228451728820801 EntMin 0.0
Epoch 4, Class Loss=0.5317919850349426, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.5317919850349426, Class Loss=0.5317919850349426, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.584093502163887
Loss made of: CE 0.40923231840133667, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.68575382232666 EntMin 0.0
Epoch 5, Class Loss=0.5477520227432251, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.5477520227432251, Class Loss=0.5477520227432251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.1136202841997145
Loss made of: CE 0.44275563955307007, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.336386680603027 EntMin 0.0
Epoch 6, Class Loss=0.4905393719673157, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.4905393719673157, Class Loss=0.4905393719673157, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=5.677218547463417
Loss made of: CE 0.5362451076507568, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.920176029205322 EntMin 0.0
Epoch 1, Class Loss=0.5678574442863464, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.5678574442863464, Class Loss=0.5678574442863464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.842750671505928
Loss made of: CE 0.5952568650245667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.577145576477051 EntMin 0.0
Epoch 2, Class Loss=0.5361773371696472, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.5361773371696472, Class Loss=0.5361773371696472, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.736249905824661
Loss made of: CE 0.43361538648605347, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358667850494385 EntMin 0.0
Epoch 3, Class Loss=0.5223366618156433, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.5223366618156433, Class Loss=0.5223366618156433, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.457461512088775
Loss made of: CE 0.5496329069137573, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.328763484954834 EntMin 0.0
Epoch 4, Class Loss=0.49719569087028503, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.49719569087028503, Class Loss=0.49719569087028503, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.498548078536987
Loss made of: CE 0.4617534577846527, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.508918285369873 EntMin 0.0
Epoch 5, Class Loss=0.48213234543800354, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.48213234543800354, Class Loss=0.48213234543800354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.342456579208374
Loss made of: CE 0.4213138818740845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.630964279174805 EntMin 0.0
Epoch 6, Class Loss=0.4705007076263428, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.4705007076263428, Class Loss=0.4705007076263428, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=10.752805984020233
Loss made of: CE 0.9074327349662781, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.20981216430664 EntMin 0.0
Epoch 1, Class Loss=0.9315080642700195, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.9315080642700195, Class Loss=0.9315080642700195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=8.377147686481475
Loss made of: CE 0.6115432381629944, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.545316696166992 EntMin 0.0
Epoch 2, Class Loss=0.6085119247436523, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.6085119247436523, Class Loss=0.6085119247436523, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=8.157496091723441
Loss made of: CE 0.4542769193649292, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.691905498504639 EntMin 0.0
Epoch 3, Class Loss=0.44451838731765747, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.44451838731765747, Class Loss=0.44451838731765747, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=7.830598604679108
Loss made of: CE 0.37120765447616577, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.330159664154053 EntMin 0.0
Epoch 4, Class Loss=0.41372787952423096, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.41372787952423096, Class Loss=0.41372787952423096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.446133702993393
Loss made of: CE 0.3449136018753052, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.490603923797607 EntMin 0.0
Epoch 5, Class Loss=0.3856579661369324, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.3856579661369324, Class Loss=0.3856579661369324, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=7.364192807674408
Loss made of: CE 0.36029669642448425, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.898565292358398 EntMin 0.0
Epoch 6, Class Loss=0.38609743118286133, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.38609743118286133, Class Loss=0.38609743118286133, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=11.209370028972625
Loss made of: CE 1.0599030256271362, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.206138610839844 EntMin 0.0
Epoch 1, Class Loss=0.9800867438316345, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.9800867438316345, Class Loss=0.9800867438316345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/12, Loss=8.759557938575744
Loss made of: CE 0.8615023493766785, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.4275007247924805 EntMin 0.0
Epoch 2, Class Loss=0.6684052348136902, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.6684052348136902, Class Loss=0.6684052348136902, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=8.240227714180946
Loss made of: CE 0.4941936433315277, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.697601318359375 EntMin 0.0
Epoch 3, Class Loss=0.4776262640953064, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.4776262640953064, Class Loss=0.4776262640953064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/12, Loss=7.861049300432205
Loss made of: CE 0.3739640712738037, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.772261619567871 EntMin 0.0
Epoch 4, Class Loss=0.41382092237472534, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.41382092237472534, Class Loss=0.41382092237472534, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/12, Loss=7.535998013615608
Loss made of: CE 0.36596331000328064, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.690074920654297 EntMin 0.0
Epoch 5, Class Loss=0.39281174540519714, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.39281174540519714, Class Loss=0.39281174540519714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/12, Loss=7.458596077561379
Loss made of: CE 0.3717356324195862, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.785675048828125 EntMin 0.0
Epoch 6, Class Loss=0.4052463471889496, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.4052463471889496, Class Loss=0.4052463471889496, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6680059432983398, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.802258
Mean Acc: 0.327414
FreqW Acc: 0.662059
Mean IoU: 0.278398
Class IoU:
	class 0: 0.8039297
	class 1: 0.47232014
	class 2: 0.2860156
	class 3: 0.21785207
	class 4: 0.24350409
	class 5: 0.5503592
	class 6: 0.66737616
	class 7: 0.76080525
	class 8: 0.69301313
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.061215147
	class 13: 0.1723361
	class 14: 0.0
	class 15: 0.6562888
	class 16: 0.00057236734
	class 17: 0.0
	class 18: 0.023100773
	class 19: 0.18160371
	class 20: 0.05605991
Class Acc:
	class 0: 0.9916275
	class 1: 0.47414336
	class 2: 0.58703375
	class 3: 0.21787956
	class 4: 0.24791291
	class 5: 0.56631136
	class 6: 0.6867254
	class 7: 0.8607241
	class 8: 0.75615966
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.061244402
	class 13: 0.4589266
	class 14: 0.0
	class 15: 0.6866648
	class 16: 0.00057236734
	class 17: 0.0
	class 18: 0.023401588
	class 19: 0.1994134
	class 20: 0.056961402

federated global round: 34, step: 6
select part of clients to conduct local training
[19, 18, 27, 14]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.789368250966072
Loss made of: CE 0.48505696654319763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.514499187469482 EntMin 0.0
Epoch 1, Class Loss=0.6627737879753113, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.6627737879753113, Class Loss=0.6627737879753113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=6.1144325017929075
Loss made of: CE 0.6023838520050049, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.729583263397217 EntMin 0.0
Epoch 2, Class Loss=0.5803220868110657, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.5803220868110657, Class Loss=0.5803220868110657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.694144758582115
Loss made of: CE 0.5196923017501831, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2593560218811035 EntMin 0.0
Epoch 3, Class Loss=0.5381919741630554, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.5381919741630554, Class Loss=0.5381919741630554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.382185825705529
Loss made of: CE 0.47327154874801636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.969998359680176 EntMin 0.0
Epoch 4, Class Loss=0.5061626434326172, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.5061626434326172, Class Loss=0.5061626434326172, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.618252766132355
Loss made of: CE 0.4605814218521118, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.078813076019287 EntMin 0.0
Epoch 5, Class Loss=0.5132027864456177, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.5132027864456177, Class Loss=0.5132027864456177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.405032667517662
Loss made of: CE 0.44584840536117554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.047408103942871 EntMin 0.0
Epoch 6, Class Loss=0.5044916272163391, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.5044916272163391, Class Loss=0.5044916272163391, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.500306016206741
Loss made of: CE 0.6383973956108093, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.725288391113281 EntMin 0.0
Epoch 1, Class Loss=0.6454761624336243, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.6454761624336243, Class Loss=0.6454761624336243, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.573210793733597
Loss made of: CE 0.5262140035629272, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.744281768798828 EntMin 0.0
Epoch 2, Class Loss=0.5401045083999634, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.5401045083999634, Class Loss=0.5401045083999634, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.496767324209213
Loss made of: CE 0.44911378622055054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.163416385650635 EntMin 0.0
Epoch 3, Class Loss=0.5023267269134521, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.5023267269134521, Class Loss=0.5023267269134521, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.256714227795601
Loss made of: CE 0.508219301700592, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.163739204406738 EntMin 0.0
Epoch 4, Class Loss=0.5042694807052612, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.5042694807052612, Class Loss=0.5042694807052612, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.171697637438774
Loss made of: CE 0.6099543571472168, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5582990646362305 EntMin 0.0
Epoch 5, Class Loss=0.4865858256816864, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.4865858256816864, Class Loss=0.4865858256816864, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.128383752703667
Loss made of: CE 0.4196453094482422, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3697052001953125 EntMin 0.0
Epoch 6, Class Loss=0.48297327756881714, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.48297327756881714, Class Loss=0.48297327756881714, Reg Loss=0.0
Current Client Index:  27
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.523386663198471
Loss made of: CE 0.5770933628082275, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.525366306304932 EntMin 0.0
Epoch 1, Class Loss=0.6510794758796692, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.6510794758796692, Class Loss=0.6510794758796692, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/13, Loss=5.464635682106018
Loss made of: CE 0.8115608096122742, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.082294464111328 EntMin 0.0
Epoch 2, Class Loss=0.5864325165748596, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.5864325165748596, Class Loss=0.5864325165748596, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/13, Loss=5.397499841451645
Loss made of: CE 0.5245754718780518, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.791940212249756 EntMin 0.0
Epoch 3, Class Loss=0.5416443943977356, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.5416443943977356, Class Loss=0.5416443943977356, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/13, Loss=5.20868566930294
Loss made of: CE 0.4512426555156708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.116503715515137 EntMin 0.0
Epoch 4, Class Loss=0.5236742496490479, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.5236742496490479, Class Loss=0.5236742496490479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/13, Loss=5.225027400255203
Loss made of: CE 0.5606402158737183, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.310842514038086 EntMin 0.0
Epoch 5, Class Loss=0.5193426012992859, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.5193426012992859, Class Loss=0.5193426012992859, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/13, Loss=5.07105165719986
Loss made of: CE 0.5292913913726807, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.190448760986328 EntMin 0.0
Epoch 6, Class Loss=0.5009531378746033, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.5009531378746033, Class Loss=0.5009531378746033, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 1, Batch 10/13, Loss=6.666051054000855
Loss made of: CE 0.748449981212616, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.200168132781982 EntMin 0.0
Epoch 1, Class Loss=0.6479141712188721, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.6479141712188721, Class Loss=0.6479141712188721, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/13, Loss=5.624487322568894
Loss made of: CE 0.4553748369216919, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.979291915893555 EntMin 0.0
Epoch 2, Class Loss=0.5584487915039062, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.5584487915039062, Class Loss=0.5584487915039062, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/13, Loss=5.392540967464447
Loss made of: CE 0.4659411907196045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.362923622131348 EntMin 0.0
Epoch 3, Class Loss=0.5153685808181763, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.5153685808181763, Class Loss=0.5153685808181763, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/13, Loss=5.2388499051332476
Loss made of: CE 0.4293917417526245, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7901225090026855 EntMin 0.0
Epoch 4, Class Loss=0.5030538439750671, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.5030538439750671, Class Loss=0.5030538439750671, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/13, Loss=5.202073967456817
Loss made of: CE 0.45898836851119995, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.299465656280518 EntMin 0.0
Epoch 5, Class Loss=0.49235743284225464, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.49235743284225464, Class Loss=0.49235743284225464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/13, Loss=5.211685115098954
Loss made of: CE 0.5336193442344666, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.257545471191406 EntMin 0.0
Epoch 6, Class Loss=0.491774320602417, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.491774320602417, Class Loss=0.491774320602417, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7092523574829102, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.797615
Mean Acc: 0.329834
FreqW Acc: 0.660162
Mean IoU: 0.266765
Class IoU:
	class 0: 0.8105041
	class 1: 0.484692
	class 2: 0.28685948
	class 3: 0.2327644
	class 4: 0.23997867
	class 5: 0.51003784
	class 6: 0.34094718
	class 7: 0.74936795
	class 8: 0.7592958
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.045431264
	class 13: 0.18797614
	class 14: 0.0
	class 15: 0.668761
	class 16: 0.002604577
	class 17: 0.0
	class 18: 0.075545445
	class 19: 0.0
	class 20: 0.20729129
Class Acc:
	class 0: 0.9905271
	class 1: 0.4860492
	class 2: 0.5786803
	class 3: 0.23282471
	class 4: 0.24342214
	class 5: 0.52203393
	class 6: 0.34520036
	class 7: 0.8353075
	class 8: 0.83409196
	class 9: 0.0
	class 10: 0.0
	class 11: 0.0
	class 12: 0.04545521
	class 13: 0.57200235
	class 14: 0.0
	class 15: 0.69565314
	class 16: 0.002604577
	class 17: 0.0
	class 18: 0.07880029
	class 19: 0.0
	class 20: 0.46385807

voc_8-2_OURS-APL On GPUs 2
Run in 84446s
