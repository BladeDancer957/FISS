nohup: ignoring input
35
kvoc_8-2_PLOP On GPUs 2\Writing in results/seed_2023-ov/2023-03-14_voc_8-2_PLOP.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[4, 3, 1, 2]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[0, 4, 7, 2]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  4
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[1, 9, 3, 8]
Current Client Index:  1
Current Client Index:  9
Current Client Index:  3
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[5, 9, 0, 4]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Current Client Index:  0
Current Client Index:  4
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.11884414404630661, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.959771
Mean Acc: 0.898867
FreqW Acc: 0.927997
Mean IoU: 0.803600
Class IoU:
	class 0: 0.9505860659032771
	class 1: 0.9002121543757099
	class 2: 0.3911996045853578
	class 3: 0.7886755539422873
	class 4: 0.7227948650191602
	class 5: 0.777020491034379
	class 6: 0.9452273228283382
	class 7: 0.8680230685934628
	class 8: 0.8886579855388657
Class Acc:
	class 0: 0.9742031520387592
	class 1: 0.9505188705401268
	class 2: 0.8333169681087177
	class 3: 0.7954920085182839
	class 4: 0.8649138231019398
	class 5: 0.839701567241524
	class 6: 0.9766569231687022
	class 7: 0.9440467027150332
	class 8: 0.910953836504121

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[5, 11, 7, 1]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/27, Loss=10.285164153575897
Loss made of: CE 0.7412740588188171, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.781871795654297 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.766862282156945
Loss made of: CE 0.3420451581478119, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.694243907928467 EntMin 0.0
Epoch 1, Class Loss=0.6760214567184448, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.6760214567184448, Class Loss=0.6760214567184448, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.518588861823082
Loss made of: CE 0.4372292160987854, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.140501022338867 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.021767023205757
Loss made of: CE 0.36979568004608154, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.352304458618164 EntMin 0.0
Epoch 2, Class Loss=0.4437756836414337, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.4437756836414337, Class Loss=0.4437756836414337, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.800364929437637
Loss made of: CE 0.46115928888320923, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.07527494430542 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.421074107289314
Loss made of: CE 0.35437464714050293, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.950554847717285 EntMin 0.0
Epoch 3, Class Loss=0.4132874608039856, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.4132874608039856, Class Loss=0.4132874608039856, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.410782569646836
Loss made of: CE 0.3554779887199402, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.129863739013672 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.063898023962975
Loss made of: CE 0.40320834517478943, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.560657501220703 EntMin 0.0
Epoch 4, Class Loss=0.40338990092277527, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.40338990092277527, Class Loss=0.40338990092277527, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.102869123220444
Loss made of: CE 0.32538071274757385, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.828292369842529 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.957873541116714
Loss made of: CE 0.38376039266586304, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.410162925720215 EntMin 0.0
Epoch 5, Class Loss=0.37467724084854126, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.37467724084854126, Class Loss=0.37467724084854126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=6.104743859171867
Loss made of: CE 0.3812905251979828, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5499114990234375 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.844868704676628
Loss made of: CE 0.37044140696525574, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.298795700073242 EntMin 0.0
Epoch 6, Class Loss=0.35268765687942505, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.35268765687942505, Class Loss=0.35268765687942505, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1818509101867676, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=1.1818509101867676, Class Loss=1.1818509101867676, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.8557916879653931, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.8557916879653931, Class Loss=0.8557916879653931, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6833714246749878, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.6833714246749878, Class Loss=0.6833714246749878, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5920562744140625, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5920562744140625, Class Loss=0.5920562744140625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5158582925796509, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5158582925796509, Class Loss=0.5158582925796509, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4696844816207886, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.4696844816207886, Class Loss=0.4696844816207886, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.224789601564407
Loss made of: CE 0.7511242628097534, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.08847713470459 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.548264083266258
Loss made of: CE 0.5440800189971924, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.6591596603393555 EntMin 0.0
Epoch 1, Class Loss=0.6674026250839233, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.6674026250839233, Class Loss=0.6674026250839233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.572092786431313
Loss made of: CE 0.6030964851379395, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.260644912719727 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.105941787362099
Loss made of: CE 0.4731632471084595, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.033372402191162 EntMin 0.0
Epoch 2, Class Loss=0.4412582516670227, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.4412582516670227, Class Loss=0.4412582516670227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.60406574010849
Loss made of: CE 0.46023136377334595, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.133347511291504 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.482557672262192
Loss made of: CE 0.4105508029460907, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.84121036529541 EntMin 0.0
Epoch 3, Class Loss=0.425494909286499, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.425494909286499, Class Loss=0.425494909286499, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.198348358273506
Loss made of: CE 0.3775758147239685, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.922001838684082 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.271978181600571
Loss made of: CE 0.512413740158081, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.788661956787109 EntMin 0.0
Epoch 4, Class Loss=0.4072263836860657, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4072263836860657, Class Loss=0.4072263836860657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.245366266369819
Loss made of: CE 0.4127635657787323, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.364303112030029 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.809521296620369
Loss made of: CE 0.28706830739974976, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.366970062255859 EntMin 0.0
Epoch 5, Class Loss=0.38275885581970215, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.38275885581970215, Class Loss=0.38275885581970215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=5.893691548705101
Loss made of: CE 0.3914378881454468, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.475269317626953 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.063565394282341
Loss made of: CE 0.2880060374736786, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.714249610900879 EntMin 0.0
Epoch 6, Class Loss=0.36381205916404724, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.36381205916404724, Class Loss=0.36381205916404724, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.101723754405976
Loss made of: CE 0.6790789365768433, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.217063903808594 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.781900092959404
Loss made of: CE 0.4622718095779419, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.151480674743652 EntMin 0.0
Epoch 1, Class Loss=0.6639537215232849, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.6639537215232849, Class Loss=0.6639537215232849, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.35887633562088
Loss made of: CE 0.3671454191207886, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.647672653198242 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.202939906716347
Loss made of: CE 0.46823176741600037, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.78189754486084 EntMin 0.0
Epoch 2, Class Loss=0.43201959133148193, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.43201959133148193, Class Loss=0.43201959133148193, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.501792547106743
Loss made of: CE 0.4200943410396576, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.377233505249023 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.431533363461495
Loss made of: CE 0.4408852458000183, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.858079433441162 EntMin 0.0
Epoch 3, Class Loss=0.42133647203445435, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.42133647203445435, Class Loss=0.42133647203445435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.150845861434936
Loss made of: CE 0.426384299993515, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.757993698120117 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.249730411171913
Loss made of: CE 0.382786363363266, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.773569107055664 EntMin 0.0
Epoch 4, Class Loss=0.3990137279033661, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.3990137279033661, Class Loss=0.3990137279033661, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=5.894021958112717
Loss made of: CE 0.39204567670822144, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.476123332977295 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.015055474638939
Loss made of: CE 0.30699825286865234, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.719922065734863 EntMin 0.0
Epoch 5, Class Loss=0.38463708758354187, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.38463708758354187, Class Loss=0.38463708758354187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=5.773380541801453
Loss made of: CE 0.3540456295013428, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.260910511016846 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.776332038640976
Loss made of: CE 0.37432169914245605, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.305239677429199 EntMin 0.0
Epoch 6, Class Loss=0.35668227076530457, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.35668227076530457, Class Loss=0.35668227076530457, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3232324719429016, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.924631
Mean Acc: 0.759617
FreqW Acc: 0.863422
Mean IoU: 0.645486
Class IoU:
	class 0: 0.9117466
	class 1: 0.85410875
	class 2: 0.34380028
	class 3: 0.84696114
	class 4: 0.7100125
	class 5: 0.7854156
	class 6: 0.92077565
	class 7: 0.857107
	class 8: 0.8704179
	class 9: 0.0
	class 10: 0.0
Class Acc:
	class 0: 0.96782374
	class 1: 0.9763235
	class 2: 0.9233154
	class 3: 0.8676241
	class 4: 0.8840723
	class 5: 0.8887599
	class 6: 0.97835493
	class 7: 0.9201259
	class 8: 0.9493886
	class 9: 0.0
	class 10: 0.0

federated global round: 6, step: 1
select part of clients to conduct local training
[1, 6, 7, 3]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=6.049473977088928
Loss made of: CE 0.32377898693084717, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5442938804626465 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.928787237405777
Loss made of: CE 0.38806962966918945, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.540839672088623 EntMin 0.0
Epoch 1, Class Loss=0.3804144859313965, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.3804144859313965, Class Loss=0.3804144859313965, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=5.639814323186874
Loss made of: CE 0.36114567518234253, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.478060722351074 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.8797216475009915
Loss made of: CE 0.3737521171569824, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.507165431976318 EntMin 0.0
Epoch 2, Class Loss=0.3655756413936615, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.3655756413936615, Class Loss=0.3655756413936615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=5.549128532409668
Loss made of: CE 0.3474394977092743, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.384891033172607 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.598299103975296
Loss made of: CE 0.36417895555496216, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.153675079345703 EntMin 0.0
Epoch 3, Class Loss=0.34658104181289673, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.34658104181289673, Class Loss=0.34658104181289673, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=5.52874245941639
Loss made of: CE 0.3626640737056732, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.106902122497559 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.656058502197266
Loss made of: CE 0.32967647910118103, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.42062520980835 EntMin 0.0
Epoch 4, Class Loss=0.3312489986419678, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.3312489986419678, Class Loss=0.3312489986419678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.356748795509338
Loss made of: CE 0.3242485225200653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.912292003631592 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.521032819151879
Loss made of: CE 0.2741129994392395, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.32961368560791 EntMin 0.0
Epoch 5, Class Loss=0.319187730550766, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.319187730550766, Class Loss=0.319187730550766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=5.319507594406605
Loss made of: CE 0.33188608288764954, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.968321800231934 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.314327338337899
Loss made of: CE 0.31055569648742676, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.984856128692627 EntMin 0.0
Epoch 6, Class Loss=0.30930668115615845, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.30930668115615845, Class Loss=0.30930668115615845, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.067030689120292
Loss made of: CE 0.3554844856262207, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.35914945602417 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.802460747957229
Loss made of: CE 0.30149030685424805, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5556464195251465 EntMin 0.0
Epoch 1, Class Loss=0.39364543557167053, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.39364543557167053, Class Loss=0.39364543557167053, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/27, Loss=5.751152276992798
Loss made of: CE 0.35377439856529236, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.591578483581543 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.852233177423477
Loss made of: CE 0.39387425780296326, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.579448699951172 EntMin 0.0
Epoch 2, Class Loss=0.3747110962867737, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3747110962867737, Class Loss=0.3747110962867737, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/27, Loss=5.604411381483078
Loss made of: CE 0.3882610499858856, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.252744197845459 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.62630358338356
Loss made of: CE 0.36844658851623535, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.027007102966309 EntMin 0.0
Epoch 3, Class Loss=0.3473292887210846, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.3473292887210846, Class Loss=0.3473292887210846, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/27, Loss=5.492316505312919
Loss made of: CE 0.36364638805389404, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.036924839019775 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.509695991873741
Loss made of: CE 0.2877885103225708, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.086391925811768 EntMin 0.0
Epoch 4, Class Loss=0.3330015540122986, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3330015540122986, Class Loss=0.3330015540122986, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/27, Loss=5.536469048261642
Loss made of: CE 0.31934845447540283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.743453025817871 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.425949715077877
Loss made of: CE 0.2267673909664154, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.106236457824707 EntMin 0.0
Epoch 5, Class Loss=0.31806665658950806, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.31806665658950806, Class Loss=0.31806665658950806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/27, Loss=5.4196504145860676
Loss made of: CE 0.27056530117988586, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1026153564453125 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.289077690243721
Loss made of: CE 0.2500874996185303, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.541466236114502 EntMin 0.0
Epoch 6, Class Loss=0.30278700590133667, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.30278700590133667, Class Loss=0.30278700590133667, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=6.134828811883926
Loss made of: CE 0.4071483612060547, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.894186973571777 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.886171200871468
Loss made of: CE 0.4445759654045105, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.444004058837891 EntMin 0.0
Epoch 1, Class Loss=0.3798196613788605, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.3798196613788605, Class Loss=0.3798196613788605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=5.963356104493141
Loss made of: CE 0.5068631172180176, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.93801212310791 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.781680801510811
Loss made of: CE 0.40963608026504517, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.003655433654785 EntMin 0.0
Epoch 2, Class Loss=0.3688177764415741, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.3688177764415741, Class Loss=0.3688177764415741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=5.653909015655517
Loss made of: CE 0.3983418345451355, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.389077186584473 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.690214577317238
Loss made of: CE 0.35392218828201294, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.175241470336914 EntMin 0.0
Epoch 3, Class Loss=0.3553071916103363, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3553071916103363, Class Loss=0.3553071916103363, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=5.513078597187996
Loss made of: CE 0.30813562870025635, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.118414402008057 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.612276697158814
Loss made of: CE 0.4261946678161621, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.202951908111572 EntMin 0.0
Epoch 4, Class Loss=0.3353402614593506, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.3353402614593506, Class Loss=0.3353402614593506, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.6850384145975115
Loss made of: CE 0.3685801029205322, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.898961067199707 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.249160581827164
Loss made of: CE 0.2401183545589447, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.065622806549072 EntMin 0.0
Epoch 5, Class Loss=0.3238385319709778, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.3238385319709778, Class Loss=0.3238385319709778, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=5.483346024155617
Loss made of: CE 0.35489606857299805, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.116411209106445 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.71260163038969
Loss made of: CE 0.2539798617362976, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.441287994384766 EntMin 0.0
Epoch 6, Class Loss=0.3174750506877899, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.3174750506877899, Class Loss=0.3174750506877899, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1229753494262695, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=1.1229753494262695, Class Loss=1.1229753494262695, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=1.0267839431762695, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=1.0267839431762695, Class Loss=1.0267839431762695, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.8524147868156433, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.8524147868156433, Class Loss=0.8524147868156433, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.7260186076164246, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.7260186076164246, Class Loss=0.7260186076164246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.6297712326049805, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.6297712326049805, Class Loss=0.6297712326049805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.5067422389984131, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.5067422389984131, Class Loss=0.5067422389984131, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3144958019256592, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.925470
Mean Acc: 0.763041
FreqW Acc: 0.865025
Mean IoU: 0.650422
Class IoU:
	class 0: 0.9125923
	class 1: 0.84821564
	class 2: 0.3481866
	class 3: 0.8575103
	class 4: 0.71547693
	class 5: 0.7899229
	class 6: 0.9241176
	class 7: 0.8608198
	class 8: 0.87242275
	class 9: 0.025378024
	class 10: 0.0
Class Acc:
	class 0: 0.9680701
	class 1: 0.9760484
	class 2: 0.9223005
	class 3: 0.8818345
	class 4: 0.87656885
	class 5: 0.8961924
	class 6: 0.97472996
	class 7: 0.91930807
	class 8: 0.95244664
	class 9: 0.025946356
	class 10: 0.0

federated global round: 7, step: 1
select part of clients to conduct local training
[13, 8, 0, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1090106964111328, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=1.1090106964111328, Class Loss=1.1090106964111328, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.0256444215774536, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=1.0256444215774536, Class Loss=1.0256444215774536, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.8424620628356934, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.8424620628356934, Class Loss=0.8424620628356934, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.7103049755096436, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.7103049755096436, Class Loss=0.7103049755096436, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5763428211212158, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5763428211212158, Class Loss=0.5763428211212158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.5155304670333862, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5155304670333862, Class Loss=0.5155304670333862, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2106013298034668, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.2106013298034668, Class Loss=1.2106013298034668, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.0313854217529297, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=1.0313854217529297, Class Loss=1.0313854217529297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.8931875228881836, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.8931875228881836, Class Loss=0.8931875228881836, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.7960653305053711, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.7960653305053711, Class Loss=0.7960653305053711, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6112716794013977, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6112716794013977, Class Loss=0.6112716794013977, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.4981027841567993, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.4981027841567993, Class Loss=0.4981027841567993, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1578495502471924, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=1.1578495502471924, Class Loss=1.1578495502471924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.0896730422973633, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=1.0896730422973633, Class Loss=1.0896730422973633, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.9019705653190613, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.9019705653190613, Class Loss=0.9019705653190613, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.7086607813835144, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.7086607813835144, Class Loss=0.7086607813835144, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5617430210113525, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.5617430210113525, Class Loss=0.5617430210113525, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.47810232639312744, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.47810232639312744, Class Loss=0.47810232639312744, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=5.770853859186173
Loss made of: CE 0.29291486740112305, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4836883544921875 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.59325530230999
Loss made of: CE 0.2524256706237793, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0054931640625 EntMin 0.0
Epoch 1, Class Loss=0.32761406898498535, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.32761406898498535, Class Loss=0.32761406898498535, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/27, Loss=5.410943660140037
Loss made of: CE 0.29951751232147217, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1466288566589355 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.276680114865303
Loss made of: CE 0.2658792734146118, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.891572952270508 EntMin 0.0
Epoch 2, Class Loss=0.3093743622303009, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3093743622303009, Class Loss=0.3093743622303009, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/27, Loss=5.446785473823548
Loss made of: CE 0.32923588156700134, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1252522468566895 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.226664212346077
Loss made of: CE 0.24436670541763306, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.095571517944336 EntMin 0.0
Epoch 3, Class Loss=0.29610010981559753, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.29610010981559753, Class Loss=0.29610010981559753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/27, Loss=5.419132520258427
Loss made of: CE 0.26496621966362, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3847150802612305 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.314839999377727
Loss made of: CE 0.29757747054100037, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8396501541137695 EntMin 0.0
Epoch 4, Class Loss=0.29096418619155884, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.29096418619155884, Class Loss=0.29096418619155884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/27, Loss=5.3750183686614035
Loss made of: CE 0.25424107909202576, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.206155776977539 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.205999712646007
Loss made of: CE 0.27503904700279236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760802268981934 EntMin 0.0
Epoch 5, Class Loss=0.28264319896698, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.28264319896698, Class Loss=0.28264319896698, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/27, Loss=5.316095018386841
Loss made of: CE 0.3158332109451294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.880792617797852 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.206503574550152
Loss made of: CE 0.2797979414463043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.764955997467041 EntMin 0.0
Epoch 6, Class Loss=0.27647486329078674, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.27647486329078674, Class Loss=0.27647486329078674, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2835919260978699, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923453
Mean Acc: 0.761151
FreqW Acc: 0.862941
Mean IoU: 0.643182
Class IoU:
	class 0: 0.911222
	class 1: 0.8484493
	class 2: 0.33236504
	class 3: 0.8531028
	class 4: 0.6728267
	class 5: 0.76972723
	class 6: 0.9250637
	class 7: 0.86124235
	class 8: 0.8683638
	class 9: 0.032640312
	class 10: 0.0
Class Acc:
	class 0: 0.96728384
	class 1: 0.9688814
	class 2: 0.94804674
	class 3: 0.88557166
	class 4: 0.8752497
	class 5: 0.8846887
	class 6: 0.9657504
	class 7: 0.9067987
	class 8: 0.935969
	class 9: 0.034421127
	class 10: 0.0

federated global round: 8, step: 1
select part of clients to conduct local training
[12, 9, 4, 13]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.610849547386169
Loss made of: CE 0.48284873366355896, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.513194561004639 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.548860216140747
Loss made of: CE 0.3248639702796936, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.007603645324707 EntMin 0.0
Epoch 1, Class Loss=0.3599611520767212, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.3599611520767212, Class Loss=0.3599611520767212, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.346985659003257
Loss made of: CE 0.3330153226852417, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.09349250793457 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.5188702046871185
Loss made of: CE 0.3878316879272461, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.338009834289551 EntMin 0.0
Epoch 2, Class Loss=0.31657737493515015, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.31657737493515015, Class Loss=0.31657737493515015, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=5.42307284027338
Loss made of: CE 0.27809080481529236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.840512752532959 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.44125075340271
Loss made of: CE 0.2711509168148041, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734200477600098 EntMin 0.0
Epoch 3, Class Loss=0.2934929430484772, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.2934929430484772, Class Loss=0.2934929430484772, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.309879915416241
Loss made of: CE 0.25911861658096313, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.962400436401367 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.090330807864666
Loss made of: CE 0.2741144597530365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.837590217590332 EntMin 0.0
Epoch 4, Class Loss=0.27701595425605774, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.27701595425605774, Class Loss=0.27701595425605774, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.3073219656944275
Loss made of: CE 0.2545810639858246, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.233745574951172 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.315950457751751
Loss made of: CE 0.2627791166305542, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.250087261199951 EntMin 0.0
Epoch 5, Class Loss=0.26790517568588257, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.26790517568588257, Class Loss=0.26790517568588257, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.060092820227146
Loss made of: CE 0.33181139826774597, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.592728614807129 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.047237120568752
Loss made of: CE 0.2901437282562256, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.000561714172363 EntMin 0.0
Epoch 6, Class Loss=0.2669846713542938, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.2669846713542938, Class Loss=0.2669846713542938, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.5971188545227051, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5971188545227051, Class Loss=0.5971188545227051, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.5246498584747314, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.5246498584747314, Class Loss=0.5246498584747314, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.428107351064682, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.428107351064682, Class Loss=0.428107351064682, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.3825458288192749, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.3825458288192749, Class Loss=0.3825458288192749, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.3539038300514221, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.3539038300514221, Class Loss=0.3539038300514221, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.3125825524330139, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3125825524330139, Class Loss=0.3125825524330139, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.560254696011543
Loss made of: CE 0.3893664479255676, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.677221298217773 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.718944618105889
Loss made of: CE 0.35205405950546265, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3283820152282715 EntMin 0.0
Epoch 1, Class Loss=0.36885857582092285, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.36885857582092285, Class Loss=0.36885857582092285, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.498648357391358
Loss made of: CE 0.295487642288208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.695632457733154 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.538048252463341
Loss made of: CE 0.3519589602947235, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758658409118652 EntMin 0.0
Epoch 2, Class Loss=0.3260228931903839, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.3260228931903839, Class Loss=0.3260228931903839, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=5.174525174498558
Loss made of: CE 0.32523468136787415, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.817930221557617 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.4555618673563
Loss made of: CE 0.328576922416687, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.794426918029785 EntMin 0.0
Epoch 3, Class Loss=0.29987064003944397, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.29987064003944397, Class Loss=0.29987064003944397, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.285717287659645
Loss made of: CE 0.34048590064048767, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.296360015869141 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.429350593686104
Loss made of: CE 0.2551448941230774, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.084694862365723 EntMin 0.0
Epoch 4, Class Loss=0.28721582889556885, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.28721582889556885, Class Loss=0.28721582889556885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.264617872238159
Loss made of: CE 0.2630915641784668, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.25393009185791 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.245082549750805
Loss made of: CE 0.2663966417312622, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.90939998626709 EntMin 0.0
Epoch 5, Class Loss=0.28315070271492004, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.28315070271492004, Class Loss=0.28315070271492004, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.130214831233024
Loss made of: CE 0.3538246154785156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.912432670593262 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.09752077460289
Loss made of: CE 0.2866513729095459, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.469011306762695 EntMin 0.0
Epoch 6, Class Loss=0.2743909955024719, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.2743909955024719, Class Loss=0.2743909955024719, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.5729632377624512, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5729632377624512, Class Loss=0.5729632377624512, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Epoch 2, Class Loss=0.5370906591415405, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.5370906591415405, Class Loss=0.5370906591415405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.4734918773174286, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.4734918773174286, Class Loss=0.4734918773174286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.44310617446899414, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.44310617446899414, Class Loss=0.44310617446899414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.39412420988082886, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.39412420988082886, Class Loss=0.39412420988082886, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.38839274644851685, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.38839274644851685, Class Loss=0.38839274644851685, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.25439536571502686, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.925956
Mean Acc: 0.763705
FreqW Acc: 0.867359
Mean IoU: 0.654132
Class IoU:
	class 0: 0.9145147
	class 1: 0.8620183
	class 2: 0.34593523
	class 3: 0.85853505
	class 4: 0.6962892
	class 5: 0.7866838
	class 6: 0.9232143
	class 7: 0.86595696
	class 8: 0.87532246
	class 9: 0.0669828
	class 10: 0.0
Class Acc:
	class 0: 0.96951056
	class 1: 0.96742946
	class 2: 0.93708014
	class 3: 0.88620865
	class 4: 0.86285895
	class 5: 0.88750356
	class 6: 0.95265317
	class 7: 0.91136235
	class 8: 0.95066595
	class 9: 0.07548025
	class 10: 0.0

federated global round: 9, step: 1
select part of clients to conduct local training
[4, 6, 13, 12]
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=5.748578435182571
Loss made of: CE 0.32778453826904297, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.108545780181885 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.360275661945343
Loss made of: CE 0.32695135474205017, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.029377460479736 EntMin 0.0
Epoch 1, Class Loss=0.3334270119667053, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.3334270119667053, Class Loss=0.3334270119667053, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.165844324231148
Loss made of: CE 0.2832268476486206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.691355228424072 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.27354978621006
Loss made of: CE 0.32300764322280884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.454894065856934 EntMin 0.0
Epoch 2, Class Loss=0.31359267234802246, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.31359267234802246, Class Loss=0.31359267234802246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=4.90729818046093
Loss made of: CE 0.32855424284935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53242301940918 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.248156832158566
Loss made of: CE 0.35975098609924316, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.658463954925537 EntMin 0.0
Epoch 3, Class Loss=0.3043273091316223, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.3043273091316223, Class Loss=0.3043273091316223, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.0082025736570355
Loss made of: CE 0.41321295499801636, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0926008224487305 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.13762743473053
Loss made of: CE 0.30192527174949646, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.818739891052246 EntMin 0.0
Epoch 4, Class Loss=0.30501607060432434, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.30501607060432434, Class Loss=0.30501607060432434, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.017652893066407
Loss made of: CE 0.2599658966064453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758232593536377 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.048523256182671
Loss made of: CE 0.2767237424850464, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.754649639129639 EntMin 0.0
Epoch 5, Class Loss=0.29158642888069153, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.29158642888069153, Class Loss=0.29158642888069153, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=5.066996045410633
Loss made of: CE 0.37357768416404724, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.78123140335083 EntMin 0.0
Epoch 6, Batch 20/27, Loss=4.945751124620438
Loss made of: CE 0.32408642768859863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.34177827835083 EntMin 0.0
Epoch 6, Class Loss=0.29420238733291626, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.29420238733291626, Class Loss=0.29420238733291626, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/27, Loss=5.692906576395035
Loss made of: CE 0.31271132826805115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.842795372009277 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.298632767796517
Loss made of: CE 0.2752475440502167, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.035593032836914 EntMin 0.0
Epoch 1, Class Loss=0.3443971276283264, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.3443971276283264, Class Loss=0.3443971276283264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/27, Loss=5.126519951224327
Loss made of: CE 0.31719595193862915, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.103212356567383 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.240342408418655
Loss made of: CE 0.369116872549057, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.040327548980713 EntMin 0.0
Epoch 2, Class Loss=0.3140776753425598, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3140776753425598, Class Loss=0.3140776753425598, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/27, Loss=5.075217428803444
Loss made of: CE 0.33700424432754517, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.782357215881348 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.118206706643105
Loss made of: CE 0.33223092555999756, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.51722526550293 EntMin 0.0
Epoch 3, Class Loss=0.30176490545272827, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.30176490545272827, Class Loss=0.30176490545272827, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/27, Loss=4.954379534721374
Loss made of: CE 0.3065147399902344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.418394088745117 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.0710508733987805
Loss made of: CE 0.256946861743927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6881632804870605 EntMin 0.0
Epoch 4, Class Loss=0.29840511083602905, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.29840511083602905, Class Loss=0.29840511083602905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/27, Loss=5.022450628876686
Loss made of: CE 0.28109031915664673, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304452896118164 EntMin 0.0
Epoch 5, Batch 20/27, Loss=4.932900829613208
Loss made of: CE 0.2124728411436081, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.831178665161133 EntMin 0.0
Epoch 5, Class Loss=0.2933034300804138, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.2933034300804138, Class Loss=0.2933034300804138, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/27, Loss=4.883592139184475
Loss made of: CE 0.22477847337722778, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.591243267059326 EntMin 0.0
Epoch 6, Batch 20/27, Loss=4.7961362794041635
Loss made of: CE 0.2507001757621765, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.310563564300537 EntMin 0.0
Epoch 6, Class Loss=0.2867225110530853, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.2867225110530853, Class Loss=0.2867225110530853, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000372
Epoch 1, Class Loss=0.6187698841094971, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.6187698841094971, Class Loss=0.6187698841094971, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.5714132785797119, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.5714132785797119, Class Loss=0.5714132785797119, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000258
Epoch 3, Class Loss=0.5349024534225464, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.5349024534225464, Class Loss=0.5349024534225464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000199
Epoch 4, Class Loss=0.5335667729377747, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5335667729377747, Class Loss=0.5335667729377747, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000138
Epoch 5, Class Loss=0.513964056968689, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.513964056968689, Class Loss=0.513964056968689, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.5195175409317017, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5195175409317017, Class Loss=0.5195175409317017, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=5.825857037305832
Loss made of: CE 0.418470561504364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.902820587158203 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.274848935008049
Loss made of: CE 0.32087036967277527, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.811489582061768 EntMin 0.0
Epoch 1, Class Loss=0.3245683014392853, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.3245683014392853, Class Loss=0.3245683014392853, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.141184484958648
Loss made of: CE 0.33860787749290466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.750212669372559 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.3545036047697065
Loss made of: CE 0.3772503733634949, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.164612770080566 EntMin 0.0
Epoch 2, Class Loss=0.3047834634780884, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.3047834634780884, Class Loss=0.3047834634780884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=5.160797953605652
Loss made of: CE 0.237921804189682, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7153239250183105 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.143571084737777
Loss made of: CE 0.27207398414611816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.460225582122803 EntMin 0.0
Epoch 3, Class Loss=0.2962106764316559, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.2962106764316559, Class Loss=0.2962106764316559, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.045726779103279
Loss made of: CE 0.2556315064430237, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807976722717285 EntMin 0.0
Epoch 4, Batch 20/27, Loss=4.8595210909843445
Loss made of: CE 0.2845260202884674, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.77720832824707 EntMin 0.0
Epoch 4, Class Loss=0.2856537103652954, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.2856537103652954, Class Loss=0.2856537103652954, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.114261817932129
Loss made of: CE 0.257451593875885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.999737739562988 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.067211875319481
Loss made of: CE 0.26372572779655457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.916505813598633 EntMin 0.0
Epoch 5, Class Loss=0.2829473912715912, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.2829473912715912, Class Loss=0.2829473912715912, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=4.966132459044457
Loss made of: CE 0.3402882516384125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594979286193848 EntMin 0.0
Epoch 6, Batch 20/27, Loss=4.855292212963104
Loss made of: CE 0.32316744327545166, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.705765724182129 EntMin 0.0
Epoch 6, Class Loss=0.28437620401382446, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.28437620401382446, Class Loss=0.28437620401382446, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2578570246696472, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.926032
Mean Acc: 0.776686
FreqW Acc: 0.869634
Mean IoU: 0.665292
Class IoU:
	class 0: 0.91479623
	class 1: 0.86653
	class 2: 0.3500811
	class 3: 0.86963594
	class 4: 0.71009076
	class 5: 0.7898252
	class 6: 0.9248626
	class 7: 0.869138
	class 8: 0.8821261
	class 9: 0.14112459
	class 10: 0.0
Class Acc:
	class 0: 0.9663225
	class 1: 0.96970314
	class 2: 0.9317542
	class 3: 0.897455
	class 4: 0.86323285
	class 5: 0.89306664
	class 6: 0.9548922
	class 7: 0.9177406
	class 8: 0.9572383
	class 9: 0.1921408
	class 10: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[10, 14, 16, 7]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=11.924573469161988
Loss made of: CE 0.3606038987636566, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.382658004760742 EntMin 0.0
Epoch 1, Batch 20/29, Loss=10.24296238720417
Loss made of: CE 0.3302919864654541, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.279376983642578 EntMin 0.0
Epoch 1, Class Loss=0.39045125246047974, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.39045125246047974, Class Loss=0.39045125246047974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=9.151776376366616
Loss made of: CE 0.2156628966331482, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.055280685424805 EntMin 0.0
Epoch 2, Batch 20/29, Loss=8.732904449105263
Loss made of: CE 0.2793118357658386, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.00552749633789 EntMin 0.0
Epoch 2, Class Loss=0.2786937654018402, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.2786937654018402, Class Loss=0.2786937654018402, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.470792715251445
Loss made of: CE 0.18353132903575897, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.550917625427246 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.801387074589729
Loss made of: CE 0.16061463952064514, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.080835342407227 EntMin 0.0
Epoch 3, Class Loss=0.22366374731063843, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.22366374731063843, Class Loss=0.22366374731063843, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=7.837689501047135
Loss made of: CE 0.20389974117279053, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.057317733764648 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.748195357620716
Loss made of: CE 0.20213770866394043, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.448223114013672 EntMin 0.0
Epoch 4, Class Loss=0.18273256719112396, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.18273256719112396, Class Loss=0.18273256719112396, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.387310759723187
Loss made of: CE 0.15625928342342377, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.6632399559021 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.554826451838016
Loss made of: CE 0.17825332283973694, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.242674827575684 EntMin 0.0
Epoch 5, Class Loss=0.14728310704231262, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.14728310704231262, Class Loss=0.14728310704231262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.085462436079979
Loss made of: CE 0.11085033416748047, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.347964286804199 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.136716838181019
Loss made of: CE 0.09727022051811218, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.502621650695801 EntMin 0.0
Epoch 6, Class Loss=0.12757369875907898, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.12757369875907898, Class Loss=0.12757369875907898, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=7.328312557935715
Loss made of: CE 0.5645232200622559, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.304460048675537 EntMin 0.0
Epoch 1, Class Loss=0.5868484377861023, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.5868484377861023, Class Loss=0.5868484377861023, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.48983618915081
Loss made of: CE 0.38790592551231384, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.883334159851074 EntMin 0.0
Epoch 2, Class Loss=0.4567053020000458, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.4567053020000458, Class Loss=0.4567053020000458, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=5.982827451825142
Loss made of: CE 0.36213988065719604, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.102428913116455 EntMin 0.0
Epoch 3, Class Loss=0.4074217975139618, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.4074217975139618, Class Loss=0.4074217975139618, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.644212535023689
Loss made of: CE 0.43120986223220825, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.568032741546631 EntMin 0.0
Epoch 4, Class Loss=0.38554656505584717, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.38554656505584717, Class Loss=0.38554656505584717, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.492388212680817
Loss made of: CE 0.33533167839050293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.822164535522461 EntMin 0.0
Epoch 5, Class Loss=0.35448262095451355, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.35448262095451355, Class Loss=0.35448262095451355, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.308246314525604
Loss made of: CE 0.37747734785079956, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.91064453125 EntMin 0.0
Epoch 6, Class Loss=0.3393910229206085, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.3393910229206085, Class Loss=0.3393910229206085, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=12.285243713855744
Loss made of: CE 0.48116955161094666, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.066584587097168 EntMin 0.0
Epoch 1, Batch 20/29, Loss=10.432220244407654
Loss made of: CE 0.2598627209663391, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.885007858276367 EntMin 0.0
Epoch 1, Class Loss=0.40449896454811096, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.40449896454811096, Class Loss=0.40449896454811096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=9.025191071629525
Loss made of: CE 0.226421058177948, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.67931079864502 EntMin 0.0
Epoch 2, Batch 20/29, Loss=9.01680684387684
Loss made of: CE 0.2939872741699219, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.776195526123047 EntMin 0.0
Epoch 2, Class Loss=0.2672020196914673, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.2672020196914673, Class Loss=0.2672020196914673, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.616490139067173
Loss made of: CE 0.29781240224838257, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.0122709274292 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.837961173057556
Loss made of: CE 0.2693048417568207, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.270026206970215 EntMin 0.0
Epoch 3, Class Loss=0.22610889375209808, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.22610889375209808, Class Loss=0.22610889375209808, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=7.895507323741913
Loss made of: CE 0.22635671496391296, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.141683578491211 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.885656051337719
Loss made of: CE 0.15917466580867767, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.858780860900879 EntMin 0.0
Epoch 4, Class Loss=0.17588335275650024, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.17588335275650024, Class Loss=0.17588335275650024, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.294104482978582
Loss made of: CE 0.12520161271095276, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.323459625244141 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.642239625006914
Loss made of: CE 0.22562234103679657, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1526641845703125 EntMin 0.0
Epoch 5, Class Loss=0.1463831514120102, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.1463831514120102, Class Loss=0.1463831514120102, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.222534308582544
Loss made of: CE 0.18803510069847107, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.41400146484375 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.229725623875856
Loss made of: CE 0.12462731450796127, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.651925563812256 EntMin 0.0
Epoch 6, Class Loss=0.13147881627082825, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.13147881627082825, Class Loss=0.13147881627082825, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=7.147007843852043
Loss made of: CE 0.5986520051956177, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.9798173904418945 EntMin 0.0
Epoch 1, Class Loss=0.5684193968772888, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.5684193968772888, Class Loss=0.5684193968772888, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.416219937801361
Loss made of: CE 0.4173316955566406, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.395356178283691 EntMin 0.0
Epoch 2, Class Loss=0.444170206785202, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.444170206785202, Class Loss=0.444170206785202, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=5.908087641000748
Loss made of: CE 0.29681020975112915, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7475199699401855 EntMin 0.0
Epoch 3, Class Loss=0.40149828791618347, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.40149828791618347, Class Loss=0.40149828791618347, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.782872200012207
Loss made of: CE 0.28869006037712097, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.078615188598633 EntMin 0.0
Epoch 4, Class Loss=0.3659583032131195, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.3659583032131195, Class Loss=0.3659583032131195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.61801600754261
Loss made of: CE 0.43946632742881775, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.766902446746826 EntMin 0.0
Epoch 5, Class Loss=0.35763779282569885, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.35763779282569885, Class Loss=0.35763779282569885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.472990560531616
Loss made of: CE 0.35137903690338135, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.668014049530029 EntMin 0.0
Epoch 6, Class Loss=0.33428266644477844, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.33428266644477844, Class Loss=0.33428266644477844, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.49207523465156555, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.869058
Mean Acc: 0.671971
FreqW Acc: 0.782938
Mean IoU: 0.531375
Class IoU:
	class 0: 0.8684733
	class 1: 0.8213311
	class 2: 0.29129615
	class 3: 0.85353065
	class 4: 0.6901545
	class 5: 0.76107305
	class 6: 0.9187763
	class 7: 0.837899
	class 8: 0.75733536
	class 9: 0.099129364
	class 10: 0.0010353805
	class 11: 0.0
	class 12: 0.007844292
Class Acc:
	class 0: 0.9490232
	class 1: 0.97237706
	class 2: 0.95215
	class 3: 0.9141894
	class 4: 0.8844676
	class 5: 0.9168673
	class 6: 0.9679108
	class 7: 0.9202914
	class 8: 0.96563476
	class 9: 0.2838229
	class 10: 0.0010355989
	class 11: 0.0
	class 12: 0.007848037

federated global round: 11, step: 2
select part of clients to conduct local training
[10, 13, 6, 1]
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/29, Loss=8.808919304609299
Loss made of: CE 0.21641938388347626, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.828441619873047 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.9459254667162895
Loss made of: CE 0.2700038552284241, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.458953380584717 EntMin 0.0
Epoch 1, Class Loss=0.2708858847618103, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.2708858847618103, Class Loss=0.2708858847618103, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/29, Loss=7.770634176582098
Loss made of: CE 0.11199434846639633, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.568216323852539 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.556065866351128
Loss made of: CE 0.18275779485702515, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.074723243713379 EntMin 0.0
Epoch 2, Class Loss=0.18388177454471588, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.18388177454471588, Class Loss=0.18388177454471588, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/29, Loss=7.376358335465193
Loss made of: CE 0.15157529711723328, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.491074562072754 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.973478350788355
Loss made of: CE 0.1314743459224701, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.496146202087402 EntMin 0.0
Epoch 3, Class Loss=0.14880675077438354, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.14880675077438354, Class Loss=0.14880675077438354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/29, Loss=7.123041473329067
Loss made of: CE 0.15868599712848663, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.251826286315918 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.102767926454544
Loss made of: CE 0.16717450320720673, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.861913681030273 EntMin 0.0
Epoch 4, Class Loss=0.12795619666576385, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.12795619666576385, Class Loss=0.12795619666576385, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=7.06098897755146
Loss made of: CE 0.13172286748886108, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.382688045501709 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.982729185372591
Loss made of: CE 0.14681878685951233, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.6693644523620605 EntMin 0.0
Epoch 5, Class Loss=0.11750801652669907, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.11750801652669907, Class Loss=0.11750801652669907, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/29, Loss=6.7849446140229706
Loss made of: CE 0.07865267992019653, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.817141532897949 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.55399409905076
Loss made of: CE 0.0844879299402237, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.15561056137085 EntMin 0.0
Epoch 6, Class Loss=0.10488034039735794, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.10488034039735794, Class Loss=0.10488034039735794, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=9.161589385569096
Loss made of: CE 0.30153772234916687, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.464616775512695 EntMin 0.0
Epoch 1, Batch 20/29, Loss=8.235907261073589
Loss made of: CE 0.16341114044189453, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.947043418884277 EntMin 0.0
Epoch 1, Class Loss=0.2588903605937958, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.2588903605937958, Class Loss=0.2588903605937958, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/29, Loss=8.0796713873744
Loss made of: CE 0.19620874524116516, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.78169584274292 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.41028770506382
Loss made of: CE 0.08912906050682068, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.333896160125732 EntMin 0.0
Epoch 2, Class Loss=0.170525461435318, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.170525461435318, Class Loss=0.170525461435318, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/29, Loss=7.247190649807453
Loss made of: CE 0.16044017672538757, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.264930725097656 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.781571887433529
Loss made of: CE 0.08519242703914642, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.128329753875732 EntMin 0.0
Epoch 3, Class Loss=0.13983267545700073, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.13983267545700073, Class Loss=0.13983267545700073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/29, Loss=7.281006409227848
Loss made of: CE 0.10714755952358246, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.276708602905273 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 20/29, Loss=7.340366741269827
Loss made of: CE 0.09143714606761932, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.057132720947266 EntMin 0.0
Epoch 4, Class Loss=0.11992237716913223, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.11992237716913223, Class Loss=0.11992237716913223, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/29, Loss=6.944583067297936
Loss made of: CE 0.06378333270549774, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.102944850921631 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.11070651486516
Loss made of: CE 0.12662380933761597, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.3491058349609375 EntMin 0.0
Epoch 5, Class Loss=0.10723831504583359, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.10723831504583359, Class Loss=0.10723831504583359, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/29, Loss=6.77961453050375
Loss made of: CE 0.10132061690092087, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.717404842376709 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.995298577845096
Loss made of: CE 0.06249522790312767, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.129866600036621 EntMin 0.0
Epoch 6, Class Loss=0.0942254289984703, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.0942254289984703, Class Loss=0.0942254289984703, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.80512373149395
Loss made of: CE 0.7069472074508667, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.618426322937012 EntMin 0.0
Epoch 1, Class Loss=0.47580772638320923, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.47580772638320923, Class Loss=0.47580772638320923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=5.981265741586685
Loss made of: CE 0.38291770219802856, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.401298522949219 EntMin 0.0
Epoch 2, Class Loss=0.42910563945770264, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.42910563945770264, Class Loss=0.42910563945770264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=5.59910252392292
Loss made of: CE 0.29559463262557983, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.653619289398193 EntMin 0.0
Epoch 3, Class Loss=0.3795231878757477, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.3795231878757477, Class Loss=0.3795231878757477, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.427111363410949
Loss made of: CE 0.40736421942710876, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2022271156311035 EntMin 0.0
Epoch 4, Class Loss=0.3335422873497009, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3335422873497009, Class Loss=0.3335422873497009, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.267194195091724
Loss made of: CE 0.3122376501560211, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.710170269012451 EntMin 0.0
Epoch 5, Class Loss=0.296329528093338, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.296329528093338, Class Loss=0.296329528093338, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.07358333170414
Loss made of: CE 0.24994634091854095, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.18212890625 EntMin 0.0
Epoch 6, Class Loss=0.27144724130630493, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.27144724130630493, Class Loss=0.27144724130630493, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.777984911203385
Loss made of: CE 0.44570669531822205, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.554921627044678 EntMin 0.0
Epoch 1, Class Loss=0.5111969709396362, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.5111969709396362, Class Loss=0.5111969709396362, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=6.040018138289452
Loss made of: CE 0.38154634833335876, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.248826026916504 EntMin 0.0
Epoch 2, Class Loss=0.43106624484062195, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.43106624484062195, Class Loss=0.43106624484062195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/13, Loss=5.408460968732834
Loss made of: CE 0.35716700553894043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.578342437744141 EntMin 0.0
Epoch 3, Class Loss=0.3679454028606415, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3679454028606415, Class Loss=0.3679454028606415, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.388242527842522
Loss made of: CE 0.3167068660259247, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.757519245147705 EntMin 0.0
Epoch 4, Class Loss=0.3268918991088867, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.3268918991088867, Class Loss=0.3268918991088867, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.209769055247307
Loss made of: CE 0.27689865231513977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.647740840911865 EntMin 0.0
Epoch 5, Class Loss=0.29299065470695496, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.29299065470695496, Class Loss=0.29299065470695496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=4.988218083977699
Loss made of: CE 0.26279112696647644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.985942363739014 EntMin 0.0
Epoch 6, Class Loss=0.2700604796409607, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.2700604796409607, Class Loss=0.2700604796409607, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.44121503829956055, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.886595
Mean Acc: 0.704478
FreqW Acc: 0.809715
Mean IoU: 0.556383
Class IoU:
	class 0: 0.88720435
	class 1: 0.8165762
	class 2: 0.28134394
	class 3: 0.84780556
	class 4: 0.69001824
	class 5: 0.7296205
	class 6: 0.91731685
	class 7: 0.8286027
	class 8: 0.75728506
	class 9: 0.10761168
	class 10: 0.0
	class 11: 0.0
	class 12: 0.369593
Class Acc:
	class 0: 0.9504401
	class 1: 0.9714212
	class 2: 0.9654904
	class 3: 0.9385867
	class 4: 0.8902529
	class 5: 0.9367673
	class 6: 0.9602401
	class 7: 0.92467254
	class 8: 0.9647073
	class 9: 0.18816508
	class 10: 0.0
	class 11: 0.0
	class 12: 0.46747038

federated global round: 12, step: 2
select part of clients to conduct local training
[11, 0, 8, 14]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.676542887091637
Loss made of: CE 0.5045844316482544, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.884664058685303 EntMin 0.0
Epoch 1, Class Loss=0.4338386654853821, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.4338386654853821, Class Loss=0.4338386654853821, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=5.9335743427276615
Loss made of: CE 0.39599961042404175, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.937213897705078 EntMin 0.0
Epoch 2, Class Loss=0.3783714473247528, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.3783714473247528, Class Loss=0.3783714473247528, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.360283485054969
Loss made of: CE 0.2873242497444153, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4956231117248535 EntMin 0.0
Epoch 3, Class Loss=0.3084500730037689, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.3084500730037689, Class Loss=0.3084500730037689, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.286342221498489
Loss made of: CE 0.29746586084365845, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.048450946807861 EntMin 0.0
Epoch 4, Class Loss=0.2579505443572998, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.2579505443572998, Class Loss=0.2579505443572998, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.378523337841034
Loss made of: CE 0.2663746476173401, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7798662185668945 EntMin 0.0
Epoch 5, Class Loss=0.24733000993728638, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.24733000993728638, Class Loss=0.24733000993728638, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=4.973992052674293
Loss made of: CE 0.2126147747039795, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.191094875335693 EntMin 0.0
Epoch 6, Class Loss=0.23379458487033844, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.23379458487033844, Class Loss=0.23379458487033844, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.10572367310524
Loss made of: CE 0.21285131573677063, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.515647888183594 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.359670501947403
Loss made of: CE 0.15512651205062866, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.215836524963379 EntMin 0.0
Epoch 1, Class Loss=0.17052537202835083, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.17052537202835083, Class Loss=0.17052537202835083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=7.07542674690485
Loss made of: CE 0.09574423730373383, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.747998237609863 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.249324386566878
Loss made of: CE 0.10133133828639984, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.186723232269287 EntMin 0.0
Epoch 2, Class Loss=0.12142111361026764, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.12142111361026764, Class Loss=0.12142111361026764, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=7.0245661467313765
Loss made of: CE 0.1399848312139511, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.289597511291504 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.681878519803286
Loss made of: CE 0.08871641010046005, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.428393840789795 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.10304015129804611, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.10304015129804611, Class Loss=0.10304015129804611, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=6.707106976211071
Loss made of: CE 0.08800798654556274, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.14488410949707 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.662989605218172
Loss made of: CE 0.08382042497396469, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.387054443359375 EntMin 0.0
Epoch 4, Class Loss=0.09294581413269043, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.09294581413269043, Class Loss=0.09294581413269043, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=6.479215169698
Loss made of: CE 0.10732865333557129, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.241806983947754 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.616657095402479
Loss made of: CE 0.09546753764152527, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.055685997009277 EntMin 0.0
Epoch 5, Class Loss=0.09042135626077652, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.09042135626077652, Class Loss=0.09042135626077652, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=6.504592223465442
Loss made of: CE 0.06861236691474915, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.115839004516602 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.530437732487917
Loss made of: CE 0.06209801137447357, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.416977882385254 EntMin 0.0
Epoch 6, Class Loss=0.07914446294307709, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.07914446294307709, Class Loss=0.07914446294307709, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.341098523139953
Loss made of: CE 0.25951087474823, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.088800430297852 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.434202043712139
Loss made of: CE 0.1594017744064331, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.733409404754639 EntMin 0.0
Epoch 1, Class Loss=0.17926692962646484, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.17926692962646484, Class Loss=0.17926692962646484, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=6.895603884756565
Loss made of: CE 0.10341942310333252, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.372198104858398 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.268146499991417
Loss made of: CE 0.14297348260879517, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.162734508514404 EntMin 0.0
Epoch 2, Class Loss=0.13035255670547485, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.13035255670547485, Class Loss=0.13035255670547485, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=7.193466480076313
Loss made of: CE 0.06452104449272156, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.393868923187256 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.128168601542711
Loss made of: CE 0.12654851377010345, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.447225093841553 EntMin 0.0
Epoch 3, Class Loss=0.1057879626750946, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.1057879626750946, Class Loss=0.1057879626750946, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=6.5343417473137375
Loss made of: CE 0.09494294226169586, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.475284576416016 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.957595433294773
Loss made of: CE 0.10886476933956146, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.224246978759766 EntMin 0.0
Epoch 4, Class Loss=0.09731898456811905, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.09731898456811905, Class Loss=0.09731898456811905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=6.7052724283188585
Loss made of: CE 0.07025536149740219, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.37917423248291 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.451608663797378
Loss made of: CE 0.08463682234287262, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.810704231262207 EntMin 0.0
Epoch 5, Class Loss=0.08838857710361481, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.08838857710361481, Class Loss=0.08838857710361481, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=6.672264535725117
Loss made of: CE 0.09539463371038437, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.199119567871094 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.635075198113919
Loss made of: CE 0.07542818039655685, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.633783340454102 EntMin 0.0
Epoch 6, Class Loss=0.08070260286331177, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.08070260286331177, Class Loss=0.08070260286331177, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.653192988038063
Loss made of: CE 0.45566123723983765, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.452270030975342 EntMin 0.0
Epoch 1, Class Loss=0.4426674544811249, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.4426674544811249, Class Loss=0.4426674544811249, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/13, Loss=5.890897363424301
Loss made of: CE 0.36942967772483826, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.007063865661621 EntMin 0.0
Epoch 2, Class Loss=0.3976181149482727, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.3976181149482727, Class Loss=0.3976181149482727, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/13, Loss=5.548334640264511
Loss made of: CE 0.30242419242858887, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.821792125701904 EntMin 0.0
Epoch 3, Class Loss=0.34079158306121826, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.34079158306121826, Class Loss=0.34079158306121826, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/13, Loss=5.215335559844971
Loss made of: CE 0.29789209365844727, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.320796012878418 EntMin 0.0
Epoch 4, Class Loss=0.2963545322418213, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.2963545322418213, Class Loss=0.2963545322418213, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/13, Loss=5.126892869174481
Loss made of: CE 0.3057653307914734, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.651476860046387 EntMin 0.0
Epoch 5, Class Loss=0.2743307650089264, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.2743307650089264, Class Loss=0.2743307650089264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/13, Loss=5.013881707191468
Loss made of: CE 0.2603035271167755, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.460453033447266 EntMin 0.0
Epoch 6, Class Loss=0.2524476945400238, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.2524476945400238, Class Loss=0.2524476945400238, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.417319655418396, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.893946
Mean Acc: 0.721247
FreqW Acc: 0.823359
Mean IoU: 0.568224
Class IoU:
	class 0: 0.89930326
	class 1: 0.8350513
	class 2: 0.2839266
	class 3: 0.84256434
	class 4: 0.6964384
	class 5: 0.7313718
	class 6: 0.91506445
	class 7: 0.82657355
	class 8: 0.78367174
	class 9: 0.11922558
	class 10: 0.0
	class 11: 0.00044386243
	class 12: 0.45327938
Class Acc:
	class 0: 0.94769394
	class 1: 0.9727768
	class 2: 0.96858376
	class 3: 0.9378818
	class 4: 0.87443954
	class 5: 0.9348104
	class 6: 0.94783896
	class 7: 0.9273821
	class 8: 0.95880914
	class 9: 0.1540908
	class 10: 0.0
	class 11: 0.0004438804
	class 12: 0.7514641

federated global round: 13, step: 2
select part of clients to conduct local training
[13, 5, 15, 7]
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/29, Loss=7.797099409997463
Loss made of: CE 0.14424143731594086, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.784867763519287 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.212075632810593
Loss made of: CE 0.100916787981987, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.356027126312256 EntMin 0.0
Epoch 1, Class Loss=0.14401133358478546, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.14401133358478546, Class Loss=0.14401133358478546, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/29, Loss=7.166976042836905
Loss made of: CE 0.13026171922683716, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.7196879386901855 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.581386644393206
Loss made of: CE 0.05632054805755615, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.438384056091309 EntMin 0.0
Epoch 2, Class Loss=0.11351682245731354, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.11351682245731354, Class Loss=0.11351682245731354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/29, Loss=6.519826831668615
Loss made of: CE 0.09452381730079651, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5188493728637695 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.001883948594331
Loss made of: CE 0.07403689622879028, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.193389415740967 EntMin 0.0
Epoch 3, Class Loss=0.09796025604009628, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.09796025604009628, Class Loss=0.09796025604009628, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/29, Loss=6.624815829098225
Loss made of: CE 0.08308170735836029, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.662342071533203 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.728106993809343
Loss made of: CE 0.05903271958231926, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.324980735778809 EntMin 0.0
Epoch 4, Class Loss=0.08830109983682632, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.08830109983682632, Class Loss=0.08830109983682632, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/29, Loss=6.462061674892903
Loss made of: CE 0.04808901995420456, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.368349075317383 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.4321448687464
Loss made of: CE 0.09533841907978058, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.687507152557373 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.08081085979938507, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.08081085979938507, Class Loss=0.08081085979938507, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/29, Loss=6.351516202837229
Loss made of: CE 0.08237209916114807, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.57758092880249 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.547713597118855
Loss made of: CE 0.07557174563407898, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3786163330078125 EntMin 0.0
Epoch 6, Class Loss=0.08222002536058426, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.08222002536058426, Class Loss=0.08222002536058426, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.232772366702557
Loss made of: CE 0.11992263793945312, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.048089027404785 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.301667349785566
Loss made of: CE 0.11599117517471313, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.081185817718506 EntMin 0.0
Epoch 1, Class Loss=0.13747386634349823, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.13747386634349823, Class Loss=0.13747386634349823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/29, Loss=7.310343787074089
Loss made of: CE 0.10548754781484604, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.983678340911865 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/29, Loss=6.871961114555598
Loss made of: CE 0.08673149347305298, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.872325897216797 EntMin 0.0
Epoch 2, Class Loss=0.10089461505413055, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.10089461505413055, Class Loss=0.10089461505413055, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/29, Loss=6.879881837219
Loss made of: CE 0.07804302871227264, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.815980911254883 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.519964861869812
Loss made of: CE 0.11984793841838837, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.392787933349609 EntMin 0.0
Epoch 3, Class Loss=0.08712662011384964, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.08712662011384964, Class Loss=0.08712662011384964, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/29, Loss=6.480365885794162
Loss made of: CE 0.07832050323486328, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.59930419921875 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.536244703084231
Loss made of: CE 0.13022497296333313, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.087187767028809 EntMin 0.0
Epoch 4, Class Loss=0.08033130317926407, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.08033130317926407, Class Loss=0.08033130317926407, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=6.321333246305585
Loss made of: CE 0.06270948052406311, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.011840343475342 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.465250641480088
Loss made of: CE 0.06787841767072678, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.939904689788818 EntMin 0.0
Epoch 5, Class Loss=0.07420940697193146, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.07420940697193146, Class Loss=0.07420940697193146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/29, Loss=6.359367752075196
Loss made of: CE 0.05352259799838066, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.843303680419922 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.332480156421662
Loss made of: CE 0.07776105403900146, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0209503173828125 EntMin 0.0
Epoch 6, Class Loss=0.07049717009067535, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.07049717009067535, Class Loss=0.07049717009067535, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.535783955454827
Loss made of: CE 0.3703185021877289, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.832602500915527 EntMin 0.0
Epoch 1, Class Loss=0.36515796184539795, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.36515796184539795, Class Loss=0.36515796184539795, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.819069826602936
Loss made of: CE 0.3306520879268646, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.643228054046631 EntMin 0.0
Epoch 2, Class Loss=0.31458213925361633, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.31458213925361633, Class Loss=0.31458213925361633, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.463055983185768
Loss made of: CE 0.222418874502182, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.52064323425293 EntMin 0.0
Epoch 3, Class Loss=0.27397584915161133, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.27397584915161133, Class Loss=0.27397584915161133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.075456036627292
Loss made of: CE 0.2516011893749237, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.100445747375488 EntMin 0.0
Epoch 4, Class Loss=0.23648259043693542, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.23648259043693542, Class Loss=0.23648259043693542, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.079009215533733
Loss made of: CE 0.20324227213859558, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.944211483001709 EntMin 0.0
Epoch 5, Class Loss=0.21396996080875397, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.21396996080875397, Class Loss=0.21396996080875397, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=4.787405051290989
Loss made of: CE 0.17823362350463867, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.455854415893555 EntMin 0.0
Epoch 6, Class Loss=0.2005966156721115, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.2005966156721115, Class Loss=0.2005966156721115, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.70923715531826
Loss made of: CE 0.38547760248184204, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.404984474182129 EntMin 0.0
Epoch 1, Class Loss=0.3955289423465729, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.3955289423465729, Class Loss=0.3955289423465729, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/13, Loss=5.838695868849754
Loss made of: CE 0.30376431345939636, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.124503135681152 EntMin 0.0
Epoch 2, Class Loss=0.3363046646118164, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.3363046646118164, Class Loss=0.3363046646118164, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.384067709743976
Loss made of: CE 0.22486482560634613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.634151935577393 EntMin 0.0
Epoch 3, Class Loss=0.2928297519683838, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.2928297519683838, Class Loss=0.2928297519683838, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/13, Loss=5.409498544037342
Loss made of: CE 0.2178814709186554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.839317321777344 EntMin 0.0
Epoch 4, Class Loss=0.257626473903656, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.257626473903656, Class Loss=0.257626473903656, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/13, Loss=5.294677171111107
Loss made of: CE 0.3130585551261902, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.896467208862305 EntMin 0.0
Epoch 5, Class Loss=0.24200114607810974, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.24200114607810974, Class Loss=0.24200114607810974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/13, Loss=5.161511304974556
Loss made of: CE 0.25132834911346436, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.583250999450684 EntMin 0.0
Epoch 6, Class Loss=0.21907556056976318, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.21907556056976318, Class Loss=0.21907556056976318, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4180610477924347, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.893098
Mean Acc: 0.727183
FreqW Acc: 0.824182
Mean IoU: 0.571211
Class IoU:
	class 0: 0.8991821
	class 1: 0.83993244
	class 2: 0.28460345
	class 3: 0.83884656
	class 4: 0.6997592
	class 5: 0.7217132
	class 6: 0.91568184
	class 7: 0.82370305
	class 8: 0.7979423
	class 9: 0.10381424
	class 10: 0.0
	class 11: 0.05382579
	class 12: 0.4467421
Class Acc:
	class 0: 0.94288325
	class 1: 0.96982557
	class 2: 0.9702201
	class 3: 0.9297893
	class 4: 0.8650702
	class 5: 0.9455677
	class 6: 0.9481056
	class 7: 0.93330866
	class 8: 0.9455405
	class 9: 0.123688474
	class 10: 0.0
	class 11: 0.054890074
	class 12: 0.82449317

federated global round: 14, step: 2
select part of clients to conduct local training
[17, 3, 12, 16]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.3225111842155455
Loss made of: CE 0.10266655683517456, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.994122505187988 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.405314610898495
Loss made of: CE 0.10508351773023605, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.982345104217529 EntMin 0.0
Epoch 1, Class Loss=0.11963997036218643, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.11963997036218643, Class Loss=0.11963997036218643, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=6.887016209587455
Loss made of: CE 0.08335023373365402, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.852850914001465 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.603063746541738
Loss made of: CE 0.07797258347272873, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.506409645080566 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.08885917067527771, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.08885917067527771, Class Loss=0.08885917067527771, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=6.251450181379914
Loss made of: CE 0.07384547591209412, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.733888149261475 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.670724173262715
Loss made of: CE 0.07260330021381378, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.072579383850098 EntMin 0.0
Epoch 3, Class Loss=0.07740683108568192, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.07740683108568192, Class Loss=0.07740683108568192, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=6.297667784616351
Loss made of: CE 0.07448714971542358, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3342485427856445 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.375684454292059
Loss made of: CE 0.0685494989156723, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.34581184387207 EntMin 0.0
Epoch 4, Class Loss=0.07275780290365219, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.07275780290365219, Class Loss=0.07275780290365219, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.373375874385237
Loss made of: CE 0.06760119646787643, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.641856670379639 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.185621591284871
Loss made of: CE 0.0600396990776062, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.740694999694824 EntMin 0.0
Epoch 5, Class Loss=0.07330361753702164, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.07330361753702164, Class Loss=0.07330361753702164, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.330920860171318
Loss made of: CE 0.08795492351055145, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.147281169891357 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.12823602706194
Loss made of: CE 0.054908715188503265, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.373202800750732 EntMin 0.0
Epoch 6, Class Loss=0.07301278412342072, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.07301278412342072, Class Loss=0.07301278412342072, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=7.612175120413303
Loss made of: CE 0.1471814215183258, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.573888778686523 EntMin 0.0
Epoch 1, Batch 20/29, Loss=6.851251232624054
Loss made of: CE 0.08323195576667786, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.45331335067749 EntMin 0.0
Epoch 1, Class Loss=0.117861308157444, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.117861308157444, Class Loss=0.117861308157444, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=6.668589781969786
Loss made of: CE 0.09061466157436371, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.299615383148193 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.664684776961804
Loss made of: CE 0.16684085130691528, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.847923278808594 EntMin 0.0
Epoch 2, Class Loss=0.0880730003118515, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.0880730003118515, Class Loss=0.0880730003118515, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=6.544963235780597
Loss made of: CE 0.07610025256872177, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9039626121521 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.678694184869528
Loss made of: CE 0.10673888772726059, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.742408752441406 EntMin 0.0
Epoch 3, Class Loss=0.08698846399784088, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.08698846399784088, Class Loss=0.08698846399784088, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=6.692544198781252
Loss made of: CE 0.08824203908443451, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.173364162445068 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.250183223187923
Loss made of: CE 0.09472235292196274, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.976428985595703 EntMin 0.0
Epoch 4, Class Loss=0.07938741892576218, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.07938741892576218, Class Loss=0.07938741892576218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.27571008913219
Loss made of: CE 0.07376828789710999, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4239020347595215 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.363831223174929
Loss made of: CE 0.08103294670581818, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.764025688171387 EntMin 0.0
Epoch 5, Class Loss=0.07091458886861801, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.07091458886861801, Class Loss=0.07091458886861801, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.219598063826561
Loss made of: CE 0.0665125846862793, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.914057731628418 EntMin 0.0
Epoch 6, Batch 20/29, Loss=5.965554403141141
Loss made of: CE 0.09680308401584625, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.519669532775879 EntMin 0.0
Epoch 6, Class Loss=0.07127638161182404, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.07127638161182404, Class Loss=0.07127638161182404, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.446581938862801
Loss made of: CE 0.2730458378791809, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.162967205047607 EntMin 0.0
Epoch 1, Class Loss=0.3456895351409912, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.3456895351409912, Class Loss=0.3456895351409912, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.671656890213489
Loss made of: CE 0.3021925687789917, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.06581449508667 EntMin 0.0
Epoch 2, Class Loss=0.28820890188217163, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.28820890188217163, Class Loss=0.28820890188217163, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/13, Loss=5.416975055634976
Loss made of: CE 0.2398819923400879, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.939340591430664 EntMin 0.0
Epoch 3, Class Loss=0.2598835825920105, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.2598835825920105, Class Loss=0.2598835825920105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.1608647257089615
Loss made of: CE 0.2025650441646576, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.994491100311279 EntMin 0.0
Epoch 4, Class Loss=0.23540471494197845, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.23540471494197845, Class Loss=0.23540471494197845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.076853503286839
Loss made of: CE 0.3152402341365814, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.89795446395874 EntMin 0.0
Epoch 5, Class Loss=0.2267628014087677, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.2267628014087677, Class Loss=0.2267628014087677, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.074568843841552
Loss made of: CE 0.21142444014549255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734439373016357 EntMin 0.0
Epoch 6, Class Loss=0.21446996927261353, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.21446996927261353, Class Loss=0.21446996927261353, Reg Loss=0.0
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/29, Loss=7.481132527440787
Loss made of: CE 0.11677368730306625, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.301846504211426 EntMin 0.0
Epoch 1, Batch 20/29, Loss=6.866876815259457
Loss made of: CE 0.0747537761926651, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.367914199829102 EntMin 0.0
Epoch 1, Class Loss=0.12156598269939423, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.12156598269939423, Class Loss=0.12156598269939423, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/29, Loss=6.524595103785396
Loss made of: CE 0.08039762824773788, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.270047187805176 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.753302058577537
Loss made of: CE 0.1010739654302597, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0086989402771 EntMin 0.0
Epoch 2, Class Loss=0.09335825592279434, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.09335825592279434, Class Loss=0.09335825592279434, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/29, Loss=6.582463849335909
Loss made of: CE 0.14984485507011414, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.888463497161865 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.345650742202997
Loss made of: CE 0.10648246854543686, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.817636013031006 EntMin 0.0
Epoch 3, Class Loss=0.09136295318603516, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.09136295318603516, Class Loss=0.09136295318603516, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/29, Loss=6.354189598560334
Loss made of: CE 0.07834184169769287, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.898826599121094 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.383047844469547
Loss made of: CE 0.06643655151128769, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.476363658905029 EntMin 0.0
Epoch 4, Class Loss=0.07919397950172424, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.07919397950172424, Class Loss=0.07919397950172424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/29, Loss=5.970967924594879
Loss made of: CE 0.07023917138576508, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.995920181274414 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.41735900901258
Loss made of: CE 0.11709927767515182, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.542713165283203 EntMin 0.0
Epoch 5, Class Loss=0.08235350996255875, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.08235350996255875, Class Loss=0.08235350996255875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 10/29, Loss=6.060802849754691
Loss made of: CE 0.09386634081602097, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.507010459899902 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.220266636833548
Loss made of: CE 0.06720437109470367, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.443936824798584 EntMin 0.0
Epoch 6, Class Loss=0.08035825192928314, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.08035825192928314, Class Loss=0.08035825192928314, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4158492386341095, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.887005
Mean Acc: 0.711317
FreqW Acc: 0.820170
Mean IoU: 0.560411
Class IoU:
	class 0: 0.89819795
	class 1: 0.84604895
	class 2: 0.29719317
	class 3: 0.7990745
	class 4: 0.6979204
	class 5: 0.7409744
	class 6: 0.9094419
	class 7: 0.8399857
	class 8: 0.7963193
	class 9: 0.05426332
	class 10: 0.0
	class 11: 0.006792137
	class 12: 0.3991304
Class Acc:
	class 0: 0.9390803
	class 1: 0.9679198
	class 2: 0.9708436
	class 3: 0.84480417
	class 4: 0.8770407
	class 5: 0.9322596
	class 6: 0.9340224
	class 7: 0.92984235
	class 8: 0.8598421
	class 9: 0.057879444
	class 10: 0.0
	class 11: 0.00679896
	class 12: 0.9267923

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[13, 9, 6, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=9.568484485149384
Loss made of: CE 0.6824063062667847, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.954973220825195 EntMin 0.0
Epoch 1, Class Loss=0.798132598400116, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.798132598400116, Class Loss=0.798132598400116, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=8.068318670988083
Loss made of: CE 0.605930745601654, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.058755874633789 EntMin 0.0
Epoch 2, Class Loss=0.608342170715332, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.608342170715332, Class Loss=0.608342170715332, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.436077362298965
Loss made of: CE 0.46319669485092163, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.099050045013428 EntMin 0.0
Epoch 3, Class Loss=0.47194698452949524, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.47194698452949524, Class Loss=0.47194698452949524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/11, Loss=6.949298968911171
Loss made of: CE 0.2952684164047241, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5528693199157715 EntMin 0.0
Epoch 4, Class Loss=0.3698177933692932, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.3698177933692932, Class Loss=0.3698177933692932, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/11, Loss=6.697563183307648
Loss made of: CE 0.3337981104850769, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.485170364379883 EntMin 0.0
Epoch 5, Class Loss=0.33624720573425293, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.33624720573425293, Class Loss=0.33624720573425293, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.5019516259431835
Loss made of: CE 0.2773234248161316, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.108832836151123 EntMin 0.0
Epoch 6, Class Loss=0.28586313128471375, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.28586313128471375, Class Loss=0.28586313128471375, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=12.052099084854126
Loss made of: CE 0.5953615307807922, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.112181663513184 EntMin 0.0
Epoch 1, Class Loss=0.6492452025413513, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.6492452025413513, Class Loss=0.6492452025413513, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=10.057497915625571
Loss made of: CE 0.6104333400726318, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.82303524017334 EntMin 0.0
Epoch 2, Class Loss=0.5118238925933838, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.5118238925933838, Class Loss=0.5118238925933838, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=8.988711711764335
Loss made of: CE 0.43093228340148926, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.420503616333008 EntMin 0.0
Epoch 3, Class Loss=0.3991166949272156, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.3991166949272156, Class Loss=0.3991166949272156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.340297058224678
Loss made of: CE 0.21552178263664246, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.080041885375977 EntMin 0.0
Epoch 4, Class Loss=0.3066267967224121, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.3066267967224121, Class Loss=0.3066267967224121, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.23458806425333
Loss made of: CE 0.28483957052230835, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.265869140625 EntMin 0.0
Epoch 5, Class Loss=0.2732844948768616, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.2732844948768616, Class Loss=0.2732844948768616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=7.876456378400325
Loss made of: CE 0.24495065212249756, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.463967323303223 EntMin 0.0
Epoch 6, Class Loss=0.24510625004768372, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.24510625004768372, Class Loss=0.24510625004768372, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=9.71972029209137
Loss made of: CE 0.7094239592552185, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.068151473999023 EntMin 0.0
Epoch 1, Class Loss=0.8088831901550293, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.8088831901550293, Class Loss=0.8088831901550293, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=8.260538065433503
Loss made of: CE 0.5227810144424438, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.418337821960449 EntMin 0.0
Epoch 2, Class Loss=0.620549201965332, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.620549201965332, Class Loss=0.620549201965332, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.476773726940155
Loss made of: CE 0.4280635118484497, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.085569381713867 EntMin 0.0
Epoch 3, Class Loss=0.4731820523738861, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4731820523738861, Class Loss=0.4731820523738861, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/11, Loss=7.040552291274071
Loss made of: CE 0.3797769546508789, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.303377151489258 EntMin 0.0
Epoch 4, Class Loss=0.3854174315929413, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3854174315929413, Class Loss=0.3854174315929413, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/11, Loss=6.743952484428883
Loss made of: CE 0.3574237823486328, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.703996658325195 EntMin 0.0
Epoch 5, Class Loss=0.32645708322525024, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.32645708322525024, Class Loss=0.32645708322525024, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.682287827134132
Loss made of: CE 0.2769787311553955, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.728442668914795 EntMin 0.0
Epoch 6, Class Loss=0.2957914173603058, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.2957914173603058, Class Loss=0.2957914173603058, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=11.895520943403245
Loss made of: CE 0.6023409366607666, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.770606994628906 EntMin 0.0
Epoch 1, Class Loss=0.6319236755371094, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.6319236755371094, Class Loss=0.6319236755371094, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=9.895134347677232
Loss made of: CE 0.5170769095420837, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.323765754699707 EntMin 0.0
Epoch 2, Class Loss=0.506017804145813, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.506017804145813, Class Loss=0.506017804145813, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=8.927501559257507
Loss made of: CE 0.3705638349056244, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.46362590789795 EntMin 0.0
Epoch 3, Class Loss=0.3863344192504883, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.3863344192504883, Class Loss=0.3863344192504883, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.476715117692947
Loss made of: CE 0.30812323093414307, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.814228057861328 EntMin 0.0
Epoch 4, Class Loss=0.3098033666610718, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.3098033666610718, Class Loss=0.3098033666610718, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.18308675289154
Loss made of: CE 0.2889832854270935, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.914857864379883 EntMin 0.0
Epoch 5, Class Loss=0.2686271369457245, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.2686271369457245, Class Loss=0.2686271369457245, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=7.743178541958332
Loss made of: CE 0.19083085656166077, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.439537048339844 EntMin 0.0
Epoch 6, Class Loss=0.23495697975158691, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.23495697975158691, Class Loss=0.23495697975158691, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5858873128890991, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.860008
Mean Acc: 0.533881
FreqW Acc: 0.762683
Mean IoU: 0.434522
Class IoU:
	class 0: 0.87216306
	class 1: 0.79213166
	class 2: 0.2948462
	class 3: 0.7364641
	class 4: 0.62239385
	class 5: 0.7201267
	class 6: 0.6083484
	class 7: 0.78825307
	class 8: 0.74283683
	class 9: 0.020777242
	class 10: 0.0
	class 11: 0.01003445
	class 12: 0.30944896
	class 13: 0.0
	class 14: 0.0
Class Acc:
	class 0: 0.97336113
	class 1: 0.8693608
	class 2: 0.8666017
	class 3: 0.79714966
	class 4: 0.7679711
	class 5: 0.801712
	class 6: 0.6131494
	class 7: 0.8551366
	class 8: 0.8493023
	class 9: 0.023162257
	class 10: 0.0
	class 11: 0.010113376
	class 12: 0.5812003
	class 13: 0.0
	class 14: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[18, 5, 20, 0]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.830063983798027
Loss made of: CE 0.2748241424560547, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.7246623039245605 EntMin 0.0
Epoch 1, Class Loss=0.3538592755794525, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.3538592755794525, Class Loss=0.3538592755794525, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=8.221241803467274
Loss made of: CE 0.4113787114620209, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.977806091308594 EntMin 0.0
Epoch 2, Class Loss=0.3051736354827881, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.3051736354827881, Class Loss=0.3051736354827881, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.730927544832229
Loss made of: CE 0.2707400918006897, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.412820339202881 EntMin 0.0
Epoch 3, Class Loss=0.25601381063461304, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.25601381063461304, Class Loss=0.25601381063461304, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.4808082863688465
Loss made of: CE 0.19808655977249146, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.712185859680176 EntMin 0.0
Epoch 4, Class Loss=0.2227591872215271, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.2227591872215271, Class Loss=0.2227591872215271, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.276086424291134
Loss made of: CE 0.18335458636283875, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.132905006408691 EntMin 0.0
Epoch 5, Class Loss=0.18216636776924133, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.18216636776924133, Class Loss=0.18216636776924133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.111317712068558
Loss made of: CE 0.20656006038188934, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.750414848327637 EntMin 0.0
Epoch 6, Class Loss=0.17335741221904755, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.17335741221904755, Class Loss=0.17335741221904755, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/12, Loss=8.72732533812523
Loss made of: CE 0.35431337356567383, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.882368087768555 EntMin 0.0
Epoch 1, Class Loss=0.354367196559906, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.354367196559906, Class Loss=0.354367196559906, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=8.111236211657523
Loss made of: CE 0.29334723949432373, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.155157089233398 EntMin 0.0
Epoch 2, Class Loss=0.3201247453689575, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3201247453689575, Class Loss=0.3201247453689575, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=7.891096863150596
Loss made of: CE 0.26825448870658875, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.881883144378662 EntMin 0.0
Epoch 3, Class Loss=0.2737816572189331, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.2737816572189331, Class Loss=0.2737816572189331, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/12, Loss=7.6047895267605785
Loss made of: CE 0.21841411292552948, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.256092071533203 EntMin 0.0
Epoch 4, Class Loss=0.23059974610805511, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.23059974610805511, Class Loss=0.23059974610805511, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.506306622922421
Loss made of: CE 0.21703314781188965, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.562588691711426 EntMin 0.0
Epoch 5, Class Loss=0.21200554072856903, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.21200554072856903, Class Loss=0.21200554072856903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=7.268145260214806
Loss made of: CE 0.1774195283651352, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.914800643920898 EntMin 0.0
Epoch 6, Class Loss=0.19315753877162933, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.19315753877162933, Class Loss=0.19315753877162933, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.738131895661354
Loss made of: CE 0.33833086490631104, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.650486946105957 EntMin 0.0
Epoch 1, Class Loss=0.3680782914161682, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.3680782914161682, Class Loss=0.3680782914161682, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=8.108535501360894
Loss made of: CE 0.2622198462486267, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.446016311645508 EntMin 0.0
Epoch 2, Class Loss=0.2978365421295166, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.2978365421295166, Class Loss=0.2978365421295166, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.899797885119915
Loss made of: CE 0.185387521982193, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.922143936157227 EntMin 0.0
Epoch 3, Class Loss=0.2507857084274292, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.2507857084274292, Class Loss=0.2507857084274292, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.542283166944981
Loss made of: CE 0.19741904735565186, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.284577369689941 EntMin 0.0
Epoch 4, Class Loss=0.21558037400245667, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.21558037400245667, Class Loss=0.21558037400245667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.346592646837235
Loss made of: CE 0.14195039868354797, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.313506126403809 EntMin 0.0
Epoch 5, Class Loss=0.19770382344722748, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.19770382344722748, Class Loss=0.19770382344722748, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.056022402644158
Loss made of: CE 0.15648505091667175, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.082298278808594 EntMin 0.0
Epoch 6, Class Loss=0.1668793261051178, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.1668793261051178, Class Loss=0.1668793261051178, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=7.70476036965847
Loss made of: CE 0.48239514231681824, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.997801780700684 EntMin 0.0
Epoch 1, Class Loss=0.446992963552475, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.446992963552475, Class Loss=0.446992963552475, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/11, Loss=6.973932033777237
Loss made of: CE 0.36154526472091675, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.957634925842285 EntMin 0.0
Epoch 2, Class Loss=0.39479994773864746, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.39479994773864746, Class Loss=0.39479994773864746, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/11, Loss=6.629011625051499
Loss made of: CE 0.3422738313674927, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5235443115234375 EntMin 0.0
Epoch 3, Class Loss=0.3393813967704773, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3393813967704773, Class Loss=0.3393813967704773, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/11, Loss=6.538640180230141
Loss made of: CE 0.2558562755584717, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.873101234436035 EntMin 0.0
Epoch 4, Class Loss=0.2872437536716461, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.2872437536716461, Class Loss=0.2872437536716461, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/11, Loss=6.315047387778759
Loss made of: CE 0.25251081585884094, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4893798828125 EntMin 0.0
Epoch 5, Class Loss=0.2567136585712433, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.2567136585712433, Class Loss=0.2567136585712433, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/11, Loss=6.146367116272449
Loss made of: CE 0.2514844536781311, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.786649227142334 EntMin 0.0
Epoch 6, Class Loss=0.22651705145835876, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.22651705145835876, Class Loss=0.22651705145835876, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5295500159263611, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.874352
Mean Acc: 0.587811
FreqW Acc: 0.782308
Mean IoU: 0.488877
Class IoU:
	class 0: 0.8791387
	class 1: 0.8137978
	class 2: 0.3319776
	class 3: 0.7584236
	class 4: 0.64455026
	class 5: 0.72005177
	class 6: 0.74643505
	class 7: 0.8100434
	class 8: 0.74451226
	class 9: 0.018024301
	class 10: 0.0
	class 11: 0.0075977477
	class 12: 0.3184616
	class 13: 0.0
	class 14: 0.5401377
Class Acc:
	class 0: 0.9739286
	class 1: 0.898802
	class 2: 0.83177745
	class 3: 0.8095623
	class 4: 0.7984818
	class 5: 0.8078441
	class 6: 0.75650674
	class 7: 0.88549125
	class 8: 0.8591235
	class 9: 0.019146672
	class 10: 0.0
	class 11: 0.007632205
	class 12: 0.5006707
	class 13: 0.0
	class 14: 0.6681933

federated global round: 17, step: 3
select part of clients to conduct local training
[1, 16, 6, 21]
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.383417201042175
Loss made of: CE 0.460549533367157, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.360541820526123 EntMin 0.0
Epoch 1, Class Loss=0.4422590732574463, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.4422590732574463, Class Loss=0.4422590732574463, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=6.923816931247711
Loss made of: CE 0.332740843296051, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.598407745361328 EntMin 0.0
Epoch 2, Class Loss=0.3881129026412964, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.3881129026412964, Class Loss=0.3881129026412964, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.54644515812397
Loss made of: CE 0.2897643446922302, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.539090156555176 EntMin 0.0
Epoch 3, Class Loss=0.30761685967445374, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.30761685967445374, Class Loss=0.30761685967445374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.291300937533379
Loss made of: CE 0.22454571723937988, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.709466457366943 EntMin 0.0
Epoch 4, Class Loss=0.2695517838001251, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.2695517838001251, Class Loss=0.2695517838001251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.036506830155849
Loss made of: CE 0.23864823579788208, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.437355041503906 EntMin 0.0
Epoch 5, Class Loss=0.21871218085289001, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.21871218085289001, Class Loss=0.21871218085289001, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=5.938300447165966
Loss made of: CE 0.2049209475517273, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.113372325897217 EntMin 0.0
Epoch 6, Class Loss=0.2035760134458542, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.2035760134458542, Class Loss=0.2035760134458542, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.237981528043747
Loss made of: CE 0.43386250734329224, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.572549343109131 EntMin 0.0
Epoch 1, Class Loss=0.44655370712280273, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.44655370712280273, Class Loss=0.44655370712280273, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=6.91157155930996
Loss made of: CE 0.3675408959388733, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.89772367477417 EntMin 0.0
Epoch 2, Class Loss=0.3832066059112549, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.3832066059112549, Class Loss=0.3832066059112549, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.575592184066773
Loss made of: CE 0.25435441732406616, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.831218242645264 EntMin 0.0
Epoch 3, Class Loss=0.30685296654701233, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.30685296654701233, Class Loss=0.30685296654701233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.39059938788414
Loss made of: CE 0.25168249011039734, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5671796798706055 EntMin 0.0
Epoch 4, Class Loss=0.2553531229496002, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.2553531229496002, Class Loss=0.2553531229496002, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.144379878044129
Loss made of: CE 0.16242274641990662, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.675339698791504 EntMin 0.0
Epoch 5, Class Loss=0.21205441653728485, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.21205441653728485, Class Loss=0.21205441653728485, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=5.87006368637085
Loss made of: CE 0.2230830192565918, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.293449401855469 EntMin 0.0
Epoch 6, Class Loss=0.19628553092479706, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.19628553092479706, Class Loss=0.19628553092479706, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.496400627493859
Loss made of: CE 0.46077021956443787, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.029452800750732 EntMin 0.0
Epoch 1, Class Loss=0.4459109902381897, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.4459109902381897, Class Loss=0.4459109902381897, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/11, Loss=7.015198269486428
Loss made of: CE 0.3838689923286438, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.769551753997803 EntMin 0.0
Epoch 2, Class Loss=0.39495429396629333, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.39495429396629333, Class Loss=0.39495429396629333, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/11, Loss=6.635072705149651
Loss made of: CE 0.3237036466598511, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4932475090026855 EntMin 0.0
Epoch 3, Class Loss=0.3311516046524048, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.3311516046524048, Class Loss=0.3311516046524048, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/11, Loss=6.345393466949463
Loss made of: CE 0.30986830592155457, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.879558086395264 EntMin 0.0
Epoch 4, Class Loss=0.28230878710746765, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.28230878710746765, Class Loss=0.28230878710746765, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/11, Loss=6.219549068808556
Loss made of: CE 0.25749891996383667, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.201289653778076 EntMin 0.0
Epoch 5, Class Loss=0.24874742329120636, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.24874742329120636, Class Loss=0.24874742329120636, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/11, Loss=6.062211211025715
Loss made of: CE 0.20288658142089844, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3509840965271 EntMin 0.0
Epoch 6, Class Loss=0.2235763967037201, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.2235763967037201, Class Loss=0.2235763967037201, Reg Loss=0.0
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=7.447187760472298
Loss made of: CE 0.2272566258907318, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.421352386474609 EntMin 0.0
Epoch 1, Class Loss=0.21553638577461243, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.21553638577461243, Class Loss=0.21553638577461243, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=7.1779173642396925
Loss made of: CE 0.2024986445903778, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.351319789886475 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.1889467090368271, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.1889467090368271, Class Loss=0.1889467090368271, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=7.172153291106224
Loss made of: CE 0.14717735350131989, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.790828227996826 EntMin 0.0
Epoch 3, Class Loss=0.17022022604942322, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.17022022604942322, Class Loss=0.17022022604942322, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=7.052310436218977
Loss made of: CE 0.17075052857398987, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.238093376159668 EntMin 0.0
Epoch 4, Class Loss=0.15164446830749512, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.15164446830749512, Class Loss=0.15164446830749512, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=6.711529347300529
Loss made of: CE 0.1484546959400177, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.747220993041992 EntMin 0.0
Epoch 5, Class Loss=0.1396317481994629, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.1396317481994629, Class Loss=0.1396317481994629, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=6.888133309781551
Loss made of: CE 0.14253921806812286, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.444853782653809 EntMin 0.0
Epoch 6, Class Loss=0.12516909837722778, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.12516909837722778, Class Loss=0.12516909837722778, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5137801170349121, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.868820
Mean Acc: 0.586809
FreqW Acc: 0.785256
Mean IoU: 0.477591
Class IoU:
	class 0: 0.8861837
	class 1: 0.8317145
	class 2: 0.31098714
	class 3: 0.7707219
	class 4: 0.6375554
	class 5: 0.72757703
	class 6: 0.74287695
	class 7: 0.81639916
	class 8: 0.73103327
	class 9: 0.046294674
	class 10: 0.0
	class 11: 0.012718249
	class 12: 0.35944587
	class 13: 0.18720843
	class 14: 0.10314979
Class Acc:
	class 0: 0.96933156
	class 1: 0.9028189
	class 2: 0.91497564
	class 3: 0.818144
	class 4: 0.7847422
	class 5: 0.82800406
	class 6: 0.7520724
	class 7: 0.90052646
	class 8: 0.8092958
	class 9: 0.052325387
	class 10: 0.0
	class 11: 0.012873941
	class 12: 0.48988008
	class 13: 0.46330288
	class 14: 0.103835665

federated global round: 18, step: 3
select part of clients to conduct local training
[8, 2, 17, 14]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=6.05724310874939
Loss made of: CE 0.26801222562789917, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5445966720581055 EntMin 0.0
Epoch 1, Class Loss=0.23779910802841187, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.23779910802841187, Class Loss=0.23779910802841187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/11, Loss=5.752952350676059
Loss made of: CE 0.18270283937454224, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.134649753570557 EntMin 0.0
Epoch 2, Class Loss=0.21733276546001434, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.21733276546001434, Class Loss=0.21733276546001434, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.741840921342373
Loss made of: CE 0.16622444987297058, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.538036346435547 EntMin 0.0
Epoch 3, Class Loss=0.1974519044160843, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.1974519044160843, Class Loss=0.1974519044160843, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.525004100799561
Loss made of: CE 0.16122934222221375, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.200774192810059 EntMin 0.0
Epoch 4, Class Loss=0.17631106078624725, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.17631106078624725, Class Loss=0.17631106078624725, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.51070354282856
Loss made of: CE 0.13494600355625153, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.708867073059082 EntMin 0.0
Epoch 5, Class Loss=0.16544273495674133, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.16544273495674133, Class Loss=0.16544273495674133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.374775430560112
Loss made of: CE 0.1367463320493698, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.699607849121094 EntMin 0.0
Epoch 6, Class Loss=0.1537274569272995, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.1537274569272995, Class Loss=0.1537274569272995, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=6.359353183209896
Loss made of: CE 0.20241257548332214, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.636025905609131 EntMin 0.0
Epoch 1, Class Loss=0.24875995516777039, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.24875995516777039, Class Loss=0.24875995516777039, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=5.976867273449898
Loss made of: CE 0.24262361228466034, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.659177780151367 EntMin 0.0
Epoch 2, Class Loss=0.2334715723991394, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.2334715723991394, Class Loss=0.2334715723991394, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.856197924911976
Loss made of: CE 0.18065544962882996, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.060301303863525 EntMin 0.0
Epoch 3, Class Loss=0.19480738043785095, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.19480738043785095, Class Loss=0.19480738043785095, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.79142357558012
Loss made of: CE 0.1814272105693817, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.514123439788818 EntMin 0.0
Epoch 4, Class Loss=0.18317946791648865, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.18317946791648865, Class Loss=0.18317946791648865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.65203959196806
Loss made of: CE 0.17405472695827484, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.577138900756836 EntMin 0.0
Epoch 5, Class Loss=0.17150135338306427, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.17150135338306427, Class Loss=0.17150135338306427, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.524241200089454
Loss made of: CE 0.16511087119579315, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.934499740600586 EntMin 0.0
Epoch 6, Class Loss=0.1590232402086258, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.1590232402086258, Class Loss=0.1590232402086258, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.766900685429572
Loss made of: CE 0.20807836949825287, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.472657203674316 EntMin 0.0
Epoch 1, Class Loss=0.28213605284690857, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.28213605284690857, Class Loss=0.28213605284690857, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=7.6136557281017305
Loss made of: CE 0.17733709514141083, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.423938274383545 EntMin 0.0
Epoch 2, Class Loss=0.2274709939956665, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.2274709939956665, Class Loss=0.2274709939956665, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=7.29472071826458
Loss made of: CE 0.16936686635017395, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.644046783447266 EntMin 0.0
Epoch 3, Class Loss=0.17416052520275116, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.17416052520275116, Class Loss=0.17416052520275116, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=7.201154211163521
Loss made of: CE 0.195527121424675, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.743442535400391 EntMin 0.0
Epoch 4, Class Loss=0.16633974015712738, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.16633974015712738, Class Loss=0.16633974015712738, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=6.783241303265095
Loss made of: CE 0.14158837497234344, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.406712532043457 EntMin 0.0
Epoch 5, Class Loss=0.13774549961090088, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.13774549961090088, Class Loss=0.13774549961090088, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=6.885737822949887
Loss made of: CE 0.12912991642951965, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.578492641448975 EntMin 0.0
Epoch 6, Class Loss=0.1317405253648758, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.1317405253648758, Class Loss=0.1317405253648758, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=6.343330453336239
Loss made of: CE 0.2341429889202118, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.196139335632324 EntMin 0.0
Epoch 1, Class Loss=0.24283425509929657, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.24283425509929657, Class Loss=0.24283425509929657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=6.078103491663933
Loss made of: CE 0.22965936362743378, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9144816398620605 EntMin 0.0
Epoch 2, Class Loss=0.22212432324886322, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.22212432324886322, Class Loss=0.22212432324886322, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.966376741230488
Loss made of: CE 0.17399567365646362, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.742892742156982 EntMin 0.0
Epoch 3, Class Loss=0.18710708618164062, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.18710708618164062, Class Loss=0.18710708618164062, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.833778654038906
Loss made of: CE 0.15407751500606537, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.243529319763184 EntMin 0.0
Epoch 4, Class Loss=0.17800140380859375, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.17800140380859375, Class Loss=0.17800140380859375, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.62726164534688
Loss made of: CE 0.14597973227500916, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.06417989730835 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.16140203177928925, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.16140203177928925, Class Loss=0.16140203177928925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.683871123194694
Loss made of: CE 0.1494341790676117, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.205944061279297 EntMin 0.0
Epoch 6, Class Loss=0.15831488370895386, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.15831488370895386, Class Loss=0.15831488370895386, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.51353520154953, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.856773
Mean Acc: 0.583936
FreqW Acc: 0.781374
Mean IoU: 0.463010
Class IoU:
	class 0: 0.8882477
	class 1: 0.8463409
	class 2: 0.3194712
	class 3: 0.74637574
	class 4: 0.6459562
	class 5: 0.7410768
	class 6: 0.76513
	class 7: 0.8209088
	class 8: 0.72976655
	class 9: 0.025364636
	class 10: 0.0
	class 11: 0.0023068115
	class 12: 0.24010974
	class 13: 0.17409666
	class 14: 0.0
Class Acc:
	class 0: 0.95828635
	class 1: 0.91486436
	class 2: 0.9191548
	class 3: 0.78488785
	class 4: 0.78368485
	class 5: 0.8412829
	class 6: 0.77477425
	class 7: 0.91169316
	class 8: 0.80243427
	class 9: 0.026678625
	class 10: 0.0
	class 11: 0.0023103398
	class 12: 0.26226857
	class 13: 0.7767242
	class 14: 0.0

federated global round: 19, step: 3
select part of clients to conduct local training
[1, 9, 8, 0]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=5.878721864521504
Loss made of: CE 0.18316245079040527, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.500661373138428 EntMin 0.0
Epoch 1, Class Loss=0.19001907110214233, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.19001907110214233, Class Loss=0.19001907110214233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/11, Loss=5.585918755829335
Loss made of: CE 0.182388573884964, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.576809883117676 EntMin 0.0
Epoch 2, Class Loss=0.19042909145355225, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.19042909145355225, Class Loss=0.19042909145355225, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/11, Loss=5.5327122375369076
Loss made of: CE 0.17440980672836304, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760867595672607 EntMin 0.0
Epoch 3, Class Loss=0.1755368560552597, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.1755368560552597, Class Loss=0.1755368560552597, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/11, Loss=5.520438396930695
Loss made of: CE 0.14471805095672607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.949926853179932 EntMin 0.0
Epoch 4, Class Loss=0.17711541056632996, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.17711541056632996, Class Loss=0.17711541056632996, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/11, Loss=5.541653047502041
Loss made of: CE 0.1833006739616394, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9736785888671875 EntMin 0.0
Epoch 5, Class Loss=0.16470180451869965, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.16470180451869965, Class Loss=0.16470180451869965, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/11, Loss=5.369822503626347
Loss made of: CE 0.1557522714138031, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.652257919311523 EntMin 0.0
Epoch 6, Class Loss=0.16101469099521637, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.16101469099521637, Class Loss=0.16101469099521637, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.594944223761559
Loss made of: CE 0.3384571969509125, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.673575401306152 EntMin 0.0
Epoch 1, Class Loss=0.3044274151325226, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.3044274151325226, Class Loss=0.3044274151325226, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/12, Loss=7.530809345841408
Loss made of: CE 0.2893941104412079, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.994291305541992 EntMin 0.0
Epoch 2, Class Loss=0.2567591071128845, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.2567591071128845, Class Loss=0.2567591071128845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/12, Loss=7.133621050417423
Loss made of: CE 0.21839332580566406, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.067497253417969 EntMin 0.0
Epoch 3, Class Loss=0.21100002527236938, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.21100002527236938, Class Loss=0.21100002527236938, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/12, Loss=6.844569718837738
Loss made of: CE 0.13115763664245605, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.792901039123535 EntMin 0.0
Epoch 4, Class Loss=0.19022899866104126, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.19022899866104126, Class Loss=0.19022899866104126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/12, Loss=7.109564720094204
Loss made of: CE 0.24356302618980408, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.794882774353027 EntMin 0.0
Epoch 5, Class Loss=0.16921842098236084, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.16921842098236084, Class Loss=0.16921842098236084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/12, Loss=6.945086985081434
Loss made of: CE 0.1342882215976715, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.828629970550537 EntMin 0.0
Epoch 6, Class Loss=0.1713404655456543, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.1713404655456543, Class Loss=0.1713404655456543, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/11, Loss=5.6557873204350475
Loss made of: CE 0.21090275049209595, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.038460731506348 EntMin 0.0
Epoch 1, Class Loss=0.18313956260681152, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.18313956260681152, Class Loss=0.18313956260681152, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/11, Loss=5.320116204023361
Loss made of: CE 0.16440337896347046, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.840234279632568 EntMin 0.0
Epoch 2, Class Loss=0.1776367425918579, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.1776367425918579, Class Loss=0.1776367425918579, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/11, Loss=5.324110503494739
Loss made of: CE 0.15956470370292664, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.247818470001221 EntMin 0.0
Epoch 3, Class Loss=0.17049697041511536, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.17049697041511536, Class Loss=0.17049697041511536, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/11, Loss=5.26429185718298
Loss made of: CE 0.1612883061170578, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.087021827697754 EntMin 0.0
Epoch 4, Class Loss=0.16265076398849487, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.16265076398849487, Class Loss=0.16265076398849487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/11, Loss=5.310605165362358
Loss made of: CE 0.13612811267375946, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.595769882202148 EntMin 0.0
Epoch 5, Class Loss=0.15658335387706757, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.15658335387706757, Class Loss=0.15658335387706757, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/11, Loss=5.282949376106262
Loss made of: CE 0.14155438542366028, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.403482437133789 EntMin 0.0
Epoch 6, Class Loss=0.15769124031066895, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.15769124031066895, Class Loss=0.15769124031066895, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/11, Loss=5.714718207716942
Loss made of: CE 0.2171735018491745, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5339674949646 EntMin 0.0
Epoch 1, Class Loss=0.19885700941085815, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.19885700941085815, Class Loss=0.19885700941085815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/11, Loss=5.607208646833897
Loss made of: CE 0.1835058182477951, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.782195091247559 EntMin 0.0
Epoch 2, Class Loss=0.18867067992687225, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.18867067992687225, Class Loss=0.18867067992687225, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/11, Loss=5.591896311938763
Loss made of: CE 0.16374418139457703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.664084434509277 EntMin 0.0
Epoch 3, Class Loss=0.17199456691741943, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.17199456691741943, Class Loss=0.17199456691741943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/11, Loss=5.577385552227497
Loss made of: CE 0.1541963815689087, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.104519844055176 EntMin 0.0
Epoch 4, Class Loss=0.16567207872867584, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.16567207872867584, Class Loss=0.16567207872867584, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/11, Loss=5.461854706704616
Loss made of: CE 0.16959401965141296, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7318115234375 EntMin 0.0
Epoch 5, Class Loss=0.16645920276641846, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.16645920276641846, Class Loss=0.16645920276641846, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/11, Loss=5.495685577392578
Loss made of: CE 0.13666236400604248, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0483622550964355 EntMin 0.0
Epoch 6, Class Loss=0.16074149310588837, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.16074149310588837, Class Loss=0.16074149310588837, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5159362554550171, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.852329
Mean Acc: 0.583185
FreqW Acc: 0.780218
Mean IoU: 0.460074
Class IoU:
	class 0: 0.88841623
	class 1: 0.8483018
	class 2: 0.32143322
	class 3: 0.7379064
	class 4: 0.64893246
	class 5: 0.7417507
	class 6: 0.7698721
	class 7: 0.8257862
	class 8: 0.7245691
	class 9: 0.02101986
	class 10: 0.0
	class 11: 0.0013158936
	class 12: 0.20814681
	class 13: 0.16366476
	class 14: 0.0
Class Acc:
	class 0: 0.95374775
	class 1: 0.9155301
	class 2: 0.9243984
	class 3: 0.7762627
	class 4: 0.7905438
	class 5: 0.83736944
	class 6: 0.7794999
	class 7: 0.9120339
	class 8: 0.7866552
	class 9: 0.021906342
	class 10: 0.0
	class 11: 0.0013170723
	class 12: 0.22155342
	class 13: 0.82696074
	class 14: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[11, 2, 15, 6]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=9.145247259736061
Loss made of: CE 0.5417490005493164, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.212726593017578 EntMin 0.0
Epoch 1, Class Loss=0.6020667552947998, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.6020667552947998, Class Loss=0.6020667552947998, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.441978952288627
Loss made of: CE 0.49766457080841064, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.224306106567383 EntMin 0.0
Epoch 2, Class Loss=0.5156814455986023, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.5156814455986023, Class Loss=0.5156814455986023, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.558999294042588
Loss made of: CE 0.43578478693962097, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.864940166473389 EntMin 0.0
Epoch 3, Class Loss=0.42548999190330505, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.42548999190330505, Class Loss=0.42548999190330505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.116254398226738
Loss made of: CE 0.3572905659675598, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.696202754974365 EntMin 0.0
Epoch 4, Class Loss=0.40074920654296875, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.40074920654296875, Class Loss=0.40074920654296875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.924817864596844
Loss made of: CE 0.37300023436546326, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.591519832611084 EntMin 0.0
Epoch 5, Class Loss=0.3516431748867035, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.3516431748867035, Class Loss=0.3516431748867035, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.854597210884094
Loss made of: CE 0.38694947957992554, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.680163860321045 EntMin 0.0
Epoch 6, Class Loss=0.3522535264492035, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.3522535264492035, Class Loss=0.3522535264492035, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=9.066399765014648
Loss made of: CE 0.7437927722930908, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.445250988006592 EntMin 0.0
Epoch 1, Class Loss=0.5772169828414917, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.5772169828414917, Class Loss=0.5772169828414917, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.268848010897637
Loss made of: CE 0.4609350562095642, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1941633224487305 EntMin 0.0
Epoch 2, Class Loss=0.4936407208442688, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.4936407208442688, Class Loss=0.4936407208442688, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.5329336524009705
Loss made of: CE 0.38213634490966797, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.267167568206787 EntMin 0.0
Epoch 3, Class Loss=0.43152377009391785, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.43152377009391785, Class Loss=0.43152377009391785, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.251294952630997
Loss made of: CE 0.25660449266433716, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7154541015625 EntMin 0.0
Epoch 4, Class Loss=0.3800544738769531, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.3800544738769531, Class Loss=0.3800544738769531, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.847596502304077
Loss made of: CE 0.34755250811576843, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.551928520202637 EntMin 0.0
Epoch 5, Class Loss=0.36567407846450806, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.36567407846450806, Class Loss=0.36567407846450806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.907961699366569
Loss made of: CE 0.45574089884757996, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.355503082275391 EntMin 0.0
Epoch 6, Class Loss=0.35160768032073975, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.35160768032073975, Class Loss=0.35160768032073975, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.974321776628495
Loss made of: CE 0.5237313508987427, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.200028896331787 EntMin 0.0
Epoch 1, Class Loss=0.5477832555770874, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.5477832555770874, Class Loss=0.5477832555770874, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.247154039144516
Loss made of: CE 0.3967739939689636, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.203505516052246 EntMin 0.0
Epoch 2, Class Loss=0.47616586089134216, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.47616586089134216, Class Loss=0.47616586089134216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.544194419682026
Loss made of: CE 0.4784770905971527, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.861301422119141 EntMin 0.0
Epoch 3, Class Loss=0.4176405370235443, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.4176405370235443, Class Loss=0.4176405370235443, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.320575195550918
Loss made of: CE 0.2807181775569916, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9183855056762695 EntMin 0.0
Epoch 4, Class Loss=0.3621227741241455, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.3621227741241455, Class Loss=0.3621227741241455, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.974729534983635
Loss made of: CE 0.4505409598350525, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.907839775085449 EntMin 0.0
Epoch 5, Class Loss=0.3516632914543152, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.3516632914543152, Class Loss=0.3516632914543152, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.866657537221909
Loss made of: CE 0.3107088506221771, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8913798332214355 EntMin 0.0
Epoch 6, Class Loss=0.348641961812973, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.348641961812973, Class Loss=0.348641961812973, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=8.458205419778825
Loss made of: CE 0.5643420219421387, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.933904647827148 EntMin 0.0
Epoch 1, Batch 20/97, Loss=7.462674549221992
Loss made of: CE 0.4531821608543396, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.798839092254639 EntMin 0.0
Epoch 1, Batch 30/97, Loss=6.502637520432472
Loss made of: CE 0.2907451093196869, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0211100578308105 EntMin 0.0
Epoch 1, Batch 40/97, Loss=6.246396338939666
Loss made of: CE 0.40787672996520996, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.577831268310547 EntMin 0.0
Epoch 1, Batch 50/97, Loss=6.244305640459061
Loss made of: CE 0.367838978767395, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.005622863769531 EntMin 0.0
Epoch 1, Batch 60/97, Loss=6.07395840883255
Loss made of: CE 0.27482593059539795, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.475051403045654 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.976792497932911
Loss made of: CE 0.3308258056640625, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.372922420501709 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.628943015635014
Loss made of: CE 0.33593472838401794, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.697273254394531 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.620011897385121
Loss made of: CE 0.25218090415000916, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.302521228790283 EntMin 0.0
Epoch 1, Class Loss=0.3682011663913727, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.3682011663913727, Class Loss=0.3682011663913727, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/97, Loss=5.697682364284992
Loss made of: CE 0.23407241702079773, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.81976842880249 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.631079930067062
Loss made of: CE 0.2682764232158661, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.060571670532227 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.333634577691555
Loss made of: CE 0.1930217444896698, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.074359893798828 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.476156640052795
Loss made of: CE 0.23223862051963806, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.632735729217529 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.339954587817192
Loss made of: CE 0.23079189658164978, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8203043937683105 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.386570796370506
Loss made of: CE 0.21982845664024353, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2580366134643555 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.356900860369206
Loss made of: CE 0.1937481164932251, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.299918174743652 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.460267229378223
Loss made of: CE 0.23646573722362518, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.472780227661133 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.433669613301754
Loss made of: CE 0.15169349312782288, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.325676441192627 EntMin 0.0
Epoch 2, Class Loss=0.2207081913948059, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.2207081913948059, Class Loss=0.2207081913948059, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/97, Loss=5.195890155434609
Loss made of: CE 0.14231976866722107, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.796710968017578 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.20768214315176
Loss made of: CE 0.19553613662719727, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.646674156188965 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.548003728687763
Loss made of: CE 0.20931045711040497, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.500653266906738 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.349702849984169
Loss made of: CE 0.1735793948173523, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.495915412902832 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.4057732105255125
Loss made of: CE 0.14416174590587616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.817166805267334 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.3394245728850365
Loss made of: CE 0.12874679267406464, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.706343650817871 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.054871326684951
Loss made of: CE 0.14222589135169983, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.721612930297852 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.32476618885994
Loss made of: CE 0.15427011251449585, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.951816082000732 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.264731771498918
Loss made of: CE 0.16418662667274475, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.915536403656006 EntMin 0.0
Epoch 3, Class Loss=0.16794170439243317, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.16794170439243317, Class Loss=0.16794170439243317, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/97, Loss=5.101632310450077
Loss made of: CE 0.12851564586162567, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.666429042816162 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.118822845816612
Loss made of: CE 0.13196220993995667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.539619445800781 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.91952041760087
Loss made of: CE 0.11071132123470306, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.428896427154541 EntMin 0.0
Epoch 4, Batch 40/97, Loss=5.03209708482027
Loss made of: CE 0.14351338148117065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.925806999206543 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.985412689298391
Loss made of: CE 0.09862637519836426, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.940131187438965 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.853223084658384
Loss made of: CE 0.13316017389297485, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8183274269104 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.101659226417541
Loss made of: CE 0.13068822026252747, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.484528541564941 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.1379469022154804
Loss made of: CE 0.1663772612810135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7772674560546875 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.954916109144688
Loss made of: CE 0.12173064798116684, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.161225318908691 EntMin 0.0
Epoch 4, Class Loss=0.14372271299362183, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.14372271299362183, Class Loss=0.14372271299362183, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/97, Loss=4.933685097843409
Loss made of: CE 0.14126893877983093, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.175038814544678 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.820032192021609
Loss made of: CE 0.1232834979891777, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.600061416625977 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.872145860642195
Loss made of: CE 0.12528488039970398, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.220184326171875 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.930855596065522
Loss made of: CE 0.18620549142360687, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.21708869934082 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.999304693192244
Loss made of: CE 0.1067528948187828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.939842224121094 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.979378218948841
Loss made of: CE 0.1368543952703476, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.358612060546875 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.65892174243927
Loss made of: CE 0.09433776140213013, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.231864929199219 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.864755648374557
Loss made of: CE 0.11673641204833984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.096122741699219 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.855243231356144
Loss made of: CE 0.10680431127548218, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251049041748047 EntMin 0.0
Epoch 5, Class Loss=0.12570051848888397, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.12570051848888397, Class Loss=0.12570051848888397, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/97, Loss=4.8900030549615625
Loss made of: CE 0.09773097932338715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.187941551208496 EntMin 0.0
Epoch 6, Batch 20/97, Loss=5.077137885242701
Loss made of: CE 0.10149522125720978, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.343214988708496 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.788962152600289
Loss made of: CE 0.08253061771392822, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.773153305053711 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.9304497838020325
Loss made of: CE 0.1458054482936859, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.953132629394531 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.765713901817799
Loss made of: CE 0.10214554518461227, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.439174652099609 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.8057710319757465
Loss made of: CE 0.11604007333517075, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.82804536819458 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.79542677551508
Loss made of: CE 0.1312989592552185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.227962493896484 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.748953609168529
Loss made of: CE 0.1138007864356041, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.945754051208496 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.959834862500429
Loss made of: CE 0.11701597273349762, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.905670642852783 EntMin 0.0
Epoch 6, Class Loss=0.11315284669399261, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.11315284669399261, Class Loss=0.11315284669399261, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7189357876777649, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.779225
Mean Acc: 0.485366
FreqW Acc: 0.657393
Mean IoU: 0.371475
Class IoU:
	class 0: 0.807886
	class 1: 0.8088917
	class 2: 0.31402114
	class 3: 0.61107427
	class 4: 0.6211149
	class 5: 0.6839995
	class 6: 0.68188375
	class 7: 0.7836927
	class 8: 0.57000643
	class 9: 0.04134232
	class 10: 0.0
	class 11: 0.00811607
	class 12: 0.24121307
	class 13: 0.14183298
	class 14: 0.0
	class 15: 7.813551e-06
	class 16: 0.0
Class Acc:
	class 0: 0.9522475
	class 1: 0.89116955
	class 2: 0.9101327
	class 3: 0.64075506
	class 4: 0.74094564
	class 5: 0.7879557
	class 6: 0.7114646
	class 7: 0.88526034
	class 8: 0.59331083
	class 9: 0.045101706
	class 10: 0.0
	class 11: 0.008158094
	class 12: 0.25573596
	class 13: 0.82897234
	class 14: 0.0
	class 15: 7.813551e-06
	class 16: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[20, 2, 24, 4]
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=6.85628427863121
Loss made of: CE 0.2794090211391449, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.943819999694824 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.541326814889908
Loss made of: CE 0.3044971227645874, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.984524250030518 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.706396697461605
Loss made of: CE 0.2594229280948639, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.252625465393066 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.74735235273838
Loss made of: CE 0.25234612822532654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5973405838012695 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.407139228284359
Loss made of: CE 0.22903883457183838, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.500615119934082 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.397059167921543
Loss made of: CE 0.18143796920776367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.960719108581543 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.526961337029934
Loss made of: CE 0.23407559096813202, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.696732997894287 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.3429929003119465
Loss made of: CE 0.2132178246974945, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760926723480225 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/97, Loss=5.630993615090847
Loss made of: CE 0.16029368340969086, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.201297283172607 EntMin 0.0
Epoch 1, Class Loss=0.250823050737381, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.250823050737381, Class Loss=0.250823050737381, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/97, Loss=5.162253235280514
Loss made of: CE 0.16205164790153503, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.002050399780273 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.183639661967755
Loss made of: CE 0.12596499919891357, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.121172904968262 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.127522274851799
Loss made of: CE 0.12586988508701324, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.947006702423096 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.1988816693425175
Loss made of: CE 0.12111121416091919, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.651816368103027 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.13775220811367
Loss made of: CE 0.1539848893880844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5629119873046875 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.083669601380825
Loss made of: CE 0.17861899733543396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.930089473724365 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.460450108349323
Loss made of: CE 0.20763389766216278, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.314660549163818 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.9962822534143925
Loss made of: CE 0.1385725438594818, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.950168609619141 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.016689895093441
Loss made of: CE 0.13057906925678253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.298421382904053 EntMin 0.0
Epoch 2, Class Loss=0.175377756357193, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.175377756357193, Class Loss=0.175377756357193, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/97, Loss=5.076824747025967
Loss made of: CE 0.12469601631164551, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.876225471496582 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.286799895763397
Loss made of: CE 0.1585315465927124, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.281002044677734 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.979162542521953
Loss made of: CE 0.12315928936004639, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.770594120025635 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.072513142973184
Loss made of: CE 0.21859852969646454, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.850546836853027 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.955091830343008
Loss made of: CE 0.1315685510635376, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.545159339904785 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.308312805742025
Loss made of: CE 0.10595938563346863, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.648831844329834 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.8522375836968425
Loss made of: CE 0.13377094268798828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.798871040344238 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.982955731451511
Loss made of: CE 0.1111169382929802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.387559413909912 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.18911147788167
Loss made of: CE 0.09122665971517563, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.44914436340332 EntMin 0.0
Epoch 3, Class Loss=0.14158275723457336, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.14158275723457336, Class Loss=0.14158275723457336, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/97, Loss=5.2172842368483545
Loss made of: CE 0.14529559016227722, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.814466953277588 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.010090401768684
Loss made of: CE 0.14874526858329773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.75130033493042 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.930188865214586
Loss made of: CE 0.12244981527328491, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.342144966125488 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.768072397261858
Loss made of: CE 0.10224154591560364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6197285652160645 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.936900985985995
Loss made of: CE 0.13422895967960358, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.612284183502197 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.73162833750248
Loss made of: CE 0.1321772336959839, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.486218452453613 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.787274996191263
Loss made of: CE 0.19259189069271088, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.947965621948242 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.22006993740797
Loss made of: CE 0.1308794468641281, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.784123420715332 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.936571124196052
Loss made of: CE 0.11909978836774826, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.868009567260742 EntMin 0.0
Epoch 4, Class Loss=0.12144366651773453, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.12144366651773453, Class Loss=0.12144366651773453, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/97, Loss=4.888283941149711
Loss made of: CE 0.06635454297065735, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.568220615386963 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.833240378648043
Loss made of: CE 0.07937734574079514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.907840251922607 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.890448796749115
Loss made of: CE 0.12448571622371674, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.028543472290039 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.800359509140253
Loss made of: CE 0.08808867633342743, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.54935884475708 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.850173968076706
Loss made of: CE 0.09891106933355331, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.123859882354736 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.742489445954561
Loss made of: CE 0.12132543325424194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.704981327056885 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.743062460422516
Loss made of: CE 0.09910695999860764, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251824855804443 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.6371725901961325
Loss made of: CE 0.11993326991796494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464456558227539 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.63427947089076
Loss made of: CE 0.100174181163311, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2237162590026855 EntMin 0.0
Epoch 5, Class Loss=0.11077761650085449, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.11077761650085449, Class Loss=0.11077761650085449, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/97, Loss=4.532316320389509
Loss made of: CE 0.0983981341123581, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.260390758514404 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.887956959754229
Loss made of: CE 0.1334567368030548, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.665276527404785 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.564082274585962
Loss made of: CE 0.10679244995117188, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.493507385253906 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.869084522873163
Loss made of: CE 0.11167516559362411, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.715803146362305 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.82203012034297
Loss made of: CE 0.11171074956655502, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.375377655029297 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.638163205981255
Loss made of: CE 0.12046708911657333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.522913932800293 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.61186144426465
Loss made of: CE 0.19159959256649017, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.548307418823242 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.76108126565814
Loss made of: CE 0.10452921688556671, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.600489616394043 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.557119420915842
Loss made of: CE 0.11939339339733124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.237665176391602 EntMin 0.0
Epoch 6, Class Loss=0.10628481954336166, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.10628481954336166, Class Loss=0.10628481954336166, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.641409446299076
Loss made of: CE 0.5309619903564453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758565425872803 EntMin 0.0
Epoch 1, Class Loss=0.3863823115825653, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.3863823115825653, Class Loss=0.3863823115825653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=5.512958952784539
Loss made of: CE 0.32095980644226074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.630037784576416 EntMin 0.0
Epoch 2, Class Loss=0.3654470443725586, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.3654470443725586, Class Loss=0.3654470443725586, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=5.3654774010181425
Loss made of: CE 0.32341623306274414, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.202268123626709 EntMin 0.0
Epoch 3, Class Loss=0.33331936597824097, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.33331936597824097, Class Loss=0.33331936597824097, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=5.405366054177284
Loss made of: CE 0.25563880801200867, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.853122711181641 EntMin 0.0
Epoch 4, Class Loss=0.3452295660972595, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.3452295660972595, Class Loss=0.3452295660972595, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.172748762369156
Loss made of: CE 0.3267334997653961, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.13426399230957 EntMin 0.0
Epoch 5, Class Loss=0.31201407313346863, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.31201407313346863, Class Loss=0.31201407313346863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=5.217790457606315
Loss made of: CE 0.41651952266693115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7591376304626465 EntMin 0.0
Epoch 6, Class Loss=0.2990061640739441, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.2990061640739441, Class Loss=0.2990061640739441, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.5238559857010845
Loss made of: CE 0.27087730169296265, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.742533206939697 EntMin 0.0
Epoch 1, Class Loss=0.3295249342918396, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.3295249342918396, Class Loss=0.3295249342918396, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.431869946420193
Loss made of: CE 0.4355451464653015, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.632239818572998 EntMin 0.0
Epoch 2, Class Loss=0.3163929581642151, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.3163929581642151, Class Loss=0.3163929581642151, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.382343886792659
Loss made of: CE 0.2721124589443207, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.43804931640625 EntMin 0.0
Epoch 3, Class Loss=0.30094990134239197, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.30094990134239197, Class Loss=0.30094990134239197, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=5.382456628978252
Loss made of: CE 0.3295987546443939, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4194135665893555 EntMin 0.0
Epoch 4, Class Loss=0.2903641164302826, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.2903641164302826, Class Loss=0.2903641164302826, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.368076831102371
Loss made of: CE 0.21639248728752136, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.03792667388916 EntMin 0.0
Epoch 5, Class Loss=0.2931149899959564, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.2931149899959564, Class Loss=0.2931149899959564, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.159318226575851
Loss made of: CE 0.2720178961753845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.850383281707764 EntMin 0.0
Epoch 6, Class Loss=0.2750965356826782, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.2750965356826782, Class Loss=0.2750965356826782, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.620901855826378
Loss made of: CE 0.2720321714878082, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.273859977722168 EntMin 0.0
Epoch 1, Class Loss=0.36851462721824646, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.36851462721824646, Class Loss=0.36851462721824646, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.678279958665371
Loss made of: CE 0.4112005829811096, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3168439865112305 EntMin 0.0
Epoch 2, Class Loss=0.3640516996383667, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.3640516996383667, Class Loss=0.3640516996383667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.6401615053415295
Loss made of: CE 0.39843034744262695, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.213823318481445 EntMin 0.0
Epoch 3, Class Loss=0.32625147700309753, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.32625147700309753, Class Loss=0.32625147700309753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=5.378764106333255
Loss made of: CE 0.5057075023651123, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.122149467468262 EntMin 0.0
Epoch 4, Class Loss=0.30820995569229126, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.30820995569229126, Class Loss=0.30820995569229126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.3516908749938015
Loss made of: CE 0.20401373505592346, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.922014236450195 EntMin 0.0
Epoch 5, Class Loss=0.3098505735397339, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.3098505735397339, Class Loss=0.3098505735397339, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.195607060194016
Loss made of: CE 0.31046754121780396, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.131752014160156 EntMin 0.0
Epoch 6, Class Loss=0.3000243604183197, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.3000243604183197, Class Loss=0.3000243604183197, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6708200573921204, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.797415
Mean Acc: 0.514561
FreqW Acc: 0.691354
Mean IoU: 0.393895
Class IoU:
	class 0: 0.8272214
	class 1: 0.8166692
	class 2: 0.30430683
	class 3: 0.65153575
	class 4: 0.6199594
	class 5: 0.70036644
	class 6: 0.7017242
	class 7: 0.78255534
	class 8: 0.62143624
	class 9: 0.041436635
	class 10: 0.0
	class 11: 0.011622746
	class 12: 0.21222432
	class 13: 0.14128324
	class 14: 0.0
	class 15: 0.26386884
	class 16: 0.0
Class Acc:
	class 0: 0.94626075
	class 1: 0.9048334
	class 2: 0.9299064
	class 3: 0.69196624
	class 4: 0.7623828
	class 5: 0.83818346
	class 6: 0.72793037
	class 7: 0.89840764
	class 8: 0.651465
	class 9: 0.045920767
	class 10: 0.0
	class 11: 0.011697387
	class 12: 0.22245759
	class 13: 0.8435482
	class 14: 0.0
	class 15: 0.27258313
	class 16: 0.0

federated global round: 22, step: 4
select part of clients to conduct local training
[21, 0, 25, 23]
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=5.977063451707363
Loss made of: CE 0.3282146155834198, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.282052993774414 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.534161554276944
Loss made of: CE 0.2122281789779663, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.23467493057251 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.333318620920181
Loss made of: CE 0.23655986785888672, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.12518835067749 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.42590966373682
Loss made of: CE 0.2226509302854538, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.590112686157227 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.2009213119745255
Loss made of: CE 0.1407594084739685, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.576071739196777 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.187755714356899
Loss made of: CE 0.13058887422084808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.616606712341309 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.102163949608803
Loss made of: CE 0.16384147107601166, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6334381103515625 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.960954628139734
Loss made of: CE 0.11344590038061142, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7767744064331055 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.017016688734293
Loss made of: CE 0.12693659961223602, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.401708126068115 EntMin 0.0
Epoch 1, Class Loss=0.1984403133392334, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.1984403133392334, Class Loss=0.1984403133392334, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.957574810087681
Loss made of: CE 0.15609335899353027, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.76183557510376 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.993098908662796
Loss made of: CE 0.10870808362960815, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.541738986968994 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.086603807657957
Loss made of: CE 0.1355028748512268, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.677221298217773 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.026710978150367
Loss made of: CE 0.1497444361448288, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.271640300750732 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.92747512832284
Loss made of: CE 0.12542925775051117, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.45688533782959 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.975727867335081
Loss made of: CE 0.12309684604406357, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.408456802368164 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.895680078864098
Loss made of: CE 0.11883138120174408, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.631458282470703 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 80/97, Loss=5.06247977912426
Loss made of: CE 0.14523154497146606, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.631551742553711 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.8065599501132965
Loss made of: CE 0.11524447053670883, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1301774978637695 EntMin 0.0
Epoch 2, Class Loss=0.14111767709255219, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.14111767709255219, Class Loss=0.14111767709255219, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.8165250569581985
Loss made of: CE 0.16110104322433472, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7338547706604 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.941390382498502
Loss made of: CE 0.14167079329490662, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.646858215332031 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.818932073563337
Loss made of: CE 0.11332850158214569, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.302987098693848 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.767315547168255
Loss made of: CE 0.08086364716291428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.661864280700684 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.7577945061028
Loss made of: CE 0.13661324977874756, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3479461669921875 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.926407881081104
Loss made of: CE 0.12124782055616379, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.604365348815918 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.88361997678876
Loss made of: CE 0.0964357852935791, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.751556396484375 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.911175858974457
Loss made of: CE 0.14986370503902435, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.649853229522705 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.7469393841922285
Loss made of: CE 0.11100982129573822, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.206019878387451 EntMin 0.0
Epoch 3, Class Loss=0.12236642837524414, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.12236642837524414, Class Loss=0.12236642837524414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.785233620554209
Loss made of: CE 0.12021341174840927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.468742370605469 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.728683052212
Loss made of: CE 0.11005117744207382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.216082572937012 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.873462157696485
Loss made of: CE 0.14065080881118774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.914855480194092 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.737431007623672
Loss made of: CE 0.16175135970115662, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.841349124908447 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.856279131025076
Loss made of: CE 0.09190749377012253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.900086879730225 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.867589277029038
Loss made of: CE 0.14760886132717133, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.132174015045166 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.789312849193811
Loss made of: CE 0.09077668190002441, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.450143337249756 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.701005783677101
Loss made of: CE 0.09383730590343475, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.236001968383789 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.842005103081465
Loss made of: CE 0.1338474005460739, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.515702247619629 EntMin 0.0
Epoch 4, Class Loss=0.11406471580266953, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.11406471580266953, Class Loss=0.11406471580266953, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.696957345306873
Loss made of: CE 0.08882288634777069, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.904036521911621 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.609117240458727
Loss made of: CE 0.08213123679161072, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.93125057220459 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.629384478181601
Loss made of: CE 0.1498466432094574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.533888816833496 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.78989160656929
Loss made of: CE 0.10695771872997284, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.768473148345947 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.497425320744514
Loss made of: CE 0.09179507941007614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.461020469665527 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.798809013515711
Loss made of: CE 0.09505414962768555, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.567543983459473 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.694357428699732
Loss made of: CE 0.11096563190221786, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.52421760559082 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.716558241844178
Loss made of: CE 0.10562436282634735, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.039921760559082 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.770608599483967
Loss made of: CE 0.10960684716701508, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.761213302612305 EntMin 0.0
Epoch 5, Class Loss=0.10416020452976227, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.10416020452976227, Class Loss=0.10416020452976227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.478698497265578
Loss made of: CE 0.14919979870319366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.711112022399902 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.539817898720503
Loss made of: CE 0.08694224059581757, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7467055320739746 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.727094654738903
Loss made of: CE 0.09538738429546356, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.098986625671387 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.688129008561373
Loss made of: CE 0.08377095311880112, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.451684474945068 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.488641539961099
Loss made of: CE 0.09406408667564392, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.234114646911621 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.582832071185112
Loss made of: CE 0.08341743052005768, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.914441108703613 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.4950314342975615
Loss made of: CE 0.08965909481048584, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.950132131576538 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.404648438841105
Loss made of: CE 0.07630982995033264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.221463680267334 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.341820580512286
Loss made of: CE 0.10230811685323715, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.910264730453491 EntMin 0.0
Epoch 6, Class Loss=0.09785659611225128, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.09785659611225128, Class Loss=0.09785659611225128, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.770854589343071
Loss made of: CE 0.27810657024383545, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.387450218200684 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.28412529528141
Loss made of: CE 0.22133037447929382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.512939453125 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.235650825500488
Loss made of: CE 0.223550945520401, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.171438694000244 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.435806766152382
Loss made of: CE 0.2239488810300827, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.313678741455078 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.199036826193333
Loss made of: CE 0.2648557424545288, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.826284408569336 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.95427518337965
Loss made of: CE 0.13182072341442108, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.206776142120361 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.148322209715843
Loss made of: CE 0.23795589804649353, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3073811531066895 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.047361098229885
Loss made of: CE 0.2105657309293747, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6640424728393555 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.302190957963466
Loss made of: CE 0.19226598739624023, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.206227779388428 EntMin 0.0
Epoch 1, Class Loss=0.2013283669948578, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.2013283669948578, Class Loss=0.2013283669948578, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=5.11512898877263
Loss made of: CE 0.1450560986995697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726687431335449 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.043034990876913
Loss made of: CE 0.14205819368362427, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.579160690307617 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.126452548801899
Loss made of: CE 0.14351247251033783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.20276403427124 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.825310579687357
Loss made of: CE 0.1381014883518219, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.615146160125732 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.001105859130621
Loss made of: CE 0.15039575099945068, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.519765853881836 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.986988464742899
Loss made of: CE 0.08959291875362396, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.062854290008545 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.104240481555462
Loss made of: CE 0.13809815049171448, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.005902290344238 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.972379035502672
Loss made of: CE 0.14969253540039062, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.099163055419922 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.955973726511002
Loss made of: CE 0.14578673243522644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.909282207489014 EntMin 0.0
Epoch 2, Class Loss=0.14213740825653076, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.14213740825653076, Class Loss=0.14213740825653076, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.764146005362273
Loss made of: CE 0.1427127718925476, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.321455478668213 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.724827228486538
Loss made of: CE 0.09827723354101181, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.452665328979492 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.966074428707361
Loss made of: CE 0.13338333368301392, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0706071853637695 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.0282037377357485
Loss made of: CE 0.1369360089302063, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9386887550354 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.076547997444868
Loss made of: CE 0.12228786945343018, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.39314079284668 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.994773954898119
Loss made of: CE 0.1139623373746872, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.828676700592041 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.032677541673183
Loss made of: CE 0.09856945276260376, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2459635734558105 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.957945394515991
Loss made of: CE 0.12024079263210297, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.285691261291504 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.957262977212667
Loss made of: CE 0.12144023180007935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5156636238098145 EntMin 0.0
Epoch 3, Class Loss=0.12333627790212631, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.12333627790212631, Class Loss=0.12333627790212631, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.687014976888895
Loss made of: CE 0.13384638726711273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.367587089538574 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.788232036679983
Loss made of: CE 0.13105356693267822, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.958154678344727 EntMin 0.0
Epoch 4, Batch 30/97, Loss=5.135644249618053
Loss made of: CE 0.11431273072957993, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.281595230102539 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.79960658326745
Loss made of: CE 0.08805325627326965, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0535888671875 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.716249969601631
Loss made of: CE 0.0995352491736412, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.027680397033691 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.6027978278696535
Loss made of: CE 0.10478010028600693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.539144515991211 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.825183998793364
Loss made of: CE 0.09777317941188812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.491336822509766 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.62608548104763
Loss made of: CE 0.12793777883052826, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0450029373168945 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.043683214485645
Loss made of: CE 0.0998167023062706, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.502243995666504 EntMin 0.0
Epoch 4, Class Loss=0.11575933545827866, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.11575933545827866, Class Loss=0.11575933545827866, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.540103740990162
Loss made of: CE 0.08655854314565659, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.168381690979004 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.808051769435406
Loss made of: CE 0.10858564078807831, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.557417869567871 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.740355351567269
Loss made of: CE 0.1040540561079979, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.353913307189941 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.828951444476843
Loss made of: CE 0.08232851326465607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.863003730773926 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.695654083788395
Loss made of: CE 0.09809662401676178, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.339829444885254 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.6530236706137655
Loss made of: CE 0.1127050369977951, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.036088943481445 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.779610458761454
Loss made of: CE 0.11436804383993149, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.188563346862793 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.4313952349126335
Loss made of: CE 0.11405124515295029, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.359264373779297 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.492493810132146
Loss made of: CE 0.08632133901119232, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.198772430419922 EntMin 0.0
Epoch 5, Class Loss=0.10250367224216461, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.10250367224216461, Class Loss=0.10250367224216461, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.498862375319004
Loss made of: CE 0.08475199341773987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.277941703796387 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.631057395040989
Loss made of: CE 0.09490518271923065, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.889241933822632 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.828294096142054
Loss made of: CE 0.12141250818967819, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.185608863830566 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.805274016410112
Loss made of: CE 0.07332225888967514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.674257755279541 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.570001780241728
Loss made of: CE 0.12540316581726074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.152548789978027 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.834386362880468
Loss made of: CE 0.10928557813167572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0987396240234375 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.5882702678442
Loss made of: CE 0.1215466856956482, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.453040599822998 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.616105177253485
Loss made of: CE 0.08855637162923813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.447416305541992 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.739846748858691
Loss made of: CE 0.07853667438030243, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.273605823516846 EntMin 0.0
Epoch 6, Class Loss=0.10158239305019379, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.10158239305019379, Class Loss=0.10158239305019379, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.720531029999256
Loss made of: CE 0.2984861135482788, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.36087703704834 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.540891890227795
Loss made of: CE 0.22869400680065155, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.155078887939453 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.317976202070713
Loss made of: CE 0.17332574725151062, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.319038391113281 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.204584838449955
Loss made of: CE 0.17425791919231415, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.842864990234375 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.195154079794884
Loss made of: CE 0.20644843578338623, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.944596290588379 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.1502407386899
Loss made of: CE 0.18002420663833618, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.798173904418945 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.289806273579598
Loss made of: CE 0.13800352811813354, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3888349533081055 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.14083910137415
Loss made of: CE 0.1578984260559082, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.05502986907959 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.3010755106806755
Loss made of: CE 0.141733318567276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.985616683959961 EntMin 0.0
Epoch 1, Class Loss=0.19631832838058472, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.19631832838058472, Class Loss=0.19631832838058472, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.795301559567451
Loss made of: CE 0.14591792225837708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.83854866027832 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.153648781776428
Loss made of: CE 0.09582847356796265, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4759440422058105 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.011778431385755
Loss made of: CE 0.12086182832717896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.338242530822754 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.18390005454421
Loss made of: CE 0.14574235677719116, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.291924476623535 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.048066098988056
Loss made of: CE 0.10140763223171234, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.615493297576904 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.243150216341019
Loss made of: CE 0.15750381350517273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.974287986755371 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.956700227409601
Loss made of: CE 0.13341335952281952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.926158905029297 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.852861396968365
Loss made of: CE 0.13410484790802002, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.433299541473389 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.903572748601436
Loss made of: CE 0.10453127324581146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.592127799987793 EntMin 0.0
Epoch 2, Class Loss=0.14383666217327118, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.14383666217327118, Class Loss=0.14383666217327118, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.765129675716162
Loss made of: CE 0.1546691358089447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.577607154846191 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.880652265250683
Loss made of: CE 0.10366243124008179, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.536340713500977 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.800464604049921
Loss made of: CE 0.10784618556499481, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.342385292053223 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.996985045075417
Loss made of: CE 0.11413844674825668, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.41049337387085 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.1249920591712
Loss made of: CE 0.07992926239967346, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6396613121032715 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.973787596076727
Loss made of: CE 0.11579927057027817, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.715796947479248 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.842756712436676
Loss made of: CE 0.1559951901435852, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.653841018676758 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.822417438775301
Loss made of: CE 0.13071966171264648, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.160405158996582 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.8440775036811825
Loss made of: CE 0.12507683038711548, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.738879680633545 EntMin 0.0
Epoch 3, Class Loss=0.12349825352430344, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.12349825352430344, Class Loss=0.12349825352430344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.797196439653635
Loss made of: CE 0.1227576732635498, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.259533882141113 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.788448340445757
Loss made of: CE 0.11449103057384491, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481534957885742 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.611032103002072
Loss made of: CE 0.11262600123882294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.445187568664551 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.766904863715172
Loss made of: CE 0.169354647397995, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.671740531921387 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.984212778508663
Loss made of: CE 0.1701679676771164, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.650275230407715 EntMin 0.0
Epoch 4, Batch 60/97, Loss=5.180633277446032
Loss made of: CE 0.13342422246932983, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.608124732971191 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.729697027802468
Loss made of: CE 0.12298448383808136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.242209434509277 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.670536974817514
Loss made of: CE 0.10679583996534348, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.105045318603516 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.697629188746214
Loss made of: CE 0.1253313422203064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.487329006195068 EntMin 0.0
Epoch 4, Class Loss=0.1120380386710167, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.1120380386710167, Class Loss=0.1120380386710167, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.48346045166254
Loss made of: CE 0.10764624923467636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.409074306488037 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.375169285386801
Loss made of: CE 0.09158271551132202, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.267706871032715 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.981115005910397
Loss made of: CE 0.12240253388881683, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8106279373168945 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.996893689036369
Loss made of: CE 0.08124085515737534, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.908082485198975 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.756367450207472
Loss made of: CE 0.12985697388648987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.697752952575684 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.802045407146215
Loss made of: CE 0.12124421447515488, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.37809944152832 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.741882617771625
Loss made of: CE 0.09757548570632935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.260210990905762 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.536850076913834
Loss made of: CE 0.1006171703338623, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500615119934082 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.711251682043075
Loss made of: CE 0.11049008369445801, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.719631671905518 EntMin 0.0
Epoch 5, Class Loss=0.10311371833086014, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.10311371833086014, Class Loss=0.10311371833086014, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.464359936118126
Loss made of: CE 0.08835695683956146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4346537590026855 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.5849031932652
Loss made of: CE 0.10742367804050446, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.13860559463501 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.5974217362701895
Loss made of: CE 0.08690955489873886, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.991000175476074 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.4623125799000265
Loss made of: CE 0.09271252155303955, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.382390022277832 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.539023131877184
Loss made of: CE 0.12435799092054367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.634161472320557 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.6907306432724
Loss made of: CE 0.07202692329883575, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.833428382873535 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.600377189368009
Loss made of: CE 0.1005258858203888, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.348966598510742 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.604457391798496
Loss made of: CE 0.07255233824253082, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.208989143371582 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.316186363250017
Loss made of: CE 0.07131299376487732, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.06497859954834 EntMin 0.0
Epoch 6, Class Loss=0.100104920566082, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.100104920566082, Class Loss=0.100104920566082, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=6.198515978455544
Loss made of: CE 0.18932925164699554, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.482247352600098 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.1129432514309885
Loss made of: CE 0.22015909850597382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.518537521362305 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.0365481361746784
Loss made of: CE 0.20575392246246338, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.744630336761475 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.289794473350048
Loss made of: CE 0.17944499850273132, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.79440975189209 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.159213989973068
Loss made of: CE 0.19228911399841309, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.119877815246582 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.918492630124092
Loss made of: CE 0.17155122756958008, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1270294189453125 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.0373609706759455
Loss made of: CE 0.1742311418056488, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.104024410247803 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.098993311822414
Loss made of: CE 0.1620473861694336, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.274026870727539 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.249991251528263
Loss made of: CE 0.1377236545085907, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.312742710113525 EntMin 0.0
Epoch 1, Class Loss=0.1981188952922821, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.1981188952922821, Class Loss=0.1981188952922821, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.9573678396642205
Loss made of: CE 0.09479188919067383, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.243773460388184 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.003494381904602
Loss made of: CE 0.15276792645454407, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.019991397857666 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.857870786637068
Loss made of: CE 0.20530501008033752, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.593350887298584 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.986865857243538
Loss made of: CE 0.12653690576553345, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.251640319824219 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.098849131166935
Loss made of: CE 0.12482299655675888, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.585383892059326 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.250248648226261
Loss made of: CE 0.146723672747612, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.903277397155762 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.968845695257187
Loss made of: CE 0.1397828459739685, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8560638427734375 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.00106473043561
Loss made of: CE 0.14860635995864868, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.94284200668335 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.064229093492031
Loss made of: CE 0.1444675326347351, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.366576671600342 EntMin 0.0
Epoch 2, Class Loss=0.1437320113182068, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.1437320113182068, Class Loss=0.1437320113182068, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.967658865451813
Loss made of: CE 0.10632889717817307, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.036988258361816 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.966410557925701
Loss made of: CE 0.1180219054222107, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8924031257629395 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.965868742018938
Loss made of: CE 0.1563851535320282, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4108734130859375 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.902424804866314
Loss made of: CE 0.16171960532665253, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.83261251449585 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.991345725953579
Loss made of: CE 0.09337256848812103, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.814387321472168 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.771492762118578
Loss made of: CE 0.09793929755687714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.420173168182373 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.721052952855826
Loss made of: CE 0.09030263870954514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.550989151000977 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.646704775094986
Loss made of: CE 0.15934674441814423, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.016839027404785 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.978262583911419
Loss made of: CE 0.15754906833171844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.750401020050049 EntMin 0.0
Epoch 3, Class Loss=0.12369070947170258, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.12369070947170258, Class Loss=0.12369070947170258, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.724225287139416
Loss made of: CE 0.11155442893505096, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.72926664352417 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.818863834440708
Loss made of: CE 0.10330667346715927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3488383293151855 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.6851653508841995
Loss made of: CE 0.12004929035902023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.748466491699219 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.9023285284638405
Loss made of: CE 0.14264580607414246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.240157127380371 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.887880630046129
Loss made of: CE 0.12458792328834534, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8604536056518555 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.635625849664211
Loss made of: CE 0.1312624216079712, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6128740310668945 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.860804494470358
Loss made of: CE 0.12129825353622437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3794941902160645 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.060223516076803
Loss made of: CE 0.08111216872930527, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.339169979095459 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.633284175395966
Loss made of: CE 0.10917966812849045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5540008544921875 EntMin 0.0
Epoch 4, Class Loss=0.11334898322820663, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.11334898322820663, Class Loss=0.11334898322820663, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.779318826645612
Loss made of: CE 0.08051910996437073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.495617389678955 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.74171162545681
Loss made of: CE 0.15204158425331116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.653075218200684 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.791405143588781
Loss made of: CE 0.11861195415258408, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.588497161865234 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.730911026895046
Loss made of: CE 0.10199792683124542, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.913420677185059 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.4968464657664295
Loss made of: CE 0.09245698153972626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.506523609161377 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.439726242423058
Loss made of: CE 0.07675505429506302, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9243998527526855 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.563347911834716
Loss made of: CE 0.11543375253677368, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249054431915283 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.531449327617883
Loss made of: CE 0.08402057737112045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.290972709655762 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.7515563316643235
Loss made of: CE 0.12174873054027557, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.284887313842773 EntMin 0.0
Epoch 5, Class Loss=0.10625635087490082, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.10625635087490082, Class Loss=0.10625635087490082, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.547987879812718
Loss made of: CE 0.09721046686172485, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.185318946838379 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.588576763123274
Loss made of: CE 0.09561437368392944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.280275821685791 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.568694019317627
Loss made of: CE 0.11540217697620392, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.453535079956055 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.414495267719031
Loss made of: CE 0.09627386182546616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.00482702255249 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.637151391804219
Loss made of: CE 0.09302417933940887, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2772111892700195 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.6333939507603645
Loss made of: CE 0.09648309648036957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.595937728881836 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.698127823323011
Loss made of: CE 0.10252785682678223, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.096297264099121 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.601764960587024
Loss made of: CE 0.10568869113922119, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.037147521972656 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.558342140167952
Loss made of: CE 0.11776348948478699, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.31929874420166 EntMin 0.0
Epoch 6, Class Loss=0.10145878046751022, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.10145878046751022, Class Loss=0.10145878046751022, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.63945472240448, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.801373
Mean Acc: 0.510203
FreqW Acc: 0.705126
Mean IoU: 0.393302
Class IoU:
	class 0: 0.8407298
	class 1: 0.83847946
	class 2: 0.34649464
	class 3: 0.5421915
	class 4: 0.6451957
	class 5: 0.65522826
	class 6: 0.7440025
	class 7: 0.8214251
	class 8: 0.6310107
	class 9: 0.0032732503
	class 10: 0.0
	class 11: 0.0
	class 12: 0.017806666
	class 13: 0.21455385
	class 14: 0.0
	class 15: 0.3857439
	class 16: 0.0
Class Acc:
	class 0: 0.9023663
	class 1: 0.91041803
	class 2: 0.8697316
	class 3: 0.55480665
	class 4: 0.7742972
	class 5: 0.6964344
	class 6: 0.756309
	class 7: 0.9063061
	class 8: 0.65462065
	class 9: 0.003300031
	class 10: 0.0
	class 11: 0.0
	class 12: 0.017834874
	class 13: 0.6947542
	class 14: 0.0
	class 15: 0.9322736
	class 16: 0.0

federated global round: 23, step: 4
select part of clients to conduct local training
[17, 3, 5, 22]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.3246925584971905
Loss made of: CE 0.1108207032084465, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.350401878356934 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/97, Loss=4.355639969557524
Loss made of: CE 0.112704798579216, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.722598075866699 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.555325561016798
Loss made of: CE 0.10343950241804123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.243551254272461 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.562074695527554
Loss made of: CE 0.08353906869888306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.851764440536499 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.577556470036507
Loss made of: CE 0.08929258584976196, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.884429693222046 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.504953131079674
Loss made of: CE 0.09741245210170746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.270313262939453 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.3554462634027
Loss made of: CE 0.09822128713130951, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.272900104522705 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.667888213694096
Loss made of: CE 0.09768454730510712, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4917826652526855 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.660424848645926
Loss made of: CE 0.10767044872045517, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.492266654968262 EntMin 0.0
Epoch 1, Class Loss=0.09811317175626755, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.09811317175626755, Class Loss=0.09811317175626755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.5220157615840435
Loss made of: CE 0.08509307354688644, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.118338584899902 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.37592174783349
Loss made of: CE 0.09370733797550201, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.414188385009766 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.804746413975954
Loss made of: CE 0.0734386295080185, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2653703689575195 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.613249223679304
Loss made of: CE 0.08255557715892792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.562450408935547 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.488298524543643
Loss made of: CE 0.08467020839452744, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.603865146636963 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.621998824924231
Loss made of: CE 0.08195781707763672, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6596832275390625 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.750823191925884
Loss made of: CE 0.0940251350402832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.640761375427246 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.630433759093284
Loss made of: CE 0.08740072697401047, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.246519088745117 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.398169873654842
Loss made of: CE 0.07092239707708359, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.094666481018066 EntMin 0.0
Epoch 2, Class Loss=0.0941888839006424, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.0941888839006424, Class Loss=0.0941888839006424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.571419870853424
Loss made of: CE 0.07583770155906677, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.540535926818848 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.619961429387331
Loss made of: CE 0.07340968400239944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.983454704284668 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.316622096300125
Loss made of: CE 0.09951221942901611, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332728385925293 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.63892932459712
Loss made of: CE 0.09856215119361877, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.409523963928223 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.314704518020153
Loss made of: CE 0.14093303680419922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.206092357635498 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.682950855791569
Loss made of: CE 0.13678687810897827, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.724306106567383 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.6032530382275585
Loss made of: CE 0.10431168973445892, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.551632881164551 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.446422378718853
Loss made of: CE 0.06577469408512115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.105544567108154 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.522540135681629
Loss made of: CE 0.10572274774312973, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.079825401306152 EntMin 0.0
Epoch 3, Class Loss=0.08922524750232697, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.08922524750232697, Class Loss=0.08922524750232697, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.3900380924344065
Loss made of: CE 0.09925558418035507, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.236713409423828 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.761025650799274
Loss made of: CE 0.1646108627319336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.877840995788574 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.510100988298655
Loss made of: CE 0.06460091471672058, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9694507122039795 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.406204873323441
Loss made of: CE 0.07462738454341888, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.040100574493408 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.556896378844977
Loss made of: CE 0.10700132697820663, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.295985221862793 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.567520585283637
Loss made of: CE 0.08843239396810532, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.277344703674316 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.370046059042215
Loss made of: CE 0.07036525756120682, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2312774658203125 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.455272518843413
Loss made of: CE 0.06580241769552231, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.242306232452393 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.184094046056271
Loss made of: CE 0.05758928880095482, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.102874755859375 EntMin 0.0
Epoch 4, Class Loss=0.08521104604005814, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.08521104604005814, Class Loss=0.08521104604005814, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.260721890255809
Loss made of: CE 0.09092281758785248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.713797092437744 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.502759312093258
Loss made of: CE 0.082762710750103, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.640647888183594 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.395593876391649
Loss made of: CE 0.061412446200847626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.278895378112793 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.496229462325573
Loss made of: CE 0.07204891741275787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.317451000213623 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.605945091694593
Loss made of: CE 0.06180223822593689, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.954041481018066 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.406078369915486
Loss made of: CE 0.05954146385192871, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.346196174621582 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.460944541171193
Loss made of: CE 0.09507925808429718, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.558582782745361 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.424793860316276
Loss made of: CE 0.09011387825012207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.403311729431152 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.486916380375623
Loss made of: CE 0.10284928977489471, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.347908020019531 EntMin 0.0
Epoch 5, Class Loss=0.08417441695928574, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.08417441695928574, Class Loss=0.08417441695928574, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.509347618743777
Loss made of: CE 0.08161699771881104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6364006996154785 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.269861636310816
Loss made of: CE 0.09542286396026611, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9267892837524414 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.361568828672171
Loss made of: CE 0.1006689965724945, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4079909324646 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.270065473392606
Loss made of: CE 0.06416545808315277, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.949544906616211 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.461900264397263
Loss made of: CE 0.08596710860729218, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.418313980102539 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.398038675263524
Loss made of: CE 0.08522103726863861, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.118419647216797 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.454098983108997
Loss made of: CE 0.09221582114696503, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7967307567596436 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.401506643742323
Loss made of: CE 0.08098375052213669, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.030328273773193 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.619029827415943
Loss made of: CE 0.10509699583053589, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.504012107849121 EntMin 0.0
Epoch 6, Class Loss=0.08542400598526001, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.08542400598526001, Class Loss=0.08542400598526001, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.718970507383347
Loss made of: CE 0.3704327940940857, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.558727264404297 EntMin 0.0
Epoch 1, Class Loss=0.5047899484634399, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.5047899484634399, Class Loss=0.5047899484634399, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=5.928098770976066
Loss made of: CE 0.4282529950141907, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.347936630249023 EntMin 0.0
Epoch 2, Class Loss=0.4915478825569153, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.4915478825569153, Class Loss=0.4915478825569153, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.515125872194767
Loss made of: CE 0.11709736287593842, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.66277551651001 EntMin 0.0
Epoch 3, Class Loss=0.46429798007011414, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.46429798007011414, Class Loss=0.46429798007011414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.480288444459438
Loss made of: CE 0.1721213310956955, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.14709997177124 EntMin 0.0
Epoch 4, Class Loss=0.3910544514656067, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.3910544514656067, Class Loss=0.3910544514656067, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.188833837211132
Loss made of: CE 0.4420573115348816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664511680603027 EntMin 0.0
Epoch 5, Class Loss=0.39467093348503113, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.39467093348503113, Class Loss=0.39467093348503113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.275145348906517
Loss made of: CE 0.1803438663482666, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.741732597351074 EntMin 0.0
Epoch 6, Class Loss=0.36001014709472656, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.36001014709472656, Class Loss=0.36001014709472656, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.193324384093285
Loss made of: CE 0.09884268045425415, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.920170545578003 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.446710056066513
Loss made of: CE 0.15571917593479156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427853107452393 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 30/97, Loss=4.420666029304266
Loss made of: CE 0.09884826838970184, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.472727298736572 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.47738688364625
Loss made of: CE 0.07706616818904877, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2601318359375 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.57289956882596
Loss made of: CE 0.08371774852275848, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53127384185791 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.4903551042079926
Loss made of: CE 0.10302933305501938, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.041365623474121 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.608005538582802
Loss made of: CE 0.09373249858617783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0300726890563965 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.785039298981428
Loss made of: CE 0.11669610440731049, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3338446617126465 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.604504762589931
Loss made of: CE 0.07474779337644577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.697317600250244 EntMin 0.0
Epoch 1, Class Loss=0.09610982239246368, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.09610982239246368, Class Loss=0.09610982239246368, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.536204022169113
Loss made of: CE 0.08135084062814713, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.685059070587158 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.6027633510529995
Loss made of: CE 0.09452160447835922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.032478332519531 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.464772171527147
Loss made of: CE 0.10304206609725952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.80386209487915 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.421671214699745
Loss made of: CE 0.11916565895080566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.278668403625488 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.594997658580541
Loss made of: CE 0.10617603361606598, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.937973976135254 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.604122111201287
Loss made of: CE 0.10635603964328766, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2150092124938965 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.531988464295864
Loss made of: CE 0.09000170230865479, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.819402694702148 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.606324210017919
Loss made of: CE 0.10491304099559784, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.452364921569824 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.745337923243642
Loss made of: CE 0.08323393017053604, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.031959533691406 EntMin 0.0
Epoch 2, Class Loss=0.09503845870494843, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.09503845870494843, Class Loss=0.09503845870494843, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.639628589898348
Loss made of: CE 0.09672912210226059, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.31264591217041 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.631089061498642
Loss made of: CE 0.07194370031356812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.658594131469727 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.479899956285953
Loss made of: CE 0.09724061191082001, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035520553588867 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.6721415117383005
Loss made of: CE 0.06935626268386841, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.151484489440918 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.607540899515152
Loss made of: CE 0.10080169886350632, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.39731502532959 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.541700319200754
Loss made of: CE 0.12459265440702438, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.378165245056152 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.4592266894876955
Loss made of: CE 0.07612259685993195, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.063969135284424 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.471774455159903
Loss made of: CE 0.08849635720252991, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.167257785797119 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.462774000316858
Loss made of: CE 0.09070082753896713, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.572936534881592 EntMin 0.0
Epoch 3, Class Loss=0.09307850152254105, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.09307850152254105, Class Loss=0.09307850152254105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.561078025400638
Loss made of: CE 0.09915167093276978, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5728631019592285 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.379255620390177
Loss made of: CE 0.07806141674518585, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.194703578948975 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.428873068839311
Loss made of: CE 0.09332425892353058, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.246747016906738 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.351227563619614
Loss made of: CE 0.12874126434326172, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.043262958526611 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.256026760488749
Loss made of: CE 0.1000172346830368, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.172184944152832 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.438642662763596
Loss made of: CE 0.08737444877624512, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6005096435546875 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.446988250315189
Loss made of: CE 0.08636647462844849, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.539685249328613 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.215215253829956
Loss made of: CE 0.0673445463180542, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7628607749938965 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.365512317419052
Loss made of: CE 0.09472902119159698, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9694595336914062 EntMin 0.0
Epoch 4, Class Loss=0.088990218937397, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.088990218937397, Class Loss=0.088990218937397, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.47613046914339
Loss made of: CE 0.09468042850494385, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.383002758026123 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.471941450238228
Loss made of: CE 0.07794158160686493, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.187509059906006 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.382095417380333
Loss made of: CE 0.09604261815547943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.378217697143555 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.164356855303049
Loss made of: CE 0.06657841056585312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.343207359313965 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.33105441853404
Loss made of: CE 0.10256841033697128, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9575538635253906 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.594931342080235
Loss made of: CE 0.09019862115383148, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3783159255981445 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.455881641432643
Loss made of: CE 0.06852493435144424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.805917263031006 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.527043720707297
Loss made of: CE 0.08137267082929611, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9624767303466797 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.427695341780781
Loss made of: CE 0.08307107537984848, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.180958271026611 EntMin 0.0
Epoch 5, Class Loss=0.08313795179128647, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.08313795179128647, Class Loss=0.08313795179128647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.353353763371706
Loss made of: CE 0.07974529266357422, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.183516502380371 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.184038196504116
Loss made of: CE 0.07372558861970901, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7751052379608154 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.175474441051483
Loss made of: CE 0.14017656445503235, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.021782398223877 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.583492232114077
Loss made of: CE 0.14062504470348358, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.035093307495117 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.229858546704054
Loss made of: CE 0.08226944506168365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.624392509460449 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.296595855057239
Loss made of: CE 0.0658959448337555, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.770520210266113 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.154230342060328
Loss made of: CE 0.0718405693769455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2701416015625 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.251356100291014
Loss made of: CE 0.09323637932538986, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3258771896362305 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.333518333733082
Loss made of: CE 0.07756596803665161, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.023887634277344 EntMin 0.0
Epoch 6, Class Loss=0.08783262223005295, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.08783262223005295, Class Loss=0.08783262223005295, Reg Loss=0.0
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.776824030280113
Loss made of: CE 0.5757792592048645, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.850322246551514 EntMin 0.0
Epoch 1, Class Loss=0.551669716835022, Reg Loss=0.0
Clinet index 22, End of Epoch 1/6, Average Loss=0.551669716835022, Class Loss=0.551669716835022, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=5.811541470885277
Loss made of: CE 0.3886985778808594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.887828826904297 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.47632071375846863, Reg Loss=0.0
Clinet index 22, End of Epoch 2/6, Average Loss=0.47632071375846863, Class Loss=0.47632071375846863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.436322009563446
Loss made of: CE 0.730815589427948, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.124786376953125 EntMin 0.0
Epoch 3, Class Loss=0.45401573181152344, Reg Loss=0.0
Clinet index 22, End of Epoch 3/6, Average Loss=0.45401573181152344, Class Loss=0.45401573181152344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.506548455357551
Loss made of: CE 0.42909663915634155, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.06696891784668 EntMin 0.0
Epoch 4, Class Loss=0.4476053714752197, Reg Loss=0.0
Clinet index 22, End of Epoch 4/6, Average Loss=0.4476053714752197, Class Loss=0.4476053714752197, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.214954409003258
Loss made of: CE 0.26317059993743896, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.376991271972656 EntMin 0.0
Epoch 5, Class Loss=0.3894715905189514, Reg Loss=0.0
Clinet index 22, End of Epoch 5/6, Average Loss=0.3894715905189514, Class Loss=0.3894715905189514, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.080066607892514
Loss made of: CE 0.2189294993877411, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.578158378601074 EntMin 0.0
Epoch 6, Class Loss=0.36738190054893494, Reg Loss=0.0
Clinet index 22, End of Epoch 6/6, Average Loss=0.36738190054893494, Class Loss=0.36738190054893494, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6509940028190613, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.801066
Mean Acc: 0.528584
FreqW Acc: 0.707566
Mean IoU: 0.399866
Class IoU:
	class 0: 0.83892393
	class 1: 0.8450495
	class 2: 0.32721654
	class 3: 0.5891194
	class 4: 0.6461743
	class 5: 0.66871494
	class 6: 0.7647
	class 7: 0.80924773
	class 8: 0.66085577
	class 9: 0.013220448
	class 10: 0.0
	class 11: 7.04945e-07
	class 12: 0.027603898
	class 13: 0.19725412
	class 14: 0.0
	class 15: 0.40964806
	class 16: 0.0
Class Acc:
	class 0: 0.89415514
	class 1: 0.92670876
	class 2: 0.90614986
	class 3: 0.6081251
	class 4: 0.7738199
	class 5: 0.7188458
	class 6: 0.7846582
	class 7: 0.90908635
	class 8: 0.6930602
	class 9: 0.013356992
	class 10: 0.0
	class 11: 7.04945e-07
	class 12: 0.027655948
	class 13: 0.79275864
	class 14: 0.0
	class 15: 0.93754387
	class 16: 0.0

federated global round: 24, step: 4
select part of clients to conduct local training
[16, 9, 12, 2]
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=4.680588376522064
Loss made of: CE 0.08731803297996521, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.938141345977783 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.3545094288885595
Loss made of: CE 0.1019410714507103, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.33042049407959 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.3467887625098225
Loss made of: CE 0.07461504638195038, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.964897871017456 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.389829807728529
Loss made of: CE 0.11351041495800018, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.525294303894043 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.328742968663573
Loss made of: CE 0.10800260305404663, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.731621265411377 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.70837240293622
Loss made of: CE 0.09636209905147552, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.609577178955078 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.527942921966314
Loss made of: CE 0.06579169631004333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.713679313659668 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.512614864856005
Loss made of: CE 0.1325349509716034, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9724693298339844 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.417641204595566
Loss made of: CE 0.10397940874099731, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.682778835296631 EntMin 0.0
Epoch 1, Class Loss=0.0946834459900856, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.0946834459900856, Class Loss=0.0946834459900856, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/97, Loss=4.57180925309658
Loss made of: CE 0.07403481006622314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5771613121032715 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.411639875918627
Loss made of: CE 0.06473886966705322, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.319971561431885 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.556758944690228
Loss made of: CE 0.0979292020201683, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.849660873413086 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.646051272749901
Loss made of: CE 0.10423072427511215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.615474700927734 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.356777513772249
Loss made of: CE 0.06459774821996689, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.444005489349365 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.3877839431166645
Loss made of: CE 0.10007584095001221, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.566024303436279 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.378180281817913
Loss made of: CE 0.09901672601699829, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.551874160766602 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.437889491021633
Loss made of: CE 0.06647324562072754, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.14341926574707 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.49725991114974
Loss made of: CE 0.10057644546031952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.88760232925415 EntMin 0.0
Epoch 2, Class Loss=0.08775007724761963, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.08775007724761963, Class Loss=0.08775007724761963, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/97, Loss=4.117872988432646
Loss made of: CE 0.08340444415807724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8620622158050537 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.298784662038088
Loss made of: CE 0.06999875605106354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3140692710876465 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.682652469724417
Loss made of: CE 0.09244440495967865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.963233947753906 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.664444192871452
Loss made of: CE 0.0800899863243103, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.182343482971191 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.337416349351406
Loss made of: CE 0.06230343133211136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.599798202514648 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.353050310909748
Loss made of: CE 0.08452780544757843, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1562957763671875 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.556030650436878
Loss made of: CE 0.10431277751922607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.231912612915039 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.506249014288187
Loss made of: CE 0.08044155687093735, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.629124164581299 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.158751885592937
Loss made of: CE 0.0739409327507019, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.015923500061035 EntMin 0.0
Epoch 3, Class Loss=0.08555801212787628, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.08555801212787628, Class Loss=0.08555801212787628, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/97, Loss=4.2670301727950575
Loss made of: CE 0.09326145052909851, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.624736309051514 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.195831314846873
Loss made of: CE 0.05763953551650047, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9901909828186035 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.292817527800798
Loss made of: CE 0.06409582495689392, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.666635036468506 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.269715769588947
Loss made of: CE 0.09186945855617523, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698768615722656 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.302531735226512
Loss made of: CE 0.06397741287946701, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.946753978729248 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.25898280441761
Loss made of: CE 0.0684858113527298, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.075017929077148 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.316581936180592
Loss made of: CE 0.07152140885591507, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.302769660949707 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.227790383249522
Loss made of: CE 0.08780058473348618, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3109965324401855 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.120042667165398
Loss made of: CE 0.0590561144053936, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.266711235046387 EntMin 0.0
Epoch 4, Class Loss=0.08263843506574631, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.08263843506574631, Class Loss=0.08263843506574631, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/97, Loss=4.169819542765618
Loss made of: CE 0.08381474018096924, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4577956199646 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.267696268111467
Loss made of: CE 0.09823556244373322, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7222580909729 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.16031441166997
Loss made of: CE 0.08967497199773788, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.276048183441162 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.257415216788649
Loss made of: CE 0.05516147240996361, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.743349075317383 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.227348107844591
Loss made of: CE 0.09737171232700348, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.118426322937012 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.323380161076784
Loss made of: CE 0.08616779744625092, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.27763557434082 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.227693741768599
Loss made of: CE 0.054873257875442505, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.941638946533203 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.143108808994294
Loss made of: CE 0.07782528549432755, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.160811424255371 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.331636385619641
Loss made of: CE 0.09039555490016937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684045314788818 EntMin 0.0
Epoch 5, Class Loss=0.08225390315055847, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.08225390315055847, Class Loss=0.08225390315055847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/97, Loss=4.184223365783692
Loss made of: CE 0.06603848934173584, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7001166343688965 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.175917164608836
Loss made of: CE 0.16987335681915283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4685258865356445 EntMin 0.0
Epoch 6, Batch 30/97, Loss=3.998320144414902
Loss made of: CE 0.06624150276184082, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.481027603149414 EntMin 0.0
Epoch 6, Batch 40/97, Loss=3.994261871650815
Loss made of: CE 0.08965571224689484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.305925369262695 EntMin 0.0
Epoch 6, Batch 50/97, Loss=3.9510212771594526
Loss made of: CE 0.05503218621015549, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.435452938079834 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.06236484721303
Loss made of: CE 0.07324688136577606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9168431758880615 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.058235999196768
Loss made of: CE 0.08344363421201706, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.037116050720215 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.055146333202719
Loss made of: CE 0.0601082444190979, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.805882215499878 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.364972354471684
Loss made of: CE 0.09155014157295227, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.042867183685303 EntMin 0.0
Epoch 6, Class Loss=0.08158232271671295, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.08158232271671295, Class Loss=0.08158232271671295, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.7165121510624886
Loss made of: CE 0.5473884344100952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.434780597686768 EntMin 0.0
Epoch 1, Class Loss=0.4468885064125061, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.4468885064125061, Class Loss=0.4468885064125061, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.339669473469257
Loss made of: CE 0.506681501865387, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.077899932861328 EntMin 0.0
Epoch 2, Class Loss=0.40840479731559753, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.40840479731559753, Class Loss=0.40840479731559753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.0892541885375975
Loss made of: CE 0.24124214053153992, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.800963401794434 EntMin 0.0
Epoch 3, Class Loss=0.3828688859939575, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.3828688859939575, Class Loss=0.3828688859939575, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=5.21769908964634
Loss made of: CE 0.2786024510860443, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6099653244018555 EntMin 0.0
Epoch 4, Class Loss=0.36955925822257996, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.36955925822257996, Class Loss=0.36955925822257996, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=5.0010343253612515
Loss made of: CE 0.2436891496181488, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.572997093200684 EntMin 0.0
Epoch 5, Class Loss=0.31812605261802673, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.31812605261802673, Class Loss=0.31812605261802673, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.899462373554707
Loss made of: CE 0.16113269329071045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.926077842712402 EntMin 0.0
Epoch 6, Class Loss=0.3420505225658417, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3420505225658417, Class Loss=0.3420505225658417, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.455127575993538
Loss made of: CE 0.33749184012413025, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.11009407043457 EntMin 0.0
Epoch 1, Class Loss=0.4147769808769226, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.4147769808769226, Class Loss=0.4147769808769226, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.213225835561753
Loss made of: CE 0.39295023679733276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.871187210083008 EntMin 0.0
Epoch 2, Class Loss=0.3733595609664917, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.3733595609664917, Class Loss=0.3733595609664917, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.079802477359772
Loss made of: CE 0.22381190955638885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6052350997924805 EntMin 0.0
Epoch 3, Class Loss=0.3633670508861542, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.3633670508861542, Class Loss=0.3633670508861542, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.914693018794059
Loss made of: CE 0.4369370937347412, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.480759620666504 EntMin 0.0
Epoch 4, Class Loss=0.34459036588668823, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.34459036588668823, Class Loss=0.34459036588668823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.912408842146396
Loss made of: CE 0.326151967048645, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3531389236450195 EntMin 0.0
Epoch 5, Class Loss=0.3236045837402344, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.3236045837402344, Class Loss=0.3236045837402344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.740583524107933
Loss made of: CE 0.25008565187454224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679623603820801 EntMin 0.0
Epoch 6, Class Loss=0.3242933750152588, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.3242933750152588, Class Loss=0.3242933750152588, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.593183392286301
Loss made of: CE 0.6534550786018372, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.593274116516113 EntMin 0.0
Epoch 1, Class Loss=0.4342782199382782, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.4342782199382782, Class Loss=0.4342782199382782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/12, Loss=5.203874708712101
Loss made of: CE 0.29136568307876587, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4392523765563965 EntMin 0.0
Epoch 2, Class Loss=0.4151647686958313, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.4151647686958313, Class Loss=0.4151647686958313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/12, Loss=5.119860380887985
Loss made of: CE 0.45152291655540466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.899965286254883 EntMin 0.0
Epoch 3, Class Loss=0.41768479347229004, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.41768479347229004, Class Loss=0.41768479347229004, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/12, Loss=5.135511726140976
Loss made of: CE 0.2667939364910126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.725048542022705 EntMin 0.0
Epoch 4, Class Loss=0.3983559012413025, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.3983559012413025, Class Loss=0.3983559012413025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/12, Loss=4.949656201899051
Loss made of: CE 0.4576929807662964, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.938261032104492 EntMin 0.0
Epoch 5, Class Loss=0.40783220529556274, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.40783220529556274, Class Loss=0.40783220529556274, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/12, Loss=5.045788888633251
Loss made of: CE 0.48491281270980835, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.483846187591553 EntMin 0.0
Epoch 6, Class Loss=0.3614271283149719, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.3614271283149719, Class Loss=0.3614271283149719, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.6623026132583618, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.798136
Mean Acc: 0.534959
FreqW Acc: 0.706530
Mean IoU: 0.402151
Class IoU:
	class 0: 0.8357794
	class 1: 0.84040534
	class 2: 0.3248721
	class 3: 0.61559606
	class 4: 0.64676625
	class 5: 0.6547036
	class 6: 0.7768587
	class 7: 0.805905
	class 8: 0.6878945
	class 9: 0.016425256
	class 10: 0.0
	class 11: 1.40989005e-05
	class 12: 0.037147105
	class 13: 0.18708389
	class 14: 0.0
	class 15: 0.40712392
	class 16: 0.0
Class Acc:
	class 0: 0.88614315
	class 1: 0.9254054
	class 2: 0.898382
	class 3: 0.6428302
	class 4: 0.7952395
	class 5: 0.70635116
	class 6: 0.80667716
	class 7: 0.90925413
	class 8: 0.7271825
	class 9: 0.01663195
	class 10: 0.0
	class 11: 1.40989005e-05
	class 12: 0.037257783
	class 13: 0.8022288
	class 14: 0.0
	class 15: 0.9407134
	class 16: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 25, step: 5
select part of clients to conduct local training
[24, 7, 4, 12]
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=7.364297795295715
Loss made of: CE 0.3913179636001587, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.652064323425293 EntMin 0.0
Epoch 1, Class Loss=0.6192032694816589, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.6192032694816589, Class Loss=0.6192032694816589, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=6.066313633322716
Loss made of: CE 0.49046239256858826, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.739999771118164 EntMin 0.0
Epoch 2, Class Loss=0.524753987789154, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.524753987789154, Class Loss=0.524753987789154, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.803543588519096
Loss made of: CE 0.4390345513820648, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.495881080627441 EntMin 0.0
Epoch 3, Class Loss=0.46130260825157166, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.46130260825157166, Class Loss=0.46130260825157166, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.5683588027954105
Loss made of: CE 0.460080087184906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8599748611450195 EntMin 0.0
Epoch 4, Class Loss=0.41698092222213745, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.41698092222213745, Class Loss=0.41698092222213745, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.278174570202827
Loss made of: CE 0.3698909282684326, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.467637062072754 EntMin 0.0
Epoch 5, Class Loss=0.38948318362236023, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.38948318362236023, Class Loss=0.38948318362236023, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.101838645339012
Loss made of: CE 0.3295193910598755, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.826323509216309 EntMin 0.0
Epoch 6, Class Loss=0.3607216477394104, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.3607216477394104, Class Loss=0.3607216477394104, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.0164768695831299, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=1.0164768695831299, Class Loss=1.0164768695831299, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.8720996379852295, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.8720996379852295, Class Loss=0.8720996379852295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6306664943695068, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.6306664943695068, Class Loss=0.6306664943695068, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5595687627792358, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.5595687627792358, Class Loss=0.5595687627792358, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.4794963002204895, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4794963002204895, Class Loss=0.4794963002204895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4449164867401123, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.4449164867401123, Class Loss=0.4449164867401123, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=7.113745260238647
Loss made of: CE 0.5147430300712585, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.873150825500488 EntMin 0.0
Epoch 1, Class Loss=0.6625959277153015, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.6625959277153015, Class Loss=0.6625959277153015, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=6.069327995181084
Loss made of: CE 0.45095908641815186, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.805597305297852 EntMin 0.0
Epoch 2, Class Loss=0.5471569299697876, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.5471569299697876, Class Loss=0.5471569299697876, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.649550926685333
Loss made of: CE 0.5055505037307739, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.877488136291504 EntMin 0.0
Epoch 3, Class Loss=0.47550711035728455, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.47550711035728455, Class Loss=0.47550711035728455, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.51141523718834
Loss made of: CE 0.4436057209968567, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.419108867645264 EntMin 0.0
Epoch 4, Class Loss=0.42607542872428894, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.42607542872428894, Class Loss=0.42607542872428894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.184013855457306
Loss made of: CE 0.4564667344093323, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.458259582519531 EntMin 0.0
Epoch 5, Class Loss=0.3871113061904907, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.3871113061904907, Class Loss=0.3871113061904907, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.046016451716423
Loss made of: CE 0.35379838943481445, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7369561195373535 EntMin 0.0
Epoch 6, Class Loss=0.36506205797195435, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.36506205797195435, Class Loss=0.36506205797195435, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=7.323191630840301
Loss made of: CE 0.6593837738037109, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1088995933532715 EntMin 0.0
Epoch 1, Class Loss=0.629464864730835, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.629464864730835, Class Loss=0.629464864730835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=6.190939390659333
Loss made of: CE 0.5828139781951904, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.68458366394043 EntMin 0.0
Epoch 2, Class Loss=0.5315237641334534, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.5315237641334534, Class Loss=0.5315237641334534, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.68646177649498
Loss made of: CE 0.505998969078064, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.210396766662598 EntMin 0.0
Epoch 3, Class Loss=0.46860453486442566, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.46860453486442566, Class Loss=0.46860453486442566, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.473228317499161
Loss made of: CE 0.4110763967037201, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.580183029174805 EntMin 0.0
Epoch 4, Class Loss=0.42723414301872253, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.42723414301872253, Class Loss=0.42723414301872253, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.105305936932564
Loss made of: CE 0.374051570892334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5668745040893555 EntMin 0.0
Epoch 5, Class Loss=0.40506261587142944, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.40506261587142944, Class Loss=0.40506261587142944, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.127868902683258
Loss made of: CE 0.34240731596946716, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5106306076049805 EntMin 0.0
Epoch 6, Class Loss=0.36481040716171265, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.36481040716171265, Class Loss=0.36481040716171265, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8809932470321655, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.779233
Mean Acc: 0.427313
FreqW Acc: 0.665322
Mean IoU: 0.318541
Class IoU:
	class 0: 0.81388575
	class 1: 0.7660841
	class 2: 0.32531965
	class 3: 0.49720183
	class 4: 0.58695006
	class 5: 0.5829496
	class 6: 0.51131314
	class 7: 0.77288926
	class 8: 0.54264265
	class 9: 0.008758872
	class 10: 0.0
	class 11: 0.00028831087
	class 12: 0.029512402
	class 13: 0.15810433
	class 14: 0.0
	class 15: 0.45638287
	class 16: 0.0
	class 17: 0.0
	class 18: 1.6929592e-06
Class Acc:
	class 0: 0.91073817
	class 1: 0.829697
	class 2: 0.85800815
	class 3: 0.5133009
	class 4: 0.7180455
	class 5: 0.60521066
	class 6: 0.52522135
	class 7: 0.8610737
	class 8: 0.55805135
	class 9: 0.008844443
	class 10: 0.0
	class 11: 0.0002883225
	class 12: 0.02962354
	class 13: 0.7698174
	class 14: 0.0
	class 15: 0.93102235
	class 16: 0.0
	class 17: 0.0
	class 18: 1.6929592e-06

federated global round: 26, step: 5
select part of clients to conduct local training
[6, 28, 20, 4]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.2648856967687605
Loss made of: CE 0.4624146819114685, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.789802551269531 EntMin 0.0
Epoch 1, Class Loss=0.40698134899139404, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.40698134899139404, Class Loss=0.40698134899139404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.095333206653595
Loss made of: CE 0.3987036347389221, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.233692646026611 EntMin 0.0
Epoch 2, Class Loss=0.3932758569717407, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3932758569717407, Class Loss=0.3932758569717407, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=4.981984782218933
Loss made of: CE 0.3574211299419403, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.612172603607178 EntMin 0.0
Epoch 3, Class Loss=0.3708553910255432, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.3708553910255432, Class Loss=0.3708553910255432, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=4.707180321216583
Loss made of: CE 0.32145997881889343, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.621457099914551 EntMin 0.0
Epoch 4, Class Loss=0.339466392993927, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.339466392993927, Class Loss=0.339466392993927, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=4.952624985575676
Loss made of: CE 0.378446102142334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9381866455078125 EntMin 0.0
Epoch 5, Class Loss=0.32022660970687866, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.32022660970687866, Class Loss=0.32022660970687866, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=4.628139075636864
Loss made of: CE 0.2762092351913452, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.969135284423828 EntMin 0.0
Epoch 6, Class Loss=0.32000577449798584, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.32000577449798584, Class Loss=0.32000577449798584, Reg Loss=0.0
Current Client Index:  28
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.8352617025375366, Reg Loss=0.0
Clinet index 28, End of Epoch 1/6, Average Loss=0.8352617025375366, Class Loss=0.8352617025375366, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.6771215796470642, Reg Loss=0.0
Clinet index 28, End of Epoch 2/6, Average Loss=0.6771215796470642, Class Loss=0.6771215796470642, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.6653717756271362, Reg Loss=0.0
Clinet index 28, End of Epoch 3/6, Average Loss=0.6653717756271362, Class Loss=0.6653717756271362, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.5415862202644348, Reg Loss=0.0
Clinet index 28, End of Epoch 4/6, Average Loss=0.5415862202644348, Class Loss=0.5415862202644348, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.49199000000953674, Reg Loss=0.0
Clinet index 28, End of Epoch 5/6, Average Loss=0.49199000000953674, Class Loss=0.49199000000953674, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.40824347734451294, Reg Loss=0.0
Clinet index 28, End of Epoch 6/6, Average Loss=0.40824347734451294, Class Loss=0.40824347734451294, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.783671498298645, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.783671498298645, Class Loss=0.783671498298645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.7243527770042419, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.7243527770042419, Class Loss=0.7243527770042419, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.6213100552558899, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.6213100552558899, Class Loss=0.6213100552558899, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.5559977889060974, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.5559977889060974, Class Loss=0.5559977889060974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.5225451588630676, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.5225451588630676, Class Loss=0.5225451588630676, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.45364874601364136, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.45364874601364136, Class Loss=0.45364874601364136, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.176712447404862
Loss made of: CE 0.31745144724845886, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2450056076049805 EntMin 0.0
Epoch 1, Class Loss=0.41381847858428955, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.41381847858428955, Class Loss=0.41381847858428955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=4.955732914805412
Loss made of: CE 0.36085838079452515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.797850608825684 EntMin 0.0
Epoch 2, Class Loss=0.38050293922424316, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.38050293922424316, Class Loss=0.38050293922424316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=4.869473025202751
Loss made of: CE 0.34537839889526367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2291083335876465 EntMin 0.0
Epoch 3, Class Loss=0.3698146939277649, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.3698146939277649, Class Loss=0.3698146939277649, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=4.950165316462517
Loss made of: CE 0.38657045364379883, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.151322364807129 EntMin 0.0
Epoch 4, Class Loss=0.33615559339523315, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.33615559339523315, Class Loss=0.33615559339523315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.750328552722931
Loss made of: CE 0.38318634033203125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9571919441223145 EntMin 0.0
Epoch 5, Class Loss=0.327170193195343, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.327170193195343, Class Loss=0.327170193195343, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=4.528828856348992
Loss made of: CE 0.3352088928222656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698450565338135 EntMin 0.0
Epoch 6, Class Loss=0.30906882882118225, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.30906882882118225, Class Loss=0.30906882882118225, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8185392618179321, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.781398
Mean Acc: 0.415835
FreqW Acc: 0.662962
Mean IoU: 0.310893
Class IoU:
	class 0: 0.8117231
	class 1: 0.74396414
	class 2: 0.3283877
	class 3: 0.45590982
	class 4: 0.5752652
	class 5: 0.5360979
	class 6: 0.43509272
	class 7: 0.7784988
	class 8: 0.53808564
	class 9: 0.011836208
	class 10: 0.0
	class 11: 0.00072173245
	class 12: 0.0373898
	class 13: 0.16376185
	class 14: 0.0
	class 15: 0.4826723
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0075528217
Class Acc:
	class 0: 0.91868746
	class 1: 0.7945703
	class 2: 0.859261
	class 3: 0.46641913
	class 4: 0.69477427
	class 5: 0.55558443
	class 6: 0.44426587
	class 7: 0.8621707
	class 8: 0.5527768
	class 9: 0.01201525
	class 10: 0.0
	class 11: 0.0007218637
	class 12: 0.037584845
	class 13: 0.76432055
	class 14: 0.0
	class 15: 0.93014944
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0075613204

federated global round: 27, step: 5
select part of clients to conduct local training
[24, 8, 26, 0]
Current Client Index:  24
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.872810024023056
Loss made of: CE 0.33453354239463806, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.519790172576904 EntMin 0.0
Epoch 1, Class Loss=0.3828779458999634, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.3828779458999634, Class Loss=0.3828779458999634, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/12, Loss=5.057144796848297
Loss made of: CE 0.3087560832500458, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.819683074951172 EntMin 0.0
Epoch 2, Class Loss=0.3427218794822693, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.3427218794822693, Class Loss=0.3427218794822693, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/12, Loss=4.962615951895714
Loss made of: CE 0.2846917510032654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.323554515838623 EntMin 0.0
Epoch 3, Class Loss=0.3289503753185272, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.3289503753185272, Class Loss=0.3289503753185272, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/12, Loss=4.929275014996529
Loss made of: CE 0.34504589438438416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.380743026733398 EntMin 0.0
Epoch 4, Class Loss=0.311676561832428, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.311676561832428, Class Loss=0.311676561832428, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/12, Loss=4.669978198409081
Loss made of: CE 0.2973523736000061, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.220558166503906 EntMin 0.0
Epoch 5, Class Loss=0.2914697527885437, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.2914697527885437, Class Loss=0.2914697527885437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/12, Loss=4.610765618085861
Loss made of: CE 0.25731366872787476, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.245357513427734 EntMin 0.0
Epoch 6, Class Loss=0.29232412576675415, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.29232412576675415, Class Loss=0.29232412576675415, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.5887231826782227, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.5887231826782227, Class Loss=0.5887231826782227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.5732777118682861, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.5732777118682861, Class Loss=0.5732777118682861, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.5124649405479431, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5124649405479431, Class Loss=0.5124649405479431, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.45511725544929504, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.45511725544929504, Class Loss=0.45511725544929504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.38897475600242615, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.38897475600242615, Class Loss=0.38897475600242615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.37028825283050537, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.37028825283050537, Class Loss=0.37028825283050537, Reg Loss=0.0
Current Client Index:  26
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.771702173352241
Loss made of: CE 0.392661988735199, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.795716285705566 EntMin 0.0
Epoch 1, Class Loss=0.3947065472602844, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.3947065472602844, Class Loss=0.3947065472602844, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=5.0651244789361956
Loss made of: CE 0.4286085069179535, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.299759387969971 EntMin 0.0
Epoch 2, Class Loss=0.3585366904735565, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.3585366904735565, Class Loss=0.3585366904735565, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=4.9794631630182264
Loss made of: CE 0.3782235383987427, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.794785022735596 EntMin 0.0
Epoch 3, Class Loss=0.33534544706344604, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.33534544706344604, Class Loss=0.33534544706344604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=4.907364869117737
Loss made of: CE 0.29743003845214844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.632789611816406 EntMin 0.0
Epoch 4, Class Loss=0.3199266493320465, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.3199266493320465, Class Loss=0.3199266493320465, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.541017785668373
Loss made of: CE 0.31396323442459106, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.934961318969727 EntMin 0.0
Epoch 5, Class Loss=0.2926069498062134, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.2926069498062134, Class Loss=0.2926069498062134, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.639651797711849
Loss made of: CE 0.2834029495716095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.279397010803223 EntMin 0.0
Epoch 6, Class Loss=0.28339606523513794, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.28339606523513794, Class Loss=0.28339606523513794, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.441159111261368
Loss made of: CE 0.34846973419189453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4854936599731445 EntMin 0.0
Epoch 1, Class Loss=0.383786678314209, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.383786678314209, Class Loss=0.383786678314209, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=4.8915449500083925
Loss made of: CE 0.32439398765563965, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.233072280883789 EntMin 0.0
Epoch 2, Class Loss=0.34870290756225586, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.34870290756225586, Class Loss=0.34870290756225586, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=4.8684832662343975
Loss made of: CE 0.39295467734336853, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.414319038391113 EntMin 0.0
Epoch 3, Class Loss=0.3284640610218048, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3284640610218048, Class Loss=0.3284640610218048, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=4.709018337726593
Loss made of: CE 0.2765709459781647, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623603820800781 EntMin 0.0
Epoch 4, Class Loss=0.2966177463531494, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.2966177463531494, Class Loss=0.2966177463531494, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.648153603076935
Loss made of: CE 0.26958245038986206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.549471378326416 EntMin 0.0
Epoch 5, Class Loss=0.2830846309661865, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.2830846309661865, Class Loss=0.2830846309661865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.525351053476333
Loss made of: CE 0.2872874140739441, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8552427291870117 EntMin 0.0
Epoch 6, Class Loss=0.27069365978240967, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.27069365978240967, Class Loss=0.27069365978240967, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8220747709274292, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.786565
Mean Acc: 0.461525
FreqW Acc: 0.688195
Mean IoU: 0.342448
Class IoU:
	class 0: 0.82851934
	class 1: 0.7797691
	class 2: 0.32327148
	class 3: 0.5363671
	class 4: 0.58865285
	class 5: 0.6353505
	class 6: 0.4795056
	class 7: 0.77281517
	class 8: 0.65933317
	class 9: 0.004802841
	class 10: 0.0
	class 11: 0.001123372
	class 12: 0.067954
	class 13: 0.16183119
	class 14: 0.0
	class 15: 0.5450051
	class 16: 0.0
	class 17: 0.0
	class 18: 0.1222097
Class Acc:
	class 0: 0.90632486
	class 1: 0.84108794
	class 2: 0.87647814
	class 3: 0.5580808
	class 4: 0.73034585
	class 5: 0.67841154
	class 6: 0.49142283
	class 7: 0.8503717
	class 8: 0.6998305
	class 9: 0.0048516816
	class 10: 0.0
	class 11: 0.0011239174
	class 12: 0.06844104
	class 13: 0.7590295
	class 14: 0.0
	class 15: 0.9066427
	class 16: 0.0
	class 17: 0.0
	class 18: 0.39653695

federated global round: 28, step: 5
select part of clients to conduct local training
[8, 9, 16, 3]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.6938539147377014, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.6938539147377014, Class Loss=0.6938539147377014, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.6393854022026062, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.6393854022026062, Class Loss=0.6393854022026062, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.5906614661216736, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5906614661216736, Class Loss=0.5906614661216736, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5358945727348328, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5358945727348328, Class Loss=0.5358945727348328, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.4804145395755768, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.4804145395755768, Class Loss=0.4804145395755768, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.4646497070789337, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.4646497070789337, Class Loss=0.4646497070789337, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.7437220215797424, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.7437220215797424, Class Loss=0.7437220215797424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.5910183787345886, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.5910183787345886, Class Loss=0.5910183787345886, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.5203001499176025, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5203001499176025, Class Loss=0.5203001499176025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.5288159251213074, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.5288159251213074, Class Loss=0.5288159251213074, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.38965341448783875, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.38965341448783875, Class Loss=0.38965341448783875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.39091983437538147, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.39091983437538147, Class Loss=0.39091983437538147, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.823316976428032
Loss made of: CE 0.2843458652496338, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.646146774291992 EntMin 0.0
Epoch 1, Class Loss=0.32064366340637207, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.32064366340637207, Class Loss=0.32064366340637207, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=4.486152398586273
Loss made of: CE 0.3551274240016937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.65393590927124 EntMin 0.0
Epoch 2, Class Loss=0.3016820251941681, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.3016820251941681, Class Loss=0.3016820251941681, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.4026245206594465
Loss made of: CE 0.24376600980758667, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8821473121643066 EntMin 0.0
Epoch 3, Class Loss=0.2713691294193268, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.2713691294193268, Class Loss=0.2713691294193268, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.313558270037174
Loss made of: CE 0.2318652719259262, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9992663860321045 EntMin 0.0
Epoch 4, Class Loss=0.2681702673435211, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.2681702673435211, Class Loss=0.2681702673435211, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.322739696502685
Loss made of: CE 0.2520994246006012, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7834746837615967 EntMin 0.0
Epoch 5, Class Loss=0.25455543398857117, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.25455543398857117, Class Loss=0.25455543398857117, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.340077582001686
Loss made of: CE 0.24583041667938232, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.178717136383057 EntMin 0.0
Epoch 6, Class Loss=0.2581790089607239, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.2581790089607239, Class Loss=0.2581790089607239, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.738895776867866
Loss made of: CE 0.25988686084747314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.300601005554199 EntMin 0.0
Epoch 1, Class Loss=0.3022816777229309, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.3022816777229309, Class Loss=0.3022816777229309, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=4.726881539821624
Loss made of: CE 0.32764968276023865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.735621452331543 EntMin 0.0
Epoch 2, Class Loss=0.3069005310535431, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.3069005310535431, Class Loss=0.3069005310535431, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.682021389901638
Loss made of: CE 0.32245397567749023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.846988677978516 EntMin 0.0
Epoch 3, Class Loss=0.28930047154426575, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.28930047154426575, Class Loss=0.28930047154426575, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.5601985022425655
Loss made of: CE 0.25946271419525146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.020115375518799 EntMin 0.0
Epoch 4, Class Loss=0.2694091498851776, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.2694091498851776, Class Loss=0.2694091498851776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.553360593318939
Loss made of: CE 0.24748878180980682, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464611530303955 EntMin 0.0
Epoch 5, Class Loss=0.2638973593711853, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.2638973593711853, Class Loss=0.2638973593711853, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.478308394551277
Loss made of: CE 0.21336989104747772, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.887251853942871 EntMin 0.0
Epoch 6, Class Loss=0.25127938389778137, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.25127938389778137, Class Loss=0.25127938389778137, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7782430052757263, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.792012
Mean Acc: 0.451927
FreqW Acc: 0.686597
Mean IoU: 0.337789
Class IoU:
	class 0: 0.829242
	class 1: 0.7675531
	class 2: 0.31871906
	class 3: 0.5243027
	class 4: 0.58474135
	class 5: 0.5966532
	class 6: 0.4636735
	class 7: 0.78095657
	class 8: 0.6470947
	class 9: 0.0069339736
	class 10: 0.0
	class 11: 0.0015877667
	class 12: 0.06063191
	class 13: 0.16902734
	class 14: 0.0
	class 15: 0.53024113
	class 16: 0.0
	class 17: 0.0
	class 18: 0.13662829
Class Acc:
	class 0: 0.91777724
	class 1: 0.82268035
	class 2: 0.88200843
	class 3: 0.5418788
	class 4: 0.7119113
	class 5: 0.6317899
	class 6: 0.4733919
	class 7: 0.87147707
	class 8: 0.6840952
	class 9: 0.0070793475
	class 10: 0.0
	class 11: 0.0015889461
	class 12: 0.061041392
	class 13: 0.74245626
	class 14: 0.0
	class 15: 0.9153077
	class 16: 0.0
	class 17: 0.0
	class 18: 0.32213122

federated global round: 29, step: 5
select part of clients to conduct local training
[29, 27, 26, 18]
Current Client Index:  29
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.192399321496486
Loss made of: CE 0.33385515213012695, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.140900611877441 EntMin 0.0
Epoch 1, Class Loss=0.31687718629837036, Reg Loss=0.0
Clinet index 29, End of Epoch 1/6, Average Loss=0.31687718629837036, Class Loss=0.31687718629837036, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=4.919488656520843
Loss made of: CE 0.2526472508907318, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9239091873168945 EntMin 0.0
Epoch 2, Class Loss=0.3079676628112793, Reg Loss=0.0
Clinet index 29, End of Epoch 2/6, Average Loss=0.3079676628112793, Class Loss=0.3079676628112793, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.494545134902
Loss made of: CE 0.2521914839744568, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.194648742675781 EntMin 0.0
Epoch 3, Class Loss=0.2797604203224182, Reg Loss=0.0
Clinet index 29, End of Epoch 3/6, Average Loss=0.2797604203224182, Class Loss=0.2797604203224182, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.678579902648925
Loss made of: CE 0.2381012737751007, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1920928955078125 EntMin 0.0
Epoch 4, Class Loss=0.27183064818382263, Reg Loss=0.0
Clinet index 29, End of Epoch 4/6, Average Loss=0.27183064818382263, Class Loss=0.27183064818382263, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.505612061917782
Loss made of: CE 0.2792050540447235, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0492448806762695 EntMin 0.0
Epoch 5, Class Loss=0.258786678314209, Reg Loss=0.0
Clinet index 29, End of Epoch 5/6, Average Loss=0.258786678314209, Class Loss=0.258786678314209, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.379257941246033
Loss made of: CE 0.21812450885772705, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.791952133178711 EntMin 0.0
Epoch 6, Class Loss=0.2517663538455963, Reg Loss=0.0
Clinet index 29, End of Epoch 6/6, Average Loss=0.2517663538455963, Class Loss=0.2517663538455963, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.642604649066925, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.642604649066925, Class Loss=0.642604649066925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.6187946200370789, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.6187946200370789, Class Loss=0.6187946200370789, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.5028857588768005, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.5028857588768005, Class Loss=0.5028857588768005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.46397000551223755, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.46397000551223755, Class Loss=0.46397000551223755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.44454777240753174, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.44454777240753174, Class Loss=0.44454777240753174, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.4359806776046753, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.4359806776046753, Class Loss=0.4359806776046753, Reg Loss=0.0
Current Client Index:  26
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.249148479104042
Loss made of: CE 0.28192946314811707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4156389236450195 EntMin 0.0
Epoch 1, Class Loss=0.31432411074638367, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.31432411074638367, Class Loss=0.31432411074638367, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/12, Loss=4.621959072351456
Loss made of: CE 0.35799846053123474, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.126875877380371 EntMin 0.0
Epoch 2, Class Loss=0.2984599471092224, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.2984599471092224, Class Loss=0.2984599471092224, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/12, Loss=4.6143687382340435
Loss made of: CE 0.31418517231941223, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.780954837799072 EntMin 0.0
Epoch 3, Class Loss=0.28577953577041626, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.28577953577041626, Class Loss=0.28577953577041626, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/12, Loss=4.452422839403153
Loss made of: CE 0.27303045988082886, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.090079307556152 EntMin 0.0
Epoch 4, Class Loss=0.27916717529296875, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.27916717529296875, Class Loss=0.27916717529296875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/12, Loss=4.204534353315831
Loss made of: CE 0.26458707451820374, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291729927062988 EntMin 0.0
Epoch 5, Class Loss=0.2728312909603119, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.2728312909603119, Class Loss=0.2728312909603119, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/12, Loss=4.314501191675663
Loss made of: CE 0.2711118459701538, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.354279518127441 EntMin 0.0
Epoch 6, Class Loss=0.26801857352256775, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.26801857352256775, Class Loss=0.26801857352256775, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.285588929057122
Loss made of: CE 0.2855212390422821, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7857561111450195 EntMin 0.0
Epoch 1, Class Loss=0.3076978921890259, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.3076978921890259, Class Loss=0.3076978921890259, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=4.765378785133362
Loss made of: CE 0.28723013401031494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.502496719360352 EntMin 0.0
Epoch 2, Class Loss=0.2933511435985565, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.2933511435985565, Class Loss=0.2933511435985565, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.5897114217281345
Loss made of: CE 0.24963311851024628, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389064788818359 EntMin 0.0
Epoch 3, Class Loss=0.26779869198799133, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.26779869198799133, Class Loss=0.26779869198799133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.522172118723392
Loss made of: CE 0.26178860664367676, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.156459331512451 EntMin 0.0
Epoch 4, Class Loss=0.2525613605976105, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.2525613605976105, Class Loss=0.2525613605976105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.460454429686069
Loss made of: CE 0.25405192375183105, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.008764266967773 EntMin 0.0
Epoch 5, Class Loss=0.2587897479534149, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.2587897479534149, Class Loss=0.2587897479534149, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.371332901716232
Loss made of: CE 0.27468788623809814, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.24682092666626 EntMin 0.0
Epoch 6, Class Loss=0.25336241722106934, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.25336241722106934, Class Loss=0.25336241722106934, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.805443525314331, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.775486
Mean Acc: 0.460330
FreqW Acc: 0.686601
Mean IoU: 0.341256
Class IoU:
	class 0: 0.8253323
	class 1: 0.7915126
	class 2: 0.3295994
	class 3: 0.5388624
	class 4: 0.5859561
	class 5: 0.6240203
	class 6: 0.45615008
	class 7: 0.762669
	class 8: 0.6729389
	class 9: 0.002879968
	class 10: 0.0
	class 11: 0.0006970152
	class 12: 0.06424998
	class 13: 0.16755335
	class 14: 0.0
	class 15: 0.5655352
	class 16: 0.0
	class 17: 0.0
	class 18: 0.09590216
Class Acc:
	class 0: 0.8900845
	class 1: 0.8532725
	class 2: 0.86905074
	class 3: 0.55926746
	class 4: 0.7081228
	class 5: 0.6650281
	class 6: 0.46663496
	class 7: 0.83036417
	class 8: 0.718882
	class 9: 0.0028962865
	class 10: 0.0
	class 11: 0.00069719064
	class 12: 0.06449072
	class 13: 0.6924511
	class 14: 0.0
	class 15: 0.9043671
	class 16: 0.0
	class 17: 0.0
	class 18: 0.5206683

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 30, step: 6
select part of clients to conduct local training
[10, 30, 4, 33]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=14.45224016904831
Loss made of: CE 0.9262810945510864, LKD 0.0, LDE 0.0, LReg 0.0, POD 12.589990615844727 EntMin 0.0
Epoch 1, Class Loss=1.187296748161316, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=1.187296748161316, Class Loss=1.187296748161316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=12.281300151348114
Loss made of: CE 0.672956645488739, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.79047679901123 EntMin 0.0
Epoch 2, Class Loss=0.8269071578979492, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.8269071578979492, Class Loss=0.8269071578979492, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=11.392980867624283
Loss made of: CE 0.5735172033309937, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.905391693115234 EntMin 0.0
Epoch 3, Class Loss=0.5703091025352478, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5703091025352478, Class Loss=0.5703091025352478, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.731455239653588
Loss made of: CE 0.44800132513046265, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.327207565307617 EntMin 0.0
Epoch 4, Class Loss=0.4404679238796234, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.4404679238796234, Class Loss=0.4404679238796234, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.355675345659256
Loss made of: CE 0.36528855562210083, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.59073257446289 EntMin 0.0
Epoch 5, Class Loss=0.3612746596336365, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.3612746596336365, Class Loss=0.3612746596336365, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.768617928028107
Loss made of: CE 0.3523901402950287, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.475687980651855 EntMin 0.0
Epoch 6, Class Loss=0.3118700087070465, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.3118700087070465, Class Loss=0.3118700087070465, Reg Loss=0.0
Current Client Index:  30
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=9.622602611780167
Loss made of: CE 0.7071601748466492, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.278907775878906 EntMin 0.0
Epoch 1, Class Loss=0.5707569122314453, Reg Loss=0.0
Clinet index 30, End of Epoch 1/6, Average Loss=0.5707569122314453, Class Loss=0.5707569122314453, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=8.289075297117233
Loss made of: CE 0.42310380935668945, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.285014629364014 EntMin 0.0
Epoch 2, Class Loss=0.46614521741867065, Reg Loss=0.0
Clinet index 30, End of Epoch 2/6, Average Loss=0.46614521741867065, Class Loss=0.46614521741867065, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=7.087779745459557
Loss made of: CE 0.43192243576049805, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.05423641204834 EntMin 0.0
Epoch 3, Class Loss=0.40372955799102783, Reg Loss=0.0
Clinet index 30, End of Epoch 3/6, Average Loss=0.40372955799102783, Class Loss=0.40372955799102783, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=6.727716490626335
Loss made of: CE 0.44402289390563965, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.463780879974365 EntMin 0.0
Epoch 4, Class Loss=0.36346539855003357, Reg Loss=0.0
Clinet index 30, End of Epoch 4/6, Average Loss=0.36346539855003357, Class Loss=0.36346539855003357, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=6.521174523234367
Loss made of: CE 0.42608997225761414, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.872440338134766 EntMin 0.0
Epoch 5, Class Loss=0.33831268548965454, Reg Loss=0.0
Clinet index 30, End of Epoch 5/6, Average Loss=0.33831268548965454, Class Loss=0.33831268548965454, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=6.242683818936348
Loss made of: CE 0.3902316987514496, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.051438331604004 EntMin 0.0
Epoch 6, Class Loss=0.3314780592918396, Reg Loss=0.0
Clinet index 30, End of Epoch 6/6, Average Loss=0.3314780592918396, Class Loss=0.3314780592918396, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=14.128088343143464
Loss made of: CE 1.144866943359375, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.487136840820312 EntMin 0.0
Epoch 1, Class Loss=1.165510654449463, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=1.165510654449463, Class Loss=1.165510654449463, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=11.724092727899551
Loss made of: CE 0.7018330097198486, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.8937406539917 EntMin 0.0
Epoch 2, Class Loss=0.786993145942688, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.786993145942688, Class Loss=0.786993145942688, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/12, Loss=11.188238993287086
Loss made of: CE 0.6157240867614746, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.636711120605469 EntMin 0.0
Epoch 3, Class Loss=0.5738561749458313, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.5738561749458313, Class Loss=0.5738561749458313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.35502586066723
Loss made of: CE 0.4578884541988373, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.365127563476562 EntMin 0.0
Epoch 4, Class Loss=0.4444309175014496, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.4444309175014496, Class Loss=0.4444309175014496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.021970999240875
Loss made of: CE 0.30863794684410095, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.946859359741211 EntMin 0.0
Epoch 5, Class Loss=0.3585774302482605, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.3585774302482605, Class Loss=0.3585774302482605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.255492433905602
Loss made of: CE 0.287189245223999, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.85269546508789 EntMin 0.0
Epoch 6, Class Loss=0.32302170991897583, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.32302170991897583, Class Loss=0.32302170991897583, Reg Loss=0.0
Current Client Index:  33
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=14.33171350955963
Loss made of: CE 1.0147393941879272, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.709582328796387 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=1.1739814281463623, Reg Loss=0.0
Clinet index 33, End of Epoch 1/6, Average Loss=1.1739814281463623, Class Loss=1.1739814281463623, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=12.398513334989548
Loss made of: CE 0.7008472681045532, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.918893814086914 EntMin 0.0
Epoch 2, Class Loss=0.7979217767715454, Reg Loss=0.0
Clinet index 33, End of Epoch 2/6, Average Loss=0.7979217767715454, Class Loss=0.7979217767715454, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 3, Batch 10/12, Loss=11.647488075494767
Loss made of: CE 0.5192968845367432, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.037562370300293 EntMin 0.0
Epoch 3, Class Loss=0.5858154296875, Reg Loss=0.0
Clinet index 33, End of Epoch 3/6, Average Loss=0.5858154296875, Class Loss=0.5858154296875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.835282605886459
Loss made of: CE 0.47592687606811523, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.704352378845215 EntMin 0.0
Epoch 4, Class Loss=0.4482555389404297, Reg Loss=0.0
Clinet index 33, End of Epoch 4/6, Average Loss=0.4482555389404297, Class Loss=0.4482555389404297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.512686491012573
Loss made of: CE 0.3349641263484955, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.618720054626465 EntMin 0.0
Epoch 5, Class Loss=0.3586430251598358, Reg Loss=0.0
Clinet index 33, End of Epoch 5/6, Average Loss=0.3586430251598358, Class Loss=0.3586430251598358, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.793404358625413
Loss made of: CE 0.30395257472991943, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.614069938659668 EntMin 0.0
Epoch 6, Class Loss=0.3033766448497772, Reg Loss=0.0
Clinet index 33, End of Epoch 6/6, Average Loss=0.3033766448497772, Class Loss=0.3033766448497772, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.9357767701148987, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.760932
Mean Acc: 0.346648
FreqW Acc: 0.629298
Mean IoU: 0.247876
Class IoU:
	class 0: 0.79511064
	class 1: 0.67449814
	class 2: 0.3419099
	class 3: 0.23171043
	class 4: 0.46738937
	class 5: 0.4268119
	class 6: 0.25169805
	class 7: 0.76135033
	class 8: 0.22784078
	class 9: 0.0025455183
	class 10: 0.0
	class 11: 0.0016363814
	class 12: 0.01802666
	class 13: 0.15968427
	class 14: 0.0
	class 15: 0.5174834
	class 16: 0.0
	class 17: 0.0
	class 18: 0.03220267
	class 19: 0.29549274
	class 20: 0.0
Class Acc:
	class 0: 0.92511535
	class 1: 0.7109727
	class 2: 0.8017022
	class 3: 0.23280749
	class 4: 0.52251166
	class 5: 0.43308353
	class 6: 0.25353026
	class 7: 0.8429611
	class 8: 0.22902758
	class 9: 0.0025847992
	class 10: 0.0
	class 11: 0.0016373523
	class 12: 0.018080795
	class 13: 0.71134055
	class 14: 0.0
	class 15: 0.905913
	class 16: 0.0
	class 17: 0.0
	class 18: 0.03524139
	class 19: 0.65309393
	class 20: 0.0

federated global round: 31, step: 6
select part of clients to conduct local training
[9, 17, 14, 1]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=8.872763159871102
Loss made of: CE 0.6653802990913391, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.28144645690918 EntMin 0.0
Epoch 1, Class Loss=0.5287481546401978, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5287481546401978, Class Loss=0.5287481546401978, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=7.524081110954285
Loss made of: CE 0.294353187084198, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.094259262084961 EntMin 0.0
Epoch 2, Class Loss=0.4663738012313843, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.4663738012313843, Class Loss=0.4663738012313843, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=6.611149236559868
Loss made of: CE 0.41145309805870056, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.535837173461914 EntMin 0.0
Epoch 3, Class Loss=0.3844168484210968, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.3844168484210968, Class Loss=0.3844168484210968, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.220429083704948
Loss made of: CE 0.3363141417503357, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.987649917602539 EntMin 0.0
Epoch 4, Class Loss=0.3672883212566376, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.3672883212566376, Class Loss=0.3672883212566376, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.924944949150086
Loss made of: CE 0.3128514289855957, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.479135513305664 EntMin 0.0
Epoch 5, Class Loss=0.3296096920967102, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.3296096920967102, Class Loss=0.3296096920967102, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.854146635532379
Loss made of: CE 0.2385849952697754, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.331997871398926 EntMin 0.0
Epoch 6, Class Loss=0.3060612082481384, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3060612082481384, Class Loss=0.3060612082481384, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=9.890885013341904
Loss made of: CE 0.35560178756713867, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.893911361694336 EntMin 0.0
Epoch 1, Class Loss=0.3995625674724579, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.3995625674724579, Class Loss=0.3995625674724579, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=9.867274227738381
Loss made of: CE 0.28538772463798523, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.666086196899414 EntMin 0.0
Epoch 2, Class Loss=0.33305856585502625, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.33305856585502625, Class Loss=0.33305856585502625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/12, Loss=9.111091887950897
Loss made of: CE 0.24152784049510956, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.516759872436523 EntMin 0.0
Epoch 3, Class Loss=0.28104501962661743, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.28104501962661743, Class Loss=0.28104501962661743, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.822189588844775
Loss made of: CE 0.2454192042350769, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.450920104980469 EntMin 0.0
Epoch 4, Class Loss=0.24503663182258606, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.24503663182258606, Class Loss=0.24503663182258606, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.218453733623027
Loss made of: CE 0.24178801476955414, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.151244163513184 EntMin 0.0
Epoch 5, Class Loss=0.22657464444637299, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.22657464444637299, Class Loss=0.22657464444637299, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.897060221433639
Loss made of: CE 0.19026052951812744, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.609738349914551 EntMin 0.0
Epoch 6, Class Loss=0.2057071030139923, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.2057071030139923, Class Loss=0.2057071030139923, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=8.981402203440666
Loss made of: CE 0.5096108317375183, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.254732131958008 EntMin 0.0
Epoch 1, Class Loss=0.49619773030281067, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.49619773030281067, Class Loss=0.49619773030281067, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=7.360586252808571
Loss made of: CE 0.3148270547389984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.897332668304443 EntMin 0.0
Epoch 2, Class Loss=0.45616766810417175, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.45616766810417175, Class Loss=0.45616766810417175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=6.6960810869932175
Loss made of: CE 0.2471991777420044, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.072515487670898 EntMin 0.0
Epoch 3, Class Loss=0.40562903881073, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.40562903881073, Class Loss=0.40562903881073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.315571284294128
Loss made of: CE 0.25643593072891235, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.284162521362305 EntMin 0.0
Epoch 4, Class Loss=0.36695122718811035, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.36695122718811035, Class Loss=0.36695122718811035, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.865687906742096
Loss made of: CE 0.26975199580192566, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.102475166320801 EntMin 0.0
Epoch 5, Class Loss=0.32748058438301086, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.32748058438301086, Class Loss=0.32748058438301086, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.863835570216179
Loss made of: CE 0.42566579580307007, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0005292892456055 EntMin 0.0
Epoch 6, Class Loss=0.3022480607032776, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.3022480607032776, Class Loss=0.3022480607032776, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=10.015016025304794
Loss made of: CE 0.3735235929489136, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.347268104553223 EntMin 0.0
Epoch 1, Class Loss=0.4062408208847046, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.4062408208847046, Class Loss=0.4062408208847046, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=9.29228443801403
Loss made of: CE 0.338578999042511, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.4285407066345215 EntMin 0.0
Epoch 2, Class Loss=0.3409196734428406, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.3409196734428406, Class Loss=0.3409196734428406, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=8.797861778736115
Loss made of: CE 0.2859131991863251, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.019309997558594 EntMin 0.0
Epoch 3, Class Loss=0.29845473170280457, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.29845473170280457, Class Loss=0.29845473170280457, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.74629157781601
Loss made of: CE 0.24635031819343567, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.5819807052612305 EntMin 0.0
Epoch 4, Class Loss=0.2570943236351013, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.2570943236351013, Class Loss=0.2570943236351013, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.45916931182146
Loss made of: CE 0.2483939379453659, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.65944242477417 EntMin 0.0
Epoch 5, Class Loss=0.22857186198234558, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.22857186198234558, Class Loss=0.22857186198234558, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=8.05315865278244
Loss made of: CE 0.30128011107444763, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.011481285095215 EntMin 0.0
Epoch 6, Class Loss=0.22063735127449036, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.22063735127449036, Class Loss=0.22063735127449036, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8572303056716919, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.772510
Mean Acc: 0.387040
FreqW Acc: 0.651370
Mean IoU: 0.281519
Class IoU:
	class 0: 0.80808884
	class 1: 0.7037648
	class 2: 0.3384227
	class 3: 0.30174604
	class 4: 0.5549746
	class 5: 0.5230807
	class 6: 0.4302809
	class 7: 0.7654781
	class 8: 0.29839826
	class 9: 0.0033207005
	class 10: 0.0
	class 11: 0.003414573
	class 12: 0.036431562
	class 13: 0.1532255
	class 14: 0.0
	class 15: 0.5639568
	class 16: 0.0
	class 17: 0.0
	class 18: 0.120174475
	class 19: 0.3071479
	class 20: 0.0
Class Acc:
	class 0: 0.92431426
	class 1: 0.74382955
	class 2: 0.8086401
	class 3: 0.30370533
	class 4: 0.6688183
	class 5: 0.5411193
	class 6: 0.44188404
	class 7: 0.861986
	class 8: 0.3008456
	class 9: 0.0033948594
	class 10: 0.0
	class 11: 0.0034220382
	class 12: 0.03661771
	class 13: 0.73597175
	class 14: 0.0
	class 15: 0.8906251
	class 16: 0.0
	class 17: 0.0
	class 18: 0.17213577
	class 19: 0.6905222
	class 20: 0.0

federated global round: 32, step: 6
select part of clients to conduct local training
[3, 8, 27, 7]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=9.26503283083439
Loss made of: CE 0.3845357894897461, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.281230926513672 EntMin 0.0
Epoch 1, Class Loss=0.39838752150535583, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.39838752150535583, Class Loss=0.39838752150535583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=8.419404399394988
Loss made of: CE 0.29765233397483826, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.917779922485352 EntMin 0.0
Epoch 2, Class Loss=0.2970520257949829, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.2970520257949829, Class Loss=0.2970520257949829, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=8.034819932281971
Loss made of: CE 0.28776606917381287, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.30517578125 EntMin 0.0
Epoch 3, Class Loss=0.25036075711250305, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.25036075711250305, Class Loss=0.25036075711250305, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=7.732554979622364
Loss made of: CE 0.21879172325134277, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.982775688171387 EntMin 0.0
Epoch 4, Class Loss=0.21820691227912903, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.21820691227912903, Class Loss=0.21820691227912903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=7.42022267729044
Loss made of: CE 0.14411598443984985, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.838802337646484 EntMin 0.0
Epoch 5, Class Loss=0.20517343282699585, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.20517343282699585, Class Loss=0.20517343282699585, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=7.224994875490665
Loss made of: CE 0.23760488629341125, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.567112922668457 EntMin 0.0
Epoch 6, Class Loss=0.19879889488220215, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.19879889488220215, Class Loss=0.19879889488220215, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=7.016929680109024
Loss made of: CE 0.3114791512489319, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.522524833679199 EntMin 0.0
Epoch 1, Class Loss=0.44464442133903503, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.44464442133903503, Class Loss=0.44464442133903503, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.197019016742706
Loss made of: CE 0.39796823263168335, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1751909255981445 EntMin 0.0
Epoch 2, Class Loss=0.39243853092193604, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.39243853092193604, Class Loss=0.39243853092193604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=6.0147680133581165
Loss made of: CE 0.3181053400039673, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.309388160705566 EntMin 0.0
Epoch 3, Class Loss=0.34276944398880005, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.34276944398880005, Class Loss=0.34276944398880005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.64993487149477
Loss made of: CE 0.3891889452934265, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.851198673248291 EntMin 0.0
Epoch 4, Class Loss=0.30826103687286377, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.30826103687286377, Class Loss=0.30826103687286377, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.4812280356884004
Loss made of: CE 0.23542934656143188, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.913840293884277 EntMin 0.0
Epoch 5, Class Loss=0.2997012436389923, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.2997012436389923, Class Loss=0.2997012436389923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.236868888139725
Loss made of: CE 0.3189486265182495, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.152792453765869 EntMin 0.0
Epoch 6, Class Loss=0.2678409814834595, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.2678409814834595, Class Loss=0.2678409814834595, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=7.082684549689293
Loss made of: CE 0.4295397698879242, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.436610221862793 EntMin 0.0
Epoch 1, Class Loss=0.47470754384994507, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.47470754384994507, Class Loss=0.47470754384994507, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.029495830833912
Loss made of: CE 0.6550747156143188, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.593532085418701 EntMin 0.0
Epoch 2, Class Loss=0.39800411462783813, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.39800411462783813, Class Loss=0.39800411462783813, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.89705151617527
Loss made of: CE 0.37139058113098145, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.562716960906982 EntMin 0.0
Epoch 3, Class Loss=0.3291904032230377, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.3291904032230377, Class Loss=0.3291904032230377, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.516330197453499
Loss made of: CE 0.23017781972885132, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.401683807373047 EntMin 0.0
Epoch 4, Class Loss=0.3041646182537079, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.3041646182537079, Class Loss=0.3041646182537079, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.342184475064277
Loss made of: CE 0.30072420835494995, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760646820068359 EntMin 0.0
Epoch 5, Class Loss=0.2892608344554901, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.2892608344554901, Class Loss=0.2892608344554901, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.341867651045322
Loss made of: CE 0.2859271764755249, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7364420890808105 EntMin 0.0
Epoch 6, Class Loss=0.26451820135116577, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.26451820135116577, Class Loss=0.26451820135116577, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=7.334500586986541
Loss made of: CE 0.4627036452293396, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.210050582885742 EntMin 0.0
Epoch 1, Class Loss=0.4388759136199951, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.4388759136199951, Class Loss=0.4388759136199951, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.503489917516708
Loss made of: CE 0.35924825072288513, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.434810161590576 EntMin 0.0
Epoch 2, Class Loss=0.384679913520813, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.384679913520813, Class Loss=0.384679913520813, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.840158231556416
Loss made of: CE 0.2984871566295624, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.786688327789307 EntMin 0.0
Epoch 3, Class Loss=0.3384508192539215, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3384508192539215, Class Loss=0.3384508192539215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=6.028510400652886
Loss made of: CE 0.3321782052516937, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.106240272521973 EntMin 0.0
Epoch 4, Class Loss=0.3202684819698334, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.3202684819698334, Class Loss=0.3202684819698334, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.526330924034118
Loss made of: CE 0.280286967754364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.972930431365967 EntMin 0.0
Epoch 5, Class Loss=0.28482866287231445, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.28482866287231445, Class Loss=0.28482866287231445, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.439109401404858
Loss made of: CE 0.21391108632087708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.799415588378906 EntMin 0.0
Epoch 6, Class Loss=0.27734971046447754, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.27734971046447754, Class Loss=0.27734971046447754, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8581652045249939, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.778530
Mean Acc: 0.430153
FreqW Acc: 0.668169
Mean IoU: 0.319103
Class IoU:
	class 0: 0.8132677
	class 1: 0.7719771
	class 2: 0.33259365
	class 3: 0.37223348
	class 4: 0.5859772
	class 5: 0.59289175
	class 6: 0.6593749
	class 7: 0.7536365
	class 8: 0.411144
	class 9: 0.003973556
	class 10: 0.0
	class 11: 0.0070814113
	class 12: 0.061361883
	class 13: 0.15112759
	class 14: 0.0
	class 15: 0.5606726
	class 16: 0.0
	class 17: 0.0
	class 18: 0.14567643
	class 19: 0.38898158
	class 20: 0.08918093
Class Acc:
	class 0: 0.9130946
	class 1: 0.8429945
	class 2: 0.8203858
	class 3: 0.3776452
	class 4: 0.76734537
	class 5: 0.62776303
	class 6: 0.7099565
	class 7: 0.8351521
	class 8: 0.41772556
	class 9: 0.004042586
	class 10: 0.0
	class 11: 0.0071089007
	class 12: 0.061848097
	class 13: 0.7598397
	class 14: 0.0
	class 15: 0.904057
	class 16: 0.0
	class 17: 0.0
	class 18: 0.3859411
	class 19: 0.48520428
	class 20: 0.11310553

federated global round: 33, step: 6
select part of clients to conduct local training
[6, 20, 13, 10]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=5.530769708752632
Loss made of: CE 0.26811954379081726, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.781049728393555 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.307235985994339, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.307235985994339, Class Loss=0.307235985994339, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.578934881091118
Loss made of: CE 0.25366443395614624, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.254214286804199 EntMin 0.0
Epoch 2, Class Loss=0.27556008100509644, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.27556008100509644, Class Loss=0.27556008100509644, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.350096201896667
Loss made of: CE 0.2892605662345886, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.740045070648193 EntMin 0.0
Epoch 3, Class Loss=0.2573476731777191, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.2573476731777191, Class Loss=0.2573476731777191, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.417720909416675
Loss made of: CE 0.2669561505317688, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.202950954437256 EntMin 0.0
Epoch 4, Class Loss=0.2640916407108307, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.2640916407108307, Class Loss=0.2640916407108307, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.277933321893215
Loss made of: CE 0.23783397674560547, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.616476058959961 EntMin 0.0
Epoch 5, Class Loss=0.24689193069934845, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.24689193069934845, Class Loss=0.24689193069934845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=4.821874175965786
Loss made of: CE 0.23512732982635498, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303603172302246 EntMin 0.0
Epoch 6, Class Loss=0.24575386941432953, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.24575386941432953, Class Loss=0.24575386941432953, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=5.420571973919868
Loss made of: CE 0.2708203196525574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.906599521636963 EntMin 0.0
Epoch 1, Class Loss=0.3013628125190735, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.3013628125190735, Class Loss=0.3013628125190735, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.478328342735767
Loss made of: CE 0.2540575861930847, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5801873207092285 EntMin 0.0
Epoch 2, Class Loss=0.2723683714866638, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.2723683714866638, Class Loss=0.2723683714866638, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.417928671836853
Loss made of: CE 0.2552143335342407, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.257510185241699 EntMin 0.0
Epoch 3, Class Loss=0.2683936059474945, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.2683936059474945, Class Loss=0.2683936059474945, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.160311456024647
Loss made of: CE 0.24152636528015137, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.219889163970947 EntMin 0.0
Epoch 4, Class Loss=0.25715357065200806, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.25715357065200806, Class Loss=0.25715357065200806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.17614638209343
Loss made of: CE 0.31053709983825684, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.535004615783691 EntMin 0.0
Epoch 5, Class Loss=0.2448149025440216, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.2448149025440216, Class Loss=0.2448149025440216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.029188805818558
Loss made of: CE 0.22151921689510345, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.66395378112793 EntMin 0.0
Epoch 6, Class Loss=0.22922958433628082, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.22922958433628082, Class Loss=0.22922958433628082, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=10.569959026575088
Loss made of: CE 0.5085920691490173, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.474088668823242 EntMin 0.0
Epoch 1, Class Loss=0.5315777063369751, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5315777063369751, Class Loss=0.5315777063369751, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=8.363066503405571
Loss made of: CE 0.3260863721370697, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.859707832336426 EntMin 0.0
Epoch 2, Class Loss=0.3722027540206909, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.3722027540206909, Class Loss=0.3722027540206909, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=8.208811691403389
Loss made of: CE 0.25519514083862305, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.956540584564209 EntMin 0.0
Epoch 3, Class Loss=0.2767091393470764, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.2767091393470764, Class Loss=0.2767091393470764, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=7.884640763700008
Loss made of: CE 0.2080928534269333, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.565982818603516 EntMin 0.0
Epoch 4, Class Loss=0.2350940704345703, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.2350940704345703, Class Loss=0.2350940704345703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.467016142606735
Loss made of: CE 0.19987189769744873, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.65486478805542 EntMin 0.0
Epoch 5, Class Loss=0.2023533284664154, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.2023533284664154, Class Loss=0.2023533284664154, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=7.405796824395656
Loss made of: CE 0.20825329422950745, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.218271255493164 EntMin 0.0
Epoch 6, Class Loss=0.19780723750591278, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.19780723750591278, Class Loss=0.19780723750591278, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=11.035254821181297
Loss made of: CE 0.4798659086227417, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.468819618225098 EntMin 0.0
Epoch 1, Class Loss=0.5598117113113403, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.5598117113113403, Class Loss=0.5598117113113403, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/12, Loss=8.739787378907204
Loss made of: CE 0.46780461072921753, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.734220027923584 EntMin 0.0
Epoch 2, Class Loss=0.40579625964164734, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.40579625964164734, Class Loss=0.40579625964164734, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=8.324384246766567
Loss made of: CE 0.30945125222206116, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.933444023132324 EntMin 0.0
Epoch 3, Class Loss=0.2985598146915436, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.2985598146915436, Class Loss=0.2985598146915436, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/12, Loss=7.957127737998962
Loss made of: CE 0.24907970428466797, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.969830513000488 EntMin 0.0
Epoch 4, Class Loss=0.2500303387641907, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.2500303387641907, Class Loss=0.2500303387641907, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/12, Loss=7.622890365123749
Loss made of: CE 0.22128590941429138, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.987668991088867 EntMin 0.0
Epoch 5, Class Loss=0.22477279603481293, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.22477279603481293, Class Loss=0.22477279603481293, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/12, Loss=7.525174000859261
Loss made of: CE 0.2510696053504944, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.047468185424805 EntMin 0.0
Epoch 6, Class Loss=0.21257546544075012, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.21257546544075012, Class Loss=0.21257546544075012, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8268030881881714, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.775877
Mean Acc: 0.415445
FreqW Acc: 0.662794
Mean IoU: 0.300334
Class IoU:
	class 0: 0.8154961
	class 1: 0.71596354
	class 2: 0.33465958
	class 3: 0.35009375
	class 4: 0.56315434
	class 5: 0.56768316
	class 6: 0.50981784
	class 7: 0.76763815
	class 8: 0.4043552
	class 9: 0.0030593816
	class 10: 0.0
	class 11: 0.0055107125
	class 12: 0.050051082
	class 13: 0.15817395
	class 14: 0.0
	class 15: 0.54724866
	class 16: 0.0
	class 17: 0.0
	class 18: 0.1582944
	class 19: 0.32479382
	class 20: 0.03101723
Class Acc:
	class 0: 0.91342574
	class 1: 0.7618236
	class 2: 0.83682317
	class 3: 0.3525468
	class 4: 0.6818155
	class 5: 0.5961493
	class 6: 0.5258491
	class 7: 0.848412
	class 8: 0.40950182
	class 9: 0.0031045878
	class 10: 0.0
	class 11: 0.005530059
	class 12: 0.050400436
	class 13: 0.7537028
	class 14: 0.0
	class 15: 0.9071964
	class 16: 0.0
	class 17: 0.0
	class 18: 0.3051463
	class 19: 0.7418962
	class 20: 0.031022372

federated global round: 34, step: 6
select part of clients to conduct local training
[19, 18, 27, 14]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.5387588575482365
Loss made of: CE 0.2630305290222168, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.598172664642334 EntMin 0.0
Epoch 1, Class Loss=0.3422587215900421, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.3422587215900421, Class Loss=0.3422587215900421, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.856872837245464
Loss made of: CE 0.37402471899986267, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.885240077972412 EntMin 0.0
Epoch 2, Class Loss=0.29712966084480286, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.29712966084480286, Class Loss=0.29712966084480286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.417473885416984
Loss made of: CE 0.2776931822299957, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.196556091308594 EntMin 0.0
Epoch 3, Class Loss=0.2782004475593567, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.2782004475593567, Class Loss=0.2782004475593567, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.083448903262616
Loss made of: CE 0.2625829577445984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.963292121887207 EntMin 0.0
Epoch 4, Class Loss=0.25989699363708496, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.25989699363708496, Class Loss=0.25989699363708496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.343569448590278
Loss made of: CE 0.17394253611564636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.927733421325684 EntMin 0.0
Epoch 5, Class Loss=0.2602948248386383, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.2602948248386383, Class Loss=0.2602948248386383, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.132440392673016
Loss made of: CE 0.2121124267578125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.009827136993408 EntMin 0.0
Epoch 6, Class Loss=0.24792785942554474, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.24792785942554474, Class Loss=0.24792785942554474, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.207108445465565
Loss made of: CE 0.4048572778701782, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.683621883392334 EntMin 0.0
Epoch 1, Class Loss=0.3193732500076294, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.3193732500076294, Class Loss=0.3193732500076294, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.301701939105987
Loss made of: CE 0.24307620525360107, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645905494689941 EntMin 0.0
Epoch 2, Class Loss=0.2740594148635864, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.2740594148635864, Class Loss=0.2740594148635864, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.24302318841219
Loss made of: CE 0.2662654519081116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.175929069519043 EntMin 0.0
Epoch 3, Class Loss=0.2518099844455719, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.2518099844455719, Class Loss=0.2518099844455719, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=4.979204185307026
Loss made of: CE 0.1968674659729004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1226396560668945 EntMin 0.0
Epoch 4, Class Loss=0.2472129762172699, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.2472129762172699, Class Loss=0.2472129762172699, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=4.893016445636749
Loss made of: CE 0.2878756523132324, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.46046781539917 EntMin 0.0
Epoch 5, Class Loss=0.23886272311210632, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.23886272311210632, Class Loss=0.23886272311210632, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=4.850643786787987
Loss made of: CE 0.20065557956695557, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.252553939819336 EntMin 0.0
Epoch 6, Class Loss=0.23342008888721466, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.23342008888721466, Class Loss=0.23342008888721466, Reg Loss=0.0
Current Client Index:  27
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.266687677800656
Loss made of: CE 0.28464508056640625, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.552289009094238 EntMin 0.0
Epoch 1, Class Loss=0.32273051142692566, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.32273051142692566, Class Loss=0.32273051142692566, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/13, Loss=5.255119454860687
Loss made of: CE 0.5435829162597656, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.117032051086426 EntMin 0.0
Epoch 2, Class Loss=0.3136696517467499, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.3136696517467499, Class Loss=0.3136696517467499, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/13, Loss=5.160213199257851
Loss made of: CE 0.2512289881706238, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.808942794799805 EntMin 0.0
Epoch 3, Class Loss=0.27005571126937866, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.27005571126937866, Class Loss=0.27005571126937866, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/13, Loss=4.9653095915913585
Loss made of: CE 0.21892057359218597, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0728559494018555 EntMin 0.0
Epoch 4, Class Loss=0.2637874484062195, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.2637874484062195, Class Loss=0.2637874484062195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/13, Loss=4.967438216507435
Loss made of: CE 0.2578788995742798, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.349758148193359 EntMin 0.0
Epoch 5, Class Loss=0.2600826025009155, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.2600826025009155, Class Loss=0.2600826025009155, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/13, Loss=4.834735782444477
Loss made of: CE 0.26179254055023193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.221132278442383 EntMin 0.0
Epoch 6, Class Loss=0.2533908188343048, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.2533908188343048, Class Loss=0.2533908188343048, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 1, Batch 10/13, Loss=6.368885156512261
Loss made of: CE 0.3610168397426605, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.278103351593018 EntMin 0.0
Epoch 1, Class Loss=0.3106900155544281, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.3106900155544281, Class Loss=0.3106900155544281, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/13, Loss=5.372557431459427
Loss made of: CE 0.2188073992729187, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.977156639099121 EntMin 0.0
Epoch 2, Class Loss=0.2980380058288574, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.2980380058288574, Class Loss=0.2980380058288574, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/13, Loss=5.17040125131607
Loss made of: CE 0.20489951968193054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2739105224609375 EntMin 0.0
Epoch 3, Class Loss=0.27498897910118103, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.27498897910118103, Class Loss=0.27498897910118103, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/13, Loss=4.975624696910382
Loss made of: CE 0.20744559168815613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.765049457550049 EntMin 0.0
Epoch 4, Class Loss=0.26576822996139526, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.26576822996139526, Class Loss=0.26576822996139526, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/13, Loss=4.946944643557072
Loss made of: CE 0.17911601066589355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.301901340484619 EntMin 0.0
Epoch 5, Class Loss=0.25819307565689087, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.25819307565689087, Class Loss=0.25819307565689087, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/13, Loss=4.972737084329128
Loss made of: CE 0.3698303699493408, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.232187747955322 EntMin 0.0
Epoch 6, Class Loss=0.2588740289211273, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.2588740289211273, Class Loss=0.2588740289211273, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8659479022026062, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.766663
Mean Acc: 0.419045
FreqW Acc: 0.670424
Mean IoU: 0.296834
Class IoU:
	class 0: 0.8262195
	class 1: 0.7624724
	class 2: 0.33851793
	class 3: 0.39087266
	class 4: 0.57333916
	class 5: 0.5589059
	class 6: 0.39110547
	class 7: 0.718113
	class 8: 0.48191753
	class 9: 0.0030855732
	class 10: 0.0
	class 11: 0.007312454
	class 12: 0.07735498
	class 13: 0.15590255
	class 14: 0.0
	class 15: 0.61555636
	class 16: 0.0
	class 17: 0.0
	class 18: 0.13147447
	class 19: 0.09617628
	class 20: 0.10519197
Class Acc:
	class 0: 0.91072834
	class 1: 0.8223279
	class 2: 0.81515944
	class 3: 0.3980018
	class 4: 0.74524194
	class 5: 0.59285986
	class 6: 0.4122767
	class 7: 0.7723886
	class 8: 0.49300236
	class 9: 0.0031248392
	class 10: 0.0
	class 11: 0.007336833
	class 12: 0.07803765
	class 13: 0.7239484
	class 14: 0.0
	class 15: 0.8746622
	class 16: 0.0
	class 17: 0.0
	class 18: 0.4665459
	class 19: 0.10226898
	class 20: 0.5820264

voc_8-2_PLOP On GPUs 2
Run in 78147s
