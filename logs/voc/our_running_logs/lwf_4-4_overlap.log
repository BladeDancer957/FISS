nohup: ignoring input
25
kvoc_4-4_LWF On GPUs 0\Writing in results/seed_2023-ov/2023-03-08_voc_4-4_LWF.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[6, 0, 7, 2]
Current Client Index:  6
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[8, 5, 3, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[5, 2, 0, 3]
Current Client Index:  5
Current Client Index:  2
Current Client Index:  0
Current Client Index:  3
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[3, 6, 1, 7]
Current Client Index:  3
Current Client Index:  6
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.09879439324140549, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.957759
Mean Acc: 0.761733
FreqW Acc: 0.923270
Mean IoU: 0.703965
Class IoU:
	class 0: 0.9539515191877496
	class 1: 0.7615770129403676
	class 2: 0.2054173196242969
	class 3: 0.8920301311931925
	class 4: 0.7068509839220561
Class Acc:
	class 0: 0.9848764620456163
	class 1: 0.7738026584549321
	class 2: 0.27733822617675347
	class 3: 0.9519182023560161
	class 4: 0.8207287142557349

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[0, 12, 6, 10]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/52, Loss=7.044676804542542
Loss made of: CE 1.725390911102295, LKD 5.1471405029296875, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/52, Loss=5.605051183700562
Loss made of: CE 0.9764547348022461, LKD 4.776829719543457, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=5.002967125177383
Loss made of: CE 0.8185297250747681, LKD 3.553171396255493, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=4.13278346657753
Loss made of: CE 0.42515265941619873, LKD 3.229361057281494, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/52, Loss=4.837543660402298
Loss made of: CE 0.3880409598350525, LKD 2.9701380729675293, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.9195934534072876, Reg Loss=4.378758430480957
Clinet index 0, End of Epoch 1/6, Average Loss=5.298351764678955, Class Loss=0.9195934534072876, Reg Loss=4.378758430480957
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/52, Loss=4.461846172809601
Loss made of: CE 0.4085676074028015, LKD 3.6746528148651123, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=4.250433617830277
Loss made of: CE 0.35166627168655396, LKD 3.1432039737701416, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.203067287802696
Loss made of: CE 0.4631083607673645, LKD 3.7827506065368652, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/52, Loss=4.20506640970707
Loss made of: CE 0.4063343405723572, LKD 4.905987739562988, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=4.025435821712017
Loss made of: CE 0.3913862109184265, LKD 4.51177978515625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.39717334508895874, Reg Loss=3.787024736404419
Clinet index 0, End of Epoch 2/6, Average Loss=4.184197902679443, Class Loss=0.39717334508895874, Reg Loss=3.787024736404419
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/52, Loss=4.121739128232003
Loss made of: CE 0.303721159696579, LKD 3.90197491645813, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=4.492507040500641
Loss made of: CE 0.34011372923851013, LKD 4.303008079528809, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=4.111503264307975
Loss made of: CE 0.3893492817878723, LKD 5.5446343421936035, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.6764984250068666
Loss made of: CE 0.31083112955093384, LKD 2.6700968742370605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.5314553692936896
Loss made of: CE 0.2711237668991089, LKD 3.514158248901367, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.33068153262138367, Reg Loss=3.6443872451782227
Clinet index 0, End of Epoch 3/6, Average Loss=3.9750688076019287, Class Loss=0.33068153262138367, Reg Loss=3.6443872451782227
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/52, Loss=4.015860801935196
Loss made of: CE 0.42498135566711426, LKD 3.9110870361328125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.634655882418156
Loss made of: CE 0.23432406783103943, LKD 3.829087257385254, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=4.089740759134292
Loss made of: CE 0.292935311794281, LKD 3.353600263595581, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.9457541689276696
Loss made of: CE 0.21103402972221375, LKD 2.9212236404418945, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.4139975652098657
Loss made of: CE 0.2669254243373871, LKD 2.922266721725464, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.29142341017723083, Reg Loss=3.5298547744750977
Clinet index 0, End of Epoch 4/6, Average Loss=3.8212780952453613, Class Loss=0.29142341017723083, Reg Loss=3.5298547744750977
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/52, Loss=3.694730411469936
Loss made of: CE 0.3190898597240448, LKD 3.1611547470092773, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.665011878311634
Loss made of: CE 0.18957877159118652, LKD 3.2909538745880127, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.640845300257206
Loss made of: CE 0.28947851061820984, LKD 2.9936466217041016, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.520573742687702
Loss made of: CE 0.2796321511268616, LKD 3.4424257278442383, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.6795158624649047
Loss made of: CE 0.23144251108169556, LKD 3.3290202617645264, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.26675665378570557, Reg Loss=3.3943421840667725
Clinet index 0, End of Epoch 5/6, Average Loss=3.6610989570617676, Class Loss=0.26675665378570557, Reg Loss=3.3943421840667725
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/52, Loss=3.4285515278577803
Loss made of: CE 0.22444963455200195, LKD 3.4058451652526855, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.8188263893127443
Loss made of: CE 0.1918717622756958, LKD 3.010737419128418, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.6605237275362015
Loss made of: CE 0.26027941703796387, LKD 2.765321731567383, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.6926333039999006
Loss made of: CE 0.2860546410083771, LKD 2.263554096221924, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.681086632609367
Loss made of: CE 0.23662589490413666, LKD 2.2949254512786865, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2556549906730652, Reg Loss=3.3993330001831055
Clinet index 0, End of Epoch 6/6, Average Loss=3.6549880504608154, Class Loss=0.2556549906730652, Reg Loss=3.3993330001831055
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/40, Loss=7.956931984424591
Loss made of: CE 1.3237481117248535, LKD 5.496434688568115, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/40, Loss=6.217846220731735
Loss made of: CE 0.7117959260940552, LKD 4.1317315101623535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/40, Loss=4.854412943124771
Loss made of: CE 0.5987024307250977, LKD 3.650084972381592, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/40, Loss=4.617269411683083
Loss made of: CE 0.5061179399490356, LKD 5.289374351501465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.8598958849906921, Reg Loss=5.0517191886901855
Clinet index 12, End of Epoch 1/6, Average Loss=5.911614894866943, Class Loss=0.8598958849906921, Reg Loss=5.0517191886901855
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/40, Loss=4.79612173140049
Loss made of: CE 0.2876492142677307, LKD 4.227448463439941, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/40, Loss=4.290413758158683
Loss made of: CE 0.3364022672176361, LKD 4.523077487945557, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/40, Loss=4.301417806744576
Loss made of: CE 0.394148051738739, LKD 2.8899292945861816, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/40, Loss=4.284894305467605
Loss made of: CE 0.3121037185192108, LKD 5.04136323928833, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.36925074458122253, Reg Loss=4.048961162567139
Clinet index 12, End of Epoch 2/6, Average Loss=4.418211936950684, Class Loss=0.36925074458122253, Reg Loss=4.048961162567139
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/40, Loss=4.1991582185029985
Loss made of: CE 0.21754392981529236, LKD 3.977466344833374, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/40, Loss=3.769853387773037
Loss made of: CE 0.3371597230434418, LKD 3.9963345527648926, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/40, Loss=3.913005867600441
Loss made of: CE 0.23392920196056366, LKD 3.4884512424468994, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.111255387961864
Loss made of: CE 0.2609062194824219, LKD 3.9997880458831787, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.29280656576156616, Reg Loss=3.7055115699768066
Clinet index 12, End of Epoch 3/6, Average Loss=3.9983181953430176, Class Loss=0.29280656576156616, Reg Loss=3.7055115699768066
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/40, Loss=3.999059981107712
Loss made of: CE 0.303618848323822, LKD 3.0870933532714844, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.17559317946434
Loss made of: CE 0.24347566068172455, LKD 3.8687891960144043, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.07748489677906
Loss made of: CE 0.22916996479034424, LKD 3.0104382038116455, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/40, Loss=3.9517003551125525
Loss made of: CE 0.25046947598457336, LKD 4.545009136199951, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2715317904949188, Reg Loss=3.7794277667999268
Clinet index 12, End of Epoch 4/6, Average Loss=4.050959587097168, Class Loss=0.2715317904949188, Reg Loss=3.7794277667999268
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/40, Loss=4.03677766174078
Loss made of: CE 0.25620710849761963, LKD 3.4038383960723877, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/40, Loss=3.782215876877308
Loss made of: CE 0.2665978968143463, LKD 4.259942531585693, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/40, Loss=3.647663028538227
Loss made of: CE 0.18848800659179688, LKD 3.5760998725891113, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.144917179644108
Loss made of: CE 0.214694544672966, LKD 3.3682103157043457, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2331930696964264, Reg Loss=3.6697003841400146
Clinet index 12, End of Epoch 5/6, Average Loss=3.902893543243408, Class Loss=0.2331930696964264, Reg Loss=3.6697003841400146
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/40, Loss=3.9719641998410227
Loss made of: CE 0.18288466334342957, LKD 4.511641979217529, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/40, Loss=3.649627110362053
Loss made of: CE 0.2217787355184555, LKD 3.8304290771484375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/40, Loss=3.935765002667904
Loss made of: CE 0.22894799709320068, LKD 3.410740613937378, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/40, Loss=3.6597545817494392
Loss made of: CE 0.17533086240291595, LKD 3.073080539703369, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.22659187018871307, Reg Loss=3.577686071395874
Clinet index 12, End of Epoch 6/6, Average Loss=3.8042778968811035, Class Loss=0.22659187018871307, Reg Loss=3.577686071395874
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/34, Loss=8.087043297290801
Loss made of: CE 2.376359224319458, LKD 4.921060085296631, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=5.647008347511291
Loss made of: CE 0.784601092338562, LKD 3.94590425491333, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=5.724879515171051
Loss made of: CE 0.6761927008628845, LKD 4.55643892288208, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.2727985382080078, Reg Loss=4.996584415435791
Clinet index 6, End of Epoch 1/6, Average Loss=6.269382953643799, Class Loss=1.2727985382080078, Reg Loss=4.996584415435791
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/34, Loss=4.331087496876717
Loss made of: CE 0.3454717993736267, LKD 4.4330878257751465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.437086817622185
Loss made of: CE 0.3740081787109375, LKD 4.390511989593506, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.797132915258407
Loss made of: CE 0.34555584192276, LKD 5.327091693878174, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3535003662109375, Reg Loss=4.106246471405029
Clinet index 6, End of Epoch 2/6, Average Loss=4.459746837615967, Class Loss=0.3535003662109375, Reg Loss=4.106246471405029
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/34, Loss=4.4009294509887695
Loss made of: CE 0.3580104112625122, LKD 4.749996185302734, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.357959869503975
Loss made of: CE 0.2852875292301178, LKD 3.48899245262146, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.387162247300148
Loss made of: CE 0.3428886830806732, LKD 4.084654331207275, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.295930951833725, Reg Loss=4.053144454956055
Clinet index 6, End of Epoch 3/6, Average Loss=4.3490753173828125, Class Loss=0.295930951833725, Reg Loss=4.053144454956055
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/34, Loss=4.275261330604553
Loss made of: CE 0.24112150073051453, LKD 2.734219789505005, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=3.8533173337578774
Loss made of: CE 0.2841442823410034, LKD 3.0317635536193848, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.0941101014614105
Loss made of: CE 0.4404255747795105, LKD 3.9179439544677734, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.28430914878845215, Reg Loss=3.7686829566955566
Clinet index 6, End of Epoch 4/6, Average Loss=4.05299186706543, Class Loss=0.28430914878845215, Reg Loss=3.7686829566955566
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/34, Loss=3.922132244706154
Loss made of: CE 0.176881343126297, LKD 3.269772529602051, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=3.997038647532463
Loss made of: CE 0.252025306224823, LKD 4.701207160949707, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=3.940168631076813
Loss made of: CE 0.19711527228355408, LKD 4.317439556121826, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.25997045636177063, Reg Loss=3.730135679244995
Clinet index 6, End of Epoch 5/6, Average Loss=3.9901061058044434, Class Loss=0.25997045636177063, Reg Loss=3.730135679244995
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/34, Loss=4.157308672368527
Loss made of: CE 0.33089911937713623, LKD 3.138373851776123, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=3.850513921678066
Loss made of: CE 0.3097270727157593, LKD 4.359658241271973, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.01390777528286
Loss made of: CE 0.17076751589775085, LKD 3.529973268508911, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.25092631578445435, Reg Loss=3.708244562149048
Clinet index 6, End of Epoch 6/6, Average Loss=3.9591708183288574, Class Loss=0.25092631578445435, Reg Loss=3.708244562149048
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/33, Loss=6.54445766210556
Loss made of: CE 1.604363203048706, LKD 2.4591593742370605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/33, Loss=4.486146247386932
Loss made of: CE 1.2192095518112183, LKD 3.24163818359375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/33, Loss=3.9307103574275972
Loss made of: CE 0.7725157737731934, LKD 2.3691539764404297, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.152113437652588, Reg Loss=3.6994149684906006
Clinet index 10, End of Epoch 1/6, Average Loss=4.851528167724609, Class Loss=1.152113437652588, Reg Loss=3.6994149684906006
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/33, Loss=3.7376279890537263
Loss made of: CE 0.6521800756454468, LKD 4.106942653656006, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/33, Loss=3.172642305493355
Loss made of: CE 0.7628077864646912, LKD 3.8384363651275635, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/33, Loss=3.4082330763339996
Loss made of: CE 0.30953851342201233, LKD 2.0835814476013184, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5327426195144653, Reg Loss=2.896893262863159
Clinet index 10, End of Epoch 2/6, Average Loss=3.429636001586914, Class Loss=0.5327426195144653, Reg Loss=2.896893262863159
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/33, Loss=3.146110701560974
Loss made of: CE 0.4147380292415619, LKD 2.9824485778808594, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/33, Loss=3.075537434220314
Loss made of: CE 0.3147372603416443, LKD 3.488820791244507, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/33, Loss=3.3459829658269884
Loss made of: CE 0.24344849586486816, LKD 1.6104791164398193, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3407226800918579, Reg Loss=2.827929973602295
Clinet index 10, End of Epoch 3/6, Average Loss=3.1686525344848633, Class Loss=0.3407226800918579, Reg Loss=2.827929973602295
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/33, Loss=2.9144364833831786
Loss made of: CE 0.2583063840866089, LKD 2.3080637454986572, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/33, Loss=2.985382728278637
Loss made of: CE 0.2804597020149231, LKD 2.3457839488983154, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/33, Loss=3.1466021865606306
Loss made of: CE 0.26569217443466187, LKD 2.5575599670410156, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.29490774869918823, Reg Loss=2.688664197921753
Clinet index 10, End of Epoch 4/6, Average Loss=2.983572006225586, Class Loss=0.29490774869918823, Reg Loss=2.688664197921753
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/33, Loss=3.198254942893982
Loss made of: CE 0.3861895501613617, LKD 4.4581146240234375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/33, Loss=2.9510044649243357
Loss made of: CE 0.3495423197746277, LKD 2.90496563911438, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/33, Loss=2.8715380385518072
Loss made of: CE 0.2514907717704773, LKD 2.6087899208068848, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.27629029750823975, Reg Loss=2.672374963760376
Clinet index 10, End of Epoch 5/6, Average Loss=2.948665142059326, Class Loss=0.27629029750823975, Reg Loss=2.672374963760376
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/33, Loss=3.466963863372803
Loss made of: CE 0.39419683814048767, LKD 4.005058288574219, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/33, Loss=2.666255919635296
Loss made of: CE 0.24925319850444794, LKD 2.671940565109253, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/33, Loss=2.742144061625004
Loss made of: CE 0.2396138608455658, LKD 2.130695343017578, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.26146629452705383, Reg Loss=2.7237231731414795
Clinet index 10, End of Epoch 6/6, Average Loss=2.985189437866211, Class Loss=0.26146629452705383, Reg Loss=2.7237231731414795
federated aggregation...
Validation, Class Loss=0.366512656211853, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.859433
Mean Acc: 0.389960
FreqW Acc: 0.750916
Mean IoU: 0.342972
Class IoU:
	class 0: 0.8721513
	class 1: 0.50924873
	class 2: 0.09617101
	class 3: 0.0014655933
	class 4: 0.4920837
	class 5: 0.0
	class 6: 0.50686675
	class 7: 0.11711065
	class 8: 0.49165386
Class Acc:
	class 0: 0.99202716
	class 1: 0.5105374
	class 2: 0.11447964
	class 3: 0.0014655933
	class 4: 0.5056937
	class 5: 0.0
	class 6: 0.5288706
	class 7: 0.12279395
	class 8: 0.7337683

federated global round: 6, step: 1
select part of clients to conduct local training
[11, 5, 8, 12]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/43, Loss=3.2443994104862215
Loss made of: CE 0.32385993003845215, LKD 2.1595349311828613, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/43, Loss=3.2462877839803697
Loss made of: CE 0.47765612602233887, LKD 3.138136386871338, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/43, Loss=2.8946461766958236
Loss made of: CE 0.31419163942337036, LKD 1.9188443422317505, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/43, Loss=2.989662802219391
Loss made of: CE 0.4013092517852783, LKD 3.4687957763671875, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.44041651487350464, Reg Loss=2.6414923667907715
Clinet index 11, End of Epoch 1/6, Average Loss=3.081908941268921, Class Loss=0.44041651487350464, Reg Loss=2.6414923667907715
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/43, Loss=2.6473727852106093
Loss made of: CE 0.23477894067764282, LKD 1.4244141578674316, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/43, Loss=2.9154577374458315
Loss made of: CE 0.24538183212280273, LKD 1.840799331665039, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/43, Loss=2.998806583881378
Loss made of: CE 0.347606897354126, LKD 2.7075088024139404, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/43, Loss=2.924587482213974
Loss made of: CE 0.2794332504272461, LKD 2.0796754360198975, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.31199735403060913, Reg Loss=2.575546979904175
Clinet index 11, End of Epoch 2/6, Average Loss=2.8875443935394287, Class Loss=0.31199735403060913, Reg Loss=2.575546979904175
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/43, Loss=2.922555500268936
Loss made of: CE 0.2919461727142334, LKD 2.831394672393799, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/43, Loss=2.580902674794197
Loss made of: CE 0.25857117772102356, LKD 1.7574392557144165, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/43, Loss=2.7701596930623054
Loss made of: CE 0.26970648765563965, LKD 2.2937259674072266, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/43, Loss=2.9490707188844683
Loss made of: CE 0.2077885866165161, LKD 1.8846584558486938, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2715930938720703, Reg Loss=2.522691011428833
Clinet index 11, End of Epoch 3/6, Average Loss=2.7942841053009033, Class Loss=0.2715930938720703, Reg Loss=2.522691011428833
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/43, Loss=2.707862785458565
Loss made of: CE 0.2228819578886032, LKD 2.8503572940826416, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/43, Loss=2.6911612510681153
Loss made of: CE 0.2763020992279053, LKD 2.99535870552063, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/43, Loss=2.715006797015667
Loss made of: CE 0.268612265586853, LKD 2.4819350242614746, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/43, Loss=2.90126321464777
Loss made of: CE 0.2186470925807953, LKD 2.0664920806884766, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2505307197570801, Reg Loss=2.4892115592956543
Clinet index 11, End of Epoch 4/6, Average Loss=2.7397422790527344, Class Loss=0.2505307197570801, Reg Loss=2.4892115592956543
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/43, Loss=2.741633102297783
Loss made of: CE 0.22633451223373413, LKD 2.7518508434295654, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/43, Loss=2.663518027961254
Loss made of: CE 0.30867552757263184, LKD 1.9805710315704346, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/43, Loss=2.765458570420742
Loss made of: CE 0.17163237929344177, LKD 1.806564211845398, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/43, Loss=2.611050060391426
Loss made of: CE 0.229822039604187, LKD 2.1804323196411133, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2401362955570221, Reg Loss=2.4888482093811035
Clinet index 11, End of Epoch 5/6, Average Loss=2.7289845943450928, Class Loss=0.2401362955570221, Reg Loss=2.4888482093811035
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/43, Loss=2.343687814474106
Loss made of: CE 0.20562785863876343, LKD 1.8886691331863403, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/43, Loss=2.6780631586909296
Loss made of: CE 0.22501562535762787, LKD 2.4863593578338623, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/43, Loss=2.8669482186436652
Loss made of: CE 0.19658729434013367, LKD 3.290036201477051, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/43, Loss=2.888919375836849
Loss made of: CE 0.2601597309112549, LKD 2.366281032562256, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.22232699394226074, Reg Loss=2.484290599822998
Clinet index 11, End of Epoch 6/6, Average Loss=2.706617593765259, Class Loss=0.22232699394226074, Reg Loss=2.484290599822998
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/34, Loss=4.218830266594887
Loss made of: CE 0.39448055624961853, LKD 3.0512804985046387, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.224492770433426
Loss made of: CE 0.3529812693595886, LKD 4.1039252281188965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.268456648290157
Loss made of: CE 0.3251698315143585, LKD 6.708523750305176, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3522510826587677, Reg Loss=3.894305944442749
Clinet index 5, End of Epoch 1/6, Average Loss=4.246557235717773, Class Loss=0.3522510826587677, Reg Loss=3.894305944442749
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/34, Loss=4.345706787705422
Loss made of: CE 0.23731043934822083, LKD 3.9517197608947754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=3.8144837841391563
Loss made of: CE 0.21787135303020477, LKD 3.9803645610809326, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.424620786309243
Loss made of: CE 0.29665103554725647, LKD 4.895742893218994, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.25900524854660034, Reg Loss=3.906682252883911
Clinet index 5, End of Epoch 2/6, Average Loss=4.165687561035156, Class Loss=0.25900524854660034, Reg Loss=3.906682252883911
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/34, Loss=4.065929357707501
Loss made of: CE 0.3221855163574219, LKD 4.329232215881348, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=3.778640866279602
Loss made of: CE 0.19979111850261688, LKD 3.4354143142700195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.138356597721577
Loss made of: CE 0.28295204043388367, LKD 4.169740676879883, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2302878201007843, Reg Loss=3.7915141582489014
Clinet index 5, End of Epoch 3/6, Average Loss=4.021801948547363, Class Loss=0.2302878201007843, Reg Loss=3.7915141582489014
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/34, Loss=3.8817929700016975
Loss made of: CE 0.18969576060771942, LKD 3.424187421798706, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.222790957987309
Loss made of: CE 0.2124253362417221, LKD 4.143121242523193, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=3.896967980265617
Loss made of: CE 0.2080550640821457, LKD 3.912550210952759, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.22697840631008148, Reg Loss=3.7640790939331055
Clinet index 5, End of Epoch 4/6, Average Loss=3.9910573959350586, Class Loss=0.22697840631008148, Reg Loss=3.7640790939331055
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/34, Loss=4.0823079869151115
Loss made of: CE 0.19668704271316528, LKD 4.556754112243652, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=3.7335437566041945
Loss made of: CE 0.25649985671043396, LKD 3.6859817504882812, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.02764201760292
Loss made of: CE 0.24578671157360077, LKD 4.397824287414551, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.21377095580101013, Reg Loss=3.7160539627075195
Clinet index 5, End of Epoch 5/6, Average Loss=3.9298248291015625, Class Loss=0.21377095580101013, Reg Loss=3.7160539627075195
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/34, Loss=4.12990270704031
Loss made of: CE 0.2860656678676605, LKD 3.8402676582336426, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=3.5631367623806
Loss made of: CE 0.19158074259757996, LKD 3.6843087673187256, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.053672567009926
Loss made of: CE 0.25680825114250183, LKD 5.085036754608154, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.21602189540863037, Reg Loss=3.685380220413208
Clinet index 5, End of Epoch 6/6, Average Loss=3.901401996612549, Class Loss=0.21602189540863037, Reg Loss=3.685380220413208
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=3.7597129732370376
Loss made of: CE 0.4316171109676361, LKD 3.548309564590454, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/40, Loss=4.0095924764871596
Loss made of: CE 0.29760393500328064, LKD 4.119575500488281, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 30/40, Loss=3.720824210345745
Loss made of: CE 0.35114580392837524, LKD 2.718024492263794, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/40, Loss=4.201001515984535
Loss made of: CE 0.3367379605770111, LKD 3.526759147644043, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3578881025314331, Reg Loss=3.564894914627075
Clinet index 8, End of Epoch 1/6, Average Loss=3.9227828979492188, Class Loss=0.3578881025314331, Reg Loss=3.564894914627075
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/40, Loss=3.59142384827137
Loss made of: CE 0.25475820899009705, LKD 3.6360344886779785, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/40, Loss=3.746192975342274
Loss made of: CE 0.22267401218414307, LKD 3.8717222213745117, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/40, Loss=4.1634451895952225
Loss made of: CE 0.3412435054779053, LKD 3.7257142066955566, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/40, Loss=3.771489641070366
Loss made of: CE 0.1950012743473053, LKD 2.629092216491699, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.27588799595832825, Reg Loss=3.542250156402588
Clinet index 8, End of Epoch 2/6, Average Loss=3.8181381225585938, Class Loss=0.27588799595832825, Reg Loss=3.542250156402588
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/40, Loss=3.468255540728569
Loss made of: CE 0.19006399810314178, LKD 2.2261452674865723, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/40, Loss=3.6275358885526656
Loss made of: CE 0.19579176604747772, LKD 3.2661356925964355, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/40, Loss=3.730609790980816
Loss made of: CE 0.2349870502948761, LKD 4.846757888793945, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/40, Loss=3.7391842529177666
Loss made of: CE 0.20790717005729675, LKD 3.17002010345459, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.22750575840473175, Reg Loss=3.4138906002044678
Clinet index 8, End of Epoch 3/6, Average Loss=3.6413962841033936, Class Loss=0.22750575840473175, Reg Loss=3.4138906002044678
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/40, Loss=3.4435848236083983
Loss made of: CE 0.2787582576274872, LKD 3.5330095291137695, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/40, Loss=3.611017270386219
Loss made of: CE 0.21784988045692444, LKD 3.53385591506958, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/40, Loss=3.495468318462372
Loss made of: CE 0.21576540172100067, LKD 2.161823272705078, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/40, Loss=3.829363578557968
Loss made of: CE 0.18033000826835632, LKD 3.568305015563965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.22186148166656494, Reg Loss=3.3729970455169678
Clinet index 8, End of Epoch 4/6, Average Loss=3.5948586463928223, Class Loss=0.22186148166656494, Reg Loss=3.3729970455169678
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/40, Loss=3.2909280106425287
Loss made of: CE 0.14941111207008362, LKD 2.593477725982666, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/40, Loss=3.67301035374403
Loss made of: CE 0.28149378299713135, LKD 2.4281938076019287, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/40, Loss=3.781212778389454
Loss made of: CE 0.21116992831230164, LKD 3.5960938930511475, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/40, Loss=3.3480665519833566
Loss made of: CE 0.17396871745586395, LKD 3.287720203399658, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2079654037952423, Reg Loss=3.3153388500213623
Clinet index 8, End of Epoch 5/6, Average Loss=3.5233042240142822, Class Loss=0.2079654037952423, Reg Loss=3.3153388500213623
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/40, Loss=3.464727818965912
Loss made of: CE 0.20550940930843353, LKD 3.01601505279541, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/40, Loss=3.3758908554911615
Loss made of: CE 0.16767969727516174, LKD 3.5407752990722656, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/40, Loss=3.6479608699679376
Loss made of: CE 0.21602702140808105, LKD 3.292776346206665, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/40, Loss=3.70741306245327
Loss made of: CE 0.19276532530784607, LKD 4.392002582550049, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2012486755847931, Reg Loss=3.3477494716644287
Clinet index 8, End of Epoch 6/6, Average Loss=3.5489981174468994, Class Loss=0.2012486755847931, Reg Loss=3.3477494716644287
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/40, Loss=4.02363088130951
Loss made of: CE 0.32105571031570435, LKD 3.5262069702148438, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/40, Loss=4.083637055754662
Loss made of: CE 0.3338507413864136, LKD 3.1179141998291016, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/40, Loss=3.714594265818596
Loss made of: CE 0.4423052668571472, LKD 3.609309434890747, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/40, Loss=3.78209744989872
Loss made of: CE 0.33539026975631714, LKD 4.861071586608887, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.36449387669563293, Reg Loss=3.5364959239959717
Clinet index 12, End of Epoch 1/6, Average Loss=3.9009897708892822, Class Loss=0.36449387669563293, Reg Loss=3.5364959239959717
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/40, Loss=4.146289527416229
Loss made of: CE 0.2188493013381958, LKD 3.810046672821045, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/40, Loss=3.711121343076229
Loss made of: CE 0.21946188807487488, LKD 3.4431889057159424, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/40, Loss=3.9230968356132507
Loss made of: CE 0.3247472047805786, LKD 2.746155023574829, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/40, Loss=3.782096715271473
Loss made of: CE 0.25317370891571045, LKD 4.032139301300049, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2815130054950714, Reg Loss=3.609138250350952
Clinet index 12, End of Epoch 2/6, Average Loss=3.890651226043701, Class Loss=0.2815130054950714, Reg Loss=3.609138250350952
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/40, Loss=3.785430374741554
Loss made of: CE 0.20707246661186218, LKD 3.791649341583252, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/40, Loss=3.5238015219569205
Loss made of: CE 0.2619895040988922, LKD 3.48923397064209, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/40, Loss=3.5966074466705322
Loss made of: CE 0.21609225869178772, LKD 3.5147924423217773, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/40, Loss=3.8052235692739487
Loss made of: CE 0.25253570079803467, LKD 3.6194510459899902, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2487899363040924, Reg Loss=3.428975820541382
Clinet index 12, End of Epoch 3/6, Average Loss=3.6777658462524414, Class Loss=0.2487899363040924, Reg Loss=3.428975820541382
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/40, Loss=3.7478550761938094
Loss made of: CE 0.22753489017486572, LKD 3.2569189071655273, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/40, Loss=3.423760451376438
Loss made of: CE 0.18771913647651672, LKD 3.302539110183716, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/40, Loss=3.851134662330151
Loss made of: CE 0.2112567126750946, LKD 2.6781058311462402, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/40, Loss=3.7358898147940636
Loss made of: CE 0.24327301979064941, LKD 3.959859609603882, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.22756639122962952, Reg Loss=3.4620938301086426
Clinet index 12, End of Epoch 4/6, Average Loss=3.6896603107452393, Class Loss=0.22756639122962952, Reg Loss=3.4620938301086426
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/40, Loss=3.8627974465489388
Loss made of: CE 0.23602621257305145, LKD 3.24031925201416, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/40, Loss=3.444532372057438
Loss made of: CE 0.20954859256744385, LKD 3.9166982173919678, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/40, Loss=3.370780372619629
Loss made of: CE 0.16796299815177917, LKD 3.380235195159912, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/40, Loss=3.797627219557762
Loss made of: CE 0.21357975900173187, LKD 3.4146502017974854, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.20481622219085693, Reg Loss=3.4141182899475098
Clinet index 12, End of Epoch 5/6, Average Loss=3.6189346313476562, Class Loss=0.20481622219085693, Reg Loss=3.4141182899475098
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/40, Loss=3.7347665920853617
Loss made of: CE 0.16589641571044922, LKD 4.163264751434326, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/40, Loss=3.4793889060616494
Loss made of: CE 0.2043827772140503, LKD 3.6575417518615723, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/40, Loss=3.713769568502903
Loss made of: CE 0.20366017520427704, LKD 3.174116373062134, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/40, Loss=3.4396793469786644
Loss made of: CE 0.15789537131786346, LKD 2.726560115814209, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.21071794629096985, Reg Loss=3.381183385848999
Clinet index 12, End of Epoch 6/6, Average Loss=3.5919013023376465, Class Loss=0.21071794629096985, Reg Loss=3.381183385848999
federated aggregation...
Validation, Class Loss=0.2639264762401581, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.900212
Mean Acc: 0.544488
FreqW Acc: 0.821491
Mean IoU: 0.486578
Class IoU:
	class 0: 0.9126459
	class 1: 0.53766584
	class 2: 0.11302365
	class 3: 0.01431207
	class 4: 0.54758215
	class 5: 0.47703883
	class 6: 0.709527
	class 7: 0.46140608
	class 8: 0.60599995
Class Acc:
	class 0: 0.98724097
	class 1: 0.539303
	class 2: 0.13538723
	class 3: 0.014315307
	class 4: 0.5703443
	class 5: 0.5287372
	class 6: 0.7270686
	class 7: 0.4822729
	class 8: 0.9157247

federated global round: 7, step: 1
select part of clients to conduct local training
[0, 6, 13, 5]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/52, Loss=3.777635228633881
Loss made of: CE 0.5142075419425964, LKD 3.634121894836426, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/52, Loss=3.9080444842576982
Loss made of: CE 0.29123830795288086, LKD 4.01790714263916, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.5032283410429956
Loss made of: CE 0.2671283483505249, LKD 3.086594343185425, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.211991509795189
Loss made of: CE 0.26893502473831177, LKD 2.5638370513916016, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.6328839913010595
Loss made of: CE 0.17230360209941864, LKD 2.311396360397339, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3086487948894501, Reg Loss=3.2926430702209473
Clinet index 0, End of Epoch 1/6, Average Loss=3.6012918949127197, Class Loss=0.3086487948894501, Reg Loss=3.2926430702209473
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/52, Loss=3.4816457465291024
Loss made of: CE 0.21366921067237854, LKD 3.3328146934509277, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.595569019019604
Loss made of: CE 0.22627559304237366, LKD 3.2872631549835205, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.600101850926876
Loss made of: CE 0.3409649729728699, LKD 2.801593780517578, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.61652180403471
Loss made of: CE 0.28767770528793335, LKD 4.176681995391846, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.6632508277893066
Loss made of: CE 0.23826584219932556, LKD 3.7719078063964844, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.24495896697044373, Reg Loss=3.307375192642212
Clinet index 0, End of Epoch 2/6, Average Loss=3.5523340702056885, Class Loss=0.24495896697044373, Reg Loss=3.307375192642212
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/52, Loss=3.6524558544158934
Loss made of: CE 0.19529494643211365, LKD 3.3954970836639404, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.9384009703993796
Loss made of: CE 0.28686976432800293, LKD 3.6213197708129883, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.6949320673942565
Loss made of: CE 0.3165895640850067, LKD 4.696223735809326, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.38328917324543
Loss made of: CE 0.24881026148796082, LKD 2.3484175205230713, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.152103562653065
Loss made of: CE 0.22314809262752533, LKD 3.4133429527282715, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2441888153553009, Reg Loss=3.3076343536376953
Clinet index 0, End of Epoch 3/6, Average Loss=3.551823139190674, Class Loss=0.2441888153553009, Reg Loss=3.3076343536376953
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/52, Loss=3.632192279398441
Loss made of: CE 0.33597034215927124, LKD 3.5394716262817383, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.2597249522805214
Loss made of: CE 0.15809118747711182, LKD 3.5093538761138916, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.638881376385689
Loss made of: CE 0.22526304423809052, LKD 3.064451217651367, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.700324596464634
Loss made of: CE 0.18639181554317474, LKD 2.3743839263916016, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.2485803425312043
Loss made of: CE 0.20156341791152954, LKD 3.351029396057129, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2232244610786438, Reg Loss=3.2676053047180176
Clinet index 0, End of Epoch 4/6, Average Loss=3.4908297061920166, Class Loss=0.2232244610786438, Reg Loss=3.2676053047180176
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/52, Loss=3.510993604362011
Loss made of: CE 0.2679496705532074, LKD 3.252899169921875, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.466709592938423
Loss made of: CE 0.15058743953704834, LKD 3.2597434520721436, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.508855913579464
Loss made of: CE 0.2155710905790329, LKD 2.958629846572876, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.3390533179044724
Loss made of: CE 0.24616742134094238, LKD 3.4253921508789062, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.5239688947796823
Loss made of: CE 0.21312808990478516, LKD 3.326315402984619, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2135848104953766, Reg Loss=3.272737503051758
Clinet index 0, End of Epoch 5/6, Average Loss=3.4863224029541016, Class Loss=0.2135848104953766, Reg Loss=3.272737503051758
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/52, Loss=3.315563304722309
Loss made of: CE 0.19740158319473267, LKD 3.316038131713867, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.7011834412813185
Loss made of: CE 0.16398786008358002, LKD 3.0808260440826416, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.5294601291418077
Loss made of: CE 0.2288917750120163, LKD 2.56889009475708, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.4579089030623438
Loss made of: CE 0.21399089694023132, LKD 2.098161458969116, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.431297129392624
Loss made of: CE 0.19836318492889404, LKD 2.25515079498291, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.21786995232105255, Reg Loss=3.291172742843628
Clinet index 0, End of Epoch 6/6, Average Loss=3.509042739868164, Class Loss=0.21786995232105255, Reg Loss=3.291172742843628
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/34, Loss=4.034000711143017
Loss made of: CE 0.41994088888168335, LKD 3.2088704109191895, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=3.742878605425358
Loss made of: CE 0.21116387844085693, LKD 3.2570910453796387, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.285395258665085
Loss made of: CE 0.2657581865787506, LKD 3.5005955696105957, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.31959646940231323, Reg Loss=3.663115978240967
Clinet index 6, End of Epoch 1/6, Average Loss=3.982712507247925, Class Loss=0.31959646940231323, Reg Loss=3.663115978240967
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/34, Loss=3.6582119807600977
Loss made of: CE 0.22367233037948608, LKD 3.771779775619507, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=3.685512453317642
Loss made of: CE 0.2291196882724762, LKD 3.7553064823150635, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.19355511367321
Loss made of: CE 0.18147866427898407, LKD 4.6466193199157715, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.22520580887794495, Reg Loss=3.594005584716797
Clinet index 6, End of Epoch 2/6, Average Loss=3.819211483001709, Class Loss=0.22520580887794495, Reg Loss=3.594005584716797
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/34, Loss=3.8049970135092734
Loss made of: CE 0.22138914465904236, LKD 4.001339435577393, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=3.8525393918156623
Loss made of: CE 0.17318439483642578, LKD 3.1368188858032227, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.014908228814602
Loss made of: CE 0.25493094325065613, LKD 3.497035026550293, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.21715793013572693, Reg Loss=3.6727774143218994
Clinet index 6, End of Epoch 3/6, Average Loss=3.889935255050659, Class Loss=0.21715793013572693, Reg Loss=3.6727774143218994
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/34, Loss=3.9549153968691826
Loss made of: CE 0.16628994047641754, LKD 3.215027332305908, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=3.3939099475741386
Loss made of: CE 0.2111072540283203, LKD 2.5662477016448975, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=3.8227602154016496
Loss made of: CE 0.35919758677482605, LKD 3.954437255859375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2129410207271576, Reg Loss=3.5394346714019775
Clinet index 6, End of Epoch 4/6, Average Loss=3.752375602722168, Class Loss=0.2129410207271576, Reg Loss=3.5394346714019775
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/34, Loss=3.7483494967222213
Loss made of: CE 0.1501101851463318, LKD 2.988563299179077, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=3.7718322679400442
Loss made of: CE 0.22776755690574646, LKD 4.688746452331543, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=3.691140016913414
Loss made of: CE 0.17793028056621552, LKD 4.0715227127075195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.21086569130420685, Reg Loss=3.531856060028076
Clinet index 6, End of Epoch 5/6, Average Loss=3.7427217960357666, Class Loss=0.21086569130420685, Reg Loss=3.531856060028076
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/34, Loss=3.7811062783002853
Loss made of: CE 0.2530350089073181, LKD 2.5256967544555664, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 20/34, Loss=3.727210482954979
Loss made of: CE 0.2666069269180298, LKD 4.131526470184326, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=3.6739617571234704
Loss made of: CE 0.15053904056549072, LKD 3.664377212524414, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.20437778532505035, Reg Loss=3.4946632385253906
Clinet index 6, End of Epoch 6/6, Average Loss=3.6990411281585693, Class Loss=0.20437778532505035, Reg Loss=3.4946632385253906
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=3.7287680089473723
Loss made of: CE 0.5377507209777832, LKD 3.7676587104797363, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/33, Loss=3.598072975873947
Loss made of: CE 0.5153770446777344, LKD 2.108893871307373, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/33, Loss=3.183752605319023
Loss made of: CE 0.2885260581970215, LKD 3.0251543521881104, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.533711314201355, Reg Loss=2.9704039096832275
Clinet index 13, End of Epoch 1/6, Average Loss=3.504115104675293, Class Loss=0.533711314201355, Reg Loss=2.9704039096832275
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/33, Loss=2.9410007894039154
Loss made of: CE 0.32284995913505554, LKD 3.3019776344299316, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/33, Loss=3.1734229192137717
Loss made of: CE 0.3008648753166199, LKD 2.572829008102417, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/33, Loss=2.7812509432435037
Loss made of: CE 0.37234994769096375, LKD 3.3032073974609375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2808014750480652, Reg Loss=2.7016873359680176
Clinet index 13, End of Epoch 2/6, Average Loss=2.9824888706207275, Class Loss=0.2808014750480652, Reg Loss=2.7016873359680176
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/33, Loss=2.9283498764038085
Loss made of: CE 0.2145950049161911, LKD 2.5202250480651855, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/33, Loss=3.0412994369864466
Loss made of: CE 0.2170010507106781, LKD 3.2915070056915283, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/33, Loss=2.9760549545288084
Loss made of: CE 0.19048713147640228, LKD 2.1565115451812744, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.24998405575752258, Reg Loss=2.7553439140319824
Clinet index 13, End of Epoch 3/6, Average Loss=3.0053279399871826, Class Loss=0.24998405575752258, Reg Loss=2.7553439140319824
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/33, Loss=3.043081906437874
Loss made of: CE 0.1678156703710556, LKD 1.803687572479248, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/33, Loss=2.6683823645114897
Loss made of: CE 0.23554925620555878, LKD 2.522313117980957, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/33, Loss=2.8825333595275877
Loss made of: CE 0.26385313272476196, LKD 2.32608699798584, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.22833287715911865, Reg Loss=2.6261985301971436
Clinet index 13, End of Epoch 4/6, Average Loss=2.8545312881469727, Class Loss=0.22833287715911865, Reg Loss=2.6261985301971436
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/33, Loss=2.829342521727085
Loss made of: CE 0.2620334029197693, LKD 3.325481653213501, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/33, Loss=3.119225877523422
Loss made of: CE 0.1982201784849167, LKD 2.5038793087005615, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/33, Loss=3.006419663131237
Loss made of: CE 0.15000006556510925, LKD 1.7439299821853638, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.225202739238739, Reg Loss=2.733985662460327
Clinet index 13, End of Epoch 5/6, Average Loss=2.959188461303711, Class Loss=0.225202739238739, Reg Loss=2.733985662460327
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/33, Loss=3.0572038516402245
Loss made of: CE 0.252290278673172, LKD 3.0333971977233887, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/33, Loss=2.887210676074028
Loss made of: CE 0.17517244815826416, LKD 2.8441152572631836, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/33, Loss=3.0256116941571234
Loss made of: CE 0.19783660769462585, LKD 1.9051082134246826, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.22534169256687164, Reg Loss=2.6832473278045654
Clinet index 13, End of Epoch 6/6, Average Loss=2.9085891246795654, Class Loss=0.22534169256687164, Reg Loss=2.6832473278045654
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/34, Loss=4.031933674216271
Loss made of: CE 0.40576499700546265, LKD 3.1492366790771484, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.076765647530555
Loss made of: CE 0.28833669424057007, LKD 3.987272024154663, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.103315138816834
Loss made of: CE 0.2771145701408386, LKD 5.818140506744385, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3185683488845825, Reg Loss=3.7534584999084473
Clinet index 5, End of Epoch 1/6, Average Loss=4.07202672958374, Class Loss=0.3185683488845825, Reg Loss=3.7534584999084473
Pseudo labeling is: None
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/34, Loss=4.105769589543343
Loss made of: CE 0.20709256827831268, LKD 3.6399714946746826, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=3.5185375213623047
Loss made of: CE 0.2110626995563507, LKD 3.4898054599761963, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.190193611383438
Loss made of: CE 0.25328028202056885, LKD 4.923077583312988, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.23778514564037323, Reg Loss=3.6802585124969482
Clinet index 5, End of Epoch 2/6, Average Loss=3.918043613433838, Class Loss=0.23778514564037323, Reg Loss=3.6802585124969482
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/34, Loss=3.7424667939543723
Loss made of: CE 0.2861635386943817, LKD 3.910971164703369, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=3.7467360839247705
Loss made of: CE 0.19206993281841278, LKD 3.6701853275299072, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.031619395315647
Loss made of: CE 0.23927518725395203, LKD 3.834650993347168, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2110091894865036, Reg Loss=3.665128231048584
Clinet index 5, End of Epoch 3/6, Average Loss=3.8761374950408936, Class Loss=0.2110091894865036, Reg Loss=3.665128231048584
Pseudo labeling is: None
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/34, Loss=3.676284220814705
Loss made of: CE 0.18893137574195862, LKD 3.395174503326416, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 20/34, Loss=3.9245258912444116
Loss made of: CE 0.2183104008436203, LKD 3.6836843490600586, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=3.712863676249981
Loss made of: CE 0.23056170344352722, LKD 3.743271827697754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.20870959758758545, Reg Loss=3.5603084564208984
Clinet index 5, End of Epoch 4/6, Average Loss=3.7690181732177734, Class Loss=0.20870959758758545, Reg Loss=3.5603084564208984
Pseudo labeling is: None
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/34, Loss=3.903246599435806
Loss made of: CE 0.2178439348936081, LKD 3.875058174133301, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=3.5742760643363
Loss made of: CE 0.24067950248718262, LKD 3.422217845916748, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.034753175079823
Loss made of: CE 0.19946223497390747, LKD 4.164742469787598, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2035318911075592, Reg Loss=3.633057117462158
Clinet index 5, End of Epoch 5/6, Average Loss=3.8365890979766846, Class Loss=0.2035318911075592, Reg Loss=3.633057117462158
Pseudo labeling is: None
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/34, Loss=4.074010030925274
Loss made of: CE 0.2404339611530304, LKD 4.011847972869873, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=3.4968120723962786
Loss made of: CE 0.1779102087020874, LKD 3.291012763977051, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=3.8925888016819954
Loss made of: CE 0.22480201721191406, LKD 4.61916446685791, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2036864459514618, Reg Loss=3.633934736251831
Clinet index 5, End of Epoch 6/6, Average Loss=3.8376212120056152, Class Loss=0.2036864459514618, Reg Loss=3.633934736251831
federated aggregation...
Validation, Class Loss=0.2419162541627884, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.911949
Mean Acc: 0.572158
FreqW Acc: 0.843265
Mean IoU: 0.497082
Class IoU:
	class 0: 0.93067294
	class 1: 0.5512619
	class 2: 0.13534798
	class 3: 0.008331091
	class 4: 0.5586568
	class 5: 0.17003965
	class 6: 0.79239327
	class 7: 0.7089382
	class 8: 0.61809313
Class Acc:
	class 0: 0.98245275
	class 1: 0.5535252
	class 2: 0.17127724
	class 3: 0.008332365
	class 4: 0.58431315
	class 5: 0.17021368
	class 6: 0.9245672
	class 7: 0.8283845
	class 8: 0.9263556

federated global round: 8, step: 1
select part of clients to conduct local training
[4, 11, 9, 6]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=2.922750008106232
Loss made of: CE 0.4614139497280121, LKD 2.6170737743377686, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/25, Loss=2.612626162171364
Loss made of: CE 0.32029885053634644, LKD 2.378964900970459, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4099498987197876, Reg Loss=2.325894832611084
Clinet index 4, End of Epoch 1/6, Average Loss=2.735844612121582, Class Loss=0.4099498987197876, Reg Loss=2.325894832611084
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/25, Loss=2.4255089700222014
Loss made of: CE 0.3056778609752655, LKD 1.9193731546401978, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/25, Loss=2.421844981610775
Loss made of: CE 0.29885029792785645, LKD 3.2728545665740967, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.26701611280441284, Reg Loss=2.196549892425537
Clinet index 4, End of Epoch 2/6, Average Loss=2.4635660648345947, Class Loss=0.26701611280441284, Reg Loss=2.196549892425537
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/25, Loss=2.327232600748539
Loss made of: CE 0.18691538274288177, LKD 1.8723149299621582, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/25, Loss=2.6217803984880446
Loss made of: CE 0.24546381831169128, LKD 2.3602962493896484, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.23315498232841492, Reg Loss=2.2237491607666016
Clinet index 4, End of Epoch 3/6, Average Loss=2.456904172897339, Class Loss=0.23315498232841492, Reg Loss=2.2237491607666016
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/25, Loss=2.442891852557659
Loss made of: CE 0.22518295049667358, LKD 2.172783374786377, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/25, Loss=2.3806588634848596
Loss made of: CE 0.19754093885421753, LKD 3.2032079696655273, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1974976807832718, Reg Loss=2.1594078540802
Clinet index 4, End of Epoch 4/6, Average Loss=2.356905460357666, Class Loss=0.1974976807832718, Reg Loss=2.1594078540802
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/25, Loss=2.365643225610256
Loss made of: CE 0.1837703287601471, LKD 2.4551565647125244, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/25, Loss=2.573332816362381
Loss made of: CE 0.3896793723106384, LKD 2.6344738006591797, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.20914334058761597, Reg Loss=2.1629648208618164
Clinet index 4, End of Epoch 5/6, Average Loss=2.372108221054077, Class Loss=0.20914334058761597, Reg Loss=2.1629648208618164
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/25, Loss=2.3650555461645126
Loss made of: CE 0.2475317418575287, LKD 2.731577157974243, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/25, Loss=2.483972300589085
Loss made of: CE 0.20151373744010925, LKD 2.0416672229766846, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.18268254399299622, Reg Loss=2.2104039192199707
Clinet index 4, End of Epoch 6/6, Average Loss=2.3930864334106445, Class Loss=0.18268254399299622, Reg Loss=2.2104039192199707
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/43, Loss=2.982107290625572
Loss made of: CE 0.2738765478134155, LKD 1.7462732791900635, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/43, Loss=2.993683621287346
Loss made of: CE 0.3768402338027954, LKD 3.3036093711853027, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/43, Loss=2.777870903909206
Loss made of: CE 0.22733955085277557, LKD 1.8807893991470337, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/43, Loss=2.8074098125100138
Loss made of: CE 0.38936543464660645, LKD 3.215029001235962, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3569037914276123, Reg Loss=2.5241377353668213
Clinet index 11, End of Epoch 1/6, Average Loss=2.8810415267944336, Class Loss=0.3569037914276123, Reg Loss=2.5241377353668213
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/43, Loss=2.4775742799043656
Loss made of: CE 0.2006470412015915, LKD 1.5282604694366455, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/43, Loss=2.70643560141325
Loss made of: CE 0.1996862143278122, LKD 1.9875255823135376, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/43, Loss=2.921903072297573
Loss made of: CE 0.2779615521430969, LKD 2.702563762664795, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/43, Loss=2.7183945283293722
Loss made of: CE 0.19661560654640198, LKD 1.705678939819336, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.25429001450538635, Reg Loss=2.477224111557007
Clinet index 11, End of Epoch 2/6, Average Loss=2.7315142154693604, Class Loss=0.25429001450538635, Reg Loss=2.477224111557007
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/43, Loss=2.825442287325859
Loss made of: CE 0.23867103457450867, LKD 2.5491998195648193, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/43, Loss=2.517251858115196
Loss made of: CE 0.24435584247112274, LKD 1.6796032190322876, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/43, Loss=2.597547894716263
Loss made of: CE 0.20936481654644012, LKD 2.0450267791748047, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/43, Loss=2.9360390812158585
Loss made of: CE 0.20651471614837646, LKD 2.021475315093994, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.23579269647598267, Reg Loss=2.4821856021881104
Clinet index 11, End of Epoch 3/6, Average Loss=2.7179782390594482, Class Loss=0.23579269647598267, Reg Loss=2.4821856021881104
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/43, Loss=2.6886541157960893
Loss made of: CE 0.21530674397945404, LKD 3.5334668159484863, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/43, Loss=2.589489160478115
Loss made of: CE 0.2562680244445801, LKD 3.1009345054626465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/43, Loss=2.661616067588329
Loss made of: CE 0.21871744096279144, LKD 2.1129672527313232, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/43, Loss=2.894286625087261
Loss made of: CE 0.1943235844373703, LKD 1.858054518699646, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.22254261374473572, Reg Loss=2.4610209465026855
Clinet index 11, End of Epoch 4/6, Average Loss=2.683563470840454, Class Loss=0.22254261374473572, Reg Loss=2.4610209465026855
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/43, Loss=2.7923093527555465
Loss made of: CE 0.18096671998500824, LKD 3.07779598236084, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/43, Loss=2.656532433629036
Loss made of: CE 0.24556642770767212, LKD 2.105858564376831, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/43, Loss=2.7489609092473986
Loss made of: CE 0.16184955835342407, LKD 1.7984271049499512, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/43, Loss=2.483597342669964
Loss made of: CE 0.19085125625133514, LKD 2.200758934020996, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2223854959011078, Reg Loss=2.4705262184143066
Clinet index 11, End of Epoch 5/6, Average Loss=2.6929116249084473, Class Loss=0.2223854959011078, Reg Loss=2.4705262184143066
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/43, Loss=2.377726113051176
Loss made of: CE 0.17900803685188293, LKD 2.0758297443389893, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/43, Loss=2.457287722826004
Loss made of: CE 0.18128490447998047, LKD 1.6844955682754517, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/43, Loss=2.7455364570021628
Loss made of: CE 0.19380424916744232, LKD 2.9658966064453125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/43, Loss=2.834798961877823
Loss made of: CE 0.25668272376060486, LKD 2.3938825130462646, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2059510201215744, Reg Loss=2.4176533222198486
Clinet index 11, End of Epoch 6/6, Average Loss=2.6236042976379395, Class Loss=0.2059510201215744, Reg Loss=2.4176533222198486
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/52, Loss=3.709083046019077
Loss made of: CE 0.29865118861198425, LKD 4.010014057159424, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/52, Loss=3.6748498409986494
Loss made of: CE 0.21641312539577484, LKD 3.9513282775878906, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.726251262426376
Loss made of: CE 0.24377088248729706, LKD 3.211911678314209, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.5607473105192184
Loss made of: CE 0.32954350113868713, LKD 3.50337553024292, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.8490527361631393
Loss made of: CE 0.24605558812618256, LKD 2.7115848064422607, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2514498829841614, Reg Loss=3.439570665359497
Clinet index 9, End of Epoch 1/6, Average Loss=3.6910204887390137, Class Loss=0.2514498829841614, Reg Loss=3.439570665359497
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/52, Loss=3.6292235136032103
Loss made of: CE 0.23858503997325897, LKD 3.2764787673950195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.945162281394005
Loss made of: CE 0.2171790599822998, LKD 3.5419135093688965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.027261318266392
Loss made of: CE 0.25068819522857666, LKD 4.19830322265625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.479305037856102
Loss made of: CE 0.16947731375694275, LKD 2.925173044204712, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.646144415438175
Loss made of: CE 0.16737064719200134, LKD 3.4265308380126953, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2385518103837967, Reg Loss=3.5551233291625977
Clinet index 9, End of Epoch 2/6, Average Loss=3.793675184249878, Class Loss=0.2385518103837967, Reg Loss=3.5551233291625977
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/52, Loss=3.65952615737915
Loss made of: CE 0.26502668857574463, LKD 4.041215896606445, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.360695058107376
Loss made of: CE 0.20090794563293457, LKD 2.6951146125793457, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.838272508978844
Loss made of: CE 0.2091616839170456, LKD 2.8166613578796387, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.016142715513706
Loss made of: CE 0.21081195771694183, LKD 4.077857494354248, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.595691126585007
Loss made of: CE 0.2004813402891159, LKD 3.7509701251983643, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.21873190999031067, Reg Loss=3.519792318344116
Clinet index 9, End of Epoch 3/6, Average Loss=3.7385241985321045, Class Loss=0.21873190999031067, Reg Loss=3.519792318344116
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/52, Loss=3.9334723174571993
Loss made of: CE 0.23490647971630096, LKD 3.768195867538452, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.642998445034027
Loss made of: CE 0.20524060726165771, LKD 3.182549476623535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.491120344400406
Loss made of: CE 0.2015160769224167, LKD 3.1856276988983154, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.669453875720501
Loss made of: CE 0.17537719011306763, LKD 4.003394603729248, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.565147215127945
Loss made of: CE 0.2242198884487152, LKD 4.234528064727783, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.21294179558753967, Reg Loss=3.445136547088623
Clinet index 9, End of Epoch 4/6, Average Loss=3.65807843208313, Class Loss=0.21294179558753967, Reg Loss=3.445136547088623
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/52, Loss=3.7029727011919022
Loss made of: CE 0.21569886803627014, LKD 2.5306944847106934, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.6561621323227884
Loss made of: CE 0.20880644023418427, LKD 3.2117199897766113, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.575435829162598
Loss made of: CE 0.2500556707382202, LKD 4.55562686920166, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.7099131152033804
Loss made of: CE 0.1834825873374939, LKD 3.6857941150665283, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.520355688035488
Loss made of: CE 0.17930887639522552, LKD 3.7264552116394043, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.20457640290260315, Reg Loss=3.4033615589141846
Clinet index 9, End of Epoch 5/6, Average Loss=3.607938051223755, Class Loss=0.20457640290260315, Reg Loss=3.4033615589141846
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/52, Loss=3.4607684925198554
Loss made of: CE 0.12963885068893433, LKD 3.2929272651672363, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.9388143599033354
Loss made of: CE 0.1511024534702301, LKD 3.142260789871216, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.598525255918503
Loss made of: CE 0.16308701038360596, LKD 3.335463523864746, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.697333040833473
Loss made of: CE 0.18849560618400574, LKD 3.612973928451538, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.308208191394806
Loss made of: CE 0.18698708713054657, LKD 3.232872486114502, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.19992101192474365, Reg Loss=3.394463300704956
Clinet index 9, End of Epoch 6/6, Average Loss=3.59438419342041, Class Loss=0.19992101192474365, Reg Loss=3.394463300704956
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/34, Loss=3.7364547908306123
Loss made of: CE 0.3149581551551819, LKD 2.94943904876709, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=3.6553863480687143
Loss made of: CE 0.1981101632118225, LKD 3.3354806900024414, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.184948916733265
Loss made of: CE 0.20027457177639008, LKD 3.5986430644989014, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2553784251213074, Reg Loss=3.563204288482666
Clinet index 6, End of Epoch 1/6, Average Loss=3.818582773208618, Class Loss=0.2553784251213074, Reg Loss=3.563204288482666
Pseudo labeling is: None
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/34, Loss=3.548725073039532
Loss made of: CE 0.22982677817344666, LKD 3.748229503631592, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=3.734615793824196
Loss made of: CE 0.19087056815624237, LKD 3.4278762340545654, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=3.992729051411152
Loss made of: CE 0.1592426747083664, LKD 4.323558330535889, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.21121837198734283, Reg Loss=3.5064852237701416
Clinet index 6, End of Epoch 2/6, Average Loss=3.7177035808563232, Class Loss=0.21121837198734283, Reg Loss=3.5064852237701416
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/34, Loss=3.647932752966881
Loss made of: CE 0.18376824259757996, LKD 4.414566516876221, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=3.742846907675266
Loss made of: CE 0.1484227031469345, LKD 3.1828596591949463, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=3.7625932469964027
Loss made of: CE 0.20819970965385437, LKD 3.51875376701355, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.20315180718898773, Reg Loss=3.511712074279785
Clinet index 6, End of Epoch 3/6, Average Loss=3.7148637771606445, Class Loss=0.20315180718898773, Reg Loss=3.511712074279785
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/34, Loss=3.8091793328523638
Loss made of: CE 0.15257778763771057, LKD 2.6781911849975586, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=3.3242982774972916
Loss made of: CE 0.21636638045310974, LKD 3.320274829864502, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=3.890744735300541
Loss made of: CE 0.30735400319099426, LKD 3.7861971855163574, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.19843389093875885, Reg Loss=3.466804027557373
Clinet index 6, End of Epoch 4/6, Average Loss=3.6652379035949707, Class Loss=0.19843389093875885, Reg Loss=3.466804027557373
Pseudo labeling is: None
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/34, Loss=3.7324422374367714
Loss made of: CE 0.11894942820072174, LKD 3.2575581073760986, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=3.628950634598732
Loss made of: CE 0.25472792983055115, LKD 4.687011241912842, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=3.667919969558716
Loss made of: CE 0.16426998376846313, LKD 3.8807132244110107, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.19244487583637238, Reg Loss=3.475862979888916
Clinet index 6, End of Epoch 5/6, Average Loss=3.6683077812194824, Class Loss=0.19244487583637238, Reg Loss=3.475862979888916
Pseudo labeling is: None
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/34, Loss=3.8082947745919227
Loss made of: CE 0.2799043655395508, LKD 2.5853848457336426, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=3.6099306508898734
Loss made of: CE 0.2631012201309204, LKD 4.149326801300049, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=3.601496258378029
Loss made of: CE 0.12893405556678772, LKD 3.3871583938598633, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1956084966659546, Reg Loss=3.4556827545166016
Clinet index 6, End of Epoch 6/6, Average Loss=3.6512913703918457, Class Loss=0.1956084966659546, Reg Loss=3.4556827545166016
federated aggregation...
Validation, Class Loss=0.20448710024356842, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.925147
Mean Acc: 0.638365
FreqW Acc: 0.865401
Mean IoU: 0.574122
Class IoU:
	class 0: 0.9373039
	class 1: 0.61204106
	class 2: 0.13370399
	class 3: 0.102461256
	class 4: 0.60294473
	class 5: 0.5216
	class 6: 0.8678252
	class 7: 0.74828523
	class 8: 0.6409327
Class Acc:
	class 0: 0.984629
	class 1: 0.6152755
	class 2: 0.16708583
	class 3: 0.10265247
	class 4: 0.6416938
	class 5: 0.5379262
	class 6: 0.9259505
	class 7: 0.8695592
	class 8: 0.9005108

federated global round: 9, step: 1
select part of clients to conduct local training
[5, 0, 9, 2]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/34, Loss=4.1414755314588545
Loss made of: CE 0.2916162610054016, LKD 3.0179009437561035, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.109152357280254
Loss made of: CE 0.32138657569885254, LKD 3.935429573059082, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=3.9853049352765084
Loss made of: CE 0.3172142505645752, LKD 5.428361892700195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2914900481700897, Reg Loss=3.7857751846313477
Clinet index 5, End of Epoch 1/6, Average Loss=4.07726526260376, Class Loss=0.2914900481700897, Reg Loss=3.7857751846313477
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/34, Loss=3.8998770982027056
Loss made of: CE 0.18945690989494324, LKD 3.5504109859466553, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=3.596698631346226
Loss made of: CE 0.18412266671657562, LKD 3.6842188835144043, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.092369289696217
Loss made of: CE 0.2451317310333252, LKD 4.654059886932373, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.21879833936691284, Reg Loss=3.633307933807373
Clinet index 5, End of Epoch 2/6, Average Loss=3.8521063327789307, Class Loss=0.21879833936691284, Reg Loss=3.633307933807373
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/34, Loss=3.6736515030264854
Loss made of: CE 0.2690882980823517, LKD 3.9471395015716553, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=3.4759064242243767
Loss made of: CE 0.182418555021286, LKD 3.082846164703369, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=3.9417320653796195
Loss made of: CE 0.2772400379180908, LKD 4.1529130935668945, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.20159798860549927, Reg Loss=3.559717893600464
Clinet index 5, End of Epoch 3/6, Average Loss=3.7613158226013184, Class Loss=0.20159798860549927, Reg Loss=3.559717893600464
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/34, Loss=3.596863792836666
Loss made of: CE 0.18018755316734314, LKD 3.199803352355957, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 20/34, Loss=3.845775856077671
Loss made of: CE 0.25618070363998413, LKD 3.5324201583862305, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=3.5857006534934044
Loss made of: CE 0.19651538133621216, LKD 3.6618926525115967, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1988237500190735, Reg Loss=3.486311197280884
Clinet index 5, End of Epoch 4/6, Average Loss=3.6851348876953125, Class Loss=0.1988237500190735, Reg Loss=3.486311197280884
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/34, Loss=3.7763679921627045
Loss made of: CE 0.1750163733959198, LKD 3.909608840942383, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=3.550946818292141
Loss made of: CE 0.2171773910522461, LKD 3.3161797523498535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=3.8568517535924913
Loss made of: CE 0.21807877719402313, LKD 3.4418139457702637, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.19455870985984802, Reg Loss=3.5460093021392822
Clinet index 5, End of Epoch 5/6, Average Loss=3.740567922592163, Class Loss=0.19455870985984802, Reg Loss=3.5460093021392822
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/34, Loss=3.9586735546588896
Loss made of: CE 0.2650746703147888, LKD 3.548936605453491, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=3.5779257521033285
Loss made of: CE 0.165277361869812, LKD 3.42472505569458, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=3.8531068056821822
Loss made of: CE 0.25370413064956665, LKD 4.696425914764404, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.19812263548374176, Reg Loss=3.601405620574951
Clinet index 5, End of Epoch 6/6, Average Loss=3.7995283603668213, Class Loss=0.19812263548374176, Reg Loss=3.601405620574951
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/52, Loss=3.613920693099499
Loss made of: CE 0.3718324601650238, LKD 3.3897340297698975, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/52, Loss=3.69921049028635
Loss made of: CE 0.24553298950195312, LKD 4.322706699371338, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.4052908405661584
Loss made of: CE 0.21808454394340515, LKD 2.5779645442962646, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.082306095957756
Loss made of: CE 0.2079693078994751, LKD 2.535120964050293, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.5977787867188455
Loss made of: CE 0.16987159848213196, LKD 2.3685319423675537, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.24619631469249725, Reg Loss=3.2411813735961914
Clinet index 0, End of Epoch 1/6, Average Loss=3.487377643585205, Class Loss=0.24619631469249725, Reg Loss=3.2411813735961914
Pseudo labeling is: None
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/52, Loss=3.350459486246109
Loss made of: CE 0.20741474628448486, LKD 3.0298662185668945, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.485708186030388
Loss made of: CE 0.1961289346218109, LKD 3.02309250831604, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.525690057873726
Loss made of: CE 0.28150293231010437, LKD 2.4397521018981934, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 40/52, Loss=3.549338361620903
Loss made of: CE 0.267494261264801, LKD 4.127290725708008, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.4328649818897246
Loss made of: CE 0.19844703376293182, LKD 4.034450531005859, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.21662423014640808, Reg Loss=3.216174840927124
Clinet index 0, End of Epoch 2/6, Average Loss=3.4327991008758545, Class Loss=0.21662423014640808, Reg Loss=3.216174840927124
Pseudo labeling is: None
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/52, Loss=3.537960651516914
Loss made of: CE 0.19359497725963593, LKD 3.471907138824463, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.8149267345666886
Loss made of: CE 0.25280022621154785, LKD 3.808734655380249, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.4279961556196215
Loss made of: CE 0.3125433921813965, LKD 4.0647873878479, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.3608518928289413
Loss made of: CE 0.22181019186973572, LKD 2.454484701156616, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.171827144920826
Loss made of: CE 0.21408414840698242, LKD 3.6801278591156006, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.21798859536647797, Reg Loss=3.2245829105377197
Clinet index 0, End of Epoch 3/6, Average Loss=3.4425714015960693, Class Loss=0.21798859536647797, Reg Loss=3.2245829105377197
Pseudo labeling is: None
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/52, Loss=3.5081886395812036
Loss made of: CE 0.28884047269821167, LKD 3.4519200325012207, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.2225193977355957
Loss made of: CE 0.16038352251052856, LKD 3.6905806064605713, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.4690955564379693
Loss made of: CE 0.22054652869701385, LKD 2.6721949577331543, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.6787015780806542
Loss made of: CE 0.17020708322525024, LKD 2.5154292583465576, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.218219240009785
Loss made of: CE 0.17417746782302856, LKD 3.138155460357666, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2044026255607605, Reg Loss=3.2022013664245605
Clinet index 0, End of Epoch 4/6, Average Loss=3.406604051589966, Class Loss=0.2044026255607605, Reg Loss=3.2022013664245605
Pseudo labeling is: None
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/52, Loss=3.3362307846546173
Loss made of: CE 0.2344512641429901, LKD 3.109166383743286, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.3688820898532867
Loss made of: CE 0.16023173928260803, LKD 3.0022804737091064, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.4118117466568947
Loss made of: CE 0.23012368381023407, LKD 3.049476385116577, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.3214042827486994
Loss made of: CE 0.24992066621780396, LKD 3.0751659870147705, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.4681421652436257
Loss made of: CE 0.21727406978607178, LKD 3.227344036102295, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.203877255320549, Reg Loss=3.186260938644409
Clinet index 0, End of Epoch 5/6, Average Loss=3.3901381492614746, Class Loss=0.203877255320549, Reg Loss=3.186260938644409
Pseudo labeling is: None
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/52, Loss=3.320649376511574
Loss made of: CE 0.18958300352096558, LKD 3.1629867553710938, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.5639868855476378
Loss made of: CE 0.1522025167942047, LKD 2.981733560562134, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.2644527688622476
Loss made of: CE 0.19848358631134033, LKD 2.2604262828826904, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.4120514392852783
Loss made of: CE 0.18590402603149414, LKD 2.2253236770629883, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.315464435517788
Loss made of: CE 0.18070751428604126, LKD 2.0521631240844727, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.20358924567699432, Reg Loss=3.177450656890869
Clinet index 0, End of Epoch 6/6, Average Loss=3.38103985786438, Class Loss=0.20358924567699432, Reg Loss=3.177450656890869
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/52, Loss=3.6415046110749243
Loss made of: CE 0.30034753680229187, LKD 3.961270332336426, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/52, Loss=3.676511551439762
Loss made of: CE 0.24834084510803223, LKD 3.4602787494659424, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.8394289389252663
Loss made of: CE 0.24287931621074677, LKD 3.4268386363983154, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.5895159408450126
Loss made of: CE 0.3191119432449341, LKD 3.8242709636688232, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.7921282589435577
Loss made of: CE 0.2456178367137909, LKD 2.585188150405884, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2579289376735687, Reg Loss=3.4416627883911133
Clinet index 9, End of Epoch 1/6, Average Loss=3.699591636657715, Class Loss=0.2579289376735687, Reg Loss=3.4416627883911133
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/52, Loss=3.5969871491193772
Loss made of: CE 0.26636314392089844, LKD 3.6880836486816406, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.785143715143204
Loss made of: CE 0.23576049506664276, LKD 3.644988775253296, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.9004485860466955
Loss made of: CE 0.2298244833946228, LKD 4.025465965270996, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.305440606176853
Loss made of: CE 0.1478274017572403, LKD 2.639540195465088, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.5592095017433167
Loss made of: CE 0.17778658866882324, LKD 3.374436855316162, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2323068529367447, Reg Loss=3.4387288093566895
Clinet index 9, End of Epoch 2/6, Average Loss=3.6710357666015625, Class Loss=0.2323068529367447, Reg Loss=3.4387288093566895
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/52, Loss=3.5080225080251695
Loss made of: CE 0.24187755584716797, LKD 4.311425685882568, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.232359704375267
Loss made of: CE 0.1972842663526535, LKD 2.5766189098358154, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.785028912127018
Loss made of: CE 0.19017133116722107, LKD 2.6147286891937256, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.899280957877636
Loss made of: CE 0.19489368796348572, LKD 3.618856430053711, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.5342034071683885
Loss made of: CE 0.20855878293514252, LKD 3.59354305267334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2179718315601349, Reg Loss=3.4190239906311035
Clinet index 9, End of Epoch 3/6, Average Loss=3.636995792388916, Class Loss=0.2179718315601349, Reg Loss=3.4190239906311035
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/52, Loss=3.6966484054923057
Loss made of: CE 0.22602570056915283, LKD 3.9177968502044678, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.635517717897892
Loss made of: CE 0.21210896968841553, LKD 2.723237991333008, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.4476692512631417
Loss made of: CE 0.20821186900138855, LKD 3.286348342895508, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.5968244284391404
Loss made of: CE 0.17825965583324432, LKD 4.039393901824951, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.5590916469693186
Loss made of: CE 0.19128406047821045, LKD 3.9118807315826416, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2118101865053177, Reg Loss=3.356149673461914
Clinet index 9, End of Epoch 4/6, Average Loss=3.567959785461426, Class Loss=0.2118101865053177, Reg Loss=3.356149673461914
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/52, Loss=3.6963771760463713
Loss made of: CE 0.1918419897556305, LKD 2.3867573738098145, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.6259248062968252
Loss made of: CE 0.1971176713705063, LKD 3.7513957023620605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.576070521771908
Loss made of: CE 0.2441929429769516, LKD 4.303389072418213, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.5970152094960213
Loss made of: CE 0.1848267763853073, LKD 3.6333231925964355, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.460135020315647
Loss made of: CE 0.19280311465263367, LKD 3.637322425842285, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.20922733843326569, Reg Loss=3.3630197048187256
Clinet index 9, End of Epoch 5/6, Average Loss=3.57224702835083, Class Loss=0.20922733843326569, Reg Loss=3.3630197048187256
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/52, Loss=3.3586623534560203
Loss made of: CE 0.15884536504745483, LKD 3.0263898372650146, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.7646581277251245
Loss made of: CE 0.15475121140480042, LKD 2.9223039150238037, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.6156545326113703
Loss made of: CE 0.16504523158073425, LKD 3.3508739471435547, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.715776300430298
Loss made of: CE 0.23697654902935028, LKD 4.080297946929932, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.1868890523910522
Loss made of: CE 0.20008501410484314, LKD 3.0224721431732178, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.21019351482391357, Reg Loss=3.3200395107269287
Clinet index 9, End of Epoch 6/6, Average Loss=3.5302329063415527, Class Loss=0.21019351482391357, Reg Loss=3.3200395107269287
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=2.751029282808304
Loss made of: CE 0.33595287799835205, LKD 2.912961483001709, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/25, Loss=2.662704722583294
Loss made of: CE 0.32760781049728394, LKD 2.49892520904541, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.30190131068229675, Reg Loss=2.376424551010132
Clinet index 2, End of Epoch 1/6, Average Loss=2.678325891494751, Class Loss=0.30190131068229675, Reg Loss=2.376424551010132
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/25, Loss=2.5529591917991636
Loss made of: CE 0.21677523851394653, LKD 3.0607123374938965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/25, Loss=2.7258842661976814
Loss made of: CE 0.16176149249076843, LKD 2.1396708488464355, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.21929894387722015, Reg Loss=2.2890636920928955
Clinet index 2, End of Epoch 2/6, Average Loss=2.5083625316619873, Class Loss=0.21929894387722015, Reg Loss=2.2890636920928955
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/25, Loss=2.4512098342180253
Loss made of: CE 0.13574689626693726, LKD 1.800566554069519, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/25, Loss=2.3541600570082664
Loss made of: CE 0.19464147090911865, LKD 2.259235382080078, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.19658054411411285, Reg Loss=2.183304786682129
Clinet index 2, End of Epoch 3/6, Average Loss=2.37988543510437, Class Loss=0.19658054411411285, Reg Loss=2.183304786682129
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/25, Loss=2.412682420015335
Loss made of: CE 0.19752001762390137, LKD 2.463716983795166, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/25, Loss=2.4627545565366744
Loss made of: CE 0.15467701852321625, LKD 1.8631902933120728, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.18673890829086304, Reg Loss=2.2311694622039795
Clinet index 2, End of Epoch 4/6, Average Loss=2.4179084300994873, Class Loss=0.18673890829086304, Reg Loss=2.2311694622039795
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/25, Loss=2.4634713351726534
Loss made of: CE 0.12392328679561615, LKD 1.8828891515731812, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/25, Loss=2.394497622549534
Loss made of: CE 0.1776694506406784, LKD 2.5315146446228027, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.17145471274852753, Reg Loss=2.2361488342285156
Clinet index 2, End of Epoch 5/6, Average Loss=2.4076035022735596, Class Loss=0.17145471274852753, Reg Loss=2.2361488342285156
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/25, Loss=2.378585197031498
Loss made of: CE 0.18350230157375336, LKD 2.2965869903564453, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/25, Loss=2.444306381046772
Loss made of: CE 0.18583890795707703, LKD 1.9053151607513428, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1828598976135254, Reg Loss=2.2159478664398193
Clinet index 2, End of Epoch 6/6, Average Loss=2.3988077640533447, Class Loss=0.1828598976135254, Reg Loss=2.2159478664398193
federated aggregation...
Validation, Class Loss=0.1927054077386856, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.929577
Mean Acc: 0.666274
FreqW Acc: 0.873747
Mean IoU: 0.597787
Class IoU:
	class 0: 0.9415087
	class 1: 0.6416405
	class 2: 0.1304559
	class 3: 0.1292648
	class 4: 0.6167194
	class 5: 0.63595885
	class 6: 0.87250674
	class 7: 0.75189966
	class 8: 0.66012484
Class Acc:
	class 0: 0.9815611
	class 1: 0.6458212
	class 2: 0.16168244
	class 3: 0.12990078
	class 4: 0.6554413
	class 5: 0.67209774
	class 6: 0.9299663
	class 7: 0.8820128
	class 8: 0.93798184

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[0, 15, 1, 4]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=5.33128525018692
Loss made of: CE 1.3746846914291382, LKD 3.1524806022644043, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.389578104019165, Reg Loss=3.498960018157959
Clinet index 0, End of Epoch 1/6, Average Loss=4.888538360595703, Class Loss=1.389578104019165, Reg Loss=3.498960018157959
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=4.274835181236267
Loss made of: CE 1.0395182371139526, LKD 3.0220563411712646, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.979917049407959, Reg Loss=3.1156423091888428
Clinet index 0, End of Epoch 2/6, Average Loss=4.095559120178223, Class Loss=0.979917049407959, Reg Loss=3.1156423091888428
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=4.052024871110916
Loss made of: CE 0.7812520861625671, LKD 3.487497091293335, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7321425080299377, Reg Loss=3.06820011138916
Clinet index 0, End of Epoch 3/6, Average Loss=3.800342559814453, Class Loss=0.7321425080299377, Reg Loss=3.06820011138916
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=3.4152158737182616
Loss made of: CE 0.7869329452514648, LKD 2.6779544353485107, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5783894658088684, Reg Loss=2.955465793609619
Clinet index 0, End of Epoch 4/6, Average Loss=3.5338551998138428, Class Loss=0.5783894658088684, Reg Loss=2.955465793609619
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=3.6516194492578506
Loss made of: CE 0.5226266384124756, LKD 3.077507495880127, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4902319312095642, Reg Loss=2.975525140762329
Clinet index 0, End of Epoch 5/6, Average Loss=3.465757131576538, Class Loss=0.4902319312095642, Reg Loss=2.975525140762329
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=3.404098483920097
Loss made of: CE 0.49704766273498535, LKD 2.4444737434387207, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4559869170188904, Reg Loss=2.960228681564331
Clinet index 0, End of Epoch 6/6, Average Loss=3.416215658187866, Class Loss=0.4559869170188904, Reg Loss=2.960228681564331
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/19, Loss=5.889660131931305
Loss made of: CE 1.1823039054870605, LKD 2.858889579772949, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.5156223773956299, Reg Loss=3.6603164672851562
Clinet index 15, End of Epoch 1/6, Average Loss=5.175938606262207, Class Loss=1.5156223773956299, Reg Loss=3.6603164672851562
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=4.352571451663971
Loss made of: CE 0.6714996695518494, LKD 2.755068302154541, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=1.0672115087509155, Reg Loss=3.1344332695007324
Clinet index 15, End of Epoch 2/6, Average Loss=4.2016448974609375, Class Loss=1.0672115087509155, Reg Loss=3.1344332695007324
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=3.929524117708206
Loss made of: CE 0.7562291026115417, LKD 2.6352736949920654, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7527596354484558, Reg Loss=2.983100414276123
Clinet index 15, End of Epoch 3/6, Average Loss=3.7358601093292236, Class Loss=0.7527596354484558, Reg Loss=2.983100414276123
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=3.567683982849121
Loss made of: CE 0.5468193888664246, LKD 2.7609241008758545, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5923382639884949, Reg Loss=2.882949113845825
Clinet index 15, End of Epoch 4/6, Average Loss=3.475287437438965, Class Loss=0.5923382639884949, Reg Loss=2.882949113845825
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=3.555517715215683
Loss made of: CE 0.45867621898651123, LKD 2.9461941719055176, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5044295191764832, Reg Loss=2.9432637691497803
Clinet index 15, End of Epoch 5/6, Average Loss=3.447693347930908, Class Loss=0.5044295191764832, Reg Loss=2.9432637691497803
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=3.2457398533821107
Loss made of: CE 0.5182864665985107, LKD 2.7882988452911377, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4745534658432007, Reg Loss=2.916649103164673
Clinet index 15, End of Epoch 6/6, Average Loss=3.391202449798584, Class Loss=0.4745534658432007, Reg Loss=2.916649103164673
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=6.2639965415000916
Loss made of: CE 1.2428971529006958, LKD 5.3469085693359375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/35, Loss=5.721063083410263
Loss made of: CE 0.7780703902244568, LKD 3.9178714752197266, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/35, Loss=5.043177330493927
Loss made of: CE 0.7607854604721069, LKD 4.70038366317749, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.0630532503128052, Reg Loss=4.498227596282959
Clinet index 1, End of Epoch 1/6, Average Loss=5.561280727386475, Class Loss=1.0630532503128052, Reg Loss=4.498227596282959
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/35, Loss=4.899448838829994
Loss made of: CE 0.4209098219871521, LKD 4.153759956359863, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/35, Loss=4.674577844142914
Loss made of: CE 0.6272167563438416, LKD 3.9291062355041504, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/35, Loss=4.935879719257355
Loss made of: CE 0.34754273295402527, LKD 4.355197906494141, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.46749457716941833, Reg Loss=4.373344898223877
Clinet index 1, End of Epoch 2/6, Average Loss=4.840839385986328, Class Loss=0.46749457716941833, Reg Loss=4.373344898223877
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/35, Loss=4.634502938389778
Loss made of: CE 0.4069117307662964, LKD 4.807232856750488, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/35, Loss=4.645956286787987
Loss made of: CE 0.30076897144317627, LKD 4.734109401702881, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/35, Loss=4.584607595205307
Loss made of: CE 0.32099536061286926, LKD 4.460112571716309, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.36882758140563965, Reg Loss=4.189195156097412
Clinet index 1, End of Epoch 3/6, Average Loss=4.558022499084473, Class Loss=0.36882758140563965, Reg Loss=4.189195156097412
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/35, Loss=4.687708324193954
Loss made of: CE 0.2887805104255676, LKD 4.0725836753845215, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/35, Loss=4.47374379336834
Loss made of: CE 0.22968491911888123, LKD 3.4235246181488037, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/35, Loss=4.453045651316643
Loss made of: CE 0.2919802665710449, LKD 3.833117961883545, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3291930854320526, Reg Loss=4.192242622375488
Clinet index 1, End of Epoch 4/6, Average Loss=4.521435737609863, Class Loss=0.3291930854320526, Reg Loss=4.192242622375488
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/35, Loss=4.425491690635681
Loss made of: CE 0.2866770029067993, LKD 4.343700408935547, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/35, Loss=4.5563461691141125
Loss made of: CE 0.2788549065589905, LKD 4.212930202484131, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.350273554027081
Loss made of: CE 0.22904519736766815, LKD 3.6632509231567383, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.30112019181251526, Reg Loss=4.174832820892334
Clinet index 1, End of Epoch 5/6, Average Loss=4.475953102111816, Class Loss=0.30112019181251526, Reg Loss=4.174832820892334
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/35, Loss=4.313518834114075
Loss made of: CE 0.22438177466392517, LKD 3.798527956008911, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.549296225607395
Loss made of: CE 0.28121131658554077, LKD 4.653304100036621, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/35, Loss=4.6259280145168304
Loss made of: CE 0.32312941551208496, LKD 4.154487133026123, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2847006618976593, Reg Loss=4.17652702331543
Clinet index 1, End of Epoch 6/6, Average Loss=4.461227893829346, Class Loss=0.2847006618976593, Reg Loss=4.17652702331543
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=5.460679233074188
Loss made of: CE 1.4297010898590088, LKD 3.237692356109619, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.453419804573059, Reg Loss=3.5067009925842285
Clinet index 4, End of Epoch 1/6, Average Loss=4.960120677947998, Class Loss=1.453419804573059, Reg Loss=3.5067009925842285
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=4.172876161336899
Loss made of: CE 0.9113515615463257, LKD 3.1363275051116943, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=1.0125532150268555, Reg Loss=3.0502278804779053
Clinet index 4, End of Epoch 2/6, Average Loss=4.06278133392334, Class Loss=1.0125532150268555, Reg Loss=3.0502278804779053
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=3.7898019313812257
Loss made of: CE 0.6134834885597229, LKD 2.975161075592041, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7543498873710632, Reg Loss=2.943932294845581
Clinet index 4, End of Epoch 3/6, Average Loss=3.698282241821289, Class Loss=0.7543498873710632, Reg Loss=2.943932294845581
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=3.474227774143219
Loss made of: CE 0.5161194205284119, LKD 2.9055063724517822, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5648947954177856, Reg Loss=2.9282004833221436
Clinet index 4, End of Epoch 4/6, Average Loss=3.4930953979492188, Class Loss=0.5648947954177856, Reg Loss=2.9282004833221436
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=3.492076388001442
Loss made of: CE 0.5393421053886414, LKD 2.91994047164917, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4938438832759857, Reg Loss=2.8832366466522217
Clinet index 4, End of Epoch 5/6, Average Loss=3.3770804405212402, Class Loss=0.4938438832759857, Reg Loss=2.8832366466522217
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=3.185104650259018
Loss made of: CE 0.4020936191082001, LKD 2.8222196102142334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.45074668526649475, Reg Loss=2.9724552631378174
Clinet index 4, End of Epoch 6/6, Average Loss=3.4232020378112793, Class Loss=0.45074668526649475, Reg Loss=2.9724552631378174
federated aggregation...
Validation, Class Loss=0.5125449299812317, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.834513
Mean Acc: 0.347887
FreqW Acc: 0.749424
Mean IoU: 0.273747
Class IoU:
	class 0: 0.8981575
	class 1: 0.050456222
	class 2: 0.011268217
	class 3: 0.0
	class 4: 0.43132448
	class 5: 0.094738096
	class 6: 0.6058239
	class 7: 0.5620671
	class 8: 0.6212934
	class 9: 0.0
	class 10: 0.22223412
	class 11: 0.06134547
	class 12: 0.0
Class Acc:
	class 0: 0.9844533
	class 1: 0.050465956
	class 2: 0.011391629
	class 3: 0.0
	class 4: 0.44325057
	class 5: 0.094740674
	class 6: 0.6105316
	class 7: 0.567014
	class 8: 0.7262356
	class 9: 0.0
	class 10: 0.89018375
	class 11: 0.14426371
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[1, 13, 11, 12]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/35, Loss=5.483103936910629
Loss made of: CE 0.8404244184494019, LKD 5.144667148590088, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/35, Loss=5.238039654493332
Loss made of: CE 0.5798512697219849, LKD 3.9394264221191406, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/35, Loss=4.662938874959946
Loss made of: CE 0.5373181104660034, LKD 4.4491119384765625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6897434592247009, Reg Loss=4.365919589996338
Clinet index 1, End of Epoch 1/6, Average Loss=5.055663108825684, Class Loss=0.6897434592247009, Reg Loss=4.365919589996338
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/35, Loss=4.630382773280144
Loss made of: CE 0.36028560996055603, LKD 3.9753384590148926, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/35, Loss=4.446480831503868
Loss made of: CE 0.43185052275657654, LKD 3.928507089614868, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/35, Loss=4.8038940757513044
Loss made of: CE 0.31876328587532043, LKD 4.238801956176758, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.36750438809394836, Reg Loss=4.281931400299072
Clinet index 1, End of Epoch 2/6, Average Loss=4.649435997009277, Class Loss=0.36750438809394836, Reg Loss=4.281931400299072
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/35, Loss=4.6544216379523276
Loss made of: CE 0.32567331194877625, LKD 4.9845685958862305, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/35, Loss=4.584820365905761
Loss made of: CE 0.27200719714164734, LKD 4.542346477508545, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/35, Loss=4.499653707444668
Loss made of: CE 0.32264620065689087, LKD 4.4339141845703125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.32017454504966736, Reg Loss=4.199027061462402
Clinet index 1, End of Epoch 3/6, Average Loss=4.519201755523682, Class Loss=0.32017454504966736, Reg Loss=4.199027061462402
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/35, Loss=4.592401358485222
Loss made of: CE 0.26894611120224, LKD 3.861581563949585, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/35, Loss=4.330018851161003
Loss made of: CE 0.202768012881279, LKD 3.3525235652923584, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/35, Loss=4.447834162414074
Loss made of: CE 0.235005721449852, LKD 3.7447385787963867, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2901187539100647, Reg Loss=4.152938365936279
Clinet index 1, End of Epoch 4/6, Average Loss=4.443057060241699, Class Loss=0.2901187539100647, Reg Loss=4.152938365936279
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/35, Loss=4.3662271171808245
Loss made of: CE 0.29354530572891235, LKD 4.024077892303467, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/35, Loss=4.575915966928005
Loss made of: CE 0.29166874289512634, LKD 4.380843162536621, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.303241311013698
Loss made of: CE 0.24216821789741516, LKD 3.7811734676361084, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.27984359860420227, Reg Loss=4.186863899230957
Clinet index 1, End of Epoch 5/6, Average Loss=4.466707706451416, Class Loss=0.27984359860420227, Reg Loss=4.186863899230957
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/35, Loss=4.314131101965904
Loss made of: CE 0.20912247896194458, LKD 3.591829538345337, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.528264354169369
Loss made of: CE 0.28455978631973267, LKD 4.69853401184082, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/35, Loss=4.617206966876983
Loss made of: CE 0.2824988067150116, LKD 4.040472984313965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2770097255706787, Reg Loss=4.212164402008057
Clinet index 1, End of Epoch 6/6, Average Loss=4.489173889160156, Class Loss=0.2770097255706787, Reg Loss=4.212164402008057
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=3.6366510510444643
Loss made of: CE 0.5441282391548157, LKD 2.5761990547180176, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.528382420539856, Reg Loss=2.9273078441619873
Clinet index 13, End of Epoch 1/6, Average Loss=3.455690383911133, Class Loss=0.528382420539856, Reg Loss=2.9273078441619873
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/19, Loss=3.5458794713020323
Loss made of: CE 0.6400977373123169, LKD 2.883060932159424, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4558182656764984, Reg Loss=2.9009435176849365
Clinet index 13, End of Epoch 2/6, Average Loss=3.3567616939544678, Class Loss=0.4558182656764984, Reg Loss=2.9009435176849365
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/19, Loss=3.391550916433334
Loss made of: CE 0.3501504063606262, LKD 2.6409244537353516, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4244007468223572, Reg Loss=2.9569201469421387
Clinet index 13, End of Epoch 3/6, Average Loss=3.3813209533691406, Class Loss=0.4244007468223572, Reg Loss=2.9569201469421387
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/19, Loss=3.062910810112953
Loss made of: CE 0.4019506871700287, LKD 2.6646111011505127, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.39508989453315735, Reg Loss=2.8690590858459473
Clinet index 13, End of Epoch 4/6, Average Loss=3.2641489505767822, Class Loss=0.39508989453315735, Reg Loss=2.8690590858459473
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/19, Loss=3.2463451474905014
Loss made of: CE 0.3498111963272095, LKD 3.120522975921631, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.38393446803092957, Reg Loss=2.8899662494659424
Clinet index 13, End of Epoch 5/6, Average Loss=3.2739007472991943, Class Loss=0.38393446803092957, Reg Loss=2.8899662494659424
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/19, Loss=3.126322627067566
Loss made of: CE 0.4358921945095062, LKD 2.498356342315674, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3623332381248474, Reg Loss=2.8517026901245117
Clinet index 13, End of Epoch 6/6, Average Loss=3.214035987854004, Class Loss=0.3623332381248474, Reg Loss=2.8517026901245117
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=3.700868284702301
Loss made of: CE 0.6017591953277588, LKD 2.408395767211914, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/33, Loss=3.568951505422592
Loss made of: CE 0.6356155276298523, LKD 3.6877222061157227, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/33, Loss=3.738734745979309
Loss made of: CE 0.4564470052719116, LKD 3.060764789581299, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6297728419303894, Reg Loss=2.998748302459717
Clinet index 11, End of Epoch 1/6, Average Loss=3.628521203994751, Class Loss=0.6297728419303894, Reg Loss=2.998748302459717
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=3.384210190176964
Loss made of: CE 0.6276626586914062, LKD 2.3236348628997803, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/33, Loss=3.490322542190552
Loss made of: CE 0.5340322256088257, LKD 3.1449224948883057, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/33, Loss=3.5539009839296343
Loss made of: CE 0.6023992896080017, LKD 2.801748752593994, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5124314427375793, Reg Loss=2.9482152462005615
Clinet index 11, End of Epoch 2/6, Average Loss=3.460646629333496, Class Loss=0.5124314427375793, Reg Loss=2.9482152462005615
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=3.447619381546974
Loss made of: CE 0.3985348641872406, LKD 3.030111312866211, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/33, Loss=3.3344624996185304
Loss made of: CE 0.5287665128707886, LKD 1.9107047319412231, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/33, Loss=3.3904559701681136
Loss made of: CE 0.5035882592201233, LKD 2.7539026737213135, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.467386931180954, Reg Loss=2.905898094177246
Clinet index 11, End of Epoch 3/6, Average Loss=3.3732850551605225, Class Loss=0.467386931180954, Reg Loss=2.905898094177246
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=3.4797949463129045
Loss made of: CE 0.4478399157524109, LKD 2.8614308834075928, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/33, Loss=3.4007398068904875
Loss made of: CE 0.458310067653656, LKD 2.9403750896453857, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/33, Loss=3.392018100619316
Loss made of: CE 0.5162891149520874, LKD 3.403013229370117, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4450206160545349, Reg Loss=2.9660232067108154
Clinet index 11, End of Epoch 4/6, Average Loss=3.411043882369995, Class Loss=0.4450206160545349, Reg Loss=2.9660232067108154
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=3.4268326640129088
Loss made of: CE 0.33138060569763184, LKD 2.7723443508148193, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/33, Loss=3.2344677567481996
Loss made of: CE 0.4126860797405243, LKD 2.512692928314209, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/33, Loss=3.340226423740387
Loss made of: CE 0.48419898748397827, LKD 2.721010684967041, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4331364333629608, Reg Loss=2.9076108932495117
Clinet index 11, End of Epoch 5/6, Average Loss=3.340747356414795, Class Loss=0.4331364333629608, Reg Loss=2.9076108932495117
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=3.314213454723358
Loss made of: CE 0.4355633854866028, LKD 2.791858196258545, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/33, Loss=3.1713156908750535
Loss made of: CE 0.31819701194763184, LKD 2.931142568588257, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/33, Loss=3.278379076719284
Loss made of: CE 0.351578950881958, LKD 2.4499199390411377, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4065655469894409, Reg Loss=2.851433753967285
Clinet index 11, End of Epoch 6/6, Average Loss=3.2579994201660156, Class Loss=0.4065655469894409, Reg Loss=2.851433753967285
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=3.5732777267694473
Loss made of: CE 0.7745174169540405, LKD 2.8242251873016357, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/33, Loss=3.7171921491622926
Loss made of: CE 0.7576117515563965, LKD 3.659111738204956, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/33, Loss=3.497291859984398
Loss made of: CE 0.4521690607070923, LKD 2.553497314453125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6119721531867981, Reg Loss=2.99184250831604
Clinet index 12, End of Epoch 1/6, Average Loss=3.6038146018981934, Class Loss=0.6119721531867981, Reg Loss=2.99184250831604
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=3.544361415505409
Loss made of: CE 0.36663636565208435, LKD 2.801243543624878, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/33, Loss=3.3968645721673965
Loss made of: CE 0.6614731550216675, LKD 3.621429920196533, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/33, Loss=3.4441534906625746
Loss made of: CE 0.5455489158630371, LKD 3.291055202484131, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5104905366897583, Reg Loss=2.9645707607269287
Clinet index 12, End of Epoch 2/6, Average Loss=3.4750614166259766, Class Loss=0.5104905366897583, Reg Loss=2.9645707607269287
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=3.306640088558197
Loss made of: CE 0.4239040017127991, LKD 2.552664279937744, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/33, Loss=3.508173680305481
Loss made of: CE 0.43638405203819275, LKD 3.2878897190093994, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/33, Loss=3.349627336859703
Loss made of: CE 0.5095958113670349, LKD 3.0902457237243652, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4612331688404083, Reg Loss=2.973512649536133
Clinet index 12, End of Epoch 3/6, Average Loss=3.4347457885742188, Class Loss=0.4612331688404083, Reg Loss=2.973512649536133
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=3.605343762040138
Loss made of: CE 0.37695392966270447, LKD 2.4405932426452637, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/33, Loss=3.3717227965593337
Loss made of: CE 0.4738026559352875, LKD 3.4194223880767822, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/33, Loss=3.1804410845041273
Loss made of: CE 0.38209113478660583, LKD 2.3833911418914795, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.43788981437683105, Reg Loss=2.9498064517974854
Clinet index 12, End of Epoch 4/6, Average Loss=3.3876962661743164, Class Loss=0.43788981437683105, Reg Loss=2.9498064517974854
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=3.418304893374443
Loss made of: CE 0.31296437978744507, LKD 3.2847299575805664, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/33, Loss=3.3053520143032076
Loss made of: CE 0.5327616930007935, LKD 3.580836296081543, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/33, Loss=3.568936339020729
Loss made of: CE 0.5118094682693481, LKD 3.494718313217163, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.41930916905403137, Reg Loss=2.992225170135498
Clinet index 12, End of Epoch 5/6, Average Loss=3.411534309387207, Class Loss=0.41930916905403137, Reg Loss=2.992225170135498
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=3.5057087421417235
Loss made of: CE 0.5369148254394531, LKD 3.5589847564697266, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/33, Loss=3.4773451328277587
Loss made of: CE 0.38580581545829773, LKD 3.82753324508667, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/33, Loss=3.3076305955648424
Loss made of: CE 0.46444401144981384, LKD 3.601367712020874, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4148005545139313, Reg Loss=2.983346700668335
Clinet index 12, End of Epoch 6/6, Average Loss=3.3981473445892334, Class Loss=0.4148005545139313, Reg Loss=2.983346700668335
federated aggregation...
Validation, Class Loss=0.4292382001876831, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.861882
Mean Acc: 0.442290
FreqW Acc: 0.782601
Mean IoU: 0.361243
Class IoU:
	class 0: 0.9061969
	class 1: 0.07771088
	class 2: 0.012515189
	class 3: 0.0
	class 4: 0.4497911
	class 5: 0.2967684
	class 6: 0.743277
	class 7: 0.74431294
	class 8: 0.6949665
	class 9: 0.09054844
	class 10: 0.28116593
	class 11: 0.20780233
	class 12: 0.1911012
Class Acc:
	class 0: 0.9822575
	class 1: 0.077749856
	class 2: 0.012682451
	class 3: 0.0
	class 4: 0.4606244
	class 5: 0.29776677
	class 6: 0.75478995
	class 7: 0.76139027
	class 8: 0.80873793
	class 9: 0.15103957
	class 10: 0.9201631
	class 11: 0.32126766
	class 12: 0.20130342

federated global round: 12, step: 2
select part of clients to conduct local training
[0, 16, 7, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=3.3322684496641157
Loss made of: CE 0.5085418820381165, LKD 2.4858803749084473, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4453972280025482, Reg Loss=2.9214420318603516
Clinet index 0, End of Epoch 1/6, Average Loss=3.3668391704559326, Class Loss=0.4453972280025482, Reg Loss=2.9214420318603516
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=3.3691157937049865
Loss made of: CE 0.39535802602767944, LKD 2.7035229206085205, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.38641348481178284, Reg Loss=2.9174952507019043
Clinet index 0, End of Epoch 2/6, Average Loss=3.3039088249206543, Class Loss=0.38641348481178284, Reg Loss=2.9174952507019043
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=3.641524848341942
Loss made of: CE 0.390131413936615, LKD 3.5771825313568115, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.37817466259002686, Reg Loss=2.9674313068389893
Clinet index 0, End of Epoch 3/6, Average Loss=3.3456058502197266, Class Loss=0.37817466259002686, Reg Loss=2.9674313068389893
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=3.1661011308431624
Loss made of: CE 0.43660664558410645, LKD 2.831766128540039, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3501299321651459, Reg Loss=2.9724864959716797
Clinet index 0, End of Epoch 4/6, Average Loss=3.3226163387298584, Class Loss=0.3501299321651459, Reg Loss=2.9724864959716797
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=3.417615643143654
Loss made of: CE 0.361278772354126, LKD 3.1228299140930176, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.342837929725647, Reg Loss=2.899898052215576
Clinet index 0, End of Epoch 5/6, Average Loss=3.2427358627319336, Class Loss=0.342837929725647, Reg Loss=2.899898052215576
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=3.213888853788376
Loss made of: CE 0.41491779685020447, LKD 2.53599214553833, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.34034061431884766, Reg Loss=2.9122679233551025
Clinet index 0, End of Epoch 6/6, Average Loss=3.25260853767395, Class Loss=0.34034061431884766, Reg Loss=2.9122679233551025
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/42, Loss=4.30837659239769
Loss made of: CE 0.6024057865142822, LKD 4.234967231750488, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/42, Loss=4.336414635181427
Loss made of: CE 0.43309807777404785, LKD 3.3426105976104736, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/42, Loss=4.3079036206007
Loss made of: CE 0.5041864514350891, LKD 4.248250484466553, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/42, Loss=3.9304360687732696
Loss made of: CE 0.25460585951805115, LKD 3.7553436756134033, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5007458925247192, Reg Loss=3.6893038749694824
Clinet index 16, End of Epoch 1/6, Average Loss=4.190049648284912, Class Loss=0.5007458925247192, Reg Loss=3.6893038749694824
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/42, Loss=4.320622211694717
Loss made of: CE 0.3312602639198303, LKD 4.107924938201904, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.159133493900299
Loss made of: CE 0.27588045597076416, LKD 4.149532794952393, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/42, Loss=3.839422658085823
Loss made of: CE 0.26425713300704956, LKD 3.5103015899658203, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/42, Loss=3.82275553047657
Loss made of: CE 0.31557902693748474, LKD 2.9355924129486084, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3441039025783539, Reg Loss=3.696591854095459
Clinet index 16, End of Epoch 2/6, Average Loss=4.040695667266846, Class Loss=0.3441039025783539, Reg Loss=3.696591854095459
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/42, Loss=3.8729453146457673
Loss made of: CE 0.3440524935722351, LKD 3.610276460647583, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/42, Loss=3.958298808336258
Loss made of: CE 0.2724671959877014, LKD 4.835471153259277, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/42, Loss=3.9583283454179763
Loss made of: CE 0.28969547152519226, LKD 4.104860782623291, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.2802125960588455
Loss made of: CE 0.33396202325820923, LKD 4.461157321929932, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.33185309171676636, Reg Loss=3.672250270843506
Clinet index 16, End of Epoch 3/6, Average Loss=4.004103183746338, Class Loss=0.33185309171676636, Reg Loss=3.672250270843506
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/42, Loss=4.026408106088638
Loss made of: CE 0.31919071078300476, LKD 3.3201992511749268, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.175272282958031
Loss made of: CE 0.4084164500236511, LKD 4.033473014831543, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/42, Loss=3.944183388352394
Loss made of: CE 0.2429162710905075, LKD 3.43096923828125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/42, Loss=3.7776268303394316
Loss made of: CE 0.28253787755966187, LKD 3.1157777309417725, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3208024799823761, Reg Loss=3.6634397506713867
Clinet index 16, End of Epoch 4/6, Average Loss=3.9842422008514404, Class Loss=0.3208024799823761, Reg Loss=3.6634397506713867
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/42, Loss=3.9908378273248672
Loss made of: CE 0.3538733720779419, LKD 3.7988955974578857, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/42, Loss=3.9367695212364198
Loss made of: CE 0.395862340927124, LKD 3.0173263549804688, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/42, Loss=3.804994910955429
Loss made of: CE 0.32002922892570496, LKD 3.0989155769348145, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/42, Loss=3.9495418325066565
Loss made of: CE 0.285539835691452, LKD 4.409012317657471, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.304485023021698, Reg Loss=3.606412410736084
Clinet index 16, End of Epoch 5/6, Average Loss=3.9108974933624268, Class Loss=0.304485023021698, Reg Loss=3.606412410736084
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/42, Loss=4.0370156660676
Loss made of: CE 0.2735683023929596, LKD 3.6474742889404297, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/42, Loss=3.8928714394569397
Loss made of: CE 0.32357949018478394, LKD 3.3967740535736084, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.0466768652200695
Loss made of: CE 0.3327891230583191, LKD 3.419637680053711, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.085484082996845
Loss made of: CE 0.3818697929382324, LKD 3.4895272254943848, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3073062002658844, Reg Loss=3.675111770629883
Clinet index 16, End of Epoch 6/6, Average Loss=3.9824180603027344, Class Loss=0.3073062002658844, Reg Loss=3.675111770629883
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/31, Loss=3.3417312085628508
Loss made of: CE 0.6215146780014038, LKD 2.20959734916687, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/31, Loss=3.260598087310791
Loss made of: CE 0.5648627281188965, LKD 3.283900499343872, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/31, Loss=3.254418522119522
Loss made of: CE 0.4718869626522064, LKD 2.4211525917053223, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5556409358978271, Reg Loss=2.759345293045044
Clinet index 7, End of Epoch 1/6, Average Loss=3.314986228942871, Class Loss=0.5556409358978271, Reg Loss=2.759345293045044
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/31, Loss=3.2460001677274706
Loss made of: CE 0.42969009280204773, LKD 2.211965322494507, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/31, Loss=2.99617899954319
Loss made of: CE 0.465465247631073, LKD 2.7498409748077393, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/31, Loss=3.1617426574230194
Loss made of: CE 0.42927560210227966, LKD 3.1007513999938965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4762875437736511, Reg Loss=2.6848955154418945
Clinet index 7, End of Epoch 2/6, Average Loss=3.1611831188201904, Class Loss=0.4762875437736511, Reg Loss=2.6848955154418945
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/31, Loss=3.1669156819581987
Loss made of: CE 0.5349966287612915, LKD 3.5348422527313232, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/31, Loss=3.162598821520805
Loss made of: CE 0.4243653416633606, LKD 3.0380165576934814, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/31, Loss=3.0938954174518587
Loss made of: CE 0.4203331470489502, LKD 2.897463798522949, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.43819117546081543, Reg Loss=2.6933789253234863
Clinet index 7, End of Epoch 3/6, Average Loss=3.1315701007843018, Class Loss=0.43819117546081543, Reg Loss=2.6933789253234863
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/31, Loss=3.4312473803758623
Loss made of: CE 0.4890007972717285, LKD 2.7399418354034424, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/31, Loss=3.1061252266168595
Loss made of: CE 0.43922746181488037, LKD 3.0306265354156494, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/31, Loss=2.9402755826711653
Loss made of: CE 0.6260456442832947, LKD 2.38948130607605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.43447065353393555, Reg Loss=2.7155158519744873
Clinet index 7, End of Epoch 4/6, Average Loss=3.149986505508423, Class Loss=0.43447065353393555, Reg Loss=2.7155158519744873
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/31, Loss=3.1504044950008394
Loss made of: CE 0.3593752086162567, LKD 2.6094441413879395, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/31, Loss=2.866057258844376
Loss made of: CE 0.3978669047355652, LKD 2.778902292251587, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/31, Loss=3.145887753367424
Loss made of: CE 0.3572417199611664, LKD 3.09745717048645, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4035894274711609, Reg Loss=2.655487298965454
Clinet index 7, End of Epoch 5/6, Average Loss=3.0590767860412598, Class Loss=0.4035894274711609, Reg Loss=2.655487298965454
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/31, Loss=2.961592528223991
Loss made of: CE 0.38467106223106384, LKD 2.889491319656372, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/31, Loss=3.16246779859066
Loss made of: CE 0.43835288286209106, LKD 2.442617416381836, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/31, Loss=3.123232698440552
Loss made of: CE 0.4270549714565277, LKD 2.8730883598327637, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.39688530564308167, Reg Loss=2.692115306854248
Clinet index 7, End of Epoch 6/6, Average Loss=3.089000701904297, Class Loss=0.39688530564308167, Reg Loss=2.692115306854248
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=3.338213583827019
Loss made of: CE 0.3790484368801117, LKD 2.9033591747283936, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.46737298369407654, Reg Loss=2.840985059738159
Clinet index 4, End of Epoch 1/6, Average Loss=3.3083579540252686, Class Loss=0.46737298369407654, Reg Loss=2.840985059738159
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=3.3454097718000413
Loss made of: CE 0.4080054759979248, LKD 3.2310500144958496, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4041714072227478, Reg Loss=2.886803388595581
Clinet index 4, End of Epoch 2/6, Average Loss=3.2909748554229736, Class Loss=0.4041714072227478, Reg Loss=2.886803388595581
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=3.247752606868744
Loss made of: CE 0.3540102243423462, LKD 2.7367539405822754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.36837032437324524, Reg Loss=2.85725998878479
Clinet index 4, End of Epoch 3/6, Average Loss=3.225630283355713, Class Loss=0.36837032437324524, Reg Loss=2.85725998878479
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=3.15507595539093
Loss made of: CE 0.3642255663871765, LKD 3.07952618598938, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.36008304357528687, Reg Loss=2.821906089782715
Clinet index 4, End of Epoch 4/6, Average Loss=3.1819891929626465, Class Loss=0.36008304357528687, Reg Loss=2.821906089782715
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=3.3636737704277038
Loss made of: CE 0.3759157657623291, LKD 2.9725565910339355, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3481374680995941, Reg Loss=2.8703534603118896
Clinet index 4, End of Epoch 5/6, Average Loss=3.2184908390045166, Class Loss=0.3481374680995941, Reg Loss=2.8703534603118896
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=3.003985846042633
Loss made of: CE 0.3116189241409302, LKD 2.5626585483551025, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3368959128856659, Reg Loss=2.8439571857452393
Clinet index 4, End of Epoch 6/6, Average Loss=3.1808531284332275, Class Loss=0.3368959128856659, Reg Loss=2.8439571857452393
federated aggregation...
Validation, Class Loss=0.40519270300865173, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.880486
Mean Acc: 0.509183
FreqW Acc: 0.805792
Mean IoU: 0.415074
Class IoU:
	class 0: 0.91148746
	class 1: 0.13097747
	class 2: 0.003047059
	class 3: 0.0
	class 4: 0.42306554
	class 5: 0.2816976
	class 6: 0.73477966
	class 7: 0.75752413
	class 8: 0.6995699
	class 9: 0.15513574
	class 10: 0.49175015
	class 11: 0.29085472
	class 12: 0.5160775
Class Acc:
	class 0: 0.97529995
	class 1: 0.13100556
	class 2: 0.0030518156
	class 3: 0.0
	class 4: 0.43270767
	class 5: 0.2824126
	class 6: 0.7456288
	class 7: 0.7797099
	class 8: 0.73551035
	class 9: 0.2693042
	class 10: 0.8632607
	class 11: 0.65356934
	class 12: 0.74791384

federated global round: 13, step: 2
select part of clients to conduct local training
[14, 13, 0, 1]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=3.2450829595327377
Loss made of: CE 0.41419076919555664, LKD 3.2525956630706787, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.385271817445755, Reg Loss=2.9095840454101562
Clinet index 14, End of Epoch 1/6, Average Loss=3.294855833053589, Class Loss=0.385271817445755, Reg Loss=2.9095840454101562
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=3.219788631796837
Loss made of: CE 0.45766744017601013, LKD 3.6379613876342773, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3389325737953186, Reg Loss=2.90120005607605
Clinet index 14, End of Epoch 2/6, Average Loss=3.2401325702667236, Class Loss=0.3389325737953186, Reg Loss=2.90120005607605
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=3.1316347986459734
Loss made of: CE 0.3130801320075989, LKD 2.2695136070251465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3287702798843384, Reg Loss=2.8477275371551514
Clinet index 14, End of Epoch 3/6, Average Loss=3.1764979362487793, Class Loss=0.3287702798843384, Reg Loss=2.8477275371551514
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=3.1549889743328094
Loss made of: CE 0.283973753452301, LKD 2.972853183746338, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.31745681166648865, Reg Loss=2.9376237392425537
Clinet index 14, End of Epoch 4/6, Average Loss=3.255080461502075, Class Loss=0.31745681166648865, Reg Loss=2.9376237392425537
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=3.3922737717628477
Loss made of: CE 0.326396107673645, LKD 3.0010926723480225, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.31714391708374023, Reg Loss=2.8786299228668213
Clinet index 14, End of Epoch 5/6, Average Loss=3.1957738399505615, Class Loss=0.31714391708374023, Reg Loss=2.8786299228668213
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=3.4626547783613204
Loss made of: CE 0.2598170042037964, LKD 2.6874568462371826, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.31768032908439636, Reg Loss=2.9498846530914307
Clinet index 14, End of Epoch 6/6, Average Loss=3.2675650119781494, Class Loss=0.31768032908439636, Reg Loss=2.9498846530914307
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/19, Loss=3.434834763407707
Loss made of: CE 0.4049338102340698, LKD 2.588097333908081, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3809961974620819, Reg Loss=2.844280958175659
Clinet index 13, End of Epoch 1/6, Average Loss=3.2252771854400635, Class Loss=0.3809961974620819, Reg Loss=2.844280958175659
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/19, Loss=3.4479345798492433
Loss made of: CE 0.4652526080608368, LKD 2.812373399734497, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.34536537528038025, Reg Loss=2.8728229999542236
Clinet index 13, End of Epoch 2/6, Average Loss=3.2181882858276367, Class Loss=0.34536537528038025, Reg Loss=2.8728229999542236
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/19, Loss=3.3226528257131576
Loss made of: CE 0.2820158898830414, LKD 2.8691556453704834, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.33527514338493347, Reg Loss=2.9818367958068848
Clinet index 13, End of Epoch 3/6, Average Loss=3.3171119689941406, Class Loss=0.33527514338493347, Reg Loss=2.9818367958068848
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/19, Loss=2.9125584572553636
Loss made of: CE 0.3358995318412781, LKD 2.31196665763855, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3192039430141449, Reg Loss=2.8474366664886475
Clinet index 13, End of Epoch 4/6, Average Loss=3.166640520095825, Class Loss=0.3192039430141449, Reg Loss=2.8474366664886475
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/19, Loss=3.1821196630597113
Loss made of: CE 0.3144786059856415, LKD 3.0199553966522217, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3222549855709076, Reg Loss=2.8839218616485596
Clinet index 13, End of Epoch 5/6, Average Loss=3.2061767578125, Class Loss=0.3222549855709076, Reg Loss=2.8839218616485596
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/19, Loss=3.0230359077453612
Loss made of: CE 0.35822781920433044, LKD 2.6989731788635254, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3108091354370117, Reg Loss=2.821129560470581
Clinet index 13, End of Epoch 6/6, Average Loss=3.1319386959075928, Class Loss=0.3108091354370117, Reg Loss=2.821129560470581
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=3.293796756863594
Loss made of: CE 0.37715157866477966, LKD 2.8477964401245117, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.37395143508911133, Reg Loss=2.9429173469543457
Clinet index 0, End of Epoch 1/6, Average Loss=3.316868782043457, Class Loss=0.37395143508911133, Reg Loss=2.9429173469543457
Pseudo labeling is: None
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/19, Loss=3.341845926642418
Loss made of: CE 0.35591715574264526, LKD 2.7766273021698, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3550955653190613, Reg Loss=2.927522659301758
Clinet index 0, End of Epoch 2/6, Average Loss=3.282618284225464, Class Loss=0.3550955653190613, Reg Loss=2.927522659301758
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/19, Loss=3.571674811840057
Loss made of: CE 0.3572423458099365, LKD 3.329521417617798, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.34162336587905884, Reg Loss=2.958808660507202
Clinet index 0, End of Epoch 3/6, Average Loss=3.300431966781616, Class Loss=0.34162336587905884, Reg Loss=2.958808660507202
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/19, Loss=3.090504044294357
Loss made of: CE 0.4261329770088196, LKD 2.5405924320220947, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.32919374108314514, Reg Loss=2.9275407791137695
Clinet index 0, End of Epoch 4/6, Average Loss=3.256734609603882, Class Loss=0.32919374108314514, Reg Loss=2.9275407791137695
Pseudo labeling is: None
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/19, Loss=3.3935042440891268
Loss made of: CE 0.3408651053905487, LKD 3.0252230167388916, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3261622488498688, Reg Loss=2.9170000553131104
Clinet index 0, End of Epoch 5/6, Average Loss=3.2431623935699463, Class Loss=0.3261622488498688, Reg Loss=2.9170000553131104
Pseudo labeling is: None
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/19, Loss=3.1408283740282057
Loss made of: CE 0.38437819480895996, LKD 2.6496286392211914, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3242509663105011, Reg Loss=2.871492385864258
Clinet index 0, End of Epoch 6/6, Average Loss=3.1957433223724365, Class Loss=0.3242509663105011, Reg Loss=2.871492385864258
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=5.274587869644165
Loss made of: CE 0.6964755058288574, LKD 5.271529197692871, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/35, Loss=5.087117633223533
Loss made of: CE 0.49198946356773376, LKD 4.238786697387695, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/35, Loss=4.604093876481056
Loss made of: CE 0.47926852107048035, LKD 4.963598728179932, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5403104424476624, Reg Loss=4.366641998291016
Clinet index 1, End of Epoch 1/6, Average Loss=4.906952381134033, Class Loss=0.5403104424476624, Reg Loss=4.366641998291016
Pseudo labeling is: None
Epoch 2, lr = 0.000584
Epoch 2, Batch 10/35, Loss=4.645071947574616
Loss made of: CE 0.3625054955482483, LKD 4.04563045501709, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/35, Loss=4.464430430531502
Loss made of: CE 0.38564980030059814, LKD 3.9758167266845703, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/35, Loss=4.722629961371422
Loss made of: CE 0.3096086084842682, LKD 4.456252098083496, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3519822955131531, Reg Loss=4.307297229766846
Clinet index 1, End of Epoch 2/6, Average Loss=4.6592793464660645, Class Loss=0.3519822955131531, Reg Loss=4.307297229766846
Pseudo labeling is: None
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/35, Loss=4.573737776279449
Loss made of: CE 0.32621657848358154, LKD 5.0232462882995605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/35, Loss=4.553947715461254
Loss made of: CE 0.33788901567459106, LKD 4.935535907745361, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/35, Loss=4.445485635101795
Loss made of: CE 0.28695374727249146, LKD 4.759285926818848, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3043862283229828, Reg Loss=4.170599460601807
Clinet index 1, End of Epoch 3/6, Average Loss=4.474985599517822, Class Loss=0.3043862283229828, Reg Loss=4.170599460601807
Pseudo labeling is: None
Epoch 4, lr = 0.000487
Epoch 4, Batch 10/35, Loss=4.4650683760643
Loss made of: CE 0.23574146628379822, LKD 3.6763110160827637, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/35, Loss=4.429332771897316
Loss made of: CE 0.19826284050941467, LKD 3.276578664779663, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/35, Loss=4.360683271288872
Loss made of: CE 0.2241659164428711, LKD 3.6107003688812256, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.27343514561653137, Reg Loss=4.144351959228516
Clinet index 1, End of Epoch 4/6, Average Loss=4.417787075042725, Class Loss=0.27343514561653137, Reg Loss=4.144351959228516
Pseudo labeling is: None
Epoch 5, lr = 0.000438
Epoch 5, Batch 10/35, Loss=4.380657272040844
Loss made of: CE 0.2972140908241272, LKD 4.217895030975342, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/35, Loss=4.5216614052653314
Loss made of: CE 0.2864876389503479, LKD 4.202792167663574, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.37084309309721
Loss made of: CE 0.2381095141172409, LKD 3.9192075729370117, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.27131932973861694, Reg Loss=4.203322887420654
Clinet index 1, End of Epoch 5/6, Average Loss=4.474642276763916, Class Loss=0.27131932973861694, Reg Loss=4.203322887420654
Pseudo labeling is: None
Epoch 6, lr = 0.000389
Epoch 6, Batch 10/35, Loss=4.3046727031469345
Loss made of: CE 0.19877521693706512, LKD 3.787611722946167, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.556116685271263
Loss made of: CE 0.3072579503059387, LKD 4.743155002593994, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/35, Loss=4.602119754254818
Loss made of: CE 0.2272201031446457, LKD 3.921700954437256, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2681531012058258, Reg Loss=4.206103801727295
Clinet index 1, End of Epoch 6/6, Average Loss=4.474256992340088, Class Loss=0.2681531012058258, Reg Loss=4.206103801727295
federated aggregation...
Validation, Class Loss=0.39873963594436646, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.878745
Mean Acc: 0.507078
FreqW Acc: 0.805120
Mean IoU: 0.406775
Class IoU:
	class 0: 0.915009
	class 1: 0.058543988
	class 2: 0.014441613
	class 3: 0.0
	class 4: 0.4154992
	class 5: 0.3241449
	class 6: 0.7546952
	class 7: 0.76372784
	class 8: 0.62948805
	class 9: 0.1823766
	class 10: 0.38948798
	class 11: 0.30628523
	class 12: 0.53437454
Class Acc:
	class 0: 0.97550625
	class 1: 0.058548626
	class 2: 0.014698599
	class 3: 0.0
	class 4: 0.4243815
	class 5: 0.32599977
	class 6: 0.766538
	class 7: 0.79110014
	class 8: 0.64479864
	class 9: 0.26738992
	class 10: 0.9229864
	class 11: 0.6477465
	class 12: 0.7523202

federated global round: 14, step: 2
select part of clients to conduct local training
[16, 9, 4, 0]
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/42, Loss=4.098049533367157
Loss made of: CE 0.5290472507476807, LKD 4.268033504486084, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/42, Loss=4.189415562152862
Loss made of: CE 0.4245099425315857, LKD 3.2041609287261963, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/42, Loss=4.1298861593008045
Loss made of: CE 0.4266146123409271, LKD 3.8206193447113037, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/42, Loss=3.914142608642578
Loss made of: CE 0.2434462606906891, LKD 3.617056369781494, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4257633686065674, Reg Loss=3.630176544189453
Clinet index 16, End of Epoch 1/6, Average Loss=4.055939674377441, Class Loss=0.4257633686065674, Reg Loss=3.630176544189453
Pseudo labeling is: None
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/42, Loss=4.127432557940483
Loss made of: CE 0.3212614357471466, LKD 3.978640556335449, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.091695880889892
Loss made of: CE 0.27874523401260376, LKD 4.153334617614746, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/42, Loss=3.769168700277805
Loss made of: CE 0.2757863402366638, LKD 3.538586139678955, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/42, Loss=3.8541027545928954
Loss made of: CE 0.29779112339019775, LKD 3.0218710899353027, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.31704142689704895, Reg Loss=3.6488993167877197
Clinet index 16, End of Epoch 2/6, Average Loss=3.9659407138824463, Class Loss=0.31704142689704895, Reg Loss=3.6488993167877197
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/42, Loss=3.7755469918251037
Loss made of: CE 0.3483748435974121, LKD 3.410977840423584, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/42, Loss=3.959658646583557
Loss made of: CE 0.27776914834976196, LKD 4.727084636688232, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/42, Loss=3.8713764250278473
Loss made of: CE 0.27739402651786804, LKD 4.054307460784912, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.276580888032913
Loss made of: CE 0.32398223876953125, LKD 4.412744998931885, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.30902305245399475, Reg Loss=3.6493945121765137
Clinet index 16, End of Epoch 3/6, Average Loss=3.9584176540374756, Class Loss=0.30902305245399475, Reg Loss=3.6493945121765137
Pseudo labeling is: None
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/42, Loss=4.103311303257942
Loss made of: CE 0.30554312467575073, LKD 3.4112281799316406, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/42, Loss=3.9603942155838014
Loss made of: CE 0.3463676869869232, LKD 3.9301984310150146, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/42, Loss=3.8510030090808867
Loss made of: CE 0.21169495582580566, LKD 3.2590131759643555, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/42, Loss=3.713810387253761
Loss made of: CE 0.2585103213787079, LKD 3.157597303390503, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.30413365364074707, Reg Loss=3.628277540206909
Clinet index 16, End of Epoch 4/6, Average Loss=3.9324111938476562, Class Loss=0.30413365364074707, Reg Loss=3.628277540206909
Pseudo labeling is: None
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/42, Loss=3.9271170288324355
Loss made of: CE 0.3124209940433502, LKD 3.57059383392334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/42, Loss=3.955069887638092
Loss made of: CE 0.3552432954311371, LKD 2.9072446823120117, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/42, Loss=3.830899381637573
Loss made of: CE 0.3159290552139282, LKD 2.9935460090637207, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/42, Loss=3.9619280844926834
Loss made of: CE 0.3131408393383026, LKD 4.284265041351318, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2997097074985504, Reg Loss=3.6048152446746826
Clinet index 16, End of Epoch 5/6, Average Loss=3.9045250415802, Class Loss=0.2997097074985504, Reg Loss=3.6048152446746826
Pseudo labeling is: None
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/42, Loss=3.9538104474544524
Loss made of: CE 0.2773371934890747, LKD 3.4421772956848145, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/42, Loss=3.850879180431366
Loss made of: CE 0.2951415181159973, LKD 3.2025463581085205, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.0023855224251745
Loss made of: CE 0.34404295682907104, LKD 3.2962794303894043, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.011789281666279
Loss made of: CE 0.3539746403694153, LKD 3.4302825927734375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.30133166909217834, Reg Loss=3.6222193241119385
Clinet index 16, End of Epoch 6/6, Average Loss=3.923551082611084, Class Loss=0.30133166909217834, Reg Loss=3.6222193241119385
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=5.250621256232262
Loss made of: CE 0.593400239944458, LKD 4.926216125488281, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/35, Loss=4.9260531723499295
Loss made of: CE 0.3303072452545166, LKD 4.224997043609619, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/35, Loss=4.472465699911117
Loss made of: CE 0.3388255834579468, LKD 4.650448322296143, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4549463093280792, Reg Loss=4.364264965057373
Clinet index 9, End of Epoch 1/6, Average Loss=4.819211483001709, Class Loss=0.4549463093280792, Reg Loss=4.364264965057373
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/35, Loss=4.350887370109558
Loss made of: CE 0.3414265513420105, LKD 3.713402509689331, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/35, Loss=4.741470739245415
Loss made of: CE 0.3814191222190857, LKD 4.360353469848633, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/35, Loss=4.485191717743874
Loss made of: CE 0.28329357504844666, LKD 3.9234001636505127, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.30956023931503296, Reg Loss=4.219017028808594
Clinet index 9, End of Epoch 2/6, Average Loss=4.5285773277282715, Class Loss=0.30956023931503296, Reg Loss=4.219017028808594
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/35, Loss=4.455781470239162
Loss made of: CE 0.24907395243644714, LKD 4.456648349761963, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/35, Loss=4.813427540659904
Loss made of: CE 0.26344621181488037, LKD 3.5193734169006348, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/35, Loss=4.386472475528717
Loss made of: CE 0.2169552743434906, LKD 3.795051097869873, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.288957417011261, Reg Loss=4.23863410949707
Clinet index 9, End of Epoch 3/6, Average Loss=4.527591705322266, Class Loss=0.288957417011261, Reg Loss=4.23863410949707
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/35, Loss=4.590320733189583
Loss made of: CE 0.18287456035614014, LKD 3.523056983947754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/35, Loss=4.307215724885464
Loss made of: CE 0.31518396735191345, LKD 4.050396919250488, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/35, Loss=4.489797361195087
Loss made of: CE 0.2253858745098114, LKD 3.940835475921631, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2669726610183716, Reg Loss=4.1877522468566895
Clinet index 9, End of Epoch 4/6, Average Loss=4.4547247886657715, Class Loss=0.2669726610183716, Reg Loss=4.1877522468566895
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/35, Loss=4.188363969326019
Loss made of: CE 0.2582322061061859, LKD 3.539640426635742, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/35, Loss=4.509491634368897
Loss made of: CE 0.18178217113018036, LKD 3.827338218688965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.588523380458355
Loss made of: CE 0.253029465675354, LKD 5.247289657592773, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2622673511505127, Reg Loss=4.145220756530762
Clinet index 9, End of Epoch 5/6, Average Loss=4.407487869262695, Class Loss=0.2622673511505127, Reg Loss=4.145220756530762
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/35, Loss=4.4992120325565335
Loss made of: CE 0.214931458234787, LKD 4.540946960449219, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.3122407987713816
Loss made of: CE 0.31989961862564087, LKD 4.171392917633057, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/35, Loss=4.3236093953251835
Loss made of: CE 0.2245866060256958, LKD 4.240832805633545, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2560771405696869, Reg Loss=4.159366607666016
Clinet index 9, End of Epoch 6/6, Average Loss=4.4154438972473145, Class Loss=0.2560771405696869, Reg Loss=4.159366607666016
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=3.1678131729364396
Loss made of: CE 0.26413944363594055, LKD 2.8351447582244873, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.34885668754577637, Reg Loss=2.8371500968933105
Clinet index 4, End of Epoch 1/6, Average Loss=3.186006784439087, Class Loss=0.34885668754577637, Reg Loss=2.8371500968933105
Pseudo labeling is: None
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/19, Loss=3.2290195405483244
Loss made of: CE 0.3538847863674164, LKD 3.0422189235687256, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.34281042218208313, Reg Loss=2.8565492630004883
Clinet index 4, End of Epoch 2/6, Average Loss=3.199359655380249, Class Loss=0.34281042218208313, Reg Loss=2.8565492630004883
Pseudo labeling is: None
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/19, Loss=3.122780203819275
Loss made of: CE 0.31335052847862244, LKD 2.8161818981170654, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3250774145126343, Reg Loss=2.797213315963745
Clinet index 4, End of Epoch 3/6, Average Loss=3.12229061126709, Class Loss=0.3250774145126343, Reg Loss=2.797213315963745
Pseudo labeling is: None
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/19, Loss=3.101561999320984
Loss made of: CE 0.32377079129219055, LKD 2.998164176940918, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3288062512874603, Reg Loss=2.830021381378174
Clinet index 4, End of Epoch 4/6, Average Loss=3.158827543258667, Class Loss=0.3288062512874603, Reg Loss=2.830021381378174
Pseudo labeling is: None
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/19, Loss=3.2289275914430617
Loss made of: CE 0.3459251821041107, LKD 3.199052333831787, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3184955418109894, Reg Loss=2.8006927967071533
Clinet index 4, End of Epoch 5/6, Average Loss=3.1191883087158203, Class Loss=0.3184955418109894, Reg Loss=2.8006927967071533
Pseudo labeling is: None
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/19, Loss=3.005738705396652
Loss made of: CE 0.28394752740859985, LKD 2.479790210723877, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.31915855407714844, Reg Loss=2.8515312671661377
Clinet index 4, End of Epoch 6/6, Average Loss=3.170689821243286, Class Loss=0.31915855407714844, Reg Loss=2.8515312671661377
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000304
Epoch 1, Batch 10/19, Loss=3.265230706334114
Loss made of: CE 0.3609164357185364, LKD 2.7630155086517334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3480234742164612, Reg Loss=2.892198085784912
Clinet index 0, End of Epoch 1/6, Average Loss=3.2402215003967285, Class Loss=0.3480234742164612, Reg Loss=2.892198085784912
Pseudo labeling is: None
Epoch 2, lr = 0.000258
Epoch 2, Batch 10/19, Loss=3.343491791188717
Loss made of: CE 0.3397795557975769, LKD 2.6320879459381104, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3274588882923126, Reg Loss=2.8833842277526855
Clinet index 0, End of Epoch 2/6, Average Loss=3.210843086242676, Class Loss=0.3274588882923126, Reg Loss=2.8833842277526855
Pseudo labeling is: None
Epoch 3, lr = 0.000211
Epoch 3, Batch 10/19, Loss=3.66104579269886
Loss made of: CE 0.3510027825832367, LKD 3.3834152221679688, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3362492620944977, Reg Loss=2.963461399078369
Clinet index 0, End of Epoch 3/6, Average Loss=3.299710750579834, Class Loss=0.3362492620944977, Reg Loss=2.963461399078369
Pseudo labeling is: None
Epoch 4, lr = 0.000163
Epoch 4, Batch 10/19, Loss=3.031692439317703
Loss made of: CE 0.4028967320919037, LKD 2.705657958984375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.32252827286720276, Reg Loss=2.8549885749816895
Clinet index 0, End of Epoch 4/6, Average Loss=3.1775169372558594, Class Loss=0.32252827286720276, Reg Loss=2.8549885749816895
Pseudo labeling is: None
Epoch 5, lr = 0.000113
Epoch 5, Batch 10/19, Loss=3.399438926577568
Loss made of: CE 0.33693671226501465, LKD 2.887695550918579, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.321760356426239, Reg Loss=2.9297614097595215
Clinet index 0, End of Epoch 5/6, Average Loss=3.2515218257904053, Class Loss=0.321760356426239, Reg Loss=2.9297614097595215
Pseudo labeling is: None
Epoch 6, lr = 0.000061
Epoch 6, Batch 10/19, Loss=3.1877506554126738
Loss made of: CE 0.38541707396507263, LKD 2.5476739406585693, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3298361003398895, Reg Loss=2.95173978805542
Clinet index 0, End of Epoch 6/6, Average Loss=3.281575918197632, Class Loss=0.3298361003398895, Reg Loss=2.95173978805542
federated aggregation...
Validation, Class Loss=0.3935496211051941, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.869301
Mean Acc: 0.483397
FreqW Acc: 0.792177
Mean IoU: 0.387254
Class IoU:
	class 0: 0.9177665
	class 1: 0.072138384
	class 2: 0.0072145527
	class 3: 0.0
	class 4: 0.39849484
	class 5: 0.34191164
	class 6: 0.7648977
	class 7: 0.7761097
	class 8: 0.24516611
	class 9: 0.17705166
	class 10: 0.60912716
	class 11: 0.34404173
	class 12: 0.38038483
Class Acc:
	class 0: 0.97654015
	class 1: 0.07215397
	class 2: 0.0073399823
	class 3: 0.0
	class 4: 0.40656975
	class 5: 0.34472436
	class 6: 0.77648157
	class 7: 0.8156149
	class 8: 0.2468956
	class 9: 0.22573774
	class 10: 0.860234
	class 11: 0.61729383
	class 12: 0.9345805

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[11, 6, 10, 7]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=6.225636613368988
Loss made of: CE 1.5566953420639038, LKD 4.10438871383667, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=6.348376536369324
Loss made of: CE 1.1273596286773682, LKD 4.5593976974487305, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=5.687135475873947
Loss made of: CE 1.1045691967010498, LKD 4.803022861480713, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.664290273189545
Loss made of: CE 0.7850078344345093, LKD 4.595534324645996, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.54057537317276
Loss made of: CE 0.5971249341964722, LKD 4.135241508483887, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.250163823366165
Loss made of: CE 0.7091309428215027, LKD 3.68725323677063, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.092411345243454
Loss made of: CE 0.6016989946365356, LKD 4.379120349884033, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=5.098417747020721
Loss made of: CE 0.5793745517730713, LKD 4.256998062133789, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.860653132200241
Loss made of: CE 0.5853924751281738, LKD 5.282626152038574, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=5.199282163381577
Loss made of: CE 0.6088537573814392, LKD 5.536921977996826, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.9352961778640747, Reg Loss=4.5571675300598145
Clinet index 11, End of Epoch 1/6, Average Loss=5.4924635887146, Class Loss=0.9352961778640747, Reg Loss=4.5571675300598145
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=4.889378002285957
Loss made of: CE 0.4839410185813904, LKD 4.011826038360596, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.709859576821327
Loss made of: CE 0.4406016767024994, LKD 3.5968258380889893, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=5.157562962174415
Loss made of: CE 0.5013365149497986, LKD 5.037415981292725, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=5.01329565346241
Loss made of: CE 0.5933241248130798, LKD 4.66187858581543, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=5.00286765396595
Loss made of: CE 0.545918345451355, LKD 4.915408611297607, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=5.154738473892212
Loss made of: CE 0.5323269367218018, LKD 5.207235813140869, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.906792801618576
Loss made of: CE 0.5382055044174194, LKD 5.255929946899414, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=5.46250629723072
Loss made of: CE 0.5778721570968628, LKD 5.819918155670166, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.656149119138718
Loss made of: CE 0.53279709815979, LKD 4.6110711097717285, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.715650349855423
Loss made of: CE 0.3803824186325073, LKD 4.450709819793701, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5228142142295837, Reg Loss=4.4468536376953125
Clinet index 11, End of Epoch 2/6, Average Loss=4.969667911529541, Class Loss=0.5228142142295837, Reg Loss=4.4468536376953125
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=4.84172610938549
Loss made of: CE 0.46498972177505493, LKD 4.7086052894592285, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.665295234322548
Loss made of: CE 0.49114179611206055, LKD 4.465930938720703, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.650479197502136
Loss made of: CE 0.4864656627178192, LKD 4.428718566894531, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.8328199416399
Loss made of: CE 0.4817441403865814, LKD 4.872505187988281, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=5.130806821584701
Loss made of: CE 0.45728468894958496, LKD 4.898894786834717, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.907570761442185
Loss made of: CE 0.5230342745780945, LKD 4.239287376403809, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.826645568013191
Loss made of: CE 0.355286568403244, LKD 4.370275974273682, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.869306963682175
Loss made of: CE 0.43363460898399353, LKD 4.497713088989258, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.752519795298577
Loss made of: CE 0.3691120743751526, LKD 4.232049942016602, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=5.111655807495117
Loss made of: CE 0.4152950644493103, LKD 4.133538722991943, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.46637246012687683, Reg Loss=4.3920674324035645
Clinet index 11, End of Epoch 3/6, Average Loss=4.858439922332764, Class Loss=0.46637246012687683, Reg Loss=4.3920674324035645
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.9860520452260975
Loss made of: CE 0.4368249177932739, LKD 4.014814853668213, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.733348220586777
Loss made of: CE 0.4349188804626465, LKD 3.636195421218872, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.870946457982063
Loss made of: CE 0.49861133098602295, LKD 4.493968963623047, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.860628786683082
Loss made of: CE 0.43530744314193726, LKD 3.731142997741699, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.853697425127029
Loss made of: CE 0.49149417877197266, LKD 4.5299224853515625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.746194753050804
Loss made of: CE 0.3815402388572693, LKD 3.75321102142334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.818642628192902
Loss made of: CE 0.5066357851028442, LKD 4.884114742279053, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.902316096425056
Loss made of: CE 0.5209468007087708, LKD 4.404034614562988, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.808843502402306
Loss made of: CE 0.39457497000694275, LKD 4.0672287940979, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.788527479767799
Loss made of: CE 0.4623130261898041, LKD 3.9788055419921875, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.44247183203697205, Reg Loss=4.409701347351074
Clinet index 11, End of Epoch 4/6, Average Loss=4.852173328399658, Class Loss=0.44247183203697205, Reg Loss=4.409701347351074
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=4.964826709032058
Loss made of: CE 0.4070780277252197, LKD 5.153198719024658, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.920865708589554
Loss made of: CE 0.40967100858688354, LKD 4.8193159103393555, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.857598224282265
Loss made of: CE 0.35308998823165894, LKD 3.9077110290527344, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.756208235025406
Loss made of: CE 0.4280882477760315, LKD 4.402196407318115, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.903927618265152
Loss made of: CE 0.463767945766449, LKD 4.5150370597839355, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.889191859960556
Loss made of: CE 0.4217032194137573, LKD 5.725268840789795, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.361032122373581
Loss made of: CE 0.45231184363365173, LKD 4.780611038208008, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.506250166893006
Loss made of: CE 0.29170170426368713, LKD 4.024569511413574, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.744368517398835
Loss made of: CE 0.49335235357284546, LKD 4.702342510223389, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.864871388673782
Loss made of: CE 0.44386357069015503, LKD 4.3133015632629395, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.42106539011001587, Reg Loss=4.369298934936523
Clinet index 11, End of Epoch 5/6, Average Loss=4.7903642654418945, Class Loss=0.42106539011001587, Reg Loss=4.369298934936523
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.648022991418839
Loss made of: CE 0.4250866770744324, LKD 3.932204484939575, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.906402197480202
Loss made of: CE 0.4336519241333008, LKD 5.486546993255615, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.977116942405701
Loss made of: CE 0.43312180042266846, LKD 3.734088659286499, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.87603068947792
Loss made of: CE 0.394370973110199, LKD 3.909111738204956, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.919381782412529
Loss made of: CE 0.3491746187210083, LKD 4.412938117980957, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.79883382320404
Loss made of: CE 0.42327746748924255, LKD 5.755331516265869, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=5.0512315928936005
Loss made of: CE 0.44141149520874023, LKD 3.9403035640716553, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.665267887711525
Loss made of: CE 0.3975631594657898, LKD 5.561676025390625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.7846729964017864
Loss made of: CE 0.4435783624649048, LKD 4.013549327850342, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.648777592182159
Loss made of: CE 0.4127473831176758, LKD 4.734446048736572, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.41468390822410583, Reg Loss=4.421881198883057
Clinet index 11, End of Epoch 6/6, Average Loss=4.836565017700195, Class Loss=0.41468390822410583, Reg Loss=4.421881198883057
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=7.365234422683716
Loss made of: CE 1.3440511226654053, LKD 4.928241729736328, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=6.578851211071014
Loss made of: CE 1.2597949504852295, LKD 5.156811237335205, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.4046434164047241, Reg Loss=5.4540252685546875
Clinet index 6, End of Epoch 1/6, Average Loss=6.858668804168701, Class Loss=1.4046434164047241, Reg Loss=5.4540252685546875
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=6.175353372097016
Loss made of: CE 1.1756315231323242, LKD 5.290289878845215, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=6.31007889509201
Loss made of: CE 0.7241964340209961, LKD 4.628238201141357, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.9519968032836914, Reg Loss=5.262068271636963
Clinet index 6, End of Epoch 2/6, Average Loss=6.214065074920654, Class Loss=0.9519968032836914, Reg Loss=5.262068271636963
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=5.846131902933121
Loss made of: CE 0.7618448734283447, LKD 5.413821220397949, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=6.078094029426575
Loss made of: CE 0.7041220664978027, LKD 5.002629280090332, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.72303307056427, Reg Loss=5.177180290222168
Clinet index 6, End of Epoch 3/6, Average Loss=5.900213241577148, Class Loss=0.72303307056427, Reg Loss=5.177180290222168
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=6.028356772661209
Loss made of: CE 0.6817716956138611, LKD 6.089895248413086, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=5.801607429981232
Loss made of: CE 0.5049600601196289, LKD 4.898825168609619, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6032285094261169, Reg Loss=5.251164436340332
Clinet index 6, End of Epoch 4/6, Average Loss=5.854393005371094, Class Loss=0.6032285094261169, Reg Loss=5.251164436340332
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=5.529449361562729
Loss made of: CE 0.6945215463638306, LKD 4.4001946449279785, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=5.792511746287346
Loss made of: CE 0.4625106751918793, LKD 4.888208389282227, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5161778330802917, Reg Loss=5.255991458892822
Clinet index 6, End of Epoch 5/6, Average Loss=5.77216911315918, Class Loss=0.5161778330802917, Reg Loss=5.255991458892822
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=5.840803346037864
Loss made of: CE 0.4794778525829315, LKD 5.835312366485596, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=5.572955060005188
Loss made of: CE 0.42453739047050476, LKD 4.858862400054932, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5000247955322266, Reg Loss=5.22711706161499
Clinet index 6, End of Epoch 6/6, Average Loss=5.727141857147217, Class Loss=0.5000247955322266, Reg Loss=5.22711706161499
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=7.177576637268066
Loss made of: CE 1.5658769607543945, LKD 5.4500885009765625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=6.620451521873474
Loss made of: CE 1.097653865814209, LKD 4.743988037109375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.3972243070602417, Reg Loss=5.435075283050537
Clinet index 10, End of Epoch 1/6, Average Loss=6.832299709320068, Class Loss=1.3972243070602417, Reg Loss=5.435075283050537
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=6.608792424201965
Loss made of: CE 1.0442718267440796, LKD 6.685165882110596, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=6.050200366973877
Loss made of: CE 0.8390535712242126, LKD 6.582154273986816, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.947368860244751, Reg Loss=5.295658588409424
Clinet index 10, End of Epoch 2/6, Average Loss=6.243027687072754, Class Loss=0.947368860244751, Reg Loss=5.295658588409424
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=5.741860640048981
Loss made of: CE 0.6171824336051941, LKD 5.537454605102539, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=6.105173873901367
Loss made of: CE 0.5431474447250366, LKD 4.9487152099609375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7308786511421204, Reg Loss=5.1958699226379395
Clinet index 10, End of Epoch 3/6, Average Loss=5.926748752593994, Class Loss=0.7308786511421204, Reg Loss=5.1958699226379395
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=5.739131277799606
Loss made of: CE 0.5080173015594482, LKD 4.781944751739502, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=5.938124364614486
Loss made of: CE 0.5213215351104736, LKD 4.936984062194824, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6064314842224121, Reg Loss=5.215205192565918
Clinet index 10, End of Epoch 4/6, Average Loss=5.82163667678833, Class Loss=0.6064314842224121, Reg Loss=5.215205192565918
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=5.855099010467529
Loss made of: CE 0.4824705123901367, LKD 5.570755481719971, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=5.821561652421951
Loss made of: CE 0.5839202404022217, LKD 6.556440830230713, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5366837382316589, Reg Loss=5.279101371765137
Clinet index 10, End of Epoch 5/6, Average Loss=5.815784931182861, Class Loss=0.5366837382316589, Reg Loss=5.279101371765137
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=5.768702176213265
Loss made of: CE 0.5523622035980225, LKD 4.121014595031738, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=5.645904907584191
Loss made of: CE 0.5298633575439453, LKD 4.535680294036865, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4967643618583679, Reg Loss=5.203190803527832
Clinet index 10, End of Epoch 6/6, Average Loss=5.699954986572266, Class Loss=0.4967643618583679, Reg Loss=5.203190803527832
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=6.7742233872413635
Loss made of: CE 1.6648712158203125, LKD 5.406806945800781, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.72497946023941
Loss made of: CE 1.502486228942871, LKD 4.73601770401001, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=5.7456526279449465
Loss made of: CE 1.1055594682693481, LKD 4.384322166442871, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.825958663225174
Loss made of: CE 0.8436781764030457, LKD 5.258869647979736, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.767196601629257
Loss made of: CE 0.8241580724716187, LKD 4.84443473815918, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.108565551042557
Loss made of: CE 0.7382758855819702, LKD 4.1383209228515625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.85679925084114
Loss made of: CE 0.6656274795532227, LKD 3.8199052810668945, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=5.141818279027939
Loss made of: CE 0.6532559394836426, LKD 5.2130560874938965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=5.128779965639114
Loss made of: CE 0.5602627992630005, LKD 4.39630126953125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=5.215463030338287
Loss made of: CE 0.6025263071060181, LKD 4.284062385559082, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.9353776574134827, Reg Loss=4.5921783447265625
Clinet index 7, End of Epoch 1/6, Average Loss=5.5275559425354, Class Loss=0.9353776574134827, Reg Loss=4.5921783447265625
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=5.06450669169426
Loss made of: CE 0.5454857349395752, LKD 4.162335395812988, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.727564266324043
Loss made of: CE 0.5721272230148315, LKD 4.180930137634277, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=5.058425858616829
Loss made of: CE 0.47143635153770447, LKD 4.7493510246276855, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.966106137633323
Loss made of: CE 0.5063626766204834, LKD 3.7145192623138428, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=5.15924586057663
Loss made of: CE 0.479181706905365, LKD 3.8695502281188965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=5.162720486521721
Loss made of: CE 0.4502469301223755, LKD 4.115426540374756, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.741052746772766
Loss made of: CE 0.5170145034790039, LKD 4.025229454040527, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.944318202137947
Loss made of: CE 0.45113667845726013, LKD 3.901010274887085, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=5.035284748673439
Loss made of: CE 0.4660981595516205, LKD 4.071063041687012, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.8208593219518665
Loss made of: CE 0.4221835136413574, LKD 4.283324718475342, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5217848420143127, Reg Loss=4.444042682647705
Clinet index 7, End of Epoch 2/6, Average Loss=4.965827465057373, Class Loss=0.5217848420143127, Reg Loss=4.444042682647705
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=5.080646994709968
Loss made of: CE 0.5617520809173584, LKD 4.897637367248535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.708745613694191
Loss made of: CE 0.4208622872829437, LKD 5.014249801635742, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.874751338362694
Loss made of: CE 0.4362003803253174, LKD 4.456432819366455, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.8804868966341015
Loss made of: CE 0.4170622229576111, LKD 3.7362680435180664, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.886906599998474
Loss made of: CE 0.5398343205451965, LKD 4.193942070007324, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.860407280921936
Loss made of: CE 0.502415120601654, LKD 4.01765775680542, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=5.0202974528074265
Loss made of: CE 0.5014176368713379, LKD 5.054197788238525, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=5.068191343545914
Loss made of: CE 0.42482995986938477, LKD 5.983915328979492, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=5.04930135011673
Loss made of: CE 0.46632689237594604, LKD 4.852488040924072, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.672327661514283
Loss made of: CE 0.4302089214324951, LKD 3.618703842163086, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4680576026439667, Reg Loss=4.437131404876709
Clinet index 7, End of Epoch 3/6, Average Loss=4.905189037322998, Class Loss=0.4680576026439667, Reg Loss=4.437131404876709
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.570858988165855
Loss made of: CE 0.43992793560028076, LKD 4.084293365478516, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.999306660890579
Loss made of: CE 0.36654132604599, LKD 4.936578273773193, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=5.225603184103965
Loss made of: CE 0.5213406085968018, LKD 4.521542072296143, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.735114645957947
Loss made of: CE 0.40500205755233765, LKD 3.9419875144958496, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.830805945396423
Loss made of: CE 0.4555990397930145, LKD 4.424736022949219, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.892908599972725
Loss made of: CE 0.47269630432128906, LKD 4.6168317794799805, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.768040853738785
Loss made of: CE 0.4715453088283539, LKD 4.428520202636719, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.921529853343964
Loss made of: CE 0.4724287986755371, LKD 4.719186782836914, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.851106643676758
Loss made of: CE 0.433940052986145, LKD 3.3811933994293213, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=5.084680604934692
Loss made of: CE 0.47996658086776733, LKD 4.156536102294922, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.446053683757782, Reg Loss=4.432729244232178
Clinet index 7, End of Epoch 4/6, Average Loss=4.878782749176025, Class Loss=0.446053683757782, Reg Loss=4.432729244232178
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=5.058817303180694
Loss made of: CE 0.5426238775253296, LKD 4.552597999572754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.811521717905999
Loss made of: CE 0.5430838465690613, LKD 4.434072494506836, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.492815983295441
Loss made of: CE 0.43404802680015564, LKD 4.325789928436279, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.890026038885116
Loss made of: CE 0.36862725019454956, LKD 4.588628768920898, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.683900102972984
Loss made of: CE 0.4415820837020874, LKD 4.540560722351074, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.754721131920815
Loss made of: CE 0.4478379487991333, LKD 4.989266395568848, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=5.049933567643166
Loss made of: CE 0.39130699634552, LKD 4.018174648284912, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.887569928169251
Loss made of: CE 0.434798002243042, LKD 3.6849205493927, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=5.0686734288930895
Loss made of: CE 0.4462565779685974, LKD 4.784602165222168, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.756587880849838
Loss made of: CE 0.4971775710582733, LKD 4.6305155754089355, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.42660096287727356, Reg Loss=4.41973352432251
Clinet index 7, End of Epoch 5/6, Average Loss=4.846334457397461, Class Loss=0.42660096287727356, Reg Loss=4.41973352432251
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.938973370194435
Loss made of: CE 0.38175463676452637, LKD 4.522346496582031, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.925051108002663
Loss made of: CE 0.40218687057495117, LKD 4.948366641998291, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.8834418028593065
Loss made of: CE 0.40613096952438354, LKD 4.779930114746094, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=5.056470742821693
Loss made of: CE 0.40171417593955994, LKD 4.075264930725098, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.870911267399788
Loss made of: CE 0.46119770407676697, LKD 4.397156715393066, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.686434134840965
Loss made of: CE 0.45353907346725464, LKD 3.9985597133636475, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.619418179988861
Loss made of: CE 0.33694928884506226, LKD 4.446991920471191, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.91062451004982
Loss made of: CE 0.42733949422836304, LKD 4.755836486816406, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.811495652794838
Loss made of: CE 0.4194514751434326, LKD 4.742806911468506, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.539958992600441
Loss made of: CE 0.475679486989975, LKD 4.280092716217041, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4109916388988495, Reg Loss=4.430770397186279
Clinet index 7, End of Epoch 6/6, Average Loss=4.841762065887451, Class Loss=0.4109916388988495, Reg Loss=4.430770397186279
federated aggregation...
Validation, Class Loss=0.5067007541656494, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.839857
Mean Acc: 0.425841
FreqW Acc: 0.738159
Mean IoU: 0.323823
Class IoU:
	class 0: 0.8834775
	class 1: 0.0032333357
	class 2: 0.0
	class 3: 0.0
	class 4: 0.3376901
	class 5: 0.30289274
	class 6: 0.71005094
	class 7: 0.64686644
	class 8: 0.26949674
	class 9: 0.1311825
	class 10: 0.47084993
	class 11: 0.38921416
	class 12: 0.39790246
	class 13: 1.7865868e-05
	class 14: 0.3528198
	class 15: 0.60892034
	class 16: 0.00037492212
Class Acc:
	class 0: 0.97590613
	class 1: 0.0032333964
	class 2: 0.0
	class 3: 0.0
	class 4: 0.3430421
	class 5: 0.30472738
	class 6: 0.7333658
	class 7: 0.66024816
	class 8: 0.27039295
	class 9: 0.1410527
	class 10: 0.81364304
	class 11: 0.51758283
	class 12: 0.8444034
	class 13: 1.7865868e-05
	class 14: 0.9214759
	class 15: 0.70982766
	class 16: 0.0003753074

federated global round: 16, step: 3
select part of clients to conduct local training
[7, 11, 16, 18]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=5.011147755384445
Loss made of: CE 0.6072320342063904, LKD 4.89176082611084, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.634462544322014
Loss made of: CE 0.6023510694503784, LKD 4.356026649475098, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.839006537199021
Loss made of: CE 0.47029852867126465, LKD 3.906888484954834, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.29824735224247
Loss made of: CE 0.5271412134170532, LKD 5.079021453857422, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.352051940560341
Loss made of: CE 0.5229946374893188, LKD 4.573469638824463, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.773163136839867
Loss made of: CE 0.5048671960830688, LKD 4.165558338165283, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.504288563132286
Loss made of: CE 0.4677842855453491, LKD 3.702742338180542, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.920234104990959
Loss made of: CE 0.498241662979126, LKD 5.696518898010254, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.911849880218506
Loss made of: CE 0.46894484758377075, LKD 4.23886775970459, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=5.093370532989502
Loss made of: CE 0.4878273010253906, LKD 4.249140739440918, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.48606061935424805, Reg Loss=4.452597141265869
Clinet index 7, End of Epoch 1/6, Average Loss=4.938657760620117, Class Loss=0.48606061935424805, Reg Loss=4.452597141265869
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.855610638856888
Loss made of: CE 0.3836922347545624, LKD 3.922895669937134, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.714441254734993
Loss made of: CE 0.47797060012817383, LKD 4.398789405822754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.906519600749016
Loss made of: CE 0.3611043393611908, LKD 4.627561569213867, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.874765452742577
Loss made of: CE 0.4369903802871704, LKD 3.8032896518707275, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=5.035368600487709
Loss made of: CE 0.4273242652416229, LKD 3.942805051803589, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.948496013879776
Loss made of: CE 0.37940117716789246, LKD 4.04738712310791, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.734727683663368
Loss made of: CE 0.4424305558204651, LKD 4.20038366317749, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.767131260037422
Loss made of: CE 0.3994944095611572, LKD 3.8295626640319824, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.932792246341705
Loss made of: CE 0.4144849181175232, LKD 3.9557442665100098, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.668046525120735
Loss made of: CE 0.380138099193573, LKD 3.959151268005371, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.43526631593704224, Reg Loss=4.406071186065674
Clinet index 7, End of Epoch 2/6, Average Loss=4.84133768081665, Class Loss=0.43526631593704224, Reg Loss=4.406071186065674
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.984931066632271
Loss made of: CE 0.45192235708236694, LKD 4.744990825653076, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.593085017800331
Loss made of: CE 0.3621262311935425, LKD 4.874629974365234, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.893807229399681
Loss made of: CE 0.4196685254573822, LKD 4.631119728088379, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.765447530150413
Loss made of: CE 0.4047508239746094, LKD 3.6014721393585205, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.879839614033699
Loss made of: CE 0.4825526177883148, LKD 4.2973480224609375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.750547987222672
Loss made of: CE 0.48315876722335815, LKD 4.384293556213379, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.919159981608391
Loss made of: CE 0.457253634929657, LKD 5.192166328430176, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.954350486397743
Loss made of: CE 0.3824016749858856, LKD 5.704381942749023, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=5.035195460915565
Loss made of: CE 0.44404923915863037, LKD 4.958674430847168, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.6121059358119965
Loss made of: CE 0.41094040870666504, LKD 3.776761531829834, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4169829189777374, Reg Loss=4.418739318847656
Clinet index 7, End of Epoch 3/6, Average Loss=4.83572244644165, Class Loss=0.4169829189777374, Reg Loss=4.418739318847656
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.621075052022934
Loss made of: CE 0.3949923515319824, LKD 4.057749271392822, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.876488366723061
Loss made of: CE 0.3216395974159241, LKD 5.026564598083496, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=5.256441453099251
Loss made of: CE 0.5003201961517334, LKD 4.645665645599365, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.703873628377915
Loss made of: CE 0.3655565679073334, LKD 4.065526962280273, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.673679599165917
Loss made of: CE 0.3970153331756592, LKD 4.505417823791504, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.811045822501183
Loss made of: CE 0.4592660367488861, LKD 4.752725601196289, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.768455812335015
Loss made of: CE 0.4226563572883606, LKD 4.345715045928955, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.841809186339378
Loss made of: CE 0.4372525215148926, LKD 4.6465983390808105, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.782387483119964
Loss made of: CE 0.40843069553375244, LKD 3.530290365219116, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.978899428248406
Loss made of: CE 0.45394954085350037, LKD 4.422558784484863, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4122616648674011, Reg Loss=4.4091596603393555
Clinet index 7, End of Epoch 4/6, Average Loss=4.821421146392822, Class Loss=0.4122616648674011, Reg Loss=4.4091596603393555
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.994812035560608
Loss made of: CE 0.5148844718933105, LKD 4.6537041664123535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.8210953146219255
Loss made of: CE 0.490887850522995, LKD 4.289350986480713, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.542584675550461
Loss made of: CE 0.3682992458343506, LKD 4.405878067016602, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.854223304986954
Loss made of: CE 0.3410975933074951, LKD 4.680631637573242, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.618570944666862
Loss made of: CE 0.3870493769645691, LKD 4.543457508087158, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.806137028336525
Loss made of: CE 0.4200139045715332, LKD 5.102867126464844, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.989281818270683
Loss made of: CE 0.3747941255569458, LKD 3.898786783218384, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.811320221424102
Loss made of: CE 0.42611774802207947, LKD 3.6041293144226074, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=5.04281075000763
Loss made of: CE 0.41580232977867126, LKD 5.1188836097717285, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.701257580518723
Loss made of: CE 0.45387864112854004, LKD 4.387697696685791, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.40161216259002686, Reg Loss=4.422234058380127
Clinet index 7, End of Epoch 5/6, Average Loss=4.823846340179443, Class Loss=0.40161216259002686, Reg Loss=4.422234058380127
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.905354624986648
Loss made of: CE 0.36673492193222046, LKD 4.722459316253662, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.818999472260475
Loss made of: CE 0.3833317756652832, LKD 4.922772407531738, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.955223590135574
Loss made of: CE 0.38059040904045105, LKD 4.836721897125244, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.999391159415245
Loss made of: CE 0.39242666959762573, LKD 4.096489906311035, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.868489280343056
Loss made of: CE 0.45083996653556824, LKD 4.328472137451172, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.574604117870331
Loss made of: CE 0.4062773585319519, LKD 3.9580001831054688, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.470306026935577
Loss made of: CE 0.3299325406551361, LKD 4.447566986083984, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=5.014809626340866
Loss made of: CE 0.4205322265625, LKD 4.957068920135498, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.771086704730988
Loss made of: CE 0.39669331908226013, LKD 4.844449520111084, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.496163526177407
Loss made of: CE 0.4396950602531433, LKD 4.256383895874023, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3927854299545288, Reg Loss=4.41499662399292
Clinet index 7, End of Epoch 6/6, Average Loss=4.807782173156738, Class Loss=0.3927854299545288, Reg Loss=4.41499662399292
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=4.635366240143776
Loss made of: CE 0.5579001903533936, LKD 4.093439102172852, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.314125573635101
Loss made of: CE 0.5706762075424194, LKD 4.3266167640686035, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.8412801861763
Loss made of: CE 0.4576614797115326, LKD 4.745843410491943, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.0670413166284565
Loss made of: CE 0.45242995023727417, LKD 4.432132244110107, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.023443803191185
Loss made of: CE 0.4134880006313324, LKD 3.8177571296691895, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.976869711279869
Loss made of: CE 0.4307898283004761, LKD 3.6582083702087402, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.845271089673043
Loss made of: CE 0.4395163655281067, LKD 4.198450088500977, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.8305101782083515
Loss made of: CE 0.41768378019332886, LKD 4.221724510192871, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.509463894367218
Loss made of: CE 0.43070822954177856, LKD 4.85406494140625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.974888038635254
Loss made of: CE 0.4317266345024109, LKD 5.358792781829834, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4852941632270813, Reg Loss=4.421459197998047
Clinet index 11, End of Epoch 1/6, Average Loss=4.9067535400390625, Class Loss=0.4852941632270813, Reg Loss=4.421459197998047
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.704335886240005
Loss made of: CE 0.3664994239807129, LKD 3.7696316242218018, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.53721055984497
Loss made of: CE 0.4206487834453583, LKD 3.6381847858428955, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=5.021028691530228
Loss made of: CE 0.4113400876522064, LKD 4.898462772369385, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.883319789171219
Loss made of: CE 0.44235819578170776, LKD 4.81039571762085, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.949337273836136
Loss made of: CE 0.4544520378112793, LKD 4.929839134216309, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=5.030502593517303
Loss made of: CE 0.477877140045166, LKD 4.678445816040039, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.792034402489662
Loss made of: CE 0.46235665678977966, LKD 5.732212543487549, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=5.222516366839409
Loss made of: CE 0.5168071985244751, LKD 6.188234329223633, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.500909599661827
Loss made of: CE 0.4526790976524353, LKD 4.29317569732666, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.531786715984344
Loss made of: CE 0.3563382625579834, LKD 4.336136817932129, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.43621236085891724, Reg Loss=4.388973236083984
Clinet index 11, End of Epoch 2/6, Average Loss=4.825185775756836, Class Loss=0.43621236085891724, Reg Loss=4.388973236083984
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.799106875061989
Loss made of: CE 0.387046754360199, LKD 4.9109320640563965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.660150742530822
Loss made of: CE 0.40141910314559937, LKD 4.36941385269165, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.669797590374946
Loss made of: CE 0.4574362635612488, LKD 4.285384178161621, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.860550802946091
Loss made of: CE 0.45844316482543945, LKD 4.792817115783691, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.9668158233165745
Loss made of: CE 0.4487764239311218, LKD 4.731838226318359, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.732545402646065
Loss made of: CE 0.4495461583137512, LKD 4.024863243103027, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.703624641895294
Loss made of: CE 0.32894688844680786, LKD 4.006402969360352, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.754141795635223
Loss made of: CE 0.4101690649986267, LKD 3.9941163063049316, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.784436571598053
Loss made of: CE 0.3210770785808563, LKD 4.225988864898682, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=5.097432464361191
Loss made of: CE 0.3980066478252411, LKD 4.055524826049805, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4172510504722595, Reg Loss=4.389455795288086
Clinet index 11, End of Epoch 3/6, Average Loss=4.80670690536499, Class Loss=0.4172510504722595, Reg Loss=4.389455795288086
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.935235303640366
Loss made of: CE 0.3744102716445923, LKD 3.9034485816955566, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.676961460709572
Loss made of: CE 0.4051182270050049, LKD 3.438926935195923, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.797350576519966
Loss made of: CE 0.4724072813987732, LKD 4.730062007904053, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.739912286400795
Loss made of: CE 0.37122559547424316, LKD 3.776688814163208, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.848570716381073
Loss made of: CE 0.46292170882225037, LKD 4.567409515380859, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.633891862630844
Loss made of: CE 0.3276818096637726, LKD 3.610208511352539, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.809668454527855
Loss made of: CE 0.4649984538555145, LKD 4.996836185455322, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.912039133906364
Loss made of: CE 0.4894210696220398, LKD 4.450986385345459, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.798749649524689
Loss made of: CE 0.3630620837211609, LKD 4.243531227111816, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.745190736651421
Loss made of: CE 0.42409437894821167, LKD 3.853736400604248, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.408205509185791, Reg Loss=4.394225120544434
Clinet index 11, End of Epoch 4/6, Average Loss=4.802430629730225, Class Loss=0.408205509185791, Reg Loss=4.394225120544434
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.903492096066475
Loss made of: CE 0.3777846693992615, LKD 5.131175518035889, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.973005563020706
Loss made of: CE 0.4136607050895691, LKD 5.0532145500183105, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.937048071622849
Loss made of: CE 0.341366708278656, LKD 3.897108793258667, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.772750034928322
Loss made of: CE 0.39763766527175903, LKD 4.423350811004639, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.870947077870369
Loss made of: CE 0.4450387954711914, LKD 4.528546333312988, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.886977833509445
Loss made of: CE 0.3884703516960144, LKD 5.4962005615234375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.385474294424057
Loss made of: CE 0.40846574306488037, LKD 4.591495990753174, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.483301466703415
Loss made of: CE 0.25561028718948364, LKD 3.9837100505828857, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.73769770860672
Loss made of: CE 0.4985349178314209, LKD 5.1250901222229, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.778987067937851
Loss made of: CE 0.44155144691467285, LKD 4.123934745788574, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3972170352935791, Reg Loss=4.385751247406006
Clinet index 11, End of Epoch 5/6, Average Loss=4.782968521118164, Class Loss=0.3972170352935791, Reg Loss=4.385751247406006
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.678958874940872
Loss made of: CE 0.39948076009750366, LKD 4.011109828948975, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.761746209859848
Loss made of: CE 0.3705725073814392, LKD 5.021141052246094, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.881817352771759
Loss made of: CE 0.40697407722473145, LKD 3.975224018096924, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.845274981856346
Loss made of: CE 0.3717315196990967, LKD 4.247416019439697, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.853462296724319
Loss made of: CE 0.3191615045070648, LKD 4.117401599884033, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.656400486826897
Loss made of: CE 0.37440234422683716, LKD 5.30983829498291, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.9646030575037
Loss made of: CE 0.39156287908554077, LKD 4.007440090179443, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.6775748193264
Loss made of: CE 0.3821713328361511, LKD 5.365139007568359, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.645926353335381
Loss made of: CE 0.40172693133354187, LKD 3.95577335357666, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.490598726272583
Loss made of: CE 0.39728137850761414, LKD 4.998294830322266, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.38997912406921387, Reg Loss=4.370234489440918
Clinet index 11, End of Epoch 6/6, Average Loss=4.760213851928711, Class Loss=0.38997912406921387, Reg Loss=4.370234489440918
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/105, Loss=5.061619707942009
Loss made of: CE 0.7463929653167725, LKD 3.8594462871551514, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.810625237226486
Loss made of: CE 0.5841891765594482, LKD 4.292031288146973, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/105, Loss=4.674299496412277
Loss made of: CE 0.46541690826416016, LKD 4.262042999267578, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/105, Loss=4.902218365669251
Loss made of: CE 0.5388914346694946, LKD 4.357659816741943, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/105, Loss=4.652468305826187
Loss made of: CE 0.44404491782188416, LKD 3.2949745655059814, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.584292554855347
Loss made of: CE 0.5402795672416687, LKD 5.723764896392822, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.703641018271446
Loss made of: CE 0.3916218876838684, LKD 4.019741535186768, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/105, Loss=4.81171748638153
Loss made of: CE 0.5431672930717468, LKD 4.204345703125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/105, Loss=4.742140796780586
Loss made of: CE 0.5055378675460815, LKD 4.212099075317383, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.801065024733544
Loss made of: CE 0.41415831446647644, LKD 4.146636486053467, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5107911229133606, Reg Loss=4.268805980682373
Clinet index 16, End of Epoch 1/6, Average Loss=4.779597282409668, Class Loss=0.5107911229133606, Reg Loss=4.268805980682373
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/105, Loss=4.697184106707573
Loss made of: CE 0.3720232844352722, LKD 3.4290947914123535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.5763402670621876
Loss made of: CE 0.47322964668273926, LKD 4.256150245666504, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.384011107683182
Loss made of: CE 0.4802321791648865, LKD 4.674929618835449, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.864550474286079
Loss made of: CE 0.44736990332603455, LKD 4.243309497833252, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.517823496460915
Loss made of: CE 0.5072652697563171, LKD 4.566241264343262, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/105, Loss=4.628621280193329
Loss made of: CE 0.4215715825557709, LKD 3.3838601112365723, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/105, Loss=4.712710186839104
Loss made of: CE 0.46224653720855713, LKD 4.576757431030273, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/105, Loss=4.809220749139786
Loss made of: CE 0.44881051778793335, LKD 3.636174201965332, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/105, Loss=4.749034988880157
Loss made of: CE 0.46345585584640503, LKD 4.994096755981445, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.911383956670761
Loss made of: CE 0.4864335060119629, LKD 4.5350799560546875, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4477296471595764, Reg Loss=4.244263172149658
Clinet index 16, End of Epoch 2/6, Average Loss=4.69199275970459, Class Loss=0.4477296471595764, Reg Loss=4.244263172149658
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/105, Loss=4.869555202126503
Loss made of: CE 0.4267547130584717, LKD 5.072083950042725, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/105, Loss=4.712519973516464
Loss made of: CE 0.41948240995407104, LKD 4.327760219573975, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/105, Loss=4.480062910914421
Loss made of: CE 0.40065401792526245, LKD 4.034210205078125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/105, Loss=4.735553404688835
Loss made of: CE 0.4987853169441223, LKD 3.9009718894958496, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/105, Loss=4.559560710191727
Loss made of: CE 0.47357019782066345, LKD 4.713377952575684, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.701756635308266
Loss made of: CE 0.5130376815795898, LKD 5.416224002838135, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.441178652644157
Loss made of: CE 0.4301878809928894, LKD 3.6306240558624268, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/105, Loss=4.681319761276245
Loss made of: CE 0.44800886511802673, LKD 3.907254219055176, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/105, Loss=4.789857900142669
Loss made of: CE 0.4128058850765228, LKD 4.502621173858643, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/105, Loss=4.730893588066101
Loss made of: CE 0.4291786551475525, LKD 4.1906561851501465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4259602427482605, Reg Loss=4.235483646392822
Clinet index 16, End of Epoch 3/6, Average Loss=4.661443710327148, Class Loss=0.4259602427482605, Reg Loss=4.235483646392822
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/105, Loss=4.650324338674546
Loss made of: CE 0.3369857966899872, LKD 3.5698280334472656, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/105, Loss=4.731832504272461
Loss made of: CE 0.46721911430358887, LKD 4.49522066116333, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/105, Loss=4.821725830435753
Loss made of: CE 0.4024966359138489, LKD 4.002391338348389, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/105, Loss=4.71758259832859
Loss made of: CE 0.32404157519340515, LKD 4.034142017364502, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/105, Loss=4.7245924234390255
Loss made of: CE 0.4392992854118347, LKD 4.688612461090088, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/105, Loss=4.8214958786964415
Loss made of: CE 0.45755764842033386, LKD 4.7921061515808105, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/105, Loss=4.548758095502853
Loss made of: CE 0.34058260917663574, LKD 3.8317770957946777, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.5848254561424255
Loss made of: CE 0.3428367078304291, LKD 3.313896894454956, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.321379175782203
Loss made of: CE 0.428303599357605, LKD 3.3188588619232178, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/105, Loss=4.68560616672039
Loss made of: CE 0.4087384343147278, LKD 4.738408088684082, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4134330153465271, Reg Loss=4.250418663024902
Clinet index 16, End of Epoch 4/6, Average Loss=4.663851737976074, Class Loss=0.4134330153465271, Reg Loss=4.250418663024902
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/105, Loss=4.8037667840719225
Loss made of: CE 0.4928845763206482, LKD 4.296359539031982, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/105, Loss=4.514016914367676
Loss made of: CE 0.3716530501842499, LKD 3.91083025932312, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/105, Loss=4.343442457914352
Loss made of: CE 0.3610536456108093, LKD 3.6484203338623047, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/105, Loss=4.5855690240859985
Loss made of: CE 0.4189624786376953, LKD 4.388300895690918, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/105, Loss=4.760982051491737
Loss made of: CE 0.4426738917827606, LKD 4.233917713165283, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.630913400650025
Loss made of: CE 0.39386439323425293, LKD 4.111328601837158, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/105, Loss=4.68519237935543
Loss made of: CE 0.38389039039611816, LKD 4.6453704833984375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/105, Loss=4.462978705763817
Loss made of: CE 0.37375640869140625, LKD 4.294614315032959, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/105, Loss=5.113681012392044
Loss made of: CE 0.46885573863983154, LKD 4.755843639373779, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.653291451931
Loss made of: CE 0.4115857481956482, LKD 3.641721725463867, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4097197949886322, Reg Loss=4.246596813201904
Clinet index 16, End of Epoch 5/6, Average Loss=4.656316757202148, Class Loss=0.4097197949886322, Reg Loss=4.246596813201904
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/105, Loss=4.874232491850853
Loss made of: CE 0.384929895401001, LKD 4.4893798828125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/105, Loss=4.72157596051693
Loss made of: CE 0.4857895076274872, LKD 4.996765613555908, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/105, Loss=4.881544968485832
Loss made of: CE 0.4094620943069458, LKD 4.474337100982666, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.69138979613781
Loss made of: CE 0.38205868005752563, LKD 4.343850135803223, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.590447077155114
Loss made of: CE 0.3811895251274109, LKD 4.207783222198486, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/105, Loss=4.60619076192379
Loss made of: CE 0.39928025007247925, LKD 5.223153114318848, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/105, Loss=4.6406793802976605
Loss made of: CE 0.38637208938598633, LKD 4.979035377502441, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/105, Loss=4.363266164064408
Loss made of: CE 0.4046160578727722, LKD 3.3013534545898438, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/105, Loss=4.286324658989907
Loss made of: CE 0.3143739104270935, LKD 4.193885326385498, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/105, Loss=4.320418655872345
Loss made of: CE 0.3832734227180481, LKD 4.564592361450195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3986334502696991, Reg Loss=4.208319187164307
Clinet index 16, End of Epoch 6/6, Average Loss=4.606952667236328, Class Loss=0.3986334502696991, Reg Loss=4.208319187164307
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=5.193325293064118
Loss made of: CE 0.5488959550857544, LKD 4.321878433227539, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.869031515717507
Loss made of: CE 0.4828917980194092, LKD 4.936080455780029, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=5.069654792547226
Loss made of: CE 0.4477430284023285, LKD 4.981931209564209, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.8967036545276645
Loss made of: CE 0.4741763472557068, LKD 4.496341705322266, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.999081611633301
Loss made of: CE 0.4190743565559387, LKD 4.445708751678467, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.933940142393112
Loss made of: CE 0.5339162945747375, LKD 4.577646255493164, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.6501855790615085
Loss made of: CE 0.49480700492858887, LKD 4.008504867553711, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.9328880399465564
Loss made of: CE 0.42608755826950073, LKD 3.956146478652954, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=5.009676316380501
Loss made of: CE 0.44381827116012573, LKD 5.33128023147583, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.899920979142189
Loss made of: CE 0.4376771152019501, LKD 5.184993267059326, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.47801029682159424, Reg Loss=4.469953536987305
Clinet index 18, End of Epoch 1/6, Average Loss=4.947963714599609, Class Loss=0.47801029682159424, Reg Loss=4.469953536987305
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/102, Loss=4.8653121292591095
Loss made of: CE 0.40645116567611694, LKD 4.758421897888184, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=5.022063726186753
Loss made of: CE 0.511037290096283, LKD 5.465357780456543, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.944504716992379
Loss made of: CE 0.47863802313804626, LKD 4.179136276245117, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.688861322402954
Loss made of: CE 0.4325661063194275, LKD 4.049681663513184, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.9941240578889845
Loss made of: CE 0.5306776762008667, LKD 4.886586666107178, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=5.084654152393341
Loss made of: CE 0.4835291802883148, LKD 5.939768314361572, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.5937119901180266
Loss made of: CE 0.3718014359474182, LKD 4.015888214111328, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.83793418109417
Loss made of: CE 0.5310342907905579, LKD 4.871811866760254, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.595210355520249
Loss made of: CE 0.4944390654563904, LKD 4.312250137329102, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.901485922932625
Loss made of: CE 0.4519558846950531, LKD 4.268867492675781, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4296562075614929, Reg Loss=4.419554710388184
Clinet index 18, End of Epoch 2/6, Average Loss=4.849210739135742, Class Loss=0.4296562075614929, Reg Loss=4.419554710388184
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/102, Loss=4.6547336161136625
Loss made of: CE 0.3909582495689392, LKD 5.035395622253418, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.639835703372955
Loss made of: CE 0.40097469091415405, LKD 3.7466108798980713, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=5.117113411426544
Loss made of: CE 0.40425440669059753, LKD 3.5221669673919678, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=5.159940254688263
Loss made of: CE 0.43003472685813904, LKD 4.610632419586182, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.6562299728393555
Loss made of: CE 0.39547449350357056, LKD 4.222731113433838, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.702805855870247
Loss made of: CE 0.37042340636253357, LKD 4.039215564727783, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.67038801908493
Loss made of: CE 0.3897415101528168, LKD 5.058806419372559, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.674782195687294
Loss made of: CE 0.32453346252441406, LKD 3.4432358741760254, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=5.125782385468483
Loss made of: CE 0.38023099303245544, LKD 4.390528678894043, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.961471572518349
Loss made of: CE 0.42218905687332153, LKD 3.8417420387268066, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.41039296984672546, Reg Loss=4.440258502960205
Clinet index 18, End of Epoch 3/6, Average Loss=4.850651264190674, Class Loss=0.41039296984672546, Reg Loss=4.440258502960205
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/102, Loss=4.916463038325309
Loss made of: CE 0.4238528907299042, LKD 4.009424209594727, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.720586910843849
Loss made of: CE 0.4152197241783142, LKD 4.316173553466797, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.676125767827034
Loss made of: CE 0.31469303369522095, LKD 4.940906047821045, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=5.074418735504151
Loss made of: CE 0.47554290294647217, LKD 5.702224254608154, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.650104799866677
Loss made of: CE 0.41336125135421753, LKD 4.545482635498047, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.9465380132198336
Loss made of: CE 0.42607760429382324, LKD 4.628896713256836, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.690176799893379
Loss made of: CE 0.34779009222984314, LKD 3.6234960556030273, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.803882667422295
Loss made of: CE 0.33742886781692505, LKD 4.595260143280029, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.805770000815391
Loss made of: CE 0.35769447684288025, LKD 3.8392438888549805, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=5.1215489447116855
Loss made of: CE 0.42714545130729675, LKD 4.591462135314941, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3997636139392853, Reg Loss=4.431161403656006
Clinet index 18, End of Epoch 4/6, Average Loss=4.830924987792969, Class Loss=0.3997636139392853, Reg Loss=4.431161403656006
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/102, Loss=4.816546896100045
Loss made of: CE 0.38678091764450073, LKD 4.083214282989502, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.819901365041733
Loss made of: CE 0.36564403772354126, LKD 4.0252814292907715, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.80845143198967
Loss made of: CE 0.4272194504737854, LKD 4.559383869171143, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.74284436404705
Loss made of: CE 0.38967376947402954, LKD 4.49394416809082, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=5.049980857968331
Loss made of: CE 0.3767189383506775, LKD 4.020930290222168, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.848072502017021
Loss made of: CE 0.40815305709838867, LKD 3.9505205154418945, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.911852476000786
Loss made of: CE 0.38619107007980347, LKD 5.5923991203308105, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.781488925218582
Loss made of: CE 0.4459002912044525, LKD 4.494555950164795, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.511708867549896
Loss made of: CE 0.4563504159450531, LKD 4.354376792907715, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.81607109606266
Loss made of: CE 0.3765379786491394, LKD 4.32719612121582, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3942870497703552, Reg Loss=4.417963027954102
Clinet index 18, End of Epoch 5/6, Average Loss=4.812250137329102, Class Loss=0.3942870497703552, Reg Loss=4.417963027954102
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/102, Loss=4.424333474040031
Loss made of: CE 0.38868391513824463, LKD 5.146871089935303, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.746399801969528
Loss made of: CE 0.37682101130485535, LKD 3.915966272354126, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.8117501676082615
Loss made of: CE 0.29486536979675293, LKD 5.185575485229492, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.787615466117859
Loss made of: CE 0.3827929198741913, LKD 4.678857326507568, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.928441798686981
Loss made of: CE 0.33971163630485535, LKD 4.163360595703125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.882488331198692
Loss made of: CE 0.38511067628860474, LKD 4.523244380950928, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.769687107205391
Loss made of: CE 0.3685494363307953, LKD 4.121469020843506, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=5.1965752899646755
Loss made of: CE 0.4837205111980438, LKD 4.650424003601074, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.797196900844574
Loss made of: CE 0.45509618520736694, LKD 4.70469856262207, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.84723419547081
Loss made of: CE 0.3760092258453369, LKD 4.550760269165039, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3877487778663635, Reg Loss=4.428561687469482
Clinet index 18, End of Epoch 6/6, Average Loss=4.816310405731201, Class Loss=0.3877487778663635, Reg Loss=4.428561687469482
federated aggregation...
Validation, Class Loss=0.44149699807167053, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.858801
Mean Acc: 0.467254
FreqW Acc: 0.765241
Mean IoU: 0.375101
Class IoU:
	class 0: 0.89480203
	class 1: 0.055318195
	class 2: 0.00037608124
	class 3: 0.0
	class 4: 0.32767442
	class 5: 0.2828653
	class 6: 0.72258157
	class 7: 0.7434606
	class 8: 0.25227666
	class 9: 0.13999128
	class 10: 0.32165936
	class 11: 0.3970508
	class 12: 0.38944608
	class 13: 0.47345355
	class 14: 0.65331596
	class 15: 0.72240174
	class 16: 3.621387e-05
Class Acc:
	class 0: 0.9739925
	class 1: 0.055329982
	class 2: 0.0003762807
	class 3: 0.0
	class 4: 0.33339384
	class 5: 0.28444922
	class 6: 0.74564075
	class 7: 0.76008
	class 8: 0.25318444
	class 9: 0.15169501
	class 10: 0.4460563
	class 11: 0.55457085
	class 12: 0.8640401
	class 13: 0.7196141
	class 14: 0.9188689
	class 15: 0.88199246
	class 16: 3.621387e-05

federated global round: 17, step: 3
select part of clients to conduct local training
[5, 3, 17, 11]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=4.6312160551548
Loss made of: CE 0.43803107738494873, LKD 3.5130157470703125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.668605262041092
Loss made of: CE 0.3712361752986908, LKD 4.228553295135498, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.954575389623642
Loss made of: CE 0.4192056655883789, LKD 4.0114922523498535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.929798817634582
Loss made of: CE 0.4250915050506592, LKD 4.160289287567139, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.824606493115425
Loss made of: CE 0.4200022220611572, LKD 4.1282639503479, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.6523926764726635
Loss made of: CE 0.5420061945915222, LKD 4.850485324859619, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.582335677742958
Loss made of: CE 0.41407108306884766, LKD 4.347322940826416, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.996556678414345
Loss made of: CE 0.3851568102836609, LKD 3.507053852081299, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.668447726964951
Loss made of: CE 0.35198673605918884, LKD 3.737334966659546, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.832308861613273
Loss made of: CE 0.3607284724712372, LKD 4.789959907531738, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3919394910335541, Reg Loss=4.378281116485596
Clinet index 5, End of Epoch 1/6, Average Loss=4.770220756530762, Class Loss=0.3919394910335541, Reg Loss=4.378281116485596
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.771849489212036
Loss made of: CE 0.3753107190132141, LKD 4.172572135925293, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.957221487164498
Loss made of: CE 0.38177722692489624, LKD 4.25938081741333, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.740544667840004
Loss made of: CE 0.37977084517478943, LKD 5.2675604820251465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.536421981453896
Loss made of: CE 0.3335300087928772, LKD 4.312136650085449, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.7873294860124584
Loss made of: CE 0.3816055655479431, LKD 4.31063175201416, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.9560518026351925
Loss made of: CE 0.3439132869243622, LKD 3.985624074935913, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.446659874916077
Loss made of: CE 0.37874770164489746, LKD 3.961911678314209, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.847108152508736
Loss made of: CE 0.44518333673477173, LKD 5.023196220397949, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.715134635567665
Loss made of: CE 0.38595208525657654, LKD 4.386570453643799, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.543610650300979
Loss made of: CE 0.34343037009239197, LKD 5.090374946594238, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3848157525062561, Reg Loss=4.344629287719727
Clinet index 5, End of Epoch 2/6, Average Loss=4.729444980621338, Class Loss=0.3848157525062561, Reg Loss=4.344629287719727
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.75801012814045
Loss made of: CE 0.31549614667892456, LKD 4.098709583282471, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.691330036520958
Loss made of: CE 0.34349024295806885, LKD 4.0150628089904785, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.580548679828643
Loss made of: CE 0.3569483160972595, LKD 4.637508869171143, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.737503871321678
Loss made of: CE 0.31907057762145996, LKD 3.957374095916748, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.9994127482175825
Loss made of: CE 0.4123603105545044, LKD 4.661101818084717, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.77812080681324
Loss made of: CE 0.39431485533714294, LKD 5.651127338409424, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.620070883631707
Loss made of: CE 0.3903522193431854, LKD 5.3646039962768555, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.701233097910881
Loss made of: CE 0.41423487663269043, LKD 3.996600866317749, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.689650824666023
Loss made of: CE 0.30731216073036194, LKD 4.1097588539123535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.835869878530502
Loss made of: CE 0.34159597754478455, LKD 3.782463788986206, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3802221715450287, Reg Loss=4.346510887145996
Clinet index 5, End of Epoch 3/6, Average Loss=4.726733207702637, Class Loss=0.3802221715450287, Reg Loss=4.346510887145996
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.8171108841896055
Loss made of: CE 0.34773415327072144, LKD 4.237269401550293, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.788210192322731
Loss made of: CE 0.4088101387023926, LKD 4.34372091293335, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=5.0207791984081265
Loss made of: CE 0.3250485360622406, LKD 3.5619304180145264, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.917766264081001
Loss made of: CE 0.37809520959854126, LKD 4.146948337554932, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.454614743590355
Loss made of: CE 0.30460524559020996, LKD 4.966024875640869, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.579815673828125
Loss made of: CE 0.37272191047668457, LKD 4.422605991363525, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.534600293636322
Loss made of: CE 0.3164876103401184, LKD 3.3463492393493652, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.863112398982048
Loss made of: CE 0.3272642493247986, LKD 3.645658493041992, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.853222289681435
Loss made of: CE 0.40790697932243347, LKD 4.128414630889893, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.79933086335659
Loss made of: CE 0.4225463271141052, LKD 5.040279865264893, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3741969168186188, Reg Loss=4.376636028289795
Clinet index 5, End of Epoch 4/6, Average Loss=4.750833034515381, Class Loss=0.3741969168186188, Reg Loss=4.376636028289795
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.6673552006483074
Loss made of: CE 0.4196733832359314, LKD 4.930923938751221, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=5.072048020362854
Loss made of: CE 0.36029618978500366, LKD 4.363211154937744, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.765532726049424
Loss made of: CE 0.39481842517852783, LKD 3.908597707748413, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=5.005832600593567
Loss made of: CE 0.40198850631713867, LKD 4.862026691436768, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.734322240948677
Loss made of: CE 0.3818448483943939, LKD 4.463381767272949, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.949811068177223
Loss made of: CE 0.36117976903915405, LKD 4.677816390991211, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.534890568256378
Loss made of: CE 0.2989315986633301, LKD 3.7352607250213623, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.240174549818039
Loss made of: CE 0.3590429723262787, LKD 3.956476926803589, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.7613673388957976
Loss made of: CE 0.27769750356674194, LKD 4.798923015594482, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.703252694010734
Loss made of: CE 0.34600067138671875, LKD 3.9786128997802734, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3705105185508728, Reg Loss=4.359332084655762
Clinet index 5, End of Epoch 5/6, Average Loss=4.729842662811279, Class Loss=0.3705105185508728, Reg Loss=4.359332084655762
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.832206696271896
Loss made of: CE 0.3536617159843445, LKD 4.073784351348877, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.467687526345253
Loss made of: CE 0.35801154375076294, LKD 3.714898109436035, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.928140786290169
Loss made of: CE 0.40110906958580017, LKD 4.175919055938721, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.641660335659981
Loss made of: CE 0.3704262673854828, LKD 4.12756872177124, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.49652778506279
Loss made of: CE 0.4538353383541107, LKD 3.764676570892334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.516927835345268
Loss made of: CE 0.3497790992259979, LKD 3.9481565952301025, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.818605142831802
Loss made of: CE 0.42612096667289734, LKD 3.91314697265625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.503558942675591
Loss made of: CE 0.3598663806915283, LKD 3.7835452556610107, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.867227506637573
Loss made of: CE 0.29549625515937805, LKD 4.872626304626465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=5.01628255546093
Loss made of: CE 0.4325612187385559, LKD 4.388067245483398, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3615410625934601, Reg Loss=4.344247817993164
Clinet index 5, End of Epoch 6/6, Average Loss=4.705789089202881, Class Loss=0.3615410625934601, Reg Loss=4.344247817993164
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=4.83618665933609
Loss made of: CE 0.4606935381889343, LKD 4.304004669189453, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.736823552846909
Loss made of: CE 0.3991721570491791, LKD 4.652653217315674, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.689159965515136
Loss made of: CE 0.41531646251678467, LKD 4.783519268035889, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.670131972432136
Loss made of: CE 0.4123978018760681, LKD 4.067742824554443, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.687127417325973
Loss made of: CE 0.4005957543849945, LKD 4.86729097366333, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.698575127124786
Loss made of: CE 0.3523310124874115, LKD 4.713050842285156, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.639953389763832
Loss made of: CE 0.304570734500885, LKD 3.864072561264038, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.648460471630097
Loss made of: CE 0.48922544717788696, LKD 4.289941310882568, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.809443253278732
Loss made of: CE 0.427717924118042, LKD 4.765645980834961, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.779181057214737
Loss made of: CE 0.38491591811180115, LKD 4.171238899230957, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.38706499338150024, Reg Loss=4.346541404724121
Clinet index 3, End of Epoch 1/6, Average Loss=4.733606338500977, Class Loss=0.38706499338150024, Reg Loss=4.346541404724121
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.571805539727211
Loss made of: CE 0.4003143310546875, LKD 4.325339317321777, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=5.224706497788429
Loss made of: CE 0.41107267141342163, LKD 4.255576133728027, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.611173239350319
Loss made of: CE 0.3977218568325043, LKD 4.3612847328186035, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.7487434476614
Loss made of: CE 0.4034121036529541, LKD 4.319216251373291, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.90442880988121
Loss made of: CE 0.39441466331481934, LKD 4.123838901519775, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.5442359238862995
Loss made of: CE 0.36606287956237793, LKD 3.925750494003296, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.800937354564667
Loss made of: CE 0.32965484261512756, LKD 4.115311622619629, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.61394539475441
Loss made of: CE 0.3341754674911499, LKD 4.735413074493408, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.750035059452057
Loss made of: CE 0.3961406648159027, LKD 5.035861492156982, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.768142703175545
Loss made of: CE 0.307176798582077, LKD 3.939413547515869, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3831484317779541, Reg Loss=4.3616862297058105
Clinet index 3, End of Epoch 2/6, Average Loss=4.744834899902344, Class Loss=0.3831484317779541, Reg Loss=4.3616862297058105
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.96556248664856
Loss made of: CE 0.33912938833236694, LKD 4.0534467697143555, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=5.0448272079229355
Loss made of: CE 0.3457241952419281, LKD 4.175796985626221, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.30301167666912
Loss made of: CE 0.3497365415096283, LKD 3.506006956100464, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.658938235044479
Loss made of: CE 0.3425914943218231, LKD 4.121217250823975, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.901163616776467
Loss made of: CE 0.3392551839351654, LKD 3.583411931991577, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.9137147128582
Loss made of: CE 0.3784444332122803, LKD 4.5512871742248535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.556065511703491
Loss made of: CE 0.258880078792572, LKD 3.641706705093384, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.563177460432053
Loss made of: CE 0.3403388261795044, LKD 3.6756999492645264, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.723074966669083
Loss made of: CE 0.34611567854881287, LKD 5.5150041580200195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.560105034708977
Loss made of: CE 0.44166406989097595, LKD 4.221432685852051, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3719160258769989, Reg Loss=4.353550910949707
Clinet index 3, End of Epoch 3/6, Average Loss=4.725466728210449, Class Loss=0.3719160258769989, Reg Loss=4.353550910949707
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.960309931635857
Loss made of: CE 0.37579578161239624, LKD 4.907127380371094, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.740474900603294
Loss made of: CE 0.33483678102493286, LKD 3.8049473762512207, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=5.0592329621315
Loss made of: CE 0.40810641646385193, LKD 5.729144096374512, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.689629977941513
Loss made of: CE 0.34932059049606323, LKD 4.846889972686768, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.808746880292892
Loss made of: CE 0.3223436772823334, LKD 4.464517593383789, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.829123988747597
Loss made of: CE 0.3979988694190979, LKD 5.170173645019531, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.452833688259124
Loss made of: CE 0.4845291078090668, LKD 4.819549083709717, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.419399613142014
Loss made of: CE 0.25774627923965454, LKD 4.303940296173096, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.499253296852112
Loss made of: CE 0.32193446159362793, LKD 4.406918048858643, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.757364550232888
Loss made of: CE 0.38053035736083984, LKD 4.867743492126465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.36910706758499146, Reg Loss=4.352711200714111
Clinet index 3, End of Epoch 4/6, Average Loss=4.721818447113037, Class Loss=0.36910706758499146, Reg Loss=4.352711200714111
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.578355973958969
Loss made of: CE 0.3870578408241272, LKD 3.9821338653564453, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.717589166760445
Loss made of: CE 0.3565301299095154, LKD 5.430050849914551, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.730110642313957
Loss made of: CE 0.42366763949394226, LKD 3.9311304092407227, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.921910390257835
Loss made of: CE 0.40721437335014343, LKD 4.714158058166504, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.838128089904785
Loss made of: CE 0.369903564453125, LKD 5.662112712860107, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.846164336800575
Loss made of: CE 0.32819443941116333, LKD 4.289402008056641, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.6548187047243115
Loss made of: CE 0.3338542878627777, LKD 3.9864773750305176, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.429209542274475
Loss made of: CE 0.3679192066192627, LKD 3.6577813625335693, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.873116466403007
Loss made of: CE 0.40609297156333923, LKD 4.687528133392334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.725423607230186
Loss made of: CE 0.3409247398376465, LKD 3.5704009532928467, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.36790698766708374, Reg Loss=4.366143703460693
Clinet index 3, End of Epoch 5/6, Average Loss=4.734050750732422, Class Loss=0.36790698766708374, Reg Loss=4.366143703460693
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.63563582599163
Loss made of: CE 0.3518463969230652, LKD 4.818390846252441, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.641119366884231
Loss made of: CE 0.34648334980010986, LKD 4.483603000640869, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.693077716231346
Loss made of: CE 0.28534936904907227, LKD 3.7061848640441895, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.530683422088623
Loss made of: CE 0.3322330713272095, LKD 3.9791505336761475, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.596976628899574
Loss made of: CE 0.3941684663295746, LKD 4.722795009613037, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.913581201434136
Loss made of: CE 0.31423407793045044, LKD 4.523194789886475, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.7834081768989565
Loss made of: CE 0.3885223865509033, LKD 4.890251159667969, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.638923418521881
Loss made of: CE 0.4344932436943054, LKD 4.229055404663086, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.6745439052581785
Loss made of: CE 0.38917410373687744, LKD 5.237074375152588, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.759908524155617
Loss made of: CE 0.304204523563385, LKD 3.791234016418457, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3585919439792633, Reg Loss=4.327449798583984
Clinet index 3, End of Epoch 6/6, Average Loss=4.686041831970215, Class Loss=0.3585919439792633, Reg Loss=4.327449798583984
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=5.5130243599414825
Loss made of: CE 0.7497892379760742, LKD 4.178643226623535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.220480632781983
Loss made of: CE 0.7484449148178101, LKD 4.632308483123779, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7647495269775391, Reg Loss=4.618732929229736
Clinet index 17, End of Epoch 1/6, Average Loss=5.383482456207275, Class Loss=0.7647495269775391, Reg Loss=4.618732929229736
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/23, Loss=5.106901702284813
Loss made of: CE 0.4495578408241272, LKD 4.2687296867370605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.885643431544304
Loss made of: CE 0.4377950131893158, LKD 4.903663635253906, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5136245489120483, Reg Loss=4.488337516784668
Clinet index 17, End of Epoch 2/6, Average Loss=5.001962184906006, Class Loss=0.5136245489120483, Reg Loss=4.488337516784668
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/23, Loss=4.913961592316627
Loss made of: CE 0.32268762588500977, LKD 3.824150562286377, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.965680918097496
Loss made of: CE 0.39606887102127075, LKD 4.861867427825928, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.43269798159599304, Reg Loss=4.463772773742676
Clinet index 17, End of Epoch 3/6, Average Loss=4.896470546722412, Class Loss=0.43269798159599304, Reg Loss=4.463772773742676
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/23, Loss=4.974789109826088
Loss made of: CE 0.43379998207092285, LKD 5.384428977966309, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.780209726095199
Loss made of: CE 0.3775114417076111, LKD 5.104537010192871, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.40734413266181946, Reg Loss=4.450578689575195
Clinet index 17, End of Epoch 4/6, Average Loss=4.8579230308532715, Class Loss=0.40734413266181946, Reg Loss=4.450578689575195
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/23, Loss=4.721384778618813
Loss made of: CE 0.4200701117515564, LKD 3.806114673614502, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.921683290600777
Loss made of: CE 0.3530186414718628, LKD 4.140766143798828, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.39234596490859985, Reg Loss=4.450026035308838
Clinet index 17, End of Epoch 5/6, Average Loss=4.842371940612793, Class Loss=0.39234596490859985, Reg Loss=4.450026035308838
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/23, Loss=4.929057139158249
Loss made of: CE 0.4237932562828064, LKD 3.953972101211548, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.693287190794945
Loss made of: CE 0.35989880561828613, LKD 3.9385550022125244, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.37972989678382874, Reg Loss=4.460370063781738
Clinet index 17, End of Epoch 6/6, Average Loss=4.840099811553955, Class Loss=0.37972989678382874, Reg Loss=4.460370063781738
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/102, Loss=4.325847351551056
Loss made of: CE 0.383888304233551, LKD 3.7037763595581055, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.98301177918911
Loss made of: CE 0.41063448786735535, LKD 4.010519504547119, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.638076081871986
Loss made of: CE 0.3933694362640381, LKD 4.536340236663818, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.970306208729744
Loss made of: CE 0.3600130081176758, LKD 4.556527614593506, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.890694904327392
Loss made of: CE 0.3209143877029419, LKD 3.875286102294922, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.720216912031174
Loss made of: CE 0.32317841053009033, LKD 3.7121169567108154, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.750199267268181
Loss made of: CE 0.3607843816280365, LKD 4.220217227935791, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.752028065919876
Loss made of: CE 0.3686719834804535, LKD 4.089604377746582, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.551428511738777
Loss made of: CE 0.3613209128379822, LKD 5.052181720733643, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.843184188008308
Loss made of: CE 0.39377808570861816, LKD 4.996310710906982, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.39102664589881897, Reg Loss=4.3564887046813965
Clinet index 11, End of Epoch 1/6, Average Loss=4.7475152015686035, Class Loss=0.39102664589881897, Reg Loss=4.3564887046813965
Pseudo labeling is: None
Epoch 2, lr = 0.000600
Epoch 2, Batch 10/102, Loss=4.645441791415214
Loss made of: CE 0.3012697994709015, LKD 3.972794771194458, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.432171973586082
Loss made of: CE 0.37600284814834595, LKD 3.591158151626587, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.966372340917587
Loss made of: CE 0.31982219219207764, LKD 4.800121307373047, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.696122273802757
Loss made of: CE 0.361143559217453, LKD 4.726993560791016, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.843858185410499
Loss made of: CE 0.4038565158843994, LKD 4.968015670776367, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.983403053879738
Loss made of: CE 0.4460556209087372, LKD 5.327085971832275, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.586429670453072
Loss made of: CE 0.4031749665737152, LKD 4.902898788452148, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=5.156209275126457
Loss made of: CE 0.48625463247299194, LKD 5.766462326049805, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.497528773546219
Loss made of: CE 0.4391326904296875, LKD 4.419277191162109, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.487147316336632
Loss made of: CE 0.32187244296073914, LKD 4.3129706382751465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.38302209973335266, Reg Loss=4.353918075561523
Clinet index 11, End of Epoch 2/6, Average Loss=4.736940383911133, Class Loss=0.38302209973335266, Reg Loss=4.353918075561523
Pseudo labeling is: None
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/102, Loss=4.694382750988007
Loss made of: CE 0.3345845639705658, LKD 4.79924201965332, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.584939175844193
Loss made of: CE 0.33365631103515625, LKD 4.382321357727051, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.601775559782982
Loss made of: CE 0.4212305545806885, LKD 4.630394458770752, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.765383046865463
Loss made of: CE 0.40178316831588745, LKD 4.693516254425049, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.946642383933067
Loss made of: CE 0.39175671339035034, LKD 4.8067121505737305, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.664664795994758
Loss made of: CE 0.40358996391296387, LKD 4.251530647277832, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.654710894823074
Loss made of: CE 0.32858654856681824, LKD 4.072089672088623, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.792123767733574
Loss made of: CE 0.3819029927253723, LKD 4.212100028991699, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.747951549291611
Loss made of: CE 0.2989983558654785, LKD 4.177628040313721, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=5.069113165140152
Loss made of: CE 0.35247835516929626, LKD 4.2330217361450195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.37854984402656555, Reg Loss=4.373779773712158
Clinet index 11, End of Epoch 3/6, Average Loss=4.7523298263549805, Class Loss=0.37854984402656555, Reg Loss=4.373779773712158
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/102, Loss=4.880311048030853
Loss made of: CE 0.3528535068035126, LKD 4.016417503356934, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.523809039592743
Loss made of: CE 0.3684391975402832, LKD 3.198946714401245, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.728163340687752
Loss made of: CE 0.4051447808742523, LKD 4.268088340759277, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.732598954439164
Loss made of: CE 0.34507688879966736, LKD 3.8813323974609375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.766345581412315
Loss made of: CE 0.37262842059135437, LKD 4.566368103027344, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.574197125434876
Loss made of: CE 0.3024333715438843, LKD 3.434154510498047, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.772494179010391
Loss made of: CE 0.4090118408203125, LKD 4.913001537322998, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.856913504004479
Loss made of: CE 0.4510476589202881, LKD 4.173276901245117, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.664314669370651
Loss made of: CE 0.3264356851577759, LKD 4.185176849365234, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.727410054206848
Loss made of: CE 0.39071500301361084, LKD 4.045337677001953, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3761502504348755, Reg Loss=4.361309051513672
Clinet index 11, End of Epoch 4/6, Average Loss=4.737459182739258, Class Loss=0.3761502504348755, Reg Loss=4.361309051513672
Pseudo labeling is: None
Epoch 5, lr = 0.000504
Epoch 5, Batch 10/102, Loss=4.858327504992485
Loss made of: CE 0.35922884941101074, LKD 5.080617427825928, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.837975186109543
Loss made of: CE 0.3932863175868988, LKD 4.689668655395508, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.836180344223976
Loss made of: CE 0.29729506373405457, LKD 3.894718647003174, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.729643362760544
Loss made of: CE 0.3619954288005829, LKD 4.331623554229736, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.798393243551255
Loss made of: CE 0.41753143072128296, LKD 4.579869747161865, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.917488783597946
Loss made of: CE 0.3655019700527191, LKD 5.557186603546143, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.307125997543335
Loss made of: CE 0.39327162504196167, LKD 4.7067694664001465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.3733238354325294
Loss made of: CE 0.24905912578105927, LKD 4.167446613311768, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.778921580314636
Loss made of: CE 0.44966086745262146, LKD 5.120059490203857, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.866503116488457
Loss made of: CE 0.42442449927330017, LKD 4.289915561676025, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3699485659599304, Reg Loss=4.372507095336914
Clinet index 11, End of Epoch 5/6, Average Loss=4.74245548248291, Class Loss=0.3699485659599304, Reg Loss=4.372507095336914
Pseudo labeling is: None
Epoch 6, lr = 0.000471
Epoch 6, Batch 10/102, Loss=4.552221468091011
Loss made of: CE 0.4084147810935974, LKD 3.863283157348633, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.712318137288094
Loss made of: CE 0.3342479467391968, LKD 5.0065202713012695, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.877962517738342
Loss made of: CE 0.3809971511363983, LKD 3.876077651977539, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.796882352232933
Loss made of: CE 0.3609563708305359, LKD 4.351228713989258, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.731929445266724
Loss made of: CE 0.3000643849372864, LKD 4.295454502105713, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.743797793984413
Loss made of: CE 0.3726930022239685, LKD 5.687211990356445, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.896384021639824
Loss made of: CE 0.38962072134017944, LKD 3.978076219558716, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.706801658868789
Loss made of: CE 0.34257304668426514, LKD 5.316864490509033, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.690460687875747
Loss made of: CE 0.36276358366012573, LKD 4.117724418640137, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.463877227902413
Loss made of: CE 0.3550107479095459, LKD 4.496266841888428, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.36945703625679016, Reg Loss=4.360152721405029
Clinet index 11, End of Epoch 6/6, Average Loss=4.729609966278076, Class Loss=0.36945703625679016, Reg Loss=4.360152721405029
federated aggregation...
Validation, Class Loss=0.43505334854125977, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.856765
Mean Acc: 0.458854
FreqW Acc: 0.763435
Mean IoU: 0.364956
Class IoU:
	class 0: 0.89511204
	class 1: 0.06916206
	class 2: 0.00059187267
	class 3: 0.0
	class 4: 0.33065864
	class 5: 0.28490078
	class 6: 0.694221
	class 7: 0.7440793
	class 8: 0.25752428
	class 9: 0.14691414
	class 10: 0.09427744
	class 11: 0.39305782
	class 12: 0.402942
	class 13: 0.4408213
	class 14: 0.6863941
	class 15: 0.7429989
	class 16: 0.02059568
Class Acc:
	class 0: 0.9747877
	class 1: 0.06917627
	class 2: 0.0005927317
	class 3: 0.0
	class 4: 0.33554074
	class 5: 0.2864522
	class 6: 0.7111475
	class 7: 0.76225823
	class 8: 0.25848156
	class 9: 0.16034113
	class 10: 0.12602659
	class 11: 0.55328786
	class 12: 0.88007104
	class 13: 0.8529757
	class 14: 0.9233514
	class 15: 0.88542163
	class 16: 0.02060005

federated global round: 18, step: 3
select part of clients to conduct local training
[9, 11, 13, 10]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=4.912114122509957
Loss made of: CE 0.3680124282836914, LKD 4.586291313171387, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.4729251056909565
Loss made of: CE 0.3849893808364868, LKD 4.698788642883301, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.8866585195064545
Loss made of: CE 0.3052240312099457, LKD 3.6432266235351562, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.658306404948235
Loss made of: CE 0.3858873248100281, LKD 4.231856346130371, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.00243308544159
Loss made of: CE 0.3794226050376892, LKD 4.664155006408691, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.781132355332375
Loss made of: CE 0.35205185413360596, LKD 4.3189005851745605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.649675107002258
Loss made of: CE 0.3409075438976288, LKD 4.008413314819336, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.675619426369667
Loss made of: CE 0.39725321531295776, LKD 5.04394006729126, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.875664415955543
Loss made of: CE 0.3657725751399994, LKD 4.730441570281982, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.5850354641675946
Loss made of: CE 0.34311193227767944, LKD 4.282879829406738, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.37338826060295105, Reg Loss=4.378445148468018
Clinet index 9, End of Epoch 1/6, Average Loss=4.751833438873291, Class Loss=0.37338826060295105, Reg Loss=4.378445148468018
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/102, Loss=4.633480513095856
Loss made of: CE 0.2840283513069153, LKD 4.216013431549072, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.703160387277603
Loss made of: CE 0.41086095571517944, LKD 4.478540420532227, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.9770237624645235
Loss made of: CE 0.4179355502128601, LKD 5.160146713256836, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.430649363994599
Loss made of: CE 0.3287261724472046, LKD 3.9216995239257812, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.784667640924454
Loss made of: CE 0.35705146193504333, LKD 4.435036659240723, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.9510288536548615
Loss made of: CE 0.4042125344276428, LKD 4.404974937438965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.973133987188339
Loss made of: CE 0.34586119651794434, LKD 4.2848124504089355, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.5479260921478275
Loss made of: CE 0.3284175395965576, LKD 4.247677326202393, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.727294272184372
Loss made of: CE 0.49059948325157166, LKD 4.606374263763428, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.487470132112503
Loss made of: CE 0.33547693490982056, LKD 3.6156744956970215, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3672752380371094, Reg Loss=4.363881587982178
Clinet index 9, End of Epoch 2/6, Average Loss=4.731156826019287, Class Loss=0.3672752380371094, Reg Loss=4.363881587982178
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/102, Loss=4.559400570392609
Loss made of: CE 0.382490336894989, LKD 4.331136703491211, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.966048368811608
Loss made of: CE 0.3711095452308655, LKD 3.781189441680908, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.908201339840889
Loss made of: CE 0.44618114829063416, LKD 5.676748752593994, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.661623218655587
Loss made of: CE 0.32992276549339294, LKD 4.216217041015625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.692000991106033
Loss made of: CE 0.39594465494155884, LKD 4.7759928703308105, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.82755753993988
Loss made of: CE 0.48830270767211914, LKD 4.305773735046387, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.657522314786911
Loss made of: CE 0.41396188735961914, LKD 4.105403423309326, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.571609425544739
Loss made of: CE 0.3472188413143158, LKD 4.262410640716553, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.811482524871826
Loss made of: CE 0.38648730516433716, LKD 4.1721086502075195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.73812518119812
Loss made of: CE 0.321355402469635, LKD 4.218838691711426, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.36376211047172546, Reg Loss=4.372874736785889
Clinet index 9, End of Epoch 3/6, Average Loss=4.736636638641357, Class Loss=0.36376211047172546, Reg Loss=4.372874736785889
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/102, Loss=4.760062712430954
Loss made of: CE 0.32783830165863037, LKD 3.7893998622894287, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.687301698327064
Loss made of: CE 0.35828644037246704, LKD 4.965124607086182, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.570956432819367
Loss made of: CE 0.34432530403137207, LKD 3.9179556369781494, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.839597430825234
Loss made of: CE 0.358316034078598, LKD 4.174245834350586, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.850728118419648
Loss made of: CE 0.35588574409484863, LKD 4.26248836517334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.893932256102562
Loss made of: CE 0.34340375661849976, LKD 4.35881233215332, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.528547331690788
Loss made of: CE 0.3363375663757324, LKD 4.529824256896973, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.876768720149994
Loss made of: CE 0.4107434153556824, LKD 4.905300617218018, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.9072395116090775
Loss made of: CE 0.37259888648986816, LKD 4.575455188751221, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.560315197706222
Loss made of: CE 0.35753509402275085, LKD 4.653306007385254, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.36040422320365906, Reg Loss=4.395795822143555
Clinet index 9, End of Epoch 4/6, Average Loss=4.756199836730957, Class Loss=0.36040422320365906, Reg Loss=4.395795822143555
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.50435653924942
Loss made of: CE 0.27163130044937134, LKD 3.4690446853637695, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.9017099916934965
Loss made of: CE 0.42761534452438354, LKD 4.060250282287598, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.770156905055046
Loss made of: CE 0.2993628680706024, LKD 3.8944900035858154, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.665745159983635
Loss made of: CE 0.3105553388595581, LKD 4.36532735824585, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.724161237478256
Loss made of: CE 0.3085464835166931, LKD 4.274225234985352, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.485391417145729
Loss made of: CE 0.4005407691001892, LKD 4.54310941696167, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.633851757645607
Loss made of: CE 0.40463966131210327, LKD 4.03577184677124, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.709841638803482
Loss made of: CE 0.25038838386535645, LKD 3.68003511428833, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.850337618589402
Loss made of: CE 0.39006102085113525, LKD 4.957575798034668, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=5.177849200367928
Loss made of: CE 0.4005797803401947, LKD 4.609738826751709, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.35728389024734497, Reg Loss=4.387795925140381
Clinet index 9, End of Epoch 5/6, Average Loss=4.74507999420166, Class Loss=0.35728389024734497, Reg Loss=4.387795925140381
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/102, Loss=4.724529558420182
Loss made of: CE 0.3552827835083008, LKD 4.565910816192627, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.834012991189956
Loss made of: CE 0.3058193325996399, LKD 4.238866329193115, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.851061388850212
Loss made of: CE 0.3765949010848999, LKD 4.37597131729126, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.640595456957817
Loss made of: CE 0.3061205744743347, LKD 5.334371566772461, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.653923758864403
Loss made of: CE 0.3934469223022461, LKD 4.727604389190674, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.555814883112907
Loss made of: CE 0.3517347276210785, LKD 4.4828081130981445, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.667213290929794
Loss made of: CE 0.36164888739585876, LKD 4.519991397857666, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.658514845371246
Loss made of: CE 0.4523340165615082, LKD 4.268129825592041, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.669300132989884
Loss made of: CE 0.28359806537628174, LKD 3.616576671600342, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.7727271854877475
Loss made of: CE 0.3645559251308441, LKD 4.595821380615234, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3492202162742615, Reg Loss=4.354384422302246
Clinet index 9, End of Epoch 6/6, Average Loss=4.703604698181152, Class Loss=0.3492202162742615, Reg Loss=4.354384422302246
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000438
Epoch 1, Batch 10/102, Loss=4.45330226123333
Loss made of: CE 0.357231467962265, LKD 3.9926230907440186, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.001682639122009
Loss made of: CE 0.36385905742645264, LKD 3.9929583072662354, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.654479891061783
Loss made of: CE 0.4005328118801117, LKD 4.430604934692383, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.894872775673866
Loss made of: CE 0.3580332398414612, LKD 4.278131008148193, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.809222006797791
Loss made of: CE 0.3128599524497986, LKD 3.705404758453369, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.688085520267487
Loss made of: CE 0.32177817821502686, LKD 3.6090810298919678, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.77190480530262
Loss made of: CE 0.34320348501205444, LKD 4.2836151123046875, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.74938490986824
Loss made of: CE 0.36410245299339294, LKD 4.09758996963501, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.49745235145092
Loss made of: CE 0.3546646535396576, LKD 4.9692535400390625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.910546237230301
Loss made of: CE 0.38315773010253906, LKD 5.39678955078125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.37877681851387024, Reg Loss=4.369899749755859
Clinet index 11, End of Epoch 1/6, Average Loss=4.748676776885986, Class Loss=0.37877681851387024, Reg Loss=4.369899749755859
Pseudo labeling is: None
Epoch 2, lr = 0.000405
Epoch 2, Batch 10/102, Loss=4.596456426382065
Loss made of: CE 0.33936500549316406, LKD 4.078904628753662, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.484072902798653
Loss made of: CE 0.4056512117385864, LKD 3.557027578353882, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.962130498886109
Loss made of: CE 0.321683406829834, LKD 5.119396209716797, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.770093324780464
Loss made of: CE 0.34576934576034546, LKD 4.669325828552246, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.820207309722901
Loss made of: CE 0.36511725187301636, LKD 5.009616374969482, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.99619545340538
Loss made of: CE 0.4187896251678467, LKD 5.057311058044434, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.603929445147514
Loss made of: CE 0.4000948667526245, LKD 5.587103843688965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=5.188948571681976
Loss made of: CE 0.46467718482017517, LKD 6.006699562072754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.524544250965119
Loss made of: CE 0.42193448543548584, LKD 4.643767356872559, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.503857919573784
Loss made of: CE 0.31281185150146484, LKD 4.353850841522217, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3698221743106842, Reg Loss=4.381302833557129
Clinet index 11, End of Epoch 2/6, Average Loss=4.751124858856201, Class Loss=0.3698221743106842, Reg Loss=4.381302833557129
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/102, Loss=4.72407266497612
Loss made of: CE 0.34114009141921997, LKD 4.8030266761779785, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.4845595329999925
Loss made of: CE 0.32228967547416687, LKD 4.057517051696777, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.5431900262832645
Loss made of: CE 0.4275810122489929, LKD 4.403216361999512, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.7199217736721035
Loss made of: CE 0.38460060954093933, LKD 4.863823413848877, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.898218968510628
Loss made of: CE 0.3805099129676819, LKD 4.727856636047363, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.7508848994970325
Loss made of: CE 0.4141771197319031, LKD 4.35399055480957, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.6922233432531355
Loss made of: CE 0.3180900812149048, LKD 4.205742359161377, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.766488227248192
Loss made of: CE 0.3688219487667084, LKD 4.162295818328857, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.807756853103638
Loss made of: CE 0.2728012800216675, LKD 4.03072452545166, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.975285613536835
Loss made of: CE 0.3719368875026703, LKD 4.403622150421143, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3650660514831543, Reg Loss=4.376367568969727
Clinet index 11, End of Epoch 3/6, Average Loss=4.741433620452881, Class Loss=0.3650660514831543, Reg Loss=4.376367568969727
Pseudo labeling is: None
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=4.780593028664589
Loss made of: CE 0.3383953273296356, LKD 3.7260007858276367, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.689456218481064
Loss made of: CE 0.3725768029689789, LKD 3.6019272804260254, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.706363958120346
Loss made of: CE 0.3677883744239807, LKD 4.206071853637695, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.729339596629143
Loss made of: CE 0.3490445017814636, LKD 3.8928699493408203, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.789211970567703
Loss made of: CE 0.4000292420387268, LKD 4.640780448913574, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.574777892231941
Loss made of: CE 0.31140783429145813, LKD 3.4634039402008057, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.693254172801971
Loss made of: CE 0.4145689606666565, LKD 5.372649192810059, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.747835457324982
Loss made of: CE 0.41457438468933105, LKD 4.291992664337158, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.666254818439484
Loss made of: CE 0.33101320266723633, LKD 4.176833629608154, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.660335180163384
Loss made of: CE 0.37775057554244995, LKD 4.1209025382995605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.36585137248039246, Reg Loss=4.351016044616699
Clinet index 11, End of Epoch 4/6, Average Loss=4.716867446899414, Class Loss=0.36585137248039246, Reg Loss=4.351016044616699
Pseudo labeling is: None
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/102, Loss=4.924460592865944
Loss made of: CE 0.3507334589958191, LKD 5.2243523597717285, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.843056085705757
Loss made of: CE 0.3725518584251404, LKD 4.763219356536865, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.816295781731606
Loss made of: CE 0.29571783542633057, LKD 3.818798780441284, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.635019689798355
Loss made of: CE 0.35867366194725037, LKD 4.493306636810303, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.778008005023002
Loss made of: CE 0.4384051263332367, LKD 4.552687644958496, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.888319185376167
Loss made of: CE 0.3464190363883972, LKD 5.577535629272461, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.2926247954368595
Loss made of: CE 0.35841482877731323, LKD 4.316520690917969, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.464056548476219
Loss made of: CE 0.2260720431804657, LKD 4.1870036125183105, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.626633206009865
Loss made of: CE 0.4471701979637146, LKD 4.909063816070557, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.714214929938317
Loss made of: CE 0.39947330951690674, LKD 4.437264442443848, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.35968977212905884, Reg Loss=4.348668575286865
Clinet index 11, End of Epoch 5/6, Average Loss=4.708358287811279, Class Loss=0.35968977212905884, Reg Loss=4.348668575286865
Pseudo labeling is: None
Epoch 6, lr = 0.000270
Epoch 6, Batch 10/102, Loss=4.532866576313973
Loss made of: CE 0.3723313808441162, LKD 3.7619194984436035, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.707297480106353
Loss made of: CE 0.3535033166408539, LKD 4.799156665802002, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.852878233790397
Loss made of: CE 0.39397701621055603, LKD 3.8455865383148193, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.760221561789512
Loss made of: CE 0.3710153102874756, LKD 3.9780967235565186, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.76862373650074
Loss made of: CE 0.29463279247283936, LKD 4.271666049957275, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.744985619187355
Loss made of: CE 0.374062180519104, LKD 5.659424304962158, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.832602456212044
Loss made of: CE 0.36659592390060425, LKD 3.925187587738037, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.630638554692268
Loss made of: CE 0.3453389108181, LKD 5.249610424041748, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.67323155105114
Loss made of: CE 0.37696218490600586, LKD 3.9702460765838623, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.404872304201126
Loss made of: CE 0.3518202304840088, LKD 4.538857936859131, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3619607090950012, Reg Loss=4.33725118637085
Clinet index 11, End of Epoch 6/6, Average Loss=4.699212074279785, Class Loss=0.3619607090950012, Reg Loss=4.33725118637085
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=5.509587448835373
Loss made of: CE 0.6973413825035095, LKD 4.353201389312744, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.0505017817020414
Loss made of: CE 0.5669583082199097, LKD 4.535574913024902, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6545220017433167, Reg Loss=4.607076644897461
Clinet index 13, End of Epoch 1/6, Average Loss=5.261598587036133, Class Loss=0.6545220017433167, Reg Loss=4.607076644897461
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/23, Loss=5.280227592587471
Loss made of: CE 0.470268189907074, LKD 4.803745269775391, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.960461956262589
Loss made of: CE 0.4494750499725342, LKD 5.027764320373535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.46507230401039124, Reg Loss=4.585206031799316
Clinet index 13, End of Epoch 2/6, Average Loss=5.050278186798096, Class Loss=0.46507230401039124, Reg Loss=4.585206031799316
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/23, Loss=4.992795518040657
Loss made of: CE 0.3793647289276123, LKD 5.013963222503662, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=5.034786820411682
Loss made of: CE 0.3681495189666748, LKD 4.691824436187744, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.40906745195388794, Reg Loss=4.572198390960693
Clinet index 13, End of Epoch 3/6, Average Loss=4.981266021728516, Class Loss=0.40906745195388794, Reg Loss=4.572198390960693
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/23, Loss=5.105782404541969
Loss made of: CE 0.42516398429870605, LKD 5.00718355178833, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.787283053994178
Loss made of: CE 0.33419522643089294, LKD 4.428247928619385, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3895247280597687, Reg Loss=4.540796279907227
Clinet index 13, End of Epoch 4/6, Average Loss=4.930321216583252, Class Loss=0.3895247280597687, Reg Loss=4.540796279907227
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/23, Loss=4.695193660259247
Loss made of: CE 0.38397902250289917, LKD 3.658580780029297, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=5.017941996455193
Loss made of: CE 0.39247533679008484, LKD 4.743264198303223, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3768419325351715, Reg Loss=4.535925388336182
Clinet index 13, End of Epoch 5/6, Average Loss=4.91276741027832, Class Loss=0.3768419325351715, Reg Loss=4.535925388336182
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/23, Loss=4.932032465934753
Loss made of: CE 0.37763214111328125, LKD 5.418928623199463, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.980860498547554
Loss made of: CE 0.3623145520687103, LKD 5.33097505569458, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.37939080595970154, Reg Loss=4.579281330108643
Clinet index 13, End of Epoch 6/6, Average Loss=4.958672046661377, Class Loss=0.37939080595970154, Reg Loss=4.579281330108643
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/23, Loss=6.074927878379822
Loss made of: CE 0.6564397811889648, LKD 5.101216793060303, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.709126409888268
Loss made of: CE 0.5499756336212158, LKD 4.783555030822754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6337948441505432, Reg Loss=5.229992389678955
Clinet index 10, End of Epoch 1/6, Average Loss=5.8637871742248535, Class Loss=0.6337948441505432, Reg Loss=5.229992389678955
Pseudo labeling is: None
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/23, Loss=5.89668992459774
Loss made of: CE 0.4767530858516693, LKD 6.594847679138184, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=5.596213245391846
Loss made of: CE 0.4570302665233612, LKD 6.461745262145996, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4518216550350189, Reg Loss=5.1911797523498535
Clinet index 10, End of Epoch 2/6, Average Loss=5.643001556396484, Class Loss=0.4518216550350189, Reg Loss=5.1911797523498535
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/23, Loss=5.3720976322889324
Loss made of: CE 0.3419884443283081, LKD 5.288455009460449, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=5.769779694080353
Loss made of: CE 0.29736360907554626, LKD 4.883461952209473, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3984319567680359, Reg Loss=5.175337314605713
Clinet index 10, End of Epoch 3/6, Average Loss=5.5737690925598145, Class Loss=0.3984319567680359, Reg Loss=5.175337314605713
Pseudo labeling is: None
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/23, Loss=5.491811326146125
Loss made of: CE 0.3915809094905853, LKD 4.651472091674805, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=5.6182790219783785
Loss made of: CE 0.3746015429496765, LKD 4.7661213874816895, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.38056889176368713, Reg Loss=5.163020610809326
Clinet index 10, End of Epoch 4/6, Average Loss=5.5435895919799805, Class Loss=0.38056889176368713, Reg Loss=5.163020610809326
Pseudo labeling is: None
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/23, Loss=5.612331268191338
Loss made of: CE 0.36049020290374756, LKD 5.942168712615967, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=5.555305019021034
Loss made of: CE 0.42963907122612, LKD 6.249535083770752, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.373927503824234, Reg Loss=5.195804595947266
Clinet index 10, End of Epoch 5/6, Average Loss=5.569732189178467, Class Loss=0.373927503824234, Reg Loss=5.195804595947266
Pseudo labeling is: None
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/23, Loss=5.5912638932466505
Loss made of: CE 0.39770182967185974, LKD 4.261883735656738, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=5.373578035831452
Loss made of: CE 0.4145471453666687, LKD 4.339411735534668, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3643273413181305, Reg Loss=5.128055572509766
Clinet index 10, End of Epoch 6/6, Average Loss=5.492383003234863, Class Loss=0.3643273413181305, Reg Loss=5.128055572509766
federated aggregation...
Validation, Class Loss=0.43612536787986755, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.856873
Mean Acc: 0.470854
FreqW Acc: 0.764552
Mean IoU: 0.374712
Class IoU:
	class 0: 0.89630467
	class 1: 0.06972389
	class 2: 0.0018362316
	class 3: 0.0
	class 4: 0.3080494
	class 5: 0.2832755
	class 6: 0.6641783
	class 7: 0.72593546
	class 8: 0.2574468
	class 9: 0.1456467
	class 10: 0.03238264
	class 11: 0.392245
	class 12: 0.40259796
	class 13: 0.41771483
	class 14: 0.6829805
	class 15: 0.75539535
	class 16: 0.3343918
Class Acc:
	class 0: 0.9743409
	class 1: 0.06973657
	class 2: 0.0018412669
	class 3: 0.0
	class 4: 0.31181908
	class 5: 0.28462413
	class 6: 0.67848074
	class 7: 0.7419821
	class 8: 0.25847536
	class 9: 0.15737797
	class 10: 0.04102169
	class 11: 0.534729
	class 12: 0.8839507
	class 13: 0.88500696
	class 14: 0.91954476
	class 15: 0.89346296
	class 16: 0.3681286

federated global round: 19, step: 3
select part of clients to conduct local training
[4, 7, 17, 15]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/105, Loss=4.675109741091728
Loss made of: CE 0.456608384847641, LKD 3.5201265811920166, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.769579538702965
Loss made of: CE 0.45824262499809265, LKD 3.5751547813415527, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/105, Loss=4.512803608179093
Loss made of: CE 0.38702356815338135, LKD 3.9042062759399414, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/105, Loss=4.704352411627769
Loss made of: CE 0.3439110517501831, LKD 4.428457260131836, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/105, Loss=4.765425452589989
Loss made of: CE 0.3931214213371277, LKD 4.066303253173828, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.634083798527717
Loss made of: CE 0.3948003053665161, LKD 3.589899778366089, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.765002050995827
Loss made of: CE 0.4200066030025482, LKD 4.404421806335449, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/105, Loss=4.516426125168801
Loss made of: CE 0.31697791814804077, LKD 3.922380208969116, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/105, Loss=4.654188531637192
Loss made of: CE 0.35126814246177673, LKD 3.7838168144226074, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.3578764498233795
Loss made of: CE 0.3505335748195648, LKD 3.6512181758880615, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.39426305890083313, Reg Loss=4.243195533752441
Clinet index 4, End of Epoch 1/6, Average Loss=4.637458801269531, Class Loss=0.39426305890083313, Reg Loss=4.243195533752441
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/105, Loss=4.5592868506908415
Loss made of: CE 0.36631548404693604, LKD 4.190018653869629, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.542468962073326
Loss made of: CE 0.3683900237083435, LKD 4.98969030380249, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.270764723420143
Loss made of: CE 0.3613619804382324, LKD 4.321261882781982, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.6969624102115635
Loss made of: CE 0.4110005795955658, LKD 4.5823140144348145, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.5639451265335085
Loss made of: CE 0.37043246626853943, LKD 4.174185752868652, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/105, Loss=4.66813947558403
Loss made of: CE 0.35936111211776733, LKD 3.935767650604248, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/105, Loss=4.8302186787128445
Loss made of: CE 0.33925527334213257, LKD 4.419444561004639, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/105, Loss=4.917704230546951
Loss made of: CE 0.3446834087371826, LKD 4.601902008056641, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/105, Loss=4.5098190069198605
Loss made of: CE 0.3517886996269226, LKD 3.580923318862915, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.457431256771088
Loss made of: CE 0.34592193365097046, LKD 3.8410186767578125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.377164363861084, Reg Loss=4.214108943939209
Clinet index 4, End of Epoch 2/6, Average Loss=4.591273307800293, Class Loss=0.377164363861084, Reg Loss=4.214108943939209
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/105, Loss=4.785195168852806
Loss made of: CE 0.429904043674469, LKD 3.9550156593322754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/105, Loss=4.749923437833786
Loss made of: CE 0.2955321669578552, LKD 4.513519763946533, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/105, Loss=4.448612490296364
Loss made of: CE 0.31928566098213196, LKD 3.6006088256835938, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/105, Loss=4.554159793257713
Loss made of: CE 0.33614426851272583, LKD 3.643617868423462, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/105, Loss=4.489048215746879
Loss made of: CE 0.3909728229045868, LKD 4.736231803894043, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.670727154612541
Loss made of: CE 0.34639471769332886, LKD 3.9575443267822266, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.583720731735229
Loss made of: CE 0.4024639129638672, LKD 5.705545425415039, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/105, Loss=4.440394642949104
Loss made of: CE 0.33905738592147827, LKD 4.452147960662842, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/105, Loss=4.65454929471016
Loss made of: CE 0.4044240415096283, LKD 4.523193359375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/105, Loss=4.628025677800179
Loss made of: CE 0.4042738676071167, LKD 4.957092761993408, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.37343740463256836, Reg Loss=4.2257161140441895
Clinet index 4, End of Epoch 3/6, Average Loss=4.599153518676758, Class Loss=0.37343740463256836, Reg Loss=4.2257161140441895
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/105, Loss=4.203436994552613
Loss made of: CE 0.30253857374191284, LKD 3.5595662593841553, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/105, Loss=4.654811930656433
Loss made of: CE 0.37298887968063354, LKD 3.444917917251587, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/105, Loss=4.767494478821755
Loss made of: CE 0.3062377870082855, LKD 4.649645805358887, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/105, Loss=4.632970783114433
Loss made of: CE 0.3903861939907074, LKD 4.765482425689697, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/105, Loss=4.285540509223938
Loss made of: CE 0.3294110894203186, LKD 4.309164524078369, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/105, Loss=4.52797380387783
Loss made of: CE 0.418403685092926, LKD 3.6627118587493896, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/105, Loss=4.550306206941604
Loss made of: CE 0.32462334632873535, LKD 4.752532958984375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.566559368371964
Loss made of: CE 0.4125633239746094, LKD 4.81663703918457, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.69203989803791
Loss made of: CE 0.37872475385665894, LKD 5.070782661437988, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/105, Loss=4.970040670037269
Loss made of: CE 0.4196600317955017, LKD 3.976677417755127, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3676716089248657, Reg Loss=4.2146196365356445
Clinet index 4, End of Epoch 4/6, Average Loss=4.582291126251221, Class Loss=0.3676716089248657, Reg Loss=4.2146196365356445
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/105, Loss=4.53865864276886
Loss made of: CE 0.4282653331756592, LKD 4.161863803863525, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/105, Loss=4.8257452815771105
Loss made of: CE 0.39867815375328064, LKD 4.616756916046143, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/105, Loss=4.639854514598847
Loss made of: CE 0.4101823568344116, LKD 4.737730026245117, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/105, Loss=4.789907330274582
Loss made of: CE 0.4247072637081146, LKD 3.945626735687256, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/105, Loss=4.418934682011605
Loss made of: CE 0.362476110458374, LKD 3.6269705295562744, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.286572107672692
Loss made of: CE 0.45628684759140015, LKD 3.4241952896118164, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/105, Loss=4.512246444821358
Loss made of: CE 0.3183794915676117, LKD 3.5838074684143066, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/105, Loss=4.677986258268357
Loss made of: CE 0.34889230132102966, LKD 3.5620827674865723, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/105, Loss=4.668244481086731
Loss made of: CE 0.44928401708602905, LKD 4.122635841369629, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.30128136575222
Loss made of: CE 0.3580450415611267, LKD 4.4049553871154785, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3705204427242279, Reg Loss=4.201501846313477
Clinet index 4, End of Epoch 5/6, Average Loss=4.572022438049316, Class Loss=0.3705204427242279, Reg Loss=4.201501846313477
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/105, Loss=4.428958761692047
Loss made of: CE 0.3801512122154236, LKD 3.268810272216797, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/105, Loss=4.707676029205322
Loss made of: CE 0.3565242290496826, LKD 4.35939359664917, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/105, Loss=4.395074054598808
Loss made of: CE 0.355726420879364, LKD 3.7990429401397705, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.683450177311897
Loss made of: CE 0.3196341097354889, LKD 4.022549152374268, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.327443489432335
Loss made of: CE 0.3632470369338989, LKD 4.002295970916748, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/105, Loss=4.680041670799255
Loss made of: CE 0.34912270307540894, LKD 4.501405239105225, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/105, Loss=4.6273639619350435
Loss made of: CE 0.35269078612327576, LKD 3.629526138305664, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/105, Loss=4.593115195631981
Loss made of: CE 0.38012975454330444, LKD 3.8304760456085205, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/105, Loss=4.542390298843384
Loss made of: CE 0.45173025131225586, LKD 4.010993003845215, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/105, Loss=4.633247655630112
Loss made of: CE 0.3495631217956543, LKD 3.7493393421173096, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.36404678225517273, Reg Loss=4.190659046173096
Clinet index 4, End of Epoch 6/6, Average Loss=4.554705619812012, Class Loss=0.36404678225517273, Reg Loss=4.190659046173096
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/102, Loss=4.828809368610382
Loss made of: CE 0.4054822325706482, LKD 5.249286651611328, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.503882038593292
Loss made of: CE 0.5040930509567261, LKD 4.47402811050415, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.7449727892875675
Loss made of: CE 0.38973313570022583, LKD 4.064436912536621, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.0650087833404545
Loss made of: CE 0.45612871646881104, LKD 5.346828460693359, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.15096101462841
Loss made of: CE 0.3894461989402771, LKD 4.617868900299072, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.6178485453128815
Loss made of: CE 0.37055012583732605, LKD 4.204525947570801, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.390434357523918
Loss made of: CE 0.3289530277252197, LKD 3.5949642658233643, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.697353708744049
Loss made of: CE 0.3460036516189575, LKD 5.35489559173584, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.88374450802803
Loss made of: CE 0.3757796287536621, LKD 4.2562336921691895, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.99016173183918
Loss made of: CE 0.40488430857658386, LKD 4.270476341247559, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3794269561767578, Reg Loss=4.409738063812256
Clinet index 7, End of Epoch 1/6, Average Loss=4.789165019989014, Class Loss=0.3794269561767578, Reg Loss=4.409738063812256
Pseudo labeling is: None
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/102, Loss=4.780124801397323
Loss made of: CE 0.3194185495376587, LKD 4.1260881423950195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.548899635672569
Loss made of: CE 0.3973425328731537, LKD 4.2210869789123535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.826949188113213
Loss made of: CE 0.3022554814815521, LKD 4.775336265563965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.7242344379425045
Loss made of: CE 0.34457486867904663, LKD 3.6678202152252197, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.884564244747162
Loss made of: CE 0.37403637170791626, LKD 3.6539926528930664, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.862099379301071
Loss made of: CE 0.3214316964149475, LKD 3.7612311840057373, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.631798401474953
Loss made of: CE 0.3581538200378418, LKD 3.936565637588501, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.728212586045265
Loss made of: CE 0.33892735838890076, LKD 4.117396354675293, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.789633199572563
Loss made of: CE 0.3541281819343567, LKD 3.9502639770507812, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.532317167520523
Loss made of: CE 0.3520713746547699, LKD 4.088221073150635, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.36591699719429016, Reg Loss=4.364126682281494
Clinet index 7, End of Epoch 2/6, Average Loss=4.730043888092041, Class Loss=0.36591699719429016, Reg Loss=4.364126682281494
Pseudo labeling is: None
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/102, Loss=4.972831109166146
Loss made of: CE 0.3616136908531189, LKD 4.488485336303711, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.530255016684532
Loss made of: CE 0.308182954788208, LKD 4.8505330085754395, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.742147275805474
Loss made of: CE 0.3435360789299011, LKD 4.446985721588135, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.7668962478637695
Loss made of: CE 0.3361430764198303, LKD 3.7412478923797607, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.780091872811317
Loss made of: CE 0.43562769889831543, LKD 4.039005756378174, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.77090739607811
Loss made of: CE 0.4476888179779053, LKD 4.395243167877197, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.903390747308731
Loss made of: CE 0.4314088225364685, LKD 5.362300395965576, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.886384266614914
Loss made of: CE 0.3356425166130066, LKD 5.533502578735352, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.88468474149704
Loss made of: CE 0.3616902828216553, LKD 4.753127574920654, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.520595687627792
Loss made of: CE 0.3653121590614319, LKD 3.589101791381836, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3630206286907196, Reg Loss=4.410401344299316
Clinet index 7, End of Epoch 3/6, Average Loss=4.773421764373779, Class Loss=0.3630206286907196, Reg Loss=4.410401344299316
Pseudo labeling is: None
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=4.454068824648857
Loss made of: CE 0.3492604196071625, LKD 3.980400323867798, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.867596369981766
Loss made of: CE 0.30568259954452515, LKD 5.11342716217041, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=5.100612282752991
Loss made of: CE 0.4359877109527588, LKD 4.969432353973389, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.641790175437928
Loss made of: CE 0.3259298801422119, LKD 3.9276175498962402, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.708498650789261
Loss made of: CE 0.3567008972167969, LKD 4.515425682067871, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.6882295340299605
Loss made of: CE 0.39091238379478455, LKD 4.313636779785156, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.696088436245918
Loss made of: CE 0.41655629873275757, LKD 4.507021427154541, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.751679840683937
Loss made of: CE 0.38495370745658875, LKD 4.5071234703063965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.693427848815918
Loss made of: CE 0.3837127089500427, LKD 3.7295806407928467, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.975543877482414
Loss made of: CE 0.40606626868247986, LKD 4.372960090637207, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.36452510952949524, Reg Loss=4.385694980621338
Clinet index 7, End of Epoch 4/6, Average Loss=4.75022029876709, Class Loss=0.36452510952949524, Reg Loss=4.385694980621338
Pseudo labeling is: None
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/102, Loss=4.913282400369644
Loss made of: CE 0.5267688035964966, LKD 4.613415718078613, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.736249056458473
Loss made of: CE 0.4767799973487854, LKD 4.355655193328857, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.559972807765007
Loss made of: CE 0.3430236577987671, LKD 4.486763000488281, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.847129118442536
Loss made of: CE 0.3251417875289917, LKD 4.521394729614258, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.527780556678772
Loss made of: CE 0.38259953260421753, LKD 4.812986373901367, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.705956521630287
Loss made of: CE 0.383107990026474, LKD 4.690981864929199, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.953754425048828
Loss made of: CE 0.2998335361480713, LKD 4.029330730438232, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.780407276749611
Loss made of: CE 0.37885889410972595, LKD 3.836332321166992, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.908922278881073
Loss made of: CE 0.361197829246521, LKD 4.736656188964844, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.629523581266403
Loss made of: CE 0.40902695059776306, LKD 4.16639518737793, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3628615736961365, Reg Loss=4.4021501541137695
Clinet index 7, End of Epoch 5/6, Average Loss=4.765011787414551, Class Loss=0.3628615736961365, Reg Loss=4.4021501541137695
Pseudo labeling is: None
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/102, Loss=4.858161449432373
Loss made of: CE 0.31860822439193726, LKD 4.247067451477051, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.741874516010284
Loss made of: CE 0.34553778171539307, LKD 4.803388595581055, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.923783680796623
Loss made of: CE 0.35415053367614746, LKD 4.707544326782227, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=5.077638846635819
Loss made of: CE 0.3534627854824066, LKD 4.0185346603393555, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.719838866591454
Loss made of: CE 0.3787151277065277, LKD 4.420563220977783, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.543296557664871
Loss made of: CE 0.4177836775779724, LKD 3.92261004447937, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.521792456507683
Loss made of: CE 0.3080359399318695, LKD 4.724658489227295, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.921628424525261
Loss made of: CE 0.39820587635040283, LKD 5.136293411254883, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.759858125448227
Loss made of: CE 0.3901940584182739, LKD 4.838404178619385, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.407499161362648
Loss made of: CE 0.37441039085388184, LKD 4.017162322998047, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.36048078536987305, Reg Loss=4.4019389152526855
Clinet index 7, End of Epoch 6/6, Average Loss=4.762419700622559, Class Loss=0.36048078536987305, Reg Loss=4.4019389152526855
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/23, Loss=4.959681275486946
Loss made of: CE 0.541438639163971, LKD 4.088600158691406, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.91214320063591
Loss made of: CE 0.5642430186271667, LKD 4.806449890136719, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5189073085784912, Reg Loss=4.46858549118042
Clinet index 17, End of Epoch 1/6, Average Loss=4.987492561340332, Class Loss=0.5189073085784912, Reg Loss=4.46858549118042
Pseudo labeling is: None
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/23, Loss=4.94260683953762
Loss made of: CE 0.3905487060546875, LKD 4.089538097381592, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.8000836104154585
Loss made of: CE 0.4294155538082123, LKD 5.027426242828369, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4366861581802368, Reg Loss=4.427209377288818
Clinet index 17, End of Epoch 2/6, Average Loss=4.863895416259766, Class Loss=0.4366861581802368, Reg Loss=4.427209377288818
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/23, Loss=4.827728572487831
Loss made of: CE 0.28523537516593933, LKD 3.8989391326904297, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.8861673951148985
Loss made of: CE 0.4091021418571472, LKD 4.77977180480957, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4027046263217926, Reg Loss=4.404078483581543
Clinet index 17, End of Epoch 3/6, Average Loss=4.806783199310303, Class Loss=0.4027046263217926, Reg Loss=4.404078483581543
Pseudo labeling is: None
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/23, Loss=4.91102804839611
Loss made of: CE 0.4424045979976654, LKD 5.189361572265625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.731943666934967
Loss made of: CE 0.3760986328125, LKD 4.7387003898620605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3985031843185425, Reg Loss=4.417673110961914
Clinet index 17, End of Epoch 4/6, Average Loss=4.816176414489746, Class Loss=0.3985031843185425, Reg Loss=4.417673110961914
Pseudo labeling is: None
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/23, Loss=4.692576080560684
Loss made of: CE 0.3979305028915405, LKD 3.8232531547546387, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.928268599510193
Loss made of: CE 0.3382507562637329, LKD 4.29286003112793, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3812633752822876, Reg Loss=4.445382595062256
Clinet index 17, End of Epoch 5/6, Average Loss=4.826645851135254, Class Loss=0.3812633752822876, Reg Loss=4.445382595062256
Pseudo labeling is: None
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/23, Loss=4.943149963021279
Loss made of: CE 0.39841097593307495, LKD 3.854566812515259, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.705738979578018
Loss made of: CE 0.362562894821167, LKD 4.143962383270264, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.383486270904541, Reg Loss=4.469001293182373
Clinet index 17, End of Epoch 6/6, Average Loss=4.852487564086914, Class Loss=0.383486270904541, Reg Loss=4.469001293182373
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/103, Loss=4.8277194678783415
Loss made of: CE 0.3525816798210144, LKD 4.562749862670898, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/103, Loss=4.7105779349803925
Loss made of: CE 0.38816580176353455, LKD 4.221621036529541, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/103, Loss=4.668092587590218
Loss made of: CE 0.3602724075317383, LKD 4.043971538543701, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/103, Loss=4.6295429289340975
Loss made of: CE 0.3640965223312378, LKD 4.085770130157471, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/103, Loss=4.502014207839966
Loss made of: CE 0.4921933114528656, LKD 4.347553730010986, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/103, Loss=4.638210242986679
Loss made of: CE 0.3251454532146454, LKD 4.009060382843018, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/103, Loss=4.860595405101776
Loss made of: CE 0.35274118185043335, LKD 4.19372034072876, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/103, Loss=4.52837986946106
Loss made of: CE 0.37329578399658203, LKD 3.361771583557129, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/103, Loss=4.476696640253067
Loss made of: CE 0.34906744956970215, LKD 4.618261337280273, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/103, Loss=4.537583562731743
Loss made of: CE 0.37836307287216187, LKD 4.671923637390137, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3803171217441559, Reg Loss=4.259819507598877
Clinet index 15, End of Epoch 1/6, Average Loss=4.64013671875, Class Loss=0.3803171217441559, Reg Loss=4.259819507598877
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/103, Loss=4.917753574252129
Loss made of: CE 0.306395947933197, LKD 4.215024948120117, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/103, Loss=4.66088596880436
Loss made of: CE 0.36747270822525024, LKD 4.5204668045043945, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/103, Loss=4.648669001460076
Loss made of: CE 0.390865296125412, LKD 3.853337049484253, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/103, Loss=4.451406234502793
Loss made of: CE 0.35346168279647827, LKD 4.007856369018555, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/103, Loss=4.585387852787972
Loss made of: CE 0.38534027338027954, LKD 4.277727127075195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/103, Loss=4.819832387566566
Loss made of: CE 0.36278319358825684, LKD 4.229804039001465, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/103, Loss=4.651639339327812
Loss made of: CE 0.31273865699768066, LKD 4.209853172302246, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/103, Loss=4.40143695473671
Loss made of: CE 0.3174293637275696, LKD 4.118199825286865, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/103, Loss=4.296203437447548
Loss made of: CE 0.2588984966278076, LKD 3.146865129470825, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/103, Loss=4.8192335903644565
Loss made of: CE 0.38631150126457214, LKD 4.194551467895508, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3681981861591339, Reg Loss=4.255108833312988
Clinet index 15, End of Epoch 2/6, Average Loss=4.623307228088379, Class Loss=0.3681981861591339, Reg Loss=4.255108833312988
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/103, Loss=4.762420371174812
Loss made of: CE 0.33439892530441284, LKD 4.144554615020752, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/103, Loss=4.667653322219849
Loss made of: CE 0.3891996145248413, LKD 4.323521137237549, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/103, Loss=4.5572115778923035
Loss made of: CE 0.5231472849845886, LKD 4.785699367523193, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/103, Loss=4.652489528059959
Loss made of: CE 0.4362286925315857, LKD 3.994523048400879, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/103, Loss=4.439781528711319
Loss made of: CE 0.3539976477622986, LKD 4.074358940124512, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/103, Loss=4.5546231150627134
Loss made of: CE 0.3755875825881958, LKD 4.717540740966797, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/103, Loss=4.523830044269562
Loss made of: CE 0.4206481873989105, LKD 3.8968098163604736, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/103, Loss=4.625448018312454
Loss made of: CE 0.29101845622062683, LKD 4.247941970825195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/103, Loss=4.757439759373665
Loss made of: CE 0.3536848723888397, LKD 4.191029071807861, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/103, Loss=4.493152767419815
Loss made of: CE 0.36115750670433044, LKD 4.803004741668701, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.36156129837036133, Reg Loss=4.2432990074157715
Clinet index 15, End of Epoch 3/6, Average Loss=4.604860305786133, Class Loss=0.36156129837036133, Reg Loss=4.2432990074157715
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/103, Loss=4.870041224360466
Loss made of: CE 0.38594645261764526, LKD 5.33912467956543, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/103, Loss=4.6511708229780195
Loss made of: CE 0.38660329580307007, LKD 4.252737045288086, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/103, Loss=4.543146792054176
Loss made of: CE 0.3608153760433197, LKD 4.282806396484375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/103, Loss=4.223884397745133
Loss made of: CE 0.3034561574459076, LKD 3.6855504512786865, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/103, Loss=4.503683638572693
Loss made of: CE 0.3506121337413788, LKD 4.439760684967041, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/103, Loss=4.87349289059639
Loss made of: CE 0.3391932249069214, LKD 4.052568435668945, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/103, Loss=4.6091844737529755
Loss made of: CE 0.3760228157043457, LKD 4.068338394165039, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/103, Loss=4.4205876111984255
Loss made of: CE 0.2698160409927368, LKD 3.3501486778259277, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/103, Loss=4.793274080753326
Loss made of: CE 0.4077557921409607, LKD 4.117954730987549, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/103, Loss=4.36515177488327
Loss made of: CE 0.3489091098308563, LKD 3.7076234817504883, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.35699281096458435, Reg Loss=4.222717761993408
Clinet index 15, End of Epoch 4/6, Average Loss=4.579710483551025, Class Loss=0.35699281096458435, Reg Loss=4.222717761993408
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/103, Loss=4.497595900297165
Loss made of: CE 0.29930633306503296, LKD 3.6147854328155518, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/103, Loss=4.619284555315971
Loss made of: CE 0.3796515464782715, LKD 3.858018398284912, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/103, Loss=4.47806841135025
Loss made of: CE 0.3301253318786621, LKD 4.063811779022217, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/103, Loss=4.52863931953907
Loss made of: CE 0.32416799664497375, LKD 3.8571290969848633, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/103, Loss=4.51807946562767
Loss made of: CE 0.2986590266227722, LKD 3.9687163829803467, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/103, Loss=4.554582896828651
Loss made of: CE 0.3528934419155121, LKD 4.313146591186523, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/103, Loss=4.790412449836731
Loss made of: CE 0.40646618604660034, LKD 4.494814395904541, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/103, Loss=4.706749486923218
Loss made of: CE 0.2925611436367035, LKD 3.9142065048217773, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/103, Loss=4.862531104683876
Loss made of: CE 0.3771737217903137, LKD 4.733648300170898, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/103, Loss=4.59509089589119
Loss made of: CE 0.32707834243774414, LKD 3.896526336669922, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.36052030324935913, Reg Loss=4.242188930511475
Clinet index 15, End of Epoch 5/6, Average Loss=4.6027092933654785, Class Loss=0.36052030324935913, Reg Loss=4.242188930511475
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/103, Loss=4.666581553220749
Loss made of: CE 0.2984554171562195, LKD 4.155942440032959, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/103, Loss=4.776608541607857
Loss made of: CE 0.34346267580986023, LKD 4.413539886474609, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/103, Loss=4.572528576850891
Loss made of: CE 0.37369829416275024, LKD 4.123543739318848, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/103, Loss=4.509873726963997
Loss made of: CE 0.40045052766799927, LKD 4.059992790222168, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/103, Loss=4.502467077970505
Loss made of: CE 0.2984623908996582, LKD 3.7288975715637207, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/103, Loss=4.415841522812843
Loss made of: CE 0.309847891330719, LKD 4.282688617706299, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/103, Loss=4.796554136276245
Loss made of: CE 0.3653087019920349, LKD 4.507598400115967, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/103, Loss=4.6382532000541685
Loss made of: CE 0.39903467893600464, LKD 3.9393014907836914, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/103, Loss=4.476291865110397
Loss made of: CE 0.45090755820274353, LKD 4.292575359344482, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/103, Loss=4.61317030787468
Loss made of: CE 0.3654143214225769, LKD 3.440410614013672, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.35340365767478943, Reg Loss=4.24445104598999
Clinet index 15, End of Epoch 6/6, Average Loss=4.5978546142578125, Class Loss=0.35340365767478943, Reg Loss=4.24445104598999
federated aggregation...
Validation, Class Loss=0.4277960956096649, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.859790
Mean Acc: 0.489871
FreqW Acc: 0.769671
Mean IoU: 0.390539
Class IoU:
	class 0: 0.89873374
	class 1: 0.07285642
	class 2: 7.7354256e-05
	class 3: 0.0
	class 4: 0.34034067
	class 5: 0.28245872
	class 6: 0.696553
	class 7: 0.74421495
	class 8: 0.25612175
	class 9: 0.1500592
	class 10: 0.07182917
	class 11: 0.39475742
	class 12: 0.4037951
	class 13: 0.43471664
	class 14: 0.71448636
	class 15: 0.75482696
	class 16: 0.42332903
Class Acc:
	class 0: 0.9725994
	class 1: 0.07286971
	class 2: 7.7406316e-05
	class 3: 0.0
	class 4: 0.3457415
	class 5: 0.28394753
	class 6: 0.7155389
	class 7: 0.76227766
	class 8: 0.2571459
	class 9: 0.1636373
	class 10: 0.09343772
	class 11: 0.5595635
	class 12: 0.8878961
	class 13: 0.87016064
	class 14: 0.9276747
	class 15: 0.89824325
	class 16: 0.5169883

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[19, 23, 1, 8]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=6.629575800895691
Loss made of: CE 1.7851980924606323, LKD 5.259056091308594, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.8789362668991085
Loss made of: CE 1.4052261114120483, LKD 4.159863471984863, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.5301978588104248, Reg Loss=4.631075859069824
Clinet index 19, End of Epoch 1/6, Average Loss=6.161273956298828, Class Loss=1.5301978588104248, Reg Loss=4.631075859069824
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=5.281603211164475
Loss made of: CE 0.766488254070282, LKD 4.682056427001953, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.4497950851917265
Loss made of: CE 0.7666652202606201, LKD 4.318899154663086, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.891349732875824, Reg Loss=4.469296455383301
Clinet index 19, End of Epoch 2/6, Average Loss=5.3606462478637695, Class Loss=0.891349732875824, Reg Loss=4.469296455383301
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=5.146063026785851
Loss made of: CE 0.4870389401912689, LKD 4.283447265625, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.792790287733078
Loss made of: CE 0.5502179861068726, LKD 4.4707560539245605, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5809293985366821, Reg Loss=4.396027088165283
Clinet index 19, End of Epoch 3/6, Average Loss=4.976956367492676, Class Loss=0.5809293985366821, Reg Loss=4.396027088165283
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=4.806872242689133
Loss made of: CE 0.5410484671592712, LKD 4.658034324645996, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.740153872966767
Loss made of: CE 0.37201762199401855, LKD 4.416201591491699, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5094365477561951, Reg Loss=4.409963130950928
Clinet index 19, End of Epoch 4/6, Average Loss=4.919399738311768, Class Loss=0.5094365477561951, Reg Loss=4.409963130950928
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=4.834099653363228
Loss made of: CE 0.5567358136177063, LKD 4.126469612121582, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.80148531794548
Loss made of: CE 0.5506755709648132, LKD 4.574548721313477, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4911947250366211, Reg Loss=4.428144454956055
Clinet index 19, End of Epoch 5/6, Average Loss=4.919339179992676, Class Loss=0.4911947250366211, Reg Loss=4.428144454956055
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=4.766916012763977
Loss made of: CE 0.4709661900997162, LKD 3.7305455207824707, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.00091498196125
Loss made of: CE 0.4747235178947449, LKD 4.044260025024414, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4885498285293579, Reg Loss=4.362475395202637
Clinet index 19, End of Epoch 6/6, Average Loss=4.851025104522705, Class Loss=0.4885498285293579, Reg Loss=4.362475395202637
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/21, Loss=5.648171937465667
Loss made of: CE 1.1514962911605835, LKD 4.153235912322998, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/21, Loss=5.479381793737412
Loss made of: CE 1.1161296367645264, LKD 3.459031820297241, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.2932637929916382, Reg Loss=4.240766525268555
Clinet index 23, End of Epoch 1/6, Average Loss=5.534030437469482, Class Loss=1.2932637929916382, Reg Loss=4.240766525268555
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/21, Loss=5.276216512918472
Loss made of: CE 1.0119788646697998, LKD 4.02435302734375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/21, Loss=4.717633402347564
Loss made of: CE 0.6803648471832275, LKD 3.68961763381958, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.9366939067840576, Reg Loss=4.060538291931152
Clinet index 23, End of Epoch 2/6, Average Loss=4.997232437133789, Class Loss=0.9366939067840576, Reg Loss=4.060538291931152
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/21, Loss=5.109233921766281
Loss made of: CE 0.6846594214439392, LKD 4.384950637817383, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/21, Loss=4.532189875841141
Loss made of: CE 0.7844799757003784, LKD 4.317624092102051, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6992841362953186, Reg Loss=4.119732856750488
Clinet index 23, End of Epoch 3/6, Average Loss=4.819016933441162, Class Loss=0.6992841362953186, Reg Loss=4.119732856750488
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/21, Loss=4.834073954820633
Loss made of: CE 0.6210377216339111, LKD 4.224504470825195, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.479127830266952
Loss made of: CE 0.5797275304794312, LKD 3.518895387649536, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5871706008911133, Reg Loss=4.088491916656494
Clinet index 23, End of Epoch 4/6, Average Loss=4.675662517547607, Class Loss=0.5871706008911133, Reg Loss=4.088491916656494
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/21, Loss=4.7302374869585035
Loss made of: CE 0.4605611264705658, LKD 4.588578701019287, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.476150506734848
Loss made of: CE 0.6938343048095703, LKD 3.1348538398742676, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5405225157737732, Reg Loss=4.075251579284668
Clinet index 23, End of Epoch 5/6, Average Loss=4.615774154663086, Class Loss=0.5405225157737732, Reg Loss=4.075251579284668
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/21, Loss=4.6764188557863235
Loss made of: CE 0.5027616024017334, LKD 3.718679666519165, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.492305770516396
Loss made of: CE 0.5132300853729248, LKD 4.443149089813232, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5133351683616638, Reg Loss=4.060969352722168
Clinet index 23, End of Epoch 6/6, Average Loss=4.574304580688477, Class Loss=0.5133351683616638, Reg Loss=4.060969352722168
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=6.671119523048401
Loss made of: CE 1.5626490116119385, LKD 6.037923336029053, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.2743401527404785, Reg Loss=5.2277021408081055
Clinet index 1, End of Epoch 1/6, Average Loss=6.502042293548584, Class Loss=1.2743401527404785, Reg Loss=5.2277021408081055
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=5.9226714968681335
Loss made of: CE 0.525653064250946, LKD 4.442525863647461, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.7210379838943481, Reg Loss=4.995877742767334
Clinet index 1, End of Epoch 2/6, Average Loss=5.716915607452393, Class Loss=0.7210379838943481, Reg Loss=4.995877742767334
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.399306419491768
Loss made of: CE 0.5082902312278748, LKD 5.736544609069824, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4166388213634491, Reg Loss=4.971279144287109
Clinet index 1, End of Epoch 3/6, Average Loss=5.387917995452881, Class Loss=0.4166388213634491, Reg Loss=4.971279144287109
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.408742134273052
Loss made of: CE 0.3885820806026459, LKD 4.704840183258057, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3188692629337311, Reg Loss=4.9872050285339355
Clinet index 1, End of Epoch 4/6, Average Loss=5.306074142456055, Class Loss=0.3188692629337311, Reg Loss=4.9872050285339355
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.175466275215149
Loss made of: CE 0.2845150828361511, LKD 4.946351051330566, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.28719812631607056, Reg Loss=4.937831401824951
Clinet index 1, End of Epoch 5/6, Average Loss=5.225029468536377, Class Loss=0.28719812631607056, Reg Loss=4.937831401824951
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.030806398391723
Loss made of: CE 0.27374327182769775, LKD 4.089237213134766, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2572617828845978, Reg Loss=4.8741888999938965
Clinet index 1, End of Epoch 6/6, Average Loss=5.131450653076172, Class Loss=0.2572617828845978, Reg Loss=4.8741888999938965
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=5.921362912654876
Loss made of: CE 1.7328026294708252, LKD 4.644445419311523, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.280440151691437
Loss made of: CE 1.3751662969589233, LKD 4.417843818664551, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.554673194885254, Reg Loss=3.983781576156616
Clinet index 8, End of Epoch 1/6, Average Loss=5.538455009460449, Class Loss=1.554673194885254, Reg Loss=3.983781576156616
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=5.1628819584846495
Loss made of: CE 1.1684821844100952, LKD 4.5653862953186035, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.000462764501572
Loss made of: CE 0.90904700756073, LKD 3.9909024238586426, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=1.1530834436416626, Reg Loss=3.836970806121826
Clinet index 8, End of Epoch 2/6, Average Loss=4.990054130554199, Class Loss=1.1530834436416626, Reg Loss=3.836970806121826
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=4.837706369161606
Loss made of: CE 0.9017932415008545, LKD 3.693133592605591, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.536459612846374
Loss made of: CE 0.8807499408721924, LKD 3.7829785346984863, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.8721287250518799, Reg Loss=3.789485454559326
Clinet index 8, End of Epoch 3/6, Average Loss=4.661614418029785, Class Loss=0.8721287250518799, Reg Loss=3.789485454559326
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=4.232240772247314
Loss made of: CE 0.7092377543449402, LKD 3.4360415935516357, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.688308721780777
Loss made of: CE 0.8353769183158875, LKD 3.9429728984832764, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.7272412180900574, Reg Loss=3.800874710083008
Clinet index 8, End of Epoch 4/6, Average Loss=4.528115749359131, Class Loss=0.7272412180900574, Reg Loss=3.800874710083008
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=4.399267190694809
Loss made of: CE 0.7743921279907227, LKD 3.848060131072998, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.4573572218418125
Loss made of: CE 0.6850892901420593, LKD 3.7735137939453125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.6965696215629578, Reg Loss=3.7643089294433594
Clinet index 8, End of Epoch 5/6, Average Loss=4.460878372192383, Class Loss=0.6965696215629578, Reg Loss=3.7643089294433594
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=4.466419792175293
Loss made of: CE 0.679388701915741, LKD 4.285975456237793, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.3805404782295225
Loss made of: CE 0.6997793912887573, LKD 3.4077420234680176, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.6703655123710632, Reg Loss=3.8069703578948975
Clinet index 8, End of Epoch 6/6, Average Loss=4.4773359298706055, Class Loss=0.6703655123710632, Reg Loss=3.8069703578948975
federated aggregation...
Validation, Class Loss=0.5867162346839905, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.812974
Mean Acc: 0.333319
FreqW Acc: 0.694191
Mean IoU: 0.259220
Class IoU:
	class 0: 0.8575383
	class 1: 0.0011656728
	class 2: 8.743526e-05
	class 3: 0.0
	class 4: 0.12769264
	class 5: 0.018396616
	class 6: 0.24233244
	class 7: 0.56341106
	class 8: 0.22976117
	class 9: 0.08105203
	class 10: 0.047745418
	class 11: 0.37243405
	class 12: 0.39466634
	class 13: 0.41314176
	class 14: 0.72285944
	class 15: 0.73625857
	class 16: 0.3073583
	class 17: 0.090805314
	class 18: 0.014032552
	class 19: 0.21574947
	class 20: 0.0071237236
Class Acc:
	class 0: 0.9873236
	class 1: 0.0011656728
	class 2: 8.7440465e-05
	class 3: 0.0
	class 4: 0.12834853
	class 5: 0.018396616
	class 6: 0.24371065
	class 7: 0.5675238
	class 8: 0.23037107
	class 9: 0.083788835
	class 10: 0.052081496
	class 11: 0.41555357
	class 12: 0.85978067
	class 13: 0.8368352
	class 14: 0.820949
	class 15: 0.8098221
	class 16: 0.3221285
	class 17: 0.12162024
	class 18: 0.0145096015
	class 19: 0.47855803
	class 20: 0.0071455087

federated global round: 21, step: 4
select part of clients to conduct local training
[23, 14, 4, 11]
Current Client Index:  23
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/21, Loss=4.81233959197998
Loss made of: CE 0.8047107458114624, LKD 3.8436391353607178, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/21, Loss=4.8748819589614865
Loss made of: CE 0.6996316909790039, LKD 3.451357841491699, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.754215657711029, Reg Loss=4.077385902404785
Clinet index 23, End of Epoch 1/6, Average Loss=4.831601619720459, Class Loss=0.754215657711029, Reg Loss=4.077385902404785
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/21, Loss=4.774895316362381
Loss made of: CE 0.6734570860862732, LKD 3.8979783058166504, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/21, Loss=4.355879992246628
Loss made of: CE 0.4944540858268738, LKD 3.523761034011841, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5943813323974609, Reg Loss=3.97151780128479
Clinet index 23, End of Epoch 2/6, Average Loss=4.565898895263672, Class Loss=0.5943813323974609, Reg Loss=3.97151780128479
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/21, Loss=4.918961983919144
Loss made of: CE 0.5342411994934082, LKD 4.567637920379639, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/21, Loss=4.370255959033966
Loss made of: CE 0.612237274646759, LKD 4.426011562347412, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5294069051742554, Reg Loss=4.1253156661987305
Clinet index 23, End of Epoch 3/6, Average Loss=4.654722690582275, Class Loss=0.5294069051742554, Reg Loss=4.1253156661987305
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/21, Loss=4.6604224979877475
Loss made of: CE 0.4920734763145447, LKD 4.207043170928955, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.349357950687408
Loss made of: CE 0.49767106771469116, LKD 3.4753336906433105, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4915352463722229, Reg Loss=4.042552471160889
Clinet index 23, End of Epoch 4/6, Average Loss=4.534087657928467, Class Loss=0.4915352463722229, Reg Loss=4.042552471160889
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=4.619901859760285
Loss made of: CE 0.41601523756980896, LKD 4.547911167144775, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.52065317928791
Loss made of: CE 0.5538755655288696, LKD 3.1694092750549316, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4873847961425781, Reg Loss=4.092863082885742
Clinet index 23, End of Epoch 5/6, Average Loss=4.58024787902832, Class Loss=0.4873847961425781, Reg Loss=4.092863082885742
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/21, Loss=4.611199900507927
Loss made of: CE 0.4216456115245819, LKD 3.9063313007354736, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.377039358019829
Loss made of: CE 0.4588654935359955, LKD 4.498167037963867, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4604347050189972, Reg Loss=4.020267963409424
Clinet index 23, End of Epoch 6/6, Average Loss=4.480702877044678, Class Loss=0.4604347050189972, Reg Loss=4.020267963409424
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=5.479224395751953
Loss made of: CE 0.6783825755119324, LKD 4.536954402923584, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.095681965351105
Loss made of: CE 0.6015540957450867, LKD 4.132094383239746, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7379351854324341, Reg Loss=4.490191459655762
Clinet index 14, End of Epoch 1/6, Average Loss=5.228126525878906, Class Loss=0.7379351854324341, Reg Loss=4.490191459655762
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=5.232578927278519
Loss made of: CE 0.5902532339096069, LKD 5.1163177490234375, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=4.790614759922027
Loss made of: CE 0.38775381445884705, LKD 4.367652416229248, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5612873435020447, Reg Loss=4.454186916351318
Clinet index 14, End of Epoch 2/6, Average Loss=5.015474319458008, Class Loss=0.5612873435020447, Reg Loss=4.454186916351318
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=4.961427065730095
Loss made of: CE 0.6948332786560059, LKD 4.271327018737793, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.048411932587624
Loss made of: CE 0.45795607566833496, LKD 4.516615867614746, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.49812132120132446, Reg Loss=4.477406024932861
Clinet index 14, End of Epoch 3/6, Average Loss=4.975527286529541, Class Loss=0.49812132120132446, Reg Loss=4.477406024932861
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=4.774628841876984
Loss made of: CE 0.4291543662548065, LKD 4.75374698638916, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.155075952410698
Loss made of: CE 0.49045443534851074, LKD 4.761868000030518, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.45831096172332764, Reg Loss=4.451486110687256
Clinet index 14, End of Epoch 4/6, Average Loss=4.909797191619873, Class Loss=0.45831096172332764, Reg Loss=4.451486110687256
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=4.80272713303566
Loss made of: CE 0.4177236557006836, LKD 4.846040725708008, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=4.935409811139107
Loss made of: CE 0.3937399983406067, LKD 4.079304218292236, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4365542232990265, Reg Loss=4.429378032684326
Clinet index 14, End of Epoch 5/6, Average Loss=4.865932464599609, Class Loss=0.4365542232990265, Reg Loss=4.429378032684326
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=4.841993975639343
Loss made of: CE 0.4993963837623596, LKD 3.5805118083953857, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.908728247880935
Loss made of: CE 0.34401172399520874, LKD 4.182381629943848, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4244474768638611, Reg Loss=4.487814426422119
Clinet index 14, End of Epoch 6/6, Average Loss=4.912261962890625, Class Loss=0.4244474768638611, Reg Loss=4.487814426422119
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=5.296241915225982
Loss made of: CE 0.9004137516021729, LKD 5.103692531585693, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.296144193410873
Loss made of: CE 0.5832822322845459, LKD 4.295295238494873, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7321236729621887, Reg Loss=4.4780683517456055
Clinet index 4, End of Epoch 1/6, Average Loss=5.2101922035217285, Class Loss=0.7321236729621887, Reg Loss=4.4780683517456055
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=5.024580925703049
Loss made of: CE 0.7200068235397339, LKD 6.063586235046387, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=4.864454340934754
Loss made of: CE 0.5269299745559692, LKD 4.603755474090576, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5506882071495056, Reg Loss=4.44559383392334
Clinet index 4, End of Epoch 2/6, Average Loss=4.99628210067749, Class Loss=0.5506882071495056, Reg Loss=4.44559383392334
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=4.741663923859596
Loss made of: CE 0.45642420649528503, LKD 4.540665149688721, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=4.993541198968887
Loss made of: CE 0.5518457293510437, LKD 4.488600254058838, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4812033474445343, Reg Loss=4.396505832672119
Clinet index 4, End of Epoch 3/6, Average Loss=4.87770938873291, Class Loss=0.4812033474445343, Reg Loss=4.396505832672119
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.026831609010697
Loss made of: CE 0.4731920659542084, LKD 4.523629188537598, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=4.669065022468567
Loss made of: CE 0.5162612795829773, LKD 4.019075393676758, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.45106780529022217, Reg Loss=4.43635368347168
Clinet index 4, End of Epoch 4/6, Average Loss=4.887421607971191, Class Loss=0.45106780529022217, Reg Loss=4.43635368347168
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=4.9388992369174955
Loss made of: CE 0.4252663552761078, LKD 4.101634979248047, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=4.856443578004837
Loss made of: CE 0.4610230326652527, LKD 4.598661422729492, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.43781575560569763, Reg Loss=4.42987060546875
Clinet index 4, End of Epoch 5/6, Average Loss=4.8676862716674805, Class Loss=0.43781575560569763, Reg Loss=4.42987060546875
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=4.785505872964859
Loss made of: CE 0.4240407645702362, LKD 4.451607704162598, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.91341949403286
Loss made of: CE 0.3901779353618622, LKD 4.828636169433594, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.41708648204803467, Reg Loss=4.426654815673828
Clinet index 4, End of Epoch 6/6, Average Loss=4.843741416931152, Class Loss=0.41708648204803467, Reg Loss=4.426654815673828
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=4.9187939941883085
Loss made of: CE 1.0014489889144897, LKD 3.4691572189331055, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=4.880060893297196
Loss made of: CE 0.9140665531158447, LKD 3.891427516937256, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.9599243998527527, Reg Loss=3.8580665588378906
Clinet index 11, End of Epoch 1/6, Average Loss=4.817990779876709, Class Loss=0.9599243998527527, Reg Loss=3.8580665588378906
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/24, Loss=4.549339544773102
Loss made of: CE 0.8685523867607117, LKD 4.027115345001221, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=4.562447422742844
Loss made of: CE 0.716195285320282, LKD 4.110888481140137, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.7433077096939087, Reg Loss=3.8425204753875732
Clinet index 11, End of Epoch 2/6, Average Loss=4.5858283042907715, Class Loss=0.7433077096939087, Reg Loss=3.8425204753875732
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/24, Loss=4.358009767532349
Loss made of: CE 0.7699767351150513, LKD 4.039069652557373, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.4031691431999205
Loss made of: CE 0.6639504432678223, LKD 3.4715306758880615, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6840895414352417, Reg Loss=3.82417368888855
Clinet index 11, End of Epoch 3/6, Average Loss=4.508263111114502, Class Loss=0.6840895414352417, Reg Loss=3.82417368888855
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/24, Loss=4.3601106941699985
Loss made of: CE 0.6409278512001038, LKD 3.963146686553955, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.389409357309342
Loss made of: CE 0.7087491750717163, LKD 3.8195478916168213, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6615122556686401, Reg Loss=3.752549648284912
Clinet index 11, End of Epoch 4/6, Average Loss=4.414062023162842, Class Loss=0.6615122556686401, Reg Loss=3.752549648284912
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/24, Loss=4.498620277643203
Loss made of: CE 0.6547542214393616, LKD 3.5632741451263428, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.361385634541511
Loss made of: CE 0.5933033227920532, LKD 3.6021676063537598, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.6387536525726318, Reg Loss=3.778294324874878
Clinet index 11, End of Epoch 5/6, Average Loss=4.41704797744751, Class Loss=0.6387536525726318, Reg Loss=3.778294324874878
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/24, Loss=4.345936959981918
Loss made of: CE 0.7152301669120789, LKD 4.571697235107422, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.4196461081504825
Loss made of: CE 0.6900218725204468, LKD 4.268560409545898, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.6170294284820557, Reg Loss=3.752086639404297
Clinet index 11, End of Epoch 6/6, Average Loss=4.369115829467773, Class Loss=0.6170294284820557, Reg Loss=3.752086639404297
federated aggregation...
Validation, Class Loss=0.6296669840812683, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.813388
Mean Acc: 0.385633
FreqW Acc: 0.713920
Mean IoU: 0.268430
Class IoU:
	class 0: 0.8859134
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.0112765115
	class 5: 0.0
	class 6: 0.0008738587
	class 7: 0.30300304
	class 8: 0.20466365
	class 9: 0.09204531
	class 10: 0.045928046
	class 11: 0.39661306
	class 12: 0.46835732
	class 13: 0.44169474
	class 14: 0.7110563
	class 15: 0.7573281
	class 16: 0.31411144
	class 17: 0.28889576
	class 18: 0.27792773
	class 19: 0.24029382
	class 20: 0.19705364
Class Acc:
	class 0: 0.9752072
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.011305385
	class 5: 0.0
	class 6: 0.000873862
	class 7: 0.30360663
	class 8: 0.20487052
	class 9: 0.0953669
	class 10: 0.046392083
	class 11: 0.43782607
	class 12: 0.7967939
	class 13: 0.79543495
	class 14: 0.7631689
	class 15: 0.83227485
	class 16: 0.33456492
	class 17: 0.6885499
	class 18: 0.37274826
	class 19: 0.7471433
	class 20: 0.69216436

federated global round: 22, step: 4
select part of clients to conduct local training
[0, 8, 16, 14]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=4.478317600488663
Loss made of: CE 0.7951935529708862, LKD 4.184701442718506, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=4.473200577497482
Loss made of: CE 0.6686222553253174, LKD 3.529148817062378, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7244150042533875, Reg Loss=3.744011640548706
Clinet index 0, End of Epoch 1/6, Average Loss=4.468426704406738, Class Loss=0.7244150042533875, Reg Loss=3.744011640548706
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/24, Loss=4.359475314617157
Loss made of: CE 0.6527361273765564, LKD 3.4158170223236084, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=4.422030490636826
Loss made of: CE 0.6796457171440125, LKD 3.1440272331237793, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6420583724975586, Reg Loss=3.7555794715881348
Clinet index 0, End of Epoch 2/6, Average Loss=4.397637844085693, Class Loss=0.6420583724975586, Reg Loss=3.7555794715881348
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/24, Loss=4.432719910144806
Loss made of: CE 0.6267465353012085, LKD 4.379137992858887, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.216637617349624
Loss made of: CE 0.5562474131584167, LKD 3.842313289642334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6077735424041748, Reg Loss=3.7126693725585938
Clinet index 0, End of Epoch 3/6, Average Loss=4.320443153381348, Class Loss=0.6077735424041748, Reg Loss=3.7126693725585938
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/24, Loss=4.325582671165466
Loss made of: CE 0.6475361585617065, LKD 3.4535913467407227, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.252207514643669
Loss made of: CE 0.49824318289756775, LKD 3.488333225250244, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.598747968673706, Reg Loss=3.692317485809326
Clinet index 0, End of Epoch 4/6, Average Loss=4.291065216064453, Class Loss=0.598747968673706, Reg Loss=3.692317485809326
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/24, Loss=4.261405527591705
Loss made of: CE 0.531812310218811, LKD 3.808459997177124, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.493980377912521
Loss made of: CE 0.6601492166519165, LKD 3.964216709136963, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5799626111984253, Reg Loss=3.775324821472168
Clinet index 0, End of Epoch 5/6, Average Loss=4.355287551879883, Class Loss=0.5799626111984253, Reg Loss=3.775324821472168
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/24, Loss=4.176752433180809
Loss made of: CE 0.6299571394920349, LKD 3.617258310317993, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.365349823236466
Loss made of: CE 0.5492109060287476, LKD 3.480393171310425, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5780731439590454, Reg Loss=3.6955313682556152
Clinet index 0, End of Epoch 6/6, Average Loss=4.273604393005371, Class Loss=0.5780731439590454, Reg Loss=3.6955313682556152
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/24, Loss=4.711384630203247
Loss made of: CE 0.8915398120880127, LKD 4.60424280166626, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=4.4021543800830845
Loss made of: CE 0.7143672704696655, LKD 4.120984077453613, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7615510821342468, Reg Loss=3.804508686065674
Clinet index 8, End of Epoch 1/6, Average Loss=4.566059589385986, Class Loss=0.7615510821342468, Reg Loss=3.804508686065674
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/24, Loss=4.511255955696106
Loss made of: CE 0.6609009504318237, LKD 4.5746846199035645, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=4.503160429000855
Loss made of: CE 0.662108838558197, LKD 3.8619871139526367, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6869438886642456, Reg Loss=3.745147705078125
Clinet index 8, End of Epoch 2/6, Average Loss=4.43209171295166, Class Loss=0.6869438886642456, Reg Loss=3.745147705078125
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/24, Loss=4.451581841707229
Loss made of: CE 0.5510711669921875, LKD 3.796125650405884, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.2840836346149445
Loss made of: CE 0.7430347800254822, LKD 3.8333258628845215, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6392289400100708, Reg Loss=3.722882032394409
Clinet index 8, End of Epoch 3/6, Average Loss=4.3621110916137695, Class Loss=0.6392289400100708, Reg Loss=3.722882032394409
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/24, Loss=4.114968091249466
Loss made of: CE 0.6310280561447144, LKD 3.3974733352661133, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.462520676851272
Loss made of: CE 0.7202149629592896, LKD 4.3302741050720215, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6098448634147644, Reg Loss=3.763242483139038
Clinet index 8, End of Epoch 4/6, Average Loss=4.373087406158447, Class Loss=0.6098448634147644, Reg Loss=3.763242483139038
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/24, Loss=4.406641644239426
Loss made of: CE 0.6993646025657654, LKD 3.684931993484497, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.314646816253662
Loss made of: CE 0.6384281516075134, LKD 3.8580875396728516, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.6263434290885925, Reg Loss=3.7557430267333984
Clinet index 8, End of Epoch 5/6, Average Loss=4.382086277008057, Class Loss=0.6263434290885925, Reg Loss=3.7557430267333984
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/24, Loss=4.2798568427562715
Loss made of: CE 0.577926754951477, LKD 3.894571542739868, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.303863298892975
Loss made of: CE 0.6465126276016235, LKD 3.577326536178589, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.607271134853363, Reg Loss=3.763577461242676
Clinet index 8, End of Epoch 6/6, Average Loss=4.370848655700684, Class Loss=0.607271134853363, Reg Loss=3.763577461242676
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=5.814316374063492
Loss made of: CE 0.5724771022796631, LKD 4.991593360900879, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.571967601776123, Reg Loss=5.096977710723877
Clinet index 16, End of Epoch 1/6, Average Loss=5.6689453125, Class Loss=0.571967601776123, Reg Loss=5.096977710723877
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/19, Loss=5.3178935021162035
Loss made of: CE 0.3801817297935486, LKD 4.9920454025268555, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3652237057685852, Reg Loss=5.06613302230835
Clinet index 16, End of Epoch 2/6, Average Loss=5.431356906890869, Class Loss=0.3652237057685852, Reg Loss=5.06613302230835
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/19, Loss=5.304352477192879
Loss made of: CE 0.2753238379955292, LKD 4.762316703796387, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.30249881744384766, Reg Loss=4.93902063369751
Clinet index 16, End of Epoch 3/6, Average Loss=5.241519451141357, Class Loss=0.30249881744384766, Reg Loss=4.93902063369751
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/19, Loss=5.024537354707718
Loss made of: CE 0.28713592886924744, LKD 4.846358299255371, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2739689350128174, Reg Loss=4.898768424987793
Clinet index 16, End of Epoch 4/6, Average Loss=5.172737121582031, Class Loss=0.2739689350128174, Reg Loss=4.898768424987793
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/19, Loss=5.063793644309044
Loss made of: CE 0.2323496788740158, LKD 4.6288652420043945, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2856435775756836, Reg Loss=4.949405670166016
Clinet index 16, End of Epoch 5/6, Average Loss=5.235049247741699, Class Loss=0.2856435775756836, Reg Loss=4.949405670166016
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/19, Loss=5.266826391220093
Loss made of: CE 0.2419220507144928, LKD 4.406946659088135, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.26989778876304626, Reg Loss=4.9392290115356445
Clinet index 16, End of Epoch 6/6, Average Loss=5.209126949310303, Class Loss=0.26989778876304626, Reg Loss=4.9392290115356445
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/26, Loss=5.128114193677902
Loss made of: CE 0.4920370578765869, LKD 4.486755847930908, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=4.896253821253777
Loss made of: CE 0.5442681312561035, LKD 4.275601387023926, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5106333494186401, Reg Loss=4.473353862762451
Clinet index 14, End of Epoch 1/6, Average Loss=4.983987331390381, Class Loss=0.5106333494186401, Reg Loss=4.473353862762451
Pseudo labeling is: None
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/26, Loss=5.042425975203514
Loss made of: CE 0.5191232562065125, LKD 4.998653888702393, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=4.8138947010040285
Loss made of: CE 0.3166827857494354, LKD 4.849562168121338, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4545551538467407, Reg Loss=4.495772361755371
Clinet index 14, End of Epoch 2/6, Average Loss=4.950327396392822, Class Loss=0.4545551538467407, Reg Loss=4.495772361755371
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/26, Loss=4.8770286530256275
Loss made of: CE 0.6013921499252319, LKD 4.080512046813965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=4.8998799413442615
Loss made of: CE 0.385509729385376, LKD 4.494241714477539, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4380609095096588, Reg Loss=4.410314559936523
Clinet index 14, End of Epoch 3/6, Average Loss=4.84837532043457, Class Loss=0.4380609095096588, Reg Loss=4.410314559936523
Pseudo labeling is: None
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/26, Loss=4.739407989382744
Loss made of: CE 0.38235586881637573, LKD 4.771347999572754, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.025103744864464
Loss made of: CE 0.42849358916282654, LKD 4.66036319732666, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4099673926830292, Reg Loss=4.426442623138428
Clinet index 14, End of Epoch 4/6, Average Loss=4.836410045623779, Class Loss=0.4099673926830292, Reg Loss=4.426442623138428
Pseudo labeling is: None
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/26, Loss=4.797884440422058
Loss made of: CE 0.36325496435165405, LKD 4.818093299865723, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=4.998645687103272
Loss made of: CE 0.3855302333831787, LKD 4.238311767578125, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.41150230169296265, Reg Loss=4.467902660369873
Clinet index 14, End of Epoch 5/6, Average Loss=4.8794050216674805, Class Loss=0.41150230169296265, Reg Loss=4.467902660369873
Pseudo labeling is: None
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/26, Loss=4.736319142580032
Loss made of: CE 0.4981629252433777, LKD 3.5651028156280518, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.882354712486267
Loss made of: CE 0.3405446410179138, LKD 4.152632713317871, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.40193450450897217, Reg Loss=4.479827404022217
Clinet index 14, End of Epoch 6/6, Average Loss=4.8817620277404785, Class Loss=0.40193450450897217, Reg Loss=4.479827404022217
federated aggregation...
Validation, Class Loss=0.6389510631561279, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.811336
Mean Acc: 0.384609
FreqW Acc: 0.715772
Mean IoU: 0.262805
Class IoU:
	class 0: 0.89041376
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.0011634469
	class 5: 0.0
	class 6: 0.0
	class 7: 0.18978831
	class 8: 0.25361723
	class 9: 0.081119455
	class 10: 0.0071301637
	class 11: 0.35924911
	class 12: 0.47462747
	class 13: 0.45908043
	class 14: 0.66979444
	class 15: 0.76362246
	class 16: 0.25994834
	class 17: 0.2620871
	class 18: 0.29368496
	class 19: 0.21666484
	class 20: 0.3369134
Class Acc:
	class 0: 0.9716838
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.0011639987
	class 5: 0.0
	class 6: 0.0
	class 7: 0.18997937
	class 8: 0.25400683
	class 9: 0.08340116
	class 10: 0.007132577
	class 11: 0.38594353
	class 12: 0.753086
	class 13: 0.7339521
	class 14: 0.70755297
	class 15: 0.8393114
	class 16: 0.27284896
	class 17: 0.8282247
	class 18: 0.515052
	class 19: 0.81665546
	class 20: 0.7167882

federated global round: 23, step: 4
select part of clients to conduct local training
[12, 10, 9, 6]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=5.043436759710312
Loss made of: CE 0.47010159492492676, LKD 3.914090633392334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.19382846057415
Loss made of: CE 0.4800494611263275, LKD 4.257534503936768, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.47932666540145874, Reg Loss=4.60886812210083
Clinet index 12, End of Epoch 1/6, Average Loss=5.088194847106934, Class Loss=0.47932666540145874, Reg Loss=4.60886812210083
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/26, Loss=4.977864742279053
Loss made of: CE 0.45747631788253784, LKD 4.633272647857666, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=4.959373012185097
Loss made of: CE 0.462319940328598, LKD 3.98522686958313, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4271925091743469, Reg Loss=4.548253059387207
Clinet index 12, End of Epoch 2/6, Average Loss=4.975445747375488, Class Loss=0.4271925091743469, Reg Loss=4.548253059387207
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/26, Loss=5.059174916148185
Loss made of: CE 0.44622907042503357, LKD 4.502441883087158, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=4.949289599061013
Loss made of: CE 0.40398353338241577, LKD 4.580929756164551, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.40689998865127563, Reg Loss=4.575952053070068
Clinet index 12, End of Epoch 3/6, Average Loss=4.982851982116699, Class Loss=0.40689998865127563, Reg Loss=4.575952053070068
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/26, Loss=4.957984074950218
Loss made of: CE 0.4431367814540863, LKD 4.861944198608398, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=4.937607336044311
Loss made of: CE 0.39704567193984985, LKD 4.886030197143555, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3883725106716156, Reg Loss=4.552051067352295
Clinet index 12, End of Epoch 4/6, Average Loss=4.940423488616943, Class Loss=0.3883725106716156, Reg Loss=4.552051067352295
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/26, Loss=4.939416828751564
Loss made of: CE 0.3544327914714813, LKD 4.389981746673584, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=4.923897966742516
Loss made of: CE 0.3716740310192108, LKD 4.191511631011963, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.38689184188842773, Reg Loss=4.589998245239258
Clinet index 12, End of Epoch 5/6, Average Loss=4.9768900871276855, Class Loss=0.38689184188842773, Reg Loss=4.589998245239258
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/26, Loss=4.951539552211761
Loss made of: CE 0.30461248755455017, LKD 4.238767623901367, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.037050971388817
Loss made of: CE 0.43309730291366577, LKD 4.7262396812438965, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3793836534023285, Reg Loss=4.600367069244385
Clinet index 12, End of Epoch 6/6, Average Loss=4.979750633239746, Class Loss=0.3793836534023285, Reg Loss=4.600367069244385
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.945239114761352
Loss made of: CE 0.7824020385742188, LKD 4.209790229797363, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7931948304176331, Reg Loss=4.017333984375
Clinet index 10, End of Epoch 1/6, Average Loss=4.810528755187988, Class Loss=0.7931948304176331, Reg Loss=4.017333984375
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=4.509148299694061
Loss made of: CE 0.610655665397644, LKD 3.392186403274536, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6237518191337585, Reg Loss=3.957491874694824
Clinet index 10, End of Epoch 2/6, Average Loss=4.581243515014648, Class Loss=0.6237518191337585, Reg Loss=3.957491874694824
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=4.325878047943116
Loss made of: CE 0.6291519403457642, LKD 4.002721786499023, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5660750269889832, Reg Loss=3.9521877765655518
Clinet index 10, End of Epoch 3/6, Average Loss=4.51826286315918, Class Loss=0.5660750269889832, Reg Loss=3.9521877765655518
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=4.499440622329712
Loss made of: CE 0.5512897372245789, LKD 4.364689350128174, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.539699137210846, Reg Loss=3.8934502601623535
Clinet index 10, End of Epoch 4/6, Average Loss=4.433149337768555, Class Loss=0.539699137210846, Reg Loss=3.8934502601623535
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=4.416364780068397
Loss made of: CE 0.41649889945983887, LKD 3.8214609622955322, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5327302813529968, Reg Loss=3.896196126937866
Clinet index 10, End of Epoch 5/6, Average Loss=4.428926467895508, Class Loss=0.5327302813529968, Reg Loss=3.896196126937866
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=4.399969732761383
Loss made of: CE 0.6350787878036499, LKD 3.491013765335083, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5103669762611389, Reg Loss=3.915043830871582
Clinet index 10, End of Epoch 6/6, Average Loss=4.425410747528076, Class Loss=0.5103669762611389, Reg Loss=3.915043830871582
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/21, Loss=4.769778847694397
Loss made of: CE 0.6378327012062073, LKD 4.791938304901123, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/21, Loss=4.522626492381096
Loss made of: CE 0.5133199095726013, LKD 4.046518325805664, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5589022040367126, Reg Loss=4.09906005859375
Clinet index 9, End of Epoch 1/6, Average Loss=4.657962322235107, Class Loss=0.5589022040367126, Reg Loss=4.09906005859375
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/21, Loss=4.625358626246452
Loss made of: CE 0.4803355634212494, LKD 4.148213863372803, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/21, Loss=4.527977523207665
Loss made of: CE 0.4623595178127289, LKD 4.422707557678223, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.48289868235588074, Reg Loss=4.067795276641846
Clinet index 9, End of Epoch 2/6, Average Loss=4.550693988800049, Class Loss=0.48289868235588074, Reg Loss=4.067795276641846
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/21, Loss=4.677355739474296
Loss made of: CE 0.404369592666626, LKD 3.626817464828491, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/21, Loss=4.450899696350097
Loss made of: CE 0.4832168519496918, LKD 4.252289772033691, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.46461033821105957, Reg Loss=4.084171772003174
Clinet index 9, End of Epoch 3/6, Average Loss=4.5487823486328125, Class Loss=0.46461033821105957, Reg Loss=4.084171772003174
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/21, Loss=4.372452762722969
Loss made of: CE 0.39426249265670776, LKD 4.337404727935791, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.523251402378082
Loss made of: CE 0.4777814745903015, LKD 4.657898902893066, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.451398640871048, Reg Loss=4.031611442565918
Clinet index 9, End of Epoch 4/6, Average Loss=4.483010292053223, Class Loss=0.451398640871048, Reg Loss=4.031611442565918
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=4.576349955797196
Loss made of: CE 0.49995502829551697, LKD 3.781611919403076, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.430403825640679
Loss made of: CE 0.4173969626426697, LKD 3.698345184326172, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4360911250114441, Reg Loss=4.051532745361328
Clinet index 9, End of Epoch 5/6, Average Loss=4.487623691558838, Class Loss=0.4360911250114441, Reg Loss=4.051532745361328
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/21, Loss=4.407511594891548
Loss made of: CE 0.43555817008018494, LKD 3.703706741333008, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.711393249034882
Loss made of: CE 0.5104209184646606, LKD 3.390986919403076, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4271312654018402, Reg Loss=4.096024036407471
Clinet index 9, End of Epoch 6/6, Average Loss=4.523155212402344, Class Loss=0.4271312654018402, Reg Loss=4.096024036407471
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=4.472817492485047
Loss made of: CE 0.7304738759994507, LKD 3.5330772399902344, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=4.382633697986603
Loss made of: CE 0.5806392431259155, LKD 3.421631097793579, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6664618849754333, Reg Loss=3.710320472717285
Clinet index 6, End of Epoch 1/6, Average Loss=4.376782417297363, Class Loss=0.6664618849754333, Reg Loss=3.710320472717285
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/24, Loss=4.286672651767731
Loss made of: CE 0.6221634149551392, LKD 3.9114129543304443, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=4.101777964830399
Loss made of: CE 0.5693077445030212, LKD 3.580568790435791, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6025919914245605, Reg Loss=3.6987357139587402
Clinet index 6, End of Epoch 2/6, Average Loss=4.301327705383301, Class Loss=0.6025919914245605, Reg Loss=3.6987357139587402
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/24, Loss=4.1617937088012695
Loss made of: CE 0.526706874370575, LKD 3.422393560409546, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.461022484302521
Loss made of: CE 0.6012095212936401, LKD 4.549351215362549, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5869184136390686, Reg Loss=3.7149691581726074
Clinet index 6, End of Epoch 3/6, Average Loss=4.301887512207031, Class Loss=0.5869184136390686, Reg Loss=3.7149691581726074
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/24, Loss=4.262982589006424
Loss made of: CE 0.5592529773712158, LKD 3.791212320327759, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.329960364103317
Loss made of: CE 0.5258907079696655, LKD 3.854299306869507, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5711356401443481, Reg Loss=3.7033610343933105
Clinet index 6, End of Epoch 4/6, Average Loss=4.274496555328369, Class Loss=0.5711356401443481, Reg Loss=3.7033610343933105
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/24, Loss=4.294135928153992
Loss made of: CE 0.6867489814758301, LKD 3.5473930835723877, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.061832004785538
Loss made of: CE 0.5314866304397583, LKD 3.316815137863159, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5663739442825317, Reg Loss=3.673546552658081
Clinet index 6, End of Epoch 5/6, Average Loss=4.239920616149902, Class Loss=0.5663739442825317, Reg Loss=3.673546552658081
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/24, Loss=4.366872650384903
Loss made of: CE 0.5175506472587585, LKD 3.656965494155884, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.130465739965439
Loss made of: CE 0.477395236492157, LKD 3.8562045097351074, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5454764366149902, Reg Loss=3.7050769329071045
Clinet index 6, End of Epoch 6/6, Average Loss=4.250553131103516, Class Loss=0.5454764366149902, Reg Loss=3.7050769329071045
federated aggregation...
Validation, Class Loss=0.6496874094009399, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.806102
Mean Acc: 0.391494
FreqW Acc: 0.715183
Mean IoU: 0.260652
Class IoU:
	class 0: 0.891231
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.0035724607
	class 5: 0.0
	class 6: 0.0
	class 7: 0.1744035
	class 8: 0.19096227
	class 9: 0.08825132
	class 10: 2.3951183e-05
	class 11: 0.39775443
	class 12: 0.45646727
	class 13: 0.4825612
	class 14: 0.66569495
	class 15: 0.7734345
	class 16: 0.28381124
	class 17: 0.19437002
	class 18: 0.30863297
	class 19: 0.23423788
	class 20: 0.32828858
Class Acc:
	class 0: 0.9647529
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.0035727192
	class 5: 0.0
	class 6: 0.0
	class 7: 0.17452855
	class 8: 0.19112782
	class 9: 0.090983905
	class 10: 2.3951292e-05
	class 11: 0.43967798
	class 12: 0.68688345
	class 13: 0.65081286
	class 14: 0.7044037
	class 15: 0.8421067
	class 16: 0.30240276
	class 17: 0.94405717
	class 18: 0.6552181
	class 19: 0.8127554
	class 20: 0.758058

federated global round: 24, step: 4
select part of clients to conduct local training
[18, 24, 25, 10]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.4386903703212734
Loss made of: CE 0.6025447249412537, LKD 3.828608274459839, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6320753693580627, Reg Loss=3.8076705932617188
Clinet index 18, End of Epoch 1/6, Average Loss=4.439745903015137, Class Loss=0.6320753693580627, Reg Loss=3.8076705932617188
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/19, Loss=4.381720274686813
Loss made of: CE 0.6628385782241821, LKD 3.804781198501587, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5449481010437012, Reg Loss=3.798413038253784
Clinet index 18, End of Epoch 2/6, Average Loss=4.343360900878906, Class Loss=0.5449481010437012, Reg Loss=3.798413038253784
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/19, Loss=4.558698916435242
Loss made of: CE 0.5431686639785767, LKD 4.771122455596924, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5176281929016113, Reg Loss=3.809701681137085
Clinet index 18, End of Epoch 3/6, Average Loss=4.327329635620117, Class Loss=0.5176281929016113, Reg Loss=3.809701681137085
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/19, Loss=4.345370554924012
Loss made of: CE 0.5345675945281982, LKD 3.663430690765381, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.49615201354026794, Reg Loss=3.7886738777160645
Clinet index 18, End of Epoch 4/6, Average Loss=4.284825801849365, Class Loss=0.49615201354026794, Reg Loss=3.7886738777160645
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/19, Loss=4.422845152020455
Loss made of: CE 0.4331229627132416, LKD 3.207003355026245, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4935978949069977, Reg Loss=3.782078504562378
Clinet index 18, End of Epoch 5/6, Average Loss=4.275676250457764, Class Loss=0.4935978949069977, Reg Loss=3.782078504562378
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/19, Loss=4.314185899496079
Loss made of: CE 0.4491134285926819, LKD 4.147120952606201, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4804418981075287, Reg Loss=3.765259265899658
Clinet index 18, End of Epoch 6/6, Average Loss=4.245701313018799, Class Loss=0.4804418981075287, Reg Loss=3.765259265899658
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/21, Loss=4.564441567659378
Loss made of: CE 0.4486905336380005, LKD 3.4872002601623535, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/21, Loss=4.748790046572685
Loss made of: CE 0.4757766127586365, LKD 4.286966323852539, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.48847126960754395, Reg Loss=4.128068923950195
Clinet index 24, End of Epoch 1/6, Average Loss=4.61653995513916, Class Loss=0.48847126960754395, Reg Loss=4.128068923950195
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/21, Loss=4.404330798983574
Loss made of: CE 0.3771199584007263, LKD 4.383058547973633, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/21, Loss=4.675634434819221
Loss made of: CE 0.48939475417137146, LKD 4.825927257537842, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.43352827429771423, Reg Loss=4.0999650955200195
Clinet index 24, End of Epoch 2/6, Average Loss=4.533493518829346, Class Loss=0.43352827429771423, Reg Loss=4.0999650955200195
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/21, Loss=4.367417234182358
Loss made of: CE 0.40569132566452026, LKD 4.442111492156982, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/21, Loss=4.719377502799034
Loss made of: CE 0.4470616579055786, LKD 3.6731574535369873, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.42752185463905334, Reg Loss=4.116016864776611
Clinet index 24, End of Epoch 3/6, Average Loss=4.543538570404053, Class Loss=0.42752185463905334, Reg Loss=4.116016864776611
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/21, Loss=4.670292353630066
Loss made of: CE 0.45449504256248474, LKD 4.752778053283691, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.3091129004955295
Loss made of: CE 0.36068832874298096, LKD 4.423884868621826, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.41419538855552673, Reg Loss=4.10915470123291
Clinet index 24, End of Epoch 4/6, Average Loss=4.523350238800049, Class Loss=0.41419538855552673, Reg Loss=4.10915470123291
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/21, Loss=4.428064551949501
Loss made of: CE 0.340952605009079, LKD 3.171220302581787, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.651232507824898
Loss made of: CE 0.3593963086605072, LKD 5.047595024108887, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4044053852558136, Reg Loss=4.145514011383057
Clinet index 24, End of Epoch 5/6, Average Loss=4.549919605255127, Class Loss=0.4044053852558136, Reg Loss=4.145514011383057
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/21, Loss=4.69498695731163
Loss made of: CE 0.4262065589427948, LKD 5.441102027893066, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.39576345384121
Loss made of: CE 0.4136926829814911, LKD 3.9528462886810303, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.40488961338996887, Reg Loss=4.113248348236084
Clinet index 24, End of Epoch 6/6, Average Loss=4.5181379318237305, Class Loss=0.40488961338996887, Reg Loss=4.113248348236084
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=4.448295724391937
Loss made of: CE 0.7026144862174988, LKD 4.179738521575928, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=4.439235919713974
Loss made of: CE 0.589582085609436, LKD 3.702287435531616, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.645334005355835, Reg Loss=3.7956578731536865
Clinet index 25, End of Epoch 1/6, Average Loss=4.4409918785095215, Class Loss=0.645334005355835, Reg Loss=3.7956578731536865
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/24, Loss=4.394799697399139
Loss made of: CE 0.6798664331436157, LKD 4.659276962280273, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=4.467015248537064
Loss made of: CE 0.6989781260490417, LKD 3.9047889709472656, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6091440916061401, Reg Loss=3.818789482116699
Clinet index 25, End of Epoch 2/6, Average Loss=4.427933692932129, Class Loss=0.6091440916061401, Reg Loss=3.818789482116699
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/24, Loss=4.3436276614665985
Loss made of: CE 0.5989841222763062, LKD 3.4149632453918457, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.492301893234253
Loss made of: CE 0.577141284942627, LKD 3.5536999702453613, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5895333290100098, Reg Loss=3.81370210647583
Clinet index 25, End of Epoch 3/6, Average Loss=4.40323543548584, Class Loss=0.5895333290100098, Reg Loss=3.81370210647583
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/24, Loss=4.352588865160942
Loss made of: CE 0.6069682240486145, LKD 3.9788358211517334, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.309696039557457
Loss made of: CE 0.6200046539306641, LKD 3.971996545791626, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5698718428611755, Reg Loss=3.8280510902404785
Clinet index 25, End of Epoch 4/6, Average Loss=4.397922992706299, Class Loss=0.5698718428611755, Reg Loss=3.8280510902404785
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/24, Loss=4.378309851884842
Loss made of: CE 0.6964378356933594, LKD 3.9848103523254395, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.387303894758224
Loss made of: CE 0.6104070544242859, LKD 3.7892184257507324, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5745543241500854, Reg Loss=3.7733466625213623
Clinet index 25, End of Epoch 5/6, Average Loss=4.347900867462158, Class Loss=0.5745543241500854, Reg Loss=3.7733466625213623
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/24, Loss=4.363684606552124
Loss made of: CE 0.5656312704086304, LKD 4.468864440917969, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.381383091211319
Loss made of: CE 0.6440478563308716, LKD 3.932046890258789, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5597947835922241, Reg Loss=3.801935911178589
Clinet index 25, End of Epoch 6/6, Average Loss=4.361730575561523, Class Loss=0.5597947835922241, Reg Loss=3.801935911178589
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/19, Loss=4.81013440489769
Loss made of: CE 0.6423123478889465, LKD 4.242330551147461, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6941218376159668, Reg Loss=4.002368927001953
Clinet index 10, End of Epoch 1/6, Average Loss=4.69649076461792, Class Loss=0.6941218376159668, Reg Loss=4.002368927001953
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/19, Loss=4.517363178730011
Loss made of: CE 0.6171669363975525, LKD 3.2636682987213135, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6183081865310669, Reg Loss=3.9520444869995117
Clinet index 10, End of Epoch 2/6, Average Loss=4.570352554321289, Class Loss=0.6183081865310669, Reg Loss=3.9520444869995117
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/19, Loss=4.318761950731277
Loss made of: CE 0.6703286170959473, LKD 4.023765563964844, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5811719298362732, Reg Loss=3.9710726737976074
Clinet index 10, End of Epoch 3/6, Average Loss=4.552244663238525, Class Loss=0.5811719298362732, Reg Loss=3.9710726737976074
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/19, Loss=4.499532389640808
Loss made of: CE 0.5773919820785522, LKD 4.519865989685059, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5601698160171509, Reg Loss=3.9036612510681152
Clinet index 10, End of Epoch 4/6, Average Loss=4.463830947875977, Class Loss=0.5601698160171509, Reg Loss=3.9036612510681152
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/19, Loss=4.448488408327103
Loss made of: CE 0.404214471578598, LKD 3.7526040077209473, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5553375482559204, Reg Loss=3.944894552230835
Clinet index 10, End of Epoch 5/6, Average Loss=4.500232219696045, Class Loss=0.5553375482559204, Reg Loss=3.944894552230835
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/19, Loss=4.376242715120315
Loss made of: CE 0.6175197958946228, LKD 3.428795576095581, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5333828926086426, Reg Loss=3.8722727298736572
Clinet index 10, End of Epoch 6/6, Average Loss=4.405655860900879, Class Loss=0.5333828926086426, Reg Loss=3.8722727298736572
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.6805157661437988, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.792285
Mean Acc: 0.366497
FreqW Acc: 0.703210
Mean IoU: 0.238426
Class IoU:
	class 0: 0.88375103
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.009175371
	class 5: 0.0
	class 6: 5.263436e-07
	class 7: 0.087218724
	class 8: 0.16308513
	class 9: 0.0845087
	class 10: 0.0
	class 11: 0.3917725
	class 12: 0.39597702
	class 13: 0.38616964
	class 14: 0.6320765
	class 15: 0.7773425
	class 16: 0.24889484
	class 17: 0.15074573
	class 18: 0.24860924
	class 19: 0.2512186
	class 20: 0.29640055
Class Acc:
	class 0: 0.958692
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.00917564
	class 5: 0.0
	class 6: 5.263436e-07
	class 7: 0.08726692
	class 8: 0.16315734
	class 9: 0.086956106
	class 10: 0.0
	class 11: 0.426136
	class 12: 0.52359563
	class 13: 0.45851484
	class 14: 0.662273
	class 15: 0.84337205
	class 16: 0.26172706
	class 17: 0.9657355
	class 18: 0.702557
	class 19: 0.7762592
	class 20: 0.7710084

voc_4-4_LWF On GPUs 0
Run in 43043s
