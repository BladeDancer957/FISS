nohup: ignoring input
25
kvoc_4-4_OURS-FSC On GPUs 0\Writing in results/seed_2023-ov/2023-03-19_voc_4-4_OURS-FSC.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[6, 0, 7, 2]
Current Client Index:  6
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[8, 5, 3, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[5, 2, 0, 3]
Current Client Index:  5
Current Client Index:  2
Current Client Index:  0
Current Client Index:  3
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[3, 6, 1, 7]
Current Client Index:  3
Current Client Index:  6
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.09879439324140549, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.957759
Mean Acc: 0.761733
FreqW Acc: 0.923270
Mean IoU: 0.703965
Class IoU:
	class 0: 0.9539515191877496
	class 1: 0.7615770129403676
	class 2: 0.2054173196242969
	class 3: 0.8920301311931925
	class 4: 0.7068509839220561
Class Acc:
	class 0: 0.9848764620456163
	class 1: 0.7738026584549321
	class 2: 0.27733822617675347
	class 3: 0.9519182023560161
	class 4: 0.8207287142557349

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[0, 12, 6, 10]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/52, Loss=8.755212777853012
Loss made of: CE 0.9759065508842468, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.268233299255371 EntMin 0.0
Epoch 1, Batch 20/52, Loss=7.0375491917133335
Loss made of: CE 0.7761063575744629, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.023681640625 EntMin 0.0
Epoch 1, Batch 30/52, Loss=6.570126736164093
Loss made of: CE 0.7297674417495728, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.416325569152832 EntMin 0.0
Epoch 1, Batch 40/52, Loss=5.977093493938446
Loss made of: CE 0.4935743808746338, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.055215835571289 EntMin 0.0
Epoch 1, Batch 50/52, Loss=5.871552830934524
Loss made of: CE 0.5280951261520386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.745391845703125 EntMin 0.0
Epoch 1, Class Loss=0.7953341007232666, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.7953341007232666, Class Loss=0.7953341007232666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/52, Loss=5.513790801167488
Loss made of: CE 0.534291684627533, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.774737358093262 EntMin 0.0
Epoch 2, Batch 20/52, Loss=5.220344313979149
Loss made of: CE 0.5521368980407715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.660387992858887 EntMin 0.0
Epoch 2, Batch 30/52, Loss=5.350390240550041
Loss made of: CE 0.4366909861564636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.65220308303833 EntMin 0.0
Epoch 2, Batch 40/52, Loss=5.086272460222244
Loss made of: CE 0.365700900554657, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.072443008422852 EntMin 0.0
Epoch 2, Batch 50/52, Loss=4.943563055992127
Loss made of: CE 0.45444631576538086, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0223188400268555 EntMin 0.0
Epoch 2, Class Loss=0.48971521854400635, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.48971521854400635, Class Loss=0.48971521854400635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/52, Loss=4.972391191124916
Loss made of: CE 0.32995373010635376, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.711799144744873 EntMin 0.0
Epoch 3, Batch 20/52, Loss=4.746963319182396
Loss made of: CE 0.3364579677581787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.161900997161865 EntMin 0.0
Epoch 3, Batch 30/52, Loss=4.766664460301399
Loss made of: CE 0.40503865480422974, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7877891063690186 EntMin 0.0
Epoch 3, Batch 40/52, Loss=4.660070872306823
Loss made of: CE 0.2866222858428955, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.962407112121582 EntMin 0.0
Epoch 3, Batch 50/52, Loss=4.644409796595573
Loss made of: CE 0.3024292290210724, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.253572463989258 EntMin 0.0
Epoch 3, Class Loss=0.3678054213523865, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3678054213523865, Class Loss=0.3678054213523865, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/52, Loss=4.739514681696892
Loss made of: CE 0.45805424451828003, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.167377471923828 EntMin 0.0
Epoch 4, Batch 20/52, Loss=4.46734154522419
Loss made of: CE 0.3140876293182373, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.187040328979492 EntMin 0.0
Epoch 4, Batch 30/52, Loss=4.420344191789627
Loss made of: CE 0.21288391947746277, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7209699153900146 EntMin 0.0
Epoch 4, Batch 40/52, Loss=4.39030494838953
Loss made of: CE 0.2227695882320404, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9188718795776367 EntMin 0.0
Epoch 4, Batch 50/52, Loss=4.1904452934861185
Loss made of: CE 0.30028849840164185, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6870477199554443 EntMin 0.0
Epoch 4, Class Loss=0.29798370599746704, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.29798370599746704, Class Loss=0.29798370599746704, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/52, Loss=4.294331189990044
Loss made of: CE 0.2586548328399658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6242616176605225 EntMin 0.0
Epoch 5, Batch 20/52, Loss=4.285397264361381
Loss made of: CE 0.2151484489440918, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.891624927520752 EntMin 0.0
Epoch 5, Batch 30/52, Loss=4.248466478288174
Loss made of: CE 0.2874968647956848, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.820098400115967 EntMin 0.0
Epoch 5, Batch 40/52, Loss=4.2291894346475605
Loss made of: CE 0.2615390419960022, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.345954895019531 EntMin 0.0
Epoch 5, Batch 50/52, Loss=4.195539727807045
Loss made of: CE 0.2595689296722412, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7954063415527344 EntMin 0.0
Epoch 5, Class Loss=0.26816797256469727, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.26816797256469727, Class Loss=0.26816797256469727, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/52, Loss=4.0084059804677965
Loss made of: CE 0.19738903641700745, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.583981513977051 EntMin 0.0
Epoch 6, Batch 20/52, Loss=4.209029370546341
Loss made of: CE 0.22802552580833435, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.460631847381592 EntMin 0.0
Epoch 6, Batch 30/52, Loss=4.308260598778725
Loss made of: CE 0.35487276315689087, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8499674797058105 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.945380023121834
Loss made of: CE 0.25717267394065857, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.455082893371582 EntMin 0.0
Epoch 6, Batch 50/52, Loss=4.100064243376255
Loss made of: CE 0.29454776644706726, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.141566753387451 EntMin 0.0
Epoch 6, Class Loss=0.24392257630825043, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.24392257630825043, Class Loss=0.24392257630825043, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=9.492868876457214
Loss made of: CE 0.925798773765564, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.049278259277344 EntMin 0.0
Epoch 1, Batch 20/40, Loss=8.390182024240493
Loss made of: CE 0.5857114791870117, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.407011985778809 EntMin 0.0
Epoch 1, Batch 30/40, Loss=7.496842536330223
Loss made of: CE 0.5590121746063232, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.28511381149292 EntMin 0.0
Epoch 1, Batch 40/40, Loss=6.9509180903434755
Loss made of: CE 0.4075866937637329, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.389230251312256 EntMin 0.0
Epoch 1, Class Loss=0.7384763956069946, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.7384763956069946, Class Loss=0.7384763956069946, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/40, Loss=6.727032926678658
Loss made of: CE 0.24150526523590088, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.492456436157227 EntMin 0.0
Epoch 2, Batch 20/40, Loss=6.2249377995729445
Loss made of: CE 0.44428086280822754, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.111868858337402 EntMin 0.0
Epoch 2, Batch 30/40, Loss=6.040274208784103
Loss made of: CE 0.42581844329833984, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.438237190246582 EntMin 0.0
Epoch 2, Batch 40/40, Loss=6.198219376802444
Loss made of: CE 0.4210127592086792, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.141656875610352 EntMin 0.0
Epoch 2, Class Loss=0.4372011721134186, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.4372011721134186, Class Loss=0.4372011721134186, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/40, Loss=5.815394619107247
Loss made of: CE 0.3201214373111725, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.264914512634277 EntMin 0.0
Epoch 3, Batch 20/40, Loss=5.689050257205963
Loss made of: CE 0.5109879374504089, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.47688102722168 EntMin 0.0
Epoch 3, Batch 30/40, Loss=5.6255356758832935
Loss made of: CE 0.29161131381988525, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.310046672821045 EntMin 0.0
Epoch 3, Batch 40/40, Loss=5.43429653942585
Loss made of: CE 0.23542684316635132, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.911617755889893 EntMin 0.0
Epoch 3, Class Loss=0.3644808232784271, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.3644808232784271, Class Loss=0.3644808232784271, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/40, Loss=5.288473889231682
Loss made of: CE 0.36982226371765137, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.437875270843506 EntMin 0.0
Epoch 4, Batch 20/40, Loss=5.323599794507027
Loss made of: CE 0.1901281476020813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.599809646606445 EntMin 0.0
Epoch 4, Batch 30/40, Loss=5.297401721775532
Loss made of: CE 0.29602882266044617, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.065253734588623 EntMin 0.0
Epoch 4, Batch 40/40, Loss=5.372341020405292
Loss made of: CE 0.2874840497970581, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.213946342468262 EntMin 0.0
Epoch 4, Class Loss=0.31118258833885193, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.31118258833885193, Class Loss=0.31118258833885193, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/40, Loss=5.21154962182045
Loss made of: CE 0.2744537591934204, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.825747489929199 EntMin 0.0
Epoch 5, Batch 20/40, Loss=5.053561297059059
Loss made of: CE 0.3540585935115814, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.08933687210083 EntMin 0.0
Epoch 5, Batch 30/40, Loss=5.150565192103386
Loss made of: CE 0.21423207223415375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.415502071380615 EntMin 0.0
Epoch 5, Batch 40/40, Loss=5.153885453939438
Loss made of: CE 0.2577000558376312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.621707916259766 EntMin 0.0
Epoch 5, Class Loss=0.2759106755256653, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.2759106755256653, Class Loss=0.2759106755256653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/40, Loss=4.953775949776173
Loss made of: CE 0.19921496510505676, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53006649017334 EntMin 0.0
Epoch 6, Batch 20/40, Loss=5.104198534786701
Loss made of: CE 0.22556781768798828, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.282466888427734 EntMin 0.0
Epoch 6, Batch 30/40, Loss=5.163492897152901
Loss made of: CE 0.24774765968322754, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.18717098236084 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.823215471208096
Loss made of: CE 0.1812112182378769, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.418439865112305 EntMin 0.0
Epoch 6, Class Loss=0.26004642248153687, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.26004642248153687, Class Loss=0.26004642248153687, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=9.968951845169068
Loss made of: CE 1.1323668956756592, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.157610893249512 EntMin 0.0
Epoch 1, Batch 20/34, Loss=8.40188525915146
Loss made of: CE 0.7025853991508484, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.40174674987793 EntMin 0.0
Epoch 1, Batch 30/34, Loss=7.98933829665184
Loss made of: CE 0.721707284450531, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.789891719818115 EntMin 0.0
Epoch 1, Class Loss=0.9337232112884521, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.9337232112884521, Class Loss=0.9337232112884521, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/34, Loss=7.1397118926048275
Loss made of: CE 0.5858482718467712, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.36137580871582 EntMin 0.0
Epoch 2, Batch 20/34, Loss=6.559963220357895
Loss made of: CE 0.6191410422325134, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.597485542297363 EntMin 0.0
Epoch 2, Batch 30/34, Loss=6.329979014396668
Loss made of: CE 0.43865489959716797, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.982133388519287 EntMin 0.0
Epoch 2, Class Loss=0.5698111653327942, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.5698111653327942, Class Loss=0.5698111653327942, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/34, Loss=5.891636592149735
Loss made of: CE 0.5214206576347351, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.198204040527344 EntMin 0.0
Epoch 3, Batch 20/34, Loss=5.8101631820201876
Loss made of: CE 0.42520207166671753, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.112466812133789 EntMin 0.0
Epoch 3, Batch 30/34, Loss=5.931664994359016
Loss made of: CE 0.39441460371017456, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.223528861999512 EntMin 0.0
Epoch 3, Class Loss=0.4322575032711029, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4322575032711029, Class Loss=0.4322575032711029, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/34, Loss=5.556329628825187
Loss made of: CE 0.3670107126235962, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.17693567276001 EntMin 0.0
Epoch 4, Batch 20/34, Loss=5.480363637208939
Loss made of: CE 0.3169454336166382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.687506675720215 EntMin 0.0
Epoch 4, Batch 30/34, Loss=5.5326189517974855
Loss made of: CE 0.4606242775917053, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.719598770141602 EntMin 0.0
Epoch 4, Class Loss=0.3535804748535156, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3535804748535156, Class Loss=0.3535804748535156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/34, Loss=5.2557192027568815
Loss made of: CE 0.2745113968849182, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.489247798919678 EntMin 0.0
Epoch 5, Batch 20/34, Loss=5.364890587329865
Loss made of: CE 0.33362650871276855, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.540774345397949 EntMin 0.0
Epoch 5, Batch 30/34, Loss=5.047282464802265
Loss made of: CE 0.2598955035209656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.72247314453125 EntMin 0.0
Epoch 5, Class Loss=0.2935905456542969, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.2935905456542969, Class Loss=0.2935905456542969, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/34, Loss=5.106936648488045
Loss made of: CE 0.3001742959022522, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.889316082000732 EntMin 0.0
Epoch 6, Batch 20/34, Loss=5.083126500248909
Loss made of: CE 0.26842519640922546, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.635262489318848 EntMin 0.0
Epoch 6, Batch 30/34, Loss=5.170768441259861
Loss made of: CE 0.23359337449073792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.529029846191406 EntMin 0.0
Epoch 6, Class Loss=0.26416170597076416, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.26416170597076416, Class Loss=0.26416170597076416, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=9.177590131759644
Loss made of: CE 1.1246684789657593, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.432374477386475 EntMin 0.0
Epoch 1, Batch 20/33, Loss=7.858996385335923
Loss made of: CE 0.887169361114502, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.172667980194092 EntMin 0.0
Epoch 1, Batch 30/33, Loss=6.690935182571411
Loss made of: CE 0.6853217482566833, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.014434814453125 EntMin 0.0
Epoch 1, Class Loss=0.9154999852180481, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.9154999852180481, Class Loss=0.9154999852180481, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/33, Loss=6.204817193746567
Loss made of: CE 0.6502261161804199, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.200382232666016 EntMin 0.0
Epoch 2, Batch 20/33, Loss=5.883891630172729
Loss made of: CE 0.770873486995697, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5838823318481445 EntMin 0.0
Epoch 2, Batch 30/33, Loss=5.461509615182877
Loss made of: CE 0.5490430593490601, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.868399620056152 EntMin 0.0
Epoch 2, Class Loss=0.6296123266220093, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.6296123266220093, Class Loss=0.6296123266220093, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/33, Loss=5.157494768500328
Loss made of: CE 0.5096727013587952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.45339822769165 EntMin 0.0
Epoch 3, Batch 20/33, Loss=5.117391720414162
Loss made of: CE 0.41790157556533813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.373114585876465 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.959877607226372
Loss made of: CE 0.44575977325439453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.262493133544922 EntMin 0.0
Epoch 3, Class Loss=0.47986823320388794, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.47986823320388794, Class Loss=0.47986823320388794, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/33, Loss=4.678790929913521
Loss made of: CE 0.38772591948509216, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.933872699737549 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.779390439391136
Loss made of: CE 0.35812515020370483, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291776657104492 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.5982656031847
Loss made of: CE 0.2880863547325134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.908111333847046 EntMin 0.0
Epoch 4, Class Loss=0.37592512369155884, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.37592512369155884, Class Loss=0.37592512369155884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/33, Loss=4.48843267261982
Loss made of: CE 0.3879512548446655, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.192267417907715 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.389048752188683
Loss made of: CE 0.2439168095588684, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.809018850326538 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.560696375370026
Loss made of: CE 0.3270438313484192, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.955020904541016 EntMin 0.0
Epoch 5, Class Loss=0.3043292760848999, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.3043292760848999, Class Loss=0.3043292760848999, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/33, Loss=4.317529422044754
Loss made of: CE 0.3605494499206543, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.142520904541016 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.375352518260479
Loss made of: CE 0.23550426959991455, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.878387212753296 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.214812669157982
Loss made of: CE 0.2435370683670044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.232611656188965 EntMin 0.0
Epoch 6, Class Loss=0.2674649655818939, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.2674649655818939, Class Loss=0.2674649655818939, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.39709821343421936, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.853037
Mean Acc: 0.542699
FreqW Acc: 0.739555
Mean IoU: 0.408393
Class IoU:
	class 0: 0.85610276
	class 1: 0.776925
	class 2: 0.32527873
	class 3: 0.60867864
	class 4: 0.47781205
	class 5: 0.0
	class 6: 0.06886246
	class 7: 0.018715044
	class 8: 0.5431655
Class Acc:
	class 0: 0.97815716
	class 1: 0.8419272
	class 2: 0.5916609
	class 3: 0.93131405
	class 4: 0.868863
	class 5: 0.0
	class 6: 0.06887838
	class 7: 0.018737532
	class 8: 0.58474904

federated global round: 6, step: 1
select part of clients to conduct local training
[11, 5, 8, 12]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/43, Loss=5.125253346562386
Loss made of: CE 0.35639071464538574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.763557434082031 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.687791806459427
Loss made of: CE 0.5158141851425171, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.914422035217285 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.504999679327011
Loss made of: CE 0.3181324601173401, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6762895584106445 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.427553866803646
Loss made of: CE 0.3414599299430847, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.750282049179077 EntMin 0.0
Epoch 1, Class Loss=0.3725551664829254, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.3725551664829254, Class Loss=0.3725551664829254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/43, Loss=4.2529285699129105
Loss made of: CE 0.29215967655181885, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.873838186264038 EntMin 0.0
Epoch 2, Batch 20/43, Loss=4.104590912163258
Loss made of: CE 0.282105416059494, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.835814952850342 EntMin 0.0
Epoch 2, Batch 30/43, Loss=4.139819717407226
Loss made of: CE 0.3370761573314667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.268791198730469 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.201757384836673
Loss made of: CE 0.3345632553100586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.090836524963379 EntMin 0.0
Epoch 2, Class Loss=0.28882089257240295, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.28882089257240295, Class Loss=0.28882089257240295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/43, Loss=3.932041722536087
Loss made of: CE 0.342556893825531, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.572695255279541 EntMin 0.0
Epoch 3, Batch 20/43, Loss=4.015865306556225
Loss made of: CE 0.24992862343788147, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9913642406463623 EntMin 0.0
Epoch 3, Batch 30/43, Loss=4.018284785747528
Loss made of: CE 0.2863580584526062, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6540536880493164 EntMin 0.0
Epoch 3, Batch 40/43, Loss=3.9389951810240746
Loss made of: CE 0.1850394457578659, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.825197696685791 EntMin 0.0
Epoch 3, Class Loss=0.26446717977523804, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.26446717977523804, Class Loss=0.26446717977523804, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/43, Loss=3.8315121233463287
Loss made of: CE 0.3108489215373993, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.045267581939697 EntMin 0.0
Epoch 4, Batch 20/43, Loss=3.862825846672058
Loss made of: CE 0.2517615556716919, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.537391424179077 EntMin 0.0
Epoch 4, Batch 30/43, Loss=3.894879053533077
Loss made of: CE 0.283128947019577, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.486879348754883 EntMin 0.0
Epoch 4, Batch 40/43, Loss=3.859748177230358
Loss made of: CE 0.22084812819957733, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3638548851013184 EntMin 0.0
Epoch 4, Class Loss=0.24291743338108063, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.24291743338108063, Class Loss=0.24291743338108063, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/43, Loss=3.8521735951304437
Loss made of: CE 0.2355574667453766, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.890671491622925 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.7953596606850626
Loss made of: CE 0.27478092908859253, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.589428424835205 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.84082151055336
Loss made of: CE 0.24336960911750793, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6138627529144287 EntMin 0.0
Epoch 5, Batch 40/43, Loss=3.8784653306007386
Loss made of: CE 0.2554500102996826, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.240736961364746 EntMin 0.0
Epoch 5, Class Loss=0.23697704076766968, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.23697704076766968, Class Loss=0.23697704076766968, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/43, Loss=3.7481653466820717
Loss made of: CE 0.2718130648136139, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3408029079437256 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.805310443043709
Loss made of: CE 0.2518768906593323, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4677999019622803 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.7281834542751313
Loss made of: CE 0.17368555068969727, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2092103958129883 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.6065269887447355
Loss made of: CE 0.22962459921836853, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.36002779006958 EntMin 0.0
Epoch 6, Class Loss=0.21611666679382324, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.21611666679382324, Class Loss=0.21611666679382324, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=5.5456670939922335
Loss made of: CE 0.4791414737701416, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.048377990722656 EntMin 0.0
Epoch 1, Batch 20/34, Loss=5.43368416428566
Loss made of: CE 0.3281865119934082, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.205195426940918 EntMin 0.0
Epoch 1, Batch 30/34, Loss=5.26948549002409
Loss made of: CE 0.20845453441143036, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0260233879089355 EntMin 0.0
Epoch 1, Class Loss=0.34334009885787964, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.34334009885787964, Class Loss=0.34334009885787964, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/34, Loss=5.102403211593628
Loss made of: CE 0.25594374537467957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.803535461425781 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.882498280704022
Loss made of: CE 0.23465518653392792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7745513916015625 EntMin 0.0
Epoch 2, Batch 30/34, Loss=5.185618250072002
Loss made of: CE 0.2763224244117737, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.587965965270996 EntMin 0.0
Epoch 2, Class Loss=0.24289333820343018, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.24289333820343018, Class Loss=0.24289333820343018, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/34, Loss=4.686518822610378
Loss made of: CE 0.21837064623832703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.461298942565918 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.84588199108839
Loss made of: CE 0.22180013358592987, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.540788173675537 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.933470715582371
Loss made of: CE 0.2536260485649109, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.351138591766357 EntMin 0.0
Epoch 3, Class Loss=0.21566538512706757, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.21566538512706757, Class Loss=0.21566538512706757, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/34, Loss=4.808767144382
Loss made of: CE 0.19663292169570923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.519196033477783 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.826003184914589
Loss made of: CE 0.2075905054807663, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.654831886291504 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.6726065337657925
Loss made of: CE 0.2120138257741928, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.984713315963745 EntMin 0.0
Epoch 4, Class Loss=0.20692004263401031, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.20692004263401031, Class Loss=0.20692004263401031, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/34, Loss=4.547437576949596
Loss made of: CE 0.19843190908432007, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.585245132446289 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.6648289129138
Loss made of: CE 0.22219380736351013, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.177836894989014 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.58190443366766
Loss made of: CE 0.24503839015960693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.884161949157715 EntMin 0.0
Epoch 5, Class Loss=0.19827722012996674, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.19827722012996674, Class Loss=0.19827722012996674, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/34, Loss=4.720927512645721
Loss made of: CE 0.23707148432731628, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.138446807861328 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.487297363579273
Loss made of: CE 0.18463334441184998, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.509533882141113 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.443148377537727
Loss made of: CE 0.29759448766708374, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.103997230529785 EntMin 0.0
Epoch 6, Class Loss=0.2023729830980301, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.2023729830980301, Class Loss=0.2023729830980301, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=5.665456634759903
Loss made of: CE 0.3606277108192444, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.376073837280273 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.504144297540188
Loss made of: CE 0.2166559398174286, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.347229957580566 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.482586985826492
Loss made of: CE 0.29839304089546204, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.796767234802246 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.147423133254051
Loss made of: CE 0.16990695893764496, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.31512451171875 EntMin 0.0
Epoch 1, Class Loss=0.3007879853248596, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.3007879853248596, Class Loss=0.3007879853248596, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/40, Loss=5.196803072094918
Loss made of: CE 0.32560527324676514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.566886901855469 EntMin 0.0
Epoch 2, Batch 20/40, Loss=4.857718345522881
Loss made of: CE 0.1324053406715393, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4380598068237305 EntMin 0.0
Epoch 2, Batch 30/40, Loss=5.127449932694435
Loss made of: CE 0.29528117179870605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.837985038757324 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.044635353982448
Loss made of: CE 0.25765568017959595, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.98007345199585 EntMin 0.0
Epoch 2, Class Loss=0.2527775466442108, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.2527775466442108, Class Loss=0.2527775466442108, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/40, Loss=4.907673352956772
Loss made of: CE 0.19349507987499237, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.680914878845215 EntMin 0.0
Epoch 3, Batch 20/40, Loss=4.75676105171442
Loss made of: CE 0.22906462848186493, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.453468322753906 EntMin 0.0
Epoch 3, Batch 30/40, Loss=4.878053556382656
Loss made of: CE 0.27347731590270996, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.247334957122803 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.839719212055206
Loss made of: CE 0.20968011021614075, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.056149482727051 EntMin 0.0
Epoch 3, Class Loss=0.23365120589733124, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.23365120589733124, Class Loss=0.23365120589733124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/40, Loss=4.77072427123785
Loss made of: CE 0.3200298547744751, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.733729362487793 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.647930800914764
Loss made of: CE 0.20276741683483124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.50100040435791 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.737111920118332
Loss made of: CE 0.215926393866539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.995984077453613 EntMin 0.0
Epoch 4, Batch 40/40, Loss=4.685595791041851
Loss made of: CE 0.20334039628505707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.215580463409424 EntMin 0.0
Epoch 4, Class Loss=0.22407649457454681, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.22407649457454681, Class Loss=0.22407649457454681, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/40, Loss=4.6245419219136235
Loss made of: CE 0.23713049292564392, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.124743938446045 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.6743139892816545
Loss made of: CE 0.24668404459953308, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.163595676422119 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.464755843579769
Loss made of: CE 0.23291265964508057, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5030317306518555 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.613709123432637
Loss made of: CE 0.17148634791374207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4179840087890625 EntMin 0.0
Epoch 5, Class Loss=0.21177493035793304, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.21177493035793304, Class Loss=0.21177493035793304, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/40, Loss=4.407179985940457
Loss made of: CE 0.18643145263195038, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.389496803283691 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.542863236367703
Loss made of: CE 0.18923097848892212, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1249284744262695 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.411839801073074
Loss made of: CE 0.20375566184520721, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.178953170776367 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.517417596280575
Loss made of: CE 0.1626245379447937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.01423978805542 EntMin 0.0
Epoch 6, Class Loss=0.19641835987567902, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.19641835987567902, Class Loss=0.19641835987567902, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/40, Loss=5.672425249218941
Loss made of: CE 0.3817620277404785, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.390604019165039 EntMin 0.0
Epoch 1, Batch 20/40, Loss=5.573999753594398
Loss made of: CE 0.34220606088638306, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0892534255981445 EntMin 0.0
Epoch 1, Batch 30/40, Loss=5.395745731890202
Loss made of: CE 0.4894574284553528, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.859376907348633 EntMin 0.0
Epoch 1, Batch 40/40, Loss=5.334232902526855
Loss made of: CE 0.295302152633667, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.02667236328125 EntMin 0.0
Epoch 1, Class Loss=0.4013441205024719, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.4013441205024719, Class Loss=0.4013441205024719, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/40, Loss=5.278357574343682
Loss made of: CE 0.1848217248916626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.333187103271484 EntMin 0.0
Epoch 2, Batch 20/40, Loss=5.094000144302845
Loss made of: CE 0.27921515703201294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.294887542724609 EntMin 0.0
Epoch 2, Batch 30/40, Loss=4.981848993897438
Loss made of: CE 0.38057971000671387, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4548845291137695 EntMin 0.0
Epoch 2, Batch 40/40, Loss=5.201158541440964
Loss made of: CE 0.2765045166015625, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2902607917785645 EntMin 0.0
Epoch 2, Class Loss=0.2995899021625519, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.2995899021625519, Class Loss=0.2995899021625519, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/40, Loss=5.036020152270794
Loss made of: CE 0.2145172655582428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.523190498352051 EntMin 0.0
Epoch 3, Batch 20/40, Loss=4.966193656623363
Loss made of: CE 0.40819835662841797, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7701263427734375 EntMin 0.0
Epoch 3, Batch 30/40, Loss=4.937994745373726
Loss made of: CE 0.20574121177196503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.763938903808594 EntMin 0.0
Epoch 3, Batch 40/40, Loss=4.803785678744316
Loss made of: CE 0.18765407800674438, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.491538047790527 EntMin 0.0
Epoch 3, Class Loss=0.27392658591270447, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.27392658591270447, Class Loss=0.27392658591270447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/40, Loss=4.6690643519163135
Loss made of: CE 0.25951653718948364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.315865516662598 EntMin 0.0
Epoch 4, Batch 20/40, Loss=4.824295726418495
Loss made of: CE 0.15556836128234863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.173488140106201 EntMin 0.0
Epoch 4, Batch 30/40, Loss=4.736375983059406
Loss made of: CE 0.24483004212379456, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.742160320281982 EntMin 0.0
Epoch 4, Batch 40/40, Loss=4.9205162361264225
Loss made of: CE 0.24418377876281738, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.813511848449707 EntMin 0.0
Epoch 4, Class Loss=0.24896657466888428, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.24896657466888428, Class Loss=0.24896657466888428, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/40, Loss=4.746918980777264
Loss made of: CE 0.20217818021774292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3866353034973145 EntMin 0.0
Epoch 5, Batch 20/40, Loss=4.589471411705017
Loss made of: CE 0.29440468549728394, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698002338409424 EntMin 0.0
Epoch 5, Batch 30/40, Loss=4.674113865196705
Loss made of: CE 0.17200541496276855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1804094314575195 EntMin 0.0
Epoch 5, Batch 40/40, Loss=4.616276578605175
Loss made of: CE 0.2167455404996872, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.203577518463135 EntMin 0.0
Epoch 5, Class Loss=0.22091399133205414, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.22091399133205414, Class Loss=0.22091399133205414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/40, Loss=4.4839077353477474
Loss made of: CE 0.17756438255310059, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.447360992431641 EntMin 0.0
Epoch 6, Batch 20/40, Loss=4.653390629589557
Loss made of: CE 0.19971618056297302, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.713120937347412 EntMin 0.0
Epoch 6, Batch 30/40, Loss=4.731859089434147
Loss made of: CE 0.23441943526268005, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0736494064331055 EntMin 0.0
Epoch 6, Batch 40/40, Loss=4.392350178956986
Loss made of: CE 0.15758846700191498, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.151083946228027 EntMin 0.0
Epoch 6, Class Loss=0.20797209441661835, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.20797209441661835, Class Loss=0.20797209441661835, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.29746270179748535, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.900650
Mean Acc: 0.671357
FreqW Acc: 0.821969
Mean IoU: 0.563628
Class IoU:
	class 0: 0.8957947
	class 1: 0.8163146
	class 2: 0.3229379
	class 3: 0.73820364
	class 4: 0.56045103
	class 5: 0.10066425
	class 6: 0.6827846
	class 7: 0.25111192
	class 8: 0.704393
Class Acc:
	class 0: 0.9702898
	class 1: 0.84862643
	class 2: 0.5749139
	class 3: 0.82734585
	class 4: 0.86725354
	class 5: 0.10143712
	class 6: 0.7114114
	class 7: 0.25449964
	class 8: 0.8864337

federated global round: 7, step: 1
select part of clients to conduct local training
[0, 6, 13, 5]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/52, Loss=4.365281629562378
Loss made of: CE 0.5213959217071533, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.978703260421753 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.071800175309181
Loss made of: CE 0.27991923689842224, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5529966354370117 EntMin 0.0
Epoch 1, Batch 30/52, Loss=4.00877905189991
Loss made of: CE 0.23030225932598114, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.597768783569336 EntMin 0.0
Epoch 1, Batch 40/52, Loss=4.128018964827061
Loss made of: CE 0.26505735516548157, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202420711517334 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.993120500445366
Loss made of: CE 0.19812612235546112, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4158549308776855 EntMin 0.0
Epoch 1, Class Loss=0.3040352165699005, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.3040352165699005, Class Loss=0.3040352165699005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/52, Loss=4.031144466996193
Loss made of: CE 0.28268057107925415, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6352109909057617 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.905036520957947
Loss made of: CE 0.24182884395122528, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4611096382141113 EntMin 0.0
Epoch 2, Batch 30/52, Loss=4.083092631399632
Loss made of: CE 0.3437812328338623, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8977150917053223 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.9952257722616196
Loss made of: CE 0.206491619348526, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.356135129928589 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.9723820596933366
Loss made of: CE 0.25677794218063354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.295822620391846 EntMin 0.0
Epoch 2, Class Loss=0.23519685864448547, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.23519685864448547, Class Loss=0.23519685864448547, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/52, Loss=3.941367708146572
Loss made of: CE 0.178796648979187, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.789459228515625 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.885174472630024
Loss made of: CE 0.21000443398952484, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5636816024780273 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.984809571504593
Loss made of: CE 0.23133091628551483, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.34342098236084 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.9156252801418305
Loss made of: CE 0.18195949494838715, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.632068634033203 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.9198324605822563
Loss made of: CE 0.2062707543373108, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.529848098754883 EntMin 0.0
Epoch 3, Class Loss=0.21513664722442627, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.21513664722442627, Class Loss=0.21513664722442627, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/52, Loss=4.060887721180916
Loss made of: CE 0.24477677047252655, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6370370388031006 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.714138761162758
Loss made of: CE 0.16372177004814148, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5849950313568115 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.7379446923732758
Loss made of: CE 0.14155811071395874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.214425563812256 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.779653385281563
Loss made of: CE 0.17184476554393768, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.241029739379883 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.5865676924586296
Loss made of: CE 0.17084716260433197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2187023162841797 EntMin 0.0
Epoch 4, Class Loss=0.19396813213825226, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.19396813213825226, Class Loss=0.19396813213825226, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/52, Loss=3.746509152650833
Loss made of: CE 0.171550914645195, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3721046447753906 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.7122648507356644
Loss made of: CE 0.16667219996452332, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.34104585647583 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.743276873230934
Loss made of: CE 0.19291453063488007, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.529313564300537 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.7285129532217978
Loss made of: CE 0.20629404485225677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8126635551452637 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.6582555666565897
Loss made of: CE 0.18081146478652954, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3562278747558594 EntMin 0.0
Epoch 5, Class Loss=0.1897832155227661, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.1897832155227661, Class Loss=0.1897832155227661, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/52, Loss=3.6125528484582903
Loss made of: CE 0.161008819937706, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.311140775680542 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.7164779752492905
Loss made of: CE 0.15968720614910126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1501662731170654 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.8942309260368346
Loss made of: CE 0.2563086748123169, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.536304235458374 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.4978924199938772
Loss made of: CE 0.17361848056316376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.335247755050659 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.6681592226028443
Loss made of: CE 0.1987770050764084, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.614017963409424 EntMin 0.0
Epoch 6, Class Loss=0.18326133489608765, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.18326133489608765, Class Loss=0.18326133489608765, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/34, Loss=5.22166086435318
Loss made of: CE 0.4561980366706848, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7685089111328125 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.677873481810093
Loss made of: CE 0.20556679368019104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.044488430023193 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.919071099162101
Loss made of: CE 0.27011382579803467, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.485076904296875 EntMin 0.0
Epoch 1, Class Loss=0.32077258825302124, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.32077258825302124, Class Loss=0.32077258825302124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/34, Loss=4.669703637063503
Loss made of: CE 0.21730773150920868, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581853866577148 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.455527441203595
Loss made of: CE 0.23317377269268036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.144082069396973 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.427940148115158
Loss made of: CE 0.1782679557800293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.36578369140625 EntMin 0.0
Epoch 2, Class Loss=0.22064954042434692, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.22064954042434692, Class Loss=0.22064954042434692, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/34, Loss=4.314349548518658
Loss made of: CE 0.22022569179534912, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.01490592956543 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.382172618806362
Loss made of: CE 0.19128531217575073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.126259803771973 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.471602565050125
Loss made of: CE 0.20810237526893616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.121338367462158 EntMin 0.0
Epoch 3, Class Loss=0.20954690873622894, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.20954690873622894, Class Loss=0.20954690873622894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/34, Loss=4.391942472755909
Loss made of: CE 0.2031846046447754, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.080506324768066 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.401785562932491
Loss made of: CE 0.20265649259090424, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9716925621032715 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.450554971396923
Loss made of: CE 0.2954075336456299, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.638256072998047 EntMin 0.0
Epoch 4, Class Loss=0.20621608197689056, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.20621608197689056, Class Loss=0.20621608197689056, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/34, Loss=4.221934266388416
Loss made of: CE 0.17498299479484558, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6863417625427246 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.3669172614812855
Loss made of: CE 0.26731330156326294, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.310364723205566 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.1068180203437805
Loss made of: CE 0.1851201355457306, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9783520698547363 EntMin 0.0
Epoch 5, Class Loss=0.19815748929977417, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.19815748929977417, Class Loss=0.19815748929977417, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/34, Loss=4.245903287827969
Loss made of: CE 0.21380439400672913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.132575035095215 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.197387161850929
Loss made of: CE 0.21488675475120544, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.102917194366455 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.258750250935554
Loss made of: CE 0.15939633548259735, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6400041580200195 EntMin 0.0
Epoch 6, Class Loss=0.19089514017105103, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.19089514017105103, Class Loss=0.19089514017105103, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=6.325947853922844
Loss made of: CE 0.4288163185119629, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.129979610443115 EntMin 0.0
Epoch 1, Batch 20/33, Loss=5.373874178528785
Loss made of: CE 0.5309879779815674, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.131669044494629 EntMin 0.0
Epoch 1, Batch 30/33, Loss=4.9590999111533165
Loss made of: CE 0.25133079290390015, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.441767692565918 EntMin 0.0
Epoch 1, Class Loss=0.46481290459632874, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.46481290459632874, Class Loss=0.46481290459632874, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/33, Loss=4.687157997488976
Loss made of: CE 0.27066725492477417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.245623588562012 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.539318650960922
Loss made of: CE 0.20781509578227997, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2207818031311035 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.305295205116272
Loss made of: CE 0.2627844512462616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.220700263977051 EntMin 0.0
Epoch 2, Class Loss=0.23688381910324097, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.23688381910324097, Class Loss=0.23688381910324097, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/33, Loss=4.289351791143417
Loss made of: CE 0.2259349524974823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590188026428223 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.234776817262173
Loss made of: CE 0.17555326223373413, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.03981876373291 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.078880457580089
Loss made of: CE 0.19871054589748383, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7842531204223633 EntMin 0.0
Epoch 3, Class Loss=0.20493847131729126, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.20493847131729126, Class Loss=0.20493847131729126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/33, Loss=4.000387574732303
Loss made of: CE 0.16246704757213593, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7285125255584717 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.083457916975021
Loss made of: CE 0.20653998851776123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.08475923538208 EntMin 0.0
Epoch 4, Batch 30/33, Loss=3.9839028492569923
Loss made of: CE 0.18403232097625732, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5810370445251465 EntMin 0.0
Epoch 4, Class Loss=0.186837300658226, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.186837300658226, Class Loss=0.186837300658226, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/33, Loss=4.049686208367348
Loss made of: CE 0.19167017936706543, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5154502391815186 EntMin 0.0
Epoch 5, Batch 20/33, Loss=3.934512434899807
Loss made of: CE 0.20305106043815613, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5138823986053467 EntMin 0.0
Epoch 5, Batch 30/33, Loss=3.842027318477631
Loss made of: CE 0.2091808170080185, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7940616607666016 EntMin 0.0
Epoch 5, Class Loss=0.18447676301002502, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.18447676301002502, Class Loss=0.18447676301002502, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/33, Loss=3.9077733397483825
Loss made of: CE 0.20647959411144257, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.792722463607788 EntMin 0.0
Epoch 6, Batch 20/33, Loss=3.8627385601401327
Loss made of: CE 0.1442168653011322, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.485283613204956 EntMin 0.0
Epoch 6, Batch 30/33, Loss=3.7371373191475867
Loss made of: CE 0.19284825026988983, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8628406524658203 EntMin 0.0
Epoch 6, Class Loss=0.18534573912620544, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.18534573912620544, Class Loss=0.18534573912620544, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/34, Loss=5.153417807817459
Loss made of: CE 0.47638076543807983, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.873330116271973 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.9655534222722055
Loss made of: CE 0.29462873935699463, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.701645851135254 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.758881315588951
Loss made of: CE 0.22595921158790588, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4750165939331055 EntMin 0.0
Epoch 1, Class Loss=0.3181205689907074, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.3181205689907074, Class Loss=0.3181205689907074, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/34, Loss=4.686309008300304
Loss made of: CE 0.21322393417358398, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.321025848388672 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.466590604186058
Loss made of: CE 0.22912947833538055, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.592504024505615 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.788142794370652
Loss made of: CE 0.27412232756614685, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.168656826019287 EntMin 0.0
Epoch 2, Class Loss=0.233427032828331, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.233427032828331, Class Loss=0.233427032828331, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/34, Loss=4.306617629528046
Loss made of: CE 0.19648846983909607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0407023429870605 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.432201635837555
Loss made of: CE 0.20707669854164124, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.082536697387695 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.539421147108078
Loss made of: CE 0.22491052746772766, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0428266525268555 EntMin 0.0
Epoch 3, Class Loss=0.20650872588157654, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.20650872588157654, Class Loss=0.20650872588157654, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/34, Loss=4.386268427968025
Loss made of: CE 0.17951762676239014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.018587589263916 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.452604113519191
Loss made of: CE 0.2188442200422287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.331449508666992 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.3882899463176726
Loss made of: CE 0.20303462445735931, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8144032955169678 EntMin 0.0
Epoch 4, Class Loss=0.19930130243301392, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.19930130243301392, Class Loss=0.19930130243301392, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/34, Loss=4.297525888681411
Loss made of: CE 0.17568789422512054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.092020034790039 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.4018984079360965
Loss made of: CE 0.21461686491966248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.095335960388184 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.313366295397282
Loss made of: CE 0.20830565690994263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421088695526123 EntMin 0.0
Epoch 5, Class Loss=0.19063489139080048, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.19063489139080048, Class Loss=0.19063489139080048, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/34, Loss=4.527777694165707
Loss made of: CE 0.20208978652954102, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.967967987060547 EntMin 0.0
Epoch 6, Batch 20/34, Loss=4.286309652030468
Loss made of: CE 0.14696082472801208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1354875564575195 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.207333487272263
Loss made of: CE 0.254022479057312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5122270584106445 EntMin 0.0
Epoch 6, Class Loss=0.19211161136627197, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.19211161136627197, Class Loss=0.19211161136627197, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2640702724456787, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.921428
Mean Acc: 0.746822
FreqW Acc: 0.860546
Mean IoU: 0.625925
Class IoU:
	class 0: 0.92074144
	class 1: 0.8333135
	class 2: 0.3394196
	class 3: 0.7209031
	class 4: 0.66781247
	class 5: 0.0
	class 6: 0.8064535
	class 7: 0.6390163
	class 8: 0.70566714
Class Acc:
	class 0: 0.96212256
	class 1: 0.8722177
	class 2: 0.62857294
	class 3: 0.7917307
	class 4: 0.8780767
	class 5: 0.0
	class 6: 0.955264
	class 7: 0.73509055
	class 8: 0.898327

federated global round: 8, step: 1
select part of clients to conduct local training
[4, 11, 9, 6]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=5.362269660830497
Loss made of: CE 0.34992560744285583, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6348161697387695 EntMin 0.0
Epoch 1, Batch 20/25, Loss=4.979115128517151
Loss made of: CE 0.2443099468946457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.341207981109619 EntMin 0.0
Epoch 1, Class Loss=0.30913838744163513, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.30913838744163513, Class Loss=0.30913838744163513, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/25, Loss=4.596537663042545
Loss made of: CE 0.25050076842308044, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.97632098197937 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.512693743407726
Loss made of: CE 0.2674576938152313, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.483671188354492 EntMin 0.0
Epoch 2, Class Loss=0.23647737503051758, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.23647737503051758, Class Loss=0.23647737503051758, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/25, Loss=4.39711449444294
Loss made of: CE 0.23838816583156586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0826735496521 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.191404244303703
Loss made of: CE 0.23462709784507751, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.043746471405029 EntMin 0.0
Epoch 3, Class Loss=0.22554004192352295, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.22554004192352295, Class Loss=0.22554004192352295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/25, Loss=4.301135297119617
Loss made of: CE 0.19451701641082764, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.168105125427246 EntMin 0.0
Epoch 4, Batch 20/25, Loss=4.106947991251945
Loss made of: CE 0.14595238864421844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.046509265899658 EntMin 0.0
Epoch 4, Class Loss=0.1970958262681961, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.1970958262681961, Class Loss=0.1970958262681961, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/25, Loss=4.10506202429533
Loss made of: CE 0.16140131652355194, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.744525671005249 EntMin 0.0
Epoch 5, Batch 20/25, Loss=4.268146102130413
Loss made of: CE 0.34525689482688904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7016448974609375 EntMin 0.0
Epoch 5, Class Loss=0.20422466099262238, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.20422466099262238, Class Loss=0.20422466099262238, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/25, Loss=3.9936809971928597
Loss made of: CE 0.16488990187644958, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9369139671325684 EntMin 0.0
Epoch 6, Batch 20/25, Loss=4.06045843064785
Loss made of: CE 0.21526716649532318, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.242269515991211 EntMin 0.0
Epoch 6, Class Loss=0.1783469319343567, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.1783469319343567, Class Loss=0.1783469319343567, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/43, Loss=4.970202594995499
Loss made of: CE 0.35012173652648926, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.601808547973633 EntMin 0.0
Epoch 1, Batch 20/43, Loss=4.4718634486198425
Loss made of: CE 0.41886502504348755, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.859877109527588 EntMin 0.0
Epoch 1, Batch 30/43, Loss=4.268198689818382
Loss made of: CE 0.24448001384735107, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.36721134185791 EntMin 0.0
Epoch 1, Batch 40/43, Loss=4.1159736007452015
Loss made of: CE 0.3001154065132141, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5416102409362793 EntMin 0.0
Epoch 1, Class Loss=0.3369734585285187, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.3369734585285187, Class Loss=0.3369734585285187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/43, Loss=3.992546533048153
Loss made of: CE 0.22236400842666626, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.572744131088257 EntMin 0.0
Epoch 2, Batch 20/43, Loss=3.8986142188310624
Loss made of: CE 0.22955471277236938, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.829496383666992 EntMin 0.0
Epoch 2, Batch 30/43, Loss=3.8576349571347235
Loss made of: CE 0.3016674518585205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.107721328735352 EntMin 0.0
Epoch 2, Batch 40/43, Loss=4.029316863417625
Loss made of: CE 0.27220648527145386, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8851747512817383 EntMin 0.0
Epoch 2, Class Loss=0.2389734387397766, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.2389734387397766, Class Loss=0.2389734387397766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/43, Loss=3.697592905163765
Loss made of: CE 0.2406688928604126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2964396476745605 EntMin 0.0
Epoch 3, Batch 20/43, Loss=3.8618013337254524
Loss made of: CE 0.2144649624824524, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8825926780700684 EntMin 0.0
Epoch 3, Batch 30/43, Loss=3.8458950102329252
Loss made of: CE 0.23483054339885712, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5479836463928223 EntMin 0.0
Epoch 3, Batch 40/43, Loss=3.7607385948300363
Loss made of: CE 0.15181222558021545, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6589083671569824 EntMin 0.0
Epoch 3, Class Loss=0.21618875861167908, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.21618875861167908, Class Loss=0.21618875861167908, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/43, Loss=3.6640505477786065
Loss made of: CE 0.19640366733074188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.87349271774292 EntMin 0.0
Epoch 4, Batch 20/43, Loss=3.693246525526047
Loss made of: CE 0.2190084159374237, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2885539531707764 EntMin 0.0
Epoch 4, Batch 30/43, Loss=3.691293501853943
Loss made of: CE 0.23658303916454315, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.450254440307617 EntMin 0.0
Epoch 4, Batch 40/43, Loss=3.652615800499916
Loss made of: CE 0.1759650707244873, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.226367950439453 EntMin 0.0
Epoch 4, Class Loss=0.19421473145484924, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.19421473145484924, Class Loss=0.19421473145484924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/43, Loss=3.6466134935617447
Loss made of: CE 0.17270642518997192, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7869343757629395 EntMin 0.0
Epoch 5, Batch 20/43, Loss=3.6281232491135595
Loss made of: CE 0.2154657542705536, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.665524959564209 EntMin 0.0
Epoch 5, Batch 30/43, Loss=3.6840537235140802
Loss made of: CE 0.17626163363456726, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5110673904418945 EntMin 0.0
Epoch 5, Batch 40/43, Loss=3.696587599813938
Loss made of: CE 0.1825815737247467, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0714564323425293 EntMin 0.0
Epoch 5, Class Loss=0.18931767344474792, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.18931767344474792, Class Loss=0.18931767344474792, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/43, Loss=3.6142229616641997
Loss made of: CE 0.20725741982460022, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.167294502258301 EntMin 0.0
Epoch 6, Batch 20/43, Loss=3.654266785085201
Loss made of: CE 0.19191467761993408, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.526901960372925 EntMin 0.0
Epoch 6, Batch 30/43, Loss=3.594872611761093
Loss made of: CE 0.1471250355243683, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1933486461639404 EntMin 0.0
Epoch 6, Batch 40/43, Loss=3.412810744345188
Loss made of: CE 0.1715478152036667, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.227506637573242 EntMin 0.0
Epoch 6, Class Loss=0.1796530783176422, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.1796530783176422, Class Loss=0.1796530783176422, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/52, Loss=3.741397759318352
Loss made of: CE 0.2466454952955246, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6401278972625732 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.043762324750423
Loss made of: CE 0.1529175341129303, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7762200832366943 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.8780715346336363
Loss made of: CE 0.10497687757015228, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.783069372177124 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.6550714701414107
Loss made of: CE 0.1666434109210968, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.407771110534668 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.740733210742474
Loss made of: CE 0.15445028245449066, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9309206008911133 EntMin 0.0
Epoch 1, Class Loss=0.16956181824207306, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.16956181824207306, Class Loss=0.16956181824207306, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/52, Loss=3.7564916118979452
Loss made of: CE 0.16212725639343262, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5706567764282227 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.8225156709551813
Loss made of: CE 0.18177101016044617, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.560486316680908 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.9289494499564173
Loss made of: CE 0.1288263201713562, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3723950386047363 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.757428497821093
Loss made of: CE 0.14721599221229553, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4157605171203613 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.795321023464203
Loss made of: CE 0.14469191431999207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0445990562438965 EntMin 0.0
Epoch 2, Class Loss=0.1624651998281479, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.1624651998281479, Class Loss=0.1624651998281479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/52, Loss=3.7712593749165535
Loss made of: CE 0.1644514501094818, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.690314292907715 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.726387080550194
Loss made of: CE 0.23471587896347046, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.440738677978516 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.7231898635625837
Loss made of: CE 0.18251900374889374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3961400985717773 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.8013090163469316
Loss made of: CE 0.14760246872901917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6958847045898438 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.8686067566275595
Loss made of: CE 0.15130239725112915, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.68776798248291 EntMin 0.0
Epoch 3, Class Loss=0.16772222518920898, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.16772222518920898, Class Loss=0.16772222518920898, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/52, Loss=3.7674955144524573
Loss made of: CE 0.1527014970779419, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5152783393859863 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.665927466750145
Loss made of: CE 0.1692918837070465, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.512070417404175 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.6226228162646295
Loss made of: CE 0.17344999313354492, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3640003204345703 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.7463140219449995
Loss made of: CE 0.14092467725276947, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.51596999168396 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.6335964888334273
Loss made of: CE 0.12079370021820068, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.830843925476074 EntMin 0.0
Epoch 4, Class Loss=0.16524861752986908, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.16524861752986908, Class Loss=0.16524861752986908, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/52, Loss=3.406929551064968
Loss made of: CE 0.16996631026268005, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3364834785461426 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.7041003540158273
Loss made of: CE 0.1527550220489502, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6053466796875 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.461477081477642
Loss made of: CE 0.17771196365356445, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5001626014709473 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.8016402795910835
Loss made of: CE 0.15465867519378662, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5605850219726562 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.811782757937908
Loss made of: CE 0.1956632435321808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.103426456451416 EntMin 0.0
Epoch 5, Class Loss=0.16817449033260345, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.16817449033260345, Class Loss=0.16817449033260345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/52, Loss=3.5941917739808558
Loss made of: CE 0.11185165494680405, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3009843826293945 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.603783466666937
Loss made of: CE 0.1840219348669052, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7927818298339844 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.610532119870186
Loss made of: CE 0.141591414809227, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3939871788024902 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.7843900620937347
Loss made of: CE 0.19688382744789124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.572512149810791 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.666543133556843
Loss made of: CE 0.15093247592449188, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6795670986175537 EntMin 0.0
Epoch 6, Class Loss=0.17258651554584503, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.17258651554584503, Class Loss=0.17258651554584503, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/34, Loss=4.726499941945076
Loss made of: CE 0.28820109367370605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.082389831542969 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.275803239643574
Loss made of: CE 0.18362560868263245, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5479724407196045 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.590416766703129
Loss made of: CE 0.21253786981105804, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.978976011276245 EntMin 0.0
Epoch 1, Class Loss=0.2505001127719879, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.2505001127719879, Class Loss=0.2505001127719879, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/34, Loss=4.3717569127678875
Loss made of: CE 0.1905834972858429, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.270847797393799 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.163622817397117
Loss made of: CE 0.19625315070152283, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9222800731658936 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.255413953959942
Loss made of: CE 0.17738334834575653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594637870788574 EntMin 0.0
Epoch 2, Class Loss=0.19691257178783417, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.19691257178783417, Class Loss=0.19691257178783417, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/34, Loss=4.105047897994519
Loss made of: CE 0.18299329280853271, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.639286518096924 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.107896256446838
Loss made of: CE 0.1632813662290573, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.736457586288452 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.239319944381714
Loss made of: CE 0.1630849689245224, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7187514305114746 EntMin 0.0
Epoch 3, Class Loss=0.1917467564344406, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.1917467564344406, Class Loss=0.1917467564344406, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/34, Loss=4.133803340792656
Loss made of: CE 0.19332218170166016, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.857107639312744 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.147565189003944
Loss made of: CE 0.19148048758506775, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.761923313140869 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.223295846581459
Loss made of: CE 0.2387567013502121, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500308036804199 EntMin 0.0
Epoch 4, Class Loss=0.19066575169563293, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.19066575169563293, Class Loss=0.19066575169563293, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/34, Loss=4.03079487234354
Loss made of: CE 0.1558290272951126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.408477306365967 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.2268393948674206
Loss made of: CE 0.2986406087875366, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.302313804626465 EntMin 0.0
Epoch 5, Batch 30/34, Loss=3.8837113320827483
Loss made of: CE 0.17872802913188934, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7459895610809326 EntMin 0.0
Epoch 5, Class Loss=0.18481381237506866, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.18481381237506866, Class Loss=0.18481381237506866, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/34, Loss=4.054510653018951
Loss made of: CE 0.2055043876171112, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8927254676818848 EntMin 0.0
Epoch 6, Batch 20/34, Loss=3.9868490517139437
Loss made of: CE 0.22575323283672333, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.071174621582031 EntMin 0.0
Epoch 6, Batch 30/34, Loss=4.153974024951458
Loss made of: CE 0.15025755763053894, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.472442388534546 EntMin 0.0
Epoch 6, Class Loss=0.18311600387096405, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.18311600387096405, Class Loss=0.18311600387096405, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2404329478740692, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.929555
Mean Acc: 0.800987
FreqW Acc: 0.875833
Mean IoU: 0.665753
Class IoU:
	class 0: 0.9260182
	class 1: 0.8148854
	class 2: 0.34833363
	class 3: 0.72293663
	class 4: 0.6522774
	class 5: 0.22009821
	class 6: 0.8811012
	class 7: 0.6672956
	class 8: 0.7588322
Class Acc:
	class 0: 0.95874727
	class 1: 0.8611579
	class 2: 0.6396263
	class 3: 0.9322616
	class 4: 0.8960712
	class 5: 0.22478661
	class 6: 0.9569957
	class 7: 0.8818327
	class 8: 0.85740536

federated global round: 9, step: 1
select part of clients to conduct local training
[5, 0, 9, 2]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/34, Loss=4.957562583684921
Loss made of: CE 0.3078352212905884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.376963138580322 EntMin 0.0
Epoch 1, Batch 20/34, Loss=4.773908241093158
Loss made of: CE 0.323097825050354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.551207542419434 EntMin 0.0
Epoch 1, Batch 30/34, Loss=4.63631177097559
Loss made of: CE 0.29020217061042786, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3478851318359375 EntMin 0.0
Epoch 1, Class Loss=0.3063352406024933, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.3063352406024933, Class Loss=0.3063352406024933, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/34, Loss=4.509374921023846
Loss made of: CE 0.2255074679851532, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1180830001831055 EntMin 0.0
Epoch 2, Batch 20/34, Loss=4.1189617842435835
Loss made of: CE 0.2009875774383545, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2550201416015625 EntMin 0.0
Epoch 2, Batch 30/34, Loss=4.562704129517078
Loss made of: CE 0.28875523805618286, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.048989295959473 EntMin 0.0
Epoch 2, Class Loss=0.21862570941448212, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.21862570941448212, Class Loss=0.21862570941448212, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/34, Loss=4.057046611607075
Loss made of: CE 0.1740134209394455, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8510284423828125 EntMin 0.0
Epoch 3, Batch 20/34, Loss=4.106240962445736
Loss made of: CE 0.22044312953948975, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.846783638000488 EntMin 0.0
Epoch 3, Batch 30/34, Loss=4.3646241769194605
Loss made of: CE 0.235430046916008, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.871974945068359 EntMin 0.0
Epoch 3, Class Loss=0.18702779710292816, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.18702779710292816, Class Loss=0.18702779710292816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/34, Loss=4.193123793601989
Loss made of: CE 0.18198733031749725, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.819143295288086 EntMin 0.0
Epoch 4, Batch 20/34, Loss=4.257764303684235
Loss made of: CE 0.21967414021492004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.305587291717529 EntMin 0.0
Epoch 4, Batch 30/34, Loss=4.145235303044319
Loss made of: CE 0.19692568480968475, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.428046941757202 EntMin 0.0
Epoch 4, Class Loss=0.19066612422466278, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.19066612422466278, Class Loss=0.19066612422466278, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/34, Loss=4.04758335351944
Loss made of: CE 0.1565099060535431, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.018920421600342 EntMin 0.0
Epoch 5, Batch 20/34, Loss=4.170948831737041
Loss made of: CE 0.20302900671958923, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.831810712814331 EntMin 0.0
Epoch 5, Batch 30/34, Loss=4.154661747813225
Loss made of: CE 0.19978106021881104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.349734306335449 EntMin 0.0
Epoch 5, Class Loss=0.1803484559059143, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.1803484559059143, Class Loss=0.1803484559059143, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/34, Loss=4.3095273047685625
Loss made of: CE 0.1934797465801239, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6002233028411865 EntMin 0.0
Epoch 6, Batch 20/34, Loss=3.9793891429901125
Loss made of: CE 0.14539003372192383, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.051327705383301 EntMin 0.0
Epoch 6, Batch 30/34, Loss=3.9608197405934336
Loss made of: CE 0.21139103174209595, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.408999443054199 EntMin 0.0
Epoch 6, Class Loss=0.18050645291805267, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.18050645291805267, Class Loss=0.18050645291805267, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/52, Loss=4.032644057273865
Loss made of: CE 0.3337630033493042, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4710559844970703 EntMin 0.0
Epoch 1, Batch 20/52, Loss=3.514423117041588
Loss made of: CE 0.2052757441997528, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.189526081085205 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.6202865153551103
Loss made of: CE 0.2145911455154419, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2046356201171875 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.710700035095215
Loss made of: CE 0.19405409693717957, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.792964458465576 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.691011780500412
Loss made of: CE 0.14649581909179688, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1698930263519287 EntMin 0.0
Epoch 1, Class Loss=0.22153569757938385, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.22153569757938385, Class Loss=0.22153569757938385, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/52, Loss=3.696754164993763
Loss made of: CE 0.25943225622177124, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.405792713165283 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.5450365588068964
Loss made of: CE 0.17129254341125488, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1543703079223633 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.8166670888662337
Loss made of: CE 0.22211991250514984, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7838656902313232 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.6429905146360397
Loss made of: CE 0.17962591350078583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.127715587615967 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.6376357287168504
Loss made of: CE 0.1925029456615448, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9831461906433105 EntMin 0.0
Epoch 2, Class Loss=0.1916969269514084, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.1916969269514084, Class Loss=0.1916969269514084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/52, Loss=3.6333267614245415
Loss made of: CE 0.15630261600017548, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5167670249938965 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.6029931262135504
Loss made of: CE 0.15969118475914001, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3526408672332764 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.683232417702675
Loss made of: CE 0.20044946670532227, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1760711669921875 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.6412218153476714
Loss made of: CE 0.17713236808776855, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3774871826171875 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.6792084142565726
Loss made of: CE 0.1904585063457489, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.311568021774292 EntMin 0.0
Epoch 3, Class Loss=0.1868412345647812, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.1868412345647812, Class Loss=0.1868412345647812, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/52, Loss=3.8328091502189636
Loss made of: CE 0.20059888064861298, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.329132080078125 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.476588676869869
Loss made of: CE 0.14713314175605774, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.343099594116211 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.5370343804359434
Loss made of: CE 0.12030148506164551, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0456933975219727 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.5846182987093926
Loss made of: CE 0.19288313388824463, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.158437728881836 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.4028851449489594
Loss made of: CE 0.1647377461194992, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2654836177825928 EntMin 0.0
Epoch 4, Class Loss=0.17791730165481567, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.17791730165481567, Class Loss=0.17791730165481567, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/52, Loss=3.585918740928173
Loss made of: CE 0.15756714344024658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3484249114990234 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.480544921755791
Loss made of: CE 0.14145582914352417, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3202762603759766 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.4879945158958434
Loss made of: CE 0.1785438060760498, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2976531982421875 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.49704482704401
Loss made of: CE 0.19927117228507996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6547346115112305 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.41417273581028
Loss made of: CE 0.18756727874279022, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0674562454223633 EntMin 0.0
Epoch 5, Class Loss=0.17439201474189758, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.17439201474189758, Class Loss=0.17439201474189758, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/52, Loss=3.382228797674179
Loss made of: CE 0.15646715462207794, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.048896551132202 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.511267361044884
Loss made of: CE 0.15801726281642914, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.16402006149292 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.610619643330574
Loss made of: CE 0.2193201780319214, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.229361057281494 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.3151463687419893
Loss made of: CE 0.19528687000274658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.13627290725708 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.4574339911341667
Loss made of: CE 0.18958646059036255, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3424508571624756 EntMin 0.0
Epoch 6, Class Loss=0.1741921603679657, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.1741921603679657, Class Loss=0.1741921603679657, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/52, Loss=3.749375730752945
Loss made of: CE 0.3113856017589569, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.495155096054077 EntMin 0.0
Epoch 1, Batch 20/52, Loss=4.088291451334953
Loss made of: CE 0.2272304892539978, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7978646755218506 EntMin 0.0
Epoch 1, Batch 30/52, Loss=3.8223083913326263
Loss made of: CE 0.1927615851163864, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.591618061065674 EntMin 0.0
Epoch 1, Batch 40/52, Loss=3.5669333681464197
Loss made of: CE 0.2296655774116516, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3124072551727295 EntMin 0.0
Epoch 1, Batch 50/52, Loss=3.704201452434063
Loss made of: CE 0.179408997297287, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8727684020996094 EntMin 0.0
Epoch 1, Class Loss=0.23328372836112976, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.23328372836112976, Class Loss=0.23328372836112976, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/52, Loss=3.7141190364956858
Loss made of: CE 0.2009657323360443, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4510788917541504 EntMin 0.0
Epoch 2, Batch 20/52, Loss=3.7565748944878576
Loss made of: CE 0.20132450759410858, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3042073249816895 EntMin 0.0
Epoch 2, Batch 30/52, Loss=3.8279298320412636
Loss made of: CE 0.16086044907569885, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.176820755004883 EntMin 0.0
Epoch 2, Batch 40/52, Loss=3.6219484880566597
Loss made of: CE 0.15095818042755127, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.261922836303711 EntMin 0.0
Epoch 2, Batch 50/52, Loss=3.6291297510266305
Loss made of: CE 0.1590387374162674, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0031304359436035 EntMin 0.0
Epoch 2, Class Loss=0.1982744336128235, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.1982744336128235, Class Loss=0.1982744336128235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/52, Loss=3.6363176852464676
Loss made of: CE 0.18623292446136475, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.455501079559326 EntMin 0.0
Epoch 3, Batch 20/52, Loss=3.7515988811850547
Loss made of: CE 0.23468604683876038, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.597314357757568 EntMin 0.0
Epoch 3, Batch 30/52, Loss=3.564485049247742
Loss made of: CE 0.18618294596672058, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.226522207260132 EntMin 0.0
Epoch 3, Batch 40/52, Loss=3.714145991206169
Loss made of: CE 0.2226252257823944, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.72841215133667 EntMin 0.0
Epoch 3, Batch 50/52, Loss=3.788343244791031
Loss made of: CE 0.167403906583786, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6375949382781982 EntMin 0.0
Epoch 3, Class Loss=0.1895352005958557, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.1895352005958557, Class Loss=0.1895352005958557, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/52, Loss=3.6972517177462576
Loss made of: CE 0.1712946742773056, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3683929443359375 EntMin 0.0
Epoch 4, Batch 20/52, Loss=3.5468704134225844
Loss made of: CE 0.20068304240703583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2986104488372803 EntMin 0.0
Epoch 4, Batch 30/52, Loss=3.6059204399585725
Loss made of: CE 0.19241556525230408, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4556994438171387 EntMin 0.0
Epoch 4, Batch 40/52, Loss=3.733767357468605
Loss made of: CE 0.16223551332950592, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4570682048797607 EntMin 0.0
Epoch 4, Batch 50/52, Loss=3.45089081376791
Loss made of: CE 0.1547737717628479, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.568626880645752 EntMin 0.0
Epoch 4, Class Loss=0.1855565458536148, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.1855565458536148, Class Loss=0.1855565458536148, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/52, Loss=3.394768679141998
Loss made of: CE 0.16661787033081055, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.299816608428955 EntMin 0.0
Epoch 5, Batch 20/52, Loss=3.680430807173252
Loss made of: CE 0.15675917267799377, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6024723052978516 EntMin 0.0
Epoch 5, Batch 30/52, Loss=3.3521025657653807
Loss made of: CE 0.200367733836174, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.208367347717285 EntMin 0.0
Epoch 5, Batch 40/52, Loss=3.830189324915409
Loss made of: CE 0.17947250604629517, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6774661540985107 EntMin 0.0
Epoch 5, Batch 50/52, Loss=3.7096247121691706
Loss made of: CE 0.2102125585079193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1959004402160645 EntMin 0.0
Epoch 5, Class Loss=0.1808123141527176, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.1808123141527176, Class Loss=0.1808123141527176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/52, Loss=3.522281302511692
Loss made of: CE 0.1410893201828003, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.274073362350464 EntMin 0.0
Epoch 6, Batch 20/52, Loss=3.5141163572669027
Loss made of: CE 0.15670526027679443, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7176482677459717 EntMin 0.0
Epoch 6, Batch 30/52, Loss=3.4686591193079948
Loss made of: CE 0.13785891234874725, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1165549755096436 EntMin 0.0
Epoch 6, Batch 40/52, Loss=3.642963533103466
Loss made of: CE 0.20261189341545105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4819769859313965 EntMin 0.0
Epoch 6, Batch 50/52, Loss=3.528296537697315
Loss made of: CE 0.1422399878501892, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4256160259246826 EntMin 0.0
Epoch 6, Class Loss=0.17713598906993866, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.17713598906993866, Class Loss=0.17713598906993866, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=4.764838342368603
Loss made of: CE 0.24883419275283813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.222320079803467 EntMin 0.0
Epoch 1, Batch 20/25, Loss=4.339957006275654
Loss made of: CE 0.2806572914123535, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.317971229553223 EntMin 0.0
Epoch 1, Class Loss=0.21728648245334625, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.21728648245334625, Class Loss=0.21728648245334625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/25, Loss=4.142557866871357
Loss made of: CE 0.17399214208126068, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.658067226409912 EntMin 0.0
Epoch 2, Batch 20/25, Loss=4.17988429069519
Loss made of: CE 0.12650549411773682, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.010074138641357 EntMin 0.0
Epoch 2, Class Loss=0.17676791548728943, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.17676791548728943, Class Loss=0.17676791548728943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/25, Loss=3.9935721814632417
Loss made of: CE 0.13977302610874176, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8305368423461914 EntMin 0.0
Epoch 3, Batch 20/25, Loss=4.211737148463726
Loss made of: CE 0.14974325895309448, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6745429039001465 EntMin 0.0
Epoch 3, Class Loss=0.1739053726196289, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.1739053726196289, Class Loss=0.1739053726196289, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/25, Loss=4.018631805479527
Loss made of: CE 0.16800324618816376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.547163486480713 EntMin 0.0
Epoch 4, Batch 20/25, Loss=3.9973241701722144
Loss made of: CE 0.14366522431373596, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4370455741882324 EntMin 0.0
Epoch 4, Class Loss=0.17121165990829468, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.17121165990829468, Class Loss=0.17121165990829468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/25, Loss=3.909280086308718
Loss made of: CE 0.1225220188498497, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5285186767578125 EntMin 0.0
Epoch 5, Batch 20/25, Loss=3.9376384034752845
Loss made of: CE 0.17407771944999695, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5540871620178223 EntMin 0.0
Epoch 5, Class Loss=0.16792993247509003, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.16792993247509003, Class Loss=0.16792993247509003, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/25, Loss=3.867136190831661
Loss made of: CE 0.22835677862167358, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.713113784790039 EntMin 0.0
Epoch 6, Batch 20/25, Loss=3.9073003247380256
Loss made of: CE 0.17836353182792664, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7601096630096436 EntMin 0.0
Epoch 6, Class Loss=0.1798693686723709, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.1798693686723709, Class Loss=0.1798693686723709, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2398328334093094, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.928608
Mean Acc: 0.811208
FreqW Acc: 0.875771
Mean IoU: 0.672885
Class IoU:
	class 0: 0.9256383
	class 1: 0.8073953
	class 2: 0.35158184
	class 3: 0.7326063
	class 4: 0.65238655
	class 5: 0.3142065
	class 6: 0.88296056
	class 7: 0.651852
	class 8: 0.73734164
Class Acc:
	class 0: 0.9517388
	class 1: 0.8509135
	class 2: 0.64410007
	class 3: 0.8331095
	class 4: 0.9058269
	class 5: 0.32531193
	class 6: 0.96014315
	class 7: 0.9053954
	class 8: 0.9243347

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[0, 15, 1, 4]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=7.40373742878437
Loss made of: CE 0.48472532629966736, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.588084697723389 EntMin 0.0
Epoch 1, Class Loss=0.6424562335014343, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.6424562335014343, Class Loss=0.6424562335014343, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=5.981295847892762
Loss made of: CE 0.6938356161117554, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.178977966308594 EntMin 0.0
Epoch 2, Class Loss=0.6939082741737366, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.6939082741737366, Class Loss=0.6939082741737366, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.601969283819199
Loss made of: CE 0.7379236221313477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.538278102874756 EntMin 0.0
Epoch 3, Class Loss=0.69548499584198, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.69548499584198, Class Loss=0.69548499584198, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.474106192588806
Loss made of: CE 0.761538028717041, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2930216789245605 EntMin 0.0
Epoch 4, Class Loss=0.7073143720626831, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.7073143720626831, Class Loss=0.7073143720626831, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.146949917078018
Loss made of: CE 0.7479286193847656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.529390335083008 EntMin 0.0
Epoch 5, Class Loss=0.6967908143997192, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.6967908143997192, Class Loss=0.6967908143997192, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.250966423749924
Loss made of: CE 0.6249213218688965, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.414059638977051 EntMin 0.0
Epoch 6, Class Loss=0.6698746085166931, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.6698746085166931, Class Loss=0.6698746085166931, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=7.248135888576508
Loss made of: CE 0.5694910287857056, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5076141357421875 EntMin 0.0
Epoch 1, Class Loss=0.623759388923645, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.623759388923645, Class Loss=0.623759388923645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.17472927570343
Loss made of: CE 0.6994069814682007, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.683680534362793 EntMin 0.0
Epoch 2, Class Loss=0.6833090782165527, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.6833090782165527, Class Loss=0.6833090782165527, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.650314968824387
Loss made of: CE 0.8766373991966248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.819003582000732 EntMin 0.0
Epoch 3, Class Loss=0.7155420780181885, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.7155420780181885, Class Loss=0.7155420780181885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.45377801656723
Loss made of: CE 0.7717660665512085, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.737195014953613 EntMin 0.0
Epoch 4, Class Loss=0.7205957770347595, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.7205957770347595, Class Loss=0.7205957770347595, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.245698398351669
Loss made of: CE 0.6165826320648193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.845938682556152 EntMin 0.0
Epoch 5, Class Loss=0.7147523760795593, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.7147523760795593, Class Loss=0.7147523760795593, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.208952111005783
Loss made of: CE 0.6620801687240601, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9591140747070312 EntMin 0.0
Epoch 6, Class Loss=0.6848887205123901, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.6848887205123901, Class Loss=0.6848887205123901, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=8.691041243076324
Loss made of: CE 0.5743901133537292, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.589284896850586 EntMin 0.0
Epoch 1, Batch 20/35, Loss=7.125654003024101
Loss made of: CE 0.4526859521865845, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.108573913574219 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.725139221549034
Loss made of: CE 0.42421984672546387, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.535178184509277 EntMin 0.0
Epoch 1, Class Loss=0.5336901545524597, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.5336901545524597, Class Loss=0.5336901545524597, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/35, Loss=6.319811671972275
Loss made of: CE 0.5144929885864258, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.803401947021484 EntMin 0.0
Epoch 2, Batch 20/35, Loss=6.346828830242157
Loss made of: CE 0.5682644844055176, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.219091415405273 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.984079962968826
Loss made of: CE 0.3033284842967987, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.880514144897461 EntMin 0.0
Epoch 2, Class Loss=0.49744415283203125, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.49744415283203125, Class Loss=0.49744415283203125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/35, Loss=5.799397161602974
Loss made of: CE 0.4593566358089447, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.116366386413574 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.981002077460289
Loss made of: CE 0.3868539035320282, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.544581413269043 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.719816863536835
Loss made of: CE 0.42682260274887085, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.670068264007568 EntMin 0.0
Epoch 3, Class Loss=0.48001906275749207, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.48001906275749207, Class Loss=0.48001906275749207, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/35, Loss=5.61098346710205
Loss made of: CE 0.400042861700058, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.483943939208984 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.504992637038231
Loss made of: CE 0.36777275800704956, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.947258949279785 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.670348119735718
Loss made of: CE 0.42723047733306885, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1613450050354 EntMin 0.0
Epoch 4, Class Loss=0.42346158623695374, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.42346158623695374, Class Loss=0.42346158623695374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/35, Loss=5.4649580925703045
Loss made of: CE 0.3397829532623291, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.907987594604492 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.586477455496788
Loss made of: CE 0.41450080275535583, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.288137435913086 EntMin 0.0
Epoch 5, Batch 30/35, Loss=5.046782663464546
Loss made of: CE 0.274330198764801, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4788360595703125 EntMin 0.0
Epoch 5, Class Loss=0.3771252930164337, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.3771252930164337, Class Loss=0.3771252930164337, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/35, Loss=5.161608016490936
Loss made of: CE 0.27022606134414673, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.797111511230469 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.254911296069622
Loss made of: CE 0.3549409806728363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.84076452255249 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.458216017484665
Loss made of: CE 0.4102960526943207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.659584045410156 EntMin 0.0
Epoch 6, Class Loss=0.35362377762794495, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.35362377762794495, Class Loss=0.35362377762794495, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=7.293325605988502
Loss made of: CE 0.7165330648422241, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.219638347625732 EntMin 0.0
Epoch 1, Class Loss=0.6216577291488647, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.6216577291488647, Class Loss=0.6216577291488647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=6.119444584846496
Loss made of: CE 0.6191028952598572, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.155844688415527 EntMin 0.0
Epoch 2, Class Loss=0.6738888621330261, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.6738888621330261, Class Loss=0.6738888621330261, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=5.767881715297699
Loss made of: CE 0.6184055209159851, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3941545486450195 EntMin 0.0
Epoch 3, Class Loss=0.7156148552894592, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.7156148552894592, Class Loss=0.7156148552894592, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=5.417411601543426
Loss made of: CE 0.7278081178665161, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.403257369995117 EntMin 0.0
Epoch 4, Class Loss=0.7118327021598816, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.7118327021598816, Class Loss=0.7118327021598816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=5.337957698106766
Loss made of: CE 0.7786971926689148, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.472433567047119 EntMin 0.0
Epoch 5, Class Loss=0.7088586688041687, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.7088586688041687, Class Loss=0.7088586688041687, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=5.238825082778931
Loss made of: CE 0.6133657693862915, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4953155517578125 EntMin 0.0
Epoch 6, Class Loss=0.6812936067581177, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6812936067581177, Class Loss=0.6812936067581177, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5911681056022644, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.854657
Mean Acc: 0.469671
FreqW Acc: 0.746845
Mean IoU: 0.375776
Class IoU:
	class 0: 0.8715057
	class 1: 0.44165868
	class 2: 0.2039789
	class 3: 0.4659861
	class 4: 0.61118317
	class 5: 0.31547543
	class 6: 0.86096
	class 7: 0.6201485
	class 8: 0.49400997
	class 9: 0.0
	class 10: 0.00017668986
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.97537667
	class 1: 0.44367358
	class 2: 0.2798568
	class 3: 0.5721087
	class 4: 0.7843515
	class 5: 0.3262713
	class 6: 0.9430691
	class 7: 0.88285226
	class 8: 0.8979919
	class 9: 0.0
	class 10: 0.00017668986
	class 11: 0.0
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[1, 13, 11, 12]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/35, Loss=7.523778754472732
Loss made of: CE 0.897260844707489, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.684335708618164 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.470960575342178
Loss made of: CE 0.6705161333084106, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.123431205749512 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.230230665206909
Loss made of: CE 0.6807655692100525, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.667932510375977 EntMin 0.0
Epoch 1, Class Loss=0.7973071932792664, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.7973071932792664, Class Loss=0.7973071932792664, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/35, Loss=5.843761450052261
Loss made of: CE 0.5024652481079102, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.09647798538208 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.929287379980087
Loss made of: CE 0.6061567664146423, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.805366039276123 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.649994125962257
Loss made of: CE 0.382162868976593, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.364306449890137 EntMin 0.0
Epoch 2, Class Loss=0.4922802448272705, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.4922802448272705, Class Loss=0.4922802448272705, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/35, Loss=5.328524354100227
Loss made of: CE 0.4096529483795166, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.963654041290283 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.600481796264648
Loss made of: CE 0.3341696858406067, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.359752178192139 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.4025157421827315
Loss made of: CE 0.38023829460144043, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.338997840881348 EntMin 0.0
Epoch 3, Class Loss=0.3821299076080322, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3821299076080322, Class Loss=0.3821299076080322, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/35, Loss=5.346069252490997
Loss made of: CE 0.3151725232601166, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.058444976806641 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.123776713013649
Loss made of: CE 0.2942056357860565, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.752618789672852 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.312845480442047
Loss made of: CE 0.31137320399284363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.78248929977417 EntMin 0.0
Epoch 4, Class Loss=0.33393338322639465, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.33393338322639465, Class Loss=0.33393338322639465, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/35, Loss=5.23054472208023
Loss made of: CE 0.2765749990940094, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664440631866455 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.311090323328972
Loss made of: CE 0.32407066226005554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.954028129577637 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.80621183514595
Loss made of: CE 0.23448622226715088, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.443573951721191 EntMin 0.0
Epoch 5, Class Loss=0.300893634557724, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.300893634557724, Class Loss=0.300893634557724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/35, Loss=4.878056548535824
Loss made of: CE 0.21846482157707214, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758420467376709 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.081253857910633
Loss made of: CE 0.2505147457122803, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3004679679870605 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.2304354652762415
Loss made of: CE 0.3800992965698242, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.355006217956543 EntMin 0.0
Epoch 6, Class Loss=0.2867231070995331, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.2867231070995331, Class Loss=0.2867231070995331, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.702589139342308
Loss made of: CE 0.28215622901916504, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.488776683807373 EntMin 0.0
Epoch 1, Class Loss=0.33204159140586853, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.33204159140586853, Class Loss=0.33204159140586853, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/19, Loss=4.90772145986557
Loss made of: CE 0.4146634042263031, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8983893394470215 EntMin 0.0
Epoch 2, Class Loss=0.44375500082969666, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.44375500082969666, Class Loss=0.44375500082969666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/19, Loss=4.824678972363472
Loss made of: CE 0.4604215621948242, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.402243614196777 EntMin 0.0
Epoch 3, Class Loss=0.4739614725112915, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.4739614725112915, Class Loss=0.4739614725112915, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/19, Loss=4.87784765958786
Loss made of: CE 0.5779899954795837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.180250644683838 EntMin 0.0
Epoch 4, Class Loss=0.49028292298316956, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.49028292298316956, Class Loss=0.49028292298316956, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/19, Loss=4.879902479052544
Loss made of: CE 0.41353246569633484, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6462273597717285 EntMin 0.0
Epoch 5, Class Loss=0.48386821150779724, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.48386821150779724, Class Loss=0.48386821150779724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/19, Loss=4.704469686746597
Loss made of: CE 0.4858059883117676, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.285879135131836 EntMin 0.0
Epoch 6, Class Loss=0.48564809560775757, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.48564809560775757, Class Loss=0.48564809560775757, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=5.016875442862511
Loss made of: CE 0.38370442390441895, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.884000778198242 EntMin 0.0
Epoch 1, Batch 20/33, Loss=4.897437414526939
Loss made of: CE 0.2743320167064667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.543066024780273 EntMin 0.0
Epoch 1, Batch 30/33, Loss=4.8657296895980835
Loss made of: CE 0.21911251544952393, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4298505783081055 EntMin 0.0
Epoch 1, Class Loss=0.3271784782409668, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.3271784782409668, Class Loss=0.3271784782409668, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=5.157319489121437
Loss made of: CE 0.4795292615890503, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.335793972015381 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.737848883867263
Loss made of: CE 0.37030029296875, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.83726167678833 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.7247443944215775
Loss made of: CE 0.4410007894039154, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.504819869995117 EntMin 0.0
Epoch 2, Class Loss=0.4201366603374481, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.4201366603374481, Class Loss=0.4201366603374481, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.747754663228989
Loss made of: CE 0.3862147331237793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.453531265258789 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.87244188785553
Loss made of: CE 0.541866660118103, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.336676597595215 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.727993467450142
Loss made of: CE 0.5373716354370117, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464427947998047 EntMin 0.0
Epoch 3, Class Loss=0.44715407490730286, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.44715407490730286, Class Loss=0.44715407490730286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.729960417747497
Loss made of: CE 0.4420754909515381, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.003066062927246 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.821752333641053
Loss made of: CE 0.39610370993614197, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291713714599609 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.6929790586233135
Loss made of: CE 0.48163601756095886, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.755837917327881 EntMin 0.0
Epoch 4, Class Loss=0.4622983932495117, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4622983932495117, Class Loss=0.4622983932495117, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.613627526164055
Loss made of: CE 0.4078670144081116, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8668174743652344 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.894441726803779
Loss made of: CE 0.3955925405025482, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9786272048950195 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.5777223110198975
Loss made of: CE 0.5057541131973267, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.998621702194214 EntMin 0.0
Epoch 5, Class Loss=0.4615524709224701, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.4615524709224701, Class Loss=0.4615524709224701, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.482890039682388
Loss made of: CE 0.47388482093811035, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9878766536712646 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.510490256547928
Loss made of: CE 0.4222935438156128, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4563539028167725 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.499393486976624
Loss made of: CE 0.36581486463546753, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.409328460693359 EntMin 0.0
Epoch 6, Class Loss=0.4594305753707886, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.4594305753707886, Class Loss=0.4594305753707886, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=5.0564782589674
Loss made of: CE 0.38989004492759705, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.23423957824707 EntMin 0.0
Epoch 1, Batch 20/33, Loss=4.78577916175127
Loss made of: CE 0.3720663785934448, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5547895431518555 EntMin 0.0
Epoch 1, Batch 30/33, Loss=4.718663074076176
Loss made of: CE 0.2593131959438324, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.466996669769287 EntMin 0.0
Epoch 1, Class Loss=0.3207348585128784, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.3207348585128784, Class Loss=0.3207348585128784, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=4.761388203501701
Loss made of: CE 0.2688690423965454, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5184502601623535 EntMin 0.0
Epoch 2, Batch 20/33, Loss=4.893175694346428
Loss made of: CE 0.41524672508239746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.530825614929199 EntMin 0.0
Epoch 2, Batch 30/33, Loss=4.761771777272225
Loss made of: CE 0.4348272681236267, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9745519161224365 EntMin 0.0
Epoch 2, Class Loss=0.41224223375320435, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.41224223375320435, Class Loss=0.41224223375320435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=4.739301428198814
Loss made of: CE 0.5704667568206787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9139277935028076 EntMin 0.0
Epoch 3, Batch 20/33, Loss=4.844936138391494
Loss made of: CE 0.45444995164871216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2959794998168945 EntMin 0.0
Epoch 3, Batch 30/33, Loss=4.616368725895882
Loss made of: CE 0.4789666533470154, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.887387275695801 EntMin 0.0
Epoch 3, Class Loss=0.45675739645957947, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.45675739645957947, Class Loss=0.45675739645957947, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=4.699940767884255
Loss made of: CE 0.40648970007896423, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.248122215270996 EntMin 0.0
Epoch 4, Batch 20/33, Loss=4.6018473386764525
Loss made of: CE 0.4319869875907898, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9711170196533203 EntMin 0.0
Epoch 4, Batch 30/33, Loss=4.540720346570015
Loss made of: CE 0.5181074738502502, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.914300918579102 EntMin 0.0
Epoch 4, Class Loss=0.4590637981891632, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.4590637981891632, Class Loss=0.4590637981891632, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=4.651346480846405
Loss made of: CE 0.42498359084129333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.676652431488037 EntMin 0.0
Epoch 5, Batch 20/33, Loss=4.44171302318573
Loss made of: CE 0.6261165142059326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6207187175750732 EntMin 0.0
Epoch 5, Batch 30/33, Loss=4.4219336152076725
Loss made of: CE 0.5224549174308777, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.003512382507324 EntMin 0.0
Epoch 5, Class Loss=0.4558559060096741, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.4558559060096741, Class Loss=0.4558559060096741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=4.449219185113907
Loss made of: CE 0.487895667552948, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.981905937194824 EntMin 0.0
Epoch 6, Batch 20/33, Loss=4.620085987448692
Loss made of: CE 0.4311223030090332, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.655177116394043 EntMin 0.0
Epoch 6, Batch 30/33, Loss=4.6631603866815565
Loss made of: CE 0.41151732206344604, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5114407539367676 EntMin 0.0
Epoch 6, Class Loss=0.457079142332077, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.457079142332077, Class Loss=0.457079142332077, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.48425284028053284, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.870297
Mean Acc: 0.565324
FreqW Acc: 0.776994
Mean IoU: 0.431741
Class IoU:
	class 0: 0.89255846
	class 1: 0.43094692
	class 2: 0.30343515
	class 3: 0.51555777
	class 4: 0.58610165
	class 5: 0.29908252
	class 6: 0.86333144
	class 7: 0.61399573
	class 8: 0.5426401
	class 9: 0.0
	class 10: 0.5649094
	class 11: 7.683901e-05
	class 12: 0.0
Class Acc:
	class 0: 0.9678732
	class 1: 0.43274298
	class 2: 0.5170921
	class 3: 0.7058358
	class 4: 0.86348885
	class 5: 0.30837786
	class 6: 0.9134883
	class 7: 0.89025295
	class 8: 0.9334843
	class 9: 0.0
	class 10: 0.81650287
	class 11: 7.683901e-05
	class 12: 0.0

federated global round: 12, step: 2
select part of clients to conduct local training
[0, 16, 7, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.86626413166523
Loss made of: CE 0.6311146020889282, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.748576641082764 EntMin 0.0
Epoch 1, Class Loss=0.552009105682373, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.552009105682373, Class Loss=0.552009105682373, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.601659178733826
Loss made of: CE 0.49403899908065796, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.116684913635254 EntMin 0.0
Epoch 2, Class Loss=0.49789077043533325, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.49789077043533325, Class Loss=0.49789077043533325, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.60954210460186
Loss made of: CE 0.48428067564964294, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8227784633636475 EntMin 0.0
Epoch 3, Class Loss=0.45686912536621094, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.45686912536621094, Class Loss=0.45686912536621094, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.549768728017807
Loss made of: CE 0.5095595121383667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.691779136657715 EntMin 0.0
Epoch 4, Class Loss=0.43064048886299133, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.43064048886299133, Class Loss=0.43064048886299133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.412138426303864
Loss made of: CE 0.43797391653060913, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9772098064422607 EntMin 0.0
Epoch 5, Class Loss=0.4058999717235565, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.4058999717235565, Class Loss=0.4058999717235565, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.425109022855759
Loss made of: CE 0.40675148367881775, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.824784278869629 EntMin 0.0
Epoch 6, Class Loss=0.3821921944618225, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.3821921944618225, Class Loss=0.3821921944618225, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/42, Loss=5.321301579475403
Loss made of: CE 0.35789597034454346, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.183398723602295 EntMin 0.0
Epoch 1, Batch 20/42, Loss=5.016711714863777
Loss made of: CE 0.2456752061843872, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.962655067443848 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.173226630687713
Loss made of: CE 0.30387359857559204, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679653167724609 EntMin 0.0
Epoch 1, Batch 40/42, Loss=4.761978156864643
Loss made of: CE 0.23114094138145447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.002274513244629 EntMin 0.0
Epoch 1, Class Loss=0.3113981783390045, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.3113981783390045, Class Loss=0.3113981783390045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/42, Loss=5.038280235230923
Loss made of: CE 0.22568759322166443, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.534546375274658 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.870058472454548
Loss made of: CE 0.2298438996076584, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.932707786560059 EntMin 0.0
Epoch 2, Batch 30/42, Loss=4.760498875379563
Loss made of: CE 0.19799456000328064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.675788879394531 EntMin 0.0
Epoch 2, Batch 40/42, Loss=4.8605187207460405
Loss made of: CE 0.288665771484375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.257721900939941 EntMin 0.0
Epoch 2, Class Loss=0.28697794675827026, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.28697794675827026, Class Loss=0.28697794675827026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/42, Loss=4.6925139129161835
Loss made of: CE 0.3012586832046509, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.118424415588379 EntMin 0.0
Epoch 3, Batch 20/42, Loss=4.963472835719585
Loss made of: CE 0.16778641939163208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.927639484405518 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.638720391690731
Loss made of: CE 0.15825816988945007, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.612239360809326 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.5706969365477566
Loss made of: CE 0.16258922219276428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.607619285583496 EntMin 0.0
Epoch 3, Class Loss=0.2730531394481659, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.2730531394481659, Class Loss=0.2730531394481659, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/42, Loss=4.80833652317524
Loss made of: CE 0.3093348443508148, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2837419509887695 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.638417369127273
Loss made of: CE 0.3327760696411133, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.279064655303955 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.659289507567882
Loss made of: CE 0.23816542327404022, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9880809783935547 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.679346397519112
Loss made of: CE 0.29569825530052185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.284500598907471 EntMin 0.0
Epoch 4, Class Loss=0.298907995223999, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.298907995223999, Class Loss=0.298907995223999, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/42, Loss=4.484609231352806
Loss made of: CE 0.30935388803482056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.026046276092529 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.646237426996231
Loss made of: CE 0.2962397038936615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.213534355163574 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.676929025352001
Loss made of: CE 0.3969900608062744, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.582891464233398 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.614899080991745
Loss made of: CE 0.2823587656021118, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.074654579162598 EntMin 0.0
Epoch 5, Class Loss=0.29574650526046753, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.29574650526046753, Class Loss=0.29574650526046753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/42, Loss=4.686476247012616
Loss made of: CE 0.28758203983306885, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.820194959640503 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.467599193751812
Loss made of: CE 0.311760812997818, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.138026714324951 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.647278694808483
Loss made of: CE 0.3416707515716553, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.403242111206055 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.7698466122150425
Loss made of: CE 0.3050333559513092, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.783506870269775 EntMin 0.0
Epoch 6, Class Loss=0.30396515130996704, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.30396515130996704, Class Loss=0.30396515130996704, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/31, Loss=5.3577857747673985
Loss made of: CE 0.329897940158844, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.856303691864014 EntMin 0.0
Epoch 1, Batch 20/31, Loss=5.159122316539287
Loss made of: CE 0.2798052728176117, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.942019939422607 EntMin 0.0
Epoch 1, Batch 30/31, Loss=4.960464708507061
Loss made of: CE 0.25873997807502747, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.46980619430542 EntMin 0.0
Epoch 1, Class Loss=0.28935331106185913, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.28935331106185913, Class Loss=0.28935331106185913, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/31, Loss=4.81898335814476
Loss made of: CE 0.3560192286968231, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.343353271484375 EntMin 0.0
Epoch 2, Batch 20/31, Loss=4.623925104737282
Loss made of: CE 0.315976083278656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1047749519348145 EntMin 0.0
Epoch 2, Batch 30/31, Loss=5.0466313973069195
Loss made of: CE 0.26572567224502563, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8972880840301514 EntMin 0.0
Epoch 2, Class Loss=0.34100982546806335, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.34100982546806335, Class Loss=0.34100982546806335, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/31, Loss=4.853525367379189
Loss made of: CE 0.3449123799800873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.34902286529541 EntMin 0.0
Epoch 3, Batch 20/31, Loss=4.55733500123024
Loss made of: CE 0.44136184453964233, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.922905921936035 EntMin 0.0
Epoch 3, Batch 30/31, Loss=4.677774348855019
Loss made of: CE 0.24984782934188843, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6694936752319336 EntMin 0.0
Epoch 3, Class Loss=0.37241727113723755, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.37241727113723755, Class Loss=0.37241727113723755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/31, Loss=4.540336483716965
Loss made of: CE 0.505915105342865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.65490198135376 EntMin 0.0
Epoch 4, Batch 20/31, Loss=4.615684154629707
Loss made of: CE 0.4048226475715637, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.028655529022217 EntMin 0.0
Epoch 4, Batch 30/31, Loss=4.6867021262645725
Loss made of: CE 0.5232746601104736, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.555773735046387 EntMin 0.0
Epoch 4, Class Loss=0.3911645710468292, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.3911645710468292, Class Loss=0.3911645710468292, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/31, Loss=4.475885939598084
Loss made of: CE 0.3760561943054199, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.103704452514648 EntMin 0.0
Epoch 5, Batch 20/31, Loss=4.549806821346283
Loss made of: CE 0.35181695222854614, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8334412574768066 EntMin 0.0
Epoch 5, Batch 30/31, Loss=4.763777521252632
Loss made of: CE 0.28706657886505127, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9715278148651123 EntMin 0.0
Epoch 5, Class Loss=0.3941822946071625, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.3941822946071625, Class Loss=0.3941822946071625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/31, Loss=4.482218021154404
Loss made of: CE 0.49316152930259705, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.991119384765625 EntMin 0.0
Epoch 6, Batch 20/31, Loss=4.661039093136788
Loss made of: CE 0.3714132010936737, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.336015701293945 EntMin 0.0
Epoch 6, Batch 30/31, Loss=4.437238085269928
Loss made of: CE 0.4438099265098572, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7132070064544678 EntMin 0.0
Epoch 6, Class Loss=0.40088197588920593, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.40088197588920593, Class Loss=0.40088197588920593, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=4.800531423091888
Loss made of: CE 0.4414253234863281, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.343993186950684 EntMin 0.0
Epoch 1, Class Loss=0.5629858374595642, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.5629858374595642, Class Loss=0.5629858374595642, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=4.791420292854309
Loss made of: CE 0.47946619987487793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.262296676635742 EntMin 0.0
Epoch 2, Class Loss=0.5024873614311218, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.5024873614311218, Class Loss=0.5024873614311218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=4.641750466823578
Loss made of: CE 0.3724158704280853, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5984411239624023 EntMin 0.0
Epoch 3, Class Loss=0.4601253271102905, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.4601253271102905, Class Loss=0.4601253271102905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=4.460267055034637
Loss made of: CE 0.3864095211029053, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.686522960662842 EntMin 0.0
Epoch 4, Class Loss=0.4175078868865967, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.4175078868865967, Class Loss=0.4175078868865967, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=4.441765061020851
Loss made of: CE 0.40553903579711914, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9758658409118652 EntMin 0.0
Epoch 5, Class Loss=0.4024847447872162, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.4024847447872162, Class Loss=0.4024847447872162, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=4.454282876849175
Loss made of: CE 0.3614560067653656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8989014625549316 EntMin 0.0
Epoch 6, Class Loss=0.38599056005477905, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.38599056005477905, Class Loss=0.38599056005477905, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4556659460067749, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.866171
Mean Acc: 0.595257
FreqW Acc: 0.781609
Mean IoU: 0.444156
Class IoU:
	class 0: 0.8970612
	class 1: 0.39870605
	class 2: 0.32761815
	class 3: 0.5224075
	class 4: 0.58051616
	class 5: 0.29284078
	class 6: 0.8548155
	class 7: 0.5842696
	class 8: 0.50202924
	class 9: 0.00032048387
	class 10: 0.52651256
	class 11: 0.28421858
	class 12: 0.002707813
Class Acc:
	class 0: 0.9552816
	class 1: 0.40012026
	class 2: 0.5841002
	class 3: 0.7554989
	class 4: 0.8848277
	class 5: 0.30053058
	class 6: 0.89581776
	class 7: 0.8975215
	class 8: 0.9492057
	class 9: 0.0003204881
	class 10: 0.5706648
	class 11: 0.5417465
	class 12: 0.002707861

federated global round: 13, step: 2
select part of clients to conduct local training
[14, 13, 0, 1]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=4.294666668772697
Loss made of: CE 0.19301563501358032, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8857879638671875 EntMin 0.0
Epoch 1, Class Loss=0.18760225176811218, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.18760225176811218, Class Loss=0.18760225176811218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=4.284760826826096
Loss made of: CE 0.2586281895637512, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7150864601135254 EntMin 0.0
Epoch 2, Class Loss=0.2428046315908432, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.2428046315908432, Class Loss=0.2428046315908432, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=4.397514699399471
Loss made of: CE 0.23609738051891327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.498414039611816 EntMin 0.0
Epoch 3, Class Loss=0.2807224988937378, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.2807224988937378, Class Loss=0.2807224988937378, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=4.291148740053177
Loss made of: CE 0.29515713453292847, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8593738079071045 EntMin 0.0
Epoch 4, Class Loss=0.30855053663253784, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.30855053663253784, Class Loss=0.30855053663253784, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=4.230877101421356
Loss made of: CE 0.2672373056411743, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5748891830444336 EntMin 0.0
Epoch 5, Class Loss=0.3226950466632843, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.3226950466632843, Class Loss=0.3226950466632843, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=4.256473129987716
Loss made of: CE 0.31074488162994385, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7204813957214355 EntMin 0.0
Epoch 6, Class Loss=0.33222880959510803, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.33222880959510803, Class Loss=0.33222880959510803, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/19, Loss=4.395762333273888
Loss made of: CE 0.39154013991355896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9451513290405273 EntMin 0.0
Epoch 1, Class Loss=0.42590010166168213, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.42590010166168213, Class Loss=0.42590010166168213, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/19, Loss=4.397620701789856
Loss made of: CE 0.4338837265968323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.361486911773682 EntMin 0.0
Epoch 2, Class Loss=0.38402026891708374, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.38402026891708374, Class Loss=0.38402026891708374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/19, Loss=4.370242241024971
Loss made of: CE 0.30896395444869995, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.040508270263672 EntMin 0.0
Epoch 3, Class Loss=0.36503148078918457, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.36503148078918457, Class Loss=0.36503148078918457, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/19, Loss=4.413429528474808
Loss made of: CE 0.39092153310775757, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.896355390548706 EntMin 0.0
Epoch 4, Class Loss=0.3503066301345825, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.3503066301345825, Class Loss=0.3503066301345825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/19, Loss=4.465623342990876
Loss made of: CE 0.27183443307876587, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.359278440475464 EntMin 0.0
Epoch 5, Class Loss=0.3415434956550598, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.3415434956550598, Class Loss=0.3415434956550598, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/19, Loss=4.183582130074501
Loss made of: CE 0.3201462924480438, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.752652645111084 EntMin 0.0
Epoch 6, Class Loss=0.337394654750824, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.337394654750824, Class Loss=0.337394654750824, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.422715830802917
Loss made of: CE 0.40695273876190186, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.319839954376221 EntMin 0.0
Epoch 1, Class Loss=0.4284980297088623, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.4284980297088623, Class Loss=0.4284980297088623, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/19, Loss=4.257979324460029
Loss made of: CE 0.4719960689544678, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9732465744018555 EntMin 0.0
Epoch 2, Class Loss=0.3989652097225189, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.3989652097225189, Class Loss=0.3989652097225189, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/19, Loss=4.311632660031319
Loss made of: CE 0.44569265842437744, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6772964000701904 EntMin 0.0
Epoch 3, Class Loss=0.38471508026123047, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.38471508026123047, Class Loss=0.38471508026123047, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/19, Loss=4.355713540315628
Loss made of: CE 0.48471421003341675, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.652471542358398 EntMin 0.0
Epoch 4, Class Loss=0.37391719222068787, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.37391719222068787, Class Loss=0.37391719222068787, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/19, Loss=4.205204054713249
Loss made of: CE 0.37022751569747925, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.919067144393921 EntMin 0.0
Epoch 5, Class Loss=0.35295939445495605, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.35295939445495605, Class Loss=0.35295939445495605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/19, Loss=4.313425001502037
Loss made of: CE 0.40449628233909607, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7399954795837402 EntMin 0.0
Epoch 6, Class Loss=0.355773001909256, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.355773001909256, Class Loss=0.355773001909256, Reg Loss=0.0
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/35, Loss=7.490722960233688
Loss made of: CE 0.6906394958496094, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.080913543701172 EntMin 0.0
Epoch 1, Batch 20/35, Loss=6.368398773670196
Loss made of: CE 0.5022503733634949, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.381442546844482 EntMin 0.0
Epoch 1, Batch 30/35, Loss=6.05490905046463
Loss made of: CE 0.4340192675590515, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.643695831298828 EntMin 0.0
Epoch 1, Class Loss=0.6009343266487122, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.6009343266487122, Class Loss=0.6009343266487122, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000584
Epoch 2, Batch 10/35, Loss=5.688927119970321
Loss made of: CE 0.32580167055130005, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.291849136352539 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.752355483174324
Loss made of: CE 0.37769708037376404, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.709508895874023 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.446790260076523
Loss made of: CE 0.3021509051322937, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.497567653656006 EntMin 0.0
Epoch 2, Class Loss=0.3548855781555176, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.3548855781555176, Class Loss=0.3548855781555176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/35, Loss=5.093596398830414
Loss made of: CE 0.2886105179786682, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8055620193481445 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.437140959501266
Loss made of: CE 0.27926450967788696, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.23682975769043 EntMin 0.0
Epoch 3, Batch 30/35, Loss=5.231469403207302
Loss made of: CE 0.2977306544780731, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.131350517272949 EntMin 0.0
Epoch 3, Class Loss=0.295393168926239, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.295393168926239, Class Loss=0.295393168926239, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000487
Epoch 4, Batch 10/35, Loss=5.178640756011009
Loss made of: CE 0.22636935114860535, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.057828903198242 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.042109207808972
Loss made of: CE 0.24120816588401794, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.648813247680664 EntMin 0.0
Epoch 4, Batch 30/35, Loss=5.167096194624901
Loss made of: CE 0.25820523500442505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.956662178039551 EntMin 0.0
Epoch 4, Class Loss=0.27058273553848267, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.27058273553848267, Class Loss=0.27058273553848267, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000438
Epoch 5, Batch 10/35, Loss=5.194996197521687
Loss made of: CE 0.2522108554840088, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.519092559814453 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.180432941019535
Loss made of: CE 0.2801840901374817, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.026371955871582 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.746247451007366
Loss made of: CE 0.21688435971736908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.262646675109863 EntMin 0.0
Epoch 5, Class Loss=0.2543274164199829, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.2543274164199829, Class Loss=0.2543274164199829, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000389
Epoch 6, Batch 10/35, Loss=4.837573516368866
Loss made of: CE 0.19813522696495056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.475149631500244 EntMin 0.0
Epoch 6, Batch 20/35, Loss=5.044761961698532
Loss made of: CE 0.21880552172660828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3134918212890625 EntMin 0.0
Epoch 6, Batch 30/35, Loss=5.093719784915447
Loss made of: CE 0.28260472416877747, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.395509719848633 EntMin 0.0
Epoch 6, Class Loss=0.2507769763469696, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.2507769763469696, Class Loss=0.2507769763469696, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.42560532689094543, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.872652
Mean Acc: 0.618029
FreqW Acc: 0.791031
Mean IoU: 0.457061
Class IoU:
	class 0: 0.90210366
	class 1: 0.39052993
	class 2: 0.3184861
	class 3: 0.5466078
	class 4: 0.59083265
	class 5: 0.30852255
	class 6: 0.85018593
	class 7: 0.5881552
	class 8: 0.5765694
	class 9: 0.0045281514
	class 10: 0.5109744
	class 11: 0.28795928
	class 12: 0.066334605
Class Acc:
	class 0: 0.95430094
	class 1: 0.39161962
	class 2: 0.5566288
	class 3: 0.72502065
	class 4: 0.88515353
	class 5: 0.3167626
	class 6: 0.88847846
	class 7: 0.8920382
	class 8: 0.9309992
	class 9: 0.004537623
	class 10: 0.8902507
	class 11: 0.5319588
	class 12: 0.06662451

federated global round: 14, step: 2
select part of clients to conduct local training
[16, 9, 4, 0]
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/42, Loss=5.36171840429306
Loss made of: CE 0.6097772121429443, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2513108253479 EntMin 0.0
Epoch 1, Batch 20/42, Loss=5.075234773755073
Loss made of: CE 0.40179443359375, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.572124004364014 EntMin 0.0
Epoch 1, Batch 30/42, Loss=5.192735323309899
Loss made of: CE 0.4148198962211609, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581491470336914 EntMin 0.0
Epoch 1, Batch 40/42, Loss=4.724922448396683
Loss made of: CE 0.2732654809951782, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8839073181152344 EntMin 0.0
Epoch 1, Class Loss=0.4759513735771179, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.4759513735771179, Class Loss=0.4759513735771179, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/42, Loss=4.844631692767143
Loss made of: CE 0.27303874492645264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.266504287719727 EntMin 0.0
Epoch 2, Batch 20/42, Loss=4.638352218270302
Loss made of: CE 0.2844884395599365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.85385799407959 EntMin 0.0
Epoch 2, Batch 30/42, Loss=4.567017734050751
Loss made of: CE 0.25674742460250854, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427184104919434 EntMin 0.0
Epoch 2, Batch 40/42, Loss=4.714987328648567
Loss made of: CE 0.3560764193534851, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.849470853805542 EntMin 0.0
Epoch 2, Class Loss=0.32842016220092773, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.32842016220092773, Class Loss=0.32842016220092773, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/42, Loss=4.550093057751655
Loss made of: CE 0.296680212020874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9331166744232178 EntMin 0.0
Epoch 3, Batch 20/42, Loss=4.847635722160339
Loss made of: CE 0.25080037117004395, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8863983154296875 EntMin 0.0
Epoch 3, Batch 30/42, Loss=4.493536823987961
Loss made of: CE 0.2507874369621277, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.479530334472656 EntMin 0.0
Epoch 3, Batch 40/42, Loss=4.461526337265968
Loss made of: CE 0.2621617019176483, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.714293479919434 EntMin 0.0
Epoch 3, Class Loss=0.29778623580932617, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.29778623580932617, Class Loss=0.29778623580932617, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/42, Loss=4.58991102874279
Loss made of: CE 0.26884925365448, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.956782817840576 EntMin 0.0
Epoch 4, Batch 20/42, Loss=4.405163453519345
Loss made of: CE 0.29887259006500244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.134592056274414 EntMin 0.0
Epoch 4, Batch 30/42, Loss=4.386731059849263
Loss made of: CE 0.23863565921783447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5508980751037598 EntMin 0.0
Epoch 4, Batch 40/42, Loss=4.525422155857086
Loss made of: CE 0.2807827591896057, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.523201942443848 EntMin 0.0
Epoch 4, Class Loss=0.2918008267879486, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.2918008267879486, Class Loss=0.2918008267879486, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/42, Loss=4.237771236896515
Loss made of: CE 0.2928203046321869, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8413963317871094 EntMin 0.0
Epoch 5, Batch 20/42, Loss=4.547571510076523
Loss made of: CE 0.3071911334991455, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.942500591278076 EntMin 0.0
Epoch 5, Batch 30/42, Loss=4.583631989359856
Loss made of: CE 0.3626675605773926, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679852485656738 EntMin 0.0
Epoch 5, Batch 40/42, Loss=4.4272036224603655
Loss made of: CE 0.32375532388687134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.991140842437744 EntMin 0.0
Epoch 5, Class Loss=0.28605836629867554, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.28605836629867554, Class Loss=0.28605836629867554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/42, Loss=4.523437622189522
Loss made of: CE 0.2389901727437973, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7025716304779053 EntMin 0.0
Epoch 6, Batch 20/42, Loss=4.268836578726768
Loss made of: CE 0.3044761121273041, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8529052734375 EntMin 0.0
Epoch 6, Batch 30/42, Loss=4.519511944055557
Loss made of: CE 0.3572790026664734, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.102720260620117 EntMin 0.0
Epoch 6, Batch 40/42, Loss=4.599670150876046
Loss made of: CE 0.32773035764694214, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.621838569641113 EntMin 0.0
Epoch 6, Class Loss=0.2909024953842163, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.2909024953842163, Class Loss=0.2909024953842163, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/35, Loss=6.255135311186313
Loss made of: CE 0.3490554094314575, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.36607551574707 EntMin 0.0
Epoch 1, Batch 20/35, Loss=5.7529931396245955
Loss made of: CE 0.19419662654399872, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.165561199188232 EntMin 0.0
Epoch 1, Batch 30/35, Loss=5.38888281583786
Loss made of: CE 0.19049963355064392, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2690839767456055 EntMin 0.0
Epoch 1, Class Loss=0.20813652873039246, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.20813652873039246, Class Loss=0.20813652873039246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/35, Loss=5.538708898425102
Loss made of: CE 0.2642536163330078, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.636246681213379 EntMin 0.0
Epoch 2, Batch 20/35, Loss=5.272836078703404
Loss made of: CE 0.15750764310359955, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.75536584854126 EntMin 0.0
Epoch 2, Batch 30/35, Loss=5.11766143143177
Loss made of: CE 0.16080860793590546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.695923805236816 EntMin 0.0
Epoch 2, Class Loss=0.1909184455871582, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.1909184455871582, Class Loss=0.1909184455871582, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/35, Loss=5.235421845316887
Loss made of: CE 0.16728556156158447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.844141006469727 EntMin 0.0
Epoch 3, Batch 20/35, Loss=5.173335635662079
Loss made of: CE 0.1728021800518036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.687748432159424 EntMin 0.0
Epoch 3, Batch 30/35, Loss=4.982062324881554
Loss made of: CE 0.19205886125564575, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.382753372192383 EntMin 0.0
Epoch 3, Class Loss=0.21200735867023468, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.21200735867023468, Class Loss=0.21200735867023468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/35, Loss=5.126373043656349
Loss made of: CE 0.1856551617383957, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.091879844665527 EntMin 0.0
Epoch 4, Batch 20/35, Loss=5.116495169699192
Loss made of: CE 0.21612653136253357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.248377799987793 EntMin 0.0
Epoch 4, Batch 30/35, Loss=4.993320488929749
Loss made of: CE 0.22578756511211395, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.28167724609375 EntMin 0.0
Epoch 4, Class Loss=0.22698535025119781, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.22698535025119781, Class Loss=0.22698535025119781, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/35, Loss=4.707148836553097
Loss made of: CE 0.20977374911308289, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.786252021789551 EntMin 0.0
Epoch 5, Batch 20/35, Loss=5.017451302707196
Loss made of: CE 0.19010204076766968, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.366119384765625 EntMin 0.0
Epoch 5, Batch 30/35, Loss=4.79064146578312
Loss made of: CE 0.2789652943611145, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.396029472351074 EntMin 0.0
Epoch 5, Class Loss=0.23473574221134186, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.23473574221134186, Class Loss=0.23473574221134186, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/35, Loss=4.999517473578453
Loss made of: CE 0.24811169505119324, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.008281230926514 EntMin 0.0
Epoch 6, Batch 20/35, Loss=4.64917765557766
Loss made of: CE 0.3156391382217407, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.714397430419922 EntMin 0.0
Epoch 6, Batch 30/35, Loss=4.818885415792465
Loss made of: CE 0.19959747791290283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.588918685913086 EntMin 0.0
Epoch 6, Class Loss=0.24104268848896027, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.24104268848896027, Class Loss=0.24104268848896027, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=4.262795236706734
Loss made of: CE 0.3137689530849457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.827108860015869 EntMin 0.0
Epoch 1, Class Loss=0.37574970722198486, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.37574970722198486, Class Loss=0.37574970722198486, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/19, Loss=4.420147576928139
Loss made of: CE 0.3386651277542114, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9102790355682373 EntMin 0.0
Epoch 2, Class Loss=0.35291776061058044, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.35291776061058044, Class Loss=0.35291776061058044, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/19, Loss=4.270134368538857
Loss made of: CE 0.2872885465621948, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2956109046936035 EntMin 0.0
Epoch 3, Class Loss=0.3457108438014984, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.3457108438014984, Class Loss=0.3457108438014984, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/19, Loss=4.150277414917946
Loss made of: CE 0.3178270757198334, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4611682891845703 EntMin 0.0
Epoch 4, Class Loss=0.33487656712532043, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.33487656712532043, Class Loss=0.33487656712532043, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/19, Loss=4.123271864652634
Loss made of: CE 0.34873300790786743, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5751590728759766 EntMin 0.0
Epoch 5, Class Loss=0.332003116607666, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.332003116607666, Class Loss=0.332003116607666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/19, Loss=4.196241492033005
Loss made of: CE 0.31129881739616394, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.73720645904541 EntMin 0.0
Epoch 6, Class Loss=0.32864826917648315, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.32864826917648315, Class Loss=0.32864826917648315, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000304
Epoch 1, Batch 10/19, Loss=4.344416821002961
Loss made of: CE 0.37368959188461304, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.362433910369873 EntMin 0.0
Epoch 1, Class Loss=0.3750317096710205, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.3750317096710205, Class Loss=0.3750317096710205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000258
Epoch 2, Batch 10/19, Loss=4.16205403804779
Loss made of: CE 0.4278883934020996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8745946884155273 EntMin 0.0
Epoch 2, Class Loss=0.3675132691860199, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.3675132691860199, Class Loss=0.3675132691860199, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000211
Epoch 3, Batch 10/19, Loss=4.199073153734207
Loss made of: CE 0.39518582820892334, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.456162214279175 EntMin 0.0
Epoch 3, Class Loss=0.35655006766319275, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.35655006766319275, Class Loss=0.35655006766319275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000163
Epoch 4, Batch 10/19, Loss=4.223588219285011
Loss made of: CE 0.4522509276866913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.426114082336426 EntMin 0.0
Epoch 4, Class Loss=0.35110586881637573, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.35110586881637573, Class Loss=0.35110586881637573, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000113
Epoch 5, Batch 10/19, Loss=4.106435170769691
Loss made of: CE 0.35280632972717285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.750897169113159 EntMin 0.0
Epoch 5, Class Loss=0.3469386398792267, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.3469386398792267, Class Loss=0.3469386398792267, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000061
Epoch 6, Batch 10/19, Loss=4.262832707166671
Loss made of: CE 0.36970099806785583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5587944984436035 EntMin 0.0
Epoch 6, Class Loss=0.3525278866291046, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.3525278866291046, Class Loss=0.3525278866291046, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4018191993236542, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.891317
Mean Acc: 0.660137
FreqW Acc: 0.819786
Mean IoU: 0.522059
Class IoU:
	class 0: 0.9037079
	class 1: 0.36781716
	class 2: 0.30938122
	class 3: 0.58060414
	class 4: 0.57538044
	class 5: 0.32576874
	class 6: 0.8514067
	class 7: 0.5678582
	class 8: 0.69957036
	class 9: 0.019922694
	class 10: 0.72853315
	class 11: 0.3111152
	class 12: 0.5457003
Class Acc:
	class 0: 0.9496081
	class 1: 0.3688894
	class 2: 0.5358194
	class 3: 0.71105087
	class 4: 0.90091914
	class 5: 0.3345671
	class 6: 0.8911628
	class 7: 0.9032814
	class 8: 0.82625806
	class 9: 0.020070849
	class 10: 0.77757084
	class 11: 0.5302834
	class 12: 0.8322981

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[11, 6, 10, 7]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=6.734545508027077
Loss made of: CE 0.339479923248291, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.000810146331787 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.673072707653046
Loss made of: CE 0.3355332314968109, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.10498046875 EntMin 0.0
Epoch 1, Batch 30/102, Loss=5.8360528841614725
Loss made of: CE 0.2098383605480194, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.717391014099121 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.6326302081346515
Loss made of: CE 0.1817443072795868, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.258632183074951 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.457487659156323
Loss made of: CE 0.23929326236248016, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.512002944946289 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.408200956881046
Loss made of: CE 0.1839420050382614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.204134464263916 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.08652176707983
Loss made of: CE 0.24563342332839966, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.631499290466309 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.962911964952946
Loss made of: CE 0.2039964646100998, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.896481513977051 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.914112716913223
Loss made of: CE 0.15587042272090912, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.328337669372559 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.9547418966889385
Loss made of: CE 0.19645467400550842, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.705150127410889 EntMin 0.0
Epoch 1, Class Loss=0.2207285612821579, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.2207285612821579, Class Loss=0.2207285612821579, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=4.8577275484800335
Loss made of: CE 0.3024473786354065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.080777168273926 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.837770715355873
Loss made of: CE 0.27816805243492126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.476186752319336 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.9710317447781565
Loss made of: CE 0.2863401770591736, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.776223182678223 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.911624476313591
Loss made of: CE 0.21375195682048798, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.06723690032959 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.841018559038639
Loss made of: CE 0.19772087037563324, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.331454277038574 EntMin 0.0
Epoch 2, Batch 60/102, Loss=5.0069411367177965
Loss made of: CE 0.2770651876926422, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249318599700928 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.495485416054725
Loss made of: CE 0.1979641318321228, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.869011402130127 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.709819507598877
Loss made of: CE 0.13762471079826355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.518791675567627 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.8813257068395615
Loss made of: CE 0.29655852913856506, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.74306058883667 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.854228280484676
Loss made of: CE 0.18431709706783295, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.981208801269531 EntMin 0.0
Epoch 2, Class Loss=0.2558327615261078, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.2558327615261078, Class Loss=0.2558327615261078, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=4.788204962015152
Loss made of: CE 0.2842748165130615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.060837745666504 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.766995450854301
Loss made of: CE 0.4075791835784912, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.059924125671387 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.814171215891838
Loss made of: CE 0.2331823706626892, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.065455436706543 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.821837145090103
Loss made of: CE 0.18492859601974487, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.347586631774902 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.819146664440632
Loss made of: CE 0.2585524320602417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.147255897521973 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.626852366328239
Loss made of: CE 0.2552345097064972, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.262411117553711 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.532173982262611
Loss made of: CE 0.22720471024513245, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.461962699890137 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 80/102, Loss=4.727510476112366
Loss made of: CE 0.21486416459083557, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.656806945800781 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.786474256217479
Loss made of: CE 0.32121795415878296, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8714609146118164 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.552993105351925
Loss made of: CE 0.20179328322410583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6721973419189453 EntMin 0.0
Epoch 3, Class Loss=0.2727281153202057, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.2727281153202057, Class Loss=0.2727281153202057, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.513053318858146
Loss made of: CE 0.3632349669933319, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.177748680114746 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.781117656826973
Loss made of: CE 0.2791118025779724, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0093865394592285 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.578953845798969
Loss made of: CE 0.3724544942378998, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.172979831695557 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.637972922623158
Loss made of: CE 0.31169676780700684, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8104512691497803 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.759335899353028
Loss made of: CE 0.2501541078090668, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.34732723236084 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.839764530956745
Loss made of: CE 0.35608649253845215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.527592182159424 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.691288532316685
Loss made of: CE 0.3286113739013672, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.381681442260742 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.803332769870758
Loss made of: CE 0.34467923641204834, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.759204864501953 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.701440043747425
Loss made of: CE 0.2779996991157532, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.27426815032959 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.629155153036118
Loss made of: CE 0.27181780338287354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.821470260620117 EntMin 0.0
Epoch 4, Class Loss=0.30906206369400024, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.30906206369400024, Class Loss=0.30906206369400024, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=4.927330729365349
Loss made of: CE 0.3133349120616913, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.177487850189209 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.810089913010597
Loss made of: CE 0.2738107144832611, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.146981239318848 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.751225519180298
Loss made of: CE 0.34649062156677246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.156769275665283 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.418716664612293
Loss made of: CE 0.3965405225753784, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.019101142883301 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.726581937074661
Loss made of: CE 0.2898181080818176, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.072165489196777 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.229522335529327
Loss made of: CE 0.3903888165950775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.082669734954834 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.52722800374031
Loss made of: CE 0.301140159368515, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6783547401428223 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.341919426620007
Loss made of: CE 0.2821711003780365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.622339248657227 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.659773378074169
Loss made of: CE 0.2724423408508301, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7272841930389404 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.5851711392402645
Loss made of: CE 0.25347793102264404, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.60244607925415 EntMin 0.0
Epoch 5, Class Loss=0.3075525760650635, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.3075525760650635, Class Loss=0.3075525760650635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.662174409627914
Loss made of: CE 0.32567673921585083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.315907955169678 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.390993486344814
Loss made of: CE 0.29610544443130493, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.099230766296387 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.243626755475998
Loss made of: CE 0.2931962013244629, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084681510925293 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.788357627391815
Loss made of: CE 0.39530324935913086, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0141191482543945 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.337284910678863
Loss made of: CE 0.30644625425338745, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.956493854522705 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.48211125433445
Loss made of: CE 0.3543219268321991, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.084816932678223 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.3284106940031055
Loss made of: CE 0.25709396600723267, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.239681243896484 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.4548159256577495
Loss made of: CE 0.2776660919189453, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8378963470458984 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.418990989029408
Loss made of: CE 0.2680358588695526, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8749523162841797 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.536313298344612
Loss made of: CE 0.27597731351852417, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.838486433029175 EntMin 0.0
Epoch 6, Class Loss=0.3042418658733368, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.3042418658733368, Class Loss=0.3042418658733368, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=6.6119812451303
Loss made of: CE 0.32285231351852417, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.602808952331543 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.627805987000466
Loss made of: CE 0.19577109813690186, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.045717239379883 EntMin 0.0
Epoch 1, Class Loss=0.235767662525177, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.235767662525177, Class Loss=0.235767662525177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=5.461824056506157
Loss made of: CE 0.43803223967552185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.987558841705322 EntMin 0.0
Epoch 2, Batch 20/23, Loss=5.096037566661835
Loss made of: CE 0.41556888818740845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.585415840148926 EntMin 0.0
Epoch 2, Class Loss=0.42643314599990845, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.42643314599990845, Class Loss=0.42643314599990845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=5.051845321059227
Loss made of: CE 0.43543919920921326, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53305196762085 EntMin 0.0
Epoch 3, Batch 20/23, Loss=5.022600930929184
Loss made of: CE 0.6235120296478271, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.234775543212891 EntMin 0.0
Epoch 3, Class Loss=0.5143504738807678, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.5143504738807678, Class Loss=0.5143504738807678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.722068551182747
Loss made of: CE 0.6113366484642029, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.462851047515869 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.790300008654595
Loss made of: CE 0.5171936750411987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684150218963623 EntMin 0.0
Epoch 4, Class Loss=0.568083643913269, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.568083643913269, Class Loss=0.568083643913269, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=4.768642616271973
Loss made of: CE 0.6174034476280212, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.145022392272949 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.758958256244659
Loss made of: CE 0.600318968296051, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.517422676086426 EntMin 0.0
Epoch 5, Class Loss=0.6139248013496399, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.6139248013496399, Class Loss=0.6139248013496399, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.689865005016327
Loss made of: CE 0.6954204440116882, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2946600914001465 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.682056546211243
Loss made of: CE 0.5442265868186951, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.548096179962158 EntMin 0.0
Epoch 6, Class Loss=0.6006482839584351, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.6006482839584351, Class Loss=0.6006482839584351, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=6.798508185148239
Loss made of: CE 0.23182789981365204, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.995013236999512 EntMin 0.0
Epoch 1, Batch 20/23, Loss=5.715859517455101
Loss made of: CE 0.2662353813648224, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.468249320983887 EntMin 0.0
Epoch 1, Class Loss=0.2384103685617447, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.2384103685617447, Class Loss=0.2384103685617447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=5.3083789229393
Loss made of: CE 0.36293643712997437, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.532040596008301 EntMin 0.0
Epoch 2, Batch 20/23, Loss=5.01671679019928
Loss made of: CE 0.3868693709373474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.772208213806152 EntMin 0.0
Epoch 2, Class Loss=0.420283704996109, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.420283704996109, Class Loss=0.420283704996109, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=4.939727732539177
Loss made of: CE 0.47593316435813904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.839472770690918 EntMin 0.0
Epoch 3, Batch 20/23, Loss=5.049970877170563
Loss made of: CE 0.4419748783111572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.663090705871582 EntMin 0.0
Epoch 3, Class Loss=0.5454878807067871, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5454878807067871, Class Loss=0.5454878807067871, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=4.81354643702507
Loss made of: CE 0.623002290725708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.221829414367676 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.817471364140511
Loss made of: CE 0.5693869590759277, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.952937126159668 EntMin 0.0
Epoch 4, Class Loss=0.5780751705169678, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.5780751705169678, Class Loss=0.5780751705169678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=5.016983807086945
Loss made of: CE 0.5887844562530518, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.960634708404541 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.710422304272652
Loss made of: CE 0.6772363185882568, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.950258255004883 EntMin 0.0
Epoch 5, Class Loss=0.6053994297981262, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.6053994297981262, Class Loss=0.6053994297981262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=4.704864490032196
Loss made of: CE 0.5138598680496216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.077268123626709 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 20/23, Loss=4.710978031158447
Loss made of: CE 0.6216166615486145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.253379821777344 EntMin 0.0
Epoch 6, Class Loss=0.6177111268043518, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6177111268043518, Class Loss=0.6177111268043518, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=6.840397492051125
Loss made of: CE 0.16963446140289307, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.55342960357666 EntMin 0.0
Epoch 1, Batch 20/102, Loss=6.35113417506218
Loss made of: CE 0.254984050989151, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.251084327697754 EntMin 0.0
Epoch 1, Batch 30/102, Loss=5.783724102377891
Loss made of: CE 0.29975420236587524, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8276166915893555 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.390593065321445
Loss made of: CE 0.19184689223766327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6712846755981445 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.442681823670864
Loss made of: CE 0.2954448163509369, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.680935859680176 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.338456144928932
Loss made of: CE 0.26175692677497864, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594452857971191 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.282935045659542
Loss made of: CE 0.16173408925533295, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.527859687805176 EntMin 0.0
Epoch 1, Batch 80/102, Loss=5.05084186643362
Loss made of: CE 0.14635069668293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251707077026367 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.474013501405716
Loss made of: CE 0.18962836265563965, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.170902729034424 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.5828660562634465
Loss made of: CE 0.2674577832221985, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.376819610595703 EntMin 0.0
Epoch 1, Class Loss=0.22765818238258362, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.22765818238258362, Class Loss=0.22765818238258362, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=5.2386120617389675
Loss made of: CE 0.22139430046081543, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202829837799072 EntMin 0.0
Epoch 2, Batch 20/102, Loss=5.014021095633507
Loss made of: CE 0.2906961143016815, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.18747615814209 EntMin 0.0
Epoch 2, Batch 30/102, Loss=5.093471227586269
Loss made of: CE 0.22472146153450012, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.024937629699707 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.899106822907925
Loss made of: CE 0.18581072986125946, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.949228286743164 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.562231120467186
Loss made of: CE 0.20433162152767181, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.017142295837402 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.833607125282287
Loss made of: CE 0.28102636337280273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.676092147827148 EntMin 0.0
Epoch 2, Batch 70/102, Loss=5.212651053071022
Loss made of: CE 0.3121456503868103, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.274886608123779 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.644478058815002
Loss made of: CE 0.18546736240386963, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.800748825073242 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.598332867026329
Loss made of: CE 0.25977596640586853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.713143348693848 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.926805290579796
Loss made of: CE 0.2512356638908386, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.05704402923584 EntMin 0.0
Epoch 2, Class Loss=0.25414296984672546, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.25414296984672546, Class Loss=0.25414296984672546, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=5.139841103553772
Loss made of: CE 0.263364315032959, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5814690589904785 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.932199758291245
Loss made of: CE 0.324698269367218, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.676774740219116 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.6811788603663445
Loss made of: CE 0.32827872037887573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7209553718566895 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.6988568097352985
Loss made of: CE 0.23111854493618011, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.424127578735352 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.73804530352354
Loss made of: CE 0.23021575808525085, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.874960899353027 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.576824854314327
Loss made of: CE 0.19026564061641693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.063137054443359 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.820822367072106
Loss made of: CE 0.21639220416545868, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.921442985534668 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.434147699177265
Loss made of: CE 0.15549437701702118, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0056071281433105 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.515431480109692
Loss made of: CE 0.22040139138698578, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.004328727722168 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.509159669280052
Loss made of: CE 0.2214735448360443, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.123070240020752 EntMin 0.0
Epoch 3, Class Loss=0.2735615074634552, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.2735615074634552, Class Loss=0.2735615074634552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=4.9661917179822925
Loss made of: CE 0.32779666781425476, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358702182769775 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.629375636577606
Loss made of: CE 0.3407829701900482, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.375612258911133 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.60906159132719
Loss made of: CE 0.23908475041389465, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.797313690185547 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.5321354672312735
Loss made of: CE 0.3169180154800415, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.108637809753418 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.630803894996643
Loss made of: CE 0.364190936088562, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303285598754883 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.7154357269406315
Loss made of: CE 0.2688349485397339, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.494307041168213 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.45321519523859
Loss made of: CE 0.28115975856781006, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0880045890808105 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.584417049586773
Loss made of: CE 0.2985767126083374, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.158924579620361 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.713023328781128
Loss made of: CE 0.29902416467666626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.701262474060059 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.87799651324749
Loss made of: CE 0.2303023785352707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1741414070129395 EntMin 0.0
Epoch 4, Class Loss=0.3062168061733246, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.3062168061733246, Class Loss=0.3062168061733246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/102, Loss=4.396755364537239
Loss made of: CE 0.30810317397117615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.917069435119629 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.871100872755051
Loss made of: CE 0.26725032925605774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.538186550140381 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.766009053587913
Loss made of: CE 0.3401775360107422, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.16326379776001 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.593660566210747
Loss made of: CE 0.2765718102455139, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.902404308319092 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.368769350647926
Loss made of: CE 0.31209689378738403, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.12202262878418 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.514513888955117
Loss made of: CE 0.2773214280605316, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.52138614654541 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.6387159585952755
Loss made of: CE 0.24805352091789246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.526616096496582 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.650004786252976
Loss made of: CE 0.22407488524913788, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.086198329925537 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.4787536531686785
Loss made of: CE 0.38837966322898865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.079442024230957 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.332221135497093
Loss made of: CE 0.31532523036003113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.770010948181152 EntMin 0.0
Epoch 5, Class Loss=0.306393027305603, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.306393027305603, Class Loss=0.306393027305603, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=4.433920058608055
Loss made of: CE 0.3410295248031616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5754547119140625 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.862839993834496
Loss made of: CE 0.29780471324920654, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6699788570404053 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.093507108092308
Loss made of: CE 0.26518985629081726, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.587623119354248 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.475305831432342
Loss made of: CE 0.3037911653518677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.790283203125 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.595943282544613
Loss made of: CE 0.3275013267993927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.093644142150879 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.668334598839283
Loss made of: CE 0.2247273176908493, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9603590965270996 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.464761674404144
Loss made of: CE 0.2678508162498474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.032669544219971 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.354895743727684
Loss made of: CE 0.22971603274345398, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.214155673980713 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.5108041048049925
Loss made of: CE 0.2632250487804413, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.271688938140869 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.476547227799893
Loss made of: CE 0.3771383762359619, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.926804304122925 EntMin 0.0
Epoch 6, Class Loss=0.30177149176597595, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.30177149176597595, Class Loss=0.30177149176597595, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6355210542678833, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.822586
Mean Acc: 0.507824
FreqW Acc: 0.704285
Mean IoU: 0.375147
Class IoU:
	class 0: 0.83709097
	class 1: 0.24952458
	class 2: 0.23463075
	class 3: 0.249307
	class 4: 0.45691055
	class 5: 0.17846419
	class 6: 0.83334506
	class 7: 0.664668
	class 8: 0.6883123
	class 9: 0.0004021338
	class 10: 0.4714343
	class 11: 0.27758434
	class 12: 0.46962506
	class 13: 0.0
	class 14: 0.53448915
	class 15: 0.23170453
	class 16: 0.0
Class Acc:
	class 0: 0.95472026
	class 1: 0.2499509
	class 2: 0.3931818
	class 3: 0.26415643
	class 4: 0.8872293
	class 5: 0.17970082
	class 6: 0.9027146
	class 7: 0.88647157
	class 8: 0.7963072
	class 9: 0.00040213703
	class 10: 0.6976925
	class 11: 0.63038886
	class 12: 0.82036906
	class 13: 0.0
	class 14: 0.73189306
	class 15: 0.23782398
	class 16: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[7, 11, 16, 18]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=5.2324506640434265
Loss made of: CE 0.8374366164207458, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7957258224487305 EntMin 0.0
Epoch 1, Batch 20/102, Loss=5.220255821943283
Loss made of: CE 0.7396783828735352, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.03940486907959 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.970223659276963
Loss made of: CE 0.6152762174606323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3931121826171875 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.986949795484543
Loss made of: CE 0.5360581874847412, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.146337985992432 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.186594265699386
Loss made of: CE 0.6668440699577332, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.780667304992676 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.052366846799851
Loss made of: CE 0.5957021713256836, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8250679969787598 EntMin 0.0
Epoch 1, Batch 70/102, Loss=5.109612262248993
Loss made of: CE 0.5814193487167358, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421905040740967 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.992116758227349
Loss made of: CE 0.6717521548271179, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9526565074920654 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.469250196218491
Loss made of: CE 0.520587682723999, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7457046508789062 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.454251217842102
Loss made of: CE 0.5894472599029541, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.991326093673706 EntMin 0.0
Epoch 1, Class Loss=0.6216797828674316, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.6216797828674316, Class Loss=0.6216797828674316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.965143883228302
Loss made of: CE 0.5449704527854919, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7366130352020264 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.585219076275825
Loss made of: CE 0.527608335018158, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2692155838012695 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.777910506725311
Loss made of: CE 0.4506608843803406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.945856094360352 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.688183405995369
Loss made of: CE 0.5031296014785767, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.444736957550049 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.449112451076507
Loss made of: CE 0.4026472270488739, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.738178253173828 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.770967102050781
Loss made of: CE 0.47088170051574707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.391464710235596 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.905622965097427
Loss made of: CE 0.5045711398124695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.442503929138184 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.618005254864693
Loss made of: CE 0.4531165063381195, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.708829641342163 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 90/102, Loss=4.543914473056793
Loss made of: CE 0.42018699645996094, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.10491943359375 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.822394871711731
Loss made of: CE 0.41529130935668945, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6752166748046875 EntMin 0.0
Epoch 2, Class Loss=0.49875545501708984, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.49875545501708984, Class Loss=0.49875545501708984, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.8763680279254915
Loss made of: CE 0.5082768797874451, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0969038009643555 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.616293245553971
Loss made of: CE 0.44253039360046387, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4491050243377686 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.501549544930458
Loss made of: CE 0.43786224722862244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.021122932434082 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.6918493002653126
Loss made of: CE 0.4373142421245575, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421392440795898 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.539422166347504
Loss made of: CE 0.4380534887313843, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481406211853027 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.457958650588989
Loss made of: CE 0.4478721022605896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6868896484375 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.7725604444742205
Loss made of: CE 0.446128785610199, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7337288856506348 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.454789090156555
Loss made of: CE 0.3966066241264343, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.856184720993042 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.503184500336647
Loss made of: CE 0.44216427206993103, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7585625648498535 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.413001224398613
Loss made of: CE 0.38906535506248474, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8193299770355225 EntMin 0.0
Epoch 3, Class Loss=0.46416258811950684, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.46416258811950684, Class Loss=0.46416258811950684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.6686490148305895
Loss made of: CE 0.4683212637901306, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0347137451171875 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.4867337256669995
Loss made of: CE 0.4058065414428711, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.125454425811768 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.538196197152137
Loss made of: CE 0.3985595107078552, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.560723304748535 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.540561705827713
Loss made of: CE 0.38814327120780945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.817359447479248 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.472713804244995
Loss made of: CE 0.4932876527309418, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7931809425354004 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.609073093533516
Loss made of: CE 0.477799654006958, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.237600803375244 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.407699212431908
Loss made of: CE 0.41752415895462036, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9483821392059326 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.632009181380272
Loss made of: CE 0.47077780961990356, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.916236400604248 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.497157970070839
Loss made of: CE 0.4078075587749481, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.154796600341797 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.767373618483544
Loss made of: CE 0.37905943393707275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8610188961029053 EntMin 0.0
Epoch 4, Class Loss=0.44189971685409546, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.44189971685409546, Class Loss=0.44189971685409546, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.232663413882255
Loss made of: CE 0.5163233280181885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.162542343139648 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.625399509072304
Loss made of: CE 0.42483577132225037, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.863935947418213 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.755789506435394
Loss made of: CE 0.5656851530075073, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9901750087738037 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.43157417178154
Loss made of: CE 0.3905528485774994, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5576770305633545 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.375846832990646
Loss made of: CE 0.4274563789367676, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.308516979217529 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.416582936048508
Loss made of: CE 0.4068424105644226, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.512118339538574 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.5058249026536945
Loss made of: CE 0.3808509111404419, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.368264198303223 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.6133744567632675
Loss made of: CE 0.3936779499053955, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.236196994781494 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.4124150544404985
Loss made of: CE 0.4859505593776703, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7924795150756836 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.435654819011688
Loss made of: CE 0.42259129881858826, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746798515319824 EntMin 0.0
Epoch 5, Class Loss=0.42145630717277527, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.42145630717277527, Class Loss=0.42145630717277527, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.351057779788971
Loss made of: CE 0.44427841901779175, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2895760536193848 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.683543655276298
Loss made of: CE 0.420010507106781, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.599303960800171 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.098676255345344
Loss made of: CE 0.3942902982234955, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3156185150146484 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.52518655359745
Loss made of: CE 0.39100098609924316, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.756544589996338 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.466240003705025
Loss made of: CE 0.45442432165145874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.812424421310425 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.306159698963166
Loss made of: CE 0.3380492329597473, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.945168972015381 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.371423047780991
Loss made of: CE 0.3967684507369995, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.864501714706421 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.279377260804177
Loss made of: CE 0.35027334094047546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8836750984191895 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.365199235081673
Loss made of: CE 0.4001055955886841, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9986531734466553 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.352519604563713
Loss made of: CE 0.49941810965538025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5297980308532715 EntMin 0.0
Epoch 6, Class Loss=0.4102793335914612, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.4102793335914612, Class Loss=0.4102793335914612, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=5.241355699300766
Loss made of: CE 0.7944173812866211, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.470109939575195 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.8892816543579105
Loss made of: CE 0.7134382128715515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.239676475524902 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 30/102, Loss=4.974737286567688
Loss made of: CE 0.681063175201416, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.521520614624023 EntMin 0.0
Epoch 1, Batch 40/102, Loss=5.209597653150558
Loss made of: CE 0.5927941799163818, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.583517074584961 EntMin 0.0
Epoch 1, Batch 50/102, Loss=5.1557097911834715
Loss made of: CE 0.5709441304206848, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.27397346496582 EntMin 0.0
Epoch 1, Batch 60/102, Loss=5.155748802423477
Loss made of: CE 0.6023882627487183, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.777918815612793 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.91392948627472
Loss made of: CE 0.5933805704116821, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.039303302764893 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.873922678828239
Loss made of: CE 0.5281965136528015, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.076601505279541 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.75835080742836
Loss made of: CE 0.526328444480896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.793496608734131 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.760183289647102
Loss made of: CE 0.5865180492401123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.034853458404541 EntMin 0.0
Epoch 1, Class Loss=0.6280220150947571, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.6280220150947571, Class Loss=0.6280220150947571, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=4.6207301914691925
Loss made of: CE 0.4914970397949219, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7932002544403076 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.568016797304153
Loss made of: CE 0.473909854888916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.001015663146973 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.653667837381363
Loss made of: CE 0.5965356230735779, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.224000453948975 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.79315351843834
Loss made of: CE 0.584900975227356, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.004343032836914 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.835003194212914
Loss made of: CE 0.4838125705718994, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.178549766540527 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.864523023366928
Loss made of: CE 0.5050630569458008, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.049414157867432 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.489015966653824
Loss made of: CE 0.5184723138809204, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.608870029449463 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.568786346912384
Loss made of: CE 0.527962327003479, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.157361030578613 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.7599614530801775
Loss made of: CE 0.5582262873649597, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7749738693237305 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.871807557344437
Loss made of: CE 0.430922269821167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.531034469604492 EntMin 0.0
Epoch 2, Class Loss=0.5014673471450806, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.5014673471450806, Class Loss=0.5014673471450806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=4.561200815439224
Loss made of: CE 0.5901544094085693, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9899802207946777 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.51308251619339
Loss made of: CE 0.49561595916748047, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.724595785140991 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.783733114600182
Loss made of: CE 0.4514997899532318, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.845120429992676 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.592419311404228
Loss made of: CE 0.4567660689353943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9549942016601562 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.655959120392799
Loss made of: CE 0.44681495428085327, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9651260375976562 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.543198353052139
Loss made of: CE 0.4150967001914978, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.081216335296631 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.536528387665749
Loss made of: CE 0.3590770363807678, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.945070266723633 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.754655742645264
Loss made of: CE 0.46415144205093384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581403732299805 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.722900977730751
Loss made of: CE 0.370292603969574, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7024831771850586 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.564409282803536
Loss made of: CE 0.3895401358604431, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7811191082000732 EntMin 0.0
Epoch 3, Class Loss=0.45739176869392395, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.45739176869392395, Class Loss=0.45739176869392395, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=4.524390611052513
Loss made of: CE 0.4378926753997803, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8695225715637207 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.611470440030098
Loss made of: CE 0.36195048689842224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.341872692108154 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.487487548589707
Loss made of: CE 0.5665508508682251, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.001573085784912 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.57510078549385
Loss made of: CE 0.38769644498825073, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.757093906402588 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.724020600318909
Loss made of: CE 0.5067354440689087, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.386558532714844 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.671645990014076
Loss made of: CE 0.40845808386802673, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.289158821105957 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.56960341334343
Loss made of: CE 0.551896333694458, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.223090648651123 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.632890722155571
Loss made of: CE 0.5758368372917175, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.398835182189941 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.5986409962177275
Loss made of: CE 0.3695921301841736, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.890536308288574 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.648641309142112
Loss made of: CE 0.5115700960159302, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.986451148986816 EntMin 0.0
Epoch 4, Class Loss=0.4405105710029602, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4405105710029602, Class Loss=0.4405105710029602, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=4.798585996031761
Loss made of: CE 0.46549177169799805, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7002837657928467 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.69380204975605
Loss made of: CE 0.4371442496776581, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.023039817810059 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.564185896515847
Loss made of: CE 0.36076900362968445, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7396984100341797 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.312489420175552
Loss made of: CE 0.5065819025039673, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.813877582550049 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.484737068414688
Loss made of: CE 0.3852001428604126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.319082260131836 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.2246204853057865
Loss made of: CE 0.44528859853744507, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7822279930114746 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.305948281288147
Loss made of: CE 0.4891134202480316, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6033763885498047 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.358939963579178
Loss made of: CE 0.3437996804714203, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.127440929412842 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.355023962259293
Loss made of: CE 0.48381543159484863, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.501270055770874 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.404795953631401
Loss made of: CE 0.3902962803840637, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.209346771240234 EntMin 0.0
Epoch 5, Class Loss=0.41664159297943115, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.41664159297943115, Class Loss=0.41664159297943115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=4.706470501422882
Loss made of: CE 0.34580230712890625, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.054008960723877 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.239247477054596
Loss made of: CE 0.41714680194854736, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6705803871154785 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.119578951597214
Loss made of: CE 0.3866884112358093, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.933858871459961 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.669476360082626
Loss made of: CE 0.49329280853271484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.949434280395508 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.284269580245018
Loss made of: CE 0.3746662139892578, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.695594549179077 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.5453137278556826
Loss made of: CE 0.45848140120506287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.03730583190918 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.159709027409553
Loss made of: CE 0.3925984799861908, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9814329147338867 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.3660693764686584
Loss made of: CE 0.40934133529663086, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7693328857421875 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.389526385068893
Loss made of: CE 0.3814648389816284, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5814645290374756 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.501820477843284
Loss made of: CE 0.4195936322212219, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.85410737991333 EntMin 0.0
Epoch 6, Class Loss=0.4064384996891022, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.4064384996891022, Class Loss=0.4064384996891022, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/105, Loss=4.884562013670802
Loss made of: CE 0.23474299907684326, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.36030912399292 EntMin 0.0
Epoch 1, Batch 20/105, Loss=4.426013703271747
Loss made of: CE 0.09302636235952377, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.650886058807373 EntMin 0.0
Epoch 1, Batch 30/105, Loss=4.197925106436014
Loss made of: CE 0.08607536554336548, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.356144905090332 EntMin 0.0
Epoch 1, Batch 40/105, Loss=4.469019140303135
Loss made of: CE 0.07085709273815155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7316155433654785 EntMin 0.0
Epoch 1, Batch 50/105, Loss=4.309136171638966
Loss made of: CE 0.12978509068489075, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9700722694396973 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.356888018548489
Loss made of: CE 0.1298723816871643, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.158432960510254 EntMin 0.0
Epoch 1, Batch 70/105, Loss=4.3351214624941345
Loss made of: CE 0.11117182672023773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.414741039276123 EntMin 0.0
Epoch 1, Batch 80/105, Loss=4.627127069607377
Loss made of: CE 0.08266428858041763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.883146286010742 EntMin 0.0
Epoch 1, Batch 90/105, Loss=4.529030492529273
Loss made of: CE 0.06856876611709595, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.39483642578125 EntMin 0.0
Epoch 1, Batch 100/105, Loss=4.291576855629683
Loss made of: CE 0.07517731189727783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.396520137786865 EntMin 0.0
Epoch 1, Class Loss=0.0966845229268074, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.0966845229268074, Class Loss=0.0966845229268074, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/105, Loss=4.217372538149357
Loss made of: CE 0.1960296630859375, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.950948476791382 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.3418956190347675
Loss made of: CE 0.16371725499629974, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.049357891082764 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.454746349900961
Loss made of: CE 0.13040053844451904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.701490879058838 EntMin 0.0
Epoch 2, Batch 40/105, Loss=4.404769592732191
Loss made of: CE 0.17001883685588837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3412041664123535 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.221647798269987
Loss made of: CE 0.11214008182287216, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6613149642944336 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 60/105, Loss=4.3879894755780695
Loss made of: CE 0.115696482360363, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8576793670654297 EntMin 0.0
Epoch 2, Batch 70/105, Loss=4.7270695060491565
Loss made of: CE 0.11749138683080673, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.871509075164795 EntMin 0.0
Epoch 2, Batch 80/105, Loss=4.42820310741663
Loss made of: CE 0.12467216700315475, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202024459838867 EntMin 0.0
Epoch 2, Batch 90/105, Loss=4.6419988259673115
Loss made of: CE 0.10666251182556152, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.607851982116699 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.307176385819912
Loss made of: CE 0.10294739902019501, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9082679748535156 EntMin 0.0
Epoch 2, Class Loss=0.14749877154827118, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.14749877154827118, Class Loss=0.14749877154827118, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/105, Loss=4.342753136903047
Loss made of: CE 0.12442559003829956, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9543838500976562 EntMin 0.0
Epoch 3, Batch 20/105, Loss=4.5836624398827555
Loss made of: CE 0.18484525382518768, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.42121696472168 EntMin 0.0
Epoch 3, Batch 30/105, Loss=4.2769911482930185
Loss made of: CE 0.16829954087734222, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.224125862121582 EntMin 0.0
Epoch 3, Batch 40/105, Loss=4.620702639222145
Loss made of: CE 0.2070133090019226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.094301223754883 EntMin 0.0
Epoch 3, Batch 50/105, Loss=4.313361638784409
Loss made of: CE 0.1630958914756775, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.933238983154297 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.442213851213455
Loss made of: CE 0.20898845791816711, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8214423656463623 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.300237163901329
Loss made of: CE 0.1887701004743576, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7399134635925293 EntMin 0.0
Epoch 3, Batch 80/105, Loss=4.43306569904089
Loss made of: CE 0.24195611476898193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.189108371734619 EntMin 0.0
Epoch 3, Batch 90/105, Loss=4.275611207634211
Loss made of: CE 0.16593338549137115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.11283016204834 EntMin 0.0
Epoch 3, Batch 100/105, Loss=4.2811259716749195
Loss made of: CE 0.16815969347953796, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.566062927246094 EntMin 0.0
Epoch 3, Class Loss=0.19447922706604004, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.19447922706604004, Class Loss=0.19447922706604004, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/105, Loss=4.1721402779221535
Loss made of: CE 0.17827336490154266, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8018388748168945 EntMin 0.0
Epoch 4, Batch 20/105, Loss=4.335215555131436
Loss made of: CE 0.21017912030220032, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.914854049682617 EntMin 0.0
Epoch 4, Batch 30/105, Loss=4.220250029861927
Loss made of: CE 0.20992964506149292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.856322765350342 EntMin 0.0
Epoch 4, Batch 40/105, Loss=4.477503195405006
Loss made of: CE 0.19696442782878876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.786207914352417 EntMin 0.0
Epoch 4, Batch 50/105, Loss=4.2859842628240585
Loss made of: CE 0.18967363238334656, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.997671604156494 EntMin 0.0
Epoch 4, Batch 60/105, Loss=4.324907593429089
Loss made of: CE 0.18198329210281372, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.902557373046875 EntMin 0.0
Epoch 4, Batch 70/105, Loss=4.399824485182762
Loss made of: CE 0.2474476546049118, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7185726165771484 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.20440798252821
Loss made of: CE 0.2711467146873474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0874176025390625 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.4105361476540565
Loss made of: CE 0.25707006454467773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.555349349975586 EntMin 0.0
Epoch 4, Batch 100/105, Loss=4.319286277890205
Loss made of: CE 0.1835954636335373, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8469839096069336 EntMin 0.0
Epoch 4, Class Loss=0.22774139046669006, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.22774139046669006, Class Loss=0.22774139046669006, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/105, Loss=4.746622107923031
Loss made of: CE 0.2627238631248474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.058274745941162 EntMin 0.0
Epoch 5, Batch 20/105, Loss=4.345415133237839
Loss made of: CE 0.24865949153900146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.052639961242676 EntMin 0.0
Epoch 5, Batch 30/105, Loss=4.277357006072998
Loss made of: CE 0.26101624965667725, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6691813468933105 EntMin 0.0
Epoch 5, Batch 40/105, Loss=4.773187930881977
Loss made of: CE 0.2505423128604889, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.917716026306152 EntMin 0.0
Epoch 5, Batch 50/105, Loss=4.2706237360835075
Loss made of: CE 0.2682422995567322, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2295026779174805 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.388785190880299
Loss made of: CE 0.31464698910713196, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8783836364746094 EntMin 0.0
Epoch 5, Batch 70/105, Loss=4.177876226603985
Loss made of: CE 0.25554049015045166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7366538047790527 EntMin 0.0
Epoch 5, Batch 80/105, Loss=4.501572622358799
Loss made of: CE 0.2553083598613739, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5728187561035156 EntMin 0.0
Epoch 5, Batch 90/105, Loss=4.412028668820858
Loss made of: CE 0.29599639773368835, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.951159954071045 EntMin 0.0
Epoch 5, Batch 100/105, Loss=4.3103671103715895
Loss made of: CE 0.30515995621681213, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.519565105438232 EntMin 0.0
Epoch 5, Class Loss=0.26375138759613037, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.26375138759613037, Class Loss=0.26375138759613037, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/105, Loss=4.294291968643665
Loss made of: CE 0.29510951042175293, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.786266326904297 EntMin 0.0
Epoch 6, Batch 20/105, Loss=4.72572755664587
Loss made of: CE 0.4570416212081909, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.244016647338867 EntMin 0.0
Epoch 6, Batch 30/105, Loss=4.252155090868473
Loss made of: CE 0.32400941848754883, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3806533813476562 EntMin 0.0
Epoch 6, Batch 40/105, Loss=4.132083903253078
Loss made of: CE 0.2542606294155121, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4389352798461914 EntMin 0.0
Epoch 6, Batch 50/105, Loss=4.228313288092613
Loss made of: CE 0.21625876426696777, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6370418071746826 EntMin 0.0
Epoch 6, Batch 60/105, Loss=4.401668813824654
Loss made of: CE 0.27956998348236084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.160650730133057 EntMin 0.0
Epoch 6, Batch 70/105, Loss=4.136276216804982
Loss made of: CE 0.25679993629455566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1936516761779785 EntMin 0.0
Epoch 6, Batch 80/105, Loss=4.547642958164215
Loss made of: CE 0.2537477910518646, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.488809585571289 EntMin 0.0
Epoch 6, Batch 90/105, Loss=4.195561307668686
Loss made of: CE 0.2709171175956726, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.45748233795166 EntMin 0.0
Epoch 6, Batch 100/105, Loss=4.2460295170545574
Loss made of: CE 0.24164412915706635, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7710587978363037 EntMin 0.0
Epoch 6, Class Loss=0.2899809777736664, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.2899809777736664, Class Loss=0.2899809777736664, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=4.234962474927306
Loss made of: CE 0.11477449536323547, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.518859386444092 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.949598837643862
Loss made of: CE 0.109843909740448, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.896029949188232 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.460732828080654
Loss made of: CE 0.0810437947511673, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.714446544647217 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.2864403881132604
Loss made of: CE 0.06890781968832016, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8448426723480225 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.119139276072383
Loss made of: CE 0.06040103733539581, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.792731761932373 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.924719630926847
Loss made of: CE 0.11814051121473312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1161789894104 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.598630861192942
Loss made of: CE 0.11885464191436768, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.639535903930664 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.276916510239244
Loss made of: CE 0.07638770341873169, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9215400218963623 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.466748241335154
Loss made of: CE 0.09661390632390976, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6338438987731934 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.507251467555761
Loss made of: CE 0.07045453041791916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.069919109344482 EntMin 0.0
Epoch 1, Class Loss=0.09461996704339981, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.09461996704339981, Class Loss=0.09461996704339981, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/102, Loss=4.21479434221983
Loss made of: CE 0.14470624923706055, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.90783953666687 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.331770744174719
Loss made of: CE 0.07290499657392502, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.967952251434326 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 30/102, Loss=4.37756944745779
Loss made of: CE 0.1553565263748169, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.059054851531982 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.4420217588543895
Loss made of: CE 0.20252691209316254, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.738601207733154 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.176021782308817
Loss made of: CE 0.15271694958209991, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.540170669555664 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.553388262540102
Loss made of: CE 0.07954998314380646, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.52114200592041 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.621178535372019
Loss made of: CE 0.13576288521289825, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.745624303817749 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.388846089690924
Loss made of: CE 0.16757218539714813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.74363374710083 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.187266647070646
Loss made of: CE 0.16525283455848694, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.480962753295898 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.182189658284187
Loss made of: CE 0.09878288209438324, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.998981237411499 EntMin 0.0
Epoch 2, Class Loss=0.13777868449687958, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.13777868449687958, Class Loss=0.13777868449687958, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/102, Loss=4.49964173734188
Loss made of: CE 0.1652592420578003, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9259681701660156 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.723544737696647
Loss made of: CE 0.24582885205745697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.752479553222656 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.211012049019336
Loss made of: CE 0.2281346619129181, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.274178504943848 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.415440881252289
Loss made of: CE 0.12688301503658295, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8206934928894043 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.492624524235725
Loss made of: CE 0.23012325167655945, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.58338737487793 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.222743019461632
Loss made of: CE 0.1814122200012207, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.752932071685791 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.397176890075206
Loss made of: CE 0.15873311460018158, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.125974178314209 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.305371263623238
Loss made of: CE 0.17563693225383759, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0648651123046875 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.228254564851523
Loss made of: CE 0.1722361445426941, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8699681758880615 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.2615680426359175
Loss made of: CE 0.1337631642818451, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8560070991516113 EntMin 0.0
Epoch 3, Class Loss=0.180345356464386, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.180345356464386, Class Loss=0.180345356464386, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/102, Loss=4.510257674753666
Loss made of: CE 0.23678874969482422, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.002557754516602 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.401060311496257
Loss made of: CE 0.21401634812355042, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202162742614746 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.492365118861199
Loss made of: CE 0.22058574855327606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.843527317047119 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.032433748245239
Loss made of: CE 0.21218347549438477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0898284912109375 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.475479336082936
Loss made of: CE 0.22779034078121185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.139914512634277 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.2653258189558985
Loss made of: CE 0.20192520320415497, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.707928419113159 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.186905471980571
Loss made of: CE 0.17911241948604584, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.309412002563477 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.098100642859936
Loss made of: CE 0.1822270154953003, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.914198875427246 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.665864700078965
Loss made of: CE 0.24439939856529236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.290931701660156 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.178423437476158
Loss made of: CE 0.17409907281398773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8015942573547363 EntMin 0.0
Epoch 4, Class Loss=0.21179096400737762, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.21179096400737762, Class Loss=0.21179096400737762, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/102, Loss=4.106857182085514
Loss made of: CE 0.25857505202293396, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.616741180419922 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.512236222624779
Loss made of: CE 0.24401096999645233, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.298153877258301 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.2941887602210045
Loss made of: CE 0.19033882021903992, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.083625793457031 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.214755511283874
Loss made of: CE 0.2217787206172943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.311130523681641 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.201923750340939
Loss made of: CE 0.20487487316131592, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5719947814941406 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.168687090277672
Loss made of: CE 0.23870041966438293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.113564491271973 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.025987878441811
Loss made of: CE 0.2664003372192383, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.692189931869507 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.115174618363381
Loss made of: CE 0.2048928439617157, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.044015884399414 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.727966485917568
Loss made of: CE 0.2991114556789398, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.244781494140625 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.109881846606731
Loss made of: CE 0.1975923478603363, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6252810955047607 EntMin 0.0
Epoch 5, Class Loss=0.2420172542333603, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.2420172542333603, Class Loss=0.2420172542333603, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/102, Loss=4.419774341583252
Loss made of: CE 0.2443910837173462, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9868733882904053 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.3669378980994225
Loss made of: CE 0.29637622833251953, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8678386211395264 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.4592162609100345
Loss made of: CE 0.23132595419883728, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.277993679046631 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.217144179344177
Loss made of: CE 0.3051663041114807, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9422574043273926 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.692976921796799
Loss made of: CE 0.24569007754325867, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9622068405151367 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.251823619008064
Loss made of: CE 0.1953095644712448, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.125070571899414 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.39721083343029
Loss made of: CE 0.3159240484237671, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.270573616027832 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.328677095472813
Loss made of: CE 0.2805664837360382, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.757908344268799 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.238642145693302
Loss made of: CE 0.2095191478729248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.77235221862793 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.149570100009441
Loss made of: CE 0.30683666467666626, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.354824066162109 EntMin 0.0
Epoch 6, Class Loss=0.25622108578681946, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.25622108578681946, Class Loss=0.25622108578681946, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.47842174768447876, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.863367
Mean Acc: 0.551212
FreqW Acc: 0.771512
Mean IoU: 0.428809
Class IoU:
	class 0: 0.8845228
	class 1: 0.34228078
	class 2: 0.24315703
	class 3: 0.42505234
	class 4: 0.5251771
	class 5: 0.13850507
	class 6: 0.8387838
	class 7: 0.65563923
	class 8: 0.70895314
	class 9: 0.0
	class 10: 0.47481814
	class 11: 0.29810637
	class 12: 0.490904
	class 13: 0.0
	class 14: 0.63864106
	class 15: 0.62521666
	class 16: 0.0
Class Acc:
	class 0: 0.9494035
	class 1: 0.34306923
	class 2: 0.41060755
	class 3: 0.44951737
	class 4: 0.88796294
	class 5: 0.13936287
	class 6: 0.89219445
	class 7: 0.8951506
	class 8: 0.7885043
	class 9: 0.0
	class 10: 0.6644092
	class 11: 0.4907395
	class 12: 0.7534568
	class 13: 0.0
	class 14: 0.82601
	class 15: 0.8802114
	class 16: 0.0

federated global round: 17, step: 3
select part of clients to conduct local training
[5, 3, 17, 11]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=3.7079092955216764
Loss made of: CE 0.05118217319250107, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.336986541748047 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/102, Loss=4.106966226734221
Loss made of: CE 0.025064777582883835, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6289613246917725 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.7685360297560693
Loss made of: CE 0.05788169801235199, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.384519100189209 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.8292035959661006
Loss made of: CE 0.037889063358306885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0538330078125 EntMin 0.0
Epoch 1, Batch 50/102, Loss=3.8705708594992756
Loss made of: CE 0.03872925788164139, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.945528030395508 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.201497818343341
Loss made of: CE 0.03548750281333923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.399392127990723 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.304398465529085
Loss made of: CE 0.04148207604885101, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.94093656539917 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.142753924801946
Loss made of: CE 0.055370889604091644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1564836502075195 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.988630470447242
Loss made of: CE 0.0367080494761467, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.917123556137085 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.78461889103055
Loss made of: CE 0.04197637736797333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2363743782043457 EntMin 0.0
Epoch 1, Class Loss=0.04599447175860405, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.04599447175860405, Class Loss=0.04599447175860405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.180438896641135
Loss made of: CE 0.08286961913108826, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.908339500427246 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.051593256369233
Loss made of: CE 0.0848776251077652, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.436384916305542 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.016101475805044
Loss made of: CE 0.07377216964960098, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6513049602508545 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.1528849329799415
Loss made of: CE 0.13212037086486816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.190541744232178 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.3344281781464815
Loss made of: CE 0.07507222145795822, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.179414749145508 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.175713241100311
Loss made of: CE 0.11754786223173141, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8124749660491943 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.986213580518961
Loss made of: CE 0.11577656865119934, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.929645299911499 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.261218285188079
Loss made of: CE 0.09922908246517181, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.744073867797852 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.9988029219210146
Loss made of: CE 0.0681542158126831, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4387834072113037 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.357648074626923
Loss made of: CE 0.08912847936153412, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.953233480453491 EntMin 0.0
Epoch 2, Class Loss=0.08485238999128342, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.08485238999128342, Class Loss=0.08485238999128342, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.058236641436816
Loss made of: CE 0.1387665867805481, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8680667877197266 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.127583908289671
Loss made of: CE 0.11330640316009521, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.941070079803467 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.038646081089974
Loss made of: CE 0.13975916802883148, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4307239055633545 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.2969826005399225
Loss made of: CE 0.12754355370998383, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3182783126831055 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.109231802076101
Loss made of: CE 0.1834624707698822, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6342074871063232 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.020069491863251
Loss made of: CE 0.16657574474811554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249203205108643 EntMin 0.0
Epoch 3, Batch 70/102, Loss=3.9939975656569002
Loss made of: CE 0.10532011836767197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.809626579284668 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.359652326256037
Loss made of: CE 0.09114941954612732, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.591179847717285 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.159655573964119
Loss made of: CE 0.10239028185606003, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6736326217651367 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.8653089337050917
Loss made of: CE 0.11963313817977905, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8582406044006348 EntMin 0.0
Epoch 3, Class Loss=0.12678097188472748, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.12678097188472748, Class Loss=0.12678097188472748, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=4.166103281080723
Loss made of: CE 0.14651720225811005, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.763725996017456 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.8432010114192963
Loss made of: CE 0.15126106142997742, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8066375255584717 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.383059190958738
Loss made of: CE 0.16460181772708893, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.001472473144531 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.191945300996304
Loss made of: CE 0.15661220252513885, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.884215831756592 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.1717788122594355
Loss made of: CE 0.13009297847747803, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.92682147026062 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.200199402868748
Loss made of: CE 0.22917935252189636, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6974101066589355 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.115185958147049
Loss made of: CE 0.17226096987724304, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.299534320831299 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.020418393611908
Loss made of: CE 0.17341527342796326, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.670694351196289 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.155650628358126
Loss made of: CE 0.18534420430660248, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7742862701416016 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.2690692961215975
Loss made of: CE 0.14705464243888855, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.985442876815796 EntMin 0.0
Epoch 4, Class Loss=0.15813325345516205, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.15813325345516205, Class Loss=0.15813325345516205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.1215057000517845
Loss made of: CE 0.23610322177410126, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8732080459594727 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.003089651465416
Loss made of: CE 0.1714523285627365, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.81208872795105 EntMin 0.0
Epoch 5, Batch 30/102, Loss=3.9154192954301834
Loss made of: CE 0.17477571964263916, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8605058193206787 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.0178343370556835
Loss made of: CE 0.2400038093328476, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6290087699890137 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.0868417650461195
Loss made of: CE 0.15370061993598938, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.713212728500366 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.021976146101951
Loss made of: CE 0.2278960645198822, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4925527572631836 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.093286745250225
Loss made of: CE 0.1483163833618164, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8239006996154785 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.532917720079422
Loss made of: CE 0.21646055579185486, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5536985397338867 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.923255467414856
Loss made of: CE 0.2150440514087677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.81953763961792 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.117633788287639
Loss made of: CE 0.18392491340637207, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.774984836578369 EntMin 0.0
Epoch 5, Class Loss=0.19292227923870087, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.19292227923870087, Class Loss=0.19292227923870087, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.048045854270458
Loss made of: CE 0.2026759833097458, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8129231929779053 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.9535952374339103
Loss made of: CE 0.19132928550243378, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.310739994049072 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.9599940583109854
Loss made of: CE 0.25164562463760376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.603447914123535 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.278143192827701
Loss made of: CE 0.19395524263381958, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7311182022094727 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.195134988427162
Loss made of: CE 0.29879558086395264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.854410171508789 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.0109978199005125
Loss made of: CE 0.14904966950416565, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3705787658691406 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.926311822235584
Loss made of: CE 0.2066558301448822, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.510423421859741 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.809431439638138
Loss made of: CE 0.1976780891418457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.712240695953369 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 90/102, Loss=4.290931195020676
Loss made of: CE 0.23662057518959045, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.862741231918335 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.316885928809643
Loss made of: CE 0.2098807543516159, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.66364049911499 EntMin 0.0
Epoch 6, Class Loss=0.21509060263633728, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.21509060263633728, Class Loss=0.21509060263633728, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=3.8652626167982818
Loss made of: CE 0.03929286077618599, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4417829513549805 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.8383107487112285
Loss made of: CE 0.0279754139482975, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.280726432800293 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.081590523757041
Loss made of: CE 0.05224623903632164, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3701236248016357 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.985543402284384
Loss made of: CE 0.11064314842224121, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.383241176605225 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.132201519049704
Loss made of: CE 0.05013653635978699, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.225986957550049 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.163162908703089
Loss made of: CE 0.035274654626846313, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.676438331604004 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.058949519507587
Loss made of: CE 0.05814974755048752, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.89647102355957 EntMin 0.0
Epoch 1, Batch 80/102, Loss=3.9935018423944713
Loss made of: CE 0.056349508464336395, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.295976400375366 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.228666980192065
Loss made of: CE 0.04375001788139343, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6887168884277344 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.106578974053264
Loss made of: CE 0.024363093078136444, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6015841960906982 EntMin 0.0
Epoch 1, Class Loss=0.04607595503330231, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.04607595503330231, Class Loss=0.04607595503330231, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=4.164791498333216
Loss made of: CE 0.10961320251226425, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.211830139160156 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.9516725711524487
Loss made of: CE 0.11826269328594208, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6861133575439453 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.299131513386965
Loss made of: CE 0.09404274076223373, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.696981191635132 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.03926199823618
Loss made of: CE 0.07487613707780838, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.35068416595459 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.127402447164059
Loss made of: CE 0.08031276613473892, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4596803188323975 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.100799449533224
Loss made of: CE 0.09016021341085434, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.606901168823242 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.133912056311965
Loss made of: CE 0.0714595764875412, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2049126625061035 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.157491230219603
Loss made of: CE 0.07848896086215973, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.266086101531982 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.203249663859606
Loss made of: CE 0.055624574422836304, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.656085968017578 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.327523987740278
Loss made of: CE 0.09357752650976181, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7067995071411133 EntMin 0.0
Epoch 2, Class Loss=0.08340093493461609, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.08340093493461609, Class Loss=0.08340093493461609, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=4.236417562514544
Loss made of: CE 0.10200630128383636, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.925941228866577 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.049875374138355
Loss made of: CE 0.14425262808799744, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.404815673828125 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.249149059504271
Loss made of: CE 0.16584019362926483, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.037543296813965 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.046766943484545
Loss made of: CE 0.10624488443136215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128496170043945 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.466873852908611
Loss made of: CE 0.1406169980764389, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.094994068145752 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.922214226424694
Loss made of: CE 0.11703880876302719, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4320998191833496 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.2796031452715395
Loss made of: CE 0.08528991788625717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.037485122680664 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.264108674973249
Loss made of: CE 0.10332465171813965, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8291172981262207 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.045615538954735
Loss made of: CE 0.10863983631134033, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.524400472640991 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.1168272465467455
Loss made of: CE 0.15064801275730133, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.283894062042236 EntMin 0.0
Epoch 3, Class Loss=0.1264185905456543, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.1264185905456543, Class Loss=0.1264185905456543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=3.990826652944088
Loss made of: CE 0.13138842582702637, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.669517993927002 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.3235050529241565
Loss made of: CE 0.19356897473335266, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.786426544189453 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.060873448848724
Loss made of: CE 0.16177207231521606, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.626014232635498 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.339912821352482
Loss made of: CE 0.15487739443778992, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5647125244140625 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.2032167375087734
Loss made of: CE 0.15009161829948425, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8174800872802734 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.118006919324398
Loss made of: CE 0.21242710947990417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.016978740692139 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 70/102, Loss=4.562538523972035
Loss made of: CE 0.19741106033325195, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0114240646362305 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.3004900850355625
Loss made of: CE 0.10880555957555771, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.026271343231201 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.118658103048801
Loss made of: CE 0.1338014006614685, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4615607261657715 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.105541329085827
Loss made of: CE 0.18810681998729706, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.800583600997925 EntMin 0.0
Epoch 4, Class Loss=0.1580960601568222, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.1580960601568222, Class Loss=0.1580960601568222, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=4.070503664016724
Loss made of: CE 0.22936129570007324, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7836453914642334 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.186353389918804
Loss made of: CE 0.21172651648521423, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6959877014160156 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.0566115692257885
Loss made of: CE 0.21140512824058533, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.431492805480957 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.164040164649487
Loss made of: CE 0.24696969985961914, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4466772079467773 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.294690330326557
Loss made of: CE 0.1552722454071045, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7234244346618652 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.00152620524168
Loss made of: CE 0.14939609169960022, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5851173400878906 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.332369470596314
Loss made of: CE 0.17988590896129608, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9075374603271484 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.526761274039745
Loss made of: CE 0.20065955817699432, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.673993110656738 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.0310695067048075
Loss made of: CE 0.16834303736686707, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.944472312927246 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.048891633749008
Loss made of: CE 0.22143492102622986, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6156039237976074 EntMin 0.0
Epoch 5, Class Loss=0.19592200219631195, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.19592200219631195, Class Loss=0.19592200219631195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=4.064036384224892
Loss made of: CE 0.2472974956035614, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.457279682159424 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.0202191233634945
Loss made of: CE 0.2082197368144989, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.550865650177002 EntMin 0.0
Epoch 6, Batch 30/102, Loss=4.121734647452831
Loss made of: CE 0.22807322442531586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53195333480835 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.811851991713047
Loss made of: CE 0.18811681866645813, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3728408813476562 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.079048779606819
Loss made of: CE 0.2243967354297638, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.368367671966553 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.055169884860516
Loss made of: CE 0.22829362750053406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.560884952545166 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.810119481384754
Loss made of: CE 0.19685503840446472, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5952343940734863 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.975413639843464
Loss made of: CE 0.2367560863494873, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.568066120147705 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.294358742237091
Loss made of: CE 0.19923099875450134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4303064346313477 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.15325348675251
Loss made of: CE 0.17940175533294678, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.673135280609131 EntMin 0.0
Epoch 6, Class Loss=0.2143612951040268, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.2143612951040268, Class Loss=0.2143612951040268, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=5.365595169365406
Loss made of: CE 0.308625191450119, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.736435413360596 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.7991798430681225
Loss made of: CE 0.16549627482891083, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.543971538543701 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.1829787939786911, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.1829787939786911, Class Loss=0.1829787939786911, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/23, Loss=4.64413665831089
Loss made of: CE 0.34848958253860474, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8121049404144287 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.494321054220199
Loss made of: CE 0.20958004891872406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.157541275024414 EntMin 0.0
Epoch 2, Class Loss=0.2935575544834137, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.2935575544834137, Class Loss=0.2935575544834137, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/23, Loss=4.525529244542122
Loss made of: CE 0.3094096779823303, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.036192893981934 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.512555815279484
Loss made of: CE 0.48077821731567383, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291070461273193 EntMin 0.0
Epoch 3, Class Loss=0.37154290080070496, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.37154290080070496, Class Loss=0.37154290080070496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/23, Loss=4.64707373380661
Loss made of: CE 0.5036037564277649, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8138513565063477 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.358595588803292
Loss made of: CE 0.3013932704925537, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.137840270996094 EntMin 0.0
Epoch 4, Class Loss=0.38271990418434143, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.38271990418434143, Class Loss=0.38271990418434143, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/23, Loss=4.4907449066638945
Loss made of: CE 0.3313433527946472, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.240520477294922 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.213876557350159
Loss made of: CE 0.3201770484447479, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.019474029541016 EntMin 0.0
Epoch 5, Class Loss=0.381302148103714, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.381302148103714, Class Loss=0.381302148103714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/23, Loss=4.3911996826529505
Loss made of: CE 0.4072507619857788, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.786116361618042 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.287316158413887
Loss made of: CE 0.4367058277130127, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8299078941345215 EntMin 0.0
Epoch 6, Class Loss=0.38897791504859924, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.38897791504859924, Class Loss=0.38897791504859924, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.155350184440612
Loss made of: CE 0.454256534576416, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8958892822265625 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.26990128159523
Loss made of: CE 0.4806738793849945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8449904918670654 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.257922691106796
Loss made of: CE 0.4705802798271179, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.215315341949463 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.549682769179344
Loss made of: CE 0.38274478912353516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.168691158294678 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.502454996109009
Loss made of: CE 0.38250964879989624, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.903885841369629 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.463409724831581
Loss made of: CE 0.41745519638061523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3817434310913086 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.262548092007637
Loss made of: CE 0.43405434489250183, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8516898155212402 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.380945089459419
Loss made of: CE 0.41072744131088257, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.48171854019165 EntMin 0.0
Epoch 1, Batch 90/102, Loss=4.150758418440819
Loss made of: CE 0.39748215675354004, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3438100814819336 EntMin 0.0
Epoch 1, Batch 100/102, Loss=4.263529253005982
Loss made of: CE 0.4510790705680847, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8256478309631348 EntMin 0.0
Epoch 1, Class Loss=0.43083932995796204, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.43083932995796204, Class Loss=0.43083932995796204, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000600
Epoch 2, Batch 10/102, Loss=4.199549266695977
Loss made of: CE 0.3443346917629242, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.484318733215332 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.129328387975693
Loss made of: CE 0.382249116897583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.692413806915283 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.254424694180488
Loss made of: CE 0.4990249574184418, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6495187282562256 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.268055021762848
Loss made of: CE 0.4659446179866791, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3772449493408203 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.311041659116745
Loss made of: CE 0.4162067770957947, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8054120540618896 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.3247948557138445
Loss made of: CE 0.4427323043346405, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5179591178894043 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.975614047050476
Loss made of: CE 0.43117350339889526, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3662338256835938 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.280280721187592
Loss made of: CE 0.5213053822517395, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332064151763916 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.222305795550346
Loss made of: CE 0.4545961618423462, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.652829170227051 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.385780933499336
Loss made of: CE 0.3594193458557129, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.033786773681641 EntMin 0.0
Epoch 2, Class Loss=0.4116678237915039, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.4116678237915039, Class Loss=0.4116678237915039, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/102, Loss=4.22621029317379
Loss made of: CE 0.4676407277584076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.070252895355225 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.165172731876373
Loss made of: CE 0.3842446208000183, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.226058006286621 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.217621329426765
Loss made of: CE 0.40593239665031433, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.416571617126465 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.225089424848557
Loss made of: CE 0.43899840116500854, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7919921875 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.370607775449753
Loss made of: CE 0.4019095301628113, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4375367164611816 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.1878265857696535
Loss made of: CE 0.37743669748306274, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.775689125061035 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.04241042137146
Loss made of: CE 0.3172896206378937, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.81711483001709 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.443337261676788
Loss made of: CE 0.4349631667137146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.283825874328613 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.335579416155815
Loss made of: CE 0.323793888092041, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2819151878356934 EntMin 0.0
Epoch 3, Batch 100/102, Loss=4.202654165029526
Loss made of: CE 0.38675498962402344, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2095532417297363 EntMin 0.0
Epoch 3, Class Loss=0.3999306559562683, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.3999306559562683, Class Loss=0.3999306559562683, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/102, Loss=4.125058916211128
Loss made of: CE 0.3887724280357361, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.288386821746826 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.395359870791435
Loss made of: CE 0.35039377212524414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363436698913574 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.2032037556171415
Loss made of: CE 0.48325905203819275, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8993794918060303 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.081871122121811
Loss made of: CE 0.3590826392173767, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2328994274139404 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.364356076717376
Loss made of: CE 0.4617193639278412, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9219613075256348 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.342026397585869
Loss made of: CE 0.3649263381958008, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8691864013671875 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.262319418787956
Loss made of: CE 0.4962198734283447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7417049407958984 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.357571142911911
Loss made of: CE 0.5224944353103638, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.787735462188721 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.315849721431732
Loss made of: CE 0.3225851356983185, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.618713140487671 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.230595362186432
Loss made of: CE 0.48941025137901306, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.710644721984863 EntMin 0.0
Epoch 4, Class Loss=0.39963266253471375, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.39963266253471375, Class Loss=0.39963266253471375, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000504
Epoch 5, Batch 10/102, Loss=4.457672220468521
Loss made of: CE 0.42548656463623047, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.543912887573242 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.395034599304199
Loss made of: CE 0.3642736077308655, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7974469661712646 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.238300156593323
Loss made of: CE 0.3356136679649353, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4795920848846436 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.065639343857765
Loss made of: CE 0.44192999601364136, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.800621509552002 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.306774082779884
Loss made of: CE 0.38029754161834717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.193976402282715 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.9398969441652296
Loss made of: CE 0.4188774824142456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.839901924133301 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.037375736236572
Loss made of: CE 0.453316867351532, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6066768169403076 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.163528850674629
Loss made of: CE 0.30419838428497314, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.826465606689453 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.200039085745812
Loss made of: CE 0.4337526261806488, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.319080114364624 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.218166446685791
Loss made of: CE 0.34680628776550293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.098862171173096 EntMin 0.0
Epoch 5, Class Loss=0.3835500180721283, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.3835500180721283, Class Loss=0.3835500180721283, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000471
Epoch 6, Batch 10/102, Loss=4.478752136230469
Loss made of: CE 0.3124842345714569, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.560689926147461 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.034196615219116
Loss made of: CE 0.4101155996322632, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6064577102661133 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.8893358558416367
Loss made of: CE 0.3465835452079773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.680107593536377 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.2701515585184096
Loss made of: CE 0.43804237246513367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.69175386428833 EntMin 0.0
Epoch 6, Batch 50/102, Loss=4.005049955844879
Loss made of: CE 0.36531543731689453, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3275697231292725 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.172737416625023
Loss made of: CE 0.42321252822875977, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7630906105041504 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.8890462547540663
Loss made of: CE 0.38215169310569763, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5216634273529053 EntMin 0.0
Epoch 6, Batch 80/102, Loss=4.1114651441574095
Loss made of: CE 0.37025874853134155, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.60404372215271 EntMin 0.0
Epoch 6, Batch 90/102, Loss=4.189208635687828
Loss made of: CE 0.3782055377960205, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6840929985046387 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.215302208065987
Loss made of: CE 0.4148666262626648, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.430384397506714 EntMin 0.0
Epoch 6, Class Loss=0.3792821168899536, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.3792821168899536, Class Loss=0.3792821168899536, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.46166500449180603, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.862579
Mean Acc: 0.576041
FreqW Acc: 0.774154
Mean IoU: 0.435934
Class IoU:
	class 0: 0.88562435
	class 1: 0.37561342
	class 2: 0.25682363
	class 3: 0.46802792
	class 4: 0.48051527
	class 5: 0.15563901
	class 6: 0.84501284
	class 7: 0.6352854
	class 8: 0.7173146
	class 9: 0.0
	class 10: 0.4838087
	class 11: 0.29275146
	class 12: 0.5086657
	class 13: 0.060400303
	class 14: 0.6240851
	class 15: 0.6213021
	class 16: 0.0
Class Acc:
	class 0: 0.937409
	class 1: 0.3767284
	class 2: 0.45244995
	class 3: 0.5018314
	class 4: 0.9178128
	class 5: 0.1570427
	class 6: 0.90927047
	class 7: 0.9060357
	class 8: 0.82747495
	class 9: 0.0
	class 10: 0.6959142
	class 11: 0.53814983
	class 12: 0.7663069
	class 13: 0.060569033
	class 14: 0.84563774
	class 15: 0.9000706
	class 16: 0.0

federated global round: 18, step: 3
select part of clients to conduct local training
[9, 11, 13, 10]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.210851523652673
Loss made of: CE 0.053400974720716476, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4234824180603027 EntMin 0.0
Epoch 1, Batch 20/102, Loss=3.876948285102844
Loss made of: CE 0.038295403122901917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4177603721618652 EntMin 0.0
Epoch 1, Batch 30/102, Loss=3.96438329257071
Loss made of: CE 0.04460201784968376, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.980057716369629 EntMin 0.0
Epoch 1, Batch 40/102, Loss=3.83800885733217
Loss made of: CE 0.027456318959593773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.506056547164917 EntMin 0.0
Epoch 1, Batch 50/102, Loss=3.930024411715567
Loss made of: CE 0.06045161187648773, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8533153533935547 EntMin 0.0
Epoch 1, Batch 60/102, Loss=3.87588773034513
Loss made of: CE 0.046463169157505035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.064054012298584 EntMin 0.0
Epoch 1, Batch 70/102, Loss=3.923979738354683
Loss made of: CE 0.043896645307540894, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6824593544006348 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.02038695178926
Loss made of: CE 0.03877566009759903, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8333749771118164 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.904427221417427
Loss made of: CE 0.03718137741088867, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5131680965423584 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.7927961738780143
Loss made of: CE 0.03522840142250061, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.58148193359375 EntMin 0.0
Epoch 1, Class Loss=0.039864152669906616, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.039864152669906616, Class Loss=0.039864152669906616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/102, Loss=3.8864886917173864
Loss made of: CE 0.06330783665180206, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.486510992050171 EntMin 0.0
Epoch 2, Batch 20/102, Loss=4.144069979339838
Loss made of: CE 0.0803808942437172, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9191036224365234 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.001812111586332
Loss made of: CE 0.1303679347038269, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.769540786743164 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.111486131325364
Loss made of: CE 0.07447131723165512, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.641610860824585 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.2531280849128965
Loss made of: CE 0.042689912021160126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5985565185546875 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.04280374199152
Loss made of: CE 0.04535752162337303, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8746583461761475 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.9695331543684005
Loss made of: CE 0.06578798592090607, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8252227306365967 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.01591844111681
Loss made of: CE 0.07558637112379074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5881974697113037 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.946885296702385
Loss made of: CE 0.05564527213573456, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.684920310974121 EntMin 0.0
Epoch 2, Batch 100/102, Loss=3.7368285171687603
Loss made of: CE 0.0598030611872673, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.622368574142456 EntMin 0.0
Epoch 2, Class Loss=0.07338710874319077, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.07338710874319077, Class Loss=0.07338710874319077, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/102, Loss=4.024825740605593
Loss made of: CE 0.12343256920576096, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.346312046051025 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.205207373201847
Loss made of: CE 0.10242816805839539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1546311378479 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.9483230628073214
Loss made of: CE 0.149754598736763, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9301066398620605 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.121544229239225
Loss made of: CE 0.09320827573537827, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.714435338973999 EntMin 0.0
Epoch 3, Batch 50/102, Loss=3.777114060521126
Loss made of: CE 0.1328355371952057, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.812962293624878 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.9640372224152087
Loss made of: CE 0.13224369287490845, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.900313138961792 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.28932603597641
Loss made of: CE 0.14478370547294617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3865132331848145 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.013627878576517
Loss made of: CE 0.10226249694824219, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3547801971435547 EntMin 0.0
Epoch 3, Batch 90/102, Loss=3.765350417792797
Loss made of: CE 0.14179840683937073, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7627272605895996 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.9024859458208083
Loss made of: CE 0.11524294316768646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6576805114746094 EntMin 0.0
Epoch 3, Class Loss=0.11515437066555023, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.11515437066555023, Class Loss=0.11515437066555023, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/102, Loss=4.1068157739937305
Loss made of: CE 0.18674683570861816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.528010368347168 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.404715836048126
Loss made of: CE 0.1254819929599762, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1038818359375 EntMin 0.0
Epoch 4, Batch 30/102, Loss=3.9403681479394437
Loss made of: CE 0.1459009349346161, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7824904918670654 EntMin 0.0
Epoch 4, Batch 40/102, Loss=4.158204746246338
Loss made of: CE 0.1774081289768219, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9569196701049805 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.01902888789773
Loss made of: CE 0.10024049133062363, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.872431516647339 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.065859054774046
Loss made of: CE 0.1053573414683342, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.217611074447632 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.123695582896471
Loss made of: CE 0.1706700623035431, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.86824893951416 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.041251212358475
Loss made of: CE 0.15521526336669922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.339306831359863 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.166530384868383
Loss made of: CE 0.14020082354545593, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6723971366882324 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.09176583737135
Loss made of: CE 0.1754363626241684, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249176979064941 EntMin 0.0
Epoch 4, Class Loss=0.14947709441184998, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.14947709441184998, Class Loss=0.14947709441184998, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=3.9834637582302093
Loss made of: CE 0.20681457221508026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18679666519165 EntMin 0.0
Epoch 5, Batch 20/102, Loss=3.846616192162037
Loss made of: CE 0.16605977714061737, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7439627647399902 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 30/102, Loss=3.8483084738254547
Loss made of: CE 0.17729346454143524, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9091224670410156 EntMin 0.0
Epoch 5, Batch 40/102, Loss=4.181775294244289
Loss made of: CE 0.19186297059059143, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6543335914611816 EntMin 0.0
Epoch 5, Batch 50/102, Loss=3.891914168000221
Loss made of: CE 0.14038234949111938, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4696080684661865 EntMin 0.0
Epoch 5, Batch 60/102, Loss=4.0280311584472654
Loss made of: CE 0.25101685523986816, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8245186805725098 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.078657339513302
Loss made of: CE 0.19677484035491943, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8963780403137207 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.067994631826878
Loss made of: CE 0.13516387343406677, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.731637477874756 EntMin 0.0
Epoch 5, Batch 90/102, Loss=4.019190116226673
Loss made of: CE 0.17590299248695374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.247267723083496 EntMin 0.0
Epoch 5, Batch 100/102, Loss=4.0364649295806885
Loss made of: CE 0.16897213459014893, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3411996364593506 EntMin 0.0
Epoch 5, Class Loss=0.18041794002056122, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.18041794002056122, Class Loss=0.18041794002056122, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/102, Loss=4.125098913908005
Loss made of: CE 0.19206687808036804, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.784126043319702 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.799784944951534
Loss made of: CE 0.2064843475818634, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5999972820281982 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.9046368703246115
Loss made of: CE 0.20061680674552917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.305569648742676 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.019551719725132
Loss made of: CE 0.1800997257232666, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.27938175201416 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.942202128469944
Loss made of: CE 0.18525230884552002, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.766294479370117 EntMin 0.0
Epoch 6, Batch 60/102, Loss=4.076232352852822
Loss made of: CE 0.18487265706062317, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3309168815612793 EntMin 0.0
Epoch 6, Batch 70/102, Loss=4.10048945248127
Loss made of: CE 0.22394447028636932, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.113374710083008 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.9191738620400427
Loss made of: CE 0.22121159732341766, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8564834594726562 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.8805975928902625
Loss made of: CE 0.20103377103805542, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6743640899658203 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.770537573099136
Loss made of: CE 0.15414628386497498, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.212820053100586 EntMin 0.0
Epoch 6, Class Loss=0.2003277838230133, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.2003277838230133, Class Loss=0.2003277838230133, Reg Loss=0.0
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000438
Epoch 1, Batch 10/102, Loss=4.21406369805336
Loss made of: CE 0.4817417562007904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.481456756591797 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/102, Loss=4.0289656817913055
Loss made of: CE 0.43066978454589844, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5171661376953125 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.076408350467682
Loss made of: CE 0.4648240804672241, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9692330360412598 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.2876614212989805
Loss made of: CE 0.38012608885765076, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.697763204574585 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.179530283808708
Loss made of: CE 0.365016907453537, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6411519050598145 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.31408888399601
Loss made of: CE 0.3780028820037842, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.266432523727417 EntMin 0.0
Epoch 1, Batch 70/102, Loss=3.9748490184545515
Loss made of: CE 0.4406742453575134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.681797742843628 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.101467725634575
Loss made of: CE 0.40429580211639404, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8074166774749756 EntMin 0.0
Epoch 1, Batch 90/102, Loss=3.9161039292812347
Loss made of: CE 0.3580114543437958, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.279613494873047 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.958317795395851
Loss made of: CE 0.3901705741882324, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.544675588607788 EntMin 0.0
Epoch 1, Class Loss=0.4121006429195404, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.4121006429195404, Class Loss=0.4121006429195404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000405
Epoch 2, Batch 10/102, Loss=3.9464962065219877
Loss made of: CE 0.3218340277671814, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1894783973693848 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.892767584323883
Loss made of: CE 0.37234723567962646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.488797664642334 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.029259371757507
Loss made of: CE 0.5106555819511414, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5117557048797607 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.086991459131241
Loss made of: CE 0.4228029251098633, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4218759536743164 EntMin 0.0
Epoch 2, Batch 50/102, Loss=4.093167445063591
Loss made of: CE 0.3967471122741699, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.539212703704834 EntMin 0.0
Epoch 2, Batch 60/102, Loss=4.184330251812935
Loss made of: CE 0.4281648099422455, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.735370397567749 EntMin 0.0
Epoch 2, Batch 70/102, Loss=3.7957487136125563
Loss made of: CE 0.4240686297416687, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.247792959213257 EntMin 0.0
Epoch 2, Batch 80/102, Loss=4.119969716668129
Loss made of: CE 0.4560977816581726, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8587942123413086 EntMin 0.0
Epoch 2, Batch 90/102, Loss=4.009015002846718
Loss made of: CE 0.46730345487594604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4288482666015625 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.075592800974846
Loss made of: CE 0.36012178659439087, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.787052869796753 EntMin 0.0
Epoch 2, Class Loss=0.39189594984054565, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.39189594984054565, Class Loss=0.39189594984054565, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/102, Loss=4.001969987154007
Loss made of: CE 0.40456610918045044, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.610370635986328 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 20/102, Loss=3.9730206727981567
Loss made of: CE 0.3722979426383972, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.36245059967041 EntMin 0.0
Epoch 3, Batch 30/102, Loss=4.066403126716613
Loss made of: CE 0.4083518981933594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.415310859680176 EntMin 0.0
Epoch 3, Batch 40/102, Loss=4.043514269590378
Loss made of: CE 0.42096543312072754, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.716287612915039 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.229671344161034
Loss made of: CE 0.374589204788208, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4240121841430664 EntMin 0.0
Epoch 3, Batch 60/102, Loss=4.018206518888474
Loss made of: CE 0.34489402174949646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4464452266693115 EntMin 0.0
Epoch 3, Batch 70/102, Loss=3.9790967077016832
Loss made of: CE 0.3042246103286743, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8103342056274414 EntMin 0.0
Epoch 3, Batch 80/102, Loss=4.109637019038201
Loss made of: CE 0.4147067964076996, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8921735286712646 EntMin 0.0
Epoch 3, Batch 90/102, Loss=4.153787848353386
Loss made of: CE 0.31596988439559937, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.313357353210449 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.984466350078583
Loss made of: CE 0.33100369572639465, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2280795574188232 EntMin 0.0
Epoch 3, Class Loss=0.3811923563480377, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.3811923563480377, Class Loss=0.3811923563480377, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=3.914451962709427
Loss made of: CE 0.37396085262298584, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3690078258514404 EntMin 0.0
Epoch 4, Batch 20/102, Loss=4.0897758394479755
Loss made of: CE 0.3153466582298279, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6204872131347656 EntMin 0.0
Epoch 4, Batch 30/102, Loss=4.1225590974092485
Loss made of: CE 0.44615694880485535, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7443413734436035 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.981439846754074
Loss made of: CE 0.3389691710472107, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.055509567260742 EntMin 0.0
Epoch 4, Batch 50/102, Loss=4.115945252776146
Loss made of: CE 0.4293140172958374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.407639980316162 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.257236072421074
Loss made of: CE 0.29961079359054565, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9648709297180176 EntMin 0.0
Epoch 4, Batch 70/102, Loss=4.065380710363388
Loss made of: CE 0.46842432022094727, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6484322547912598 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.048713883757591
Loss made of: CE 0.4430936276912689, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.36329460144043 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.109265315532684
Loss made of: CE 0.32519006729125977, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4696731567382812 EntMin 0.0
Epoch 4, Batch 100/102, Loss=3.922476276755333
Loss made of: CE 0.4397312104701996, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.078701972961426 EntMin 0.0
Epoch 4, Class Loss=0.3797233998775482, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.3797233998775482, Class Loss=0.3797233998775482, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/102, Loss=4.191255205869675
Loss made of: CE 0.42692312598228455, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.457653522491455 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.167580878734588
Loss made of: CE 0.35021016001701355, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.647491216659546 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.015895879268646
Loss made of: CE 0.32776299118995667, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.186206817626953 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.8499218583106996
Loss made of: CE 0.401531457901001, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3592348098754883 EntMin 0.0
Epoch 5, Batch 50/102, Loss=4.056520459055901
Loss made of: CE 0.36400389671325684, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.34622859954834 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.780401647090912
Loss made of: CE 0.3847470283508301, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6036431789398193 EntMin 0.0
Epoch 5, Batch 70/102, Loss=3.7928426027297975
Loss made of: CE 0.38615378737449646, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.227692127227783 EntMin 0.0
Epoch 5, Batch 80/102, Loss=3.888460612297058
Loss made of: CE 0.26649922132492065, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7280163764953613 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.9549946904182436
Loss made of: CE 0.4343990683555603, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1150221824645996 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.938170003890991
Loss made of: CE 0.3478504419326782, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8835501670837402 EntMin 0.0
Epoch 5, Class Loss=0.37009575963020325, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.37009575963020325, Class Loss=0.37009575963020325, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000270
Epoch 6, Batch 10/102, Loss=4.1419715076684955
Loss made of: CE 0.30956220626831055, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3172149658203125 EntMin 0.0
Epoch 6, Batch 20/102, Loss=3.8321886867284776
Loss made of: CE 0.40096086263656616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4671130180358887 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.7564539551734923
Loss made of: CE 0.3298538625240326, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8314740657806396 EntMin 0.0
Epoch 6, Batch 40/102, Loss=4.136452907323838
Loss made of: CE 0.4031931161880493, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.835935354232788 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.8748399913311005
Loss made of: CE 0.3391525149345398, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2723917961120605 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.97859602868557
Loss made of: CE 0.36530420184135437, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5087831020355225 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.755565360188484
Loss made of: CE 0.3810764253139496, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2741737365722656 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.903103792667389
Loss made of: CE 0.3686663508415222, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.421376943588257 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.8852089643478394
Loss made of: CE 0.3556751608848572, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.231215476989746 EntMin 0.0
Epoch 6, Batch 100/102, Loss=4.057735389471054
Loss made of: CE 0.39859169721603394, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.432576894760132 EntMin 0.0
Epoch 6, Class Loss=0.3674454391002655, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.3674454391002655, Class Loss=0.3674454391002655, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=4.599878087639809
Loss made of: CE 0.1403529942035675, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.842984199523926 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.464773590862751
Loss made of: CE 0.13362479209899902, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9818570613861084 EntMin 0.0
Epoch 1, Class Loss=0.13416500389575958, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.13416500389575958, Class Loss=0.13416500389575958, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/23, Loss=4.330588939785957
Loss made of: CE 0.28989169001579285, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.200503826141357 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.449401992559433
Loss made of: CE 0.20600752532482147, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.025424003601074 EntMin 0.0
Epoch 2, Class Loss=0.22081442177295685, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.22081442177295685, Class Loss=0.22081442177295685, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/23, Loss=4.168378020823002
Loss made of: CE 0.3755844235420227, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427620887756348 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.228636872768402
Loss made of: CE 0.29611900448799133, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.723587989807129 EntMin 0.0
Epoch 3, Class Loss=0.2708682119846344, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.2708682119846344, Class Loss=0.2708682119846344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/23, Loss=4.028465977311134
Loss made of: CE 0.3085951507091522, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7840631008148193 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.357781377434731
Loss made of: CE 0.2462213635444641, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5070009231567383 EntMin 0.0
Epoch 4, Class Loss=0.2936496138572693, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.2936496138572693, Class Loss=0.2936496138572693, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/23, Loss=4.198405429720879
Loss made of: CE 0.3032843768596649, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.618241786956787 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.119135797023773
Loss made of: CE 0.2631203532218933, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3446273803710938 EntMin 0.0
Epoch 5, Class Loss=0.3432498574256897, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.3432498574256897, Class Loss=0.3432498574256897, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/23, Loss=4.162634998559952
Loss made of: CE 0.5064883232116699, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.087207317352295 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.092763385176658
Loss made of: CE 0.2644774317741394, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.999572277069092 EntMin 0.0
Epoch 6, Class Loss=0.37947824597358704, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.37947824597358704, Class Loss=0.37947824597358704, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/23, Loss=5.586570650339127
Loss made of: CE 0.8390442728996277, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.17368221282959 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.8463215827941895
Loss made of: CE 0.7915443181991577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.114494800567627 EntMin 0.0
Epoch 1, Class Loss=0.8146679997444153, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.8146679997444153, Class Loss=0.8146679997444153, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/23, Loss=4.743925261497497
Loss made of: CE 0.6132593154907227, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4602179527282715 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.344259569048882
Loss made of: CE 0.6055912971496582, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.984541416168213 EntMin 0.0
Epoch 2, Class Loss=0.6437775492668152, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.6437775492668152, Class Loss=0.6437775492668152, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/23, Loss=4.375567197799683
Loss made of: CE 0.5384368300437927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.033197402954102 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.381117781996727
Loss made of: CE 0.42342609167099, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.038617134094238 EntMin 0.0
Epoch 3, Class Loss=0.5729312300682068, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5729312300682068, Class Loss=0.5729312300682068, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/23, Loss=4.185161942243576
Loss made of: CE 0.4763815104961395, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9261975288391113 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.3093139171600345
Loss made of: CE 0.5853933095932007, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6380457878112793 EntMin 0.0
Epoch 4, Class Loss=0.5247493386268616, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.5247493386268616, Class Loss=0.5247493386268616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/23, Loss=4.436376032233238
Loss made of: CE 0.5154804587364197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5617873668670654 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.174791726469993
Loss made of: CE 0.6435065865516663, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.147575855255127 EntMin 0.0
Epoch 5, Class Loss=0.5085337162017822, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.5085337162017822, Class Loss=0.5085337162017822, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/23, Loss=4.208320367336273
Loss made of: CE 0.3777140974998474, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.735170364379883 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.184774279594421
Loss made of: CE 0.4716298580169678, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.748967170715332 EntMin 0.0
Epoch 6, Class Loss=0.4984286427497864, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.4984286427497864, Class Loss=0.4984286427497864, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.43414053320884705, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.873392
Mean Acc: 0.598077
FreqW Acc: 0.788244
Mean IoU: 0.469361
Class IoU:
	class 0: 0.8897638
	class 1: 0.35272107
	class 2: 0.25754574
	class 3: 0.5284431
	class 4: 0.5070609
	class 5: 0.18984458
	class 6: 0.8448711
	class 7: 0.6592894
	class 8: 0.72255075
	class 9: 0.0
	class 10: 0.50291955
	class 11: 0.2931802
	class 12: 0.5409972
	class 13: 0.3888409
	class 14: 0.63682294
	class 15: 0.6642901
	class 16: 0.0
Class Acc:
	class 0: 0.9449147
	class 1: 0.35342315
	class 2: 0.4484327
	class 3: 0.58944464
	class 4: 0.90415186
	class 5: 0.19212458
	class 6: 0.89965326
	class 7: 0.8908638
	class 8: 0.83927494
	class 9: 0.0
	class 10: 0.5820984
	class 11: 0.51271874
	class 12: 0.79392254
	class 13: 0.45833826
	class 14: 0.8626328
	class 15: 0.8953091
	class 16: 0.0

federated global round: 19, step: 3
select part of clients to conduct local training
[4, 7, 17, 15]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/105, Loss=4.202092771232128
Loss made of: CE 0.07101581990718842, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.928454399108887 EntMin 0.0
Epoch 1, Batch 20/105, Loss=3.951844272390008
Loss made of: CE 0.08626885712146759, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8582587242126465 EntMin 0.0
Epoch 1, Batch 30/105, Loss=3.819089660793543
Loss made of: CE 0.06134083867073059, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.270233154296875 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 40/105, Loss=3.7267051788046954
Loss made of: CE 0.05109478160738945, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5164923667907715 EntMin 0.0
Epoch 1, Batch 50/105, Loss=3.584194351732731
Loss made of: CE 0.03628449887037277, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.313108205795288 EntMin 0.0
Epoch 1, Batch 60/105, Loss=4.060007522068918
Loss made of: CE 0.0358198881149292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4053332805633545 EntMin 0.0
Epoch 1, Batch 70/105, Loss=3.920420286618173
Loss made of: CE 0.02100304327905178, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5850579738616943 EntMin 0.0
Epoch 1, Batch 80/105, Loss=3.8322095781564713
Loss made of: CE 0.031159717589616776, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5578246116638184 EntMin 0.0
Epoch 1, Batch 90/105, Loss=3.877978564798832
Loss made of: CE 0.01945343241095543, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4804799556732178 EntMin 0.0
Epoch 1, Batch 100/105, Loss=3.939874579012394
Loss made of: CE 0.0416901521384716, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.109441757202148 EntMin 0.0
Epoch 1, Class Loss=0.04927495867013931, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.04927495867013931, Class Loss=0.04927495867013931, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/105, Loss=3.924691231921315
Loss made of: CE 0.0897570252418518, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.395841360092163 EntMin 0.0
Epoch 2, Batch 20/105, Loss=4.0991600677371025
Loss made of: CE 0.05332378298044205, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.982387065887451 EntMin 0.0
Epoch 2, Batch 30/105, Loss=4.0624547027051445
Loss made of: CE 0.05562815070152283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2919111251831055 EntMin 0.0
Epoch 2, Batch 40/105, Loss=3.9582648195326327
Loss made of: CE 0.07411502301692963, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7776341438293457 EntMin 0.0
Epoch 2, Batch 50/105, Loss=4.034540191292763
Loss made of: CE 0.06980796158313751, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8427979946136475 EntMin 0.0
Epoch 2, Batch 60/105, Loss=4.010116799920797
Loss made of: CE 0.08664818853139877, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9410152435302734 EntMin 0.0
Epoch 2, Batch 70/105, Loss=3.965740531682968
Loss made of: CE 0.08647555112838745, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9778261184692383 EntMin 0.0
Epoch 2, Batch 80/105, Loss=3.8721183028072117
Loss made of: CE 0.06623311340808868, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5641212463378906 EntMin 0.0
Epoch 2, Batch 90/105, Loss=3.8475131668150424
Loss made of: CE 0.06630430370569229, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5159497261047363 EntMin 0.0
Epoch 2, Batch 100/105, Loss=4.057364512607455
Loss made of: CE 0.08965492248535156, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9738125801086426 EntMin 0.0
Epoch 2, Class Loss=0.08361309766769409, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.08361309766769409, Class Loss=0.08361309766769409, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/105, Loss=4.2245152704417706
Loss made of: CE 0.13416367769241333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6355323791503906 EntMin 0.0
Epoch 3, Batch 20/105, Loss=3.702709187194705
Loss made of: CE 0.11261720955371857, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5271804332733154 EntMin 0.0
Epoch 3, Batch 30/105, Loss=3.911247295886278
Loss made of: CE 0.1119823306798935, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.518526792526245 EntMin 0.0
Epoch 3, Batch 40/105, Loss=3.957692716270685
Loss made of: CE 0.14096930623054504, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.055974006652832 EntMin 0.0
Epoch 3, Batch 50/105, Loss=3.8257707834243773
Loss made of: CE 0.1322985142469406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5100669860839844 EntMin 0.0
Epoch 3, Batch 60/105, Loss=4.033038836717606
Loss made of: CE 0.08964896202087402, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.436396598815918 EntMin 0.0
Epoch 3, Batch 70/105, Loss=4.060520737618208
Loss made of: CE 0.19562213122844696, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.000953674316406 EntMin 0.0
Epoch 3, Batch 80/105, Loss=3.8937785610556603
Loss made of: CE 0.0789014995098114, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0488345623016357 EntMin 0.0
Epoch 3, Batch 90/105, Loss=3.7887276344001295
Loss made of: CE 0.12203319370746613, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7163796424865723 EntMin 0.0
Epoch 3, Batch 100/105, Loss=3.8857145234942436
Loss made of: CE 0.09106822311878204, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3580386638641357 EntMin 0.0
Epoch 3, Class Loss=0.11817743629217148, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.11817743629217148, Class Loss=0.11817743629217148, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/105, Loss=3.9509361453354357
Loss made of: CE 0.1565474569797516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.29951810836792 EntMin 0.0
Epoch 4, Batch 20/105, Loss=3.9635844118893147
Loss made of: CE 0.1723286360502243, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5871856212615967 EntMin 0.0
Epoch 4, Batch 30/105, Loss=3.9206170618534086
Loss made of: CE 0.13966867327690125, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7210144996643066 EntMin 0.0
Epoch 4, Batch 40/105, Loss=3.83520570024848
Loss made of: CE 0.19544163346290588, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.496455192565918 EntMin 0.0
Epoch 4, Batch 50/105, Loss=3.825942023098469
Loss made of: CE 0.21631625294685364, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3754241466522217 EntMin 0.0
Epoch 4, Batch 60/105, Loss=3.8219901114702224
Loss made of: CE 0.1956678330898285, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.885573148727417 EntMin 0.0
Epoch 4, Batch 70/105, Loss=3.6705357491970063
Loss made of: CE 0.16615255177021027, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1776719093322754 EntMin 0.0
Epoch 4, Batch 80/105, Loss=4.0402014926075935
Loss made of: CE 0.13351896405220032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.197899341583252 EntMin 0.0
Epoch 4, Batch 90/105, Loss=4.008139115571976
Loss made of: CE 0.13754668831825256, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4467177391052246 EntMin 0.0
Epoch 4, Batch 100/105, Loss=3.664645183086395
Loss made of: CE 0.14599063992500305, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1223902702331543 EntMin 0.0
Epoch 4, Class Loss=0.16059082746505737, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.16059082746505737, Class Loss=0.16059082746505737, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/105, Loss=3.8260080114007
Loss made of: CE 0.21017837524414062, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.543578624725342 EntMin 0.0
Epoch 5, Batch 20/105, Loss=3.8936064705252647
Loss made of: CE 0.24506786465644836, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.41170072555542 EntMin 0.0
Epoch 5, Batch 30/105, Loss=3.7561519160866736
Loss made of: CE 0.21537935733795166, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.620506763458252 EntMin 0.0
Epoch 5, Batch 40/105, Loss=3.9911727502942087
Loss made of: CE 0.19573578238487244, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4861485958099365 EntMin 0.0
Epoch 5, Batch 50/105, Loss=3.688654600083828
Loss made of: CE 0.1556662917137146, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1765646934509277 EntMin 0.0
Epoch 5, Batch 60/105, Loss=4.180556750297546
Loss made of: CE 0.22916406393051147, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.284921646118164 EntMin 0.0
Epoch 5, Batch 70/105, Loss=3.844530113041401
Loss made of: CE 0.20068582892417908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623404026031494 EntMin 0.0
Epoch 5, Batch 80/105, Loss=3.816181054711342
Loss made of: CE 0.16511937975883484, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.674729347229004 EntMin 0.0
Epoch 5, Batch 90/105, Loss=3.7806525439023972
Loss made of: CE 0.1460082232952118, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.087801456451416 EntMin 0.0
Epoch 5, Batch 100/105, Loss=3.964608204364777
Loss made of: CE 0.20843493938446045, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.529991626739502 EntMin 0.0
Epoch 5, Class Loss=0.19996723532676697, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.19996723532676697, Class Loss=0.19996723532676697, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/105, Loss=4.1022537931799885
Loss made of: CE 0.21919243037700653, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.312206983566284 EntMin 0.0
Epoch 6, Batch 20/105, Loss=3.747237502038479
Loss made of: CE 0.21976624429225922, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.337024450302124 EntMin 0.0
Epoch 6, Batch 30/105, Loss=3.674468296766281
Loss made of: CE 0.18382582068443298, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.897158145904541 EntMin 0.0
Epoch 6, Batch 40/105, Loss=3.9528882279992104
Loss made of: CE 0.1829068511724472, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.005819797515869 EntMin 0.0
Epoch 6, Batch 50/105, Loss=3.985347156226635
Loss made of: CE 0.23768286406993866, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0859575271606445 EntMin 0.0
Epoch 6, Batch 60/105, Loss=3.7630346134305
Loss made of: CE 0.20733605325222015, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6415860652923584 EntMin 0.0
Epoch 6, Batch 70/105, Loss=3.6732391148805617
Loss made of: CE 0.19516992568969727, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2642695903778076 EntMin 0.0
Epoch 6, Batch 80/105, Loss=3.8668151289224624
Loss made of: CE 0.17974728345870972, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.486147880554199 EntMin 0.0
Epoch 6, Batch 90/105, Loss=3.6908431723713875
Loss made of: CE 0.2747545838356018, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.590913772583008 EntMin 0.0
Epoch 6, Batch 100/105, Loss=3.6064947351813315
Loss made of: CE 0.20336799323558807, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.26711368560791 EntMin 0.0
Epoch 6, Class Loss=0.2273775041103363, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.2273775041103363, Class Loss=0.2273775041103363, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=4.145188015699387
Loss made of: CE 0.4347091615200043, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.518340587615967 EntMin 0.0
Epoch 1, Batch 20/102, Loss=4.269409009814263
Loss made of: CE 0.4398363530635834, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.052651405334473 EntMin 0.0
Epoch 1, Batch 30/102, Loss=4.209360963106155
Loss made of: CE 0.3566926419734955, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.092083930969238 EntMin 0.0
Epoch 1, Batch 40/102, Loss=4.156693589687348
Loss made of: CE 0.348734587430954, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7256789207458496 EntMin 0.0
Epoch 1, Batch 50/102, Loss=4.2341189056634905
Loss made of: CE 0.4015994071960449, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.169049263000488 EntMin 0.0
Epoch 1, Batch 60/102, Loss=4.218570134043693
Loss made of: CE 0.3711245059967041, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4320545196533203 EntMin 0.0
Epoch 1, Batch 70/102, Loss=4.233923631906509
Loss made of: CE 0.40598639845848083, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8900411128997803 EntMin 0.0
Epoch 1, Batch 80/102, Loss=4.186088609695434
Loss made of: CE 0.4329527020454407, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.408423900604248 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/102, Loss=3.7745270490646363
Loss made of: CE 0.3700583577156067, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1150989532470703 EntMin 0.0
Epoch 1, Batch 100/102, Loss=3.941115307807922
Loss made of: CE 0.4113510549068451, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4470441341400146 EntMin 0.0
Epoch 1, Class Loss=0.4002673625946045, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.4002673625946045, Class Loss=0.4002673625946045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/102, Loss=4.115626338124275
Loss made of: CE 0.38037794828414917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3610548973083496 EntMin 0.0
Epoch 2, Batch 20/102, Loss=3.938123694062233
Loss made of: CE 0.3922211825847626, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.813502788543701 EntMin 0.0
Epoch 2, Batch 30/102, Loss=4.215874463319778
Loss made of: CE 0.35853061079978943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.556110382080078 EntMin 0.0
Epoch 2, Batch 40/102, Loss=4.102348312735558
Loss made of: CE 0.4032159745693207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.111449718475342 EntMin 0.0
Epoch 2, Batch 50/102, Loss=3.8834602564573286
Loss made of: CE 0.31789034605026245, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3212404251098633 EntMin 0.0
Epoch 2, Batch 60/102, Loss=3.985083666443825
Loss made of: CE 0.3064257502555847, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.728383779525757 EntMin 0.0
Epoch 2, Batch 70/102, Loss=4.334798738360405
Loss made of: CE 0.37160393595695496, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9288368225097656 EntMin 0.0
Epoch 2, Batch 80/102, Loss=3.928020787239075
Loss made of: CE 0.3524110019207001, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.23844313621521 EntMin 0.0
Epoch 2, Batch 90/102, Loss=3.9959446668624876
Loss made of: CE 0.34811079502105713, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3818016052246094 EntMin 0.0
Epoch 2, Batch 100/102, Loss=4.173385033011437
Loss made of: CE 0.33772289752960205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.103808879852295 EntMin 0.0
Epoch 2, Class Loss=0.3748292624950409, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.3748292624950409, Class Loss=0.3748292624950409, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/102, Loss=4.293982979655266
Loss made of: CE 0.3583821654319763, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.308228015899658 EntMin 0.0
Epoch 3, Batch 20/102, Loss=4.0662054270505905
Loss made of: CE 0.3566809296607971, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.976107597351074 EntMin 0.0
Epoch 3, Batch 30/102, Loss=3.9119582206010817
Loss made of: CE 0.35234570503234863, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5576462745666504 EntMin 0.0
Epoch 3, Batch 40/102, Loss=3.9824253141880037
Loss made of: CE 0.37214311957359314, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9481558799743652 EntMin 0.0
Epoch 3, Batch 50/102, Loss=4.083225607872009
Loss made of: CE 0.38173604011535645, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.259442329406738 EntMin 0.0
Epoch 3, Batch 60/102, Loss=3.899550348520279
Loss made of: CE 0.3465791642665863, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2672228813171387 EntMin 0.0
Epoch 3, Batch 70/102, Loss=4.072205391526222
Loss made of: CE 0.3964018225669861, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.526496648788452 EntMin 0.0
Epoch 3, Batch 80/102, Loss=3.9070046573877333
Loss made of: CE 0.31067532300949097, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.486351490020752 EntMin 0.0
Epoch 3, Batch 90/102, Loss=3.9921045869588854
Loss made of: CE 0.3890342116355896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2422540187835693 EntMin 0.0
Epoch 3, Batch 100/102, Loss=3.8110300064086915
Loss made of: CE 0.3261793255805969, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3459460735321045 EntMin 0.0
Epoch 3, Class Loss=0.3690987825393677, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3690987825393677, Class Loss=0.3690987825393677, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=4.055656310915947
Loss made of: CE 0.39253002405166626, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.565398693084717 EntMin 0.0
Epoch 4, Batch 20/102, Loss=3.906794419884682
Loss made of: CE 0.38670608401298523, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.552861213684082 EntMin 0.0
Epoch 4, Batch 30/102, Loss=3.942052897810936
Loss made of: CE 0.3176220655441284, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1293139457702637 EntMin 0.0
Epoch 4, Batch 40/102, Loss=3.9754503101110457
Loss made of: CE 0.3297497034072876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4423911571502686 EntMin 0.0
Epoch 4, Batch 50/102, Loss=3.8787059456110002
Loss made of: CE 0.3755888342857361, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.0882058143615723 EntMin 0.0
Epoch 4, Batch 60/102, Loss=4.055468687415123
Loss made of: CE 0.3973548412322998, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.525750160217285 EntMin 0.0
Epoch 4, Batch 70/102, Loss=3.9347192674875258
Loss made of: CE 0.3564286231994629, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5022854804992676 EntMin 0.0
Epoch 4, Batch 80/102, Loss=4.021374541521072
Loss made of: CE 0.3844482898712158, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4993715286254883 EntMin 0.0
Epoch 4, Batch 90/102, Loss=4.037002038955689
Loss made of: CE 0.36922991275787354, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9471821784973145 EntMin 0.0
Epoch 4, Batch 100/102, Loss=4.205292928218841
Loss made of: CE 0.35577842593193054, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.351900815963745 EntMin 0.0
Epoch 4, Class Loss=0.3711930215358734, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.3711930215358734, Class Loss=0.3711930215358734, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/102, Loss=3.816739895939827
Loss made of: CE 0.43649882078170776, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5009818077087402 EntMin 0.0
Epoch 5, Batch 20/102, Loss=4.033704084157944
Loss made of: CE 0.379502534866333, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8088808059692383 EntMin 0.0
Epoch 5, Batch 30/102, Loss=4.155084171891213
Loss made of: CE 0.42756521701812744, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.65506911277771 EntMin 0.0
Epoch 5, Batch 40/102, Loss=3.9790288507938385
Loss made of: CE 0.3069411516189575, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1713805198669434 EntMin 0.0
Epoch 5, Batch 50/102, Loss=3.856049355864525
Loss made of: CE 0.4168243110179901, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6742959022521973 EntMin 0.0
Epoch 5, Batch 60/102, Loss=3.894342687726021
Loss made of: CE 0.3188287019729614, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.049276828765869 EntMin 0.0
Epoch 5, Batch 70/102, Loss=4.036399856209755
Loss made of: CE 0.3296436071395874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7198054790496826 EntMin 0.0
Epoch 5, Batch 80/102, Loss=4.058148485422135
Loss made of: CE 0.3181195855140686, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4384608268737793 EntMin 0.0
Epoch 5, Batch 90/102, Loss=3.821706146001816
Loss made of: CE 0.4431731402873993, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4909772872924805 EntMin 0.0
Epoch 5, Batch 100/102, Loss=3.892168864607811
Loss made of: CE 0.307685911655426, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.282791614532471 EntMin 0.0
Epoch 5, Class Loss=0.36154791712760925, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.36154791712760925, Class Loss=0.36154791712760925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/102, Loss=3.906172749400139
Loss made of: CE 0.37129244208335876, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1119823455810547 EntMin 0.0
Epoch 6, Batch 20/102, Loss=4.128386783599853
Loss made of: CE 0.375623881816864, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.8453006744384766 EntMin 0.0
Epoch 6, Batch 30/102, Loss=3.6591365456581117
Loss made of: CE 0.3477236032485962, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.206674098968506 EntMin 0.0
Epoch 6, Batch 40/102, Loss=3.8619088411331175
Loss made of: CE 0.31331247091293335, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1576781272888184 EntMin 0.0
Epoch 6, Batch 50/102, Loss=3.947887209057808
Loss made of: CE 0.4120402932167053, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5123162269592285 EntMin 0.0
Epoch 6, Batch 60/102, Loss=3.8404729276895524
Loss made of: CE 0.2900547385215759, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3801467418670654 EntMin 0.0
Epoch 6, Batch 70/102, Loss=3.8987188786268234
Loss made of: CE 0.3313947916030884, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.650200605392456 EntMin 0.0
Epoch 6, Batch 80/102, Loss=3.7920128405094147
Loss made of: CE 0.314904123544693, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5685794353485107 EntMin 0.0
Epoch 6, Batch 90/102, Loss=3.9006827980279923
Loss made of: CE 0.3758828043937683, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2684972286224365 EntMin 0.0
Epoch 6, Batch 100/102, Loss=3.850033035874367
Loss made of: CE 0.43229442834854126, LKD 0.0, LDE 0.0, LReg 0.0, POD 2.9265007972717285 EntMin 0.0
Epoch 6, Class Loss=0.35939157009124756, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.35939157009124756, Class Loss=0.35939157009124756, Reg Loss=0.0
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/23, Loss=5.322489023208618
Loss made of: CE 0.7657610177993774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304867267608643 EntMin 0.0
Epoch 1, Batch 20/23, Loss=4.887958514690399
Loss made of: CE 0.7215752005577087, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.066957473754883 EntMin 0.0
Epoch 1, Class Loss=0.7584999799728394, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.7584999799728394, Class Loss=0.7584999799728394, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/23, Loss=4.778890550136566
Loss made of: CE 0.6741024851799011, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9608325958251953 EntMin 0.0
Epoch 2, Batch 20/23, Loss=4.583662027120591
Loss made of: CE 0.5306896567344666, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8879668712615967 EntMin 0.0
Epoch 2, Class Loss=0.6446576118469238, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.6446576118469238, Class Loss=0.6446576118469238, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/23, Loss=4.504820001125336
Loss made of: CE 0.45077425241470337, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6260392665863037 EntMin 0.0
Epoch 3, Batch 20/23, Loss=4.579866746068001
Loss made of: CE 0.7232826948165894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4502129554748535 EntMin 0.0
Epoch 3, Class Loss=0.5848061442375183, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.5848061442375183, Class Loss=0.5848061442375183, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/23, Loss=4.596659138798714
Loss made of: CE 0.6826173067092896, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.653867244720459 EntMin 0.0
Epoch 4, Batch 20/23, Loss=4.383197170495987
Loss made of: CE 0.4646458625793457, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8279757499694824 EntMin 0.0
Epoch 4, Class Loss=0.5486676096916199, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.5486676096916199, Class Loss=0.5486676096916199, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/23, Loss=4.462518951296806
Loss made of: CE 0.4629364013671875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.119141578674316 EntMin 0.0
Epoch 5, Batch 20/23, Loss=4.294204026460648
Loss made of: CE 0.49145814776420593, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0202178955078125 EntMin 0.0
Epoch 5, Class Loss=0.5343506336212158, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.5343506336212158, Class Loss=0.5343506336212158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/23, Loss=4.429663807153702
Loss made of: CE 0.5186775326728821, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8153185844421387 EntMin 0.0
Epoch 6, Batch 20/23, Loss=4.268342909216881
Loss made of: CE 0.513862133026123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.428997993469238 EntMin 0.0
Epoch 6, Class Loss=0.5205774307250977, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.5205774307250977, Class Loss=0.5205774307250977, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/103, Loss=3.534021314792335
Loss made of: CE 0.051277726888656616, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.562091588973999 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/103, Loss=3.624731695652008
Loss made of: CE 0.04073043167591095, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.482045888900757 EntMin 0.0
Epoch 1, Batch 30/103, Loss=3.7415634777396916
Loss made of: CE 0.05491912364959717, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6541614532470703 EntMin 0.0
Epoch 1, Batch 40/103, Loss=3.972437824308872
Loss made of: CE 0.04067375883460045, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.309842109680176 EntMin 0.0
Epoch 1, Batch 50/103, Loss=4.116991456411779
Loss made of: CE 0.07839258015155792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.898235321044922 EntMin 0.0
Epoch 1, Batch 60/103, Loss=3.8478573873639106
Loss made of: CE 0.040649157017469406, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.877791404724121 EntMin 0.0
Epoch 1, Batch 70/103, Loss=3.854877693764865
Loss made of: CE 0.026680974289774895, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7585198879241943 EntMin 0.0
Epoch 1, Batch 80/103, Loss=4.084010688588023
Loss made of: CE 0.056786492466926575, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.396030426025391 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/103, Loss=3.7474003495648502
Loss made of: CE 0.047193847596645355, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.1829380989074707 EntMin 0.0
Epoch 1, Batch 100/103, Loss=3.941230666078627
Loss made of: CE 0.03833138197660446, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.67962646484375 EntMin 0.0
Epoch 1, Class Loss=0.042006153613328934, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.042006153613328934, Class Loss=0.042006153613328934, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/103, Loss=3.8290835086256267
Loss made of: CE 0.06011156365275383, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5225830078125 EntMin 0.0
Epoch 2, Batch 20/103, Loss=4.053255328908563
Loss made of: CE 0.08268497884273529, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6209487915039062 EntMin 0.0
Epoch 2, Batch 30/103, Loss=3.860827611759305
Loss made of: CE 0.05932655185461044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18817138671875 EntMin 0.0
Epoch 2, Batch 40/103, Loss=4.038429614529013
Loss made of: CE 0.11074831336736679, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.119505405426025 EntMin 0.0
Epoch 2, Batch 50/103, Loss=3.742198234051466
Loss made of: CE 0.05533815920352936, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.318380832672119 EntMin 0.0
Epoch 2, Batch 60/103, Loss=4.10133234411478
Loss made of: CE 0.04684942960739136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.218745708465576 EntMin 0.0
Epoch 2, Batch 70/103, Loss=3.8923457607626917
Loss made of: CE 0.08248046785593033, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.825232982635498 EntMin 0.0
Epoch 2, Batch 80/103, Loss=3.8537963047623633
Loss made of: CE 0.08992797136306763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.338351249694824 EntMin 0.0
Epoch 2, Batch 90/103, Loss=4.066751416027546
Loss made of: CE 0.09348322451114655, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.189935684204102 EntMin 0.0
Epoch 2, Batch 100/103, Loss=4.235350741446018
Loss made of: CE 0.06515488773584366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.105755805969238 EntMin 0.0
Epoch 2, Class Loss=0.07260239124298096, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.07260239124298096, Class Loss=0.07260239124298096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/103, Loss=3.983853325992823
Loss made of: CE 0.10203896462917328, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5433874130249023 EntMin 0.0
Epoch 3, Batch 20/103, Loss=3.895727363973856
Loss made of: CE 0.1364884227514267, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.765775203704834 EntMin 0.0
Epoch 3, Batch 30/103, Loss=3.917829466611147
Loss made of: CE 0.13699358701705933, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.358396530151367 EntMin 0.0
Epoch 3, Batch 40/103, Loss=3.9830694898962973
Loss made of: CE 0.09096704423427582, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.637981414794922 EntMin 0.0
Epoch 3, Batch 50/103, Loss=4.119540648162365
Loss made of: CE 0.14292967319488525, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500138282775879 EntMin 0.0
Epoch 3, Batch 60/103, Loss=3.8862683318555353
Loss made of: CE 0.08500441163778305, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5316553115844727 EntMin 0.0
Epoch 3, Batch 70/103, Loss=4.001204800605774
Loss made of: CE 0.10283784568309784, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.514211654663086 EntMin 0.0
Epoch 3, Batch 80/103, Loss=3.9266231186687945
Loss made of: CE 0.09053975343704224, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.705817222595215 EntMin 0.0
Epoch 3, Batch 90/103, Loss=3.9127310030162334
Loss made of: CE 0.08357678353786469, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.424409866333008 EntMin 0.0
Epoch 3, Batch 100/103, Loss=3.8755503416061403
Loss made of: CE 0.12129507958889008, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.193750381469727 EntMin 0.0
Epoch 3, Class Loss=0.10583411902189255, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.10583411902189255, Class Loss=0.10583411902189255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/103, Loss=3.8901031970977784
Loss made of: CE 0.164214089512825, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.352435827255249 EntMin 0.0
Epoch 4, Batch 20/103, Loss=3.951121252030134
Loss made of: CE 0.19047895073890686, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.529421806335449 EntMin 0.0
Epoch 4, Batch 30/103, Loss=3.8885231368243693
Loss made of: CE 0.13945242762565613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.261269569396973 EntMin 0.0
Epoch 4, Batch 40/103, Loss=3.6618742883205413
Loss made of: CE 0.13158555328845978, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4435994625091553 EntMin 0.0
Epoch 4, Batch 50/103, Loss=3.843892741203308
Loss made of: CE 0.1458771675825119, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.930166244506836 EntMin 0.0
Epoch 4, Batch 60/103, Loss=3.8696492701768874
Loss made of: CE 0.1320679783821106, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6254196166992188 EntMin 0.0
Epoch 4, Batch 70/103, Loss=3.741040163487196
Loss made of: CE 0.15894779562950134, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.815366506576538 EntMin 0.0
Epoch 4, Batch 80/103, Loss=3.902944713830948
Loss made of: CE 0.14070288836956024, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2879157066345215 EntMin 0.0
Epoch 4, Batch 90/103, Loss=3.6776740573346616
Loss made of: CE 0.132399320602417, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5797066688537598 EntMin 0.0
Epoch 4, Batch 100/103, Loss=3.984264051914215
Loss made of: CE 0.1583070009946823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.430901050567627 EntMin 0.0
Epoch 4, Class Loss=0.14570479094982147, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.14570479094982147, Class Loss=0.14570479094982147, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/103, Loss=3.9189796045422556
Loss made of: CE 0.1613267958164215, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.672074317932129 EntMin 0.0
Epoch 5, Batch 20/103, Loss=4.05238973647356
Loss made of: CE 0.1683398187160492, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.889419078826904 EntMin 0.0
Epoch 5, Batch 30/103, Loss=3.762668652832508
Loss made of: CE 0.20882251858711243, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2450103759765625 EntMin 0.0
Epoch 5, Batch 40/103, Loss=3.8839912325143815
Loss made of: CE 0.15977618098258972, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.115038871765137 EntMin 0.0
Epoch 5, Batch 50/103, Loss=3.9067041307687758
Loss made of: CE 0.14752237498760223, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.330439329147339 EntMin 0.0
Epoch 5, Batch 60/103, Loss=3.735665997862816
Loss made of: CE 0.2212209701538086, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.509948492050171 EntMin 0.0
Epoch 5, Batch 70/103, Loss=3.6704341530799867
Loss made of: CE 0.1776372194290161, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5454578399658203 EntMin 0.0
Epoch 5, Batch 80/103, Loss=3.9981083303689955
Loss made of: CE 0.16565516591072083, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.3820910453796387 EntMin 0.0
Epoch 5, Batch 90/103, Loss=3.91709523499012
Loss made of: CE 0.17095977067947388, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.563882350921631 EntMin 0.0
Epoch 5, Batch 100/103, Loss=3.8890134915709496
Loss made of: CE 0.15637823939323425, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5292975902557373 EntMin 0.0
Epoch 5, Class Loss=0.18309606611728668, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.18309606611728668, Class Loss=0.18309606611728668, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/103, Loss=3.7229670569300652
Loss made of: CE 0.1909836381673813, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.2619032859802246 EntMin 0.0
Epoch 6, Batch 20/103, Loss=3.7928373515605927
Loss made of: CE 0.1886560320854187, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.03603458404541 EntMin 0.0
Epoch 6, Batch 30/103, Loss=3.738170216977596
Loss made of: CE 0.17925283312797546, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.510469436645508 EntMin 0.0
Epoch 6, Batch 40/103, Loss=3.6458937227725983
Loss made of: CE 0.17899654805660248, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.114093780517578 EntMin 0.0
Epoch 6, Batch 50/103, Loss=3.7903734877705575
Loss made of: CE 0.1915653795003891, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.5125675201416016 EntMin 0.0
Epoch 6, Batch 60/103, Loss=3.854427732527256
Loss made of: CE 0.18102721869945526, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.45810604095459 EntMin 0.0
Epoch 6, Batch 70/103, Loss=4.100280213356018
Loss made of: CE 0.17110031843185425, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.70334792137146 EntMin 0.0
Epoch 6, Batch 80/103, Loss=3.9533752620220186
Loss made of: CE 0.2488459348678589, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4854259490966797 EntMin 0.0
Epoch 6, Batch 90/103, Loss=4.04970758408308
Loss made of: CE 0.21265941858291626, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.552232503890991 EntMin 0.0
Epoch 6, Batch 100/103, Loss=3.903451254963875
Loss made of: CE 0.22605650126934052, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7390198707580566 EntMin 0.0
Epoch 6, Class Loss=0.20976445078849792, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.20976445078849792, Class Loss=0.20976445078849792, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.43688949942588806, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.869744
Mean Acc: 0.598185
FreqW Acc: 0.785058
Mean IoU: 0.457712
Class IoU:
	class 0: 0.8900096
	class 1: 0.3511305
	class 2: 0.27262312
	class 3: 0.5572751
	class 4: 0.5102843
	class 5: 0.18810968
	class 6: 0.84312254
	class 7: 0.6303988
	class 8: 0.7277863
	class 9: 0.0
	class 10: 0.23255585
	class 11: 0.2890081
	class 12: 0.5598817
	class 13: 0.41680732
	class 14: 0.64808255
	class 15: 0.6640317
	class 16: 0.0
Class Acc:
	class 0: 0.9401302
	class 1: 0.3519537
	class 2: 0.4846481
	class 3: 0.6210869
	class 4: 0.90646976
	class 5: 0.19039847
	class 6: 0.90206534
	class 7: 0.90561223
	class 8: 0.825629
	class 9: 0.0
	class 10: 0.23619704
	class 11: 0.5359448
	class 12: 0.8212716
	class 13: 0.7006133
	class 14: 0.8482897
	class 15: 0.8988381
	class 16: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[19, 23, 1, 8]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=8.79100524187088
Loss made of: CE 0.8018616437911987, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.453587532043457 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=7.602811187505722
Loss made of: CE 0.5228416919708252, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.005462646484375 EntMin 0.0
Epoch 1, Class Loss=0.7871851325035095, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.7871851325035095, Class Loss=0.7871851325035095, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=7.542557847499848
Loss made of: CE 0.7309364080429077, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.043388366699219 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.874204057455063
Loss made of: CE 0.6951042413711548, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1621294021606445 EntMin 0.0
Epoch 2, Class Loss=0.7235164046287537, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.7235164046287537, Class Loss=0.7235164046287537, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=6.386035043001175
Loss made of: CE 0.6245604753494263, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.543262481689453 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.103745609521866
Loss made of: CE 0.5023002624511719, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9455413818359375 EntMin 0.0
Epoch 3, Class Loss=0.6591179966926575, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.6591179966926575, Class Loss=0.6591179966926575, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=5.838959836959839
Loss made of: CE 0.6178104281425476, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.153520584106445 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.263283604383469
Loss made of: CE 0.5759273767471313, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.264842510223389 EntMin 0.0
Epoch 4, Class Loss=0.6263488531112671, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.6263488531112671, Class Loss=0.6263488531112671, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=5.543746000528335
Loss made of: CE 0.6116689443588257, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.70063591003418 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.99456524848938
Loss made of: CE 0.5014114379882812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.678871154785156 EntMin 0.0
Epoch 5, Class Loss=0.5839359760284424, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.5839359760284424, Class Loss=0.5839359760284424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=5.791476917266846
Loss made of: CE 0.5403448939323425, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.105680465698242 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.3933058142662045
Loss made of: CE 0.6037100553512573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594947814941406 EntMin 0.0
Epoch 6, Class Loss=0.5355627536773682, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.5355627536773682, Class Loss=0.5355627536773682, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=7.983479589223862
Loss made of: CE 0.5395442247390747, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.588061809539795 EntMin 0.0
Epoch 1, Batch 20/21, Loss=6.9480255335569385
Loss made of: CE 0.5742493867874146, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.917499542236328 EntMin 0.0
Epoch 1, Class Loss=0.5642569065093994, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.5642569065093994, Class Loss=0.5642569065093994, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/21, Loss=6.429977595806122
Loss made of: CE 0.5541824698448181, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.396024227142334 EntMin 0.0
Epoch 2, Batch 20/21, Loss=6.0111042857170105
Loss made of: CE 0.5186431407928467, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.909832000732422 EntMin 0.0
Epoch 2, Class Loss=0.5815615057945251, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.5815615057945251, Class Loss=0.5815615057945251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/21, Loss=6.316059404611588
Loss made of: CE 0.4635143280029297, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.959376335144043 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.950311359763146
Loss made of: CE 0.566141664981842, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.805018901824951 EntMin 0.0
Epoch 3, Class Loss=0.5828325152397156, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.5828325152397156, Class Loss=0.5828325152397156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/21, Loss=6.1326158285140995
Loss made of: CE 0.5950636863708496, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.611917018890381 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.371261572837829
Loss made of: CE 0.45821890234947205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.442798614501953 EntMin 0.0
Epoch 4, Class Loss=0.5869413614273071, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.5869413614273071, Class Loss=0.5869413614273071, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/21, Loss=5.736683517694473
Loss made of: CE 0.5718626976013184, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.170627593994141 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.698149809241295
Loss made of: CE 0.6060766577720642, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.974287509918213 EntMin 0.0
Epoch 5, Class Loss=0.5659627318382263, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.5659627318382263, Class Loss=0.5659627318382263, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/21, Loss=5.570446121692657
Loss made of: CE 0.5687946081161499, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.897848129272461 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.527289116382599
Loss made of: CE 0.5759059190750122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.371381759643555 EntMin 0.0
Epoch 6, Class Loss=0.5667792558670044, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.5667792558670044, Class Loss=0.5667792558670044, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=10.04807140827179
Loss made of: CE 0.9704086780548096, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.227807998657227 EntMin 0.0
Epoch 1, Class Loss=0.901984453201294, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.901984453201294, Class Loss=0.901984453201294, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/19, Loss=7.9344256043434145
Loss made of: CE 0.7979888916015625, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.091794967651367 EntMin 0.0
Epoch 2, Class Loss=0.800476610660553, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.800476610660553, Class Loss=0.800476610660553, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=7.326294708251953
Loss made of: CE 0.7437750101089478, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.6923041343688965 EntMin 0.0
Epoch 3, Class Loss=0.7217990756034851, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.7217990756034851, Class Loss=0.7217990756034851, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=6.809064149856567
Loss made of: CE 0.6112953424453735, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.518191814422607 EntMin 0.0
Epoch 4, Class Loss=0.6391214728355408, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.6391214728355408, Class Loss=0.6391214728355408, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=6.556712555885315
Loss made of: CE 0.5520982146263123, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.15243673324585 EntMin 0.0
Epoch 5, Class Loss=0.5498465299606323, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.5498465299606323, Class Loss=0.5498465299606323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=6.137894865870476
Loss made of: CE 0.4508912265300751, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3022661209106445 EntMin 0.0
Epoch 6, Class Loss=0.48619160056114197, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.48619160056114197, Class Loss=0.48619160056114197, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=8.779914417862893
Loss made of: CE 0.27930063009262085, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.930368423461914 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=7.329012453556061
Loss made of: CE 0.33184245228767395, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.27658224105835 EntMin 0.0
Epoch 1, Class Loss=0.4845293164253235, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.4845293164253235, Class Loss=0.4845293164253235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=6.969741657376289
Loss made of: CE 0.4643157422542572, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.114988327026367 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.630006295442581
Loss made of: CE 0.4695565402507782, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.649991512298584 EntMin 0.0
Epoch 2, Class Loss=0.54979407787323, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.54979407787323, Class Loss=0.54979407787323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=6.553539907932281
Loss made of: CE 0.6474224925041199, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6142578125 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.183087354898452
Loss made of: CE 0.4108113646507263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.956592559814453 EntMin 0.0
Epoch 3, Class Loss=0.5479326248168945, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5479326248168945, Class Loss=0.5479326248168945, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=6.0077362060546875
Loss made of: CE 0.6509211659431458, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.658722877502441 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.116351014375686
Loss made of: CE 0.41749340295791626, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2079176902771 EntMin 0.0
Epoch 4, Class Loss=0.5861286520957947, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5861286520957947, Class Loss=0.5861286520957947, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=6.264874839782715
Loss made of: CE 0.7388566732406616, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.513803482055664 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.7493769228458405
Loss made of: CE 0.5099763870239258, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.949703216552734 EntMin 0.0
Epoch 5, Class Loss=0.5950505137443542, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.5950505137443542, Class Loss=0.5950505137443542, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=5.957814311981201
Loss made of: CE 0.5663792490959167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.798214912414551 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.7256629228591915
Loss made of: CE 0.5240274667739868, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.30354118347168 EntMin 0.0
Epoch 6, Class Loss=0.6047513484954834, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6047513484954834, Class Loss=0.6047513484954834, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6625635027885437, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.827492
Mean Acc: 0.412270
FreqW Acc: 0.703303
Mean IoU: 0.322743
Class IoU:
	class 0: 0.83280635
	class 1: 0.109304786
	class 2: 0.14143398
	class 3: 0.33519006
	class 4: 0.44958332
	class 5: 0.04507873
	class 6: 0.71501905
	class 7: 0.7081243
	class 8: 0.65864235
	class 9: 0.0
	class 10: 0.22014774
	class 11: 0.31007954
	class 12: 0.5261373
	class 13: 0.35162735
	class 14: 0.66560316
	class 15: 0.70882404
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0
Class Acc:
	class 0: 0.96602756
	class 1: 0.1093648
	class 2: 0.1773178
	class 3: 0.3485364
	class 4: 0.85293293
	class 5: 0.04508402
	class 6: 0.7965507
	class 7: 0.8462133
	class 8: 0.70577
	class 9: 0.0
	class 10: 0.2391397
	class 11: 0.5547969
	class 12: 0.7636803
	class 13: 0.5505253
	class 14: 0.8254207
	class 15: 0.8763147
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[23, 14, 4, 11]
Current Client Index:  23
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=6.325822961330414
Loss made of: CE 0.7407892942428589, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.699763298034668 EntMin 0.0
Epoch 1, Batch 20/21, Loss=6.034880495071411
Loss made of: CE 0.8151119947433472, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.475879192352295 EntMin 0.0
Epoch 1, Class Loss=0.8272135853767395, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.8272135853767395, Class Loss=0.8272135853767395, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/21, Loss=5.673987609148026
Loss made of: CE 0.6923813223838806, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.458863258361816 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.520433765649796
Loss made of: CE 0.6272444725036621, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.705894470214844 EntMin 0.0
Epoch 2, Class Loss=0.6982883214950562, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.6982883214950562, Class Loss=0.6982883214950562, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/21, Loss=6.019068235158921
Loss made of: CE 0.5983560085296631, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.836329460144043 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.596463084220886
Loss made of: CE 0.6828224062919617, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.281718730926514 EntMin 0.0
Epoch 3, Class Loss=0.6391181945800781, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.6391181945800781, Class Loss=0.6391181945800781, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/21, Loss=5.60942794084549
Loss made of: CE 0.6029746532440186, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.003880500793457 EntMin 0.0
Epoch 4, Batch 20/21, Loss=5.025438210368156
Loss made of: CE 0.48208650946617126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.17866325378418 EntMin 0.0
Epoch 4, Class Loss=0.5695554614067078, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.5695554614067078, Class Loss=0.5695554614067078, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=5.344723433256149
Loss made of: CE 0.4754640460014343, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.775460720062256 EntMin 0.0
Epoch 5, Batch 20/21, Loss=5.338604295253754
Loss made of: CE 0.5548989772796631, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.578736305236816 EntMin 0.0
Epoch 5, Class Loss=0.5190125703811646, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.5190125703811646, Class Loss=0.5190125703811646, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/21, Loss=5.3220726490020756
Loss made of: CE 0.4411603808403015, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.317537307739258 EntMin 0.0
Epoch 6, Batch 20/21, Loss=5.061067482829094
Loss made of: CE 0.4512156844139099, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.192782878875732 EntMin 0.0
Epoch 6, Class Loss=0.4580554664134979, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.4580554664134979, Class Loss=0.4580554664134979, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=6.879985770583152
Loss made of: CE 0.3417435586452484, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.671384334564209 EntMin 0.0
Epoch 1, Batch 20/26, Loss=6.153881472349167
Loss made of: CE 0.3470822274684906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8037614822387695 EntMin 0.0
Epoch 1, Class Loss=0.39953750371932983, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.39953750371932983, Class Loss=0.39953750371932983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=6.232251816987992
Loss made of: CE 0.4694318473339081, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.101572036743164 EntMin 0.0
Epoch 2, Batch 20/26, Loss=5.9664306253194805
Loss made of: CE 0.5632659792900085, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.959348201751709 EntMin 0.0
Epoch 2, Class Loss=0.4352138042449951, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.4352138042449951, Class Loss=0.4352138042449951, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/26, Loss=5.675650808215141
Loss made of: CE 0.4406815469264984, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.253296375274658 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.722308287024498
Loss made of: CE 0.3775637149810791, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.752921104431152 EntMin 0.0
Epoch 3, Class Loss=0.4390024244785309, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.4390024244785309, Class Loss=0.4390024244785309, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.357978975772857
Loss made of: CE 0.4457572400569916, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.079383850097656 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.330593904852867
Loss made of: CE 0.4433324933052063, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.657977104187012 EntMin 0.0
Epoch 4, Class Loss=0.43710383772850037, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.43710383772850037, Class Loss=0.43710383772850037, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.227866962552071
Loss made of: CE 0.646659791469574, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.059682369232178 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.3126443207263945
Loss made of: CE 0.4457457959651947, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.893843650817871 EntMin 0.0
Epoch 5, Class Loss=0.4365929961204529, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.4365929961204529, Class Loss=0.4365929961204529, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.233128094673157
Loss made of: CE 0.4351637065410614, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.006150245666504 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.186663615703583
Loss made of: CE 0.39138612151145935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.934548854827881 EntMin 0.0
Epoch 6, Class Loss=0.43807709217071533, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.43807709217071533, Class Loss=0.43807709217071533, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=6.524547746777534
Loss made of: CE 0.3901394307613373, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0968499183654785 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/26, Loss=6.301757410168648
Loss made of: CE 0.4384097456932068, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.806068420410156 EntMin 0.0
Epoch 1, Class Loss=0.3949497938156128, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.3949497938156128, Class Loss=0.3949497938156128, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=5.815121945738793
Loss made of: CE 0.45947203040122986, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.791325569152832 EntMin 0.0
Epoch 2, Batch 20/26, Loss=6.039944985508919
Loss made of: CE 0.4743811786174774, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.618125915527344 EntMin 0.0
Epoch 2, Class Loss=0.4339413344860077, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.4339413344860077, Class Loss=0.4339413344860077, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=5.8982229173183445
Loss made of: CE 0.4477784037590027, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.065883159637451 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.457886481285096
Loss made of: CE 0.42451009154319763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.978743076324463 EntMin 0.0
Epoch 3, Class Loss=0.4353993535041809, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.4353993535041809, Class Loss=0.4353993535041809, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=5.380516773462295
Loss made of: CE 0.34763702750205994, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623601913452148 EntMin 0.0
Epoch 4, Batch 20/26, Loss=5.223949497938156
Loss made of: CE 0.3693488538265228, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.619194984436035 EntMin 0.0
Epoch 4, Class Loss=0.43763598799705505, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.43763598799705505, Class Loss=0.43763598799705505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=5.452660986781121
Loss made of: CE 0.3831969201564789, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.364783763885498 EntMin 0.0
Epoch 5, Batch 20/26, Loss=5.4536865025758745
Loss made of: CE 0.39959630370140076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.634763717651367 EntMin 0.0
Epoch 5, Class Loss=0.4378914535045624, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.4378914535045624, Class Loss=0.4378914535045624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=5.593586003780365
Loss made of: CE 0.3606651723384857, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.556602478027344 EntMin 0.0
Epoch 6, Batch 20/26, Loss=5.035888430476189
Loss made of: CE 0.4988456964492798, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2581658363342285 EntMin 0.0
Epoch 6, Class Loss=0.428654283285141, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.428654283285141, Class Loss=0.428654283285141, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=6.4234837397933005
Loss made of: CE 0.4091835618019104, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4039306640625 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=6.223298490047455
Loss made of: CE 0.45234912633895874, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.831297874450684 EntMin 0.0
Epoch 1, Class Loss=0.3491381108760834, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.3491381108760834, Class Loss=0.3491381108760834, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/24, Loss=6.050317060947418
Loss made of: CE 0.43085384368896484, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.317641735076904 EntMin 0.0
Epoch 2, Batch 20/24, Loss=6.027136713266373
Loss made of: CE 0.40945935249328613, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0131731033325195 EntMin 0.0
Epoch 2, Class Loss=0.40516898036003113, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.40516898036003113, Class Loss=0.40516898036003113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/24, Loss=5.835018590092659
Loss made of: CE 0.48113879561424255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.895452499389648 EntMin 0.0
Epoch 3, Batch 20/24, Loss=6.031537413597107
Loss made of: CE 0.3168365955352783, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.733371257781982 EntMin 0.0
Epoch 3, Class Loss=0.4563380479812622, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4563380479812622, Class Loss=0.4563380479812622, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/24, Loss=5.349980053305626
Loss made of: CE 0.47502613067626953, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.751612663269043 EntMin 0.0
Epoch 4, Batch 20/24, Loss=6.118933978676796
Loss made of: CE 0.49807000160217285, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.24431037902832 EntMin 0.0
Epoch 4, Class Loss=0.4725392460823059, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4725392460823059, Class Loss=0.4725392460823059, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/24, Loss=5.803743812441826
Loss made of: CE 0.47649192810058594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9712934494018555 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.697021773457527
Loss made of: CE 0.4245557188987732, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.357413291931152 EntMin 0.0
Epoch 5, Class Loss=0.48845189809799194, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.48845189809799194, Class Loss=0.48845189809799194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/24, Loss=5.454819139838219
Loss made of: CE 0.5166466236114502, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.099200248718262 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.408052745461464
Loss made of: CE 0.5067408084869385, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.71856689453125 EntMin 0.0
Epoch 6, Class Loss=0.48647767305374146, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.48647767305374146, Class Loss=0.48647767305374146, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.598406195640564, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.836742
Mean Acc: 0.446011
FreqW Acc: 0.720813
Mean IoU: 0.352787
Class IoU:
	class 0: 0.84290946
	class 1: 0.16676751
	class 2: 0.13486144
	class 3: 0.32962283
	class 4: 0.44027314
	class 5: 0.053763993
	class 6: 0.7326936
	class 7: 0.6869222
	class 8: 0.6875097
	class 9: 0.0
	class 10: 0.3326856
	class 11: 0.28142878
	class 12: 0.53685445
	class 13: 0.37066767
	class 14: 0.6560335
	class 15: 0.72574383
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.26792097
	class 20: 0.16185786
Class Acc:
	class 0: 0.964268
	class 1: 0.16693161
	class 2: 0.16669449
	class 3: 0.34172466
	class 4: 0.86494434
	class 5: 0.053768363
	class 6: 0.7925106
	class 7: 0.84555393
	class 8: 0.75894034
	class 9: 0.0
	class 10: 0.39680165
	class 11: 0.5745431
	class 12: 0.78464365
	class 13: 0.503456
	class 14: 0.84282
	class 15: 0.86148965
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
	class 19: 0.27374578
	class 20: 0.17338757

federated global round: 22, step: 4
select part of clients to conduct local training
[0, 8, 16, 14]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.201470693945884
Loss made of: CE 0.35373610258102417, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.071210861206055 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.725945672392845
Loss made of: CE 0.3606959283351898, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.707551956176758 EntMin 0.0
Epoch 1, Class Loss=0.27188408374786377, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.27188408374786377, Class Loss=0.27188408374786377, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/24, Loss=5.661387655138969
Loss made of: CE 0.2267942875623703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0343475341796875 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.235897333920002
Loss made of: CE 0.2685207724571228, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.189029693603516 EntMin 0.0
Epoch 2, Class Loss=0.3024898171424866, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.3024898171424866, Class Loss=0.3024898171424866, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/24, Loss=5.453338220715523
Loss made of: CE 0.30011722445487976, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.140984535217285 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.8292192935943605
Loss made of: CE 0.3055031895637512, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.149821758270264 EntMin 0.0
Epoch 3, Class Loss=0.34331053495407104, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.34331053495407104, Class Loss=0.34331053495407104, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/24, Loss=5.279208412766456
Loss made of: CE 0.3906165361404419, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.822033882141113 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.640694975852966
Loss made of: CE 0.4308207929134369, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.265133857727051 EntMin 0.0
Epoch 4, Class Loss=0.38835686445236206, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.38835686445236206, Class Loss=0.38835686445236206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/24, Loss=5.644816452264786
Loss made of: CE 0.4315175414085388, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.657157897949219 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.217828840017319
Loss made of: CE 0.37704938650131226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.682827472686768 EntMin 0.0
Epoch 5, Class Loss=0.39790886640548706, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.39790886640548706, Class Loss=0.39790886640548706, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/24, Loss=5.294907781481743
Loss made of: CE 0.4612640142440796, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6759443283081055 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.36016483604908
Loss made of: CE 0.33831679821014404, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.95281982421875 EntMin 0.0
Epoch 6, Class Loss=0.4150654971599579, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.4150654971599579, Class Loss=0.4150654971599579, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=6.453603607416153
Loss made of: CE 0.6710068583488464, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.200740814208984 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.661881840229034
Loss made of: CE 0.5210081934928894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.424395561218262 EntMin 0.0
Epoch 1, Class Loss=0.7097124457359314, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.7097124457359314, Class Loss=0.7097124457359314, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/24, Loss=5.842167270183563
Loss made of: CE 0.47768956422805786, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.653046131134033 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.55674849152565
Loss made of: CE 0.5000471472740173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.987473487854004 EntMin 0.0
Epoch 2, Class Loss=0.5879587531089783, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.5879587531089783, Class Loss=0.5879587531089783, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/24, Loss=5.548733416199684
Loss made of: CE 0.5242877006530762, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5128278732299805 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.555155780911446
Loss made of: CE 0.5643267035484314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.822272300720215 EntMin 0.0
Epoch 3, Class Loss=0.5213993191719055, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5213993191719055, Class Loss=0.5213993191719055, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/24, Loss=5.389309391379356
Loss made of: CE 0.5407993793487549, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.067918300628662 EntMin 0.0
Epoch 4, Batch 20/24, Loss=5.388212868571282
Loss made of: CE 0.42295679450035095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.482517242431641 EntMin 0.0
Epoch 4, Class Loss=0.4834701716899872, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.4834701716899872, Class Loss=0.4834701716899872, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/24, Loss=5.554718065261841
Loss made of: CE 0.557026743888855, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.859272003173828 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.092702281475067
Loss made of: CE 0.44205614924430847, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.449039459228516 EntMin 0.0
Epoch 5, Class Loss=0.4540219306945801, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.4540219306945801, Class Loss=0.4540219306945801, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/24, Loss=5.273067471385002
Loss made of: CE 0.4129490852355957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.529921531677246 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.050073346495628
Loss made of: CE 0.4103490114212036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478436470031738 EntMin 0.0
Epoch 6, Class Loss=0.4333207309246063, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.4333207309246063, Class Loss=0.4333207309246063, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=7.97678425014019
Loss made of: CE 0.3760976195335388, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.47012996673584 EntMin 0.0
Epoch 1, Class Loss=0.40800240635871887, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.40800240635871887, Class Loss=0.40800240635871887, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/19, Loss=6.629640144109726
Loss made of: CE 0.39713162183761597, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.161391735076904 EntMin 0.0
Epoch 2, Class Loss=0.41847434639930725, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.41847434639930725, Class Loss=0.41847434639930725, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/19, Loss=6.030005943775177
Loss made of: CE 0.5073816180229187, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.420924663543701 EntMin 0.0
Epoch 3, Class Loss=0.39221107959747314, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.39221107959747314, Class Loss=0.39221107959747314, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/19, Loss=5.83815324306488
Loss made of: CE 0.49407047033309937, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.646869659423828 EntMin 0.0
Epoch 4, Class Loss=0.3676421344280243, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.3676421344280243, Class Loss=0.3676421344280243, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/19, Loss=5.6713683813810345
Loss made of: CE 0.2722364068031311, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.072734355926514 EntMin 0.0
Epoch 5, Class Loss=0.3411429822444916, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3411429822444916, Class Loss=0.3411429822444916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/19, Loss=5.375571459531784
Loss made of: CE 0.2689530849456787, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.438910484313965 EntMin 0.0
Epoch 6, Class Loss=0.31913357973098755, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.31913357973098755, Class Loss=0.31913357973098755, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=6.227318435907364
Loss made of: CE 0.6402756571769714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7450385093688965 EntMin 0.0
Epoch 1, Batch 20/26, Loss=5.403307566046715
Loss made of: CE 0.5110114812850952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.144160747528076 EntMin 0.0
Epoch 1, Class Loss=0.5857623815536499, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.5857623815536499, Class Loss=0.5857623815536499, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/26, Loss=5.676466715335846
Loss made of: CE 0.5279665589332581, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.393974304199219 EntMin 0.0
Epoch 2, Batch 20/26, Loss=5.533504366874695
Loss made of: CE 0.4644708037376404, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.658196926116943 EntMin 0.0
Epoch 2, Class Loss=0.4786079525947571, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.4786079525947571, Class Loss=0.4786079525947571, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/26, Loss=5.288690757751465
Loss made of: CE 0.5335714817047119, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.604656219482422 EntMin 0.0
Epoch 3, Batch 20/26, Loss=5.297307449579239
Loss made of: CE 0.4157130718231201, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.395514488220215 EntMin 0.0
Epoch 3, Class Loss=0.4365730583667755, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.4365730583667755, Class Loss=0.4365730583667755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/26, Loss=5.108210134506225
Loss made of: CE 0.3824378252029419, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.000425338745117 EntMin 0.0
Epoch 4, Batch 20/26, Loss=4.998532545566559
Loss made of: CE 0.45271140336990356, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.162172317504883 EntMin 0.0
Epoch 4, Class Loss=0.4041655361652374, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.4041655361652374, Class Loss=0.4041655361652374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/26, Loss=4.952524861693382
Loss made of: CE 0.5412606000900269, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.32276725769043 EntMin 0.0
Epoch 5, Batch 20/26, Loss=4.962099379301071
Loss made of: CE 0.41560086607933044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.950420379638672 EntMin 0.0
Epoch 5, Class Loss=0.38484761118888855, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.38484761118888855, Class Loss=0.38484761118888855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/26, Loss=4.884961631894112
Loss made of: CE 0.35553890466690063, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.386363506317139 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.888682782649994
Loss made of: CE 0.32496941089630127, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.138617038726807 EntMin 0.0
Epoch 6, Class Loss=0.37113720178604126, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.37113720178604126, Class Loss=0.37113720178604126, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5668966174125671, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.846878
Mean Acc: 0.484036
FreqW Acc: 0.739924
Mean IoU: 0.384703
Class IoU:
	class 0: 0.85748506
	class 1: 0.21064284
	class 2: 0.14278266
	class 3: 0.43202728
	class 4: 0.465955
	class 5: 0.04961856
	class 6: 0.7367302
	class 7: 0.7008927
	class 8: 0.70323396
	class 9: 0.0
	class 10: 0.3500748
	class 11: 0.2715124
	class 12: 0.5401085
	class 13: 0.3706412
	class 14: 0.66451395
	class 15: 0.7303902
	class 16: 0.0
	class 17: 0.0
	class 18: 0.050366133
	class 19: 0.4645784
	class 20: 0.3372037
Class Acc:
	class 0: 0.96115047
	class 1: 0.21095727
	class 2: 0.18057962
	class 3: 0.45999476
	class 4: 0.864974
	class 5: 0.049623266
	class 6: 0.76823485
	class 7: 0.8667905
	class 8: 0.7976634
	class 9: 0.0
	class 10: 0.43177465
	class 11: 0.5337402
	class 12: 0.82536304
	class 13: 0.48282757
	class 14: 0.83567464
	class 15: 0.870828
	class 16: 0.0
	class 17: 0.0
	class 18: 0.05170354
	class 19: 0.5638572
	class 20: 0.40901387

federated global round: 23, step: 4
select part of clients to conduct local training
[12, 10, 9, 6]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=5.166139383614063
Loss made of: CE 0.21112188696861267, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.124984264373779 EntMin 0.0
Epoch 1, Batch 20/26, Loss=4.883834362030029
Loss made of: CE 0.16556717455387115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.054902076721191 EntMin 0.0
Epoch 1, Class Loss=0.23235298693180084, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.23235298693180084, Class Loss=0.23235298693180084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/26, Loss=4.831689602136612
Loss made of: CE 0.2796558141708374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9532713890075684 EntMin 0.0
Epoch 2, Batch 20/26, Loss=4.9102320045232775
Loss made of: CE 0.19525109231472015, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.60506534576416 EntMin 0.0
Epoch 2, Class Loss=0.23849959671497345, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.23849959671497345, Class Loss=0.23849959671497345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/26, Loss=5.030547842383385
Loss made of: CE 0.263277143239975, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.851009368896484 EntMin 0.0
Epoch 3, Batch 20/26, Loss=4.884095606207848
Loss made of: CE 0.25363993644714355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.133023262023926 EntMin 0.0
Epoch 3, Class Loss=0.2704315483570099, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.2704315483570099, Class Loss=0.2704315483570099, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/26, Loss=4.721101441979409
Loss made of: CE 0.2955215573310852, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.729266166687012 EntMin 0.0
Epoch 4, Batch 20/26, Loss=4.69284350425005
Loss made of: CE 0.2650057077407837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.031389236450195 EntMin 0.0
Epoch 4, Class Loss=0.2873290181159973, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.2873290181159973, Class Loss=0.2873290181159973, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/26, Loss=4.623491716384888
Loss made of: CE 0.3219159245491028, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2225799560546875 EntMin 0.0
Epoch 5, Batch 20/26, Loss=4.894493743777275
Loss made of: CE 0.3757370114326477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.868738174438477 EntMin 0.0
Epoch 5, Class Loss=0.30991801619529724, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.30991801619529724, Class Loss=0.30991801619529724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/26, Loss=4.5018733739852905
Loss made of: CE 0.3132603168487549, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.649663925170898 EntMin 0.0
Epoch 6, Batch 20/26, Loss=4.377706667780876
Loss made of: CE 0.2949292063713074, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.914198875427246 EntMin 0.0
Epoch 6, Class Loss=0.31851258873939514, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.31851258873939514, Class Loss=0.31851258873939514, Reg Loss=0.0
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=6.580259969830513
Loss made of: CE 0.2925034463405609, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.116680145263672 EntMin 0.0
Epoch 1, Class Loss=0.3374399244785309, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.3374399244785309, Class Loss=0.3374399244785309, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=5.826442843675613
Loss made of: CE 0.2750065326690674, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5246901512146 EntMin 0.0
Epoch 2, Class Loss=0.3881688416004181, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.3881688416004181, Class Loss=0.3881688416004181, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=5.400711968541145
Loss made of: CE 0.4465104937553406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.697978973388672 EntMin 0.0
Epoch 3, Class Loss=0.4500598907470703, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.4500598907470703, Class Loss=0.4500598907470703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=5.501032903790474
Loss made of: CE 0.4380152225494385, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.237955093383789 EntMin 0.0
Epoch 4, Class Loss=0.42781227827072144, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.42781227827072144, Class Loss=0.42781227827072144, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=5.381503784656525
Loss made of: CE 0.45852184295654297, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.646628379821777 EntMin 0.0
Epoch 5, Class Loss=0.43657025694847107, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.43657025694847107, Class Loss=0.43657025694847107, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=5.042058125138283
Loss made of: CE 0.46626925468444824, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.137199401855469 EntMin 0.0
Epoch 6, Class Loss=0.4374427795410156, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.4374427795410156, Class Loss=0.4374427795410156, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=5.434353141486644
Loss made of: CE 0.20042769610881805, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.156224727630615 EntMin 0.0
Epoch 1, Batch 20/21, Loss=4.862804433703422
Loss made of: CE 0.17668911814689636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.551224708557129 EntMin 0.0
Epoch 1, Class Loss=0.2022283673286438, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.2022283673286438, Class Loss=0.2022283673286438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/21, Loss=4.7768491983413695
Loss made of: CE 0.23467448353767395, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6217122077941895 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.076192866265774
Loss made of: CE 0.20290327072143555, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.168128490447998 EntMin 0.0
Epoch 2, Class Loss=0.27896440029144287, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.27896440029144287, Class Loss=0.27896440029144287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/21, Loss=5.181035746634007
Loss made of: CE 0.33173057436943054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143345355987549 EntMin 0.0
Epoch 3, Batch 20/21, Loss=5.0069985628128055
Loss made of: CE 0.31551533937454224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.757108211517334 EntMin 0.0
Epoch 3, Class Loss=0.3097148537635803, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.3097148537635803, Class Loss=0.3097148537635803, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/21, Loss=4.672936117649078
Loss made of: CE 0.3605208396911621, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128340721130371 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.8698404908180235
Loss made of: CE 0.27803534269332886, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8352198600769043 EntMin 0.0
Epoch 4, Class Loss=0.31427696347236633, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.31427696347236633, Class Loss=0.31427696347236633, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=4.632062023878097
Loss made of: CE 0.27750805020332336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726941108703613 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.71669810116291
Loss made of: CE 0.29818475246429443, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6960458755493164 EntMin 0.0
Epoch 5, Class Loss=0.33220040798187256, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.33220040798187256, Class Loss=0.33220040798187256, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/21, Loss=5.026847809553146
Loss made of: CE 0.33281210064888, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.389345169067383 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.855426648259163
Loss made of: CE 0.35416850447654724, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.712991714477539 EntMin 0.0
Epoch 6, Class Loss=0.3508940041065216, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3508940041065216, Class Loss=0.3508940041065216, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=5.632991902530193
Loss made of: CE 0.21796098351478577, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.255923748016357 EntMin 0.0
Epoch 1, Batch 20/24, Loss=5.516863851249218
Loss made of: CE 0.21395134925842285, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746280670166016 EntMin 0.0
Epoch 1, Class Loss=0.21667703986167908, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.21667703986167908, Class Loss=0.21667703986167908, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/24, Loss=5.132755383849144
Loss made of: CE 0.290924608707428, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.551700592041016 EntMin 0.0
Epoch 2, Batch 20/24, Loss=5.347744899988174
Loss made of: CE 0.2545076608657837, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.513201713562012 EntMin 0.0
Epoch 2, Class Loss=0.2421366572380066, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.2421366572380066, Class Loss=0.2421366572380066, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/24, Loss=5.2455048829317095
Loss made of: CE 0.3031378984451294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.629850387573242 EntMin 0.0
Epoch 3, Batch 20/24, Loss=5.051207894086838
Loss made of: CE 0.2559938430786133, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.62447452545166 EntMin 0.0
Epoch 3, Class Loss=0.3024720847606659, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.3024720847606659, Class Loss=0.3024720847606659, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/24, Loss=5.114624273777008
Loss made of: CE 0.29178887605667114, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.702571868896484 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.958884692192077
Loss made of: CE 0.40330490469932556, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.48012638092041 EntMin 0.0
Epoch 4, Class Loss=0.3332136869430542, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3332136869430542, Class Loss=0.3332136869430542, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/24, Loss=5.043447801470757
Loss made of: CE 0.5288054943084717, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.469119548797607 EntMin 0.0
Epoch 5, Batch 20/24, Loss=5.416163840889931
Loss made of: CE 0.3080255687236786, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202569961547852 EntMin 0.0
Epoch 5, Class Loss=0.3538460433483124, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.3538460433483124, Class Loss=0.3538460433483124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/24, Loss=5.188708385825157
Loss made of: CE 0.4339127242565155, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.496267318725586 EntMin 0.0
Epoch 6, Batch 20/24, Loss=5.027352824807167
Loss made of: CE 0.4202701449394226, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.679402828216553 EntMin 0.0
Epoch 6, Class Loss=0.38068243861198425, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.38068243861198425, Class Loss=0.38068243861198425, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5684613585472107, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.848827
Mean Acc: 0.527531
FreqW Acc: 0.750348
Mean IoU: 0.405923
Class IoU:
	class 0: 0.86484635
	class 1: 0.22448903
	class 2: 0.14107509
	class 3: 0.4709303
	class 4: 0.41014728
	class 5: 0.061698295
	class 6: 0.7405983
	class 7: 0.6762014
	class 8: 0.71804476
	class 9: 0.0
	class 10: 0.34584355
	class 11: 0.267183
	class 12: 0.54343086
	class 13: 0.38797185
	class 14: 0.64742833
	class 15: 0.73030216
	class 16: 0.0
	class 17: 0.1646769
	class 18: 0.26213127
	class 19: 0.44890797
	class 20: 0.41846952
Class Acc:
	class 0: 0.9480452
	class 1: 0.22489206
	class 2: 0.17881718
	class 3: 0.5078437
	class 4: 0.8987773
	class 5: 0.06174152
	class 6: 0.77424264
	class 7: 0.8851516
	class 8: 0.7979461
	class 9: 0.0
	class 10: 0.41094056
	class 11: 0.57793695
	class 12: 0.86318797
	class 13: 0.52497065
	class 14: 0.8455711
	class 15: 0.8782353
	class 16: 0.0
	class 17: 0.16953593
	class 18: 0.35337308
	class 19: 0.5142372
	class 20: 0.66270274

federated global round: 24, step: 4
select part of clients to conduct local training
[18, 24, 25, 10]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=6.12607873827219
Loss made of: CE 0.2974759340286255, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.125968933105469 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.2494913935661316, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.2494913935661316, Class Loss=0.2494913935661316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/19, Loss=5.554308207333088
Loss made of: CE 0.34541767835617065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9109697341918945 EntMin 0.0
Epoch 2, Class Loss=0.2988687753677368, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.2988687753677368, Class Loss=0.2988687753677368, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/19, Loss=5.371094751358032
Loss made of: CE 0.3525680899620056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.58223819732666 EntMin 0.0
Epoch 3, Class Loss=0.35089534521102905, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.35089534521102905, Class Loss=0.35089534521102905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/19, Loss=5.2448546350002285
Loss made of: CE 0.2805630564689636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.137087821960449 EntMin 0.0
Epoch 4, Class Loss=0.36596742272377014, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.36596742272377014, Class Loss=0.36596742272377014, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/19, Loss=5.30837522149086
Loss made of: CE 0.3466448187828064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.815850734710693 EntMin 0.0
Epoch 5, Class Loss=0.3837376534938812, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.3837376534938812, Class Loss=0.3837376534938812, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/19, Loss=4.97056878209114
Loss made of: CE 0.41697490215301514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.48961067199707 EntMin 0.0
Epoch 6, Class Loss=0.4025961756706238, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.4025961756706238, Class Loss=0.4025961756706238, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=4.541812413930893
Loss made of: CE 0.14931678771972656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.174246788024902 EntMin 0.0
Epoch 1, Batch 20/21, Loss=4.978925225138664
Loss made of: CE 0.1740119457244873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.855132102966309 EntMin 0.0
Epoch 1, Class Loss=0.17481759190559387, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.17481759190559387, Class Loss=0.17481759190559387, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/21, Loss=4.593596982955932
Loss made of: CE 0.22381484508514404, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.034417629241943 EntMin 0.0
Epoch 2, Batch 20/21, Loss=5.186935450136661
Loss made of: CE 0.32204651832580566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.910529136657715 EntMin 0.0
Epoch 2, Class Loss=0.23355603218078613, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.23355603218078613, Class Loss=0.23355603218078613, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/21, Loss=4.924102182686329
Loss made of: CE 0.3258463740348816, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3758955001831055 EntMin 0.0
Epoch 3, Batch 20/21, Loss=4.655278454720974
Loss made of: CE 0.2808268666267395, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.461579322814941 EntMin 0.0
Epoch 3, Class Loss=0.263944536447525, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.263944536447525, Class Loss=0.263944536447525, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/21, Loss=4.447158893942833
Loss made of: CE 0.30299052596092224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.518072128295898 EntMin 0.0
Epoch 4, Batch 20/21, Loss=4.575169803202153
Loss made of: CE 0.3339824676513672, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.216113090515137 EntMin 0.0
Epoch 4, Class Loss=0.2874071002006531, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.2874071002006531, Class Loss=0.2874071002006531, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/21, Loss=4.495788520574569
Loss made of: CE 0.33596348762512207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.274645805358887 EntMin 0.0
Epoch 5, Batch 20/21, Loss=4.525067587196827
Loss made of: CE 0.3059069514274597, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.044459342956543 EntMin 0.0
Epoch 5, Class Loss=0.3175205886363983, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.3175205886363983, Class Loss=0.3175205886363983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/21, Loss=4.809250357747078
Loss made of: CE 0.4358696937561035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.334795951843262 EntMin 0.0
Epoch 6, Batch 20/21, Loss=4.27054692208767
Loss made of: CE 0.3286646604537964, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.602505683898926 EntMin 0.0
Epoch 6, Class Loss=0.3461569845676422, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.3461569845676422, Class Loss=0.3461569845676422, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=5.05846106633544
Loss made of: CE 0.12508293986320496, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.707395553588867 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/24, Loss=4.95882213562727
Loss made of: CE 0.19246968626976013, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.246675968170166 EntMin 0.0
Epoch 1, Class Loss=0.15549421310424805, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.15549421310424805, Class Loss=0.15549421310424805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/24, Loss=5.138104674220085
Loss made of: CE 0.24470369517803192, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.797995090484619 EntMin 0.0
Epoch 2, Batch 20/24, Loss=4.794326868653298
Loss made of: CE 0.193431556224823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.717449188232422 EntMin 0.0
Epoch 2, Class Loss=0.202021524310112, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.202021524310112, Class Loss=0.202021524310112, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/24, Loss=5.128119188547134
Loss made of: CE 0.2472848892211914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.870361804962158 EntMin 0.0
Epoch 3, Batch 20/24, Loss=4.730317436158657
Loss made of: CE 0.19433268904685974, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.201356887817383 EntMin 0.0
Epoch 3, Class Loss=0.25144368410110474, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.25144368410110474, Class Loss=0.25144368410110474, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/24, Loss=5.022796779870987
Loss made of: CE 0.2895646095275879, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.218061447143555 EntMin 0.0
Epoch 4, Batch 20/24, Loss=4.943023489415646
Loss made of: CE 0.29855960607528687, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.628933429718018 EntMin 0.0
Epoch 4, Class Loss=0.30573371052742004, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.30573371052742004, Class Loss=0.30573371052742004, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/24, Loss=4.879790675640106
Loss made of: CE 0.3199191093444824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.139303207397461 EntMin 0.0
Epoch 5, Batch 20/24, Loss=4.870576721429825
Loss made of: CE 0.4363095164299011, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.381080627441406 EntMin 0.0
Epoch 5, Class Loss=0.3482106924057007, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.3482106924057007, Class Loss=0.3482106924057007, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/24, Loss=4.529262238740921
Loss made of: CE 0.4315657317638397, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.286006927490234 EntMin 0.0
Epoch 6, Batch 20/24, Loss=4.774072903394699
Loss made of: CE 0.45761045813560486, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.616400241851807 EntMin 0.0
Epoch 6, Class Loss=0.38607436418533325, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.38607436418533325, Class Loss=0.38607436418533325, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=6.2784633994102474
Loss made of: CE 0.6384748220443726, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.501626014709473 EntMin 0.0
Epoch 1, Class Loss=0.68218594789505, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.68218594789505, Class Loss=0.68218594789505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/19, Loss=5.649733400344848
Loss made of: CE 0.4341472089290619, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.013472080230713 EntMin 0.0
Epoch 2, Class Loss=0.5980855822563171, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.5980855822563171, Class Loss=0.5980855822563171, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/19, Loss=5.315218496322632
Loss made of: CE 0.5129184722900391, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746868133544922 EntMin 0.0
Epoch 3, Class Loss=0.5583947896957397, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5583947896957397, Class Loss=0.5583947896957397, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/19, Loss=5.512882280349731
Loss made of: CE 0.5606478452682495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.159395217895508 EntMin 0.0
Epoch 4, Class Loss=0.49703457951545715, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.49703457951545715, Class Loss=0.49703457951545715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/19, Loss=5.337838846445083
Loss made of: CE 0.48824891448020935, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.455963134765625 EntMin 0.0
Epoch 5, Class Loss=0.48676878213882446, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.48676878213882446, Class Loss=0.48676878213882446, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/19, Loss=4.996992301940918
Loss made of: CE 0.5025475025177002, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.167490482330322 EntMin 0.0
Epoch 6, Class Loss=0.47484222054481506, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.47484222054481506, Class Loss=0.47484222054481506, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.5971603393554688, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.841596
Mean Acc: 0.546519
FreqW Acc: 0.747127
Mean IoU: 0.399557
Class IoU:
	class 0: 0.86318636
	class 1: 0.1998071
	class 2: 0.15244164
	class 3: 0.4695711
	class 4: 0.35296828
	class 5: 0.058083296
	class 6: 0.7444085
	class 7: 0.66669655
	class 8: 0.731434
	class 9: 0.0
	class 10: 0.21950394
	class 11: 0.26426473
	class 12: 0.5748623
	class 13: 0.365771
	class 14: 0.6416852
	class 15: 0.7332264
	class 16: 0.0
	class 17: 0.3583078
	class 18: 0.27228063
	class 19: 0.33717158
	class 20: 0.38503328
Class Acc:
	class 0: 0.93629146
	class 1: 0.20011544
	class 2: 0.1989887
	class 3: 0.5075167
	class 4: 0.91940427
	class 5: 0.05813742
	class 6: 0.78779364
	class 7: 0.8880591
	class 8: 0.799888
	class 9: 0.0
	class 10: 0.22808324
	class 11: 0.60755074
	class 12: 0.84215426
	class 13: 0.45316485
	class 14: 0.83543485
	class 15: 0.8652983
	class 16: 0.0
	class 17: 0.75885195
	class 18: 0.5179123
	class 19: 0.35076016
	class 20: 0.7215024

voc_4-4_OURS-FSC On GPUs 0
Run in 86168s
