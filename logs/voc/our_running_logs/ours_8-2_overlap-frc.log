nohup: ignoring input
35
kvoc_8-2_OURS-FRC On GPUs 1\Writing in results/seed_2023-ov/2023-03-19_voc_8-2_OURS-FRC.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[4, 3, 1, 2]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[0, 4, 7, 2]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  4
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[1, 9, 3, 8]
Current Client Index:  1
Current Client Index:  9
Current Client Index:  3
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[5, 9, 0, 4]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Current Client Index:  0
Current Client Index:  4
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.11884414404630661, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.959771
Mean Acc: 0.898867
FreqW Acc: 0.927997
Mean IoU: 0.803600
Class IoU:
	class 0: 0.9505860659032771
	class 1: 0.9002121543757099
	class 2: 0.3911996045853578
	class 3: 0.7886755539422873
	class 4: 0.7227948650191602
	class 5: 0.777020491034379
	class 6: 0.9452273228283382
	class 7: 0.8680230685934628
	class 8: 0.8886579855388657
Class Acc:
	class 0: 0.9742031520387592
	class 1: 0.9505188705401268
	class 2: 0.8333169681087177
	class 3: 0.7954920085182839
	class 4: 0.8649138231019398
	class 5: 0.839701567241524
	class 6: 0.9766569231687022
	class 7: 0.9440467027150332
	class 8: 0.910953836504121

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[5, 11, 7, 1]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/27, Loss=10.43725282549858
Loss made of: CE 0.7794597148895264, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.771620750427246 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.856961342692376
Loss made of: CE 0.33990031480789185, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.774083137512207 EntMin 0.0
Epoch 1, Class Loss=0.743691623210907, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.743691623210907, Class Loss=0.743691623210907, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.62578010559082
Loss made of: CE 0.45806753635406494, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.223050594329834 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.099026209115982
Loss made of: CE 0.39992523193359375, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.399737358093262 EntMin 0.0
Epoch 2, Class Loss=0.4690706133842468, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.4690706133842468, Class Loss=0.4690706133842468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.85302882194519
Loss made of: CE 0.505770742893219, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.082294464111328 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.493318775296212
Loss made of: CE 0.3860585689544678, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.980884075164795 EntMin 0.0
Epoch 3, Class Loss=0.4521431028842926, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.4521431028842926, Class Loss=0.4521431028842926, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.477943921089173
Loss made of: CE 0.393272340297699, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.179311752319336 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.12833642065525
Loss made of: CE 0.44651898741722107, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.592666149139404 EntMin 0.0
Epoch 4, Class Loss=0.45222800970077515, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.45222800970077515, Class Loss=0.45222800970077515, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.162235829234123
Loss made of: CE 0.3722953200340271, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.854577541351318 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.0232608139514925
Loss made of: CE 0.43748897314071655, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.415851593017578 EntMin 0.0
Epoch 5, Class Loss=0.428384393453598, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.428384393453598, Class Loss=0.428384393453598, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=6.186092150211334
Loss made of: CE 0.4560823440551758, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.558449745178223 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.9171482682228085
Loss made of: CE 0.41732078790664673, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3140549659729 EntMin 0.0
Epoch 6, Class Loss=0.4158608913421631, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.4158608913421631, Class Loss=0.4158608913421631, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2891910076141357, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=1.2891910076141357, Class Loss=1.2891910076141357, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.9365488290786743, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.9365488290786743, Class Loss=0.9365488290786743, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.7495167255401611, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.7495167255401611, Class Loss=0.7495167255401611, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.6608229279518127, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.6608229279518127, Class Loss=0.6608229279518127, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5980861783027649, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5980861783027649, Class Loss=0.5980861783027649, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.5422910451889038, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5422910451889038, Class Loss=0.5422910451889038, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.465528309345245
Loss made of: CE 0.7882525324821472, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.152322769165039 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.677881157398224
Loss made of: CE 0.5601667761802673, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.810811519622803 EntMin 0.0
Epoch 1, Class Loss=0.7524752020835876, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.7524752020835876, Class Loss=0.7524752020835876, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.69479187130928
Loss made of: CE 0.5855259299278259, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.358947277069092 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.212544953823089
Loss made of: CE 0.5018662214279175, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.062972545623779 EntMin 0.0
Epoch 2, Class Loss=0.4670409560203552, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.4670409560203552, Class Loss=0.4670409560203552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.69574111700058
Loss made of: CE 0.49932169914245605, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.206618309020996 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.55423404276371
Loss made of: CE 0.4417271018028259, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.850248336791992 EntMin 0.0
Epoch 3, Class Loss=0.46353140473365784, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.46353140473365784, Class Loss=0.46353140473365784, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.2787459075450895
Loss made of: CE 0.446015328168869, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.955333232879639 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.346840709447861
Loss made of: CE 0.5743945837020874, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.766234397888184 EntMin 0.0
Epoch 4, Class Loss=0.4567432701587677, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4567432701587677, Class Loss=0.4567432701587677, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.320830717682838
Loss made of: CE 0.46245265007019043, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.359847545623779 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.868915775418282
Loss made of: CE 0.33021432161331177, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.385026931762695 EntMin 0.0
Epoch 5, Class Loss=0.4392060339450836, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4392060339450836, Class Loss=0.4392060339450836, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=5.994041538238525
Loss made of: CE 0.4625438451766968, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.495511531829834 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.156771099567413
Loss made of: CE 0.3776220679283142, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.732901573181152 EntMin 0.0
Epoch 6, Class Loss=0.43596798181533813, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.43596798181533813, Class Loss=0.43596798181533813, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.259319466352462
Loss made of: CE 0.7194597125053406, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.282621383666992 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.85691824555397
Loss made of: CE 0.489175409078598, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.218649864196777 EntMin 0.0
Epoch 1, Class Loss=0.7338369488716125, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.7338369488716125, Class Loss=0.7338369488716125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.4627550303936
Loss made of: CE 0.3937813639640808, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.742034435272217 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.284935164451599
Loss made of: CE 0.49741673469543457, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.813274383544922 EntMin 0.0
Epoch 2, Class Loss=0.46030378341674805, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.46030378341674805, Class Loss=0.46030378341674805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.566164848208428
Loss made of: CE 0.4470553994178772, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.397231101989746 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.487878566980362
Loss made of: CE 0.49174952507019043, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8852858543396 EntMin 0.0
Epoch 3, Class Loss=0.4592354893684387, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.4592354893684387, Class Loss=0.4592354893684387, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.2126545161008835
Loss made of: CE 0.4663044810295105, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.795784950256348 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.314549988508224
Loss made of: CE 0.42031043767929077, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.84647274017334 EntMin 0.0
Epoch 4, Class Loss=0.4449489116668701, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.4449489116668701, Class Loss=0.4449489116668701, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=5.960614186525345
Loss made of: CE 0.44976431131362915, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.501396179199219 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.071403801441193
Loss made of: CE 0.35025906562805176, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.743537425994873 EntMin 0.0
Epoch 5, Class Loss=0.4368162751197815, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4368162751197815, Class Loss=0.4368162751197815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=5.838619157671928
Loss made of: CE 0.39672982692718506, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2688679695129395 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.877268660068512
Loss made of: CE 0.4705459475517273, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.319545269012451 EntMin 0.0
Epoch 6, Class Loss=0.43365737795829773, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.43365737795829773, Class Loss=0.43365737795829773, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.32225367426872253, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.924523
Mean Acc: 0.733202
FreqW Acc: 0.862023
Mean IoU: 0.640490
Class IoU:
	class 0: 0.9114557
	class 1: 0.8701182
	class 2: 0.35516074
	class 3: 0.7826765
	class 4: 0.70776355
	class 5: 0.7805052
	class 6: 0.92668235
	class 7: 0.83915377
	class 8: 0.8718785
	class 9: 0.0
	class 10: 0.0
Class Acc:
	class 0: 0.9753978
	class 1: 0.95838684
	class 2: 0.8804624
	class 3: 0.79363024
	class 4: 0.84337586
	class 5: 0.8556532
	class 6: 0.9594338
	class 7: 0.87213486
	class 8: 0.92674726
	class 9: 0.0
	class 10: 0.0

federated global round: 6, step: 1
select part of clients to conduct local training
[1, 6, 7, 3]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=6.12145721912384
Loss made of: CE 0.38308411836624146, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.550986289978027 EntMin 0.0
Epoch 1, Batch 20/27, Loss=6.011000984907151
Loss made of: CE 0.4713922142982483, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.563169479370117 EntMin 0.0
Epoch 1, Class Loss=0.4500197470188141, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.4500197470188141, Class Loss=0.4500197470188141, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=5.715445509552955
Loss made of: CE 0.41899800300598145, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.473202705383301 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.962827056646347
Loss made of: CE 0.43501919507980347, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5313825607299805 EntMin 0.0
Epoch 2, Class Loss=0.43766072392463684, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.43766072392463684, Class Loss=0.43766072392463684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=5.6214633077383045
Loss made of: CE 0.4094270169734955, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.389123916625977 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.683465513586998
Loss made of: CE 0.4344507157802582, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.151244640350342 EntMin 0.0
Epoch 3, Class Loss=0.4209257662296295, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.4209257662296295, Class Loss=0.4209257662296295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=5.605019792914391
Loss made of: CE 0.4248867928981781, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.10496187210083 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.7263808190822605
Loss made of: CE 0.39009684324264526, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.425724029541016 EntMin 0.0
Epoch 4, Class Loss=0.40546002984046936, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.40546002984046936, Class Loss=0.40546002984046936, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.438195165991783
Loss made of: CE 0.39926981925964355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9129557609558105 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.5994645178318025
Loss made of: CE 0.34107348322868347, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.347955226898193 EntMin 0.0
Epoch 5, Class Loss=0.3941100239753723, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.3941100239753723, Class Loss=0.3941100239753723, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=5.406213489174843
Loss made of: CE 0.3877002000808716, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.972898483276367 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.403960189223289
Loss made of: CE 0.41327735781669617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.981422424316406 EntMin 0.0
Epoch 6, Class Loss=0.39037683606147766, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.39037683606147766, Class Loss=0.39037683606147766, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.084265077114106
Loss made of: CE 0.3743845820426941, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.363523006439209 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.805501464009285
Loss made of: CE 0.31374359130859375, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5605082511901855 EntMin 0.0
Epoch 1, Class Loss=0.4038951098918915, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.4038951098918915, Class Loss=0.4038951098918915, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/27, Loss=5.782312539219856
Loss made of: CE 0.38509154319763184, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.594002723693848 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.884583652019501
Loss made of: CE 0.4198329448699951, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.567389965057373 EntMin 0.0
Epoch 2, Class Loss=0.4127216041088104, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.4127216041088104, Class Loss=0.4127216041088104, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/27, Loss=5.655883491039276
Loss made of: CE 0.4457375407218933, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.246707916259766 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.678504359722138
Loss made of: CE 0.4344649314880371, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.029508113861084 EntMin 0.0
Epoch 3, Class Loss=0.3997218608856201, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.3997218608856201, Class Loss=0.3997218608856201, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/27, Loss=5.547041350603104
Loss made of: CE 0.4110225439071655, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.034554481506348 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.582593226432801
Loss made of: CE 0.33354294300079346, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.097509384155273 EntMin 0.0
Epoch 4, Class Loss=0.3931710422039032, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3931710422039032, Class Loss=0.3931710422039032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/27, Loss=5.611467921733857
Loss made of: CE 0.38637280464172363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807565689086914 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.495142522454262
Loss made of: CE 0.27693527936935425, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.092214584350586 EntMin 0.0
Epoch 5, Class Loss=0.38776466250419617, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.38776466250419617, Class Loss=0.38776466250419617, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/27, Loss=5.499929112195969
Loss made of: CE 0.3516899049282074, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.111026763916016 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.363865140080452
Loss made of: CE 0.3204774558544159, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.539027690887451 EntMin 0.0
Epoch 6, Class Loss=0.38205641508102417, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.38205641508102417, Class Loss=0.38205641508102417, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=6.22877913415432
Loss made of: CE 0.5008254051208496, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.890033721923828 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.974518606066704
Loss made of: CE 0.5083779692649841, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.449118614196777 EntMin 0.0
Epoch 1, Class Loss=0.4577943980693817, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.4577943980693817, Class Loss=0.4577943980693817, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=6.045827689766884
Loss made of: CE 0.6474279761314392, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.954689025878906 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.864819473028183
Loss made of: CE 0.4666481018066406, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.01011848449707 EntMin 0.0
Epoch 2, Class Loss=0.44620439410209656, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.44620439410209656, Class Loss=0.44620439410209656, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=5.73460813164711
Loss made of: CE 0.47339266538619995, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.394650936126709 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.765898796916008
Loss made of: CE 0.4351334571838379, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.160092353820801 EntMin 0.0
Epoch 3, Class Loss=0.4322846829891205, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.4322846829891205, Class Loss=0.4322846829891205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=5.58876151740551
Loss made of: CE 0.3783084750175476, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.145542621612549 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.701045548915863
Loss made of: CE 0.535507321357727, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.216727256774902 EntMin 0.0
Epoch 4, Class Loss=0.41241055727005005, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.41241055727005005, Class Loss=0.41241055727005005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.779791966080666
Loss made of: CE 0.45104217529296875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.901365280151367 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.331512534618378
Loss made of: CE 0.3124232888221741, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.057518005371094 EntMin 0.0
Epoch 5, Class Loss=0.4078233540058136, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4078233540058136, Class Loss=0.4078233540058136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=5.561733248829841
Loss made of: CE 0.44708725810050964, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.12053918838501 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.7917646884918215
Loss made of: CE 0.31517964601516724, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.444417476654053 EntMin 0.0
Epoch 6, Class Loss=0.3941388428211212, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.3941388428211212, Class Loss=0.3941388428211212, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.138915777206421, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=1.138915777206421, Class Loss=1.138915777206421, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=1.0571377277374268, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=1.0571377277374268, Class Loss=1.0571377277374268, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.8816016912460327, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.8816016912460327, Class Loss=0.8816016912460327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.7741577625274658, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.7741577625274658, Class Loss=0.7741577625274658, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.6934188604354858, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.6934188604354858, Class Loss=0.6934188604354858, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.5790570974349976, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.5790570974349976, Class Loss=0.5790570974349976, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.30624374747276306, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.925941
Mean Acc: 0.738203
FreqW Acc: 0.864322
Mean IoU: 0.645899
Class IoU:
	class 0: 0.9128559
	class 1: 0.863953
	class 2: 0.36336586
	class 3: 0.81402904
	class 4: 0.70909595
	class 5: 0.7866209
	class 6: 0.92822105
	class 7: 0.85107577
	class 8: 0.8755543
	class 9: 0.000119237244
	class 10: 0.0
Class Acc:
	class 0: 0.9755771
	class 1: 0.9609165
	class 2: 0.8780915
	class 3: 0.8279137
	class 4: 0.8357248
	class 5: 0.867788
	class 6: 0.9581898
	class 7: 0.88537323
	class 8: 0.93053746
	class 9: 0.00011925886
	class 10: 0.0

federated global round: 7, step: 1
select part of clients to conduct local training
[13, 8, 0, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1106140613555908, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=1.1106140613555908, Class Loss=1.1106140613555908, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.0442824363708496, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=1.0442824363708496, Class Loss=1.0442824363708496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.874176025390625, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.874176025390625, Class Loss=0.874176025390625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.7542494535446167, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.7542494535446167, Class Loss=0.7542494535446167, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6413700580596924, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.6413700580596924, Class Loss=0.6413700580596924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.5938193202018738, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5938193202018738, Class Loss=0.5938193202018738, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1940813064575195, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.1940813064575195, Class Loss=1.1940813064575195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.0435566902160645, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=1.0435566902160645, Class Loss=1.0435566902160645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.9268977642059326, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.9268977642059326, Class Loss=0.9268977642059326, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.8442128896713257, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.8442128896713257, Class Loss=0.8442128896713257, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6797369718551636, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6797369718551636, Class Loss=0.6797369718551636, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.5816773176193237, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.5816773176193237, Class Loss=0.5816773176193237, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.147260069847107, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=1.147260069847107, Class Loss=1.147260069847107, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.104355812072754, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=1.104355812072754, Class Loss=1.104355812072754, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.9313127398490906, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.9313127398490906, Class Loss=0.9313127398490906, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.7590550780296326, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.7590550780296326, Class Loss=0.7590550780296326, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6245503425598145, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.6245503425598145, Class Loss=0.6245503425598145, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.5595221519470215, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.5595221519470215, Class Loss=0.5595221519470215, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=5.856777611374855
Loss made of: CE 0.36939355731010437, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.487187385559082 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.676943275332451
Loss made of: CE 0.32392317056655884, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.007435321807861 EntMin 0.0
Epoch 1, Class Loss=0.40820714831352234, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.40820714831352234, Class Loss=0.40820714831352234, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/27, Loss=5.492461717128753
Loss made of: CE 0.3769300878047943, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.142323970794678 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.364982998371124
Loss made of: CE 0.3433064818382263, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.909329414367676 EntMin 0.0
Epoch 2, Class Loss=0.38907739520072937, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.38907739520072937, Class Loss=0.38907739520072937, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/27, Loss=5.521131372451782
Loss made of: CE 0.4096483588218689, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1316986083984375 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.3131531238555905
Loss made of: CE 0.32588693499565125, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.094491958618164 EntMin 0.0
Epoch 3, Class Loss=0.37598109245300293, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.37598109245300293, Class Loss=0.37598109245300293, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/27, Loss=5.4925818026065825
Loss made of: CE 0.3251912295818329, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.370769023895264 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.406069663167
Loss made of: CE 0.3760468363761902, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.847560882568359 EntMin 0.0
Epoch 4, Class Loss=0.3746993839740753, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.3746993839740753, Class Loss=0.3746993839740753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/27, Loss=5.4849072933197025
Loss made of: CE 0.35663458704948425, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.201122283935547 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.285636335611343
Loss made of: CE 0.35841190814971924, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.763157367706299 EntMin 0.0
Epoch 5, Class Loss=0.3766334056854248, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.3766334056854248, Class Loss=0.3766334056854248, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/27, Loss=5.410892963409424
Loss made of: CE 0.43136072158813477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.882203578948975 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.297919374704361
Loss made of: CE 0.3453247547149658, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.76414680480957 EntMin 0.0
Epoch 6, Class Loss=0.3653348684310913, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.3653348684310913, Class Loss=0.3653348684310913, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2788119912147522, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923247
Mean Acc: 0.733338
FreqW Acc: 0.860219
Mean IoU: 0.636989
Class IoU:
	class 0: 0.9100716
	class 1: 0.86429846
	class 2: 0.3566881
	class 3: 0.81616336
	class 4: 0.67610097
	class 5: 0.75658983
	class 6: 0.9164938
	class 7: 0.84590995
	class 8: 0.86435705
	class 9: 0.00020755181
	class 10: 0.0
Class Acc:
	class 0: 0.9751806
	class 1: 0.9517061
	class 2: 0.9116586
	class 3: 0.83437353
	class 4: 0.82871443
	class 5: 0.8476257
	class 6: 0.9400706
	class 7: 0.8699261
	class 8: 0.9072561
	class 9: 0.0002083012
	class 10: 0.0

federated global round: 8, step: 1
select part of clients to conduct local training
[12, 9, 4, 13]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.6401827991008755
Loss made of: CE 0.5034421682357788, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.512391567230225 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.575934413075447
Loss made of: CE 0.34273016452789307, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.006581783294678 EntMin 0.0
Epoch 1, Class Loss=0.390166699886322, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.390166699886322, Class Loss=0.390166699886322, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.395991468429566
Loss made of: CE 0.4040280282497406, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.092453956604004 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.568278792500496
Loss made of: CE 0.4482399821281433, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3379435539245605 EntMin 0.0
Epoch 2, Class Loss=0.36874598264694214, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.36874598264694214, Class Loss=0.36874598264694214, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=5.4792083382606505
Loss made of: CE 0.33263251185417175, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8364973068237305 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.5022454261779785
Loss made of: CE 0.3192901015281677, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.737634658813477 EntMin 0.0
Epoch 3, Class Loss=0.35444176197052, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.35444176197052, Class Loss=0.35444176197052, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.381788581609726
Loss made of: CE 0.3281368911266327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.975019454956055 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.162420296669007
Loss made of: CE 0.346597284078598, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.84393835067749 EntMin 0.0
Epoch 4, Class Loss=0.3481241464614868, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.3481241464614868, Class Loss=0.3481241464614868, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.37799713909626
Loss made of: CE 0.33889278769493103, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2302470207214355 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.396815273165703
Loss made of: CE 0.32934725284576416, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.262585639953613 EntMin 0.0
Epoch 5, Class Loss=0.3472265601158142, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.3472265601158142, Class Loss=0.3472265601158142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.144190901517868
Loss made of: CE 0.42048919200897217, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590826988220215 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.132855385541916
Loss made of: CE 0.37581607699394226, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.000509738922119 EntMin 0.0
Epoch 6, Class Loss=0.3543769419193268, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.3543769419193268, Class Loss=0.3543769419193268, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.6337459683418274, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.6337459683418274, Class Loss=0.6337459683418274, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.5841517448425293, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.5841517448425293, Class Loss=0.5841517448425293, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.5035518407821655, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5035518407821655, Class Loss=0.5035518407821655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.4753565490245819, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.4753565490245819, Class Loss=0.4753565490245819, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.45165663957595825, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.45165663957595825, Class Loss=0.45165663957595825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.41827982664108276, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.41827982664108276, Class Loss=0.41827982664108276, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.59657301902771
Loss made of: CE 0.4511311948299408, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6829352378845215 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.747445419430733
Loss made of: CE 0.3839716911315918, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.328011989593506 EntMin 0.0
Epoch 1, Class Loss=0.3994452953338623, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.3994452953338623, Class Loss=0.3994452953338623, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.541439652442932
Loss made of: CE 0.3407633900642395, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.699453353881836 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.593136614561081
Loss made of: CE 0.4140264391899109, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.751900672912598 EntMin 0.0
Epoch 2, Class Loss=0.37788352370262146, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.37788352370262146, Class Loss=0.37788352370262146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=5.241953140497207
Loss made of: CE 0.3927220106124878, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.814044952392578 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.518376016616822
Loss made of: CE 0.39504390954971313, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.797280311584473 EntMin 0.0
Epoch 3, Class Loss=0.3638819754123688, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.3638819754123688, Class Loss=0.3638819754123688, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.344079345464706
Loss made of: CE 0.41016775369644165, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.273650646209717 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.499186787009239
Loss made of: CE 0.3063157796859741, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.081088066101074 EntMin 0.0
Epoch 4, Class Loss=0.35851940512657166, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.35851940512657166, Class Loss=0.35851940512657166, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.336154532432556
Loss made of: CE 0.33451253175735474, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.248315811157227 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.316293966770172
Loss made of: CE 0.34813612699508667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.895824909210205 EntMin 0.0
Epoch 5, Class Loss=0.3605108857154846, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.3605108857154846, Class Loss=0.3605108857154846, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.209297615289688
Loss made of: CE 0.4504629969596863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.918381690979004 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.189813888072967
Loss made of: CE 0.3750375211238861, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464523792266846 EntMin 0.0
Epoch 6, Class Loss=0.3627125918865204, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.3627125918865204, Class Loss=0.3627125918865204, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.6559321880340576, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.6559321880340576, Class Loss=0.6559321880340576, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Epoch 2, Class Loss=0.6159708499908447, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.6159708499908447, Class Loss=0.6159708499908447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.5552903413772583, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.5552903413772583, Class Loss=0.5552903413772583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5338402986526489, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5338402986526489, Class Loss=0.5338402986526489, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.4971838891506195, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.4971838891506195, Class Loss=0.4971838891506195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.49460965394973755, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.49460965394973755, Class Loss=0.49460965394973755, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.24917054176330566, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.925240
Mean Acc: 0.728276
FreqW Acc: 0.862939
Mean IoU: 0.642144
Class IoU:
	class 0: 0.91237736
	class 1: 0.8739858
	class 2: 0.36995426
	class 3: 0.8116897
	class 4: 0.6839389
	class 5: 0.77497715
	class 6: 0.9084443
	class 7: 0.84624964
	class 8: 0.8767423
	class 9: 0.0052231858
	class 10: 0.0
Class Acc:
	class 0: 0.97832906
	class 1: 0.94455034
	class 2: 0.8864191
	class 3: 0.82636124
	class 4: 0.8043792
	class 5: 0.8502544
	class 6: 0.92344135
	class 7: 0.8681514
	class 8: 0.92389625
	class 9: 0.005252533
	class 10: 0.0

federated global round: 9, step: 1
select part of clients to conduct local training
[4, 6, 13, 12]
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=5.8484732747077945
Loss made of: CE 0.4383975863456726, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.110222339630127 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.475466638803482
Loss made of: CE 0.4532357454299927, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.034115791320801 EntMin 0.0
Epoch 1, Class Loss=0.4365224242210388, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.4365224242210388, Class Loss=0.4365224242210388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.264791569113731
Loss made of: CE 0.35870155692100525, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6857686042785645 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.374941891431808
Loss made of: CE 0.42680978775024414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.457656383514404 EntMin 0.0
Epoch 2, Class Loss=0.4137120842933655, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.4137120842933655, Class Loss=0.4137120842933655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=5.007341229915619
Loss made of: CE 0.4336203932762146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.532728672027588 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.3319188237190245
Loss made of: CE 0.45582324266433716, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.653738021850586 EntMin 0.0
Epoch 3, Class Loss=0.39372843503952026, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.39372843503952026, Class Loss=0.39372843503952026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.119091957807541
Loss made of: CE 0.5668534636497498, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.09498405456543 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.242643728852272
Loss made of: CE 0.37390345335006714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.808961868286133 EntMin 0.0
Epoch 4, Class Loss=0.408954918384552, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.408954918384552, Class Loss=0.408954918384552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.109673857688904
Loss made of: CE 0.3625563085079193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7572150230407715 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.143975389003754
Loss made of: CE 0.387470006942749, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.757898807525635 EntMin 0.0
Epoch 5, Class Loss=0.38895681500434875, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.38895681500434875, Class Loss=0.38895681500434875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=5.163429015874863
Loss made of: CE 0.47196531295776367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.784306526184082 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.0490535408258435
Loss made of: CE 0.4241732954978943, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.349945545196533 EntMin 0.0
Epoch 6, Class Loss=0.3945489823818207, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.3945489823818207, Class Loss=0.3945489823818207, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/27, Loss=5.813288643956184
Loss made of: CE 0.42976853251457214, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.84604549407959 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.401692962646484
Loss made of: CE 0.36598658561706543, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.038340091705322 EntMin 0.0
Epoch 1, Class Loss=0.4531402587890625, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.4531402587890625, Class Loss=0.4531402587890625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/27, Loss=5.235778778791428
Loss made of: CE 0.41009876132011414, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.106869697570801 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.35413880944252
Loss made of: CE 0.5121929049491882, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.050698757171631 EntMin 0.0
Epoch 2, Class Loss=0.4188348352909088, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.4188348352909088, Class Loss=0.4188348352909088, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/27, Loss=5.179073783755302
Loss made of: CE 0.453683078289032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.782342910766602 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.2318071752786635
Loss made of: CE 0.46693500876426697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.517523765563965 EntMin 0.0
Epoch 3, Class Loss=0.4106382727622986, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4106382727622986, Class Loss=0.4106382727622986, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/27, Loss=5.041111809015274
Loss made of: CE 0.3903535008430481, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.415471076965332 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.174842223525047
Loss made of: CE 0.3410365581512451, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.691479682922363 EntMin 0.0
Epoch 4, Class Loss=0.3956475555896759, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3956475555896759, Class Loss=0.3956475555896759, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/27, Loss=5.124161097407341
Loss made of: CE 0.3643377721309662, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3148193359375 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.028810286521912
Loss made of: CE 0.29922112822532654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.827394485473633 EntMin 0.0
Epoch 5, Class Loss=0.392313152551651, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.392313152551651, Class Loss=0.392313152551651, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/27, Loss=4.984166851639747
Loss made of: CE 0.3310891389846802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590036869049072 EntMin 0.0
Epoch 6, Batch 20/27, Loss=4.900186619162559
Loss made of: CE 0.34636855125427246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.310925483703613 EntMin 0.0
Epoch 6, Class Loss=0.3877558708190918, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.3877558708190918, Class Loss=0.3877558708190918, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000372
Epoch 1, Class Loss=0.724815309047699, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.724815309047699, Class Loss=0.724815309047699, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.6685180068016052, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.6685180068016052, Class Loss=0.6685180068016052, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000258
Epoch 3, Class Loss=0.6326693892478943, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.6326693892478943, Class Loss=0.6326693892478943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000199
Epoch 4, Class Loss=0.6364369988441467, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.6364369988441467, Class Loss=0.6364369988441467, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000138
Epoch 5, Class Loss=0.6177968978881836, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.6177968978881836, Class Loss=0.6177968978881836, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.6285173892974854, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.6285173892974854, Class Loss=0.6285173892974854, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=5.939086470007896
Loss made of: CE 0.527125895023346, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.904392242431641 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.368638861179352
Loss made of: CE 0.4053085744380951, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807743072509766 EntMin 0.0
Epoch 1, Class Loss=0.4267716407775879, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.4267716407775879, Class Loss=0.4267716407775879, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.232552567124367
Loss made of: CE 0.4286000728607178, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7532148361206055 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.456793466210366
Loss made of: CE 0.5209144353866577, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.167537689208984 EntMin 0.0
Epoch 2, Class Loss=0.39953306317329407, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.39953306317329407, Class Loss=0.39953306317329407, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=5.25405686199665
Loss made of: CE 0.3800703287124634, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.715645790100098 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.242082950472832
Loss made of: CE 0.34849679470062256, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.454996585845947 EntMin 0.0
Epoch 3, Class Loss=0.3914555609226227, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.3914555609226227, Class Loss=0.3914555609226227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.141995441913605
Loss made of: CE 0.3511185348033905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807183742523193 EntMin 0.0
Epoch 4, Batch 20/27, Loss=4.956703713536262
Loss made of: CE 0.3998163640499115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.777590751647949 EntMin 0.0
Epoch 4, Class Loss=0.3834736943244934, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.3834736943244934, Class Loss=0.3834736943244934, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.205075842142105
Loss made of: CE 0.3590089678764343, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.999441146850586 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.162436899542809
Loss made of: CE 0.33628249168395996, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.92780876159668 EntMin 0.0
Epoch 5, Class Loss=0.3764966130256653, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.3764966130256653, Class Loss=0.3764966130256653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=5.063721290230751
Loss made of: CE 0.43890589475631714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.60109806060791 EntMin 0.0
Epoch 6, Batch 20/27, Loss=4.953874772787094
Loss made of: CE 0.4116244316101074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6991448402404785 EntMin 0.0
Epoch 6, Class Loss=0.3809358775615692, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.3809358775615692, Class Loss=0.3809358775615692, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2519187033176422, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.926822
Mean Acc: 0.736107
FreqW Acc: 0.866017
Mean IoU: 0.650694
Class IoU:
	class 0: 0.91385585
	class 1: 0.8784716
	class 2: 0.37056503
	class 3: 0.82287985
	class 4: 0.6939182
	class 5: 0.7827413
	class 6: 0.91384727
	class 7: 0.85488516
	class 8: 0.88661116
	class 9: 0.039856892
	class 10: 0.0
Class Acc:
	class 0: 0.97774607
	class 1: 0.9471713
	class 2: 0.882971
	class 3: 0.83727515
	class 4: 0.8086667
	class 5: 0.85638624
	class 6: 0.92859346
	class 7: 0.87991256
	class 8: 0.93747
	class 9: 0.040989365
	class 10: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[10, 14, 16, 7]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=11.4363014558563
Loss made of: CE 0.03679239749908447, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.33418083190918 EntMin 0.0
Epoch 1, Batch 20/29, Loss=9.833393868333951
Loss made of: CE 0.01385564636439085, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.227416038513184 EntMin 0.0
Epoch 1, Class Loss=0.02925609052181244, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.02925609052181244, Class Loss=0.02925609052181244, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=8.854443905502558
Loss made of: CE 0.05491435155272484, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.97224235534668 EntMin 0.0
Epoch 2, Batch 20/29, Loss=8.463524885475636
Loss made of: CE 0.11435284465551376, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.942809581756592 EntMin 0.0
Epoch 2, Class Loss=0.08093350380659103, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.08093350380659103, Class Loss=0.08093350380659103, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.33683613538742
Loss made of: CE 0.11803869903087616, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.475935935974121 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.699029759317637
Loss made of: CE 0.17733529210090637, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.029417037963867 EntMin 0.0
Epoch 3, Class Loss=0.17964395880699158, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.17964395880699158, Class Loss=0.17964395880699158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=7.930176764726639
Loss made of: CE 0.38226088881492615, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.003984451293945 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.814895035326481
Loss made of: CE 0.3120499849319458, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.431931495666504 EntMin 0.0
Epoch 4, Class Loss=0.289014607667923, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.289014607667923, Class Loss=0.289014607667923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.5919730991125105
Loss made of: CE 0.37153658270835876, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.638915061950684 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.750248843431473
Loss made of: CE 0.39282864332199097, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.203851699829102 EntMin 0.0
Epoch 5, Class Loss=0.3433476686477661, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.3433476686477661, Class Loss=0.3433476686477661, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.324146795272827
Loss made of: CE 0.35134726762771606, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.306333541870117 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.342789676785469
Loss made of: CE 0.2691601514816284, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.478857040405273 EntMin 0.0
Epoch 6, Class Loss=0.3529931306838989, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.3529931306838989, Class Loss=0.3529931306838989, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.7035032285377385
Loss made of: CE 0.026725642383098602, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.266507625579834 EntMin 0.0
Epoch 1, Class Loss=0.03180088847875595, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.03180088847875595, Class Loss=0.03180088847875595, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.146191743761301
Loss made of: CE 0.11073379218578339, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.847682952880859 EntMin 0.0
Epoch 2, Class Loss=0.1608504354953766, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.1608504354953766, Class Loss=0.1608504354953766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=5.866057059168815
Loss made of: CE 0.30696171522140503, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.049053192138672 EntMin 0.0
Epoch 3, Class Loss=0.3430234491825104, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.3430234491825104, Class Loss=0.3430234491825104, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.696933293342591
Loss made of: CE 0.5396785140037537, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.535882472991943 EntMin 0.0
Epoch 4, Class Loss=0.4670861065387726, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.4670861065387726, Class Loss=0.4670861065387726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.639627909660339
Loss made of: CE 0.472691148519516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.804619312286377 EntMin 0.0
Epoch 5, Class Loss=0.5215361714363098, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5215361714363098, Class Loss=0.5215361714363098, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.508312821388245
Loss made of: CE 0.5985558032989502, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.904934883117676 EntMin 0.0
Epoch 6, Class Loss=0.5458388328552246, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5458388328552246, Class Loss=0.5458388328552246, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=11.741257924004458
Loss made of: CE 0.013810343109071255, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.993814468383789 EntMin 0.0
Epoch 1, Batch 20/29, Loss=9.99796023007948
Loss made of: CE 0.03733981028199196, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.824602127075195 EntMin 0.0
Epoch 1, Class Loss=0.022051330655813217, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.022051330655813217, Class Loss=0.022051330655813217, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=8.75869458410889
Loss made of: CE 0.02815769426524639, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.605236053466797 EntMin 0.0
Epoch 2, Batch 20/29, Loss=8.758160916343332
Loss made of: CE 0.1281014084815979, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.720197677612305 EntMin 0.0
Epoch 2, Class Loss=0.0843309536576271, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.0843309536576271, Class Loss=0.0843309536576271, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.467826104164123
Loss made of: CE 0.12997284531593323, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.934112548828125 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.766612508147955
Loss made of: CE 0.30837103724479675, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.227741241455078 EntMin 0.0
Epoch 3, Class Loss=0.1869761347770691, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.1869761347770691, Class Loss=0.1869761347770691, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=7.988392341136932
Loss made of: CE 0.3655621409416199, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.107211112976074 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.974239799380302
Loss made of: CE 0.305529922246933, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.852751731872559 EntMin 0.0
Epoch 4, Class Loss=0.29612231254577637, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.29612231254577637, Class Loss=0.29612231254577637, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.49111755490303
Loss made of: CE 0.293512225151062, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.332814693450928 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.82508011162281
Loss made of: CE 0.4416111409664154, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.134045600891113 EntMin 0.0
Epoch 5, Class Loss=0.33872130513191223, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.33872130513191223, Class Loss=0.33872130513191223, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.464321747422218
Loss made of: CE 0.49029356241226196, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.416818618774414 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.43611478805542
Loss made of: CE 0.3327159881591797, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.646073341369629 EntMin 0.0
Epoch 6, Class Loss=0.3533934950828552, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.3533934950828552, Class Loss=0.3533934950828552, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.5739098476246
Loss made of: CE 0.02856329083442688, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.953944683074951 EntMin 0.0
Epoch 1, Class Loss=0.027360204607248306, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.027360204607248306, Class Loss=0.027360204607248306, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.096743834763766
Loss made of: CE 0.11918896436691284, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.344272613525391 EntMin 0.0
Epoch 2, Class Loss=0.15394112467765808, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.15394112467765808, Class Loss=0.15394112467765808, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=5.770505030453205
Loss made of: CE 0.18165385723114014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.703437328338623 EntMin 0.0
Epoch 3, Class Loss=0.3219898045063019, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3219898045063019, Class Loss=0.3219898045063019, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.823802164196968
Loss made of: CE 0.296169638633728, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.039827823638916 EntMin 0.0
Epoch 4, Class Loss=0.4482713043689728, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4482713043689728, Class Loss=0.4482713043689728, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.7552203178405765
Loss made of: CE 0.6436187624931335, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.740357398986816 EntMin 0.0
Epoch 5, Class Loss=0.5147786140441895, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.5147786140441895, Class Loss=0.5147786140441895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.6722249507904055
Loss made of: CE 0.5816369652748108, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.648187160491943 EntMin 0.0
Epoch 6, Class Loss=0.5436610579490662, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.5436610579490662, Class Loss=0.5436610579490662, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5090165734291077, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.875943
Mean Acc: 0.652200
FreqW Acc: 0.781946
Mean IoU: 0.534594
Class IoU:
	class 0: 0.86566526
	class 1: 0.82293665
	class 2: 0.311413
	class 3: 0.8497443
	class 4: 0.6861365
	class 5: 0.77419764
	class 6: 0.92298377
	class 7: 0.8466703
	class 8: 0.78335875
	class 9: 0.0866124
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.9635598
	class 1: 0.96484977
	class 2: 0.9400187
	class 3: 0.889053
	class 4: 0.86191785
	class 5: 0.89732903
	class 6: 0.9656132
	class 7: 0.90523535
	class 8: 0.9545595
	class 9: 0.1364691
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[10, 13, 6, 1]
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/29, Loss=9.511902463436126
Loss made of: CE 0.7948641777038574, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.826465606689453 EntMin 0.0
Epoch 1, Batch 20/29, Loss=8.469193094968796
Loss made of: CE 0.7253392934799194, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.450325012207031 EntMin 0.0
Epoch 1, Class Loss=0.8328575491905212, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.8328575491905212, Class Loss=0.8328575491905212, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/29, Loss=8.175334149599076
Loss made of: CE 0.5361155271530151, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.602325439453125 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.942770776152611
Loss made of: CE 0.4655545651912689, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.131233215332031 EntMin 0.0
Epoch 2, Class Loss=0.5348271131515503, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.5348271131515503, Class Loss=0.5348271131515503, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/29, Loss=7.734419596195221
Loss made of: CE 0.476083368062973, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.505988121032715 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.302017039060592
Loss made of: CE 0.3799062967300415, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.511116027832031 EntMin 0.0
Epoch 3, Class Loss=0.46198755502700806, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.46198755502700806, Class Loss=0.46198755502700806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/29, Loss=7.4462643474340435
Loss made of: CE 0.48105013370513916, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.2903337478637695 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.39257860481739
Loss made of: CE 0.36809787154197693, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.858232498168945 EntMin 0.0
Epoch 4, Class Loss=0.43311405181884766, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.43311405181884766, Class Loss=0.43311405181884766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=7.392295265197754
Loss made of: CE 0.38496702909469604, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.402890205383301 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.2542842537164685
Loss made of: CE 0.36517199873924255, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.645864486694336 EntMin 0.0
Epoch 5, Class Loss=0.42066457867622375, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.42066457867622375, Class Loss=0.42066457867622375, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/29, Loss=7.053565606474876
Loss made of: CE 0.31848883628845215, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.81175422668457 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.834629935026169
Loss made of: CE 0.37073445320129395, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.183656215667725 EntMin 0.0
Epoch 6, Class Loss=0.38069820404052734, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.38069820404052734, Class Loss=0.38069820404052734, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.852686583646573
Loss made of: CE 0.006035351660102606, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.444643020629883 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.968767296010628
Loss made of: CE 0.0034266686998307705, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.896430015563965 EntMin 0.0
Epoch 1, Class Loss=0.010410749353468418, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.010410749353468418, Class Loss=0.010410749353468418, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/29, Loss=7.8928669424727556
Loss made of: CE 0.01691153459250927, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.755414962768555 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.271774814091623
Loss made of: CE 0.017314471304416656, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.269576549530029 EntMin 0.0
Epoch 2, Class Loss=0.04897525906562805, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.04897525906562805, Class Loss=0.04897525906562805, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/29, Loss=7.193976483494043
Loss made of: CE 0.12041419744491577, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.230263710021973 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.717701242491603
Loss made of: CE 0.09454888105392456, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.086890697479248 EntMin 0.0
Epoch 3, Class Loss=0.11744043231010437, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.11744043231010437, Class Loss=0.11744043231010437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/29, Loss=7.349288183450699
Loss made of: CE 0.1883929967880249, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.26564884185791 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 20/29, Loss=7.389645321667194
Loss made of: CE 0.16262130439281464, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.990099906921387 EntMin 0.0
Epoch 4, Class Loss=0.21088342368602753, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.21088342368602753, Class Loss=0.21088342368602753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/29, Loss=7.0983294799923895
Loss made of: CE 0.1761045902967453, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.087048530578613 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.2405648797750475
Loss made of: CE 0.2922414541244507, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.340085983276367 EntMin 0.0
Epoch 5, Class Loss=0.2551880478858948, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.2551880478858948, Class Loss=0.2551880478858948, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/29, Loss=6.983061543107032
Loss made of: CE 0.2994117736816406, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.7075700759887695 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.173907952010632
Loss made of: CE 0.23014913499355316, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.113297939300537 EntMin 0.0
Epoch 6, Class Loss=0.29364013671875, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.29364013671875, Class Loss=0.29364013671875, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.347254942916334
Loss made of: CE 0.03653263673186302, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.619564056396484 EntMin 0.0
Epoch 1, Class Loss=0.039463575929403305, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.039463575929403305, Class Loss=0.039463575929403305, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=5.683689242601394
Loss made of: CE 0.15778426826000214, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.372612953186035 EntMin 0.0
Epoch 2, Class Loss=0.15089651942253113, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.15089651942253113, Class Loss=0.15089651942253113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=5.4853260725736614
Loss made of: CE 0.21439024806022644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.627242565155029 EntMin 0.0
Epoch 3, Class Loss=0.29020223021507263, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.29020223021507263, Class Loss=0.29020223021507263, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.481058698892594
Loss made of: CE 0.5660938620567322, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.177401065826416 EntMin 0.0
Epoch 4, Class Loss=0.40764060616493225, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.40764060616493225, Class Loss=0.40764060616493225, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.4264908075332645
Loss made of: CE 0.44093751907348633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.689499855041504 EntMin 0.0
Epoch 5, Class Loss=0.47294121980667114, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.47294121980667114, Class Loss=0.47294121980667114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.286559930443763
Loss made of: CE 0.47552117705345154, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.161242485046387 EntMin 0.0
Epoch 6, Class Loss=0.49293240904808044, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.49293240904808044, Class Loss=0.49293240904808044, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.291240565478802
Loss made of: CE 0.046128399670124054, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.562545299530029 EntMin 0.0
Epoch 1, Class Loss=0.03761973977088928, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.03761973977088928, Class Loss=0.03761973977088928, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=5.738585491478443
Loss made of: CE 0.17290961742401123, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2295732498168945 EntMin 0.0
Epoch 2, Class Loss=0.15197277069091797, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.15197277069091797, Class Loss=0.15197277069091797, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/13, Loss=5.3064163878560064
Loss made of: CE 0.27324485778808594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.553760528564453 EntMin 0.0
Epoch 3, Class Loss=0.29518887400627136, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.29518887400627136, Class Loss=0.29518887400627136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.451958122849464
Loss made of: CE 0.3754527270793915, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.747332572937012 EntMin 0.0
Epoch 4, Class Loss=0.4101865589618683, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.4101865589618683, Class Loss=0.4101865589618683, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.372323855757713
Loss made of: CE 0.4259262681007385, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.629712104797363 EntMin 0.0
Epoch 5, Class Loss=0.46602556109428406, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.46602556109428406, Class Loss=0.46602556109428406, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.206641122698784
Loss made of: CE 0.5144796371459961, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.975110054016113 EntMin 0.0
Epoch 6, Class Loss=0.5012143850326538, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.5012143850326538, Class Loss=0.5012143850326538, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4082691967487335, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.888493
Mean Acc: 0.648510
FreqW Acc: 0.798886
Mean IoU: 0.550011
Class IoU:
	class 0: 0.8775207
	class 1: 0.84225273
	class 2: 0.33476153
	class 3: 0.8434303
	class 4: 0.68210673
	class 5: 0.7800555
	class 6: 0.9043208
	class 7: 0.8499007
	class 8: 0.79265827
	class 9: 0.00881031
	class 10: 0.0
	class 11: 0.0
	class 12: 0.23432481
Class Acc:
	class 0: 0.97426736
	class 1: 0.9370594
	class 2: 0.91860723
	class 3: 0.87712896
	class 4: 0.80964
	class 5: 0.8912965
	class 6: 0.92675596
	class 7: 0.89364386
	class 8: 0.9460552
	class 9: 0.009051137
	class 10: 0.0
	class 11: 0.0
	class 12: 0.24712501

federated global round: 12, step: 2
select part of clients to conduct local training
[11, 0, 8, 14]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.271856990270317
Loss made of: CE 0.03806878253817558, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.871734142303467 EntMin 0.0
Epoch 1, Class Loss=0.04076142609119415, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.04076142609119415, Class Loss=0.04076142609119415, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=5.696461533010006
Loss made of: CE 0.2191825956106186, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.924776077270508 EntMin 0.0
Epoch 2, Class Loss=0.16003580391407013, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.16003580391407013, Class Loss=0.16003580391407013, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.315306854248047
Loss made of: CE 0.24098555743694305, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.460838317871094 EntMin 0.0
Epoch 3, Class Loss=0.2841658294200897, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.2841658294200897, Class Loss=0.2841658294200897, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.382962059974671
Loss made of: CE 0.42042064666748047, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.024008750915527 EntMin 0.0
Epoch 4, Class Loss=0.36602747440338135, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.36602747440338135, Class Loss=0.36602747440338135, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.567241385579109
Loss made of: CE 0.46162018179893494, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7617716789245605 EntMin 0.0
Epoch 5, Class Loss=0.43865638971328735, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.43865638971328735, Class Loss=0.43865638971328735, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.18477289378643
Loss made of: CE 0.39724671840667725, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.180257797241211 EntMin 0.0
Epoch 6, Class Loss=0.46745577454566956, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.46745577454566956, Class Loss=0.46745577454566956, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.8941884574036525
Loss made of: CE 0.0048310901038348675, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.501585006713867 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.186327476412407
Loss made of: CE 0.02624627575278282, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.190331935882568 EntMin 0.0
Epoch 1, Class Loss=0.006509364116936922, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.006509364116936922, Class Loss=0.006509364116936922, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=6.951839393097908
Loss made of: CE 0.02208298072218895, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.709277153015137 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.137462900392711
Loss made of: CE 0.030074775218963623, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.133849143981934 EntMin 0.0
Epoch 2, Class Loss=0.031684454530477524, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.031684454530477524, Class Loss=0.031684454530477524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=6.972845446690917
Loss made of: CE 0.11510809510946274, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.294007301330566 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.630115401744843
Loss made of: CE 0.09436994791030884, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.397599220275879 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.0774277001619339, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.0774277001619339, Class Loss=0.0774277001619339, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=6.741279400140047
Loss made of: CE 0.10306336730718613, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.123458385467529 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.685009200125933
Loss made of: CE 0.13642030954360962, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.374707221984863 EntMin 0.0
Epoch 4, Class Loss=0.1361638605594635, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.1361638605594635, Class Loss=0.1361638605594635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=6.573232743144035
Loss made of: CE 0.2300405502319336, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.202374458312988 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.705759163200855
Loss made of: CE 0.21942687034606934, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.00917387008667 EntMin 0.0
Epoch 5, Class Loss=0.1962166577577591, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.1962166577577591, Class Loss=0.1962166577577591, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=6.657938833534717
Loss made of: CE 0.22492170333862305, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.129842281341553 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.678097373247146
Loss made of: CE 0.205400750041008, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4029669761657715 EntMin 0.0
Epoch 6, Class Loss=0.23787671327590942, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.23787671327590942, Class Loss=0.23787671327590942, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.115410366014112
Loss made of: CE 0.00859229639172554, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.081772804260254 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.253579672297929
Loss made of: CE 0.005343225784599781, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.7178874015808105 EntMin 0.0
Epoch 1, Class Loss=0.009623576886951923, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.009623576886951923, Class Loss=0.009623576886951923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=6.7655468689277765
Loss made of: CE 0.022367551922798157, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.338018417358398 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.136918178386987
Loss made of: CE 0.02240649424493313, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.10377311706543 EntMin 0.0
Epoch 2, Class Loss=0.03235851600766182, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.03235851600766182, Class Loss=0.03235851600766182, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=7.133235688507557
Loss made of: CE 0.04476957768201828, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3499956130981445 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.072905308380723
Loss made of: CE 0.09767161309719086, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.434737205505371 EntMin 0.0
Epoch 3, Class Loss=0.07716522365808487, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.07716522365808487, Class Loss=0.07716522365808487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=6.562071880698204
Loss made of: CE 0.11072467267513275, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.460644721984863 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.966445124149322
Loss made of: CE 0.09437059611082077, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.23282527923584 EntMin 0.0
Epoch 4, Class Loss=0.13432034850120544, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.13432034850120544, Class Loss=0.13432034850120544, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=6.788464966416359
Loss made of: CE 0.17359551787376404, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.355914115905762 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.535052144527436
Loss made of: CE 0.16898858547210693, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.780816078186035 EntMin 0.0
Epoch 5, Class Loss=0.19357867538928986, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.19357867538928986, Class Loss=0.19357867538928986, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=6.821676479279995
Loss made of: CE 0.23885878920555115, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.196625709533691 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.776516535878182
Loss made of: CE 0.21139714121818542, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.622992515563965 EntMin 0.0
Epoch 6, Class Loss=0.23896372318267822, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.23896372318267822, Class Loss=0.23896372318267822, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.956124895811081
Loss made of: CE 0.7775027751922607, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4611711502075195 EntMin 0.0
Epoch 1, Class Loss=0.7472694516181946, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.7472694516181946, Class Loss=0.7472694516181946, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/13, Loss=6.154107308387756
Loss made of: CE 0.5825745463371277, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.006791591644287 EntMin 0.0
Epoch 2, Class Loss=0.660614550113678, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.660614550113678, Class Loss=0.660614550113678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/13, Loss=5.768070849776268
Loss made of: CE 0.5448827743530273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8274383544921875 EntMin 0.0
Epoch 3, Class Loss=0.5579331517219543, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.5579331517219543, Class Loss=0.5579331517219543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/13, Loss=5.41576609313488
Loss made of: CE 0.523213803768158, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.327907085418701 EntMin 0.0
Epoch 4, Class Loss=0.4908641576766968, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.4908641576766968, Class Loss=0.4908641576766968, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/13, Loss=5.309267050027847
Loss made of: CE 0.5426195859909058, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.645015239715576 EntMin 0.0
Epoch 5, Class Loss=0.4599025547504425, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.4599025547504425, Class Loss=0.4599025547504425, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/13, Loss=5.191798394918441
Loss made of: CE 0.4609857499599457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.458310127258301 EntMin 0.0
Epoch 6, Class Loss=0.4339546263217926, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.4339546263217926, Class Loss=0.4339546263217926, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.39408978819847107, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.898429
Mean Acc: 0.678385
FreqW Acc: 0.819537
Mean IoU: 0.568755
Class IoU:
	class 0: 0.8942486
	class 1: 0.8535717
	class 2: 0.3275837
	class 3: 0.854729
	class 4: 0.6855954
	class 5: 0.7727131
	class 6: 0.9052761
	class 7: 0.8488699
	class 8: 0.79724044
	class 9: 0.0198942
	class 10: 0.0
	class 11: 0.0
	class 12: 0.43409845
Class Acc:
	class 0: 0.9710049
	class 1: 0.9500165
	class 2: 0.9330715
	class 3: 0.90959775
	class 4: 0.81477654
	class 5: 0.9018984
	class 6: 0.9269169
	class 7: 0.89834446
	class 8: 0.9544172
	class 9: 0.020482307
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5384854

federated global round: 13, step: 2
select part of clients to conduct local training
[13, 5, 15, 7]
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/29, Loss=8.365300661325454
Loss made of: CE 0.5824099779129028, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.7750749588012695 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.655384567379952
Loss made of: CE 0.5093165636062622, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.360982418060303 EntMin 0.0
Epoch 1, Class Loss=0.6254993677139282, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.6254993677139282, Class Loss=0.6254993677139282, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/29, Loss=7.542083066701889
Loss made of: CE 0.5196331739425659, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.748857021331787 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.952802929282188
Loss made of: CE 0.4118580222129822, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.449397087097168 EntMin 0.0
Epoch 2, Class Loss=0.46531856060028076, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.46531856060028076, Class Loss=0.46531856060028076, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/29, Loss=6.862338638305664
Loss made of: CE 0.37831926345825195, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.548982620239258 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.358210855722428
Loss made of: CE 0.40014585852622986, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.21028995513916 EntMin 0.0
Epoch 3, Class Loss=0.424012154340744, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.424012154340744, Class Loss=0.424012154340744, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/29, Loss=6.960100135207176
Loss made of: CE 0.36366236209869385, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.696336269378662 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.064880645275116
Loss made of: CE 0.3828776776790619, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.327122688293457 EntMin 0.0
Epoch 4, Class Loss=0.4102730453014374, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.4102730453014374, Class Loss=0.4102730453014374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/29, Loss=6.801269617676735
Loss made of: CE 0.42500537633895874, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.381546974182129 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.724420920014381
Loss made of: CE 0.36194658279418945, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.708732604980469 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.38650715351104736, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.38650715351104736, Class Loss=0.38650715351104736, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/29, Loss=6.639084449410438
Loss made of: CE 0.340692937374115, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.578803539276123 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.871452760696411
Loss made of: CE 0.39566493034362793, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.389285564422607 EntMin 0.0
Epoch 6, Class Loss=0.383259654045105, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.383259654045105, Class Loss=0.383259654045105, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.073192410281626
Loss made of: CE 0.013369604013860226, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.033051490783691 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.158086695085513
Loss made of: CE 0.005659695714712143, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.074267387390137 EntMin 0.0
Epoch 1, Class Loss=0.006357958540320396, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.006357958540320396, Class Loss=0.006357958540320396, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/29, Loss=7.20435533230193
Loss made of: CE 0.031219055876135826, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.965949058532715 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/29, Loss=6.774175976868719
Loss made of: CE 0.040115565061569214, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.834361553192139 EntMin 0.0
Epoch 2, Class Loss=0.026337364688515663, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.026337364688515663, Class Loss=0.026337364688515663, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/29, Loss=6.8360803060233595
Loss made of: CE 0.042371734976768494, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.793258190155029 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.479625221341848
Loss made of: CE 0.09539401531219482, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.353900909423828 EntMin 0.0
Epoch 3, Class Loss=0.06939013302326202, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.06939013302326202, Class Loss=0.06939013302326202, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/29, Loss=6.499443815648556
Loss made of: CE 0.1399998962879181, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.578154563903809 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.5514030992984775
Loss made of: CE 0.17327919602394104, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.067508220672607 EntMin 0.0
Epoch 4, Class Loss=0.12436125427484512, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.12436125427484512, Class Loss=0.12436125427484512, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=6.406725130975246
Loss made of: CE 0.14437609910964966, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.99481201171875 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.549597012996673
Loss made of: CE 0.150375097990036, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.910933971405029 EntMin 0.0
Epoch 5, Class Loss=0.1789398491382599, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.1789398491382599, Class Loss=0.1789398491382599, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/29, Loss=6.520877715945244
Loss made of: CE 0.21803593635559082, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.839061260223389 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.4846534460783
Loss made of: CE 0.22991272807121277, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.009549617767334 EntMin 0.0
Epoch 6, Class Loss=0.23240460455417633, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.23240460455417633, Class Loss=0.23240460455417633, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.203500709310174
Loss made of: CE 0.04319806396961212, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.818826198577881 EntMin 0.0
Epoch 1, Class Loss=0.03450195491313934, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.03450195491313934, Class Loss=0.03450195491313934, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.607946753501892
Loss made of: CE 0.1448614001274109, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.633423805236816 EntMin 0.0
Epoch 2, Class Loss=0.1211390420794487, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.1211390420794487, Class Loss=0.1211390420794487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.416498965024948
Loss made of: CE 0.1606667935848236, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.492515563964844 EntMin 0.0
Epoch 3, Class Loss=0.24207864701747894, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.24207864701747894, Class Loss=0.24207864701747894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.143196597695351
Loss made of: CE 0.33573150634765625, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.087482452392578 EntMin 0.0
Epoch 4, Class Loss=0.3202248215675354, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.3202248215675354, Class Loss=0.3202248215675354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.2369860917329785
Loss made of: CE 0.35148510336875916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.937168121337891 EntMin 0.0
Epoch 5, Class Loss=0.3800309896469116, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.3800309896469116, Class Loss=0.3800309896469116, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=4.990954428911209
Loss made of: CE 0.4283324182033539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.434370040893555 EntMin 0.0
Epoch 6, Class Loss=0.42204609513282776, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.42204609513282776, Class Loss=0.42204609513282776, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.973851138353348
Loss made of: CE 0.6281054615974426, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3931708335876465 EntMin 0.0
Epoch 1, Class Loss=0.6596621870994568, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.6596621870994568, Class Loss=0.6596621870994568, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/13, Loss=6.052014568448067
Loss made of: CE 0.453322172164917, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.112362384796143 EntMin 0.0
Epoch 2, Class Loss=0.5642610192298889, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.5642610192298889, Class Loss=0.5642610192298889, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.584147164225579
Loss made of: CE 0.40765541791915894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.636877536773682 EntMin 0.0
Epoch 3, Class Loss=0.48860111832618713, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.48860111832618713, Class Loss=0.48860111832618713, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/13, Loss=5.596995458006859
Loss made of: CE 0.4091653525829315, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.845203876495361 EntMin 0.0
Epoch 4, Class Loss=0.44595420360565186, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.44595420360565186, Class Loss=0.44595420360565186, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/13, Loss=5.472557181119919
Loss made of: CE 0.5116249322891235, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.894713401794434 EntMin 0.0
Epoch 5, Class Loss=0.42587634921073914, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.42587634921073914, Class Loss=0.42587634921073914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/13, Loss=5.34413310289383
Loss made of: CE 0.44975852966308594, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.579604625701904 EntMin 0.0
Epoch 6, Class Loss=0.39893510937690735, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.39893510937690735, Class Loss=0.39893510937690735, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.36567139625549316, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.901871
Mean Acc: 0.667370
FreqW Acc: 0.824243
Mean IoU: 0.571548
Class IoU:
	class 0: 0.8981085
	class 1: 0.85861754
	class 2: 0.34798595
	class 3: 0.8415167
	class 4: 0.66419727
	class 5: 0.78320366
	class 6: 0.8749153
	class 7: 0.8504777
	class 8: 0.82035613
	class 9: 0.000607548
	class 10: 0.0
	class 11: 0.0
	class 12: 0.49013767
Class Acc:
	class 0: 0.9764824
	class 1: 0.9176746
	class 2: 0.91122574
	class 3: 0.8766305
	class 4: 0.75295746
	class 5: 0.8936002
	class 6: 0.8879695
	class 7: 0.8891968
	class 8: 0.9397504
	class 9: 0.0006097953
	class 10: 0.0
	class 11: 0.0
	class 12: 0.62971246

federated global round: 14, step: 2
select part of clients to conduct local training
[17, 3, 12, 16]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.18646234550979
Loss made of: CE 0.012365233153104782, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.982443809509277 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.266175890799786
Loss made of: CE 0.005688074976205826, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.9703192710876465 EntMin 0.0
Epoch 1, Class Loss=0.006200938951224089, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.006200938951224089, Class Loss=0.006200938951224089, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=6.794046243932098
Loss made of: CE 0.008150134235620499, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.832821846008301 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.515350277442485
Loss made of: CE 0.006997802294790745, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.479120254516602 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.02359377220273018, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.02359377220273018, Class Loss=0.02359377220273018, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=6.212678374350071
Loss made of: CE 0.0801885724067688, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.704448699951172 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.62022698521614
Loss made of: CE 0.047653358429670334, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.066281318664551 EntMin 0.0
Epoch 3, Class Loss=0.05696869269013405, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.05696869269013405, Class Loss=0.05696869269013405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=6.31907431781292
Loss made of: CE 0.09485402703285217, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.301780700683594 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.387988350540399
Loss made of: CE 0.11533653736114502, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.337055206298828 EntMin 0.0
Epoch 4, Class Loss=0.10782554000616074, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.10782554000616074, Class Loss=0.10782554000616074, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.477154694497585
Loss made of: CE 0.15645073354244232, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.648204803466797 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.271548958867788
Loss made of: CE 0.1719409078359604, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.735887050628662 EntMin 0.0
Epoch 5, Class Loss=0.1844756305217743, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.1844756305217743, Class Loss=0.1844756305217743, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.504808324575424
Loss made of: CE 0.27035588026046753, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.124019622802734 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.296661825478077
Loss made of: CE 0.2421306073665619, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.372105598449707 EntMin 0.0
Epoch 6, Class Loss=0.2599709630012512, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.2599709630012512, Class Loss=0.2599709630012512, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=7.47448012806708
Loss made of: CE 0.004797149449586868, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.5665812492370605 EntMin 0.0
Epoch 1, Batch 20/29, Loss=6.727563317480962
Loss made of: CE 0.0015131705440580845, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.445571422576904 EntMin 0.0
Epoch 1, Class Loss=0.005691022612154484, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.005691022612154484, Class Loss=0.005691022612154484, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=6.577286121249199
Loss made of: CE 0.01537180133163929, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.2748308181762695 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.573409084696323
Loss made of: CE 0.06565484404563904, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.821602821350098 EntMin 0.0
Epoch 2, Class Loss=0.02534572407603264, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.02534572407603264, Class Loss=0.02534572407603264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=6.499208750203252
Loss made of: CE 0.054511092603206635, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.87565803527832 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.62879936248064
Loss made of: CE 0.09801065921783447, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.711846351623535 EntMin 0.0
Epoch 3, Class Loss=0.06070543825626373, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.06070543825626373, Class Loss=0.06070543825626373, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=6.7055616796016695
Loss made of: CE 0.14176441729068756, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.169999122619629 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.259010764211416
Loss made of: CE 0.12809154391288757, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.933745384216309 EntMin 0.0
Epoch 4, Class Loss=0.11610565334558487, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.11610565334558487, Class Loss=0.11610565334558487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.368856710195542
Loss made of: CE 0.1813620924949646, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.410264015197754 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.452358029782772
Loss made of: CE 0.19602489471435547, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.749162673950195 EntMin 0.0
Epoch 5, Class Loss=0.18129655718803406, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.18129655718803406, Class Loss=0.18129655718803406, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.378131790459156
Loss made of: CE 0.24818450212478638, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9021100997924805 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.131766909360886
Loss made of: CE 0.2795008420944214, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.491597652435303 EntMin 0.0
Epoch 6, Class Loss=0.24998906254768372, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.24998906254768372, Class Loss=0.24998906254768372, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.106646195985377
Loss made of: CE 0.01929916813969612, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.150636672973633 EntMin 0.0
Epoch 1, Class Loss=0.036551158875226974, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.036551158875226974, Class Loss=0.036551158875226974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.491322330385446
Loss made of: CE 0.1096632182598114, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.040692329406738 EntMin 0.0
Epoch 2, Class Loss=0.13120973110198975, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.13120973110198975, Class Loss=0.13120973110198975, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/13, Loss=5.382558311522007
Loss made of: CE 0.22808019816875458, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.932121276855469 EntMin 0.0
Epoch 3, Class Loss=0.23831528425216675, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.23831528425216675, Class Loss=0.23831528425216675, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.2267903000116345
Loss made of: CE 0.2700776755809784, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.987878799438477 EntMin 0.0
Epoch 4, Class Loss=0.32619714736938477, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.32619714736938477, Class Loss=0.32619714736938477, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.238549986481667
Loss made of: CE 0.5666491389274597, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.883614540100098 EntMin 0.0
Epoch 5, Class Loss=0.4022004306316376, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.4022004306316376, Class Loss=0.4022004306316376, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.307000362873078
Loss made of: CE 0.4727633595466614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.719242095947266 EntMin 0.0
Epoch 6, Class Loss=0.456059068441391, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.456059068441391, Class Loss=0.456059068441391, Reg Loss=0.0
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=8.111380785703659
Loss made of: CE 0.60340815782547, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.309020042419434 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.291348874568939
Loss made of: CE 0.5401822924613953, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.399450302124023 EntMin 0.0
Epoch 1, Class Loss=0.5710397958755493, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.5710397958755493, Class Loss=0.5710397958755493, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/29, Loss=6.901892334222794
Loss made of: CE 0.4185069501399994, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.281064987182617 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.101918005943299
Loss made of: CE 0.36272376775741577, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.014689922332764 EntMin 0.0
Epoch 2, Class Loss=0.444231778383255, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.444231778383255, Class Loss=0.444231778383255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/29, Loss=6.950459626317024
Loss made of: CE 0.5368562340736389, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.901888847351074 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.663316062092781
Loss made of: CE 0.3550108075141907, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.830760478973389 EntMin 0.0
Epoch 3, Class Loss=0.4238315224647522, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.4238315224647522, Class Loss=0.4238315224647522, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/29, Loss=6.693145436048508
Loss made of: CE 0.38619014620780945, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.895102500915527 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.716848790645599
Loss made of: CE 0.32169052958488464, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.512040615081787 EntMin 0.0
Epoch 4, Class Loss=0.4002821147441864, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.4002821147441864, Class Loss=0.4002821147441864, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/29, Loss=6.3307624638080595
Loss made of: CE 0.42258453369140625, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.006867408752441 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.724779027700424
Loss made of: CE 0.29423651099205017, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.538873672485352 EntMin 0.0
Epoch 5, Class Loss=0.4003663659095764, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.4003663659095764, Class Loss=0.4003663659095764, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/29, Loss=6.37028127014637
Loss made of: CE 0.35242336988449097, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.508737564086914 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.547664844989777
Loss made of: CE 0.3461020886898041, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.456165313720703 EntMin 0.0
Epoch 6, Class Loss=0.39819014072418213, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.39819014072418213, Class Loss=0.39819014072418213, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3621273338794708, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.907195
Mean Acc: 0.681523
FreqW Acc: 0.836001
Mean IoU: 0.578594
Class IoU:
	class 0: 0.9096433
	class 1: 0.8644108
	class 2: 0.3503412
	class 3: 0.8227634
	class 4: 0.6670484
	class 5: 0.7864465
	class 6: 0.87508255
	class 7: 0.85558766
	class 8: 0.84095484
	class 9: 1.1233802e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5494315
Class Acc:
	class 0: 0.97459406
	class 1: 0.9260329
	class 2: 0.9107498
	class 3: 0.84739476
	class 4: 0.7735138
	class 5: 0.89087987
	class 6: 0.88820976
	class 7: 0.8930682
	class 8: 0.9118997
	class 9: 1.1250836e-05
	class 10: 0.0
	class 11: 0.0
	class 12: 0.84344405

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[13, 9, 6, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=8.82960359826684
Loss made of: CE 0.06585252285003662, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.941044807434082 EntMin 0.0
Epoch 1, Class Loss=0.08057989925146103, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.08057989925146103, Class Loss=0.08057989925146103, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=7.675859650969505
Loss made of: CE 0.2301948517560959, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0384440422058105 EntMin 0.0
Epoch 2, Class Loss=0.24367186427116394, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.24367186427116394, Class Loss=0.24367186427116394, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.320922753214836
Loss made of: CE 0.39788973331451416, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.057431221008301 EntMin 0.0
Epoch 3, Class Loss=0.3942553997039795, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.3942553997039795, Class Loss=0.3942553997039795, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/11, Loss=7.00674062371254
Loss made of: CE 0.36416566371917725, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.513689994812012 EntMin 0.0
Epoch 4, Class Loss=0.47282323241233826, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.47282323241233826, Class Loss=0.47282323241233826, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/11, Loss=6.8634845316410065
Loss made of: CE 0.5108749866485596, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.44943380355835 EntMin 0.0
Epoch 5, Class Loss=0.5355477929115295, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5355477929115295, Class Loss=0.5355477929115295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.7213661670684814
Loss made of: CE 0.4869837760925293, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.085109233856201 EntMin 0.0
Epoch 6, Class Loss=0.532558023929596, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.532558023929596, Class Loss=0.532558023929596, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=11.421441949531436
Loss made of: CE 0.014726651832461357, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.113935470581055 EntMin 0.0
Epoch 1, Class Loss=0.026764845475554466, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.026764845475554466, Class Loss=0.026764845475554466, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=9.649533200263978
Loss made of: CE 0.20965826511383057, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.824888229370117 EntMin 0.0
Epoch 2, Class Loss=0.11939524114131927, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.11939524114131927, Class Loss=0.11939524114131927, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=8.867624415457248
Loss made of: CE 0.2808450758457184, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.398152351379395 EntMin 0.0
Epoch 3, Class Loss=0.29562699794769287, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.29562699794769287, Class Loss=0.29562699794769287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.495283541083335
Loss made of: CE 0.3537359833717346, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.071834087371826 EntMin 0.0
Epoch 4, Class Loss=0.46549052000045776, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.46549052000045776, Class Loss=0.46549052000045776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.52768725156784
Loss made of: CE 0.5800565481185913, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.264954566955566 EntMin 0.0
Epoch 5, Class Loss=0.5592629909515381, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5592629909515381, Class Loss=0.5592629909515381, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=8.207414296269416
Loss made of: CE 0.5122205018997192, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.480490684509277 EntMin 0.0
Epoch 6, Class Loss=0.5614012479782104, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.5614012479782104, Class Loss=0.5614012479782104, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=8.964744945243002
Loss made of: CE 0.06538209319114685, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.041091918945312 EntMin 0.0
Epoch 1, Class Loss=0.08978626877069473, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.08978626877069473, Class Loss=0.08978626877069473, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=7.879840004444122
Loss made of: CE 0.16793541610240936, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.400678634643555 EntMin 0.0
Epoch 2, Class Loss=0.2562764883041382, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.2562764883041382, Class Loss=0.2562764883041382, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.364507627487183
Loss made of: CE 0.28996866941452026, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.052485466003418 EntMin 0.0
Epoch 3, Class Loss=0.3978266716003418, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.3978266716003418, Class Loss=0.3978266716003418, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/11, Loss=7.105227011442184
Loss made of: CE 0.40804243087768555, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.258309841156006 EntMin 0.0
Epoch 4, Class Loss=0.49124932289123535, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.49124932289123535, Class Loss=0.49124932289123535, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/11, Loss=6.919722068309784
Loss made of: CE 0.5200864672660828, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.663869380950928 EntMin 0.0
Epoch 5, Class Loss=0.5303630828857422, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.5303630828857422, Class Loss=0.5303630828857422, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.894895601272583
Loss made of: CE 0.47047895193099976, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.704693794250488 EntMin 0.0
Epoch 6, Class Loss=0.5307484865188599, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5307484865188599, Class Loss=0.5307484865188599, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=11.27788952360861
Loss made of: CE 0.0121281323954463, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.7579984664917 EntMin 0.0
Epoch 1, Class Loss=0.0257967971265316, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.0257967971265316, Class Loss=0.0257967971265316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=9.499571265280247
Loss made of: CE 0.13267409801483154, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.326605796813965 EntMin 0.0
Epoch 2, Class Loss=0.12772902846336365, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.12772902846336365, Class Loss=0.12772902846336365, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=8.825320127606393
Loss made of: CE 0.3168976306915283, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.449231147766113 EntMin 0.0
Epoch 3, Class Loss=0.2988170385360718, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.2988170385360718, Class Loss=0.2988170385360718, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.59821817278862
Loss made of: CE 0.41747769713401794, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.792956829071045 EntMin 0.0
Epoch 4, Class Loss=0.4559440016746521, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.4559440016746521, Class Loss=0.4559440016746521, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.459064435958862
Loss made of: CE 0.5453872680664062, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.924958229064941 EntMin 0.0
Epoch 5, Class Loss=0.545337438583374, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.545337438583374, Class Loss=0.545337438583374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=8.055371871590614
Loss made of: CE 0.49043792486190796, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.435397624969482 EntMin 0.0
Epoch 6, Class Loss=0.545465350151062, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.545465350151062, Class Loss=0.545465350151062, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6209211945533752, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.865477
Mean Acc: 0.525584
FreqW Acc: 0.763519
Mean IoU: 0.440818
Class IoU:
	class 0: 0.8695194
	class 1: 0.79344743
	class 2: 0.32185957
	class 3: 0.7616758
	class 4: 0.622089
	class 5: 0.7193872
	class 6: 0.59952974
	class 7: 0.7738393
	class 8: 0.7748272
	class 9: 0.0012410663
	class 10: 0.0
	class 11: 0.0
	class 12: 0.374858
	class 13: 0.0
	class 14: 0.0
Class Acc:
	class 0: 0.9812087
	class 1: 0.8558707
	class 2: 0.80284184
	class 3: 0.80995643
	class 4: 0.7506667
	class 5: 0.7805436
	class 6: 0.6031119
	class 7: 0.8135169
	class 8: 0.89347553
	class 9: 0.0012617009
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5913074
	class 13: 0.0
	class 14: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[18, 5, 20, 0]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.501841880148277
Loss made of: CE 0.01739753782749176, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.723710060119629 EntMin 0.0
Epoch 1, Class Loss=0.014281715266406536, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.014281715266406536, Class Loss=0.014281715266406536, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=7.97570909447968
Loss made of: CE 0.09522917866706848, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.978923320770264 EntMin 0.0
Epoch 2, Class Loss=0.07463571429252625, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.07463571429252625, Class Loss=0.07463571429252625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.6603816658258435
Loss made of: CE 0.22569763660430908, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.411660194396973 EntMin 0.0
Epoch 3, Class Loss=0.20253103971481323, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.20253103971481323, Class Loss=0.20253103971481323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.591353479027748
Loss made of: CE 0.3081013560295105, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.698945999145508 EntMin 0.0
Epoch 4, Class Loss=0.348842591047287, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.348842591047287, Class Loss=0.348842591047287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.497351595759392
Loss made of: CE 0.40594610571861267, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.126540660858154 EntMin 0.0
Epoch 5, Class Loss=0.4092303514480591, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.4092303514480591, Class Loss=0.4092303514480591, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.386931958794594
Loss made of: CE 0.466330349445343, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.752261638641357 EntMin 0.0
Epoch 6, Class Loss=0.4509054124355316, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.4509054124355316, Class Loss=0.4509054124355316, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/12, Loss=9.178648364543914
Loss made of: CE 0.8154206275939941, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.87445068359375 EntMin 0.0
Epoch 1, Class Loss=0.7986340522766113, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.7986340522766113, Class Loss=0.7986340522766113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=8.44259445667267
Loss made of: CE 0.6220940947532654, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.1701021194458 EntMin 0.0
Epoch 2, Class Loss=0.6416710615158081, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.6416710615158081, Class Loss=0.6416710615158081, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=8.138962262868882
Loss made of: CE 0.5127264261245728, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.903690338134766 EntMin 0.0
Epoch 3, Class Loss=0.5086865425109863, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.5086865425109863, Class Loss=0.5086865425109863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/12, Loss=7.8092600256204605
Loss made of: CE 0.3904021978378296, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.284184455871582 EntMin 0.0
Epoch 4, Class Loss=0.414023220539093, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.414023220539093, Class Loss=0.414023220539093, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.683308663964271
Loss made of: CE 0.359477162361145, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.598787307739258 EntMin 0.0
Epoch 5, Class Loss=0.3758152425289154, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.3758152425289154, Class Loss=0.3758152425289154, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=7.429260715842247
Loss made of: CE 0.347714900970459, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.917073726654053 EntMin 0.0
Epoch 6, Class Loss=0.34291335940361023, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.34291335940361023, Class Loss=0.34291335940361023, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.381773061864077
Loss made of: CE 0.008237820118665695, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.63338851928711 EntMin 0.0
Epoch 1, Class Loss=0.016458626836538315, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.016458626836538315, Class Loss=0.016458626836538315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=7.8668901797384025
Loss made of: CE 0.06831040978431702, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.423236846923828 EntMin 0.0
Epoch 2, Class Loss=0.0720464363694191, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.0720464363694191, Class Loss=0.0720464363694191, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.836748495697975
Loss made of: CE 0.16259317100048065, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.885697841644287 EntMin 0.0
Epoch 3, Class Loss=0.20302820205688477, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.20302820205688477, Class Loss=0.20302820205688477, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.661803784966469
Loss made of: CE 0.31997406482696533, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.271527290344238 EntMin 0.0
Epoch 4, Class Loss=0.34304261207580566, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.34304261207580566, Class Loss=0.34304261207580566, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.575783097743988
Loss made of: CE 0.3227160573005676, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.315925598144531 EntMin 0.0
Epoch 5, Class Loss=0.42891842126846313, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.42891842126846313, Class Loss=0.42891842126846313, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.342456817626953
Loss made of: CE 0.4145955741405487, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.064337730407715 EntMin 0.0
Epoch 6, Class Loss=0.4466879069805145, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.4466879069805145, Class Loss=0.4466879069805145, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=7.290875577926636
Loss made of: CE 0.0518655851483345, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.976256847381592 EntMin 0.0
Epoch 1, Class Loss=0.05152054876089096, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.05152054876089096, Class Loss=0.05152054876089096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/11, Loss=6.704820289462805
Loss made of: CE 0.14590898156166077, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.921730041503906 EntMin 0.0
Epoch 2, Class Loss=0.15834715962409973, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.15834715962409973, Class Loss=0.15834715962409973, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/11, Loss=6.541478423774242
Loss made of: CE 0.3419366776943207, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.483226776123047 EntMin 0.0
Epoch 3, Class Loss=0.2913227081298828, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.2913227081298828, Class Loss=0.2913227081298828, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/11, Loss=6.5893792599439625
Loss made of: CE 0.33769330382347107, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.841440200805664 EntMin 0.0
Epoch 4, Class Loss=0.38193702697753906, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.38193702697753906, Class Loss=0.38193702697753906, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/11, Loss=6.455901837348938
Loss made of: CE 0.437278687953949, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.456448554992676 EntMin 0.0
Epoch 5, Class Loss=0.43383654952049255, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.43383654952049255, Class Loss=0.43383654952049255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/11, Loss=6.326530569791794
Loss made of: CE 0.4401836395263672, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.764365196228027 EntMin 0.0
Epoch 6, Class Loss=0.4379013776779175, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.4379013776779175, Class Loss=0.4379013776779175, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5374398231506348, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.871275
Mean Acc: 0.554449
FreqW Acc: 0.769666
Mean IoU: 0.480170
Class IoU:
	class 0: 0.8663128
	class 1: 0.8189694
	class 2: 0.34268752
	class 3: 0.7634266
	class 4: 0.63308173
	class 5: 0.7131758
	class 6: 0.6367356
	class 7: 0.80175906
	class 8: 0.75656563
	class 9: 0.00036079495
	class 10: 0.0
	class 11: 0.0
	class 12: 0.3100735
	class 13: 0.0
	class 14: 0.5594078
Class Acc:
	class 0: 0.9831297
	class 1: 0.87161344
	class 2: 0.7779851
	class 3: 0.79324055
	class 4: 0.73351204
	class 5: 0.771466
	class 6: 0.640538
	class 7: 0.8409162
	class 8: 0.8701848
	class 9: 0.00036324127
	class 10: 0.0
	class 11: 0.0
	class 12: 0.38647568
	class 13: 0.0
	class 14: 0.64731514

federated global round: 17, step: 3
select part of clients to conduct local training
[1, 16, 6, 21]
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=7.988714967668057
Loss made of: CE 0.0521334633231163, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.340981483459473 EntMin 0.0
Epoch 1, Class Loss=0.051750414073467255, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.051750414073467255, Class Loss=0.051750414073467255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=6.663451066613197
Loss made of: CE 0.12125454843044281, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.556579113006592 EntMin 0.0
Epoch 2, Class Loss=0.15790140628814697, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.15790140628814697, Class Loss=0.15790140628814697, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.471412186324597
Loss made of: CE 0.2638434171676636, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.500247001647949 EntMin 0.0
Epoch 3, Class Loss=0.26739656925201416, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.26739656925201416, Class Loss=0.26739656925201416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.3630850374698635
Loss made of: CE 0.31489336490631104, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.673909664154053 EntMin 0.0
Epoch 4, Class Loss=0.38582199811935425, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.38582199811935425, Class Loss=0.38582199811935425, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.199981129169464
Loss made of: CE 0.4305558204650879, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.394128322601318 EntMin 0.0
Epoch 5, Class Loss=0.41818633675575256, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.41818633675575256, Class Loss=0.41818633675575256, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=6.130836170911789
Loss made of: CE 0.4348679780960083, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.082324981689453 EntMin 0.0
Epoch 6, Class Loss=0.4345659017562866, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.4345659017562866, Class Loss=0.4345659017562866, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=7.822359281219542
Loss made of: CE 0.04611361026763916, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.559427261352539 EntMin 0.0
Epoch 1, Class Loss=0.046973783522844315, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.046973783522844315, Class Loss=0.046973783522844315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=6.65796399563551
Loss made of: CE 0.14207640290260315, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.867499351501465 EntMin 0.0
Epoch 2, Class Loss=0.15887166559696198, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.15887166559696198, Class Loss=0.15887166559696198, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.509845575690269
Loss made of: CE 0.20856451988220215, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.792143821716309 EntMin 0.0
Epoch 3, Class Loss=0.2782016396522522, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.2782016396522522, Class Loss=0.2782016396522522, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.486256355047226
Loss made of: CE 0.40087366104125977, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.515643119812012 EntMin 0.0
Epoch 4, Class Loss=0.3858712911605835, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.3858712911605835, Class Loss=0.3858712911605835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.3064963042736055
Loss made of: CE 0.3578472137451172, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.643145561218262 EntMin 0.0
Epoch 5, Class Loss=0.41106900572776794, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.41106900572776794, Class Loss=0.41106900572776794, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=6.075319540500641
Loss made of: CE 0.44888895750045776, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.2547712326049805 EntMin 0.0
Epoch 6, Class Loss=0.4292300045490265, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.4292300045490265, Class Loss=0.4292300045490265, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.813956242799758
Loss made of: CE 0.7405900955200195, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.024456024169922 EntMin 0.0
Epoch 1, Class Loss=0.7659671306610107, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.7659671306610107, Class Loss=0.7659671306610107, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/11, Loss=7.244868421554566
Loss made of: CE 0.6239885091781616, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.779252529144287 EntMin 0.0
Epoch 2, Class Loss=0.62302565574646, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.62302565574646, Class Loss=0.62302565574646, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/11, Loss=6.800862574577332
Loss made of: CE 0.4764479398727417, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4979166984558105 EntMin 0.0
Epoch 3, Class Loss=0.4978410601615906, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4978410601615906, Class Loss=0.4978410601615906, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/11, Loss=6.474200373888015
Loss made of: CE 0.4021744430065155, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.8774542808532715 EntMin 0.0
Epoch 4, Class Loss=0.4137779176235199, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4137779176235199, Class Loss=0.4137779176235199, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/11, Loss=6.318321704864502
Loss made of: CE 0.31927165389060974, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.182651042938232 EntMin 0.0
Epoch 5, Class Loss=0.35503625869750977, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.35503625869750977, Class Loss=0.35503625869750977, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/11, Loss=6.151158419251442
Loss made of: CE 0.27883368730545044, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.331253528594971 EntMin 0.0
Epoch 6, Class Loss=0.31996282935142517, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.31996282935142517, Class Loss=0.31996282935142517, Reg Loss=0.0
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=7.233344008633867
Loss made of: CE 0.005094228312373161, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.4254069328308105 EntMin 0.0
Epoch 1, Class Loss=0.005476297810673714, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.005476297810673714, Class Loss=0.005476297810673714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=7.012511399760842
Loss made of: CE 0.06093597039580345, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.327029705047607 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.04034504294395447, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.04034504294395447, Class Loss=0.04034504294395447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=7.105824150145054
Loss made of: CE 0.11734098196029663, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.773303508758545 EntMin 0.0
Epoch 3, Class Loss=0.11783832311630249, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.11783832311630249, Class Loss=0.11783832311630249, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=7.106412813067436
Loss made of: CE 0.2578563690185547, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.232841491699219 EntMin 0.0
Epoch 4, Class Loss=0.21873077750205994, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.21873077750205994, Class Loss=0.21873077750205994, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=6.854523608088494
Loss made of: CE 0.2899898886680603, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.743227958679199 EntMin 0.0
Epoch 5, Class Loss=0.29621726274490356, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.29621726274490356, Class Loss=0.29621726274490356, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=7.085913547873497
Loss made of: CE 0.31121838092803955, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.44037389755249 EntMin 0.0
Epoch 6, Class Loss=0.3226543068885803, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.3226543068885803, Class Loss=0.3226543068885803, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.49503910541534424, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.870991
Mean Acc: 0.555513
FreqW Acc: 0.772727
Mean IoU: 0.481014
Class IoU:
	class 0: 0.8700298
	class 1: 0.823005
	class 2: 0.33417526
	class 3: 0.76789105
	class 4: 0.6073125
	class 5: 0.7252274
	class 6: 0.6303821
	class 7: 0.8164175
	class 8: 0.74974877
	class 9: 0.0042679003
	class 10: 0.0
	class 11: 0.0
	class 12: 0.30430642
	class 13: 0.1724495
	class 14: 0.40999654
Class Acc:
	class 0: 0.9841309
	class 1: 0.86450166
	class 2: 0.85563076
	class 3: 0.7976484
	class 4: 0.6927215
	class 5: 0.7875007
	class 6: 0.63418394
	class 7: 0.8569403
	class 8: 0.83210856
	class 9: 0.0043617883
	class 10: 0.0
	class 11: 0.0
	class 12: 0.37977046
	class 13: 0.2268554
	class 14: 0.41633788

federated global round: 18, step: 3
select part of clients to conduct local training
[8, 2, 17, 14]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=5.825722195580601
Loss made of: CE 0.028217311948537827, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.520211219787598 EntMin 0.0
Epoch 1, Class Loss=0.02831835113465786, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.02831835113465786, Class Loss=0.02831835113465786, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/11, Loss=5.5977874509990215
Loss made of: CE 0.06822486221790314, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.102187633514404 EntMin 0.0
Epoch 2, Class Loss=0.09279383718967438, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.09279383718967438, Class Loss=0.09279383718967438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.691203551739454
Loss made of: CE 0.1638576090335846, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5119242668151855 EntMin 0.0
Epoch 3, Class Loss=0.16748599708080292, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.16748599708080292, Class Loss=0.16748599708080292, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.552929721772671
Loss made of: CE 0.21178176999092102, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.152319431304932 EntMin 0.0
Epoch 4, Class Loss=0.23924469947814941, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.23924469947814941, Class Loss=0.23924469947814941, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.610915450751781
Loss made of: CE 0.24789385497570038, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.680379390716553 EntMin 0.0
Epoch 5, Class Loss=0.2950023412704468, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.2950023412704468, Class Loss=0.2950023412704468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.516259291768074
Loss made of: CE 0.2926753759384155, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6809916496276855 EntMin 0.0
Epoch 6, Class Loss=0.3187618851661682, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.3187618851661682, Class Loss=0.3187618851661682, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=6.113323105499148
Loss made of: CE 0.017375648021697998, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.60626745223999 EntMin 0.0
Epoch 1, Class Loss=0.02847781591117382, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.02847781591117382, Class Loss=0.02847781591117382, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=5.807990129292011
Loss made of: CE 0.08975403010845184, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.628710746765137 EntMin 0.0
Epoch 2, Class Loss=0.09254299849271774, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.09254299849271774, Class Loss=0.09254299849271774, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.788472604751587
Loss made of: CE 0.17051702737808228, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.028531074523926 EntMin 0.0
Epoch 3, Class Loss=0.16324003040790558, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.16324003040790558, Class Loss=0.16324003040790558, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.8089066535234455
Loss made of: CE 0.24621789157390594, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.476403713226318 EntMin 0.0
Epoch 4, Class Loss=0.23908551037311554, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.23908551037311554, Class Loss=0.23908551037311554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.746247747540474
Loss made of: CE 0.29421350359916687, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.545892715454102 EntMin 0.0
Epoch 5, Class Loss=0.2934620976448059, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.2934620976448059, Class Loss=0.2934620976448059, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.660240015387535
Loss made of: CE 0.3199843168258667, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.900186538696289 EntMin 0.0
Epoch 6, Class Loss=0.3205185532569885, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.3205185532569885, Class Loss=0.3205185532569885, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.481457920302637
Loss made of: CE 0.004667520988732576, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.455895900726318 EntMin 0.0
Epoch 1, Class Loss=0.011971651576459408, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.011971651576459408, Class Loss=0.011971651576459408, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=7.422784248739481
Loss made of: CE 0.0473453663289547, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.41120719909668 EntMin 0.0
Epoch 2, Class Loss=0.055042997002601624, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.055042997002601624, Class Loss=0.055042997002601624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=7.261773733794689
Loss made of: CE 0.156190887093544, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.626680374145508 EntMin 0.0
Epoch 3, Class Loss=0.14725199341773987, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.14725199341773987, Class Loss=0.14725199341773987, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=7.287720657885075
Loss made of: CE 0.2869518995285034, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.731362819671631 EntMin 0.0
Epoch 4, Class Loss=0.26118993759155273, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.26118993759155273, Class Loss=0.26118993759155273, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=6.940799516439438
Loss made of: CE 0.26875004172325134, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.416917324066162 EntMin 0.0
Epoch 5, Class Loss=0.3046571612358093, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.3046571612358093, Class Loss=0.3046571612358093, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=7.119177308678627
Loss made of: CE 0.28690820932388306, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.573609828948975 EntMin 0.0
Epoch 6, Class Loss=0.3674757480621338, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.3674757480621338, Class Loss=0.3674757480621338, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=6.09439967283979
Loss made of: CE 0.015325700864195824, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.170583724975586 EntMin 0.0
Epoch 1, Class Loss=0.023442458361387253, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.023442458361387253, Class Loss=0.023442458361387253, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=5.910377340763807
Loss made of: CE 0.08821241557598114, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.880630016326904 EntMin 0.0
Epoch 2, Class Loss=0.08460914343595505, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.08460914343595505, Class Loss=0.08460914343595505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.898908971250057
Loss made of: CE 0.1638706475496292, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.702422142028809 EntMin 0.0
Epoch 3, Class Loss=0.16093818843364716, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.16093818843364716, Class Loss=0.16093818843364716, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.863960540294647
Loss made of: CE 0.21006403863430023, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.21752405166626 EntMin 0.0
Epoch 4, Class Loss=0.24336421489715576, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.24336421489715576, Class Loss=0.24336421489715576, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.724327784776688
Loss made of: CE 0.2550909221172333, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.037633895874023 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.29114779829978943, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.29114779829978943, Class Loss=0.29114779829978943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.8260102570056915
Loss made of: CE 0.3043619990348816, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.163356781005859 EntMin 0.0
Epoch 6, Class Loss=0.3324931561946869, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.3324931561946869, Class Loss=0.3324931561946869, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4912758767604828, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.868097
Mean Acc: 0.558901
FreqW Acc: 0.778542
Mean IoU: 0.464502
Class IoU:
	class 0: 0.88332534
	class 1: 0.8393019
	class 2: 0.33492768
	class 3: 0.7798071
	class 4: 0.6194612
	class 5: 0.7431201
	class 6: 0.70139647
	class 7: 0.82532614
	class 8: 0.75266635
	class 9: 0.0025475805
	class 10: 0.0
	class 11: 0.0
	class 12: 0.27305475
	class 13: 0.20978388
	class 14: 0.0028164198
Class Acc:
	class 0: 0.97980714
	class 1: 0.88722503
	class 2: 0.87834454
	class 3: 0.81462777
	class 4: 0.71143997
	class 5: 0.8197343
	class 6: 0.7066129
	class 7: 0.8807076
	class 8: 0.84405476
	class 9: 0.0025841563
	class 10: 0.0
	class 11: 0.0
	class 12: 0.30377945
	class 13: 0.5517842
	class 14: 0.0028166745

federated global round: 19, step: 3
select part of clients to conduct local training
[1, 9, 8, 0]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=6.06710821390152
Loss made of: CE 0.3785439431667328, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.474475860595703 EntMin 0.0
Epoch 1, Class Loss=0.4086821973323822, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.4086821973323822, Class Loss=0.4086821973323822, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/11, Loss=5.7517041832208635
Loss made of: CE 0.37093693017959595, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.557099342346191 EntMin 0.0
Epoch 2, Class Loss=0.37179115414619446, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.37179115414619446, Class Loss=0.37179115414619446, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/11, Loss=5.68762184381485
Loss made of: CE 0.35407567024230957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.753783226013184 EntMin 0.0
Epoch 3, Class Loss=0.34295400977134705, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.34295400977134705, Class Loss=0.34295400977134705, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/11, Loss=5.65427325963974
Loss made of: CE 0.30054396390914917, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.950629234313965 EntMin 0.0
Epoch 4, Class Loss=0.32189393043518066, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.32189393043518066, Class Loss=0.32189393043518066, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/11, Loss=5.665837317705154
Loss made of: CE 0.31751105189323425, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.951206207275391 EntMin 0.0
Epoch 5, Class Loss=0.3019217550754547, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.3019217550754547, Class Loss=0.3019217550754547, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/11, Loss=5.495426738262177
Loss made of: CE 0.28137537837028503, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.638429164886475 EntMin 0.0
Epoch 6, Class Loss=0.2988535463809967, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.2988535463809967, Class Loss=0.2988535463809967, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.989227712154388
Loss made of: CE 0.6181994676589966, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.669230937957764 EntMin 0.0
Epoch 1, Class Loss=0.6877014636993408, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.6877014636993408, Class Loss=0.6877014636993408, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/12, Loss=7.782056459784508
Loss made of: CE 0.4622526168823242, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.984073162078857 EntMin 0.0
Epoch 2, Class Loss=0.505834698677063, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.505834698677063, Class Loss=0.505834698677063, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/12, Loss=7.319351157546043
Loss made of: CE 0.3601702153682709, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0672712326049805 EntMin 0.0
Epoch 3, Class Loss=0.39316993951797485, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.39316993951797485, Class Loss=0.39316993951797485, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/12, Loss=7.002383536100387
Loss made of: CE 0.2852576971054077, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.813016891479492 EntMin 0.0
Epoch 4, Class Loss=0.339602530002594, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.339602530002594, Class Loss=0.339602530002594, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/12, Loss=7.260864919424057
Loss made of: CE 0.37893974781036377, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.783194541931152 EntMin 0.0
Epoch 5, Class Loss=0.32145658135414124, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.32145658135414124, Class Loss=0.32145658135414124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/12, Loss=7.10493015050888
Loss made of: CE 0.28288161754608154, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8413543701171875 EntMin 0.0
Epoch 6, Class Loss=0.3249219059944153, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3249219059944153, Class Loss=0.3249219059944153, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/11, Loss=5.854496413469315
Loss made of: CE 0.4149629473686218, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.005861282348633 EntMin 0.0
Epoch 1, Class Loss=0.40185266733169556, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.40185266733169556, Class Loss=0.40185266733169556, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/11, Loss=5.495223662257194
Loss made of: CE 0.37028276920318604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.82858943939209 EntMin 0.0
Epoch 2, Class Loss=0.37136054039001465, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.37136054039001465, Class Loss=0.37136054039001465, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/11, Loss=5.487847927212715
Loss made of: CE 0.3286081552505493, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.240580081939697 EntMin 0.0
Epoch 3, Class Loss=0.3478015661239624, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.3478015661239624, Class Loss=0.3478015661239624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/11, Loss=5.408719518780709
Loss made of: CE 0.28902727365493774, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0800323486328125 EntMin 0.0
Epoch 4, Class Loss=0.32143688201904297, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.32143688201904297, Class Loss=0.32143688201904297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/11, Loss=5.4580674976110455
Loss made of: CE 0.2739144265651703, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.580008506774902 EntMin 0.0
Epoch 5, Class Loss=0.3161190450191498, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.3161190450191498, Class Loss=0.3161190450191498, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/11, Loss=5.418343353271484
Loss made of: CE 0.2988779544830322, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.386477947235107 EntMin 0.0
Epoch 6, Class Loss=0.30528637766838074, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.30528637766838074, Class Loss=0.30528637766838074, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/11, Loss=5.90791554749012
Loss made of: CE 0.42102986574172974, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.507102966308594 EntMin 0.0
Epoch 1, Class Loss=0.41588348150253296, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.41588348150253296, Class Loss=0.41588348150253296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/11, Loss=5.760468816757202
Loss made of: CE 0.3640807569026947, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.773364543914795 EntMin 0.0
Epoch 2, Class Loss=0.36002689599990845, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.36002689599990845, Class Loss=0.36002689599990845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/11, Loss=5.7359416544437405
Loss made of: CE 0.304324209690094, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.658267021179199 EntMin 0.0
Epoch 3, Class Loss=0.3286568224430084, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3286568224430084, Class Loss=0.3286568224430084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/11, Loss=5.704182574152947
Loss made of: CE 0.29532426595687866, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0843505859375 EntMin 0.0
Epoch 4, Class Loss=0.3121371865272522, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3121371865272522, Class Loss=0.3121371865272522, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/11, Loss=5.58225579559803
Loss made of: CE 0.3141167163848877, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.728333950042725 EntMin 0.0
Epoch 5, Class Loss=0.296816885471344, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.296816885471344, Class Loss=0.296816885471344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/11, Loss=5.617341837286949
Loss made of: CE 0.26284366846084595, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.040505409240723 EntMin 0.0
Epoch 6, Class Loss=0.2913973927497864, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.2913973927497864, Class Loss=0.2913973927497864, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4789133071899414, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.857016
Mean Acc: 0.523298
FreqW Acc: 0.761784
Mean IoU: 0.435204
Class IoU:
	class 0: 0.87576133
	class 1: 0.82015234
	class 2: 0.3492901
	class 3: 0.75031185
	class 4: 0.5700778
	class 5: 0.71554166
	class 6: 0.56930685
	class 7: 0.8221702
	class 8: 0.71814317
	class 9: 0.00032920987
	class 10: 0.0
	class 11: 0.0
	class 12: 0.12623747
	class 13: 0.20772511
	class 14: 0.0030087205
Class Acc:
	class 0: 0.98259765
	class 1: 0.84823143
	class 2: 0.86472815
	class 3: 0.7722181
	class 4: 0.6306598
	class 5: 0.7646248
	class 6: 0.5714252
	class 7: 0.85705
	class 8: 0.7745927
	class 9: 0.00033045313
	class 10: 0.0
	class 11: 0.0
	class 12: 0.12947884
	class 13: 0.6505276
	class 14: 0.003009055

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[11, 2, 15, 6]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=8.600868766009807
Loss made of: CE 0.04259643703699112, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.208036422729492 EntMin 0.0
Epoch 1, Class Loss=0.06649206578731537, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.06649206578731537, Class Loss=0.06649206578731537, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.160530090332031
Loss made of: CE 0.25279420614242554, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.221738338470459 EntMin 0.0
Epoch 2, Class Loss=0.23164403438568115, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.23164403438568115, Class Loss=0.23164403438568115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.520588907599449
Loss made of: CE 0.3981516361236572, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.846944808959961 EntMin 0.0
Epoch 3, Class Loss=0.39445894956588745, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.39445894956588745, Class Loss=0.39445894956588745, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.228507217764855
Loss made of: CE 0.48097747564315796, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.690491676330566 EntMin 0.0
Epoch 4, Class Loss=0.5218737721443176, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5218737721443176, Class Loss=0.5218737721443176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.13421445786953
Loss made of: CE 0.5651718974113464, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.582437038421631 EntMin 0.0
Epoch 5, Class Loss=0.5632166862487793, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5632166862487793, Class Loss=0.5632166862487793, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.128048664331436
Loss made of: CE 0.5970271825790405, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.666315078735352 EntMin 0.0
Epoch 6, Class Loss=0.6316021680831909, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.6316021680831909, Class Loss=0.6316021680831909, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=8.567231697775423
Loss made of: CE 0.06575426459312439, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.462405204772949 EntMin 0.0
Epoch 1, Class Loss=0.0564090758562088, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.0564090758562088, Class Loss=0.0564090758562088, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=6.977566134929657
Loss made of: CE 0.22289827466011047, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.206417083740234 EntMin 0.0
Epoch 2, Class Loss=0.20856861770153046, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.20856861770153046, Class Loss=0.20856861770153046, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.4912979990243915
Loss made of: CE 0.3266479969024658, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.269868850708008 EntMin 0.0
Epoch 3, Class Loss=0.39210206270217896, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.39210206270217896, Class Loss=0.39210206270217896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.366399908065796
Loss made of: CE 0.37349289655685425, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.708079814910889 EntMin 0.0
Epoch 4, Class Loss=0.49944305419921875, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.49944305419921875, Class Loss=0.49944305419921875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.066438364982605
Loss made of: CE 0.6201953291893005, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.561979293823242 EntMin 0.0
Epoch 5, Class Loss=0.583899736404419, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.583899736404419, Class Loss=0.583899736404419, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.1963045835495
Loss made of: CE 0.6948916912078857, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.351336479187012 EntMin 0.0
Epoch 6, Class Loss=0.6391271352767944, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.6391271352767944, Class Loss=0.6391271352767944, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.472326538525522
Loss made of: CE 0.03962866961956024, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.214399337768555 EntMin 0.0
Epoch 1, Class Loss=0.046765223145484924, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.046765223145484924, Class Loss=0.046765223145484924, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=6.969475761055946
Loss made of: CE 0.20438742637634277, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.207853317260742 EntMin 0.0
Epoch 2, Class Loss=0.19984853267669678, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.19984853267669678, Class Loss=0.19984853267669678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.49413737654686
Loss made of: CE 0.4594535827636719, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.840625762939453 EntMin 0.0
Epoch 3, Class Loss=0.3714274764060974, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.3714274764060974, Class Loss=0.3714274764060974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.452667102217674
Loss made of: CE 0.4057329595088959, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.926430702209473 EntMin 0.0
Epoch 4, Class Loss=0.4993135333061218, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.4993135333061218, Class Loss=0.4993135333061218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.234336921572686
Loss made of: CE 0.7152748107910156, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.901706218719482 EntMin 0.0
Epoch 5, Class Loss=0.5966703295707703, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.5966703295707703, Class Loss=0.5966703295707703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.159207314252853
Loss made of: CE 0.6032284498214722, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.899423599243164 EntMin 0.0
Epoch 6, Class Loss=0.6278791427612305, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.6278791427612305, Class Loss=0.6278791427612305, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=7.982418360561132
Loss made of: CE 0.10251962393522263, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.942749977111816 EntMin 0.0
Epoch 1, Batch 20/97, Loss=7.037098339945078
Loss made of: CE 0.08417811244726181, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.802596092224121 EntMin 0.0
Epoch 1, Batch 30/97, Loss=6.161689730919898
Loss made of: CE 0.13820311427116394, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.002223968505859 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.9305503303185105
Loss made of: CE 0.08346907794475555, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.553375244140625 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.916277619823814
Loss made of: CE 0.056162457913160324, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.993838310241699 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.818841734528542
Loss made of: CE 0.04265367239713669, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.461111068725586 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.710156101174652
Loss made of: CE 0.04964642971754074, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.358861923217773 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.378190671280026
Loss made of: CE 0.1334964632987976, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.660422325134277 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.368913577497006
Loss made of: CE 0.0591425895690918, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.269211769104004 EntMin 0.0
Epoch 1, Class Loss=0.07000405341386795, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.07000405341386795, Class Loss=0.07000405341386795, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/97, Loss=5.586149001121521
Loss made of: CE 0.13074207305908203, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.836611747741699 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.533696934580803
Loss made of: CE 0.1399390697479248, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.041634559631348 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.237050335109234
Loss made of: CE 0.1795825958251953, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.037961483001709 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.364426329731941
Loss made of: CE 0.13207192718982697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.620840072631836 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.241234300285578
Loss made of: CE 0.12351878732442856, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.778675556182861 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.268576005846262
Loss made of: CE 0.11724711954593658, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.245721340179443 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.260858395695687
Loss made of: CE 0.14735983312129974, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.277014255523682 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.3507128551602365
Loss made of: CE 0.1654539704322815, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.46522855758667 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.356246206909418
Loss made of: CE 0.11439216136932373, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.308173656463623 EntMin 0.0
Epoch 2, Class Loss=0.143388032913208, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.143388032913208, Class Loss=0.143388032913208, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/97, Loss=5.214907252788544
Loss made of: CE 0.14019441604614258, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.796259880065918 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.230697190761566
Loss made of: CE 0.25284266471862793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.621912956237793 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.5767085120081905
Loss made of: CE 0.28210434317588806, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4954729080200195 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.386580561101437
Loss made of: CE 0.22090181708335876, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.497962951660156 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.418161951005459
Loss made of: CE 0.1979660987854004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.830187797546387 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.35262648165226
Loss made of: CE 0.15344195067882538, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.68756103515625 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.059277376532554
Loss made of: CE 0.20147882401943207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.703185558319092 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.328326600790024
Loss made of: CE 0.16430875658988953, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.929412841796875 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.26989786028862
Loss made of: CE 0.2038569152355194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.920720100402832 EntMin 0.0
Epoch 3, Class Loss=0.20189164578914642, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.20189164578914642, Class Loss=0.20189164578914642, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/97, Loss=5.199197608232498
Loss made of: CE 0.2490953654050827, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.669496059417725 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.208957205712795
Loss made of: CE 0.22347241640090942, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.530081748962402 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.9960161164402965
Loss made of: CE 0.17010287940502167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.401878356933594 EntMin 0.0
Epoch 4, Batch 40/97, Loss=5.100193873047829
Loss made of: CE 0.24937419593334198, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.937223434448242 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.061818417906761
Loss made of: CE 0.1774507462978363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.94069766998291 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.9350341856479645
Loss made of: CE 0.23269663751125336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.812404632568359 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.167381803691387
Loss made of: CE 0.20297811925411224, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.46245002746582 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.199467059969902
Loss made of: CE 0.25476938486099243, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.772122859954834 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.016630598902703
Loss made of: CE 0.20300760865211487, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.113101482391357 EntMin 0.0
Epoch 4, Class Loss=0.2367628812789917, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.2367628812789917, Class Loss=0.2367628812789917, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/97, Loss=5.07903500944376
Loss made of: CE 0.2986525297164917, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.172701358795166 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.981627184152603
Loss made of: CE 0.2881520390510559, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.600502014160156 EntMin 0.0
Epoch 5, Batch 30/97, Loss=5.007355359196663
Loss made of: CE 0.2980434000492096, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2171173095703125 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.087844282388687
Loss made of: CE 0.3636641502380371, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2205095291137695 EntMin 0.0
Epoch 5, Batch 50/97, Loss=5.149155488610267
Loss made of: CE 0.2247312068939209, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.914368629455566 EntMin 0.0
Epoch 5, Batch 60/97, Loss=5.1343037545681
Loss made of: CE 0.31201398372650146, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.330424785614014 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.802513396739959
Loss made of: CE 0.30371376872062683, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.234928131103516 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.992597216367722
Loss made of: CE 0.24587123095989227, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0998992919921875 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.99464259147644
Loss made of: CE 0.22282712161540985, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2689032554626465 EntMin 0.0
Epoch 5, Class Loss=0.2802087664604187, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.2802087664604187, Class Loss=0.2802087664604187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/97, Loss=5.116421607136727
Loss made of: CE 0.340523898601532, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.198639869689941 EntMin 0.0
Epoch 6, Batch 20/97, Loss=5.270940154790878
Loss made of: CE 0.2582199275493622, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.339240550994873 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.9781690567731856
Loss made of: CE 0.28961992263793945, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.765850067138672 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.15441802740097
Loss made of: CE 0.3368975520133972, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.958174705505371 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.961858187615872
Loss made of: CE 0.29345083236694336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4238057136535645 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.999678909778595
Loss made of: CE 0.29706916213035583, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.835996627807617 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.979313418269157
Loss made of: CE 0.38555097579956055, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.225712776184082 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.9225201740860935
Loss made of: CE 0.26565980911254883, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.946602821350098 EntMin 0.0
Epoch 6, Batch 90/97, Loss=5.152001038193703
Loss made of: CE 0.3380274772644043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.901279926300049 EntMin 0.0
Epoch 6, Class Loss=0.313141793012619, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.313141793012619, Class Loss=0.313141793012619, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6913529634475708, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.791927
Mean Acc: 0.459684
FreqW Acc: 0.651288
Mean IoU: 0.372020
Class IoU:
	class 0: 0.79944754
	class 1: 0.8174769
	class 2: 0.3379514
	class 3: 0.7071165
	class 4: 0.5582691
	class 5: 0.7066127
	class 6: 0.5426395
	class 7: 0.7951892
	class 8: 0.67297035
	class 9: 0.017091429
	class 10: 2.74813e-06
	class 11: 0.0
	class 12: 0.1789904
	class 13: 0.19056791
	class 14: 1.7312068e-05
	class 15: 0.0
	class 16: 0.0
Class Acc:
	class 0: 0.98041666
	class 1: 0.86929166
	class 2: 0.9133852
	class 3: 0.7287534
	class 4: 0.61853087
	class 5: 0.78515375
	class 6: 0.5502745
	class 7: 0.84374857
	class 8: 0.70485437
	class 9: 0.017414365
	class 10: 2.748509e-06
	class 11: 0.0
	class 12: 0.18297367
	class 13: 0.61980623
	class 14: 1.7312075e-05
	class 15: 0.0
	class 16: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[20, 2, 24, 4]
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=6.569984972290695
Loss made of: CE 0.0356900654733181, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.930325508117676 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.28133089337498
Loss made of: CE 0.02373497746884823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.953805923461914 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.437231948226691
Loss made of: CE 0.0429648831486702, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.225940227508545 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.485277443006635
Loss made of: CE 0.05215425044298172, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.561293125152588 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.187857215944677
Loss made of: CE 0.02308952622115612, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.487668991088867 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.18031591437757
Loss made of: CE 0.031343068927526474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.939516544342041 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.321761363185942
Loss made of: CE 0.03904835879802704, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.666866302490234 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.14784912802279
Loss made of: CE 0.048466991633176804, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.725202560424805 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/97, Loss=5.4585075251758095
Loss made of: CE 0.02488226443529129, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.172989845275879 EntMin 0.0
Epoch 1, Class Loss=0.042238734662532806, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.042238734662532806, Class Loss=0.042238734662532806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/97, Loss=5.067196802049875
Loss made of: CE 0.10859763622283936, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.945074558258057 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.087693527340889
Loss made of: CE 0.05788916349411011, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.082618713378906 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.030891697108745
Loss made of: CE 0.06597160547971725, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.920804977416992 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.121911711245775
Loss made of: CE 0.05452435463666916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6269330978393555 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.046250297129154
Loss made of: CE 0.08323860168457031, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.532171249389648 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.003841921687126
Loss made of: CE 0.11590354144573212, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.906208038330078 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.393220932781697
Loss made of: CE 0.15899083018302917, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.315227508544922 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.908131164312363
Loss made of: CE 0.07237803190946579, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.94423246383667 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.942493262887001
Loss made of: CE 0.05409742146730423, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.263031005859375 EntMin 0.0
Epoch 2, Class Loss=0.11132276058197021, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.11132276058197021, Class Loss=0.11132276058197021, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/97, Loss=5.074184864014387
Loss made of: CE 0.1366206258535385, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.861425399780273 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.290207506716252
Loss made of: CE 0.1581263244152069, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.246872425079346 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.987324498593807
Loss made of: CE 0.1366622895002365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7410807609558105 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.080056727677584
Loss made of: CE 0.279415488243103, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.829018592834473 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.977363060414791
Loss made of: CE 0.16421784460544586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.558682441711426 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.317080560326576
Loss made of: CE 0.16222038865089417, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.63907527923584 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.840047799050808
Loss made of: CE 0.14362159371376038, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.770513534545898 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.980439402163029
Loss made of: CE 0.13350149989128113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.384942054748535 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.188692711293697
Loss made of: CE 0.10207125544548035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.449697971343994 EntMin 0.0
Epoch 3, Class Loss=0.16525577008724213, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.16525577008724213, Class Loss=0.16525577008724213, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/97, Loss=5.301189436018467
Loss made of: CE 0.21953462064266205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.812578201293945 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.098073835670948
Loss made of: CE 0.25567203760147095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746896743774414 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.98959897607565
Loss made of: CE 0.20171412825584412, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.322296619415283 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.842446446418762
Loss made of: CE 0.17481303215026855, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.608443260192871 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.008268012106418
Loss made of: CE 0.244807168841362, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590672016143799 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.795743700861931
Loss made of: CE 0.20683424174785614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.46334981918335 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.848428800702095
Loss made of: CE 0.2615169882774353, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9629411697387695 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.279756156355143
Loss made of: CE 0.22045139968395233, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.749954700469971 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.98013076633215
Loss made of: CE 0.17746126651763916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.847970485687256 EntMin 0.0
Epoch 4, Class Loss=0.2021942138671875, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.2021942138671875, Class Loss=0.2021942138671875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/97, Loss=5.035111777484417
Loss made of: CE 0.2140236347913742, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.558055877685547 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.969307497143745
Loss made of: CE 0.21546918153762817, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.899043083190918 EntMin 0.0
Epoch 5, Batch 30/97, Loss=5.054986962676049
Loss made of: CE 0.28574079275131226, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.028794288635254 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.952824378013611
Loss made of: CE 0.20685473084449768, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.538450717926025 EntMin 0.0
Epoch 5, Batch 50/97, Loss=5.004138700664043
Loss made of: CE 0.21758905053138733, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.118244171142578 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.878772908449173
Loss made of: CE 0.2546232044696808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.682192802429199 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.887948228418827
Loss made of: CE 0.2712629735469818, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.269808769226074 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.776113104820252
Loss made of: CE 0.28086623549461365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.479366302490234 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.779956682026386
Loss made of: CE 0.23936766386032104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251791000366211 EntMin 0.0
Epoch 5, Class Loss=0.26253965497016907, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.26253965497016907, Class Loss=0.26253965497016907, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/97, Loss=4.733911480009556
Loss made of: CE 0.3297349214553833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.274078369140625 EntMin 0.0
Epoch 6, Batch 20/97, Loss=5.097204089164734
Loss made of: CE 0.3597613275051117, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.665768623352051 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.755518005788327
Loss made of: CE 0.33501505851745605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.479650497436523 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.068026307225227
Loss made of: CE 0.3018953502178192, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.724967002868652 EntMin 0.0
Epoch 6, Batch 50/97, Loss=5.006693285703659
Loss made of: CE 0.2855829894542694, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.362427711486816 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.835848443210125
Loss made of: CE 0.2799559235572815, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.518509864807129 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.810831061005592
Loss made of: CE 0.39293012022972107, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.533742904663086 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.946875227987766
Loss made of: CE 0.29979532957077026, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5935378074646 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.739349928498268
Loss made of: CE 0.30272796750068665, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2420854568481445 EntMin 0.0
Epoch 6, Class Loss=0.30201706290245056, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.30201706290245056, Class Loss=0.30201706290245056, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.007439911365509
Loss made of: CE 0.9090577960014343, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.762630462646484 EntMin 0.0
Epoch 1, Class Loss=0.7493630647659302, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.7493630647659302, Class Loss=0.7493630647659302, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=5.8343867599964145
Loss made of: CE 0.5696794390678406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.648427963256836 EntMin 0.0
Epoch 2, Class Loss=0.6795660257339478, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.6795660257339478, Class Loss=0.6795660257339478, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=5.693658971786499
Loss made of: CE 0.5698539018630981, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.213657379150391 EntMin 0.0
Epoch 3, Class Loss=0.6429494023323059, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.6429494023323059, Class Loss=0.6429494023323059, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=5.681009119749069
Loss made of: CE 0.5719660520553589, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8717756271362305 EntMin 0.0
Epoch 4, Class Loss=0.6096872687339783, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.6096872687339783, Class Loss=0.6096872687339783, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.441804051399231
Loss made of: CE 0.6305397748947144, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.142671585083008 EntMin 0.0
Epoch 5, Class Loss=0.5559769868850708, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5559769868850708, Class Loss=0.5559769868850708, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=5.472318884730339
Loss made of: CE 0.6329072713851929, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.775043964385986 EntMin 0.0
Epoch 6, Class Loss=0.5443738698959351, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.5443738698959351, Class Loss=0.5443738698959351, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.2336625685915354
Loss made of: CE 0.032541800290346146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.749601364135742 EntMin 0.0
Epoch 1, Class Loss=0.05105677992105484, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.05105677992105484, Class Loss=0.05105677992105484, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.250355790555477
Loss made of: CE 0.17548862099647522, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.625855922698975 EntMin 0.0
Epoch 2, Class Loss=0.13781875371932983, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.13781875371932983, Class Loss=0.13781875371932983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.304850749671459
Loss made of: CE 0.193987637758255, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.412255764007568 EntMin 0.0
Epoch 3, Class Loss=0.24381756782531738, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.24381756782531738, Class Loss=0.24381756782531738, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=5.441790825128555
Loss made of: CE 0.40489062666893005, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.388888835906982 EntMin 0.0
Epoch 4, Class Loss=0.36250993609428406, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.36250993609428406, Class Loss=0.36250993609428406, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.505830454826355
Loss made of: CE 0.35118305683135986, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0357465744018555 EntMin 0.0
Epoch 5, Class Loss=0.4551975727081299, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.4551975727081299, Class Loss=0.4551975727081299, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.421089008450508
Loss made of: CE 0.5349732637405396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.844056129455566 EntMin 0.0
Epoch 6, Class Loss=0.5403293371200562, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.5403293371200562, Class Loss=0.5403293371200562, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.322373075783252
Loss made of: CE 0.0421939417719841, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.259885787963867 EntMin 0.0
Epoch 1, Class Loss=0.056214816868305206, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.056214816868305206, Class Loss=0.056214816868305206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.474968949705362
Loss made of: CE 0.1905926913022995, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.295195579528809 EntMin 0.0
Epoch 2, Class Loss=0.16828197240829468, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.16828197240829468, Class Loss=0.16828197240829468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.580453439056873
Loss made of: CE 0.3396860361099243, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.203968048095703 EntMin 0.0
Epoch 3, Class Loss=0.27351707220077515, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.27351707220077515, Class Loss=0.27351707220077515, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=5.417969235777855
Loss made of: CE 0.5592057704925537, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.10381555557251 EntMin 0.0
Epoch 4, Class Loss=0.37072861194610596, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.37072861194610596, Class Loss=0.37072861194610596, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.503889882564545
Loss made of: CE 0.38984283804893494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.919220924377441 EntMin 0.0
Epoch 5, Class Loss=0.47126421332359314, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.47126421332359314, Class Loss=0.47126421332359314, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.437678104639053
Loss made of: CE 0.5707451701164246, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.132227897644043 EntMin 0.0
Epoch 6, Class Loss=0.5416678190231323, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5416678190231323, Class Loss=0.5416678190231323, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6273183226585388, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.793738
Mean Acc: 0.460337
FreqW Acc: 0.652280
Mean IoU: 0.370744
Class IoU:
	class 0: 0.7997366
	class 1: 0.81642854
	class 2: 0.31909236
	class 3: 0.74123627
	class 4: 0.5206954
	class 5: 0.7194615
	class 6: 0.46495777
	class 7: 0.80186766
	class 8: 0.7236089
	class 9: 0.014041547
	class 10: 5.377716e-05
	class 11: 0.0
	class 12: 0.17005205
	class 13: 0.19773984
	class 14: 0.0
	class 15: 0.013668827
	class 16: 0.0
Class Acc:
	class 0: 0.9813913
	class 1: 0.8618141
	class 2: 0.9343795
	class 3: 0.7693064
	class 4: 0.5696074
	class 5: 0.8227344
	class 6: 0.46799067
	class 7: 0.85277516
	class 8: 0.77586824
	class 9: 0.014251916
	class 10: 5.3792246e-05
	class 11: 0.0
	class 12: 0.17434277
	class 13: 0.5875326
	class 14: 0.0
	class 15: 0.013672572
	class 16: 0.0

federated global round: 22, step: 4
select part of clients to conduct local training
[21, 0, 25, 23]
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=5.744111670833081
Loss made of: CE 0.06091141700744629, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.292397499084473 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.338548196852207
Loss made of: CE 0.03698479384183884, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.221045017242432 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.125435754843056
Loss made of: CE 0.029537733644247055, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.099981784820557 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.235724496375769
Loss made of: CE 0.04489349573850632, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.55443000793457 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.026163415145129
Loss made of: CE 0.025998225435614586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.539637088775635 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.015769704990089
Loss made of: CE 0.025550734251737595, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.587637901306152 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.928318864572793
Loss made of: CE 0.019555341452360153, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.619875431060791 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 80/97, Loss=4.796595730120316
Loss made of: CE 0.012429538182914257, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.717714309692383 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.863294830545783
Loss made of: CE 0.01663322001695633, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.406213760375977 EntMin 0.0
Epoch 1, Class Loss=0.03360338136553764, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.03360338136553764, Class Loss=0.03360338136553764, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.873746325820685
Loss made of: CE 0.1014925166964531, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.750853061676025 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.908290458843112
Loss made of: CE 0.04983357712626457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478268623352051 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.036282409727574
Loss made of: CE 0.09834122657775879, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.676046848297119 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.965319494158029
Loss made of: CE 0.07634949684143066, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.26729679107666 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.857368912547827
Loss made of: CE 0.07465849816799164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.422125339508057 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.889311593770981
Loss made of: CE 0.07630878686904907, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.349978446960449 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.828707914799452
Loss made of: CE 0.059802889823913574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5956950187683105 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.990309238061309
Loss made of: CE 0.10899931192398071, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.582749366760254 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.725191320106387
Loss made of: CE 0.07258066534996033, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.157096862792969 EntMin 0.0
Epoch 2, Class Loss=0.08867501467466354, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.08867501467466354, Class Loss=0.08867501467466354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.816558983176947
Loss made of: CE 0.17023861408233643, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.72445011138916 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.94153319299221
Loss made of: CE 0.1332172453403473, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.624543190002441 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.828944216668606
Loss made of: CE 0.14775168895721436, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.29644775390625 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.751934559643269
Loss made of: CE 0.08089759200811386, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6642632484436035 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.754130107909441
Loss made of: CE 0.16725045442581177, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3407182693481445 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.93475234284997
Loss made of: CE 0.1281176209449768, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6436991691589355 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.881083807349205
Loss made of: CE 0.09299099445343018, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.785614967346191 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.908816113322973
Loss made of: CE 0.15762928128242493, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.628283977508545 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.730525567382574
Loss made of: CE 0.1281600445508957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.207293510437012 EntMin 0.0
Epoch 3, Class Loss=0.1382768303155899, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.1382768303155899, Class Loss=0.1382768303155899, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.857597310841084
Loss made of: CE 0.2129509449005127, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.442449569702148 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.807889105379582
Loss made of: CE 0.1988317221403122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.205392837524414 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.9396069884300235
Loss made of: CE 0.24211782217025757, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.851426124572754 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.805505222082138
Loss made of: CE 0.30439895391464233, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.848409652709961 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.9099110648036
Loss made of: CE 0.16242089867591858, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.86957311630249 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.924282446503639
Loss made of: CE 0.2018115520477295, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1189141273498535 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.822456921637058
Loss made of: CE 0.15835265815258026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.436651229858398 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.764049373567104
Loss made of: CE 0.1596660017967224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.220659255981445 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.903559622168541
Loss made of: CE 0.1787729561328888, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.509409427642822 EntMin 0.0
Epoch 4, Class Loss=0.1903872936964035, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.1903872936964035, Class Loss=0.1903872936964035, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.8062648013234135
Loss made of: CE 0.19282673299312592, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.889171600341797 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.736608323454857
Loss made of: CE 0.23174431920051575, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.917473793029785 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.7653365239501
Loss made of: CE 0.28335580229759216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.48494291305542 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.926109592616558
Loss made of: CE 0.26662755012512207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.769183158874512 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.642116871476174
Loss made of: CE 0.23673616349697113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.490448951721191 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.927297081053257
Loss made of: CE 0.2183363139629364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.555851459503174 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.8014132380485535
Loss made of: CE 0.21830421686172485, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.46964168548584 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.83728405982256
Loss made of: CE 0.2378644049167633, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.020520210266113 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.912775050103664
Loss made of: CE 0.2590586543083191, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.736602306365967 EntMin 0.0
Epoch 5, Class Loss=0.23786881566047668, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.23786881566047668, Class Loss=0.23786881566047668, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.660810798406601
Loss made of: CE 0.3360535800457001, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.677211284637451 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.724467700719833
Loss made of: CE 0.3126751184463501, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.723759651184082 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.937869568169117
Loss made of: CE 0.2603759765625, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.103818893432617 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.860522173345089
Loss made of: CE 0.28802597522735596, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4145050048828125 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.684399092197419
Loss made of: CE 0.29929524660110474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.166755676269531 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.781915259361267
Loss made of: CE 0.22563275694847107, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.893198490142822 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.679166845977306
Loss made of: CE 0.25459009408950806, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9687955379486084 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.600634388625622
Loss made of: CE 0.2455720156431198, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.23112678527832 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.532316961884499
Loss made of: CE 0.29209721088409424, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.926342010498047 EntMin 0.0
Epoch 6, Class Loss=0.28890877962112427, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.28890877962112427, Class Loss=0.28890877962112427, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.558965858258307
Loss made of: CE 0.017223510891199112, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.40494441986084 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.0771853206679225
Loss made of: CE 0.04373769089579582, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.528000354766846 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.035870240069926
Loss made of: CE 0.027022678405046463, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.147907257080078 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.231814480759203
Loss made of: CE 0.05447223037481308, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.292838096618652 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.00545460851863
Loss made of: CE 0.06644973158836365, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.809284210205078 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.798029842972755
Loss made of: CE 0.012418758124113083, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.201231002807617 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.974494630191475
Loss made of: CE 0.06619828194379807, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.297200679779053 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.883745705895126
Loss made of: CE 0.05875062197446823, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.635858058929443 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.131344337761402
Loss made of: CE 0.0742272138595581, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1720428466796875 EntMin 0.0
Epoch 1, Class Loss=0.035223785787820816, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.035223785787820816, Class Loss=0.035223785787820816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=5.038955805450678
Loss made of: CE 0.0805017352104187, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.708808898925781 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.970669043436646
Loss made of: CE 0.09714355319738388, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.574410438537598 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.045466002076864
Loss made of: CE 0.05691719055175781, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.178381443023682 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.740197940915823
Loss made of: CE 0.07325951755046844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.601653099060059 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.93636252656579
Loss made of: CE 0.10886126756668091, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.489973068237305 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.916730924323201
Loss made of: CE 0.06124318763613701, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.030494689941406 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.024946735613048
Loss made of: CE 0.08633632957935333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.978565216064453 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.8980040770024065
Loss made of: CE 0.1218433827161789, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.066061496734619 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.893210608884692
Loss made of: CE 0.10685744881629944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.888795852661133 EntMin 0.0
Epoch 2, Class Loss=0.09123910218477249, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.09123910218477249, Class Loss=0.09123910218477249, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.770104003697634
Loss made of: CE 0.1656579077243805, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.300076484680176 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.73856258392334
Loss made of: CE 0.0965004563331604, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.457713603973389 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.984764638543129
Loss made of: CE 0.14123699069023132, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.079362869262695 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.034665850549937
Loss made of: CE 0.1612943559885025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.955036163330078 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.071816345304251
Loss made of: CE 0.1178550124168396, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3719635009765625 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.983557153493166
Loss made of: CE 0.14437898993492126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.800494194030762 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.0327308177948
Loss made of: CE 0.11312079429626465, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.223487377166748 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.956674294173718
Loss made of: CE 0.13811248540878296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.261017799377441 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.944808220118285
Loss made of: CE 0.13997678458690643, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.489011764526367 EntMin 0.0
Epoch 3, Class Loss=0.14082321524620056, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.14082321524620056, Class Loss=0.14082321524620056, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.762368154525757
Loss made of: CE 0.21484887599945068, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.342338562011719 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.8437179490923885
Loss made of: CE 0.21096597611904144, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.936522483825684 EntMin 0.0
Epoch 4, Batch 30/97, Loss=5.182156519591809
Loss made of: CE 0.18821899592876434, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.255770683288574 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.839558947086334
Loss made of: CE 0.1371781826019287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.040266990661621 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.7717513486742975
Loss made of: CE 0.16803622245788574, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.019939422607422 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.658295068889856
Loss made of: CE 0.15948541462421417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.507712364196777 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.903424867987633
Loss made of: CE 0.15199467539787292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.497023105621338 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.671771308779716
Loss made of: CE 0.16966509819030762, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.009405136108398 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.081154671311379
Loss made of: CE 0.16184210777282715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.489348411560059 EntMin 0.0
Epoch 4, Class Loss=0.18641690909862518, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.18641690909862518, Class Loss=0.18641690909862518, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.677234770357609
Loss made of: CE 0.2396378517150879, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.146810531616211 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.968344253301621
Loss made of: CE 0.25165680050849915, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.577946186065674 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.876332178711891
Loss made of: CE 0.23581966757774353, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.35127067565918 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.966925340890884
Loss made of: CE 0.2447746992111206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.842438697814941 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.827957087755204
Loss made of: CE 0.25980910658836365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.336384296417236 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.765824806690216
Loss made of: CE 0.2597357928752899, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.022284030914307 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.895017601549625
Loss made of: CE 0.2873643636703491, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.164155960083008 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.551144856214523
Loss made of: CE 0.2575194537639618, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.354158878326416 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.6233653903007506
Loss made of: CE 0.2242361158132553, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.182197570800781 EntMin 0.0
Epoch 5, Class Loss=0.24360793828964233, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.24360793828964233, Class Loss=0.24360793828964233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.692668354511261
Loss made of: CE 0.25401434302330017, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2527360916137695 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.807848784327507
Loss made of: CE 0.29333555698394775, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8897337913513184 EntMin 0.0
Epoch 6, Batch 30/97, Loss=5.038140079379081
Loss made of: CE 0.30550044775009155, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.191680908203125 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.017165267467499
Loss made of: CE 0.2818480134010315, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664246559143066 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.759538903832436
Loss made of: CE 0.30437248945236206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1501784324646 EntMin 0.0
Epoch 6, Batch 60/97, Loss=5.031315162777901
Loss made of: CE 0.3065806031227112, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.114126205444336 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.761332026124
Loss made of: CE 0.2946767807006836, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.443142890930176 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.785489159822464
Loss made of: CE 0.2776165008544922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.435694694519043 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.928887929022312
Loss made of: CE 0.22693130373954773, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.288270950317383 EntMin 0.0
Epoch 6, Class Loss=0.29636457562446594, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.29636457562446594, Class Loss=0.29636457562446594, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.492944420129061
Loss made of: CE 0.06206967681646347, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.35815954208374 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.326298695988953
Loss made of: CE 0.05592427775263786, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.150591850280762 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.126692999526858
Loss made of: CE 0.014798722229897976, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.331370830535889 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.024839352536946
Loss made of: CE 0.015596932731568813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.815509796142578 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.01978126745671
Loss made of: CE 0.03936251252889633, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.916568756103516 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.976340051181614
Loss made of: CE 0.033645205199718475, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.78026008605957 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.1270370682701465
Loss made of: CE 0.02812533639371395, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.372282028198242 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.9806937226094306
Loss made of: CE 0.02773137204349041, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0502214431762695 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.156850436981768
Loss made of: CE 0.02423075959086418, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.970179080963135 EntMin 0.0
Epoch 1, Class Loss=0.03429415822029114, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.03429415822029114, Class Loss=0.03429415822029114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.721365854144096
Loss made of: CE 0.08215323090553284, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.809525966644287 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.080343242362142
Loss made of: CE 0.044143080711364746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.454285144805908 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.928845962509513
Loss made of: CE 0.0766829401254654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.297735214233398 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.108464565128088
Loss made of: CE 0.09830734133720398, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.280884742736816 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.970166020840407
Loss made of: CE 0.06099804490804672, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.587604999542236 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.161040744930506
Loss made of: CE 0.0778317004442215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.916477203369141 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.877235621213913
Loss made of: CE 0.08212416619062424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.892246723175049 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.772946154326201
Loss made of: CE 0.08018676936626434, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.415121078491211 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.840203796327114
Loss made of: CE 0.05763045698404312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6024980545043945 EntMin 0.0
Epoch 2, Class Loss=0.09181161969900131, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.09181161969900131, Class Loss=0.09181161969900131, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.771430725604295
Loss made of: CE 0.18436110019683838, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.544994354248047 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.889042669534684
Loss made of: CE 0.10597153753042221, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5145063400268555 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.794362933188677
Loss made of: CE 0.1159454807639122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303124904632568 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.0053529918193815
Loss made of: CE 0.11957564949989319, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.394197463989258 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.126694654673338
Loss made of: CE 0.055870987474918365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.620010852813721 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.986653165519238
Loss made of: CE 0.13493236899375916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.714308261871338 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.838724160939455
Loss made of: CE 0.21662530303001404, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.639381408691406 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.803505057096482
Loss made of: CE 0.16039437055587769, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.120327949523926 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.841462154686451
Loss made of: CE 0.12141045182943344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.720871925354004 EntMin 0.0
Epoch 3, Class Loss=0.1403355449438095, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.1403355449438095, Class Loss=0.1403355449438095, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.875454659759998
Loss made of: CE 0.19937020540237427, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.252882480621338 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.85548160225153
Loss made of: CE 0.206979900598526, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478945732116699 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.665724231302738
Loss made of: CE 0.2022877186536789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.452445983886719 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.822322379052639
Loss made of: CE 0.2292642593383789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.672144889831543 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.042024891078472
Loss made of: CE 0.2810986042022705, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6263275146484375 EntMin 0.0
Epoch 4, Batch 60/97, Loss=5.251173827052116
Loss made of: CE 0.24557916820049286, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5993194580078125 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.783252288401127
Loss made of: CE 0.1880761831998825, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.220732688903809 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.721474680304527
Loss made of: CE 0.15651071071624756, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.110799789428711 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.765093594789505
Loss made of: CE 0.21207861602306366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.510348796844482 EntMin 0.0
Epoch 4, Class Loss=0.1890503168106079, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.1890503168106079, Class Loss=0.1890503168106079, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.6286631122231485
Loss made of: CE 0.26195991039276123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.399959564208984 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.512806458771228
Loss made of: CE 0.21799686551094055, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291898727416992 EntMin 0.0
Epoch 5, Batch 30/97, Loss=5.117085728049278
Loss made of: CE 0.3017324209213257, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.814641952514648 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.136400172114373
Loss made of: CE 0.2091446965932846, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.913930416107178 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.892988543212414
Loss made of: CE 0.23671536147594452, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.687965393066406 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.936539950966835
Loss made of: CE 0.242892786860466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.377492904663086 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.8788701966404915
Loss made of: CE 0.2035735845565796, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.241924285888672 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.683768489956856
Loss made of: CE 0.22620698809623718, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.480052947998047 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.827740448713302
Loss made of: CE 0.2562280297279358, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6969218254089355 EntMin 0.0
Epoch 5, Class Loss=0.24806837737560272, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.24806837737560272, Class Loss=0.24806837737560272, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.648259243369102
Loss made of: CE 0.26159006357192993, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.44044828414917 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.7890733003616335
Loss made of: CE 0.2931896150112152, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.162403106689453 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.799825623631477
Loss made of: CE 0.25177356600761414, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.012206554412842 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.643445694446564
Loss made of: CE 0.27875208854675293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.368797302246094 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.721533827483654
Loss made of: CE 0.32647842168807983, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6119608879089355 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.889096060395241
Loss made of: CE 0.2654927670955658, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.804893970489502 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.784441860020161
Loss made of: CE 0.29599273204803467, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.373868942260742 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.78913489729166
Loss made of: CE 0.23722951114177704, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.213070869445801 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.493610750138759
Loss made of: CE 0.22238150238990784, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.072505950927734 EntMin 0.0
Epoch 6, Class Loss=0.2889261543750763, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.2889261543750763, Class Loss=0.2889261543750763, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.98231095969677
Loss made of: CE 0.021808600053191185, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.465476989746094 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.91259549409151
Loss made of: CE 0.02645268663764, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.514845848083496 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.8227583661675455
Loss made of: CE 0.0437127947807312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.741086483001709 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.109698855131865
Loss made of: CE 0.0249625351279974, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.740159511566162 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.965175046212972
Loss made of: CE 0.037677839398384094, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.07260799407959 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.750197093375027
Loss made of: CE 0.043212443590164185, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.075637340545654 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.862579955533147
Loss made of: CE 0.027436912059783936, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.112611770629883 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.928181398287416
Loss made of: CE 0.04107856750488281, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2418951988220215 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.095677799638361
Loss made of: CE 0.025995220988988876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.284147262573242 EntMin 0.0
Epoch 1, Class Loss=0.03620016202330589, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.03620016202330589, Class Loss=0.03620016202330589, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.876933551579714
Loss made of: CE 0.03942999243736267, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.205472469329834 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.925663588568568
Loss made of: CE 0.10056865215301514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.986696243286133 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.777279629558325
Loss made of: CE 0.10580253601074219, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.58082389831543 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.900673758238554
Loss made of: CE 0.04006385803222656, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.249047756195068 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.0229751102626325
Loss made of: CE 0.08116461336612701, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.558587074279785 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.166359484568238
Loss made of: CE 0.08733119815587997, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.876977443695068 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.892081549763679
Loss made of: CE 0.058443330228328705, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.836915969848633 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.930364920943975
Loss made of: CE 0.11783093214035034, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.92347526550293 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.996999605000019
Loss made of: CE 0.11900989711284637, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.349608421325684 EntMin 0.0
Epoch 2, Class Loss=0.08910638839006424, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.08910638839006424, Class Loss=0.08910638839006424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.96589110121131
Loss made of: CE 0.12627674639225006, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.028269290924072 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.9718451902270315
Loss made of: CE 0.09422366321086884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.843188285827637 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.972144970297814
Loss made of: CE 0.18042120337486267, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.399186611175537 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.899651952832937
Loss made of: CE 0.21094010770320892, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.796833038330078 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.9741505108773705
Loss made of: CE 0.08396809548139572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.782881736755371 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.772825007140637
Loss made of: CE 0.0960371345281601, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400198459625244 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.7085780568420885
Loss made of: CE 0.08125899732112885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.538273811340332 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.63761263936758
Loss made of: CE 0.15348279476165771, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.969371795654297 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.978193226456642
Loss made of: CE 0.17973539233207703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.70100736618042 EntMin 0.0
Epoch 3, Class Loss=0.1392047107219696, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.1392047107219696, Class Loss=0.1392047107219696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.804081088304519
Loss made of: CE 0.20119154453277588, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6887593269348145 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.8785307362675665
Loss made of: CE 0.19809076189994812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.316352367401123 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.74690634906292
Loss made of: CE 0.21062755584716797, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.725739479064941 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.951859296858311
Loss made of: CE 0.1911957561969757, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.230945587158203 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.944436457753182
Loss made of: CE 0.20500515401363373, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.803315162658691 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.68509469255805
Loss made of: CE 0.20580272376537323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.58985710144043 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.922537702322006
Loss made of: CE 0.20979398488998413, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.376299858093262 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.1135998398065565
Loss made of: CE 0.1437770128250122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.326805114746094 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.6890023544430735
Loss made of: CE 0.17704138159751892, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.558454513549805 EntMin 0.0
Epoch 4, Class Loss=0.18638820946216583, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.18638820946216583, Class Loss=0.18638820946216583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.96048384308815
Loss made of: CE 0.2285589575767517, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.508676052093506 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.886667060852051
Loss made of: CE 0.3309038579463959, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.651306629180908 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.933502067625523
Loss made of: CE 0.24949675798416138, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581186294555664 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.875495454668998
Loss made of: CE 0.23888719081878662, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.904755115509033 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.629976479709148
Loss made of: CE 0.2218930423259735, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.501216888427734 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.5635260254144665
Loss made of: CE 0.20365063846111298, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.913219928741455 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.682820917665959
Loss made of: CE 0.2535933256149292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.229442596435547 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.660732991993427
Loss made of: CE 0.21349142491817474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.263055801391602 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.8860971361398695
Loss made of: CE 0.26342207193374634, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.288727760314941 EntMin 0.0
Epoch 5, Class Loss=0.25174811482429504, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.25174811482429504, Class Loss=0.25174811482429504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.738839277625084
Loss made of: CE 0.2832474410533905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.182797908782959 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.782803675532341
Loss made of: CE 0.2891266345977783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.268727779388428 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.761829495429993
Loss made of: CE 0.331835001707077, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.48061466217041 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.600679194927215
Loss made of: CE 0.26068565249443054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.019886493682861 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.84441215544939
Loss made of: CE 0.2368960827589035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251262664794922 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.816077308356762
Loss made of: CE 0.27203255891799927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.576226711273193 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.885153064131737
Loss made of: CE 0.28395798802375793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.096973419189453 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.784355215728283
Loss made of: CE 0.3108348846435547, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.03450345993042 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.736088773608207
Loss made of: CE 0.30427104234695435, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.32075309753418 EntMin 0.0
Epoch 6, Class Loss=0.2925601303577423, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.2925601303577423, Class Loss=0.2925601303577423, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5518155097961426, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.834927
Mean Acc: 0.489844
FreqW Acc: 0.722682
Mean IoU: 0.396777
Class IoU:
	class 0: 0.85202324
	class 1: 0.82113165
	class 2: 0.30964047
	class 3: 0.7484269
	class 4: 0.49200383
	class 5: 0.72516793
	class 6: 0.41926306
	class 7: 0.8166911
	class 8: 0.754469
	class 9: 0.00051937695
	class 10: 0.0
	class 11: 0.0
	class 12: 0.043461967
	class 13: 0.21621436
	class 14: 0.0
	class 15: 0.5461986
	class 16: 0.0
Class Acc:
	class 0: 0.9695813
	class 1: 0.85621655
	class 2: 0.9324372
	class 3: 0.7722122
	class 4: 0.5333599
	class 5: 0.80177283
	class 6: 0.4204304
	class 7: 0.8608086
	class 8: 0.8417752
	class 9: 0.000520753
	class 10: 0.0
	class 11: 0.0
	class 12: 0.043726135
	class 13: 0.50415945
	class 14: 0.0
	class 15: 0.7903413
	class 16: 0.0

federated global round: 23, step: 4
select part of clients to conduct local training
[17, 3, 5, 22]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.247119173174724
Loss made of: CE 0.00672081857919693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.348026275634766 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.277269016765058
Loss made of: CE 0.013448836281895638, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.739509344100952 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.461856573540717
Loss made of: CE 0.010533577762544155, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.193868637084961 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.482737337006256
Loss made of: CE 0.01964506506919861, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.855295181274414 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 50/97, Loss=4.465022275596857
Loss made of: CE 0.013001618906855583, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8933773040771484 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.416766716912389
Loss made of: CE 0.026328593492507935, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.220076084136963 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.248310785694048
Loss made of: CE 0.010080637410283089, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.262714385986328 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.550727079017088
Loss made of: CE 0.01739080622792244, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.425207138061523 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.548182002082467
Loss made of: CE 0.015008606016635895, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.466987609863281 EntMin 0.0
Epoch 1, Class Loss=0.014408913441002369, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.014408913441002369, Class Loss=0.014408913441002369, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.4551180429756645
Loss made of: CE 0.04565136879682541, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.15131950378418 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.305801435932517
Loss made of: CE 0.05262961983680725, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.411365985870361 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.740725088492036
Loss made of: CE 0.029280539602041245, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.254911422729492 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.53013365957886
Loss made of: CE 0.02547096461057663, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5213117599487305 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.420916314981878
Loss made of: CE 0.053494349122047424, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.57732629776001 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.542224527336657
Loss made of: CE 0.05216656252741814, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.62992000579834 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.685992342978716
Loss made of: CE 0.03215382248163223, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594420909881592 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.555828453041613
Loss made of: CE 0.03015281818807125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.233356952667236 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.326882918551564
Loss made of: CE 0.041261594742536545, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.050396919250488 EntMin 0.0
Epoch 2, Class Loss=0.045969799160957336, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.045969799160957336, Class Loss=0.045969799160957336, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.565219634026289
Loss made of: CE 0.07279922068119049, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.514993667602539 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.612528131902218
Loss made of: CE 0.08179011195898056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.986105442047119 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.329799397289753
Loss made of: CE 0.10481582581996918, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.384171962738037 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.630285294353962
Loss made of: CE 0.11063158512115479, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.415288925170898 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.310466051101685
Loss made of: CE 0.11684447526931763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.226555347442627 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.672359384596348
Loss made of: CE 0.13423286378383636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.739378929138184 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.595362313464284
Loss made of: CE 0.09172990173101425, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.543992042541504 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.447143127024174
Loss made of: CE 0.06477463245391846, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.102993011474609 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.512647123634816
Loss made of: CE 0.09876863658428192, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.086369514465332 EntMin 0.0
Epoch 3, Class Loss=0.09472420811653137, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.09472420811653137, Class Loss=0.09472420811653137, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.449798612296581
Loss made of: CE 0.12765584886074066, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.228070259094238 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.812107612192631
Loss made of: CE 0.19857829809188843, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.852785587310791 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.57224278897047
Loss made of: CE 0.09599386155605316, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9673261642456055 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.450377049297094
Loss made of: CE 0.13094115257263184, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0130133628845215 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.6086011856794356
Loss made of: CE 0.1647854745388031, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.315985202789307 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.611083831638098
Loss made of: CE 0.14204847812652588, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.223286151885986 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.407001873850822
Loss made of: CE 0.1411954164505005, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.258432388305664 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.501933061331511
Loss made of: CE 0.13296101987361908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.209272384643555 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.225482349097729
Loss made of: CE 0.10226856172084808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.077303409576416 EntMin 0.0
Epoch 4, Class Loss=0.1484624743461609, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.1484624743461609, Class Loss=0.1484624743461609, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.394091230630875
Loss made of: CE 0.2403033971786499, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726481914520264 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.644711211323738
Loss made of: CE 0.21852700412273407, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.607715606689453 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.514197491109371
Loss made of: CE 0.17124776542186737, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.274356365203857 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.611900694668293
Loss made of: CE 0.2094835638999939, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304562568664551 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.714605471491813
Loss made of: CE 0.15193790197372437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.928585052490234 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.524966949224472
Loss made of: CE 0.15706497430801392, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358163833618164 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.5723747834563255
Loss made of: CE 0.18248097598552704, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.525709629058838 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.543382041156292
Loss made of: CE 0.21783658862113953, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.40617036819458 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.597596849501133
Loss made of: CE 0.2141280472278595, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.334519386291504 EntMin 0.0
Epoch 5, Class Loss=0.21281889081001282, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.21281889081001282, Class Loss=0.21281889081001282, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.692974936962128
Loss made of: CE 0.2805357873439789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.63897705078125 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.4742290586233135
Loss made of: CE 0.30865100026130676, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9292876720428467 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.526660378277302
Loss made of: CE 0.26629602909088135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4097747802734375 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.4441147819161415
Loss made of: CE 0.22494766116142273, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.957620620727539 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.632939773797989
Loss made of: CE 0.2212074249982834, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.418514251708984 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.589020669460297
Loss made of: CE 0.23115074634552002, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.11972188949585 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.624261531233787
Loss made of: CE 0.30030786991119385, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7903056144714355 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.567914931476116
Loss made of: CE 0.24789848923683167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.034661769866943 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.806167204678059
Loss made of: CE 0.3146635890007019, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5037689208984375 EntMin 0.0
Epoch 6, Class Loss=0.2741246223449707, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.2741246223449707, Class Loss=0.2741246223449707, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.300236415117979
Loss made of: CE 0.03550565987825394, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.542088508605957 EntMin 0.0
Epoch 1, Class Loss=0.0903443843126297, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.0903443843126297, Class Loss=0.0903443843126297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=5.665186204016209
Loss made of: CE 0.174444317817688, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.352630615234375 EntMin 0.0
Epoch 2, Class Loss=0.24130699038505554, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.24130699038505554, Class Loss=0.24130699038505554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.470604979246855
Loss made of: CE 0.0917440876364708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.669259071350098 EntMin 0.0
Epoch 3, Class Loss=0.4181693494319916, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.4181693494319916, Class Loss=0.4181693494319916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.588640546798706
Loss made of: CE 0.22186657786369324, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.139988899230957 EntMin 0.0
Epoch 4, Class Loss=0.5052865743637085, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.5052865743637085, Class Loss=0.5052865743637085, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.406062680482864
Loss made of: CE 0.6055353879928589, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.666481971740723 EntMin 0.0
Epoch 5, Class Loss=0.6058418154716492, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.6058418154716492, Class Loss=0.6058418154716492, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.564731734991073
Loss made of: CE 0.41983819007873535, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.73519229888916 EntMin 0.0
Epoch 6, Class Loss=0.6465132832527161, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.6465132832527161, Class Loss=0.6465132832527161, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.11501712591853
Loss made of: CE 0.01060868613421917, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9238927364349365 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.359616378881038
Loss made of: CE 0.018591534346342087, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.433058738708496 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 30/97, Loss=4.326526285591536
Loss made of: CE 0.012496589682996273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.480702877044678 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.38226808658801
Loss made of: CE 0.005292957182973623, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.232909202575684 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.475940692937002
Loss made of: CE 0.005553105846047401, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.514009952545166 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.384518629033119
Loss made of: CE 0.009574484080076218, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.030893325805664 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.516097038891166
Loss made of: CE 0.008911795914173126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0288777351379395 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.675979866902344
Loss made of: CE 0.00793650932610035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.327256202697754 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.517462212592363
Loss made of: CE 0.013353001326322556, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.698086261749268 EntMin 0.0
Epoch 1, Class Loss=0.013780494220554829, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.013780494220554829, Class Loss=0.013780494220554829, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.4712481193244455
Loss made of: CE 0.04500827193260193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.653725624084473 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.528337417170405
Loss made of: CE 0.045446403324604034, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.022987365722656 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.394728237204253
Loss made of: CE 0.06326952576637268, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.786561012268066 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.351830164156854
Loss made of: CE 0.0512867271900177, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.265866279602051 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.526178809069097
Loss made of: CE 0.05998536944389343, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.896440505981445 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.536363443732261
Loss made of: CE 0.05499383807182312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1953043937683105 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.465578887239099
Loss made of: CE 0.05593043565750122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.78308629989624 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.549821753241122
Loss made of: CE 0.052374109625816345, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4586181640625 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.672366107814014
Loss made of: CE 0.043306395411491394, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.006650924682617 EntMin 0.0
Epoch 2, Class Loss=0.04414631426334381, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.04414631426334381, Class Loss=0.04414631426334381, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.621588348597288
Loss made of: CE 0.09592076390981674, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304833889007568 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.611749739944935
Loss made of: CE 0.0713592916727066, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.634927272796631 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.454757609590888
Loss made of: CE 0.1000811979174614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.013472080230713 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.648847246170044
Loss made of: CE 0.0813012346625328, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.143552303314209 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.582728049904108
Loss made of: CE 0.09967728704214096, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.392293930053711 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.520712193846703
Loss made of: CE 0.11363311856985092, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.359422206878662 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.4371333315968515
Loss made of: CE 0.08285605162382126, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.047519683837891 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.45388329513371
Loss made of: CE 0.08886963129043579, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.176264762878418 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.446941672265529
Loss made of: CE 0.10374544560909271, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.555985927581787 EntMin 0.0
Epoch 3, Class Loss=0.09241526573896408, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.09241526573896408, Class Loss=0.09241526573896408, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.606667126715183
Loss made of: CE 0.1562362015247345, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.545063018798828 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.421578023582697
Loss made of: CE 0.11522181332111359, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.186346054077148 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.483380492031574
Loss made of: CE 0.13079451024532318, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.235513687133789 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.379349092394113
Loss made of: CE 0.20918461680412292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.049753189086914 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.301659011840821
Loss made of: CE 0.1444566696882248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.147211074829102 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.48085072785616
Loss made of: CE 0.14974388480186462, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.606884479522705 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.483469844609499
Loss made of: CE 0.1629105508327484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.522380828857422 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.250749055296183
Loss made of: CE 0.11476662009954453, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7482457160949707 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.401194547861815
Loss made of: CE 0.15157285332679749, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.953963041305542 EntMin 0.0
Epoch 4, Class Loss=0.14288565516471863, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.14288565516471863, Class Loss=0.14288565516471863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.60205412954092
Loss made of: CE 0.2251981943845749, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.377352714538574 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.586092555522919
Loss made of: CE 0.1879175305366516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1842522621154785 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.514466422796249
Loss made of: CE 0.3006209135055542, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3783063888549805 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.293131910264492
Loss made of: CE 0.2562546730041504, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.35671329498291 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.467102347314357
Loss made of: CE 0.25466060638427734, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9401016235351562 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.722456935048103
Loss made of: CE 0.2664124369621277, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.376804351806641 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.5854312479496
Loss made of: CE 0.2037145346403122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.787055969238281 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.643977428972721
Loss made of: CE 0.17941462993621826, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9483108520507812 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.567621041834355
Loss made of: CE 0.20593297481536865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.170843124389648 EntMin 0.0
Epoch 5, Class Loss=0.2192753404378891, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.2192753404378891, Class Loss=0.2192753404378891, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.529060798883438
Loss made of: CE 0.24785500764846802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.173791885375977 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.367275728285312
Loss made of: CE 0.2374430000782013, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7751693725585938 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.3622112110257145
Loss made of: CE 0.2991432547569275, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.035549163818359 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.746348397433758
Loss made of: CE 0.3032045364379883, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.043067455291748 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.409243272244931
Loss made of: CE 0.2691981792449951, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.614991664886475 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.503824460506439
Loss made of: CE 0.2614997625350952, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.770959377288818 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.318089361488819
Loss made of: CE 0.22486834228038788, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.272669792175293 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.4288205206394196
Loss made of: CE 0.2943478226661682, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.33598518371582 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.511685356497765
Loss made of: CE 0.23505067825317383, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.016763210296631 EntMin 0.0
Epoch 6, Class Loss=0.27049389481544495, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.27049389481544495, Class Loss=0.27049389481544495, Reg Loss=0.0
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.3024029321968555
Loss made of: CE 0.05955830216407776, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.844235420227051 EntMin 0.0
Epoch 1, Class Loss=0.10066437721252441, Reg Loss=0.0
Clinet index 22, End of Epoch 1/6, Average Loss=0.10066437721252441, Class Loss=0.10066437721252441, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=5.56476868391037
Loss made of: CE 0.21857216954231262, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.891406059265137 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.23753130435943604, Reg Loss=0.0
Clinet index 22, End of Epoch 2/6, Average Loss=0.23753130435943604, Class Loss=0.23753130435943604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.3833972528576854
Loss made of: CE 0.7617520093917847, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.109661102294922 EntMin 0.0
Epoch 3, Class Loss=0.4121062755584717, Reg Loss=0.0
Clinet index 22, End of Epoch 3/6, Average Loss=0.4121062755584717, Class Loss=0.4121062755584717, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.601915192604065
Loss made of: CE 0.5173565745353699, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.074067115783691 EntMin 0.0
Epoch 4, Class Loss=0.5440093278884888, Reg Loss=0.0
Clinet index 22, End of Epoch 4/6, Average Loss=0.5440093278884888, Class Loss=0.5440093278884888, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.408955550193786
Loss made of: CE 0.4262429177761078, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.375655651092529 EntMin 0.0
Epoch 5, Class Loss=0.5840865969657898, Reg Loss=0.0
Clinet index 22, End of Epoch 5/6, Average Loss=0.5840865969657898, Class Loss=0.5840865969657898, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.3360832840204235
Loss made of: CE 0.503976583480835, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.567846298217773 EntMin 0.0
Epoch 6, Class Loss=0.6433295607566833, Reg Loss=0.0
Clinet index 22, End of Epoch 6/6, Average Loss=0.6433295607566833, Class Loss=0.6433295607566833, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5491508841514587, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.838980
Mean Acc: 0.512448
FreqW Acc: 0.733913
Mean IoU: 0.408502
Class IoU:
	class 0: 0.86225736
	class 1: 0.8405708
	class 2: 0.31832153
	class 3: 0.77800596
	class 4: 0.4973138
	class 5: 0.73498666
	class 6: 0.51564485
	class 7: 0.812061
	class 8: 0.7488466
	class 9: 0.0023306333
	class 10: 0.0
	class 11: 0.0
	class 12: 0.05943971
	class 13: 0.21889631
	class 14: 0.0
	class 15: 0.55586547
	class 16: 0.0
Class Acc:
	class 0: 0.9625202
	class 1: 0.8840845
	class 2: 0.9408315
	class 3: 0.8137362
	class 4: 0.53886366
	class 5: 0.82593226
	class 6: 0.518074
	class 7: 0.87693536
	class 8: 0.8576537
	class 9: 0.0023427454
	class 10: 0.0
	class 11: 0.0
	class 12: 0.05973206
	class 13: 0.6025883
	class 14: 0.0
	class 15: 0.82832384
	class 16: 0.0

federated global round: 24, step: 4
select part of clients to conduct local training
[16, 9, 12, 2]
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=4.591891155624762
Loss made of: CE 0.01483236812055111, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.931246757507324 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.2611041981726885
Loss made of: CE 0.011470334604382515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.342052459716797 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.262640369497239
Loss made of: CE 0.014765783213078976, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.954740285873413 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.299019377818331
Loss made of: CE 0.013745605945587158, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.496523857116699 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.243867618776858
Loss made of: CE 0.007868902757763863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.725760459899902 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.624194301012904
Loss made of: CE 0.02540961280465126, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.60718297958374 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.433619570825249
Loss made of: CE 0.013234572485089302, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6901774406433105 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.424619729584083
Loss made of: CE 0.03577391430735588, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.968069076538086 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.313998073805124
Loss made of: CE 0.014325512573122978, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.648382186889648 EntMin 0.0
Epoch 1, Class Loss=0.016639433801174164, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.016639433801174164, Class Loss=0.016639433801174164, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/97, Loss=4.509633040055633
Loss made of: CE 0.03262552246451378, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.559753894805908 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.350244901515543
Loss made of: CE 0.03750063478946686, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.288439750671387 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.50226115770638
Loss made of: CE 0.056154944002628326, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.838233947753906 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.595960761606693
Loss made of: CE 0.048303067684173584, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6371002197265625 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.303674050606787
Loss made of: CE 0.03470974415540695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.430014133453369 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.320405363664031
Loss made of: CE 0.04440219700336456, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.530895233154297 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.318223163485527
Loss made of: CE 0.061807990074157715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.504570484161377 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.386810202524066
Loss made of: CE 0.020979121327400208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.131167411804199 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.436275357566774
Loss made of: CE 0.0517628937959671, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.883118629455566 EntMin 0.0
Epoch 2, Class Loss=0.04696524888277054, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.04696524888277054, Class Loss=0.04696524888277054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/97, Loss=4.102715527638793
Loss made of: CE 0.08230483531951904, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8376643657684326 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.287228690087796
Loss made of: CE 0.07379736006259918, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2714128494262695 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.662840279191732
Loss made of: CE 0.10089914500713348, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.948558807373047 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.663443609327078
Loss made of: CE 0.07901740074157715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.165710926055908 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.3311377607285975
Loss made of: CE 0.07264120876789093, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590444564819336 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.338133698701858
Loss made of: CE 0.07927408069372177, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.121634483337402 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.546510398387909
Loss made of: CE 0.10000781714916229, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.208590507507324 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.4910936683416365
Loss made of: CE 0.07287074625492096, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.608261585235596 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.147383482754231
Loss made of: CE 0.0891692191362381, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.983241558074951 EntMin 0.0
Epoch 3, Class Loss=0.09606233984231949, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.09606233984231949, Class Loss=0.09606233984231949, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/97, Loss=4.323637226223946
Loss made of: CE 0.2172311544418335, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.630241394042969 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.2249536596238615
Loss made of: CE 0.08847721666097641, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.975640058517456 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.3387242525815966
Loss made of: CE 0.1267324984073639, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.652966499328613 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.330989277362823
Loss made of: CE 0.18787531554698944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684626579284668 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.351828578114509
Loss made of: CE 0.10202556848526001, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.919663190841675 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.308686508238315
Loss made of: CE 0.128336101770401, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.061968803405762 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.355044461786747
Loss made of: CE 0.1137392520904541, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.285330772399902 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.274444027245044
Loss made of: CE 0.18203477561473846, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.283507823944092 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.1743524968624115
Loss made of: CE 0.10156817734241486, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.260561943054199 EntMin 0.0
Epoch 4, Class Loss=0.1485029011964798, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.1485029011964798, Class Loss=0.1485029011964798, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/97, Loss=4.273501564562321
Loss made of: CE 0.256852924823761, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.426436424255371 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.400686205923558
Loss made of: CE 0.21781036257743835, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.741118431091309 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.281640972197056
Loss made of: CE 0.24928510189056396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.282802581787109 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.402916058897972
Loss made of: CE 0.1704082489013672, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.741577625274658 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.332577511668205
Loss made of: CE 0.24184012413024902, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.077581405639648 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.442444293200969
Loss made of: CE 0.2439437359571457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.270455837249756 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.334677101671696
Loss made of: CE 0.15737572312355042, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9294631481170654 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.255291891098023
Loss made of: CE 0.1517762392759323, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128441333770752 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.459954711794853
Loss made of: CE 0.29529306292533875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.661036968231201 EntMin 0.0
Epoch 5, Class Loss=0.21409763395786285, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.21409763395786285, Class Loss=0.21409763395786285, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/97, Loss=4.393845540285111
Loss made of: CE 0.2756417691707611, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.694489002227783 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.358422209322453
Loss made of: CE 0.34722059965133667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4532670974731445 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.189907911419868
Loss made of: CE 0.24280604720115662, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.465867042541504 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.197481645643711
Loss made of: CE 0.33742618560791016, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.300497531890869 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.145482465624809
Loss made of: CE 0.20073512196540833, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.443920135498047 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.2616758108139035
Loss made of: CE 0.23394837975502014, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.914210081100464 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.239379553496837
Loss made of: CE 0.29692542552948, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0314836502075195 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.26053295135498
Loss made of: CE 0.2392255961894989, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.801800489425659 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.571979585289955
Loss made of: CE 0.2896523177623749, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.042340278625488 EntMin 0.0
Epoch 6, Class Loss=0.2872128486633301, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.2872128486633301, Class Loss=0.2872128486633301, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.321213590539992
Loss made of: CE 0.11873362958431244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.428487300872803 EntMin 0.0
Epoch 1, Class Loss=0.07520003616809845, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.07520003616809845, Class Loss=0.07520003616809845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.11520691215992
Loss made of: CE 0.2538052499294281, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.070330619812012 EntMin 0.0
Epoch 2, Class Loss=0.19634196162223816, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.19634196162223816, Class Loss=0.19634196162223816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.038333162665367
Loss made of: CE 0.1831093281507492, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.782717227935791 EntMin 0.0
Epoch 3, Class Loss=0.34565269947052, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.34565269947052, Class Loss=0.34565269947052, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=5.30790767967701
Loss made of: CE 0.3383966088294983, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.606409072875977 EntMin 0.0
Epoch 4, Class Loss=0.4739600121974945, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.4739600121974945, Class Loss=0.4739600121974945, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 10/12, Loss=5.192153719067574
Loss made of: CE 0.38121387362480164, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581425666809082 EntMin 0.0
Epoch 5, Class Loss=0.5178087949752808, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5178087949752808, Class Loss=0.5178087949752808, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=5.193013381958008
Loss made of: CE 0.38454508781433105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.913824081420898 EntMin 0.0
Epoch 6, Class Loss=0.6273168325424194, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.6273168325424194, Class Loss=0.6273168325424194, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.120426790416241
Loss made of: CE 0.033290889114141464, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.118639945983887 EntMin 0.0
Epoch 1, Class Loss=0.06641095131635666, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.06641095131635666, Class Loss=0.06641095131635666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.012461938709021
Loss made of: CE 0.17817524075508118, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.871241569519043 EntMin 0.0
Epoch 2, Class Loss=0.18025246262550354, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.18025246262550354, Class Loss=0.18025246262550354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.021516823768616
Loss made of: CE 0.18635308742523193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590308666229248 EntMin 0.0
Epoch 3, Class Loss=0.31869834661483765, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.31869834661483765, Class Loss=0.31869834661483765, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.974657201766968
Loss made of: CE 0.523937463760376, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.456209182739258 EntMin 0.0
Epoch 4, Class Loss=0.4283265471458435, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.4283265471458435, Class Loss=0.4283265471458435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=5.113736289739609
Loss made of: CE 0.4743651747703552, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.337392807006836 EntMin 0.0
Epoch 5, Class Loss=0.5232060551643372, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5232060551643372, Class Loss=0.5232060551643372, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=5.024897024035454
Loss made of: CE 0.5207058191299438, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6782684326171875 EntMin 0.0
Epoch 6, Class Loss=0.6155691742897034, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.6155691742897034, Class Loss=0.6155691742897034, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.010891658067703
Loss made of: CE 1.0224088430404663, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.598154067993164 EntMin 0.0
Epoch 1, Class Loss=0.8483807444572449, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.8483807444572449, Class Loss=0.8483807444572449, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/12, Loss=5.550389748811722
Loss made of: CE 0.5286359190940857, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.438474655151367 EntMin 0.0
Epoch 2, Class Loss=0.7477681636810303, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.7477681636810303, Class Loss=0.7477681636810303, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/12, Loss=5.437689024209976
Loss made of: CE 0.6942253112792969, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.913234233856201 EntMin 0.0
Epoch 3, Class Loss=0.7315516471862793, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.7315516471862793, Class Loss=0.7315516471862793, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/12, Loss=5.416009169816971
Loss made of: CE 0.7393090128898621, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734875679016113 EntMin 0.0
Epoch 4, Class Loss=0.674672544002533, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.674672544002533, Class Loss=0.674672544002533, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/12, Loss=5.2139999091625215
Loss made of: CE 0.7824004888534546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.948723793029785 EntMin 0.0
Epoch 5, Class Loss=0.6597614288330078, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.6597614288330078, Class Loss=0.6597614288330078, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/12, Loss=5.30557279586792
Loss made of: CE 0.674872100353241, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.490588188171387 EntMin 0.0
Epoch 6, Class Loss=0.6275857090950012, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.6275857090950012, Class Loss=0.6275857090950012, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.5549132227897644, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.838092
Mean Acc: 0.515243
FreqW Acc: 0.734675
Mean IoU: 0.410269
Class IoU:
	class 0: 0.8628908
	class 1: 0.8386372
	class 2: 0.32627988
	class 3: 0.78222084
	class 4: 0.50761765
	class 5: 0.7179716
	class 6: 0.5313494
	class 7: 0.8105337
	class 8: 0.7579372
	class 9: 0.0041576377
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0765847
	class 13: 0.21258777
	class 14: 0.0
	class 15: 0.5458122
	class 16: 0.0
Class Acc:
	class 0: 0.95861596
	class 1: 0.8834513
	class 2: 0.935861
	class 3: 0.8236907
	class 4: 0.55276936
	class 5: 0.7986146
	class 6: 0.5348841
	class 7: 0.8749203
	class 8: 0.8661525
	class 9: 0.004184025
	class 10: 0.0
	class 11: 0.0
	class 12: 0.07706986
	class 13: 0.61120236
	class 14: 0.0
	class 15: 0.8377212
	class 16: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 25, step: 5
select part of clients to conduct local training
[24, 7, 4, 12]
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.842482336610556
Loss made of: CE 0.06307154893875122, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.600795745849609 EntMin 0.0
Epoch 1, Class Loss=0.1448701024055481, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.1448701024055481, Class Loss=0.1448701024055481, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=5.778776362538338
Loss made of: CE 0.2700524926185608, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.70382833480835 EntMin 0.0
Epoch 2, Class Loss=0.27734696865081787, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.27734696865081787, Class Loss=0.27734696865081787, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.687615165114403
Loss made of: CE 0.3591950833797455, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.486702919006348 EntMin 0.0
Epoch 3, Class Loss=0.3849223256111145, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.3849223256111145, Class Loss=0.3849223256111145, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.590749734640122
Loss made of: CE 0.5277507305145264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.82018518447876 EntMin 0.0
Epoch 4, Class Loss=0.4849425256252289, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.4849425256252289, Class Loss=0.4849425256252289, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.402800250053406
Loss made of: CE 0.5301761031150818, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.424770355224609 EntMin 0.0
Epoch 5, Class Loss=0.5416547060012817, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.5416547060012817, Class Loss=0.5416547060012817, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.3958207190036775
Loss made of: CE 0.6601352691650391, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.774908065795898 EntMin 0.0
Epoch 6, Class Loss=0.7040111422538757, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.7040111422538757, Class Loss=0.7040111422538757, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.3055049777030945, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.3055049777030945, Class Loss=0.3055049777030945, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.47980326414108276, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.47980326414108276, Class Loss=0.47980326414108276, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.5652915239334106, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.5652915239334106, Class Loss=0.5652915239334106, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.6392567753791809, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.6392567753791809, Class Loss=0.6392567753791809, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6323274970054626, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.6323274970054626, Class Loss=0.6323274970054626, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.679033637046814, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.679033637046814, Class Loss=0.679033637046814, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.542623017728329
Loss made of: CE 0.11069965362548828, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.821591854095459 EntMin 0.0
Epoch 1, Class Loss=0.14247876405715942, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.14247876405715942, Class Loss=0.14247876405715942, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=5.771654070913792
Loss made of: CE 0.22861242294311523, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.755542278289795 EntMin 0.0
Epoch 2, Class Loss=0.2854655981063843, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.2854655981063843, Class Loss=0.2854655981063843, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.553122079372406
Loss made of: CE 0.46791601181030273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.855475902557373 EntMin 0.0
Epoch 3, Class Loss=0.40805453062057495, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.40805453062057495, Class Loss=0.40805453062057495, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.549562034010887
Loss made of: CE 0.5341678857803345, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.380495071411133 EntMin 0.0
Epoch 4, Class Loss=0.4983072876930237, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.4983072876930237, Class Loss=0.4983072876930237, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.311613315343857
Loss made of: CE 0.6286622285842896, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.427802085876465 EntMin 0.0
Epoch 5, Class Loss=0.5555846095085144, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5555846095085144, Class Loss=0.5555846095085144, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.357699775695801
Loss made of: CE 0.6485642790794373, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.696961402893066 EntMin 0.0
Epoch 6, Class Loss=0.7101432681083679, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.7101432681083679, Class Loss=0.7101432681083679, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.783133829385042
Loss made of: CE 0.1381971538066864, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.089809417724609 EntMin 0.0
Epoch 1, Class Loss=0.1459769755601883, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.1459769755601883, Class Loss=0.1459769755601883, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=5.906289225816726
Loss made of: CE 0.3413997292518616, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.640802383422852 EntMin 0.0
Epoch 2, Class Loss=0.2837849259376526, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.2837849259376526, Class Loss=0.2837849259376526, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.579509800672531
Loss made of: CE 0.4574573040008545, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.182109832763672 EntMin 0.0
Epoch 3, Class Loss=0.39839571714401245, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.39839571714401245, Class Loss=0.39839571714401245, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.498163893818855
Loss made of: CE 0.4475998282432556, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.537276268005371 EntMin 0.0
Epoch 4, Class Loss=0.48969149589538574, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.48969149589538574, Class Loss=0.48969149589538574, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.216601800918579
Loss made of: CE 0.5400148630142212, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.515607833862305 EntMin 0.0
Epoch 5, Class Loss=0.5563344955444336, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5563344955444336, Class Loss=0.5563344955444336, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.441421717405319
Loss made of: CE 0.6471866965293884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.485608100891113 EntMin 0.0
Epoch 6, Class Loss=0.7084877490997314, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.7084877490997314, Class Loss=0.7084877490997314, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8267999887466431, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.809055
Mean Acc: 0.425781
FreqW Acc: 0.687391
Mean IoU: 0.334032
Class IoU:
	class 0: 0.83139986
	class 1: 0.76891136
	class 2: 0.32425207
	class 3: 0.6688482
	class 4: 0.4634669
	class 5: 0.6628695
	class 6: 0.33502424
	class 7: 0.78735507
	class 8: 0.711519
	class 9: 0.00141863
	class 10: 0.0
	class 11: 0.0
	class 12: 0.05961689
	class 13: 0.19207256
	class 14: 0.0
	class 15: 0.5398598
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
Class Acc:
	class 0: 0.9554191
	class 1: 0.8069895
	class 2: 0.91814065
	class 3: 0.6949301
	class 4: 0.5127241
	class 5: 0.7013158
	class 6: 0.33807644
	class 7: 0.8397358
	class 8: 0.77252686
	class 9: 0.0014272489
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0601725
	class 13: 0.5990199
	class 14: 0.0
	class 15: 0.8893627
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0

federated global round: 26, step: 5
select part of clients to conduct local training
[6, 28, 20, 4]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.918064102157951
Loss made of: CE 0.11407085508108139, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.752457618713379 EntMin 0.0
Epoch 1, Class Loss=0.10124649852514267, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.10124649852514267, Class Loss=0.10124649852514267, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=4.849552014470101
Loss made of: CE 0.25318777561187744, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.184147834777832 EntMin 0.0
Epoch 2, Class Loss=0.2004089206457138, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.2004089206457138, Class Loss=0.2004089206457138, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=4.864109754562378
Loss made of: CE 0.30821964144706726, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.547148704528809 EntMin 0.0
Epoch 3, Class Loss=0.30862754583358765, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.30862754583358765, Class Loss=0.30862754583358765, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=4.721132668852806
Loss made of: CE 0.41461706161499023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.583127975463867 EntMin 0.0
Epoch 4, Class Loss=0.3994233012199402, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3994233012199402, Class Loss=0.3994233012199402, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.178059309720993
Loss made of: CE 0.6930512189865112, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.901552200317383 EntMin 0.0
Epoch 5, Class Loss=0.5917983055114746, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.5917983055114746, Class Loss=0.5917983055114746, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=4.911685526371002
Loss made of: CE 0.5920273661613464, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9337728023529053 EntMin 0.0
Epoch 6, Class Loss=0.646551251411438, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.646551251411438, Class Loss=0.646551251411438, Reg Loss=0.0
Current Client Index:  28
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.21187403798103333, Reg Loss=0.0
Clinet index 28, End of Epoch 1/6, Average Loss=0.21187403798103333, Class Loss=0.21187403798103333, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.35812702775001526, Reg Loss=0.0
Clinet index 28, End of Epoch 2/6, Average Loss=0.35812702775001526, Class Loss=0.35812702775001526, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.521084189414978, Reg Loss=0.0
Clinet index 28, End of Epoch 3/6, Average Loss=0.521084189414978, Class Loss=0.521084189414978, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.64350825548172, Reg Loss=0.0
Clinet index 28, End of Epoch 4/6, Average Loss=0.64350825548172, Class Loss=0.64350825548172, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7182618975639343, Reg Loss=0.0
Clinet index 28, End of Epoch 5/6, Average Loss=0.7182618975639343, Class Loss=0.7182618975639343, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.7335880398750305, Reg Loss=0.0
Clinet index 28, End of Epoch 6/6, Average Loss=0.7335880398750305, Class Loss=0.7335880398750305, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.21348075568675995, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.21348075568675995, Class Loss=0.21348075568675995, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.35149291157722473, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.35149291157722473, Class Loss=0.35149291157722473, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.5067410469055176, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.5067410469055176, Class Loss=0.5067410469055176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.6305788159370422, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.6305788159370422, Class Loss=0.6305788159370422, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7172512412071228, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.7172512412071228, Class Loss=0.7172512412071228, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7269541025161743, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.7269541025161743, Class Loss=0.7269541025161743, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.552837949991226
Loss made of: CE 0.7131266593933105, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.222851753234863 EntMin 0.0
Epoch 1, Class Loss=0.8320852518081665, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.8320852518081665, Class Loss=0.8320852518081665, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=5.314853918552399
Loss made of: CE 0.7615706920623779, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.772863388061523 EntMin 0.0
Epoch 2, Class Loss=0.7717797756195068, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.7717797756195068, Class Loss=0.7717797756195068, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=5.2136559009552
Loss made of: CE 0.6351625919342041, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.226493835449219 EntMin 0.0
Epoch 3, Class Loss=0.7256214022636414, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.7256214022636414, Class Loss=0.7256214022636414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=5.269952142238617
Loss made of: CE 0.7059037089347839, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1151533126831055 EntMin 0.0
Epoch 4, Class Loss=0.6769198179244995, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.6769198179244995, Class Loss=0.6769198179244995, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.0858146250247955
Loss made of: CE 0.7244400978088379, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9499359130859375 EntMin 0.0
Epoch 5, Class Loss=0.6751546263694763, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.6751546263694763, Class Loss=0.6751546263694763, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=4.819854938983918
Loss made of: CE 0.5888131856918335, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.685741424560547 EntMin 0.0
Epoch 6, Class Loss=0.6140013933181763, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6140013933181763, Class Loss=0.6140013933181763, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7610491514205933, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.805691
Mean Acc: 0.394715
FreqW Acc: 0.677356
Mean IoU: 0.311842
Class IoU:
	class 0: 0.8247963
	class 1: 0.7137791
	class 2: 0.33349252
	class 3: 0.588121
	class 4: 0.3980205
	class 5: 0.56774974
	class 6: 0.21421507
	class 7: 0.7866296
	class 8: 0.7013118
	class 9: 0.002193903
	class 10: 0.0
	class 11: 0.0
	class 12: 0.044039614
	class 13: 0.19682834
	class 14: 0.0
	class 15: 0.55381674
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
Class Acc:
	class 0: 0.96401805
	class 1: 0.7360714
	class 2: 0.8979942
	class 3: 0.5994087
	class 4: 0.4273879
	class 5: 0.5904545
	class 6: 0.21524455
	class 7: 0.82652533
	class 8: 0.7533399
	class 9: 0.0022058068
	class 10: 0.0
	class 11: 0.0
	class 12: 0.04428942
	class 13: 0.57071537
	class 14: 0.0
	class 15: 0.8719373
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0

federated global round: 27, step: 5
select part of clients to conduct local training
[24, 8, 26, 0]
Current Client Index:  24
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.19850806593895
Loss made of: CE 0.7458993792533875, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.435750961303711 EntMin 0.0
Epoch 1, Class Loss=0.7644320726394653, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.7644320726394653, Class Loss=0.7644320726394653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/12, Loss=5.363582938909531
Loss made of: CE 0.7107458114624023, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.768120765686035 EntMin 0.0
Epoch 2, Class Loss=0.6978951692581177, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.6978951692581177, Class Loss=0.6978951692581177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/12, Loss=5.267169940471649
Loss made of: CE 0.6139602661132812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303835868835449 EntMin 0.0
Epoch 3, Class Loss=0.6683094501495361, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.6683094501495361, Class Loss=0.6683094501495361, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/12, Loss=5.234984403848648
Loss made of: CE 0.6401370167732239, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3605732917785645 EntMin 0.0
Epoch 4, Class Loss=0.6500081419944763, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.6500081419944763, Class Loss=0.6500081419944763, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/12, Loss=4.966795879602432
Loss made of: CE 0.6388646960258484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1973371505737305 EntMin 0.0
Epoch 5, Class Loss=0.6147310733795166, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.6147310733795166, Class Loss=0.6147310733795166, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/12, Loss=4.884984618425369
Loss made of: CE 0.5401402711868286, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.210218906402588 EntMin 0.0
Epoch 6, Class Loss=0.6022032499313354, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.6022032499313354, Class Loss=0.6022032499313354, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.160793736577034, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.160793736577034, Class Loss=0.160793736577034, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.3280946612358093, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.3280946612358093, Class Loss=0.3280946612358093, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.4720284342765808, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.4720284342765808, Class Loss=0.4720284342765808, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.6250710487365723, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.6250710487365723, Class Loss=0.6250710487365723, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6716634035110474, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6716634035110474, Class Loss=0.6716634035110474, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6988096833229065, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6988096833229065, Class Loss=0.6988096833229065, Reg Loss=0.0
Current Client Index:  26
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.426520048826933
Loss made of: CE 0.12066700309515, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.75871467590332 EntMin 0.0
Epoch 1, Class Loss=0.09700907766819, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.09700907766819, Class Loss=0.09700907766819, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=4.855736708641052
Loss made of: CE 0.24694065749645233, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.248330116271973 EntMin 0.0
Epoch 2, Class Loss=0.19510239362716675, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.19510239362716675, Class Loss=0.19510239362716675, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=4.890206930041313
Loss made of: CE 0.3768255114555359, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.756081581115723 EntMin 0.0
Epoch 3, Class Loss=0.2955010235309601, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.2955010235309601, Class Loss=0.2955010235309601, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=4.907656148076057
Loss made of: CE 0.37508320808410645, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.57449197769165 EntMin 0.0
Epoch 4, Class Loss=0.3794092535972595, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.3794092535972595, Class Loss=0.3794092535972595, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.644545850157738
Loss made of: CE 0.49208998680114746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.920746803283691 EntMin 0.0
Epoch 5, Class Loss=0.43886539340019226, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.43886539340019226, Class Loss=0.43886539340019226, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.923898816108704
Loss made of: CE 0.5963900685310364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.249252796173096 EntMin 0.0
Epoch 6, Class Loss=0.6045003533363342, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.6045003533363342, Class Loss=0.6045003533363342, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.103128581494093
Loss made of: CE 0.07218490540981293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.451977729797363 EntMin 0.0
Epoch 1, Class Loss=0.09497520327568054, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.09497520327568054, Class Loss=0.09497520327568054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=4.663559744507074
Loss made of: CE 0.16822373867034912, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.166713714599609 EntMin 0.0
Epoch 2, Class Loss=0.18610934913158417, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.18610934913158417, Class Loss=0.18610934913158417, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=4.7620531678199765
Loss made of: CE 0.3637191951274872, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.377882957458496 EntMin 0.0
Epoch 3, Class Loss=0.2745446562767029, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.2745446562767029, Class Loss=0.2745446562767029, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=4.71015085875988
Loss made of: CE 0.3163716495037079, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.563614845275879 EntMin 0.0
Epoch 4, Class Loss=0.3505917191505432, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3505917191505432, Class Loss=0.3505917191505432, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.752380114793778
Loss made of: CE 0.4563191831111908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.484874248504639 EntMin 0.0
Epoch 5, Class Loss=0.4414035975933075, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.4414035975933075, Class Loss=0.4414035975933075, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.817835319042206
Loss made of: CE 0.6169893741607666, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.841881275177002 EntMin 0.0
Epoch 6, Class Loss=0.6029180288314819, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.6029180288314819, Class Loss=0.6029180288314819, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7569406628608704, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.812471
Mean Acc: 0.425076
FreqW Acc: 0.693081
Mean IoU: 0.334258
Class IoU:
	class 0: 0.8370129
	class 1: 0.7481127
	class 2: 0.3247736
	class 3: 0.652906
	class 4: 0.40962785
	class 5: 0.6439336
	class 6: 0.22304839
	class 7: 0.7909362
	class 8: 0.73655784
	class 9: 0.0012987793
	class 10: 0.0
	class 11: 0.0
	class 12: 0.073702164
	class 13: 0.19754595
	class 14: 0.0
	class 15: 0.5666622
	class 16: 0.0
	class 17: 0.0
	class 18: 0.14478128
Class Acc:
	class 0: 0.9611269
	class 1: 0.7742398
	class 2: 0.91378224
	class 3: 0.67412823
	class 4: 0.4425734
	class 5: 0.68420357
	class 6: 0.22411159
	class 7: 0.8366486
	class 8: 0.8294299
	class 9: 0.0013063827
	class 10: 0.0
	class 11: 0.0
	class 12: 0.074480064
	class 13: 0.59311044
	class 14: 0.0
	class 15: 0.86099416
	class 16: 0.0
	class 17: 0.0
	class 18: 0.20630834

federated global round: 28, step: 5
select part of clients to conduct local training
[8, 9, 16, 3]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.1310139894485474, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.1310139894485474, Class Loss=1.1310139894485474, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=1.0501952171325684, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=1.0501952171325684, Class Loss=1.0501952171325684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.9275447130203247, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.9275447130203247, Class Loss=0.9275447130203247, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.8290067315101624, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.8290067315101624, Class Loss=0.8290067315101624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.7560588717460632, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.7560588717460632, Class Loss=0.7560588717460632, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.7062975168228149, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.7062975168228149, Class Loss=0.7062975168228149, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.21007592976093292, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.21007592976093292, Class Loss=0.21007592976093292, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.3522374927997589, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.3522374927997589, Class Loss=0.3522374927997589, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.5169221758842468, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5169221758842468, Class Loss=0.5169221758842468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.6456188559532166, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.6456188559532166, Class Loss=0.6456188559532166, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.6690545678138733, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.6690545678138733, Class Loss=0.6690545678138733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.7485628724098206, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.7485628724098206, Class Loss=0.7485628724098206, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.51350147984922
Loss made of: CE 0.10027751326560974, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611554145812988 EntMin 0.0
Epoch 1, Class Loss=0.06391125917434692, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.06391125917434692, Class Loss=0.06391125917434692, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=4.2765554182231424
Loss made of: CE 0.22161297500133514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.600154876708984 EntMin 0.0
Epoch 2, Class Loss=0.14058032631874084, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.14058032631874084, Class Loss=0.14058032631874084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.3028479889035225
Loss made of: CE 0.18358194828033447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8525140285491943 EntMin 0.0
Epoch 3, Class Loss=0.2168266475200653, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.2168266475200653, Class Loss=0.2168266475200653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.301898086071015
Loss made of: CE 0.28669512271881104, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9328536987304688 EntMin 0.0
Epoch 4, Class Loss=0.30250948667526245, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.30250948667526245, Class Loss=0.30250948667526245, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.417279273271561
Loss made of: CE 0.3907160460948944, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.741014242172241 EntMin 0.0
Epoch 5, Class Loss=0.3923052251338959, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3923052251338959, Class Loss=0.3923052251338959, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.659257006645203
Loss made of: CE 0.6094276905059814, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.154057502746582 EntMin 0.0
Epoch 6, Class Loss=0.6035481691360474, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.6035481691360474, Class Loss=0.6035481691360474, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.463224938139319
Loss made of: CE 0.05401301383972168, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.243814945220947 EntMin 0.0
Epoch 1, Class Loss=0.07335388660430908, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.07335388660430908, Class Loss=0.07335388660430908, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=4.533628229796887
Loss made of: CE 0.19416312873363495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.700440883636475 EntMin 0.0
Epoch 2, Class Loss=0.16261467337608337, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.16261467337608337, Class Loss=0.16261467337608337, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.586516281962394
Loss made of: CE 0.25833815336227417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.777806282043457 EntMin 0.0
Epoch 3, Class Loss=0.24432995915412903, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.24432995915412903, Class Loss=0.24432995915412903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.572816762328148
Loss made of: CE 0.3187839686870575, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.978482723236084 EntMin 0.0
Epoch 4, Class Loss=0.3217768669128418, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.3217768669128418, Class Loss=0.3217768669128418, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.6577432513237
Loss made of: CE 0.3771620988845825, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.434342384338379 EntMin 0.0
Epoch 5, Class Loss=0.40465378761291504, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.40465378761291504, Class Loss=0.40465378761291504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.753761714696884
Loss made of: CE 0.5262491703033447, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.855937957763672 EntMin 0.0
Epoch 6, Class Loss=0.5640798807144165, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.5640798807144165, Class Loss=0.5640798807144165, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7140824198722839, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.810554
Mean Acc: 0.405041
FreqW Acc: 0.684707
Mean IoU: 0.321539
Class IoU:
	class 0: 0.82923925
	class 1: 0.72132134
	class 2: 0.3236084
	class 3: 0.6247926
	class 4: 0.37468028
	class 5: 0.60000604
	class 6: 0.19962908
	class 7: 0.79750425
	class 8: 0.726984
	class 9: 0.0025961115
	class 10: 0.0
	class 11: 0.0
	class 12: 0.061886083
	class 13: 0.1966463
	class 14: 0.0
	class 15: 0.5734564
	class 16: 0.0
	class 17: 0.0
	class 18: 0.07688271
Class Acc:
	class 0: 0.96855694
	class 1: 0.74292856
	class 2: 0.910524
	class 3: 0.6405792
	class 4: 0.39713353
	class 5: 0.63099456
	class 6: 0.20031151
	class 7: 0.8469719
	class 8: 0.81074935
	class 9: 0.0026208018
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0624299
	class 13: 0.56172115
	class 14: 0.0
	class 15: 0.83235157
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0879118

federated global round: 29, step: 5
select part of clients to conduct local training
[29, 27, 26, 18]
Current Client Index:  29
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.914247017353773
Loss made of: CE 0.15709739923477173, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.107067108154297 EntMin 0.0
Epoch 1, Class Loss=0.08286672085523605, Reg Loss=0.0
Clinet index 29, End of Epoch 1/6, Average Loss=0.08286672085523605, Class Loss=0.08286672085523605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=4.723627538233996
Loss made of: CE 0.15988463163375854, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8894612789154053 EntMin 0.0
Epoch 2, Class Loss=0.16076025366783142, Reg Loss=0.0
Clinet index 29, End of Epoch 2/6, Average Loss=0.16076025366783142, Class Loss=0.16076025366783142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.40768311470747
Loss made of: CE 0.21693240106105804, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.141304016113281 EntMin 0.0
Epoch 3, Class Loss=0.23778806626796722, Reg Loss=0.0
Clinet index 29, End of Epoch 3/6, Average Loss=0.23778806626796722, Class Loss=0.23778806626796722, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.682653689384461
Loss made of: CE 0.2628208100795746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.151650905609131 EntMin 0.0
Epoch 4, Class Loss=0.3251592516899109, Reg Loss=0.0
Clinet index 29, End of Epoch 4/6, Average Loss=0.3251592516899109, Class Loss=0.3251592516899109, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.595781829953194
Loss made of: CE 0.3963187634944916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.007265090942383 EntMin 0.0
Epoch 5, Class Loss=0.40059468150138855, Reg Loss=0.0
Clinet index 29, End of Epoch 5/6, Average Loss=0.40059468150138855, Class Loss=0.40059468150138855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.697578889131546
Loss made of: CE 0.5546236038208008, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.739455223083496 EntMin 0.0
Epoch 6, Class Loss=0.616952121257782, Reg Loss=0.0
Clinet index 29, End of Epoch 6/6, Average Loss=0.616952121257782, Class Loss=0.616952121257782, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.15108492970466614, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.15108492970466614, Class Loss=0.15108492970466614, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.33975812792778015, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.33975812792778015, Class Loss=0.33975812792778015, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.43728554248809814, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.43728554248809814, Class Loss=0.43728554248809814, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5625325441360474, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.5625325441360474, Class Loss=0.5625325441360474, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.686241865158081, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.686241865158081, Class Loss=0.686241865158081, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.7622233033180237, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.7622233033180237, Class Loss=0.7622233033180237, Reg Loss=0.0
Current Client Index:  26
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.620885968208313
Loss made of: CE 0.6655130982398987, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.374016761779785 EntMin 0.0
Epoch 1, Class Loss=0.7308634519577026, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.7308634519577026, Class Loss=0.7308634519577026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/12, Loss=4.972398728132248
Loss made of: CE 0.7021574974060059, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.103199005126953 EntMin 0.0
Epoch 2, Class Loss=0.6830592155456543, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.6830592155456543, Class Loss=0.6830592155456543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/12, Loss=4.923738539218903
Loss made of: CE 0.6421281099319458, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.744998455047607 EntMin 0.0
Epoch 3, Class Loss=0.6275202631950378, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.6275202631950378, Class Loss=0.6275202631950378, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/12, Loss=4.759680891036988
Loss made of: CE 0.5773779153823853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.073244094848633 EntMin 0.0
Epoch 4, Class Loss=0.609640896320343, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.609640896320343, Class Loss=0.609640896320343, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/12, Loss=4.516786712408066
Loss made of: CE 0.5715017914772034, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.283634185791016 EntMin 0.0
Epoch 5, Class Loss=0.6195741891860962, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.6195741891860962, Class Loss=0.6195741891860962, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/12, Loss=4.631443160772323
Loss made of: CE 0.5946587920188904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.319077491760254 EntMin 0.0
Epoch 6, Class Loss=0.6011960506439209, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.6011960506439209, Class Loss=0.6011960506439209, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.003466117754579
Loss made of: CE 0.09429465234279633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.754088401794434 EntMin 0.0
Epoch 1, Class Loss=0.07499215006828308, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.07499215006828308, Class Loss=0.07499215006828308, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=4.56814623773098
Loss made of: CE 0.1248750239610672, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.473791122436523 EntMin 0.0
Epoch 2, Class Loss=0.15306371450424194, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.15306371450424194, Class Loss=0.15306371450424194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.497145333886147
Loss made of: CE 0.2459750473499298, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.337159633636475 EntMin 0.0
Epoch 3, Class Loss=0.23345260322093964, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.23345260322093964, Class Loss=0.23345260322093964, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.536953201889991
Loss made of: CE 0.299468070268631, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.086569786071777 EntMin 0.0
Epoch 4, Class Loss=0.31938812136650085, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.31938812136650085, Class Loss=0.31938812136650085, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.551154160499573
Loss made of: CE 0.40295490622520447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.945044040679932 EntMin 0.0
Epoch 5, Class Loss=0.39927947521209717, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.39927947521209717, Class Loss=0.39927947521209717, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.670376640558243
Loss made of: CE 0.567527174949646, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.212182521820068 EntMin 0.0
Epoch 6, Class Loss=0.5970759391784668, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.5970759391784668, Class Loss=0.5970759391784668, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7314852476119995, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.813564
Mean Acc: 0.433113
FreqW Acc: 0.699870
Mean IoU: 0.338472
Class IoU:
	class 0: 0.8440048
	class 1: 0.76665
	class 2: 0.32884037
	class 3: 0.68054295
	class 4: 0.39747226
	class 5: 0.64309585
	class 6: 0.21601045
	class 7: 0.79039717
	class 8: 0.74541223
	class 9: 0.0005021195
	class 10: 0.0
	class 11: 0.0
	class 12: 0.092925854
	class 13: 0.19759013
	class 14: 0.0
	class 15: 0.5791786
	class 16: 0.0
	class 17: 0.0
	class 18: 0.14833781
Class Acc:
	class 0: 0.95955753
	class 1: 0.7943731
	class 2: 0.9114099
	class 3: 0.7061805
	class 4: 0.4264117
	class 5: 0.6853884
	class 6: 0.21699753
	class 7: 0.8390915
	class 8: 0.858935
	class 9: 0.0005046803
	class 10: 0.0
	class 11: 0.0
	class 12: 0.09391354
	class 13: 0.57251525
	class 14: 0.0
	class 15: 0.8443555
	class 16: 0.0
	class 17: 0.0
	class 18: 0.31951672

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 30, step: 6
select part of clients to conduct local training
[10, 30, 4, 33]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=13.198887621005998
Loss made of: CE 0.05295439809560776, LKD 0.0, LDE 0.0, LReg 0.0, POD 12.482105255126953 EntMin 0.0
Epoch 1, Class Loss=0.03511149063706398, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.03511149063706398, Class Loss=0.03511149063706398, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=11.56341419517994
Loss made of: CE 0.17899072170257568, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.730018615722656 EntMin 0.0
Epoch 2, Class Loss=0.21303574740886688, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.21303574740886688, Class Loss=0.21303574740886688, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=11.23322112262249
Loss made of: CE 0.5395297408103943, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.799997329711914 EntMin 0.0
Epoch 3, Class Loss=0.5243626832962036, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5243626832962036, Class Loss=0.5243626832962036, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.948620229959488
Loss made of: CE 0.7219836115837097, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.237695693969727 EntMin 0.0
Epoch 4, Class Loss=0.7414903044700623, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.7414903044700623, Class Loss=0.7414903044700623, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.68767586350441
Loss made of: CE 0.7035230994224548, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.461654663085938 EntMin 0.0
Epoch 5, Class Loss=0.7654034495353699, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.7654034495353699, Class Loss=0.7654034495353699, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=10.08564596772194
Loss made of: CE 0.6890203356742859, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.42329216003418 EntMin 0.0
Epoch 6, Class Loss=0.6791210174560547, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.6791210174560547, Class Loss=0.6791210174560547, Reg Loss=0.0
Current Client Index:  30
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=9.01847149967216
Loss made of: CE 0.03050161525607109, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.222333908081055 EntMin 0.0
Epoch 1, Class Loss=0.028760258108377457, Reg Loss=0.0
Clinet index 30, End of Epoch 1/6, Average Loss=0.028760258108377457, Class Loss=0.028760258108377457, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=7.90666403695941
Loss made of: CE 0.12367372959852219, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.254430294036865 EntMin 0.0
Epoch 2, Class Loss=0.14545738697052002, Reg Loss=0.0
Clinet index 30, End of Epoch 2/6, Average Loss=0.14545738697052002, Class Loss=0.14545738697052002, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=6.939948546886444
Loss made of: CE 0.3054058849811554, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.006711959838867 EntMin 0.0
Epoch 3, Class Loss=0.327188640832901, Reg Loss=0.0
Clinet index 30, End of Epoch 3/6, Average Loss=0.327188640832901, Class Loss=0.327188640832901, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=6.769771525263787
Loss made of: CE 0.5084420442581177, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.424293041229248 EntMin 0.0
Epoch 4, Class Loss=0.4569496512413025, Reg Loss=0.0
Clinet index 30, End of Epoch 4/6, Average Loss=0.4569496512413025, Class Loss=0.4569496512413025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=6.723729872703553
Loss made of: CE 0.7185160517692566, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.813116073608398 EntMin 0.0
Epoch 5, Class Loss=0.5683009624481201, Reg Loss=0.0
Clinet index 30, End of Epoch 5/6, Average Loss=0.5683009624481201, Class Loss=0.5683009624481201, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=6.602266156673432
Loss made of: CE 0.7813533544540405, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.01876974105835 EntMin 0.0
Epoch 6, Class Loss=0.7161571979522705, Reg Loss=0.0
Clinet index 30, End of Epoch 6/6, Average Loss=0.7161571979522705, Class Loss=0.7161571979522705, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=12.863234457676299
Loss made of: CE 0.024490848183631897, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.383613586425781 EntMin 0.0
Epoch 1, Class Loss=0.02435874752700329, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.02435874752700329, Class Loss=0.02435874752700329, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=11.001050275564193
Loss made of: CE 0.11848460137844086, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.79944133758545 EntMin 0.0
Epoch 2, Class Loss=0.18815205991268158, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.18815205991268158, Class Loss=0.18815205991268158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/12, Loss=11.076850438117981
Loss made of: CE 0.612224280834198, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.508001327514648 EntMin 0.0
Epoch 3, Class Loss=0.5595790147781372, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.5595790147781372, Class Loss=0.5595790147781372, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.59586209654808
Loss made of: CE 0.8380459547042847, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.245569229125977 EntMin 0.0
Epoch 4, Class Loss=0.7649627923965454, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.7649627923965454, Class Loss=0.7649627923965454, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.363292080163955
Loss made of: CE 0.6355174779891968, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.889652252197266 EntMin 0.0
Epoch 5, Class Loss=0.7583764791488647, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.7583764791488647, Class Loss=0.7583764791488647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.575001782178878
Loss made of: CE 0.6561342477798462, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.794242858886719 EntMin 0.0
Epoch 6, Class Loss=0.7000541090965271, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.7000541090965271, Class Loss=0.7000541090965271, Reg Loss=0.0
Current Client Index:  33
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=13.048478528345004
Loss made of: CE 0.003997683059424162, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.646086692810059 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.022244295105338097, Reg Loss=0.0
Clinet index 33, End of Epoch 1/6, Average Loss=0.022244295105338097, Class Loss=0.022244295105338097, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=11.692568530142307
Loss made of: CE 0.22673216462135315, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.84574031829834 EntMin 0.0
Epoch 2, Class Loss=0.1973433941602707, Reg Loss=0.0
Clinet index 33, End of Epoch 2/6, Average Loss=0.1973433941602707, Class Loss=0.1973433941602707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 3, Batch 10/12, Loss=11.508603647351265
Loss made of: CE 0.5327965021133423, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.907356262207031 EntMin 0.0
Epoch 3, Class Loss=0.5543450117111206, Reg Loss=0.0
Clinet index 33, End of Epoch 3/6, Average Loss=0.5543450117111206, Class Loss=0.5543450117111206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=11.07375459074974
Loss made of: CE 0.8419390916824341, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.608903884887695 EntMin 0.0
Epoch 4, Class Loss=0.7664977312088013, Reg Loss=0.0
Clinet index 33, End of Epoch 4/6, Average Loss=0.7664977312088013, Class Loss=0.7664977312088013, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.84780449271202
Loss made of: CE 0.7184385061264038, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.537519454956055 EntMin 0.0
Epoch 5, Class Loss=0.7663244605064392, Reg Loss=0.0
Clinet index 33, End of Epoch 5/6, Average Loss=0.7663244605064392, Class Loss=0.7663244605064392, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=10.12495824098587
Loss made of: CE 0.6473003625869751, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.555580139160156 EntMin 0.0
Epoch 6, Class Loss=0.684317946434021, Reg Loss=0.0
Clinet index 33, End of Epoch 6/6, Average Loss=0.684317946434021, Class Loss=0.684317946434021, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.9218708276748657, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.780402
Mean Acc: 0.327559
FreqW Acc: 0.635050
Mean IoU: 0.255311
Class IoU:
	class 0: 0.7943134
	class 1: 0.6773312
	class 2: 0.3421243
	class 3: 0.4226077
	class 4: 0.2828261
	class 5: 0.49209523
	class 6: 0.18827361
	class 7: 0.776912
	class 8: 0.47459942
	class 9: 0.0060496298
	class 10: 0.0
	class 11: 2.3498167e-07
	class 12: 0.02380916
	class 13: 0.19232671
	class 14: 0.0
	class 15: 0.55168265
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0032275831
	class 19: 0.13335203
	class 20: 0.0
Class Acc:
	class 0: 0.9634518
	class 1: 0.69829124
	class 2: 0.8645332
	class 3: 0.42576864
	class 4: 0.29485223
	class 5: 0.50359064
	class 6: 0.18924646
	class 7: 0.8360014
	class 8: 0.48604122
	class 9: 0.006134277
	class 10: 0.0
	class 11: 2.3498167e-07
	class 12: 0.023838935
	class 13: 0.5528227
	class 14: 0.0
	class 15: 0.8817935
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0032375024
	class 19: 0.14913918
	class 20: 0.0

federated global round: 31, step: 6
select part of clients to conduct local training
[9, 17, 14, 1]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=8.315996352117509
Loss made of: CE 0.04938099905848503, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.270328044891357 EntMin 0.0
Epoch 1, Class Loss=0.024820664897561073, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.024820664897561073, Class Loss=0.024820664897561073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=7.148156466335058
Loss made of: CE 0.05763101577758789, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.058872222900391 EntMin 0.0
Epoch 2, Class Loss=0.15188951790332794, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.15188951790332794, Class Loss=0.15188951790332794, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=6.454885603487492
Loss made of: CE 0.3667065501213074, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.471604347229004 EntMin 0.0
Epoch 3, Class Loss=0.27174946665763855, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.27174946665763855, Class Loss=0.27174946665763855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.245610508322716
Loss made of: CE 0.42312726378440857, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.940827369689941 EntMin 0.0
Epoch 4, Class Loss=0.43788838386535645, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.43788838386535645, Class Loss=0.43788838386535645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=6.0977998316288
Loss made of: CE 0.5109298229217529, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.437006950378418 EntMin 0.0
Epoch 5, Class Loss=0.5411141514778137, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5411141514778137, Class Loss=0.5411141514778137, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=6.228293520212174
Loss made of: CE 0.621523380279541, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.281473159790039 EntMin 0.0
Epoch 6, Class Loss=0.7003709077835083, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.7003709077835083, Class Loss=0.7003709077835083, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=9.4367780393688
Loss made of: CE 0.008315554820001125, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.841106414794922 EntMin 0.0
Epoch 1, Class Loss=0.014685528352856636, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.014685528352856636, Class Loss=0.014685528352856636, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=9.55692833289504
Loss made of: CE 0.07474260777235031, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.581361770629883 EntMin 0.0
Epoch 2, Class Loss=0.11058780550956726, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.11058780550956726, Class Loss=0.11058780550956726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/12, Loss=9.062998592853546
Loss made of: CE 0.27140116691589355, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.54781723022461 EntMin 0.0
Epoch 3, Class Loss=0.2949414849281311, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.2949414849281311, Class Loss=0.2949414849281311, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.9192490786314
Loss made of: CE 0.4299843907356262, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.399129390716553 EntMin 0.0
Epoch 4, Class Loss=0.4257999360561371, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.4257999360561371, Class Loss=0.4257999360561371, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.460725218057632
Loss made of: CE 0.5406482219696045, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.105582237243652 EntMin 0.0
Epoch 5, Class Loss=0.510985255241394, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.510985255241394, Class Loss=0.510985255241394, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=8.147524479031564
Loss made of: CE 0.4616822600364685, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.537841320037842 EntMin 0.0
Epoch 6, Class Loss=0.5096800923347473, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.5096800923347473, Class Loss=0.5096800923347473, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=8.471914957556873
Loss made of: CE 0.039964210242033005, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.192102432250977 EntMin 0.0
Epoch 1, Class Loss=0.029925240203738213, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.029925240203738213, Class Loss=0.029925240203738213, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=7.025914216786623
Loss made of: CE 0.07510209083557129, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.846102714538574 EntMin 0.0
Epoch 2, Class Loss=0.15606410801410675, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.15606410801410675, Class Loss=0.15606410801410675, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=6.537626931071282
Loss made of: CE 0.14339199662208557, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.034152507781982 EntMin 0.0
Epoch 3, Class Loss=0.28878054022789, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.28878054022789, Class Loss=0.28878054022789, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.339428275823593
Loss made of: CE 0.2717675566673279, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.269431114196777 EntMin 0.0
Epoch 4, Class Loss=0.4225441515445709, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.4225441515445709, Class Loss=0.4225441515445709, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=6.020112165808678
Loss made of: CE 0.4750547409057617, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.078308582305908 EntMin 0.0
Epoch 5, Class Loss=0.5199062824249268, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5199062824249268, Class Loss=0.5199062824249268, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=6.2426745116710665
Loss made of: CE 0.7794274091720581, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.98890495300293 EntMin 0.0
Epoch 6, Class Loss=0.688207745552063, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.688207745552063, Class Loss=0.688207745552063, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=9.549560407921671
Loss made of: CE 0.016774851828813553, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.293514251708984 EntMin 0.0
Epoch 1, Class Loss=0.010692707262933254, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.010692707262933254, Class Loss=0.010692707262933254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=8.999581093341112
Loss made of: CE 0.1570124477148056, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.386125564575195 EntMin 0.0
Epoch 2, Class Loss=0.1045961007475853, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.1045961007475853, Class Loss=0.1045961007475853, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=8.778136658668519
Loss made of: CE 0.24448463320732117, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.158811569213867 EntMin 0.0
Epoch 3, Class Loss=0.3045251965522766, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3045251965522766, Class Loss=0.3045251965522766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.90046055316925
Loss made of: CE 0.4450065493583679, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.532351970672607 EntMin 0.0
Epoch 4, Class Loss=0.45349133014678955, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.45349133014678955, Class Loss=0.45349133014678955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.72421716451645
Loss made of: CE 0.5036522746086121, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.624627113342285 EntMin 0.0
Epoch 5, Class Loss=0.5091016292572021, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.5091016292572021, Class Loss=0.5091016292572021, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=8.318937703967094
Loss made of: CE 0.6185284852981567, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.945565223693848 EntMin 0.0
Epoch 6, Class Loss=0.5252909064292908, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.5252909064292908, Class Loss=0.5252909064292908, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8318221569061279, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.790086
Mean Acc: 0.354318
FreqW Acc: 0.652009
Mean IoU: 0.279474
Class IoU:
	class 0: 0.8065183
	class 1: 0.702214
	class 2: 0.34452018
	class 3: 0.44337973
	class 4: 0.38081607
	class 5: 0.5511409
	class 6: 0.33731923
	class 7: 0.78197527
	class 8: 0.46293437
	class 9: 0.006590026
	class 10: 0.0
	class 11: 2.443795e-05
	class 12: 0.034629498
	class 13: 0.18882443
	class 14: 0.0
	class 15: 0.57546353
	class 16: 0.0
	class 17: 0.0
	class 18: 0.009129631
	class 19: 0.24348225
	class 20: 0.0
Class Acc:
	class 0: 0.96584123
	class 1: 0.7228324
	class 2: 0.8515956
	class 3: 0.44652063
	class 4: 0.4092261
	class 5: 0.57093275
	class 6: 0.3420789
	class 7: 0.8465326
	class 8: 0.47370246
	class 9: 0.006720285
	class 10: 0.0
	class 11: 2.4438094e-05
	class 12: 0.034677457
	class 13: 0.57901037
	class 14: 0.0
	class 15: 0.86833537
	class 16: 0.0
	class 17: 0.0
	class 18: 0.009180166
	class 19: 0.31347093
	class 20: 0.0

federated global round: 32, step: 6
select part of clients to conduct local training
[3, 8, 27, 7]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=8.807916840864346
Loss made of: CE 0.0015302037354558706, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.221195220947266 EntMin 0.0
Epoch 1, Class Loss=0.015000281855463982, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.015000281855463982, Class Loss=0.015000281855463982, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=8.169406462833285
Loss made of: CE 0.163111612200737, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.833148956298828 EntMin 0.0
Epoch 2, Class Loss=0.10025089234113693, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.10025089234113693, Class Loss=0.10025089234113693, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=8.004593090713024
Loss made of: CE 0.3874749541282654, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.22325325012207 EntMin 0.0
Epoch 3, Class Loss=0.2664189040660858, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.2664189040660858, Class Loss=0.2664189040660858, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=7.837904098629951
Loss made of: CE 0.3933517336845398, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.977944850921631 EntMin 0.0
Epoch 4, Class Loss=0.3752732276916504, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.3752732276916504, Class Loss=0.3752732276916504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=7.629089441895485
Loss made of: CE 0.34604763984680176, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.82957935333252 EntMin 0.0
Epoch 5, Class Loss=0.45503875613212585, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.45503875613212585, Class Loss=0.45503875613212585, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=7.4686931848526
Loss made of: CE 0.47095176577568054, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.502395153045654 EntMin 0.0
Epoch 6, Class Loss=0.4869025945663452, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.4869025945663452, Class Loss=0.4869025945663452, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.5575580348726366
Loss made of: CE 0.04721909016370773, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.509126663208008 EntMin 0.0
Epoch 1, Class Loss=0.03344143554568291, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.03344143554568291, Class Loss=0.03344143554568291, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=5.900059329718351
Loss made of: CE 0.15628349781036377, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.130779266357422 EntMin 0.0
Epoch 2, Class Loss=0.14458292722702026, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.14458292722702026, Class Loss=0.14458292722702026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.917626433074474
Loss made of: CE 0.2556803822517395, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.384753704071045 EntMin 0.0
Epoch 3, Class Loss=0.26479849219322205, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.26479849219322205, Class Loss=0.26479849219322205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.691883859038353
Loss made of: CE 0.46354350447654724, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.805704116821289 EntMin 0.0
Epoch 4, Class Loss=0.38537800312042236, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.38537800312042236, Class Loss=0.38537800312042236, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.642292165756226
Loss made of: CE 0.365185022354126, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.913200378417969 EntMin 0.0
Epoch 5, Class Loss=0.49672022461891174, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.49672022461891174, Class Loss=0.49672022461891174, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.590107977390289
Loss made of: CE 0.6647846102714539, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.108593940734863 EntMin 0.0
Epoch 6, Class Loss=0.6386799216270447, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6386799216270447, Class Loss=0.6386799216270447, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.577923736162484
Loss made of: CE 0.09692683070898056, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3848371505737305 EntMin 0.0
Epoch 1, Class Loss=0.03540532663464546, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.03540532663464546, Class Loss=0.03540532663464546, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=5.735842794179916
Loss made of: CE 0.20614683628082275, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.54780387878418 EntMin 0.0
Epoch 2, Class Loss=0.13903318345546722, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.13903318345546722, Class Loss=0.13903318345546722, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.799905705451965
Loss made of: CE 0.32422250509262085, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.514642238616943 EntMin 0.0
Epoch 3, Class Loss=0.2678949236869812, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.2678949236869812, Class Loss=0.2678949236869812, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.569500616192817
Loss made of: CE 0.27334779500961304, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.376574516296387 EntMin 0.0
Epoch 4, Class Loss=0.4009993374347687, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.4009993374347687, Class Loss=0.4009993374347687, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.5141488164663315
Loss made of: CE 0.4594152569770813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.721452713012695 EntMin 0.0
Epoch 5, Class Loss=0.5003959536552429, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.5003959536552429, Class Loss=0.5003959536552429, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.72379030585289
Loss made of: CE 0.6587941646575928, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.702630996704102 EntMin 0.0
Epoch 6, Class Loss=0.6735791563987732, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.6735791563987732, Class Loss=0.6735791563987732, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.899074563197791
Loss made of: CE 0.01599505916237831, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.189925193786621 EntMin 0.0
Epoch 1, Class Loss=0.028868867084383965, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.028868867084383965, Class Loss=0.028868867084383965, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.1914236664772035
Loss made of: CE 0.12303867936134338, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.390944957733154 EntMin 0.0
Epoch 2, Class Loss=0.12844064831733704, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.12844064831733704, Class Loss=0.12844064831733704, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.736104472726583
Loss made of: CE 0.2265729010105133, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760765075683594 EntMin 0.0
Epoch 3, Class Loss=0.2662895619869232, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.2662895619869232, Class Loss=0.2662895619869232, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=6.07597716152668
Loss made of: CE 0.41457363963127136, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.080959320068359 EntMin 0.0
Epoch 4, Class Loss=0.4026552140712738, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4026552140712738, Class Loss=0.4026552140712738, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.694390824437141
Loss made of: CE 0.49374639987945557, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.927664279937744 EntMin 0.0
Epoch 5, Class Loss=0.4844111502170563, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4844111502170563, Class Loss=0.4844111502170563, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.817935258150101
Loss made of: CE 0.59356689453125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.760008335113525 EntMin 0.0
Epoch 6, Class Loss=0.6755433082580566, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.6755433082580566, Class Loss=0.6755433082580566, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8094545602798462, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.791233
Mean Acc: 0.369547
FreqW Acc: 0.655813
Mean IoU: 0.291597
Class IoU:
	class 0: 0.807952
	class 1: 0.7489541
	class 2: 0.34476885
	class 3: 0.48678166
	class 4: 0.43313447
	class 5: 0.58686906
	class 6: 0.47496107
	class 7: 0.7748639
	class 8: 0.53354466
	class 9: 0.00899287
	class 10: 0.0
	class 11: 0.000121250545
	class 12: 0.052291557
	class 13: 0.1900268
	class 14: 0.0
	class 15: 0.551008
	class 16: 0.0
	class 17: 0.0
	class 18: 0.041609477
	class 19: 0.087655425
	class 20: 0.0
Class Acc:
	class 0: 0.9587255
	class 1: 0.7804544
	class 2: 0.85444456
	class 3: 0.492761
	class 4: 0.47527245
	class 5: 0.61407554
	class 6: 0.48646075
	class 7: 0.8369406
	class 8: 0.5514439
	class 9: 0.009236936
	class 10: 0.0
	class 11: 0.000121250545
	class 12: 0.052486412
	class 13: 0.6145761
	class 14: 0.0
	class 15: 0.89967823
	class 16: 0.0
	class 17: 0.0
	class 18: 0.042746093
	class 19: 0.09105823
	class 20: 0.0

federated global round: 33, step: 6
select part of clients to conduct local training
[6, 20, 13, 10]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=5.21139026042074
Loss made of: CE 0.00774987880140543, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.765023231506348 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.021218813955783844, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.021218813955783844, Class Loss=0.021218813955783844, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.34612711109221
Loss made of: CE 0.06780418753623962, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.211396217346191 EntMin 0.0
Epoch 2, Class Loss=0.08873658627271652, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.08873658627271652, Class Loss=0.08873658627271652, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.242288269102573
Loss made of: CE 0.20154301822185516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.71024227142334 EntMin 0.0
Epoch 3, Class Loss=0.18877466022968292, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.18877466022968292, Class Loss=0.18877466022968292, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.464712038636208
Loss made of: CE 0.3620269000530243, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.154018402099609 EntMin 0.0
Epoch 4, Class Loss=0.3346724212169647, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3346724212169647, Class Loss=0.3346724212169647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.433062979578972
Loss made of: CE 0.3684108257293701, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.59845495223999 EntMin 0.0
Epoch 5, Class Loss=0.43254828453063965, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.43254828453063965, Class Loss=0.43254828453063965, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.164002102613449
Loss made of: CE 0.5673088431358337, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.254734039306641 EntMin 0.0
Epoch 6, Class Loss=0.6163280606269836, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.6163280606269836, Class Loss=0.6163280606269836, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=5.108124641701579
Loss made of: CE 0.006017893552780151, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.879246711730957 EntMin 0.0
Epoch 1, Class Loss=0.022547191008925438, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.022547191008925438, Class Loss=0.022547191008925438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.269597063027322
Loss made of: CE 0.05505680292844772, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.544559955596924 EntMin 0.0
Epoch 2, Class Loss=0.09153906255960464, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.09153906255960464, Class Loss=0.09153906255960464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.326208628714085
Loss made of: CE 0.20571371912956238, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.212630748748779 EntMin 0.0
Epoch 3, Class Loss=0.20575281977653503, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.20575281977653503, Class Loss=0.20575281977653503, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.18141026198864
Loss made of: CE 0.2934489846229553, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2149505615234375 EntMin 0.0
Epoch 4, Class Loss=0.32285434007644653, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.32285434007644653, Class Loss=0.32285434007644653, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.345493412017822
Loss made of: CE 0.5150967836380005, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500091552734375 EntMin 0.0
Epoch 5, Class Loss=0.4283408224582672, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.4283408224582672, Class Loss=0.4283408224582672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.3716367959976195
Loss made of: CE 0.5957611799240112, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.63057804107666 EntMin 0.0
Epoch 6, Class Loss=0.5884050726890564, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.5884050726890564, Class Loss=0.5884050726890564, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=9.980323751224205
Loss made of: CE 0.005521903745830059, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.410632133483887 EntMin 0.0
Epoch 1, Class Loss=0.01582483947277069, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.01582483947277069, Class Loss=0.01582483947277069, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=8.06674700602889
Loss made of: CE 0.16567978262901306, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.793420791625977 EntMin 0.0
Epoch 2, Class Loss=0.13374848663806915, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.13374848663806915, Class Loss=0.13374848663806915, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=8.194379878044128
Loss made of: CE 0.2827315926551819, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.8950629234313965 EntMin 0.0
Epoch 3, Class Loss=0.32304471731185913, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.32304471731185913, Class Loss=0.32304471731185913, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=8.032505068182946
Loss made of: CE 0.4112738072872162, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.495706081390381 EntMin 0.0
Epoch 4, Class Loss=0.44664210081100464, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.44664210081100464, Class Loss=0.44664210081100464, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.685489138960838
Loss made of: CE 0.4472627639770508, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.639505386352539 EntMin 0.0
Epoch 5, Class Loss=0.47350865602493286, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.47350865602493286, Class Loss=0.47350865602493286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=7.642554780840873
Loss made of: CE 0.4956604838371277, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.101472854614258 EntMin 0.0
Epoch 6, Class Loss=0.49636849761009216, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.49636849761009216, Class Loss=0.49636849761009216, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=11.3552665412426
Loss made of: CE 0.8797950744628906, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.425153732299805 EntMin 0.0
Epoch 1, Class Loss=0.9230679273605347, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.9230679273605347, Class Loss=0.9230679273605347, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/12, Loss=8.97000584602356
Loss made of: CE 0.7614926099777222, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.696144104003906 EntMin 0.0
Epoch 2, Class Loss=0.6822516322135925, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.6822516322135925, Class Loss=0.6822516322135925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=8.51199796795845
Loss made of: CE 0.5232027769088745, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.8960700035095215 EntMin 0.0
Epoch 3, Class Loss=0.5252406597137451, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.5252406597137451, Class Loss=0.5252406597137451, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/12, Loss=8.129329228401184
Loss made of: CE 0.4495389461517334, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.930287837982178 EntMin 0.0
Epoch 4, Class Loss=0.456989049911499, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.456989049911499, Class Loss=0.456989049911499, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/12, Loss=7.784420937299728
Loss made of: CE 0.40187686681747437, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.914037704467773 EntMin 0.0
Epoch 5, Class Loss=0.4200109839439392, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.4200109839439392, Class Loss=0.4200109839439392, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/12, Loss=7.687918093800545
Loss made of: CE 0.3971370458602905, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.025101661682129 EntMin 0.0
Epoch 6, Class Loss=0.40600913763046265, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.40600913763046265, Class Loss=0.40600913763046265, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7698312997817993, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.794685
Mean Acc: 0.364809
FreqW Acc: 0.661895
Mean IoU: 0.285176
Class IoU:
	class 0: 0.8180162
	class 1: 0.7042389
	class 2: 0.34390172
	class 3: 0.42483366
	class 4: 0.358201
	class 5: 0.56784266
	class 6: 0.3865789
	class 7: 0.78012055
	class 8: 0.4826215
	class 9: 0.0064243465
	class 10: 0.0
	class 11: 1.1044056e-05
	class 12: 0.0342286
	class 13: 0.19245028
	class 14: 0.0
	class 15: 0.56074196
	class 16: 0.0
	class 17: 0.0
	class 18: 0.010465319
	class 19: 0.31802827
	class 20: 0.0
Class Acc:
	class 0: 0.9648622
	class 1: 0.72032624
	class 2: 0.84516656
	class 3: 0.4269252
	class 4: 0.3807815
	class 5: 0.5887062
	class 6: 0.391639
	class 7: 0.83136797
	class 8: 0.49583516
	class 9: 0.006521306
	class 10: 0.0
	class 11: 1.1044139e-05
	class 12: 0.034288537
	class 13: 0.56547487
	class 14: 0.0
	class 15: 0.8705492
	class 16: 0.0
	class 17: 0.0
	class 18: 0.010516475
	class 19: 0.5280162
	class 20: 0.0

federated global round: 34, step: 6
select part of clients to conduct local training
[19, 18, 27, 14]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.184763341397047
Loss made of: CE 0.004172903951257467, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.608510971069336 EntMin 0.0
Epoch 1, Class Loss=0.025056300684809685, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.025056300684809685, Class Loss=0.025056300684809685, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.645305528864265
Loss made of: CE 0.1281651258468628, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8573832511901855 EntMin 0.0
Epoch 2, Class Loss=0.12303740531206131, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.12303740531206131, Class Loss=0.12303740531206131, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.347609905898571
Loss made of: CE 0.284355103969574, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2117156982421875 EntMin 0.0
Epoch 3, Class Loss=0.2404404580593109, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.2404404580593109, Class Loss=0.2404404580593109, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.127421553432941
Loss made of: CE 0.33440345525741577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.943190574645996 EntMin 0.0
Epoch 4, Class Loss=0.34087783098220825, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.34087783098220825, Class Loss=0.34087783098220825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.516195151209831
Loss made of: CE 0.2993507981300354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9158430099487305 EntMin 0.0
Epoch 5, Class Loss=0.4684469699859619, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.4684469699859619, Class Loss=0.4684469699859619, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.490333211421967
Loss made of: CE 0.5714594125747681, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9806971549987793 EntMin 0.0
Epoch 6, Class Loss=0.6496762633323669, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.6496762633323669, Class Loss=0.6496762633323669, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=5.869968657568097
Loss made of: CE 0.026841580867767334, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.669133186340332 EntMin 0.0
Epoch 1, Class Loss=0.022148627787828445, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.022148627787828445, Class Loss=0.022148627787828445, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.09652425237
Loss made of: CE 0.08002131432294846, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.656536102294922 EntMin 0.0
Epoch 2, Class Loss=0.09553217142820358, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.09553217142820358, Class Loss=0.09553217142820358, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.150283263623715
Loss made of: CE 0.18806485831737518, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1273040771484375 EntMin 0.0
Epoch 3, Class Loss=0.20156961679458618, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.20156961679458618, Class Loss=0.20156961679458618, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.019953560829163
Loss made of: CE 0.2880985140800476, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.08483362197876 EntMin 0.0
Epoch 4, Class Loss=0.33086520433425903, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.33086520433425903, Class Loss=0.33086520433425903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.091583341360092
Loss made of: CE 0.5523078441619873, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.430087089538574 EntMin 0.0
Epoch 5, Class Loss=0.4496663808822632, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.4496663808822632, Class Loss=0.4496663808822632, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.236422276496887
Loss made of: CE 0.564469039440155, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.258699417114258 EntMin 0.0
Epoch 6, Class Loss=0.6511855721473694, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.6511855721473694, Class Loss=0.6511855721473694, Reg Loss=0.0
Current Client Index:  27
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.7406995594501495
Loss made of: CE 0.7737781405448914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.51780891418457 EntMin 0.0
Epoch 1, Class Loss=0.8362514972686768, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.8362514972686768, Class Loss=0.8362514972686768, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/13, Loss=5.699970418214798
Loss made of: CE 0.9690505862236023, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0938944816589355 EntMin 0.0
Epoch 2, Class Loss=0.7702559232711792, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.7702559232711792, Class Loss=0.7702559232711792, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/13, Loss=5.568529748916626
Loss made of: CE 0.7285051345825195, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.778041839599609 EntMin 0.0
Epoch 3, Class Loss=0.6954714059829712, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.6954714059829712, Class Loss=0.6954714059829712, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/13, Loss=5.351449447870254
Loss made of: CE 0.600372314453125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.066817760467529 EntMin 0.0
Epoch 4, Class Loss=0.668778657913208, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.668778657913208, Class Loss=0.668778657913208, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/13, Loss=5.347588908672333
Loss made of: CE 0.6001886129379272, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.341551780700684 EntMin 0.0
Epoch 5, Class Loss=0.644257128238678, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.644257128238678, Class Loss=0.644257128238678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/13, Loss=5.201081043481826
Loss made of: CE 0.6169568300247192, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.215513229370117 EntMin 0.0
Epoch 6, Class Loss=0.6298724412918091, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.6298724412918091, Class Loss=0.6298724412918091, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 1, Batch 10/13, Loss=6.8625393271446224
Loss made of: CE 0.9258930087089539, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2138566970825195 EntMin 0.0
Epoch 1, Class Loss=0.8255342841148376, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.8255342841148376, Class Loss=0.8255342841148376, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/13, Loss=5.816396927833557
Loss made of: CE 0.7270309329032898, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.932861804962158 EntMin 0.0
Epoch 2, Class Loss=0.7546366453170776, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.7546366453170776, Class Loss=0.7546366453170776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/13, Loss=5.556322622299194
Loss made of: CE 0.6005910634994507, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.280999183654785 EntMin 0.0
Epoch 3, Class Loss=0.6734631657600403, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.6734631657600403, Class Loss=0.6734631657600403, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/13, Loss=5.352231031656265
Loss made of: CE 0.5578715801239014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.752161026000977 EntMin 0.0
Epoch 4, Class Loss=0.642200767993927, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.642200767993927, Class Loss=0.642200767993927, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/13, Loss=5.310318869352341
Loss made of: CE 0.5831892490386963, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.29953145980835 EntMin 0.0
Epoch 5, Class Loss=0.6361051797866821, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.6361051797866821, Class Loss=0.6361051797866821, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/13, Loss=5.317110008001327
Loss made of: CE 0.7136691808700562, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.210470199584961 EntMin 0.0
Epoch 6, Class Loss=0.6120932698249817, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.6120932698249817, Class Loss=0.6120932698249817, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7732506394386292, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.792315
Mean Acc: 0.359640
FreqW Acc: 0.656266
Mean IoU: 0.286670
Class IoU:
	class 0: 0.81103975
	class 1: 0.74732864
	class 2: 0.34735814
	class 3: 0.45883402
	class 4: 0.35432702
	class 5: 0.56916827
	class 6: 0.3782941
	class 7: 0.75869197
	class 8: 0.5759304
	class 9: 0.005172045
	class 10: 0.0
	class 11: 2.7962818e-05
	class 12: 0.051573675
	class 13: 0.19546421
	class 14: 0.0
	class 15: 0.5539011
	class 16: 0.0
	class 17: 0.0
	class 18: 0.051313024
	class 19: 0.050904132
	class 20: 0.11074581
Class Acc:
	class 0: 0.9669143
	class 1: 0.76932436
	class 2: 0.82145876
	class 3: 0.46426764
	class 4: 0.37283346
	class 5: 0.591114
	class 6: 0.38578463
	class 7: 0.80067253
	class 8: 0.5997801
	class 9: 0.0052647484
	class 10: 0.0
	class 11: 2.7962818e-05
	class 12: 0.05180463
	class 13: 0.57077396
	class 14: 0.0
	class 15: 0.87029606
	class 16: 0.0
	class 17: 0.0
	class 18: 0.05302875
	class 19: 0.05151459
	class 20: 0.17758887

voc_8-2_OURS-FRC On GPUs 1
Run in 81187s
