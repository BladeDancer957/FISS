nohup: ignoring input
35
kvoc_8-2_OURS On GPUs 2\Writing in results/seed_2023-ov/2023-03-10_voc_8-2_OURS.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/72, Loss=1.3020207345485688
Loss made of: CE 0.8987478017807007, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/72, Loss=0.5411725640296936
Loss made of: CE 0.4610385298728943, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/72, Loss=0.377556312084198
Loss made of: CE 0.28634846210479736, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/72, Loss=0.3712794542312622
Loss made of: CE 0.32222461700439453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/72, Loss=0.3464391216635704
Loss made of: CE 0.35832130908966064, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/72, Loss=0.28717023730278013
Loss made of: CE 0.28399842977523804, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/72, Loss=0.2887910231947899
Loss made of: CE 0.333545982837677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.49583789706230164, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.49583789706230164, Class Loss=0.49583789706230164, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/72, Loss=0.24646605253219606
Loss made of: CE 0.21649491786956787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/72, Loss=0.25235511660575866
Loss made of: CE 0.21362535655498505, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/72, Loss=0.2589910373091698
Loss made of: CE 0.148628368973732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/72, Loss=0.22609985172748565
Loss made of: CE 0.14885008335113525, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/72, Loss=0.22340201884508132
Loss made of: CE 0.26891273260116577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/72, Loss=0.2204062283039093
Loss made of: CE 0.35458409786224365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/72, Loss=0.20755159705877305
Loss made of: CE 0.14693230390548706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.23256836831569672, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.23256836831569672, Class Loss=0.23256836831569672, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/72, Loss=0.1953299880027771
Loss made of: CE 0.22987544536590576, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/72, Loss=0.17804139107465744
Loss made of: CE 0.25563788414001465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/72, Loss=0.1785459041595459
Loss made of: CE 0.15922611951828003, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/72, Loss=0.13953210785984993
Loss made of: CE 0.1475963592529297, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/72, Loss=0.15820334255695342
Loss made of: CE 0.16689887642860413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/72, Loss=0.18994830399751664
Loss made of: CE 0.2068529725074768, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/72, Loss=0.16255007535219193
Loss made of: CE 0.15564388036727905, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.17343123257160187, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.17343123257160187, Class Loss=0.17343123257160187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/72, Loss=0.15861270874738692
Loss made of: CE 0.12294067442417145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/72, Loss=0.1414895161986351
Loss made of: CE 0.1197187602519989, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/72, Loss=0.13649171218276024
Loss made of: CE 0.19877231121063232, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/72, Loss=0.15717637687921523
Loss made of: CE 0.17350956797599792, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/72, Loss=0.1401928186416626
Loss made of: CE 0.14104685187339783, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/72, Loss=0.15061841607093812
Loss made of: CE 0.14643578231334686, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/72, Loss=0.13579903915524483
Loss made of: CE 0.10654187202453613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1461060792207718, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.1461060792207718, Class Loss=0.1461060792207718, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/72, Loss=0.14916369765996934
Loss made of: CE 0.13398027420043945, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/72, Loss=0.14097552374005318
Loss made of: CE 0.20432059466838837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/72, Loss=0.12066498994827271
Loss made of: CE 0.1536472737789154, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/72, Loss=0.1324765093624592
Loss made of: CE 0.1437951922416687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/72, Loss=0.12712234109640122
Loss made of: CE 0.13650165498256683, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/72, Loss=0.11009285971522331
Loss made of: CE 0.13316795229911804, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/72, Loss=0.11313341483473778
Loss made of: CE 0.1617698073387146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.12693923711776733, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.12693923711776733, Class Loss=0.12693923711776733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/72, Loss=0.11840374544262885
Loss made of: CE 0.13998201489448547, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/72, Loss=0.10177802816033363
Loss made of: CE 0.10351451486349106, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/72, Loss=0.1200393371284008
Loss made of: CE 0.17392104864120483, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/72, Loss=0.09831836745142937
Loss made of: CE 0.12441688776016235, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/72, Loss=0.12021870240569114
Loss made of: CE 0.07859410345554352, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/72, Loss=0.11029552221298218
Loss made of: CE 0.08418433368206024, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/72, Loss=0.11822293922305108
Loss made of: CE 0.16350716352462769, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11410214006900787, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.11410214006900787, Class Loss=0.11410214006900787, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/50, Loss=1.1840614199638366
Loss made of: CE 1.0328986644744873, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/50, Loss=0.6809949100017547
Loss made of: CE 0.5060256719589233, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/50, Loss=0.45724990367889407
Loss made of: CE 0.43685078620910645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/50, Loss=0.3297213464975357
Loss made of: CE 0.33297473192214966, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/50, Loss=0.28324148505926133
Loss made of: CE 0.29547449946403503, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5870537757873535, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.5870537757873535, Class Loss=0.5870537757873535, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/50, Loss=0.24009070843458175
Loss made of: CE 0.21827048063278198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/50, Loss=0.24600410908460618
Loss made of: CE 0.26633989810943604, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/50, Loss=0.21620708405971528
Loss made of: CE 0.22859594225883484, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/50, Loss=0.2517596334218979
Loss made of: CE 0.2869441509246826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/50, Loss=0.2125042662024498
Loss made of: CE 0.22981595993041992, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2333131581544876, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.2333131581544876, Class Loss=0.2333131581544876, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/50, Loss=0.21335557252168655
Loss made of: CE 0.13703840970993042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/50, Loss=0.2138110876083374
Loss made of: CE 0.29885363578796387, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/50, Loss=0.18693871796131134
Loss made of: CE 0.19656924903392792, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/50, Loss=0.1712484046816826
Loss made of: CE 0.15713462233543396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/50, Loss=0.17863057553768158
Loss made of: CE 0.19508081674575806, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.19279687106609344, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.19279687106609344, Class Loss=0.19279687106609344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/50, Loss=0.14056392163038253
Loss made of: CE 0.1173776388168335, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/50, Loss=0.16803669780492783
Loss made of: CE 0.1942530870437622, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/50, Loss=0.14937382191419601
Loss made of: CE 0.12703858315944672, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/50, Loss=0.16110781133174895
Loss made of: CE 0.25313684344291687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/50, Loss=0.15788236111402512
Loss made of: CE 0.17563065886497498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.15539291501045227, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.15539291501045227, Class Loss=0.15539291501045227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/50, Loss=0.13706009536981584
Loss made of: CE 0.1341838538646698, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/50, Loss=0.16154911741614342
Loss made of: CE 0.11137046664953232, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/50, Loss=0.16061076372861863
Loss made of: CE 0.10647260397672653, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/50, Loss=0.150844007730484
Loss made of: CE 0.12337937206029892, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/50, Loss=0.14226854369044303
Loss made of: CE 0.14598172903060913, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.15046650171279907, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.15046650171279907, Class Loss=0.15046650171279907, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/50, Loss=0.14136925637722014
Loss made of: CE 0.1061151772737503, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/50, Loss=0.11439128816127778
Loss made of: CE 0.11059757322072983, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/50, Loss=0.11836634948849678
Loss made of: CE 0.09964349865913391, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/50, Loss=0.12653062343597413
Loss made of: CE 0.1219206377863884, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/50, Loss=0.11769141182303429
Loss made of: CE 0.11570720374584198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.12366978079080582, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.12366978079080582, Class Loss=0.12366978079080582, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/61, Loss=1.226012122631073
Loss made of: CE 0.7409927248954773, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.5967216700315475
Loss made of: CE 0.5655871629714966, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.4065710484981537
Loss made of: CE 0.4431188106536865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.3558867245912552
Loss made of: CE 0.31985872983932495, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.27782005071640015
Loss made of: CE 0.29710590839385986, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.2963153898715973
Loss made of: CE 0.22532199323177338, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5213599801063538, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5213599801063538, Class Loss=0.5213599801063538, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/61, Loss=0.23421688973903657
Loss made of: CE 0.2461366504430771, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.2175990089774132
Loss made of: CE 0.16797831654548645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.20275120735168456
Loss made of: CE 0.2745482921600342, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.24951785057783127
Loss made of: CE 0.1603742241859436, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.28708834797143934
Loss made of: CE 0.2826699912548065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.20682644695043564
Loss made of: CE 0.14855729043483734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.23240207135677338, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.23240207135677338, Class Loss=0.23240207135677338, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/61, Loss=0.1988751545548439
Loss made of: CE 0.1565779745578766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.17336354851722718
Loss made of: CE 0.13288573920726776, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.18570101857185364
Loss made of: CE 0.15378910303115845, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.16814620941877365
Loss made of: CE 0.1304207742214203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.17346473932266235
Loss made of: CE 0.13111074268817902, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.17355718463659286
Loss made of: CE 0.18190133571624756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.17830701172351837, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.17830701172351837, Class Loss=0.17830701172351837, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/61, Loss=0.15352323576807975
Loss made of: CE 0.1291089951992035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.14564454406499863
Loss made of: CE 0.15807382762432098, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.17159035429358482
Loss made of: CE 0.3966836929321289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.15558848083019255
Loss made of: CE 0.11567972600460052, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.14322393983602524
Loss made of: CE 0.1233573779463768, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.15432771891355515
Loss made of: CE 0.15010812878608704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.15448640286922455, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.15448640286922455, Class Loss=0.15448640286922455, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/61, Loss=0.12668702155351638
Loss made of: CE 0.12110638618469238, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.12983916625380515
Loss made of: CE 0.10789113491773605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.11612258628010749
Loss made of: CE 0.12592348456382751, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.14243152365088463
Loss made of: CE 0.11192578077316284, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.11172234043478965
Loss made of: CE 0.11814703792333603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.1410652741789818
Loss made of: CE 0.15112565457820892, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1275225132703781, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.1275225132703781, Class Loss=0.1275225132703781, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/61, Loss=0.11883592158555985
Loss made of: CE 0.15277695655822754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.1155497632920742
Loss made of: CE 0.09365224838256836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.1129994347691536
Loss made of: CE 0.09080257266759872, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.12019893154501915
Loss made of: CE 0.20467719435691833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.10348488315939904
Loss made of: CE 0.10396692156791687, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.12533633336424826
Loss made of: CE 0.11995033919811249, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11607853323221207, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.11607853323221207, Class Loss=0.11607853323221207, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/66, Loss=1.281375128030777
Loss made of: CE 0.7191349864006042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/66, Loss=0.6174070060253143
Loss made of: CE 0.4498897194862366, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/66, Loss=0.4106332629919052
Loss made of: CE 0.2912873327732086, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/66, Loss=0.3022590011358261
Loss made of: CE 0.3000212609767914, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/66, Loss=0.2870503574609756
Loss made of: CE 0.2625423073768616, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/66, Loss=0.29243848025798796
Loss made of: CE 0.23877237737178802, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5071637034416199, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.5071637034416199, Class Loss=0.5071637034416199, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/66, Loss=0.23470670878887176
Loss made of: CE 0.24198755621910095, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/66, Loss=0.22852005064487457
Loss made of: CE 0.26449155807495117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/66, Loss=0.2227116569876671
Loss made of: CE 0.14797665178775787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/66, Loss=0.20202362388372422
Loss made of: CE 0.21462981402873993, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/66, Loss=0.18172533959150314
Loss made of: CE 0.15899662673473358, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/66, Loss=0.19139690473675727
Loss made of: CE 0.1593950390815735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.21005798876285553, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.21005798876285553, Class Loss=0.21005798876285553, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/66, Loss=0.17823184430599212
Loss made of: CE 0.12400016188621521, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/66, Loss=0.156765628606081
Loss made of: CE 0.1713147610425949, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/66, Loss=0.19723594784736634
Loss made of: CE 0.13863739371299744, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/66, Loss=0.17883224189281463
Loss made of: CE 0.13660210371017456, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/66, Loss=0.1775865897536278
Loss made of: CE 0.21143369376659393, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/66, Loss=0.15313291624188424
Loss made of: CE 0.13095919787883759, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1722627878189087, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.1722627878189087, Class Loss=0.1722627878189087, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/66, Loss=0.140888112783432
Loss made of: CE 0.09895671904087067, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/66, Loss=0.1267336942255497
Loss made of: CE 0.13822975754737854, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/66, Loss=0.15240752846002578
Loss made of: CE 0.11941276490688324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/66, Loss=0.14258983135223388
Loss made of: CE 0.11078616231679916, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/66, Loss=0.14024286419153215
Loss made of: CE 0.17228174209594727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/66, Loss=0.17835215479135513
Loss made of: CE 0.28289541602134705, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.14571532607078552, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.14571532607078552, Class Loss=0.14571532607078552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/66, Loss=0.13346030414104462
Loss made of: CE 0.181417316198349, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/66, Loss=0.1325116991996765
Loss made of: CE 0.1468788981437683, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/66, Loss=0.11790925338864326
Loss made of: CE 0.11080776154994965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/66, Loss=0.12685473933815955
Loss made of: CE 0.08753868192434311, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/66, Loss=0.15418895706534386
Loss made of: CE 0.20018593966960907, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/66, Loss=0.10424945950508117
Loss made of: CE 0.13471710681915283, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1267394721508026, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.1267394721508026, Class Loss=0.1267394721508026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/66, Loss=0.10738816037774086
Loss made of: CE 0.10453155636787415, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/66, Loss=0.10541312173008918
Loss made of: CE 0.07297393679618835, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/66, Loss=0.11849133297801018
Loss made of: CE 0.1170179694890976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/66, Loss=0.1017291009426117
Loss made of: CE 0.06920088827610016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/66, Loss=0.11549031734466553
Loss made of: CE 0.09509044140577316, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/66, Loss=0.10377318039536476
Loss made of: CE 0.13632924854755402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10685178637504578, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.10685178637504578, Class Loss=0.10685178637504578, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.29798623919487, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.893331
Mean Acc: 0.424357
FreqW Acc: 0.800098
Mean IoU: 0.388390
Class IoU:
	class 0: 0.8882238
	class 1: 0.21495773
	class 2: 0.007118635
	class 3: 0.011798785
	class 4: 0.12962909
	class 5: 0.0
	class 6: 0.826702
	class 7: 0.6813865
	class 8: 0.73569596
Class Acc:
	class 0: 0.9949935
	class 1: 0.2150124
	class 2: 0.007220289
	class 3: 0.011798785
	class 4: 0.1297501
	class 5: 0.0
	class 6: 0.9613731
	class 7: 0.6989387
	class 8: 0.8001234

federated global round: 1, step: 0
select part of clients to conduct local training
[4, 3, 1, 2]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/60, Loss=0.380925165116787
Loss made of: CE 0.3408604562282562, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/60, Loss=0.30432352870702745
Loss made of: CE 0.19871191680431366, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/60, Loss=0.2604466944932938
Loss made of: CE 0.23392483592033386, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/60, Loss=0.19697358459234238
Loss made of: CE 0.21616320312023163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/60, Loss=0.22978490218520164
Loss made of: CE 0.20837390422821045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/60, Loss=0.1834544226527214
Loss made of: CE 0.13025102019309998, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2593180537223816, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.2593180537223816, Class Loss=0.2593180537223816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/60, Loss=0.15606024116277695
Loss made of: CE 0.12342952191829681, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/60, Loss=0.18219924122095107
Loss made of: CE 0.16123491525650024, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/60, Loss=0.15126967206597328
Loss made of: CE 0.10993900150060654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/60, Loss=0.1624025397002697
Loss made of: CE 0.17588290572166443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/60, Loss=0.16981668323278426
Loss made of: CE 0.12620726227760315, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/60, Loss=0.1727891020476818
Loss made of: CE 0.11959075182676315, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1657562553882599, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.1657562553882599, Class Loss=0.1657562553882599, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/60, Loss=0.15217601880431175
Loss made of: CE 0.11536340415477753, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/60, Loss=0.1269220769405365
Loss made of: CE 0.10211127996444702, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/60, Loss=0.11794089004397393
Loss made of: CE 0.09585364907979965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/60, Loss=0.13220368176698685
Loss made of: CE 0.11109023541212082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/60, Loss=0.12515265122056007
Loss made of: CE 0.12802866101264954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/60, Loss=0.1261891558766365
Loss made of: CE 0.08217448741197586, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1300974190235138, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.1300974190235138, Class Loss=0.1300974190235138, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/60, Loss=0.11270618960261344
Loss made of: CE 0.1460893750190735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/60, Loss=0.10607703104615211
Loss made of: CE 0.09400314092636108, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/60, Loss=0.10405466184020043
Loss made of: CE 0.10289113223552704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/60, Loss=0.09608036428689956
Loss made of: CE 0.10588487982749939, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/60, Loss=0.11331687420606613
Loss made of: CE 0.08474868535995483, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/60, Loss=0.10677244663238525
Loss made of: CE 0.19037938117980957, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1065012663602829, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.1065012663602829, Class Loss=0.1065012663602829, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/60, Loss=0.099258641153574
Loss made of: CE 0.07431809604167938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/60, Loss=0.10015221312642097
Loss made of: CE 0.08391131460666656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/60, Loss=0.11689717099070548
Loss made of: CE 0.10982093214988708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/60, Loss=0.0984546609222889
Loss made of: CE 0.09787626564502716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/60, Loss=0.10466987043619155
Loss made of: CE 0.08417051285505295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/60, Loss=0.10252479687333108
Loss made of: CE 0.1453772485256195, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10365956276655197, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.10365956276655197, Class Loss=0.10365956276655197, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/60, Loss=0.10132487639784812
Loss made of: CE 0.06265968084335327, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/60, Loss=0.09614643752574921
Loss made of: CE 0.08186371624469757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/60, Loss=0.0928416095674038
Loss made of: CE 0.09947912395000458, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/60, Loss=0.08447360694408416
Loss made of: CE 0.07863867282867432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/60, Loss=0.08819539807736873
Loss made of: CE 0.1009804755449295, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/60, Loss=0.07896484583616256
Loss made of: CE 0.09434272348880768, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09032446891069412, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.09032446891069412, Class Loss=0.09032446891069412, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/61, Loss=0.4367165207862854
Loss made of: CE 0.272381454706192, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.23709281980991365
Loss made of: CE 0.23181679844856262, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.17503515779972076
Loss made of: CE 0.2878972291946411, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.20393034517765046
Loss made of: CE 0.21504780650138855, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.18932912051677703
Loss made of: CE 0.15392401814460754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.18793324530124664
Loss made of: CE 0.1802351325750351, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2373751401901245, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.2373751401901245, Class Loss=0.2373751401901245, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/61, Loss=0.18000119253993035
Loss made of: CE 0.2167850136756897, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.18648734986782073
Loss made of: CE 0.27367982268333435, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.16275613903999328
Loss made of: CE 0.11811940371990204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.14338255748152734
Loss made of: CE 0.14426420629024506, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.15983909144997596
Loss made of: CE 0.1230119988322258, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.14841470792889594
Loss made of: CE 0.1415504813194275, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.16221635043621063, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.16221635043621063, Class Loss=0.16221635043621063, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/61, Loss=0.14349391236901282
Loss made of: CE 0.12676580250263214, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.13139353841543197
Loss made of: CE 0.138514444231987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.12758429124951362
Loss made of: CE 0.1291740983724594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.1101935364305973
Loss made of: CE 0.15074723958969116, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.1281610682606697
Loss made of: CE 0.12045305967330933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.13948474451899529
Loss made of: CE 0.08116554468870163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.12982958555221558, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.12982958555221558, Class Loss=0.12982958555221558, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/61, Loss=0.11657259985804558
Loss made of: CE 0.1109679639339447, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.12616100013256074
Loss made of: CE 0.11817435920238495, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.09658192619681358
Loss made of: CE 0.102680504322052, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.10082930400967598
Loss made of: CE 0.0935162901878357, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.10176932737231255
Loss made of: CE 0.0696740671992302, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.10203289464116097
Loss made of: CE 0.12893809378147125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10681454837322235, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.10681454837322235, Class Loss=0.10681454837322235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/61, Loss=0.08950444310903549
Loss made of: CE 0.07797524333000183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.08788455985486507
Loss made of: CE 0.07309482991695404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.08936356529593467
Loss made of: CE 0.0631943941116333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.09193936437368393
Loss made of: CE 0.059644222259521484, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.09449057802557945
Loss made of: CE 0.06829579919576645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.10298012271523475
Loss made of: CE 0.12662506103515625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09236860275268555, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.09236860275268555, Class Loss=0.09236860275268555, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/61, Loss=0.08828528076410294
Loss made of: CE 0.11984696984291077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.09716042764484882
Loss made of: CE 0.1577092707157135, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.08541769906878471
Loss made of: CE 0.08444713801145554, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.08294387757778168
Loss made of: CE 0.07903029769659042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.08935881927609443
Loss made of: CE 0.09304343909025192, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.09228835105895997
Loss made of: CE 0.0637427493929863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08931268751621246, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.08931268751621246, Class Loss=0.08931268751621246, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/83, Loss=0.2822529494762421
Loss made of: CE 0.2507568597793579, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/83, Loss=0.22425316423177719
Loss made of: CE 0.1770864725112915, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/83, Loss=0.1595267489552498
Loss made of: CE 0.1630193442106247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/83, Loss=0.19103444516658782
Loss made of: CE 0.28610384464263916, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/83, Loss=0.17670630812644958
Loss made of: CE 0.11644929647445679, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/83, Loss=0.1346732795238495
Loss made of: CE 0.15807008743286133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/83, Loss=0.19069726318120955
Loss made of: CE 0.12540274858474731, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/83, Loss=0.18703992515802384
Loss made of: CE 0.1686193197965622, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1946735829114914, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.1946735829114914, Class Loss=0.1946735829114914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/83, Loss=0.1445040561258793
Loss made of: CE 0.09936922043561935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/83, Loss=0.1312183514237404
Loss made of: CE 0.1502317190170288, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/83, Loss=0.1241841197013855
Loss made of: CE 0.15279093384742737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/83, Loss=0.13009903877973555
Loss made of: CE 0.0923924669623375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/83, Loss=0.14020342975854874
Loss made of: CE 0.12799029052257538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/83, Loss=0.12590848430991172
Loss made of: CE 0.09671050310134888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/83, Loss=0.13326268494129181
Loss made of: CE 0.14250192046165466, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/83, Loss=0.12003197818994522
Loss made of: CE 0.13467378914356232, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1304667592048645, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.1304667592048645, Class Loss=0.1304667592048645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/83, Loss=0.09959263280034066
Loss made of: CE 0.0994662493467331, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/83, Loss=0.11790861561894417
Loss made of: CE 0.08391254395246506, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/83, Loss=0.11995868161320686
Loss made of: CE 0.15744084119796753, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/83, Loss=0.11206470876932144
Loss made of: CE 0.10739395022392273, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/83, Loss=0.1175144523382187
Loss made of: CE 0.13039427995681763, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/83, Loss=0.10582150891423225
Loss made of: CE 0.09463994204998016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/83, Loss=0.0966921828687191
Loss made of: CE 0.09401889890432358, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/83, Loss=0.09872748777270317
Loss made of: CE 0.10858901590108871, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10814502090215683, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.10814502090215683, Class Loss=0.10814502090215683, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/83, Loss=0.10688481107354164
Loss made of: CE 0.07510842382907867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/83, Loss=0.08803203776478767
Loss made of: CE 0.10521210730075836, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/83, Loss=0.08562777303159237
Loss made of: CE 0.07848198711872101, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/83, Loss=0.09175674170255661
Loss made of: CE 0.06688180565834045, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/83, Loss=0.08473709002137184
Loss made of: CE 0.08545254170894623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/83, Loss=0.0858180582523346
Loss made of: CE 0.09186860173940659, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/83, Loss=0.10042734742164612
Loss made of: CE 0.06184988468885422, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/83, Loss=0.09319544211030006
Loss made of: CE 0.10453831404447556, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09137555211782455, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.09137555211782455, Class Loss=0.09137555211782455, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/83, Loss=0.08273277208209037
Loss made of: CE 0.07361353933811188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/83, Loss=0.08865153938531875
Loss made of: CE 0.13980871438980103, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/83, Loss=0.07730569392442703
Loss made of: CE 0.07663517445325851, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/83, Loss=0.09175575375556946
Loss made of: CE 0.0924261063337326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/83, Loss=0.08635982498526573
Loss made of: CE 0.10642185062170029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/83, Loss=0.09646219797432423
Loss made of: CE 0.11538207530975342, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/83, Loss=0.08226270899176598
Loss made of: CE 0.08317768573760986, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/83, Loss=0.07888521477580071
Loss made of: CE 0.08615823835134506, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08569391071796417, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.08569391071796417, Class Loss=0.08569391071796417, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/83, Loss=0.07135936245322227
Loss made of: CE 0.07216163724660873, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/83, Loss=0.0891762338578701
Loss made of: CE 0.16475751996040344, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/83, Loss=0.0822355329990387
Loss made of: CE 0.07677922397851944, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/83, Loss=0.08597745560109615
Loss made of: CE 0.07958139479160309, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/83, Loss=0.06969759799540043
Loss made of: CE 0.06790421158075333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/83, Loss=0.07377838082611561
Loss made of: CE 0.08848290145397186, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/83, Loss=0.10747957043349743
Loss made of: CE 0.09241120517253876, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/83, Loss=0.08381894677877426
Loss made of: CE 0.10863511264324188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.082964688539505, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.082964688539505, Class Loss=0.082964688539505, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/66, Loss=0.27833977043628694
Loss made of: CE 0.1466359943151474, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/66, Loss=0.1550473727285862
Loss made of: CE 0.13514208793640137, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/66, Loss=0.1420198604464531
Loss made of: CE 0.11444026231765747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/66, Loss=0.13960978984832764
Loss made of: CE 0.133700892329216, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/66, Loss=0.14486611187458037
Loss made of: CE 0.13610127568244934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/66, Loss=0.14627951011061668
Loss made of: CE 0.11985896527767181, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.16493499279022217, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.16493499279022217, Class Loss=0.16493499279022217, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/66, Loss=0.1218855433166027
Loss made of: CE 0.13330785930156708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/66, Loss=0.1255797952413559
Loss made of: CE 0.1395500898361206, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/66, Loss=0.11519356295466424
Loss made of: CE 0.08904360234737396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/66, Loss=0.1106051929295063
Loss made of: CE 0.09935426712036133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/66, Loss=0.10045208483934402
Loss made of: CE 0.09349755942821503, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/66, Loss=0.10833425968885421
Loss made of: CE 0.10313113033771515, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.11309666186571121, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.11309666186571121, Class Loss=0.11309666186571121, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/66, Loss=0.09307999983429908
Loss made of: CE 0.07090692222118378, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/66, Loss=0.09134054556488991
Loss made of: CE 0.10647086799144745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/66, Loss=0.10818996652960777
Loss made of: CE 0.0843045562505722, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/66, Loss=0.09474849849939346
Loss made of: CE 0.07855004072189331, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/66, Loss=0.10069969072937965
Loss made of: CE 0.1350497454404831, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/66, Loss=0.09104536399245262
Loss made of: CE 0.08018145710229874, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.0961189866065979, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.0961189866065979, Class Loss=0.0961189866065979, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/66, Loss=0.07728995159268379
Loss made of: CE 0.06286601722240448, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/66, Loss=0.0779182467609644
Loss made of: CE 0.08338427543640137, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/66, Loss=0.08413176015019416
Loss made of: CE 0.07495788484811783, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/66, Loss=0.09021338373422623
Loss made of: CE 0.08105511218309402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/66, Loss=0.08489084988832474
Loss made of: CE 0.11169754713773727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/66, Loss=0.11077893897891045
Loss made of: CE 0.16395919024944305, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.0872005820274353, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.0872005820274353, Class Loss=0.0872005820274353, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/66, Loss=0.08489025011658669
Loss made of: CE 0.1009795293211937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/66, Loss=0.080795156955719
Loss made of: CE 0.062193289399147034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/66, Loss=0.07412930950522423
Loss made of: CE 0.06862657517194748, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/66, Loss=0.0835871733725071
Loss made of: CE 0.06552907824516296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/66, Loss=0.09173432216048241
Loss made of: CE 0.1054418534040451, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/66, Loss=0.06710602268576622
Loss made of: CE 0.06897732615470886, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08058284968137741, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.08058284968137741, Class Loss=0.08058284968137741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/66, Loss=0.07613777182996273
Loss made of: CE 0.07248704135417938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/66, Loss=0.07266144342720508
Loss made of: CE 0.06088836491107941, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/66, Loss=0.07382187321782112
Loss made of: CE 0.06105621159076691, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/66, Loss=0.06743928715586663
Loss made of: CE 0.053902797400951385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/66, Loss=0.08030364513397217
Loss made of: CE 0.07244344800710678, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/66, Loss=0.06948998495936394
Loss made of: CE 0.09233888238668442, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07311266660690308, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.07311266660690308, Class Loss=0.07311266660690308, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.17415843904018402, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.941250
Mean Acc: 0.774570
FreqW Acc: 0.892969
Mean IoU: 0.697052
Class IoU:
	class 0: 0.92999256
	class 1: 0.85940427
	class 2: 0.32978928
	class 3: 0.82153565
	class 4: 0.6892799
	class 5: 0.00128321
	class 6: 0.93027556
	class 7: 0.8429976
	class 8: 0.86890876
Class Acc:
	class 0: 0.97783583
	class 1: 0.92073613
	class 2: 0.7058954
	class 3: 0.8419589
	class 4: 0.74501485
	class 5: 0.00128321
	class 6: 0.9676101
	class 7: 0.9281002
	class 8: 0.8826909

federated global round: 2, step: 0
select part of clients to conduct local training
[0, 4, 7, 2]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/57, Loss=0.22103683575987815
Loss made of: CE 0.10850545018911362, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/57, Loss=0.16350308582186698
Loss made of: CE 0.22292129695415497, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/57, Loss=0.165792615711689
Loss made of: CE 0.19157223403453827, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/57, Loss=0.1368485264480114
Loss made of: CE 0.12449614703655243, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/57, Loss=0.13807545304298402
Loss made of: CE 0.1598563939332962, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.164710134267807, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.164710134267807, Class Loss=0.164710134267807, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/57, Loss=0.15474829077720642
Loss made of: CE 0.1360185742378235, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/57, Loss=0.14120573550462723
Loss made of: CE 0.12552298605442047, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/57, Loss=0.1212842583656311
Loss made of: CE 0.13518451154232025, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/57, Loss=0.1255221240222454
Loss made of: CE 0.18228904902935028, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/57, Loss=0.12876393944025039
Loss made of: CE 0.13177810609340668, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.13194528222084045, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.13194528222084045, Class Loss=0.13194528222084045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/57, Loss=0.10532635748386383
Loss made of: CE 0.10759066045284271, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/57, Loss=0.11208255141973496
Loss made of: CE 0.07672201842069626, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/57, Loss=0.11645712479948997
Loss made of: CE 0.11847587674856186, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/57, Loss=0.10407861173152924
Loss made of: CE 0.10031263530254364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/57, Loss=0.10838541910052299
Loss made of: CE 0.12567614018917084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10702705383300781, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.10702705383300781, Class Loss=0.10702705383300781, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/57, Loss=0.09420832023024558
Loss made of: CE 0.10529794543981552, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/57, Loss=0.09438444375991821
Loss made of: CE 0.08099018037319183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/57, Loss=0.10236257687211037
Loss made of: CE 0.07028618454933167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/57, Loss=0.10215052850544452
Loss made of: CE 0.12367425858974457, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/57, Loss=0.09515802562236786
Loss made of: CE 0.11528794467449188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09686259925365448, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.09686259925365448, Class Loss=0.09686259925365448, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/57, Loss=0.0936032947152853
Loss made of: CE 0.09891991317272186, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/57, Loss=0.0916722759604454
Loss made of: CE 0.10209979861974716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/57, Loss=0.09194733425974846
Loss made of: CE 0.09235527366399765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/57, Loss=0.07719187214970588
Loss made of: CE 0.0792580246925354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/57, Loss=0.0883098267018795
Loss made of: CE 0.11014073342084885, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08830471336841583, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.08830471336841583, Class Loss=0.08830471336841583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/57, Loss=0.08903942108154297
Loss made of: CE 0.07251206040382385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/57, Loss=0.09361849203705788
Loss made of: CE 0.07725213468074799, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/57, Loss=0.09065109193325042
Loss made of: CE 0.09093930572271347, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/57, Loss=0.0807304535061121
Loss made of: CE 0.06826210021972656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/57, Loss=0.09412723779678345
Loss made of: CE 0.11815926432609558, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08971057087182999, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.08971057087182999, Class Loss=0.08971057087182999, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/60, Loss=0.19133891239762307
Loss made of: CE 0.1934872716665268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/60, Loss=0.16692857071757317
Loss made of: CE 0.08842115104198456, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/60, Loss=0.13538939952850343
Loss made of: CE 0.11040618270635605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/60, Loss=0.1173875629901886
Loss made of: CE 0.1439911127090454, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/60, Loss=0.11924732849001884
Loss made of: CE 0.11923684924840927, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/60, Loss=0.11881304010748864
Loss made of: CE 0.11852607131004333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.14151747524738312, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.14151747524738312, Class Loss=0.14151747524738312, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007332
Epoch 2, Batch 10/60, Loss=0.1013217143714428
Loss made of: CE 0.08559101819992065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/60, Loss=0.12067877948284149
Loss made of: CE 0.1037982627749443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/60, Loss=0.09179272502660751
Loss made of: CE 0.06185610592365265, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/60, Loss=0.1111191265285015
Loss made of: CE 0.12491508573293686, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/60, Loss=0.10470917820930481
Loss made of: CE 0.0844348669052124, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/60, Loss=0.10641954094171524
Loss made of: CE 0.09178483486175537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10600685328245163, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.10600685328245163, Class Loss=0.10600685328245163, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.006943
Epoch 3, Batch 10/60, Loss=0.09918665885925293
Loss made of: CE 0.09786378592252731, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/60, Loss=0.08838444501161576
Loss made of: CE 0.061974577605724335, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/60, Loss=0.09446815624833108
Loss made of: CE 0.06952793896198273, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/60, Loss=0.0991397850215435
Loss made of: CE 0.07433589547872543, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/60, Loss=0.08852918073534966
Loss made of: CE 0.09056176990270615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/60, Loss=0.0960877574980259
Loss made of: CE 0.06500285863876343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09429933875799179, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.09429933875799179, Class Loss=0.09429933875799179, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.006551
Epoch 4, Batch 10/60, Loss=0.08314392045140266
Loss made of: CE 0.09671196341514587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/60, Loss=0.08085297793149948
Loss made of: CE 0.05826764926314354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/60, Loss=0.07849746122956276
Loss made of: CE 0.08595126867294312, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/60, Loss=0.07632518336176872
Loss made of: CE 0.07064035534858704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/60, Loss=0.08351547196507454
Loss made of: CE 0.06760749220848083, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/60, Loss=0.08104771077632904
Loss made of: CE 0.13580933213233948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08056379109621048, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.08056379109621048, Class Loss=0.08056379109621048, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006156
Epoch 5, Batch 10/60, Loss=0.07089416608214379
Loss made of: CE 0.06300650537014008, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/60, Loss=0.07583204060792922
Loss made of: CE 0.07545076310634613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/60, Loss=0.08646157123148442
Loss made of: CE 0.07277078926563263, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/60, Loss=0.07329676076769828
Loss made of: CE 0.06696373224258423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/60, Loss=0.08930064514279365
Loss made of: CE 0.06952106207609177, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/60, Loss=0.0773853249847889
Loss made of: CE 0.1173865795135498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07886175811290741, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.07886175811290741, Class Loss=0.07886175811290741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.005759
Epoch 6, Batch 10/60, Loss=0.08109226189553738
Loss made of: CE 0.04701734706759453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/60, Loss=0.0754404529929161
Loss made of: CE 0.05154766887426376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/60, Loss=0.0802665263414383
Loss made of: CE 0.07412432134151459, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/60, Loss=0.06929685845971108
Loss made of: CE 0.05784034729003906, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/60, Loss=0.08245779424905778
Loss made of: CE 0.08524277806282043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/60, Loss=0.06493433341383933
Loss made of: CE 0.07135723531246185, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0755813717842102, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.0755813717842102, Class Loss=0.0755813717842102, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/50, Loss=0.18130969107151032
Loss made of: CE 0.18895605206489563, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/50, Loss=0.18292145282030106
Loss made of: CE 0.1225232481956482, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/50, Loss=0.14431342333555222
Loss made of: CE 0.19122259318828583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/50, Loss=0.12982557862997054
Loss made of: CE 0.16787096858024597, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/50, Loss=0.12969568893313407
Loss made of: CE 0.12512275576591492, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1536131650209427, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.1536131650209427, Class Loss=0.1536131650209427, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007770
Epoch 2, Batch 10/50, Loss=0.1105675607919693
Loss made of: CE 0.10357245057821274, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/50, Loss=0.12222065404057503
Loss made of: CE 0.1255643218755722, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/50, Loss=0.10454768612980843
Loss made of: CE 0.11417102068662643, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/50, Loss=0.12072481364011764
Loss made of: CE 0.17047332227230072, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/50, Loss=0.10613516122102737
Loss made of: CE 0.12696900963783264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.11283916980028152, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.11283916980028152, Class Loss=0.11283916980028152, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.007358
Epoch 3, Batch 10/50, Loss=0.10639994069933892
Loss made of: CE 0.0769796073436737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/50, Loss=0.10954287946224213
Loss made of: CE 0.15108555555343628, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/50, Loss=0.09868608564138412
Loss made of: CE 0.09553947299718857, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/50, Loss=0.08889496028423309
Loss made of: CE 0.08490550518035889, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/50, Loss=0.09781468734145164
Loss made of: CE 0.11332392692565918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10026771575212479, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.10026771575212479, Class Loss=0.10026771575212479, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.006943
Epoch 4, Batch 10/50, Loss=0.08258484080433845
Loss made of: CE 0.05601736903190613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/50, Loss=0.09918950200080871
Loss made of: CE 0.11822327971458435, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/50, Loss=0.08553655482828618
Loss made of: CE 0.0843397006392479, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/50, Loss=0.09221611768007279
Loss made of: CE 0.08913512527942657, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/50, Loss=0.09226867333054542
Loss made of: CE 0.08728092908859253, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09035912901163101, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.09035912901163101, Class Loss=0.09035912901163101, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006525
Epoch 5, Batch 10/50, Loss=0.08116131946444512
Loss made of: CE 0.07315438985824585, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/50, Loss=0.09303491450846195
Loss made of: CE 0.07121654599905014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/50, Loss=0.09243383631110191
Loss made of: CE 0.06578853726387024, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/50, Loss=0.0917018361389637
Loss made of: CE 0.07456137984991074, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/50, Loss=0.08899181187152863
Loss made of: CE 0.09467493742704391, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08946473896503448, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.08946473896503448, Class Loss=0.08946473896503448, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006104
Epoch 6, Batch 10/50, Loss=0.08749021366238594
Loss made of: CE 0.0648726224899292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/50, Loss=0.07592178136110306
Loss made of: CE 0.06971689313650131, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/50, Loss=0.0716882087290287
Loss made of: CE 0.07413195073604584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/50, Loss=0.08435053341090679
Loss made of: CE 0.09059234708547592, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/50, Loss=0.08458410911262035
Loss made of: CE 0.0920916199684143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08080697059631348, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.08080697059631348, Class Loss=0.08080697059631348, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/66, Loss=0.1580345258116722
Loss made of: CE 0.08464326709508896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/66, Loss=0.09674651473760605
Loss made of: CE 0.11492814868688583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/66, Loss=0.09399118572473526
Loss made of: CE 0.07845520228147507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/66, Loss=0.09014723598957061
Loss made of: CE 0.0887102335691452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/66, Loss=0.09875189363956452
Loss made of: CE 0.07358594238758087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/66, Loss=0.1058762639760971
Loss made of: CE 0.07717195153236389, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.10604536533355713, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.10604536533355713, Class Loss=0.10604536533355713, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.005998
Epoch 2, Batch 10/66, Loss=0.08948498480021953
Loss made of: CE 0.0906224176287651, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/66, Loss=0.09140793606638908
Loss made of: CE 0.1275539994239807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/66, Loss=0.088334209471941
Loss made of: CE 0.0629798024892807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/66, Loss=0.08557471334934234
Loss made of: CE 0.07152065634727478, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/66, Loss=0.08056150376796722
Loss made of: CE 0.0674784928560257, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/66, Loss=0.08638971224427223
Loss made of: CE 0.07621681690216064, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08590470254421234, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.08590470254421234, Class Loss=0.08590470254421234, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.005679
Epoch 3, Batch 10/66, Loss=0.0709279716014862
Loss made of: CE 0.05880485475063324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/66, Loss=0.06877513006329536
Loss made of: CE 0.07152378559112549, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/66, Loss=0.08070466294884682
Loss made of: CE 0.06602241098880768, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/66, Loss=0.07192328535020351
Loss made of: CE 0.055608391761779785, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/66, Loss=0.07928238697350025
Loss made of: CE 0.10282555222511292, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/66, Loss=0.07387191876769066
Loss made of: CE 0.06760851293802261, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07327565550804138, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.07327565550804138, Class Loss=0.07327565550804138, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/66, Loss=0.06424519121646881
Loss made of: CE 0.061132606118917465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/66, Loss=0.06237517334520817
Loss made of: CE 0.06238366290926933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/66, Loss=0.0679550476372242
Loss made of: CE 0.0625259205698967, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/66, Loss=0.07316913977265357
Loss made of: CE 0.06310899555683136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/66, Loss=0.06882062554359436
Loss made of: CE 0.09536848962306976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/66, Loss=0.08487673401832581
Loss made of: CE 0.11961361020803452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07031706720590591, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.07031706720590591, Class Loss=0.07031706720590591, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.005036
Epoch 5, Batch 10/66, Loss=0.0721610203385353
Loss made of: CE 0.07536032050848007, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/66, Loss=0.0681250650435686
Loss made of: CE 0.05542167276144028, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/66, Loss=0.06351608112454414
Loss made of: CE 0.06062167137861252, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/66, Loss=0.06680817008018494
Loss made of: CE 0.05539211630821228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/66, Loss=0.07299538142979145
Loss made of: CE 0.09145232290029526, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/66, Loss=0.05879214406013489
Loss made of: CE 0.06526485085487366, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.06654973328113556, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.06654973328113556, Class Loss=0.06654973328113556, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.004711
Epoch 6, Batch 10/66, Loss=0.06456773728132248
Loss made of: CE 0.06732292473316193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/66, Loss=0.060531917959451675
Loss made of: CE 0.05517905205488205, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/66, Loss=0.06175506599247456
Loss made of: CE 0.05470193922519684, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/66, Loss=0.06086395047605038
Loss made of: CE 0.05221196264028549, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/66, Loss=0.06845388114452362
Loss made of: CE 0.057858895510435104, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/66, Loss=0.06589462794363499
Loss made of: CE 0.0818408727645874, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06322500854730606, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.06322500854730606, Class Loss=0.06322500854730606, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.14166390895843506, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.949366
Mean Acc: 0.827070
FreqW Acc: 0.907732
Mean IoU: 0.744235
Class IoU:
	class 0: 0.9390371
	class 1: 0.8875576
	class 2: 0.38157484
	class 3: 0.85547346
	class 4: 0.71154946
	class 5: 0.2579665
	class 6: 0.9344925
	class 7: 0.8740934
	class 8: 0.85636955
Class Acc:
	class 0: 0.97889143
	class 1: 0.93683976
	class 2: 0.8051719
	class 3: 0.870581
	class 4: 0.8269264
	class 5: 0.2594575
	class 6: 0.9711056
	class 7: 0.92529535
	class 8: 0.8693629

federated global round: 3, step: 0
select part of clients to conduct local training
[1, 9, 3, 8]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/83, Loss=0.14159771651029587
Loss made of: CE 0.0895998477935791, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/83, Loss=0.1039835825562477
Loss made of: CE 0.08219818025827408, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/83, Loss=0.08386507406830787
Loss made of: CE 0.10302518308162689, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/83, Loss=0.09041027501225471
Loss made of: CE 0.10991819202899933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/83, Loss=0.08455656729638576
Loss made of: CE 0.06022888049483299, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/83, Loss=0.07899067141115665
Loss made of: CE 0.08802050352096558, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/83, Loss=0.09856765568256379
Loss made of: CE 0.07389998435974121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/83, Loss=0.09174119010567665
Loss made of: CE 0.09215560555458069, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.09655290096998215, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.09655290096998215, Class Loss=0.09655290096998215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007137
Epoch 2, Batch 10/83, Loss=0.08117006979882717
Loss made of: CE 0.05807888135313988, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/83, Loss=0.07773522324860097
Loss made of: CE 0.0730740949511528, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/83, Loss=0.06898076049983501
Loss made of: CE 0.06763708591461182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/83, Loss=0.07418267466127873
Loss made of: CE 0.07545498013496399, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/83, Loss=0.07829320840537549
Loss made of: CE 0.06361348927021027, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/83, Loss=0.07310242094099521
Loss made of: CE 0.06202365458011627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/83, Loss=0.07759594395756722
Loss made of: CE 0.07556617259979248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/83, Loss=0.08068272285163403
Loss made of: CE 0.09375447034835815, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.07642188668251038, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.07642188668251038, Class Loss=0.07642188668251038, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.006551
Epoch 3, Batch 10/83, Loss=0.07041670940816402
Loss made of: CE 0.07956665754318237, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/83, Loss=0.07598263248801232
Loss made of: CE 0.05595645308494568, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/83, Loss=0.08830482698976994
Loss made of: CE 0.08772855252027512, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/83, Loss=0.07420794814825057
Loss made of: CE 0.06829850375652313, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/83, Loss=0.07542205229401588
Loss made of: CE 0.08329156041145325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/83, Loss=0.0712294839322567
Loss made of: CE 0.05748207867145538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/83, Loss=0.06698721721768379
Loss made of: CE 0.06644585728645325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/83, Loss=0.06674885116517544
Loss made of: CE 0.06985402852296829, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07360031455755234, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.07360031455755234, Class Loss=0.07360031455755234, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005958
Epoch 4, Batch 10/83, Loss=0.07752853780984878
Loss made of: CE 0.06269507110118866, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/83, Loss=0.07109561935067177
Loss made of: CE 0.08187104761600494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/83, Loss=0.06958675943315029
Loss made of: CE 0.05667318031191826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/83, Loss=0.0688389416784048
Loss made of: CE 0.05702495574951172, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/83, Loss=0.061891430988907814
Loss made of: CE 0.061511944979429245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/83, Loss=0.06263840310275555
Loss made of: CE 0.07497695088386536, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/83, Loss=0.07119662836194038
Loss made of: CE 0.05220050737261772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/83, Loss=0.06566082052886486
Loss made of: CE 0.0642532929778099, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.06805291771888733, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.06805291771888733, Class Loss=0.06805291771888733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.005359
Epoch 5, Batch 10/83, Loss=0.06085316650569439
Loss made of: CE 0.05417954921722412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/83, Loss=0.05998267382383347
Loss made of: CE 0.054335933178663254, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/83, Loss=0.058147969841957095
Loss made of: CE 0.054845184087753296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/83, Loss=0.06485412046313285
Loss made of: CE 0.0820106565952301, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/83, Loss=0.06482482813298703
Loss made of: CE 0.07695385813713074, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/83, Loss=0.07098805457353592
Loss made of: CE 0.09553959965705872, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/83, Loss=0.06252295337617397
Loss made of: CE 0.06483989208936691, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/83, Loss=0.060351689159870145
Loss made of: CE 0.08799195289611816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.06287291646003723, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.06287291646003723, Class Loss=0.06287291646003723, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.004752
Epoch 6, Batch 10/83, Loss=0.05472446456551552
Loss made of: CE 0.0577516183257103, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/83, Loss=0.06923873573541642
Loss made of: CE 0.08467362821102142, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/83, Loss=0.06052566654980183
Loss made of: CE 0.05634279549121857, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/83, Loss=0.06549665071070195
Loss made of: CE 0.067283034324646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/83, Loss=0.05356046035885811
Loss made of: CE 0.05563027784228325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/83, Loss=0.05871756337583065
Loss made of: CE 0.04969891160726547, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/83, Loss=0.06705016866326333
Loss made of: CE 0.05600196123123169, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/83, Loss=0.062413515895605086
Loss made of: CE 0.09269674867391586, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.061168916523456573, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.061168916523456573, Class Loss=0.061168916523456573, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/61, Loss=0.21000843048095702
Loss made of: CE 0.09398718178272247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.1525169752538204
Loss made of: CE 0.11292552947998047, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.10729213804006577
Loss made of: CE 0.10758422315120697, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.12073203101754189
Loss made of: CE 0.11658884584903717, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.10809065476059913
Loss made of: CE 0.12701347470283508, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.1196944497525692
Loss made of: CE 0.12879061698913574, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1353527456521988, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.1353527456521988, Class Loss=0.1353527456521988, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007564
Epoch 2, Batch 10/61, Loss=0.1022637527436018
Loss made of: CE 0.11432120203971863, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.08526959493756295
Loss made of: CE 0.05896401405334473, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.08922704309225082
Loss made of: CE 0.09413868188858032, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.09583307951688766
Loss made of: CE 0.07024086266756058, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.100364089012146
Loss made of: CE 0.10376350581645966, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.08802144303917885
Loss made of: CE 0.07708175480365753, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.0935501679778099, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.0935501679778099, Class Loss=0.0935501679778099, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.006943
Epoch 3, Batch 10/61, Loss=0.08070252761244774
Loss made of: CE 0.09288498759269714, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.0758934136480093
Loss made of: CE 0.04914301633834839, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.07835056073963642
Loss made of: CE 0.0540558397769928, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.07504131197929383
Loss made of: CE 0.06001140922307968, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.07547770328819751
Loss made of: CE 0.0649762824177742, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.0863211289048195
Loss made of: CE 0.08871015161275864, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07839331775903702, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.07839331775903702, Class Loss=0.07839331775903702, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.006314
Epoch 4, Batch 10/61, Loss=0.07475450225174426
Loss made of: CE 0.06425009667873383, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.07294725924730301
Loss made of: CE 0.08495264500379562, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.08562781140208245
Loss made of: CE 0.2045269012451172, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.07955246940255165
Loss made of: CE 0.04689463973045349, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.06891750171780586
Loss made of: CE 0.05570375546813011, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.07721534185111523
Loss made of: CE 0.0919957384467125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07656014710664749, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.07656014710664749, Class Loss=0.07656014710664749, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.005679
Epoch 5, Batch 10/61, Loss=0.06570768207311631
Loss made of: CE 0.056007519364356995, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.07226612828671933
Loss made of: CE 0.0718986988067627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.06710744947195053
Loss made of: CE 0.07237491756677628, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.06987495981156826
Loss made of: CE 0.052680980414152145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.06765109002590179
Loss made of: CE 0.07535937428474426, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.08316624648869038
Loss made of: CE 0.10044611990451813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07076896727085114, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.07076896727085114, Class Loss=0.07076896727085114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.005036
Epoch 6, Batch 10/61, Loss=0.07495350539684295
Loss made of: CE 0.0869862362742424, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.06619482524693013
Loss made of: CE 0.06434833258390427, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.06329613588750363
Loss made of: CE 0.05453675985336304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.06955453753471375
Loss made of: CE 0.09740331023931503, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.0622328732162714
Loss made of: CE 0.05160939693450928, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.0741809532046318
Loss made of: CE 0.06254547834396362, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06835675239562988, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.06835675239562988, Class Loss=0.06835675239562988, Reg Loss=0.0
Current Client Index:  3
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/61, Loss=0.1348961792886257
Loss made of: CE 0.10368557274341583, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.09149913117289543
Loss made of: CE 0.07219471037387848, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.0850064504891634
Loss made of: CE 0.1355346292257309, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.08746468350291252
Loss made of: CE 0.10749395936727524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.08891075477004051
Loss made of: CE 0.09866204857826233, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.08169240355491639
Loss made of: CE 0.05884946882724762, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.09444534778594971, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.09444534778594971, Class Loss=0.09444534778594971, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.007137
Epoch 2, Batch 10/61, Loss=0.08776397481560708
Loss made of: CE 0.1076575443148613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.0804601762443781
Loss made of: CE 0.10079745948314667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.07790797762572765
Loss made of: CE 0.05821532756090164, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.07226854152977466
Loss made of: CE 0.07565344125032425, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.08524921461939812
Loss made of: CE 0.07497202605009079, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.08265070170164109
Loss made of: CE 0.09126125276088715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08065907657146454, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.08065907657146454, Class Loss=0.08065907657146454, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.006551
Epoch 3, Batch 10/61, Loss=0.09045027606189252
Loss made of: CE 0.08189111202955246, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.0746329952031374
Loss made of: CE 0.08356420695781708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.07516615986824035
Loss made of: CE 0.06668289005756378, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.07066336162388324
Loss made of: CE 0.06639822572469711, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.07323909960687161
Loss made of: CE 0.061864838004112244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.07938060872256755
Loss made of: CE 0.051500774919986725, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.0770900622010231, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.0770900622010231, Class Loss=0.0770900622010231, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005958
Epoch 4, Batch 10/61, Loss=0.07432661689817906
Loss made of: CE 0.06378340721130371, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.0825006552040577
Loss made of: CE 0.0778319239616394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.06455147415399551
Loss made of: CE 0.06637296825647354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.07019640877842903
Loss made of: CE 0.06447551399469376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.06905852295458317
Loss made of: CE 0.047228168696165085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.06683691516518593
Loss made of: CE 0.08973440527915955, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.0711783692240715, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.0711783692240715, Class Loss=0.0711783692240715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.005359
Epoch 5, Batch 10/61, Loss=0.0678863249719143
Loss made of: CE 0.060022465884685516, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.0655146025121212
Loss made of: CE 0.053509216755628586, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.06443595066666603
Loss made of: CE 0.05188527703285217, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.06625876985490323
Loss made of: CE 0.04796631261706352, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.06988177299499512
Loss made of: CE 0.05297974869608879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.06882620677351951
Loss made of: CE 0.06909094005823135, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.06688296049833298, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.06688296049833298, Class Loss=0.06688296049833298, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.004752
Epoch 6, Batch 10/61, Loss=0.06219646595418453
Loss made of: CE 0.07321546971797943, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.06464690491557121
Loss made of: CE 0.07541605830192566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.06533061750233174
Loss made of: CE 0.07467450946569443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.058596442267298696
Loss made of: CE 0.06953761726617813, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.06772695817053317
Loss made of: CE 0.06525741517543793, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.06618918105959892
Loss made of: CE 0.05083971470594406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0642261877655983, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.0642261877655983, Class Loss=0.0642261877655983, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/60, Loss=0.14349544867873193
Loss made of: CE 0.09750048816204071, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/60, Loss=0.10925379768013954
Loss made of: CE 0.07202473282814026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/60, Loss=0.11368266865611076
Loss made of: CE 0.10410990566015244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/60, Loss=0.10356041938066482
Loss made of: CE 0.1266012191772461, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/60, Loss=0.12357588335871697
Loss made of: CE 0.10618758201599121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/60, Loss=0.13153121098876
Loss made of: CE 0.18023166060447693, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.12084990739822388, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.12084990739822388, Class Loss=0.12084990739822388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.009247
Epoch 2, Batch 10/60, Loss=0.11468595042824745
Loss made of: CE 0.11945043504238129, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/60, Loss=0.1040002629160881
Loss made of: CE 0.08685078471899033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/60, Loss=0.09528641551733016
Loss made of: CE 0.1240830346941948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/60, Loss=0.09512437656521797
Loss made of: CE 0.10172955691814423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/60, Loss=0.10156250521540641
Loss made of: CE 0.0826648473739624, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/60, Loss=0.1029480256140232
Loss made of: CE 0.11048883199691772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10226792842149734, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.10226792842149734, Class Loss=0.10226792842149734, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.008487
Epoch 3, Batch 10/60, Loss=0.07912963517010212
Loss made of: CE 0.08283461630344391, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/60, Loss=0.07966497279703617
Loss made of: CE 0.08401837199926376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/60, Loss=0.09070925116539001
Loss made of: CE 0.0961730033159256, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/60, Loss=0.0837154783308506
Loss made of: CE 0.07747801393270493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/60, Loss=0.07825079783797265
Loss made of: CE 0.06112810969352722, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/60, Loss=0.0916111171245575
Loss made of: CE 0.05979689210653305, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08384688198566437, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.08384688198566437, Class Loss=0.08384688198566437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.007719
Epoch 4, Batch 10/60, Loss=0.08555069118738175
Loss made of: CE 0.0621037483215332, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/60, Loss=0.07853557541966438
Loss made of: CE 0.09693445265293121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/60, Loss=0.0780848354101181
Loss made of: CE 0.09673009812831879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/60, Loss=0.07749369479715824
Loss made of: CE 0.12560059130191803, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/60, Loss=0.08825479224324226
Loss made of: CE 0.1029389500617981, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/60, Loss=0.07172449193894863
Loss made of: CE 0.062340881675481796, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07994068413972855, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.07994068413972855, Class Loss=0.07994068413972855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/60, Loss=0.06669807359576226
Loss made of: CE 0.06689908355474472, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/60, Loss=0.06587234400212764
Loss made of: CE 0.06911277770996094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/60, Loss=0.07160495892167092
Loss made of: CE 0.10724587738513947, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/60, Loss=0.07838461324572563
Loss made of: CE 0.08102762699127197, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/60, Loss=0.06950456351041794
Loss made of: CE 0.053441815078258514, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/60, Loss=0.06832985877990723
Loss made of: CE 0.08896218985319138, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07006573677062988, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.07006573677062988, Class Loss=0.07006573677062988, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.006156
Epoch 6, Batch 10/60, Loss=0.07419364228844642
Loss made of: CE 0.09518016874790192, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/60, Loss=0.06638360768556595
Loss made of: CE 0.053596124053001404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/60, Loss=0.07059923224151135
Loss made of: CE 0.09982329607009888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/60, Loss=0.06303208135068417
Loss made of: CE 0.06777250021696091, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/60, Loss=0.06957452967762948
Loss made of: CE 0.07369644939899445, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/60, Loss=0.061828988045454024
Loss made of: CE 0.041373804211616516, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0676020160317421, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.0676020160317421, Class Loss=0.0676020160317421, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.11733989417552948, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.960582
Mean Acc: 0.883566
FreqW Acc: 0.929505
Mean IoU: 0.796759
Class IoU:
	class 0: 0.9522079
	class 1: 0.87288576
	class 2: 0.3719616
	class 3: 0.84119946
	class 4: 0.7142955
	class 5: 0.68400896
	class 6: 0.94283336
	class 7: 0.87441516
	class 8: 0.91701937
Class Acc:
	class 0: 0.97649467
	class 1: 0.915564
	class 2: 0.78364146
	class 3: 0.8521658
	class 4: 0.8625444
	class 5: 0.7073935
	class 6: 0.9723607
	class 7: 0.935321
	class 8: 0.94660795

federated global round: 4, step: 0
select part of clients to conduct local training
[5, 9, 0, 4]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/58, Loss=0.1031628917902708
Loss made of: CE 0.11647456884384155, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/58, Loss=0.10448120012879372
Loss made of: CE 0.10054843127727509, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/58, Loss=0.08409482166171074
Loss made of: CE 0.07446867227554321, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/58, Loss=0.09044923186302185
Loss made of: CE 0.13699321448802948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/58, Loss=0.11228897646069527
Loss made of: CE 0.07347572594881058, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.09859145432710648, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.09859145432710648, Class Loss=0.09859145432710648, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.008487
Epoch 2, Batch 10/58, Loss=0.09704210162162781
Loss made of: CE 0.12237995117902756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/58, Loss=0.09379210472106933
Loss made of: CE 0.0938078761100769, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/58, Loss=0.10195091515779495
Loss made of: CE 0.07270433008670807, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/58, Loss=0.08157406523823738
Loss made of: CE 0.0842796191573143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/58, Loss=0.09258078634738923
Loss made of: CE 0.07763591408729553, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09554911404848099, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.09554911404848099, Class Loss=0.09554911404848099, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.006943
Epoch 3, Batch 10/58, Loss=0.08362249433994293
Loss made of: CE 0.12531597912311554, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/58, Loss=0.10482349321246147
Loss made of: CE 0.21579036116600037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/58, Loss=0.09502247422933578
Loss made of: CE 0.10460122674703598, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/58, Loss=0.08893954381346703
Loss made of: CE 0.1473608911037445, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/58, Loss=0.07942453250288964
Loss made of: CE 0.07919950783252716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.088166743516922, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.088166743516922, Class Loss=0.088166743516922, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/58, Loss=0.08329653032124043
Loss made of: CE 0.07161818444728851, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/58, Loss=0.06808731257915497
Loss made of: CE 0.05241897702217102, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/58, Loss=0.0965818364173174
Loss made of: CE 0.2325570285320282, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/58, Loss=0.08324023932218552
Loss made of: CE 0.07370460033416748, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/58, Loss=0.07860519960522652
Loss made of: CE 0.11318717896938324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08022704720497131, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.08022704720497131, Class Loss=0.08022704720497131, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.003720
Epoch 5, Batch 10/58, Loss=0.0615831982344389
Loss made of: CE 0.051135413348674774, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/58, Loss=0.07158900238573551
Loss made of: CE 0.06811699271202087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/58, Loss=0.06875217854976653
Loss made of: CE 0.04851992055773735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/58, Loss=0.07550788670778275
Loss made of: CE 0.0575447641313076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/58, Loss=0.07427557669579983
Loss made of: CE 0.06970986723899841, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0696585401892662, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.0696585401892662, Class Loss=0.0696585401892662, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.001994
Epoch 6, Batch 10/58, Loss=0.06893233507871628
Loss made of: CE 0.05537755787372589, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/58, Loss=0.06635465398430825
Loss made of: CE 0.06926869601011276, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/58, Loss=0.06473088748753071
Loss made of: CE 0.060106709599494934, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/58, Loss=0.06599606238305569
Loss made of: CE 0.06316417455673218, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/58, Loss=0.0732196968048811
Loss made of: CE 0.07203222811222076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06664153933525085, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.06664153933525085, Class Loss=0.06664153933525085, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.004384
Epoch 1, Batch 10/61, Loss=0.11686311066150665
Loss made of: CE 0.08575223386287689, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.12933917567133904
Loss made of: CE 0.09956791996955872, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.07452451400458812
Loss made of: CE 0.08139406144618988, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.08698168620467187
Loss made of: CE 0.11095911264419556, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.0819899782538414
Loss made of: CE 0.08129323273897171, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.0870818953961134
Loss made of: CE 0.08252931386232376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.09570955485105515, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.09570955485105515, Class Loss=0.09570955485105515, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.003720
Epoch 2, Batch 10/61, Loss=0.08409498855471612
Loss made of: CE 0.08916650712490082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.07003449909389019
Loss made of: CE 0.05009065195918083, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.07482355758547783
Loss made of: CE 0.07473766058683395, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.07612227238714694
Loss made of: CE 0.06850247085094452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.08461273610591888
Loss made of: CE 0.08104703575372696, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.0667955219745636
Loss made of: CE 0.05614810436964035, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.07619921118021011, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.07619921118021011, Class Loss=0.07619921118021011, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.003043
Epoch 3, Batch 10/61, Loss=0.0715644758194685
Loss made of: CE 0.08151289075613022, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.06801108345389366
Loss made of: CE 0.03817006200551987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.07381924763321876
Loss made of: CE 0.07205817103385925, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.06397149115800857
Loss made of: CE 0.0617305263876915, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.0695486232638359
Loss made of: CE 0.06561216711997986, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.07495888769626617
Loss made of: CE 0.08060188591480255, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07005489617586136, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.07005489617586136, Class Loss=0.07005489617586136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.002349
Epoch 4, Batch 10/61, Loss=0.06694052293896675
Loss made of: CE 0.06492301821708679, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.074338423833251
Loss made of: CE 0.06526721268892288, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.07586063705384731
Loss made of: CE 0.1754947006702423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.0694136530160904
Loss made of: CE 0.054116494953632355, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.06263159215450287
Loss made of: CE 0.05911358818411827, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.07641638070344925
Loss made of: CE 0.08321304619312286, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07100364565849304, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.07100364565849304, Class Loss=0.07100364565849304, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.001631
Epoch 5, Batch 10/61, Loss=0.0623575184494257
Loss made of: CE 0.051669638603925705, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.06769290491938591
Loss made of: CE 0.0649491474032402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.06170745752751827
Loss made of: CE 0.06391321122646332, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.06497316285967827
Loss made of: CE 0.057525552809238434, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.05957655496895313
Loss made of: CE 0.06667434424161911, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.07916100211441517
Loss made of: CE 0.11998572945594788, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.06579520553350449, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.06579520553350449, Class Loss=0.06579520553350449, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000874
Epoch 6, Batch 10/61, Loss=0.06872509196400642
Loss made of: CE 0.0748259425163269, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.06227489747107029
Loss made of: CE 0.05369250103831291, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.05766710415482521
Loss made of: CE 0.04454676806926727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.061967624723911284
Loss made of: CE 0.07808178663253784, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.0624882347881794
Loss made of: CE 0.0494856983423233, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.07299471087753773
Loss made of: CE 0.0647156834602356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06430798768997192, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.06430798768997192, Class Loss=0.06430798768997192, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.006943
Epoch 1, Batch 10/57, Loss=0.15507239401340484
Loss made of: CE 0.0805044174194336, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/57, Loss=0.09873711541295052
Loss made of: CE 0.08309230208396912, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/57, Loss=0.11524584889411926
Loss made of: CE 0.1219758689403534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/57, Loss=0.097715213149786
Loss made of: CE 0.08074857294559479, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/57, Loss=0.09138145297765732
Loss made of: CE 0.09033074229955673, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.11333037167787552, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.11333037167787552, Class Loss=0.11333037167787552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.005892
Epoch 2, Batch 10/57, Loss=0.0967075526714325
Loss made of: CE 0.08897372335195541, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/57, Loss=0.09715715572237968
Loss made of: CE 0.11277859658002853, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/57, Loss=0.09399318173527718
Loss made of: CE 0.07854565978050232, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/57, Loss=0.08676329180598259
Loss made of: CE 0.11407961696386337, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/57, Loss=0.09306032955646515
Loss made of: CE 0.09416346997022629, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09286264330148697, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.09286264330148697, Class Loss=0.09286264330148697, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.004820
Epoch 3, Batch 10/57, Loss=0.08223864957690238
Loss made of: CE 0.08212952315807343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/57, Loss=0.07654914371669293
Loss made of: CE 0.060702644288539886, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/57, Loss=0.08694368228316307
Loss made of: CE 0.09656374156475067, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/57, Loss=0.08327444270253181
Loss made of: CE 0.09011024236679077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/57, Loss=0.08409438766539097
Loss made of: CE 0.09235598146915436, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08196781575679779, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.08196781575679779, Class Loss=0.08196781575679779, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.003720
Epoch 4, Batch 10/57, Loss=0.07719583548605442
Loss made of: CE 0.08911624550819397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/57, Loss=0.07750035487115384
Loss made of: CE 0.06612147390842438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/57, Loss=0.08072221055626869
Loss made of: CE 0.05582326650619507, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/57, Loss=0.08663066886365414
Loss made of: CE 0.09572043269872665, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/57, Loss=0.0797414492815733
Loss made of: CE 0.08176584541797638, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07941823452711105, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.07941823452711105, Class Loss=0.07941823452711105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.002583
Epoch 5, Batch 10/57, Loss=0.07907360047101974
Loss made of: CE 0.0906267911195755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/57, Loss=0.07934825494885445
Loss made of: CE 0.08649822324514389, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/57, Loss=0.07817486301064491
Loss made of: CE 0.07682137936353683, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/57, Loss=0.06480700150132179
Loss made of: CE 0.07007281482219696, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/57, Loss=0.07731770537793636
Loss made of: CE 0.10249768197536469, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0757124274969101, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.0757124274969101, Class Loss=0.0757124274969101, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.001384
Epoch 6, Batch 10/57, Loss=0.07095703408122063
Loss made of: CE 0.06306139379739761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/57, Loss=0.07520600073039532
Loss made of: CE 0.06251531839370728, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/57, Loss=0.0782389685511589
Loss made of: CE 0.08350928127765656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/57, Loss=0.07097311057150364
Loss made of: CE 0.056964289397001266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/57, Loss=0.07812731117010116
Loss made of: CE 0.0928008183836937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07565396279096603, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.07565396279096603, Class Loss=0.07565396279096603, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.005359
Epoch 1, Batch 10/60, Loss=0.08588720038533211
Loss made of: CE 0.09926029294729233, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/60, Loss=0.09180260673165322
Loss made of: CE 0.071573905646801, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/60, Loss=0.08108854442834854
Loss made of: CE 0.07169945538043976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/60, Loss=0.07342789135873318
Loss made of: CE 0.08769924938678741, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/60, Loss=0.0815328061580658
Loss made of: CE 0.06759919971227646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/60, Loss=0.0749635960906744
Loss made of: CE 0.06651449203491211, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.0814504399895668, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.0814504399895668, Class Loss=0.0814504399895668, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.004548
Epoch 2, Batch 10/60, Loss=0.0710571076720953
Loss made of: CE 0.05542915314435959, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/60, Loss=0.07704871222376823
Loss made of: CE 0.06397270411252975, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/60, Loss=0.06939815878868102
Loss made of: CE 0.04845094680786133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/60, Loss=0.08116658292710781
Loss made of: CE 0.08038188517093658, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/60, Loss=0.08275951966643333
Loss made of: CE 0.07522527873516083, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/60, Loss=0.07350816540420055
Loss made of: CE 0.055308692157268524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.07582304626703262, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.07582304626703262, Class Loss=0.07582304626703262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.003720
Epoch 3, Batch 10/60, Loss=0.072287767380476
Loss made of: CE 0.07760854810476303, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/60, Loss=0.062359192222356793
Loss made of: CE 0.04645133018493652, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/60, Loss=0.06383086107671261
Loss made of: CE 0.05612735077738762, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/60, Loss=0.0668748389929533
Loss made of: CE 0.05204445868730545, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/60, Loss=0.0673549436032772
Loss made of: CE 0.06791259348392487, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/60, Loss=0.07246926762163639
Loss made of: CE 0.06523716449737549, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.06752948462963104, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.06752948462963104, Class Loss=0.06752948462963104, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.002872
Epoch 4, Batch 10/60, Loss=0.06537724919617176
Loss made of: CE 0.08371417969465256, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/60, Loss=0.06481562107801438
Loss made of: CE 0.051801830530166626, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/60, Loss=0.06132265292108059
Loss made of: CE 0.06276210397481918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/60, Loss=0.0683756172657013
Loss made of: CE 0.060878507792949677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/60, Loss=0.06967287920415402
Loss made of: CE 0.0623479001224041, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/60, Loss=0.06544102504849433
Loss made of: CE 0.10145112127065659, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.065834179520607, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.065834179520607, Class Loss=0.065834179520607, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.001994
Epoch 5, Batch 10/60, Loss=0.05577422119677067
Loss made of: CE 0.04732578247785568, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/60, Loss=0.06436839029192924
Loss made of: CE 0.06162353605031967, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/60, Loss=0.06510673351585865
Loss made of: CE 0.056254588067531586, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/60, Loss=0.06019957400858402
Loss made of: CE 0.051279615610837936, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/60, Loss=0.07125929817557335
Loss made of: CE 0.05226612091064453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/60, Loss=0.06782974898815156
Loss made of: CE 0.11446446180343628, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.06408966332674026, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.06408966332674026, Class Loss=0.06408966332674026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.001068
Epoch 6, Batch 10/60, Loss=0.06456560008227825
Loss made of: CE 0.03321131691336632, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/60, Loss=0.0665120955556631
Loss made of: CE 0.043803464621305466, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/60, Loss=0.06362096779048443
Loss made of: CE 0.0603552870452404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/60, Loss=0.059070102497935296
Loss made of: CE 0.05288790166378021, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/60, Loss=0.06345933116972446
Loss made of: CE 0.0715440958738327, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/60, Loss=0.05708092786371708
Loss made of: CE 0.072100430727005, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06238484010100365, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.06238484010100365, Class Loss=0.06238484010100365, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.11884117871522903, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.959774
Mean Acc: 0.898905
FreqW Acc: 0.928002
Mean IoU: 0.803614
Class IoU:
	class 0: 0.95059025
	class 1: 0.9002101
	class 2: 0.39132276
	class 3: 0.7887624
	class 4: 0.72272617
	class 5: 0.77700424
	class 6: 0.9452571
	class 7: 0.8679732
	class 8: 0.88868076
Class Acc:
	class 0: 0.97419924
	class 1: 0.95054156
	class 2: 0.8336194
	class 3: 0.7955812
	class 4: 0.8647896
	class 5: 0.8397001
	class 6: 0.9766464
	class 7: 0.9440808
	class 8: 0.91099066

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[5, 11, 7, 1]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.472295075654984
Loss made of: CE 0.7735309600830078, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.62533950805664 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.94987236559391
Loss made of: CE 0.3692651689052582, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.971362590789795 EntMin 0.0
Epoch 1, Class Loss=0.7635505199432373, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.7635505199432373, Class Loss=0.7635505199432373, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.598346704244614
Loss made of: CE 0.4490804374217987, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.291680812835693 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/27, Loss=7.099276837706566
Loss made of: CE 0.3762076795101166, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.425370216369629 EntMin 0.0
Epoch 2, Class Loss=0.46351033449172974, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.46351033449172974, Class Loss=0.46351033449172974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.8636694252491
Loss made of: CE 0.497798889875412, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.215612411499023 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.51957428753376
Loss made of: CE 0.4011245667934418, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.957364082336426 EntMin 0.0
Epoch 3, Class Loss=0.46050140261650085, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.46050140261650085, Class Loss=0.46050140261650085, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.430128648877144
Loss made of: CE 0.41568636894226074, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.445291519165039 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.219211727380753
Loss made of: CE 0.4770512580871582, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.627708435058594 EntMin 0.0
Epoch 4, Class Loss=0.4656083285808563, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.4656083285808563, Class Loss=0.4656083285808563, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.256072521209717
Loss made of: CE 0.3933483958244324, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.896892070770264 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.132729938626289
Loss made of: CE 0.4746069610118866, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.498340129852295 EntMin 0.0
Epoch 5, Class Loss=0.4363419711589813, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.4363419711589813, Class Loss=0.4363419711589813, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=6.161675068736076
Loss made of: CE 0.45245617628097534, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.700028896331787 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.9905579566955565
Loss made of: CE 0.39506393671035767, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.32607889175415 EntMin 0.0
Epoch 6, Class Loss=0.422468900680542, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.422468900680542, Class Loss=0.422468900680542, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2845239639282227, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=1.2845239639282227, Class Loss=1.2845239639282227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.9310175776481628, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.9310175776481628, Class Loss=0.9310175776481628, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.7456194758415222, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.7456194758415222, Class Loss=0.7456194758415222, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.6943963766098022, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.6943963766098022, Class Loss=0.6943963766098022, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6459033489227295, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.6459033489227295, Class Loss=0.6459033489227295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6189335584640503, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.6189335584640503, Class Loss=0.6189335584640503, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.393148183822632
Loss made of: CE 0.7556054592132568, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.117639541625977 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.612973299622535
Loss made of: CE 0.5644561052322388, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.895914077758789 EntMin 0.0
Epoch 1, Class Loss=0.7507368326187134, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.7507368326187134, Class Loss=0.7507368326187134, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.654666209220887
Loss made of: CE 0.5775518417358398, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.188026428222656 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.1429370701313015
Loss made of: CE 0.48254770040512085, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.033559322357178 EntMin 0.0
Epoch 2, Class Loss=0.46010228991508484, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.46010228991508484, Class Loss=0.46010228991508484, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.627671697735787
Loss made of: CE 0.5033361315727234, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.349427223205566 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.456962886452675
Loss made of: CE 0.4322453439235687, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.019452095031738 EntMin 0.0
Epoch 3, Class Loss=0.46649739146232605, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.46649739146232605, Class Loss=0.46649739146232605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.198866361379624
Loss made of: CE 0.4467425048351288, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.771483898162842 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.207177537679672
Loss made of: CE 0.5746396780014038, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.330006122589111 EntMin 0.0
Epoch 4, Class Loss=0.45588141679763794, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.45588141679763794, Class Loss=0.45588141679763794, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.358134642243385
Loss made of: CE 0.4445137679576874, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.555071830749512 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.815966790914535
Loss made of: CE 0.31916117668151855, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.364193916320801 EntMin 0.0
Epoch 5, Class Loss=0.44371065497398376, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.44371065497398376, Class Loss=0.44371065497398376, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=5.980381819605827
Loss made of: CE 0.4642348289489746, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.479696273803711 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 20/27, Loss=6.196171250939369
Loss made of: CE 0.4090871214866638, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.86947774887085 EntMin 0.0
Epoch 6, Class Loss=0.44195064902305603, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.44195064902305603, Class Loss=0.44195064902305603, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.280471271276474
Loss made of: CE 0.682917594909668, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.389678955078125 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.841508036851883
Loss made of: CE 0.47637271881103516, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.03825569152832 EntMin 0.0
Epoch 1, Class Loss=0.7411113977432251, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.7411113977432251, Class Loss=0.7411113977432251, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.447152909636498
Loss made of: CE 0.41475042700767517, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.980411529541016 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.241613185405731
Loss made of: CE 0.4991709589958191, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.880918025970459 EntMin 0.0
Epoch 2, Class Loss=0.4600815773010254, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.4600815773010254, Class Loss=0.4600815773010254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.627427405118942
Loss made of: CE 0.4415457844734192, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.206036567687988 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.411851236224175
Loss made of: CE 0.48334360122680664, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9387969970703125 EntMin 0.0
Epoch 3, Class Loss=0.45998433232307434, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.45998433232307434, Class Loss=0.45998433232307434, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.368843588232994
Loss made of: CE 0.469011127948761, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.870639324188232 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.342783859372139
Loss made of: CE 0.42874687910079956, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.020694255828857 EntMin 0.0
Epoch 4, Class Loss=0.44481176137924194, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.44481176137924194, Class Loss=0.44481176137924194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.148065182566643
Loss made of: CE 0.4630802571773529, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.715734481811523 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.289635476469994
Loss made of: CE 0.4153777062892914, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0218400955200195 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.4450052082538605, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4450052082538605, Class Loss=0.4450052082538605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=6.0505534499883655
Loss made of: CE 0.4469790458679199, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.594595909118652 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.083656510710716
Loss made of: CE 0.472700834274292, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.673662185668945 EntMin 0.0
Epoch 6, Class Loss=0.4410379230976105, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.4410379230976105, Class Loss=0.4410379230976105, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3191128671169281, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.926490
Mean Acc: 0.744761
FreqW Acc: 0.865578
Mean IoU: 0.648220
Class IoU:
	class 0: 0.913625
	class 1: 0.87765616
	class 2: 0.361049
	class 3: 0.8048481
	class 4: 0.7141037
	class 5: 0.78631586
	class 6: 0.9329695
	class 7: 0.8644416
	class 8: 0.8754089
	class 9: 0.0
	class 10: 0.0
Class Acc:
	class 0: 0.9740277
	class 1: 0.948084
	class 2: 0.8890839
	class 3: 0.81782246
	class 4: 0.86205715
	class 5: 0.8753245
	class 6: 0.9650102
	class 7: 0.9231567
	class 8: 0.9378051
	class 9: 0.0
	class 10: 0.0

federated global round: 6, step: 1
select part of clients to conduct local training
[1, 6, 7, 3]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/27, Loss=6.147114810347557
Loss made of: CE 0.3992251753807068, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6859025955200195 EntMin 0.0
Epoch 1, Batch 20/27, Loss=6.230580523610115
Loss made of: CE 0.4507777690887451, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.839578151702881 EntMin 0.0
Epoch 1, Class Loss=0.4508936107158661, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.4508936107158661, Class Loss=0.4508936107158661, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=6.05176265835762
Loss made of: CE 0.4647397994995117, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.816863059997559 EntMin 0.0
Epoch 2, Batch 20/27, Loss=6.149944385886192
Loss made of: CE 0.4424881637096405, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.840806007385254 EntMin 0.0
Epoch 2, Class Loss=0.4501103460788727, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.4501103460788727, Class Loss=0.4501103460788727, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=6.183967050909996
Loss made of: CE 0.4187403619289398, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7974982261657715 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.359053322672844
Loss made of: CE 0.4832778573036194, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.552219390869141 EntMin 0.0
Epoch 3, Class Loss=0.49824491143226624, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.49824491143226624, Class Loss=0.49824491143226624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=6.214191856980324
Loss made of: CE 0.4490928649902344, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.808270454406738 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.5810936152935025
Loss made of: CE 0.458811491727829, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.092774868011475 EntMin 0.0
Epoch 4, Class Loss=0.45165160298347473, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.45165160298347473, Class Loss=0.45165160298347473, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=6.550862482190132
Loss made of: CE 0.4169606566429138, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.968069076538086 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.582117742300033
Loss made of: CE 0.3466762900352478, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.221587657928467 EntMin 0.0
Epoch 5, Class Loss=0.4552050232887268, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4552050232887268, Class Loss=0.4552050232887268, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=6.202769830822945
Loss made of: CE 0.3835420608520508, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.738811016082764 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.238154876232147
Loss made of: CE 0.44122499227523804, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.716201305389404 EntMin 0.0
Epoch 6, Class Loss=0.43694037199020386, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.43694037199020386, Class Loss=0.43694037199020386, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.144488283991814
Loss made of: CE 0.37051260471343994, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.403446197509766 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.839343431591987
Loss made of: CE 0.3069959580898285, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.488733291625977 EntMin 0.0
Epoch 1, Class Loss=0.40124523639678955, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.40124523639678955, Class Loss=0.40124523639678955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/27, Loss=5.763745188713074
Loss made of: CE 0.3844493329524994, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.681265830993652 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.854189854860306
Loss made of: CE 0.4448581337928772, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.768535614013672 EntMin 0.0
Epoch 2, Class Loss=0.4100147783756256, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.4100147783756256, Class Loss=0.4100147783756256, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/27, Loss=5.66580708026886
Loss made of: CE 0.4053892493247986, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.461892127990723 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.746410766243935
Loss made of: CE 0.43034547567367554, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.156869888305664 EntMin 0.0
Epoch 3, Class Loss=0.40212059020996094, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.40212059020996094, Class Loss=0.40212059020996094, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/27, Loss=5.587845027446747
Loss made of: CE 0.3992946743965149, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.927239894866943 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.635648933053017
Loss made of: CE 0.33522066473960876, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.023015975952148 EntMin 0.0
Epoch 4, Class Loss=0.3929376006126404, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3929376006126404, Class Loss=0.3929376006126404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/27, Loss=5.676394802331925
Loss made of: CE 0.446854829788208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.974978923797607 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.531093800067902
Loss made of: CE 0.2942374348640442, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4365692138671875 EntMin 0.0
Epoch 5, Class Loss=0.39032474160194397, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.39032474160194397, Class Loss=0.39032474160194397, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/27, Loss=5.476367819309234
Loss made of: CE 0.34675464034080505, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.207000732421875 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.408617931604385
Loss made of: CE 0.33277031779289246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7872090339660645 EntMin 0.0
Epoch 6, Class Loss=0.3912428021430969, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.3912428021430969, Class Loss=0.3912428021430969, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/27, Loss=6.27928175330162
Loss made of: CE 0.46046727895736694, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.870303153991699 EntMin 0.0
Epoch 1, Batch 20/27, Loss=6.154785215854645
Loss made of: CE 0.4777814745903015, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.49554443359375 EntMin 0.0
Epoch 1, Class Loss=0.46443042159080505, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.46443042159080505, Class Loss=0.46443042159080505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=6.439255604147911
Loss made of: CE 0.6400324702262878, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.303325176239014 EntMin 0.0
Epoch 2, Batch 20/27, Loss=6.308497610688209
Loss made of: CE 0.4533514976501465, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3952789306640625 EntMin 0.0
Epoch 2, Class Loss=0.48782292008399963, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.48782292008399963, Class Loss=0.48782292008399963, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=6.248946979641914
Loss made of: CE 0.4859386384487152, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.65324592590332 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.1407828569412235
Loss made of: CE 0.41040802001953125, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.438842296600342 EntMin 0.0
Epoch 3, Class Loss=0.4747205972671509, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.4747205972671509, Class Loss=0.4747205972671509, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=6.055647784471512
Loss made of: CE 0.4807705283164978, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.596807479858398 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.092550691962242
Loss made of: CE 0.7457658052444458, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.66524076461792 EntMin 0.0
Epoch 4, Class Loss=0.4610719680786133, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4610719680786133, Class Loss=0.4610719680786133, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=6.172503507137298
Loss made of: CE 0.44342100620269775, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.420201778411865 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.916407191753388
Loss made of: CE 0.3526275157928467, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4580607414245605 EntMin 0.0
Epoch 5, Class Loss=0.4214301109313965, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4214301109313965, Class Loss=0.4214301109313965, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=5.863525396585464
Loss made of: CE 0.4529303312301636, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.336394786834717 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.026431149244308
Loss made of: CE 0.3211354613304138, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.774432182312012 EntMin 0.0
Epoch 6, Class Loss=0.42963096499443054, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.42963096499443054, Class Loss=0.42963096499443054, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1539572477340698, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=1.1539572477340698, Class Loss=1.1539572477340698, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=1.041597604751587, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=1.041597604751587, Class Loss=1.041597604751587, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.9272072315216064, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.9272072315216064, Class Loss=0.9272072315216064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.7926747798919678, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.7926747798919678, Class Loss=0.7926747798919678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7015161514282227, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.7015161514282227, Class Loss=0.7015161514282227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6104104518890381, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.6104104518890381, Class Loss=0.6104104518890381, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.31670942902565, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.922757
Mean Acc: 0.724410
FreqW Acc: 0.858533
Mean IoU: 0.631220
Class IoU:
	class 0: 0.9101348
	class 1: 0.87103593
	class 2: 0.36923975
	class 3: 0.69500625
	class 4: 0.7046209
	class 5: 0.7820114
	class 6: 0.89106685
	class 7: 0.8399145
	class 8: 0.877094
	class 9: 0.0032923955
	class 10: 0.0
Class Acc:
	class 0: 0.9755556
	class 1: 0.9104713
	class 2: 0.88853705
	class 3: 0.70025235
	class 4: 0.86892027
	class 5: 0.86705345
	class 6: 0.9196605
	class 7: 0.88232946
	class 8: 0.95242894
	class 9: 0.0033016382
	class 10: 0.0

federated global round: 7, step: 1
select part of clients to conduct local training
[13, 8, 0, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1727252006530762, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=1.1727252006530762, Class Loss=1.1727252006530762, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.0810645818710327, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=1.0810645818710327, Class Loss=1.0810645818710327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.9307117462158203, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.9307117462158203, Class Loss=0.9307117462158203, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.8146588802337646, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.8146588802337646, Class Loss=0.8146588802337646, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.7405781745910645, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.7405781745910645, Class Loss=0.7405781745910645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6486715078353882, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.6486715078353882, Class Loss=0.6486715078353882, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.2956095933914185, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.2956095933914185, Class Loss=1.2956095933914185, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.1228312253952026, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=1.1228312253952026, Class Loss=1.1228312253952026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=1.0063366889953613, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=1.0063366889953613, Class Loss=1.0063366889953613, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.8833836317062378, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.8833836317062378, Class Loss=0.8833836317062378, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.7165675163269043, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.7165675163269043, Class Loss=0.7165675163269043, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6109532117843628, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6109532117843628, Class Loss=0.6109532117843628, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2106258869171143, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=1.2106258869171143, Class Loss=1.2106258869171143, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.1053009033203125, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=1.1053009033203125, Class Loss=1.1053009033203125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.9818617701530457, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.9818617701530457, Class Loss=0.9818617701530457, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.8125696182250977, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.8125696182250977, Class Loss=0.8125696182250977, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6476702690124512, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.6476702690124512, Class Loss=0.6476702690124512, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.591062068939209, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.591062068939209, Class Loss=0.591062068939209, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=5.810504797101021
Loss made of: CE 0.35449814796447754, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.355641841888428 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.7317353039979935
Loss made of: CE 0.328372061252594, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.206101417541504 EntMin 0.0
Epoch 1, Class Loss=0.4123925268650055, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.4123925268650055, Class Loss=0.4123925268650055, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/27, Loss=5.6804488629102705
Loss made of: CE 0.3647801876068115, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.503459453582764 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.550539326667786
Loss made of: CE 0.3651926517486572, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.18571662902832 EntMin 0.0
Epoch 2, Class Loss=0.3997277617454529, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3997277617454529, Class Loss=0.3997277617454529, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/27, Loss=5.762024587392807
Loss made of: CE 0.40690529346466064, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.394842147827148 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.711871156096459
Loss made of: CE 0.37380123138427734, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.260222434997559 EntMin 0.0
Epoch 3, Class Loss=0.40270116925239563, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.40270116925239563, Class Loss=0.40270116925239563, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/27, Loss=5.763714763522148
Loss made of: CE 0.3456590175628662, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.599952220916748 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.56016697883606
Loss made of: CE 0.39787396788597107, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.032329559326172 EntMin 0.0
Epoch 4, Class Loss=0.3857214152812958, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.3857214152812958, Class Loss=0.3857214152812958, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/27, Loss=5.589657723903656
Loss made of: CE 0.3657475411891937, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.10719108581543 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.482176291942596
Loss made of: CE 0.3675951361656189, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.986788272857666 EntMin 0.0
Epoch 5, Class Loss=0.37145739793777466, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.37145739793777466, Class Loss=0.37145739793777466, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/27, Loss=5.6062432587146755
Loss made of: CE 0.42315348982810974, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.02991247177124 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.506027713418007
Loss made of: CE 0.3749935030937195, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.090479850769043 EntMin 0.0
Epoch 6, Class Loss=0.3731266260147095, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.3731266260147095, Class Loss=0.3731266260147095, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2791464626789093, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.920229
Mean Acc: 0.726074
FreqW Acc: 0.854894
Mean IoU: 0.628091
Class IoU:
	class 0: 0.9078704
	class 1: 0.86371046
	class 2: 0.35885572
	class 3: 0.82128453
	class 4: 0.67161244
	class 5: 0.7575318
	class 6: 0.79593045
	class 7: 0.851325
	class 8: 0.87156546
	class 9: 0.009310018
	class 10: 0.0
Class Acc:
	class 0: 0.97457874
	class 1: 0.9039772
	class 2: 0.92835903
	class 3: 0.8405956
	class 4: 0.827169
	class 5: 0.84894496
	class 6: 0.8042258
	class 7: 0.9048056
	class 8: 0.9447506
	class 9: 0.009408914
	class 10: 0.0

federated global round: 8, step: 1
select part of clients to conduct local training
[12, 9, 4, 13]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.638013881444931
Loss made of: CE 0.4510371685028076, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.384395599365234 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.826034787297249
Loss made of: CE 0.3780214786529541, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.126903057098389 EntMin 0.0
Epoch 1, Class Loss=0.38724765181541443, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.38724765181541443, Class Loss=0.38724765181541443, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.547705891728401
Loss made of: CE 0.3872840404510498, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.076903343200684 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.70869132578373
Loss made of: CE 0.44092506170272827, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.559423446655273 EntMin 0.0
Epoch 2, Class Loss=0.35959720611572266, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.35959720611572266, Class Loss=0.35959720611572266, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=5.505550476908684
Loss made of: CE 0.31477081775665283, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.964612007141113 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.479951319098473
Loss made of: CE 0.31189948320388794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7428693771362305 EntMin 0.0
Epoch 3, Class Loss=0.35118308663368225, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.35118308663368225, Class Loss=0.35118308663368225, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.454684495925903
Loss made of: CE 0.31127068400382996, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.117551326751709 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.216662478446961
Loss made of: CE 0.3207516372203827, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.001967430114746 EntMin 0.0
Epoch 4, Class Loss=0.34453320503234863, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.34453320503234863, Class Loss=0.34453320503234863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.414863729476929
Loss made of: CE 0.3486194312572479, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.21460485458374 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.343949469923973
Loss made of: CE 0.33065342903137207, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.976238250732422 EntMin 0.0
Epoch 5, Class Loss=0.34938615560531616, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.34938615560531616, Class Loss=0.34938615560531616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.254825904965401
Loss made of: CE 0.4207707345485687, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.779932975769043 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.1834442019462585
Loss made of: CE 0.36633801460266113, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0143818855285645 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Class Loss=0.35600247979164124, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.35600247979164124, Class Loss=0.35600247979164124, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.6468645334243774, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.6468645334243774, Class Loss=0.6468645334243774, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.597238302230835, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.597238302230835, Class Loss=0.597238302230835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.5354510545730591, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5354510545730591, Class Loss=0.5354510545730591, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.4960748851299286, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.4960748851299286, Class Loss=0.4960748851299286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.45682668685913086, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.45682668685913086, Class Loss=0.45682668685913086, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.43663281202316284, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.43663281202316284, Class Loss=0.43663281202316284, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.63185732960701
Loss made of: CE 0.4262871742248535, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.559996604919434 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.83975975215435
Loss made of: CE 0.4248616397380829, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.486789226531982 EntMin 0.0
Epoch 1, Class Loss=0.39550501108169556, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.39550501108169556, Class Loss=0.39550501108169556, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.567398348450661
Loss made of: CE 0.34756630659103394, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.851652145385742 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.634167605638504
Loss made of: CE 0.41811811923980713, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.991137504577637 EntMin 0.0
Epoch 2, Class Loss=0.37844908237457275, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.37844908237457275, Class Loss=0.37844908237457275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=5.410350659489632
Loss made of: CE 0.3718087375164032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.913346290588379 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.590812304615975
Loss made of: CE 0.41281113028526306, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.101232051849365 EntMin 0.0
Epoch 3, Class Loss=0.36415061354637146, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.36415061354637146, Class Loss=0.36415061354637146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.364177235960961
Loss made of: CE 0.4221969544887543, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2459564208984375 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.587676104903221
Loss made of: CE 0.31340011954307556, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.04911994934082 EntMin 0.0
Epoch 4, Class Loss=0.36163198947906494, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.36163198947906494, Class Loss=0.36163198947906494, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.36961929500103
Loss made of: CE 0.3403346538543701, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.334710121154785 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.3749259293079374
Loss made of: CE 0.3481166362762451, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2221503257751465 EntMin 0.0
Epoch 5, Class Loss=0.35833606123924255, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.35833606123924255, Class Loss=0.35833606123924255, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.328805792331695
Loss made of: CE 0.4232412576675415, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.26745080947876 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.261578440666199
Loss made of: CE 0.38306865096092224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.669566631317139 EntMin 0.0
Epoch 6, Class Loss=0.35523030161857605, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.35523030161857605, Class Loss=0.35523030161857605, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.6852519512176514, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.6852519512176514, Class Loss=0.6852519512176514, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Epoch 2, Class Loss=0.6462495923042297, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.6462495923042297, Class Loss=0.6462495923042297, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.6545894145965576, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.6545894145965576, Class Loss=0.6545894145965576, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5903205871582031, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5903205871582031, Class Loss=0.5903205871582031, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.5403132438659668, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5403132438659668, Class Loss=0.5403132438659668, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.5119314193725586, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5119314193725586, Class Loss=0.5119314193725586, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.24815908074378967, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.924333
Mean Acc: 0.731995
FreqW Acc: 0.861655
Mean IoU: 0.640421
Class IoU:
	class 0: 0.91223055
	class 1: 0.8742864
	class 2: 0.3714203
	class 3: 0.7992576
	class 4: 0.69071025
	class 5: 0.7730913
	class 6: 0.85536593
	class 7: 0.85584396
	class 8: 0.8818718
	class 9: 0.030551111
	class 10: 0.0
Class Acc:
	class 0: 0.9767844
	class 1: 0.9205627
	class 2: 0.8976574
	class 3: 0.8111611
	class 4: 0.8534892
	class 5: 0.8470312
	class 6: 0.8668523
	class 7: 0.8970433
	class 8: 0.9501684
	class 9: 0.031190854
	class 10: 0.0

federated global round: 9, step: 1
select part of clients to conduct local training
[4, 6, 13, 12]
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/27, Loss=5.964370608329773
Loss made of: CE 0.47566160559654236, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.256168365478516 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.569520723819733
Loss made of: CE 0.4499010443687439, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.169198989868164 EntMin 0.0
Epoch 1, Class Loss=0.4447197914123535, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.4447197914123535, Class Loss=0.4447197914123535, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.407388204336167
Loss made of: CE 0.34224733710289, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.877504825592041 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.552945163846016
Loss made of: CE 0.4353446364402771, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.721482276916504 EntMin 0.0
Epoch 2, Class Loss=0.4192325174808502, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.4192325174808502, Class Loss=0.4192325174808502, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=5.179696846008301
Loss made of: CE 0.43579137325286865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.801271915435791 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.46534218788147
Loss made of: CE 0.479644238948822, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.755588531494141 EntMin 0.0
Epoch 3, Class Loss=0.40553799271583557, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.40553799271583557, Class Loss=0.40553799271583557, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.234111207723617
Loss made of: CE 0.4688793420791626, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.187952041625977 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.437167850136757
Loss made of: CE 0.3461551070213318, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.009397029876709 EntMin 0.0
Epoch 4, Class Loss=0.3987117409706116, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.3987117409706116, Class Loss=0.3987117409706116, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.29101651608944
Loss made of: CE 0.38351306319236755, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.070950508117676 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.272731357812882
Loss made of: CE 0.37908613681793213, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.997020244598389 EntMin 0.0
Epoch 5, Class Loss=0.39529475569725037, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.39529475569725037, Class Loss=0.39529475569725037, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=5.232316198945045
Loss made of: CE 0.4814976155757904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.896684169769287 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.192218488454818
Loss made of: CE 0.4301067590713501, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.49191951751709 EntMin 0.0
Epoch 6, Class Loss=0.3947977125644684, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.3947977125644684, Class Loss=0.3947977125644684, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/27, Loss=5.894782364368439
Loss made of: CE 0.41605302691459656, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.886571884155273 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.453322619199753
Loss made of: CE 0.38604527711868286, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.096671104431152 EntMin 0.0
Epoch 1, Class Loss=0.4481651782989502, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.4481651782989502, Class Loss=0.4481651782989502, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/27, Loss=5.405071276426315
Loss made of: CE 0.38094520568847656, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.047341346740723 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.444128605723381
Loss made of: CE 0.515782356262207, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.244454860687256 EntMin 0.0
Epoch 2, Class Loss=0.4249250888824463, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.4249250888824463, Class Loss=0.4249250888824463, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/27, Loss=5.317624562978745
Loss made of: CE 0.42118000984191895, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.038681983947754 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.35487941801548
Loss made of: CE 0.459028422832489, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.928571701049805 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.4093431830406189, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4093431830406189, Class Loss=0.4093431830406189, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/27, Loss=5.273025494813919
Loss made of: CE 0.42875373363494873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5469512939453125 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.277023017406464
Loss made of: CE 0.34906232357025146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.728529930114746 EntMin 0.0
Epoch 4, Class Loss=0.4001329839229584, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4001329839229584, Class Loss=0.4001329839229584, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/27, Loss=5.251408895850181
Loss made of: CE 0.3689751625061035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.268950939178467 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.210776776075363
Loss made of: CE 0.28555306792259216, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.800820827484131 EntMin 0.0
Epoch 5, Class Loss=0.39219409227371216, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.39219409227371216, Class Loss=0.39219409227371216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/27, Loss=5.16801884174347
Loss made of: CE 0.34845519065856934, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.794246196746826 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.0504067808389665
Loss made of: CE 0.3264685571193695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.500543117523193 EntMin 0.0
Epoch 6, Class Loss=0.39602431654930115, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.39602431654930115, Class Loss=0.39602431654930115, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000372
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.7178899645805359, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.7178899645805359, Class Loss=0.7178899645805359, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.7172211408615112, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.7172211408615112, Class Loss=0.7172211408615112, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000258
Epoch 3, Class Loss=0.6655042767524719, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.6655042767524719, Class Loss=0.6655042767524719, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000199
Epoch 4, Class Loss=0.6772352457046509, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.6772352457046509, Class Loss=0.6772352457046509, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000138
Epoch 5, Class Loss=0.657710075378418, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.657710075378418, Class Loss=0.657710075378418, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.6404680013656616, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.6404680013656616, Class Loss=0.6404680013656616, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/27, Loss=6.030497127771378
Loss made of: CE 0.4961239993572235, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.112792491912842 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.471546483039856
Loss made of: CE 0.4500861167907715, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.78305721282959 EntMin 0.0
Epoch 1, Class Loss=0.43393197655677795, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.43393197655677795, Class Loss=0.43393197655677795, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.319048717617989
Loss made of: CE 0.4389275908470154, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8087897300720215 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.472213059663773
Loss made of: CE 0.5241708159446716, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1530375480651855 EntMin 0.0
Epoch 2, Class Loss=0.40943339467048645, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.40943339467048645, Class Loss=0.40943339467048645, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=5.449974331259727
Loss made of: CE 0.45411238074302673, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.04446268081665 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.410307905077934
Loss made of: CE 0.3345632553100586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.792995452880859 EntMin 0.0
Epoch 3, Class Loss=0.38959354162216187, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.38959354162216187, Class Loss=0.38959354162216187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.307872754335404
Loss made of: CE 0.32368791103363037, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.961513996124268 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.088440036773681
Loss made of: CE 0.3490784168243408, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.789797306060791 EntMin 0.0
Epoch 4, Class Loss=0.3827005922794342, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.3827005922794342, Class Loss=0.3827005922794342, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.341376766562462
Loss made of: CE 0.371390700340271, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.198759078979492 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.322351384162903
Loss made of: CE 0.3631064295768738, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9601030349731445 EntMin 0.0
Epoch 5, Class Loss=0.38420960307121277, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.38420960307121277, Class Loss=0.38420960307121277, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=5.195061764121055
Loss made of: CE 0.43853482604026794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.499579429626465 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.105904617905617
Loss made of: CE 0.39623844623565674, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.858257293701172 EntMin 0.0
Epoch 6, Class Loss=0.3883286714553833, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.3883286714553833, Class Loss=0.3883286714553833, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2543555796146393, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.925587
Mean Acc: 0.742528
FreqW Acc: 0.864799
Mean IoU: 0.650435
Class IoU:
	class 0: 0.9130197
	class 1: 0.873343
	class 2: 0.36611027
	class 3: 0.77587575
	class 4: 0.69958705
	class 5: 0.7818544
	class 6: 0.88719445
	class 7: 0.8637954
	class 8: 0.88864726
	class 9: 0.10535608
	class 10: 0.0
Class Acc:
	class 0: 0.97508234
	class 1: 0.9026927
	class 2: 0.9097105
	class 3: 0.7844344
	class 4: 0.8508338
	class 5: 0.8615525
	class 6: 0.90171754
	class 7: 0.9157982
	class 8: 0.95268536
	class 9: 0.11330459
	class 10: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[10, 14, 16, 7]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=11.302981204958632
Loss made of: CE 0.03933490440249443, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.808145523071289 EntMin 0.0
Epoch 1, Batch 20/29, Loss=9.924184102227446
Loss made of: CE 0.016846416518092155, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.490427017211914 EntMin 0.0
Epoch 1, Class Loss=0.03623219579458237, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.03623219579458237, Class Loss=0.03623219579458237, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=8.835274668037892
Loss made of: CE 0.0837835893034935, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.052790641784668 EntMin 0.0
Epoch 2, Batch 20/29, Loss=8.503414633870126
Loss made of: CE 0.14613798260688782, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.943818092346191 EntMin 0.0
Epoch 2, Class Loss=0.09441393613815308, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.09441393613815308, Class Loss=0.09441393613815308, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.29474388062954
Loss made of: CE 0.12029185891151428, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.127643585205078 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.668191795051098
Loss made of: CE 0.23209917545318604, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.894012451171875 EntMin 0.0
Epoch 3, Class Loss=0.19703271985054016, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.19703271985054016, Class Loss=0.19703271985054016, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=7.996798527240753
Loss made of: CE 0.31628286838531494, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.868893623352051 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.7177551463246346
Loss made of: CE 0.3479626774787903, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.50208854675293 EntMin 0.0
Epoch 4, Class Loss=0.3071037232875824, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.3071037232875824, Class Loss=0.3071037232875824, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.683638587594032
Loss made of: CE 0.34992218017578125, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.591472148895264 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.63519042134285
Loss made of: CE 0.39271777868270874, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.203699111938477 EntMin 0.0
Epoch 5, Class Loss=0.34072190523147583, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.34072190523147583, Class Loss=0.34072190523147583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.223786142468453
Loss made of: CE 0.3505876064300537, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.455446243286133 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.281788167357445
Loss made of: CE 0.2672226130962372, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.3010478019714355 EntMin 0.0
Epoch 6, Class Loss=0.343283474445343, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.343283474445343, Class Loss=0.343283474445343, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.696786480024457
Loss made of: CE 0.043533407151699066, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0337443351745605 EntMin 0.0
Epoch 1, Class Loss=0.04885914549231529, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.04885914549231529, Class Loss=0.04885914549231529, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.170329442620277
Loss made of: CE 0.17236928641796112, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8680572509765625 EntMin 0.0
Epoch 2, Class Loss=0.20005911588668823, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.20005911588668823, Class Loss=0.20005911588668823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=5.796122455596924
Loss made of: CE 0.2801882028579712, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.051703929901123 EntMin 0.0
Epoch 3, Class Loss=0.36753058433532715, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.36753058433532715, Class Loss=0.36753058433532715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.615877044200897
Loss made of: CE 0.528788685798645, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.597367763519287 EntMin 0.0
Epoch 4, Class Loss=0.4758663773536682, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.4758663773536682, Class Loss=0.4758663773536682, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.480487689375877
Loss made of: CE 0.5877631902694702, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.76252555847168 EntMin 0.0
Epoch 5, Class Loss=0.5278029441833496, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5278029441833496, Class Loss=0.5278029441833496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.425544303655625
Loss made of: CE 0.5550780892372131, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0576629638671875 EntMin 0.0
Epoch 6, Class Loss=0.5425432920455933, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5425432920455933, Class Loss=0.5425432920455933, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=11.693298804201186
Loss made of: CE 0.017579250037670135, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.000810623168945 EntMin 0.0
Epoch 1, Batch 20/29, Loss=9.9150474420283
Loss made of: CE 0.04183894395828247, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.970407485961914 EntMin 0.0
Epoch 1, Class Loss=0.029519570991396904, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.029519570991396904, Class Loss=0.029519570991396904, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=8.736662853509188
Loss made of: CE 0.08388523757457733, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.786283493041992 EntMin 0.0
Epoch 2, Batch 20/29, Loss=8.792244600504636
Loss made of: CE 0.15091750025749207, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.41525650024414 EntMin 0.0
Epoch 2, Class Loss=0.10204416513442993, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.10204416513442993, Class Loss=0.10204416513442993, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.54609954059124
Loss made of: CE 0.1842041015625, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.881338119506836 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.953258268535137
Loss made of: CE 0.3016010820865631, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.9881415367126465 EntMin 0.0
Epoch 3, Class Loss=0.20952224731445312, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.20952224731445312, Class Loss=0.20952224731445312, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=7.927428612112999
Loss made of: CE 0.3369719982147217, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.12913703918457 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.802404661476612
Loss made of: CE 0.3260585069656372, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.59816837310791 EntMin 0.0
Epoch 4, Class Loss=0.3085673153400421, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.3085673153400421, Class Loss=0.3085673153400421, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.3472723215818405
Loss made of: CE 0.2745029330253601, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.277327537536621 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.737789651751518
Loss made of: CE 0.46068626642227173, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.144738674163818 EntMin 0.0
Epoch 5, Class Loss=0.3396335244178772, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3396335244178772, Class Loss=0.3396335244178772, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.309691931307316
Loss made of: CE 0.451796293258667, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.521321773529053 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.407861304283142
Loss made of: CE 0.3628554344177246, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.630226135253906 EntMin 0.0
Epoch 6, Class Loss=0.3430434763431549, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.3430434763431549, Class Loss=0.3430434763431549, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.5425237130373715
Loss made of: CE 0.07697838544845581, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.7406768798828125 EntMin 0.0
Epoch 1, Class Loss=0.0478157140314579, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.0478157140314579, Class Loss=0.0478157140314579, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.094330005347729
Loss made of: CE 0.14119192957878113, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.360198974609375 EntMin 0.0
Epoch 2, Class Loss=0.20176418125629425, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.20176418125629425, Class Loss=0.20176418125629425, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=5.696241161227226
Loss made of: CE 0.2525063753128052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.826793670654297 EntMin 0.0
Epoch 3, Class Loss=0.35619035363197327, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.35619035363197327, Class Loss=0.35619035363197327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.816813415288925
Loss made of: CE 0.37740039825439453, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.193203449249268 EntMin 0.0
Epoch 4, Class Loss=0.4775059223175049, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4775059223175049, Class Loss=0.4775059223175049, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.655006277561188
Loss made of: CE 0.6388091444969177, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.663909912109375 EntMin 0.0
Epoch 5, Class Loss=0.5249933004379272, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.5249933004379272, Class Loss=0.5249933004379272, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.7442288279533384
Loss made of: CE 0.6136157512664795, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.733100891113281 EntMin 0.0
Epoch 6, Class Loss=0.5403918027877808, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.5403918027877808, Class Loss=0.5403918027877808, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5044019818305969, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.871657
Mean Acc: 0.670723
FreqW Acc: 0.781082
Mean IoU: 0.531922
Class IoU:
	class 0: 0.86644775
	class 1: 0.8540794
	class 2: 0.29376337
	class 3: 0.8544108
	class 4: 0.6663891
	class 5: 0.7582391
	class 6: 0.9257943
	class 7: 0.8167334
	class 8: 0.7492246
	class 9: 0.12990937
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.9531234
	class 1: 0.924387
	class 2: 0.96445835
	class 3: 0.8951511
	class 4: 0.913757
	class 5: 0.9184537
	class 6: 0.9648864
	class 7: 0.93310785
	class 8: 0.97226286
	class 9: 0.27981406
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[10, 13, 6, 1]
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/29, Loss=9.339306396245956
Loss made of: CE 0.772699236869812, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.862194061279297 EntMin 0.0
Epoch 1, Batch 20/29, Loss=8.488853698968887
Loss made of: CE 0.7498223185539246, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.922698020935059 EntMin 0.0
Epoch 1, Class Loss=0.8337076902389526, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.8337076902389526, Class Loss=0.8337076902389526, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/29, Loss=8.227743530273438
Loss made of: CE 0.585528552532196, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.705359935760498 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.910965862870216
Loss made of: CE 0.49637922644615173, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.218502044677734 EntMin 0.0
Epoch 2, Class Loss=0.5633494257926941, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.5633494257926941, Class Loss=0.5633494257926941, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/29, Loss=7.734474632143974
Loss made of: CE 0.47510045766830444, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.111266136169434 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.3963076055049894
Loss made of: CE 0.4385776221752167, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.132579803466797 EntMin 0.0
Epoch 3, Class Loss=0.48155149817466736, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.48155149817466736, Class Loss=0.48155149817466736, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/29, Loss=7.546475744247436
Loss made of: CE 0.5435817241668701, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.116679668426514 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 20/29, Loss=7.41401673257351
Loss made of: CE 0.41029179096221924, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.033010005950928 EntMin 0.0
Epoch 4, Class Loss=0.44977062940597534, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.44977062940597534, Class Loss=0.44977062940597534, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=7.383807617425918
Loss made of: CE 0.4207187592983246, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.354368209838867 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.480535265803337
Loss made of: CE 0.3873223066329956, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.951053142547607 EntMin 0.0
Epoch 5, Class Loss=0.46124467253685, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.46124467253685, Class Loss=0.46124467253685, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/29, Loss=7.207270562648773
Loss made of: CE 0.3713681399822235, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.922738075256348 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.148598593473435
Loss made of: CE 0.3859241008758545, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.825511932373047 EntMin 0.0
Epoch 6, Class Loss=0.4226580262184143, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.4226580262184143, Class Loss=0.4226580262184143, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.76906785205938
Loss made of: CE 0.00754069909453392, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.765949249267578 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.900056390510872
Loss made of: CE 0.008750930428504944, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.735387325286865 EntMin 0.0
Epoch 1, Class Loss=0.013392044231295586, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.013392044231295586, Class Loss=0.013392044231295586, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/29, Loss=7.87722075432539
Loss made of: CE 0.03845449537038803, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.443939208984375 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.33013053741306
Loss made of: CE 0.024348726496100426, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.53742790222168 EntMin 0.0
Epoch 2, Class Loss=0.0589851513504982, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.0589851513504982, Class Loss=0.0589851513504982, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/29, Loss=7.200938998162746
Loss made of: CE 0.1100904792547226, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.990235328674316 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 20/29, Loss=7.819210264086723
Loss made of: CE 0.12834440171718597, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.240672588348389 EntMin 0.0
Epoch 3, Class Loss=0.1325538456439972, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.1325538456439972, Class Loss=0.1325538456439972, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/29, Loss=7.322215075790882
Loss made of: CE 0.20274585485458374, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.4216508865356445 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.328384056687355
Loss made of: CE 0.14016792178153992, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.822795867919922 EntMin 0.0
Epoch 4, Class Loss=0.20722121000289917, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.20722121000289917, Class Loss=0.20722121000289917, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/29, Loss=7.262283347547054
Loss made of: CE 0.15704584121704102, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.184656143188477 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.249505968391896
Loss made of: CE 0.23812642693519592, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.17003059387207 EntMin 0.0
Epoch 5, Class Loss=0.2461313158273697, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.2461313158273697, Class Loss=0.2461313158273697, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/29, Loss=7.027985270321369
Loss made of: CE 0.27891966700553894, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.796004295349121 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.07599878013134
Loss made of: CE 0.1515982300043106, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.7643256187438965 EntMin 0.0
Epoch 6, Class Loss=0.27037814259529114, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.27037814259529114, Class Loss=0.27037814259529114, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=6.486546487919986
Loss made of: CE 0.061276085674762726, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.758013725280762 EntMin 0.0
Epoch 1, Class Loss=0.06939297914505005, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.06939297914505005, Class Loss=0.06939297914505005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=5.688833723962307
Loss made of: CE 0.1586586833000183, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.30678129196167 EntMin 0.0
Epoch 2, Class Loss=0.18708615005016327, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.18708615005016327, Class Loss=0.18708615005016327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=5.442881107330322
Loss made of: CE 0.23380911350250244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.624886512756348 EntMin 0.0
Epoch 3, Class Loss=0.3122963607311249, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.3122963607311249, Class Loss=0.3122963607311249, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.596024909615517
Loss made of: CE 0.5271438360214233, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.034242153167725 EntMin 0.0
Epoch 4, Class Loss=0.43962329626083374, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.43962329626083374, Class Loss=0.43962329626083374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.329975259304047
Loss made of: CE 0.42200884222984314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.460620880126953 EntMin 0.0
Epoch 5, Class Loss=0.4701792597770691, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.4701792597770691, Class Loss=0.4701792597770691, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.222290480136872
Loss made of: CE 0.45195817947387695, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.880354881286621 EntMin 0.0
Epoch 6, Class Loss=0.5093967914581299, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5093967914581299, Class Loss=0.5093967914581299, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.324512255750596
Loss made of: CE 0.1251366138458252, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.359514236450195 EntMin 0.0
Epoch 1, Class Loss=0.05793822929263115, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.05793822929263115, Class Loss=0.05793822929263115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=5.676515658199787
Loss made of: CE 0.1873917281627655, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.235367774963379 EntMin 0.0
Epoch 2, Class Loss=0.2028467208147049, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.2028467208147049, Class Loss=0.2028467208147049, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=5.283518216013908
Loss made of: CE 0.29308658838272095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.659111499786377 EntMin 0.0
Epoch 3, Class Loss=0.3315374553203583, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3315374553203583, Class Loss=0.3315374553203583, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.3872076869010925
Loss made of: CE 0.3510980010032654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.585319995880127 EntMin 0.0
Epoch 4, Class Loss=0.4237380921840668, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.4237380921840668, Class Loss=0.4237380921840668, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.245562362670898
Loss made of: CE 0.3379170298576355, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4006452560424805 EntMin 0.0
Epoch 5, Class Loss=0.47996023297309875, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.47996023297309875, Class Loss=0.47996023297309875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.16899074614048
Loss made of: CE 0.5211063623428345, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.054568767547607 EntMin 0.0
Epoch 6, Class Loss=0.5007791519165039, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.5007791519165039, Class Loss=0.5007791519165039, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4069540500640869, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.887318
Mean Acc: 0.664471
FreqW Acc: 0.800108
Mean IoU: 0.550448
Class IoU:
	class 0: 0.88025224
	class 1: 0.84909225
	class 2: 0.31723836
	class 3: 0.8426621
	class 4: 0.68768686
	class 5: 0.7678997
	class 6: 0.9020656
	class 7: 0.8331125
	class 8: 0.75825953
	class 9: 0.07508255
	class 10: 0.0
	class 11: 0.0
	class 12: 0.24247411
Class Acc:
	class 0: 0.9681901
	class 1: 0.89269775
	class 2: 0.9474884
	class 3: 0.8800301
	class 4: 0.8808271
	class 5: 0.90252036
	class 6: 0.92399216
	class 7: 0.927848
	class 8: 0.96811324
	class 9: 0.08949654
	class 10: 0.0
	class 11: 0.0
	class 12: 0.2569158

federated global round: 12, step: 2
select part of clients to conduct local training
[11, 0, 8, 14]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.29183645695448
Loss made of: CE 0.07553373277187347, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.825531005859375 EntMin 0.0
Epoch 1, Class Loss=0.06318293511867523, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.06318293511867523, Class Loss=0.06318293511867523, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=5.640927866846323
Loss made of: CE 0.2466571182012558, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.597524642944336 EntMin 0.0
Epoch 2, Class Loss=0.17911148071289062, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.17911148071289062, Class Loss=0.17911148071289062, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.277591320872307
Loss made of: CE 0.2836325764656067, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5008745193481445 EntMin 0.0
Epoch 3, Class Loss=0.32307443022727966, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.32307443022727966, Class Loss=0.32307443022727966, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.453132912516594
Loss made of: CE 0.5059211254119873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.808404922485352 EntMin 0.0
Epoch 4, Class Loss=0.4056761860847473, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.4056761860847473, Class Loss=0.4056761860847473, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.545195904374123
Loss made of: CE 0.4698360860347748, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.60007381439209 EntMin 0.0
Epoch 5, Class Loss=0.4572451710700989, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.4572451710700989, Class Loss=0.4572451710700989, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.074915161728859
Loss made of: CE 0.3922376334667206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291261196136475 EntMin 0.0
Epoch 6, Class Loss=0.4627063274383545, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.4627063274383545, Class Loss=0.4627063274383545, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.009853445191402
Loss made of: CE 0.009035099297761917, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.551932334899902 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.181507729698206
Loss made of: CE 0.02367323637008667, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.762164115905762 EntMin 0.0
Epoch 1, Class Loss=0.010562214069068432, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.010562214069068432, Class Loss=0.010562214069068432, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=7.014050205983222
Loss made of: CE 0.027261268347501755, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.927402019500732 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.945576238073409
Loss made of: CE 0.03594532236456871, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1007184982299805 EntMin 0.0
Epoch 2, Class Loss=0.034964557737112045, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.034964557737112045, Class Loss=0.034964557737112045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=7.099861176684499
Loss made of: CE 0.13027352094650269, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.207320690155029 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.599620911851526
Loss made of: CE 0.09690196812152863, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.285186767578125 EntMin 0.0
Epoch 3, Class Loss=0.08771481364965439, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.08771481364965439, Class Loss=0.08771481364965439, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=6.69494806304574
Loss made of: CE 0.0968233197927475, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.127076625823975 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.5513425342738625
Loss made of: CE 0.13076350092887878, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.094109535217285 EntMin 0.0
Epoch 4, Class Loss=0.13712657988071442, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.13712657988071442, Class Loss=0.13712657988071442, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=6.613484847545624
Loss made of: CE 0.22970208525657654, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.014303207397461 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.662204448878765
Loss made of: CE 0.1994091123342514, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.814456939697266 EntMin 0.0
Epoch 5, Class Loss=0.18813104927539825, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.18813104927539825, Class Loss=0.18813104927539825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=6.721373854577541
Loss made of: CE 0.19237637519836426, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.104310989379883 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.726506695151329
Loss made of: CE 0.1890142560005188, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.329949855804443 EntMin 0.0
Epoch 6, Class Loss=0.2390027791261673, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.2390027791261673, Class Loss=0.2390027791261673, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.932711603003554
Loss made of: CE 0.009943278506398201, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.648903846740723 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.414732868783176
Loss made of: CE 0.007067294325679541, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.873593807220459 EntMin 0.0
Epoch 1, Class Loss=0.012777270749211311, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.012777270749211311, Class Loss=0.012777270749211311, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=6.835466449894011
Loss made of: CE 0.03212713822722435, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.491995811462402 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.247399874590338
Loss made of: CE 0.022640882059931755, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.169593334197998 EntMin 0.0
Epoch 2, Class Loss=0.03959120810031891, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.03959120810031891, Class Loss=0.03959120810031891, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=7.176044082641601
Loss made of: CE 0.0680752620100975, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.243447780609131 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.0312619458884
Loss made of: CE 0.08850439637899399, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.624790191650391 EntMin 0.0
Epoch 3, Class Loss=0.0849379152059555, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.0849379152059555, Class Loss=0.0849379152059555, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=6.525176236778497
Loss made of: CE 0.11932522058486938, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.341504096984863 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.931229757517576
Loss made of: CE 0.07276692986488342, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.682141304016113 EntMin 0.0
Epoch 4, Class Loss=0.1398279219865799, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.1398279219865799, Class Loss=0.1398279219865799, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=6.68258907571435
Loss made of: CE 0.20036981999874115, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.086698055267334 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.652982938289642
Loss made of: CE 0.18744421005249023, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.838228225708008 EntMin 0.0
Epoch 5, Class Loss=0.19014976918697357, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.19014976918697357, Class Loss=0.19014976918697357, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=6.887507393956184
Loss made of: CE 0.2537463307380676, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5718793869018555 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.867615333199501
Loss made of: CE 0.2194092720746994, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.921195983886719 EntMin 0.0
Epoch 6, Class Loss=0.22671762108802795, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.22671762108802795, Class Loss=0.22671762108802795, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.840555626153946
Loss made of: CE 0.768871009349823, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.536362648010254 EntMin 0.0
Epoch 1, Class Loss=0.7736467719078064, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.7736467719078064, Class Loss=0.7736467719078064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/13, Loss=6.208692133426666
Loss made of: CE 0.5725650191307068, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.062996864318848 EntMin 0.0
Epoch 2, Class Loss=0.6566256284713745, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.6566256284713745, Class Loss=0.6566256284713745, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/13, Loss=5.658543056249618
Loss made of: CE 0.5345419645309448, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.668786525726318 EntMin 0.0
Epoch 3, Class Loss=0.5571802854537964, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.5571802854537964, Class Loss=0.5571802854537964, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/13, Loss=5.448803415894508
Loss made of: CE 0.5227980613708496, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.186514854431152 EntMin 0.0
Epoch 4, Class Loss=0.49372154474258423, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.49372154474258423, Class Loss=0.49372154474258423, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/13, Loss=5.248183733224868
Loss made of: CE 0.5158798694610596, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684214115142822 EntMin 0.0
Epoch 5, Class Loss=0.4601294696331024, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.4601294696331024, Class Loss=0.4601294696331024, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/13, Loss=5.116463446617127
Loss made of: CE 0.5072593092918396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.447380542755127 EntMin 0.0
Epoch 6, Class Loss=0.4405141770839691, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.4405141770839691, Class Loss=0.4405141770839691, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.39502280950546265, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.897077
Mean Acc: 0.702990
FreqW Acc: 0.822101
Mean IoU: 0.573114
Class IoU:
	class 0: 0.89755064
	class 1: 0.8631015
	class 2: 0.3142332
	class 3: 0.85757315
	class 4: 0.684413
	class 5: 0.7509389
	class 6: 0.9172777
	class 7: 0.83424157
	class 8: 0.7623849
	class 9: 0.12631702
	class 10: 0.0
	class 11: 0.0
	class 12: 0.44245636
Class Acc:
	class 0: 0.9619604
	class 1: 0.9127985
	class 2: 0.9543124
	class 3: 0.91487294
	class 4: 0.8889157
	class 5: 0.9273327
	class 6: 0.9431092
	class 7: 0.933825
	class 8: 0.9696552
	class 9: 0.16921803
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5628682

federated global round: 13, step: 2
select part of clients to conduct local training
[13, 5, 15, 7]
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/29, Loss=8.317747640609742
Loss made of: CE 0.6632227301597595, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.561645030975342 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.631574749946594
Loss made of: CE 0.5359423160552979, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.984064102172852 EntMin 0.0
Epoch 1, Class Loss=0.649615466594696, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.649615466594696, Class Loss=0.649615466594696, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/29, Loss=7.4063449770212175
Loss made of: CE 0.5234773755073547, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.582367897033691 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.009487879276276
Loss made of: CE 0.4426255226135254, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4605207443237305 EntMin 0.0
Epoch 2, Class Loss=0.5062724947929382, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.5062724947929382, Class Loss=0.5062724947929382, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/29, Loss=6.855288794636726
Loss made of: CE 0.5641291737556458, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.790914058685303 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.578044164180755
Loss made of: CE 0.3929605484008789, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.913787364959717 EntMin 0.0
Epoch 3, Class Loss=0.4498685300350189, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.4498685300350189, Class Loss=0.4498685300350189, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/29, Loss=7.07113234102726
Loss made of: CE 0.4372309148311615, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.940696716308594 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.130709332227707
Loss made of: CE 0.4077698886394501, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.313098430633545 EntMin 0.0
Epoch 4, Class Loss=0.43805304169654846, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.43805304169654846, Class Loss=0.43805304169654846, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/29, Loss=6.9444546639919285
Loss made of: CE 0.4466099441051483, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.771037578582764 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.8360472351312636
Loss made of: CE 0.49817603826522827, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.088619232177734 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.43268901109695435, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.43268901109695435, Class Loss=0.43268901109695435, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/29, Loss=6.68782447874546
Loss made of: CE 0.34625571966171265, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.305712699890137 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.919330450892448
Loss made of: CE 0.4687427878379822, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.453908920288086 EntMin 0.0
Epoch 6, Class Loss=0.3990650177001953, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.3990650177001953, Class Loss=0.3990650177001953, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.121849138638936
Loss made of: CE 0.008237914182245731, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.095499038696289 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.03420438063331
Loss made of: CE 0.005402049049735069, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.891417503356934 EntMin 0.0
Epoch 1, Class Loss=0.008785663172602654, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.008785663172602654, Class Loss=0.008785663172602654, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/29, Loss=7.206096458621323
Loss made of: CE 0.039531122893095016, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.737493515014648 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.899770333897322
Loss made of: CE 0.03794359788298607, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.124933242797852 EntMin 0.0
Epoch 2, Class Loss=0.0340607613325119, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.0340607613325119, Class Loss=0.0340607613325119, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/29, Loss=6.852526212483644
Loss made of: CE 0.05266778916120529, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.507344722747803 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.586898571997881
Loss made of: CE 0.1043519377708435, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.089723587036133 EntMin 0.0
Epoch 3, Class Loss=0.07586931437253952, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.07586931437253952, Class Loss=0.07586931437253952, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/29, Loss=6.463710401952267
Loss made of: CE 0.1485036015510559, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.499377250671387 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.484488958865404
Loss made of: CE 0.1612357646226883, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.944695472717285 EntMin 0.0
Epoch 4, Class Loss=0.1340504288673401, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.1340504288673401, Class Loss=0.1340504288673401, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=6.386743357777595
Loss made of: CE 0.14100754261016846, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7547760009765625 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.457353410124779
Loss made of: CE 0.18129894137382507, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.158062934875488 EntMin 0.0
Epoch 5, Class Loss=0.18849371373653412, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.18849371373653412, Class Loss=0.18849371373653412, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/29, Loss=6.49759052246809
Loss made of: CE 0.19949810206890106, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.779531002044678 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.579951931536198
Loss made of: CE 0.29063090682029724, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.544492721557617 EntMin 0.0
Epoch 6, Class Loss=0.22637443244457245, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.22637443244457245, Class Loss=0.22637443244457245, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=6.288695650734008
Loss made of: CE 0.05722510814666748, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.807568550109863 EntMin 0.0
Epoch 1, Class Loss=0.04868868365883827, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.04868868365883827, Class Loss=0.04868868365883827, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.571550188213587
Loss made of: CE 0.2008984535932541, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.597943305969238 EntMin 0.0
Epoch 2, Class Loss=0.16395814716815948, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.16395814716815948, Class Loss=0.16395814716815948, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.4438053458929065
Loss made of: CE 0.2280195951461792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.392695903778076 EntMin 0.0
Epoch 3, Class Loss=0.27428996562957764, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.27428996562957764, Class Loss=0.27428996562957764, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.162426799535751
Loss made of: CE 0.281769335269928, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0728044509887695 EntMin 0.0
Epoch 4, Class Loss=0.3279746174812317, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.3279746174812317, Class Loss=0.3279746174812317, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.345892986655235
Loss made of: CE 0.3769867420196533, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.084774017333984 EntMin 0.0
Epoch 5, Class Loss=0.3906081020832062, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.3906081020832062, Class Loss=0.3906081020832062, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.018779748678208
Loss made of: CE 0.43800151348114014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.379243850708008 EntMin 0.0
Epoch 6, Class Loss=0.4238347113132477, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.4238347113132477, Class Loss=0.4238347113132477, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.828160035610199
Loss made of: CE 0.6180790662765503, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.971599578857422 EntMin 0.0
Epoch 1, Class Loss=0.6761680245399475, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.6761680245399475, Class Loss=0.6761680245399475, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/13, Loss=6.078866571187973
Loss made of: CE 0.48702818155288696, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.261292457580566 EntMin 0.0
Epoch 2, Class Loss=0.5749651193618774, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.5749651193618774, Class Loss=0.5749651193618774, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.643346655368805
Loss made of: CE 0.41888412833213806, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.65970516204834 EntMin 0.0
Epoch 3, Class Loss=0.49555498361587524, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.49555498361587524, Class Loss=0.49555498361587524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/13, Loss=5.55579052567482
Loss made of: CE 0.4206995964050293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.728543281555176 EntMin 0.0
Epoch 4, Class Loss=0.44470641016960144, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.44470641016960144, Class Loss=0.44470641016960144, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/13, Loss=5.43900563120842
Loss made of: CE 0.4809150695800781, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.691694259643555 EntMin 0.0
Epoch 5, Class Loss=0.43346744775772095, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.43346744775772095, Class Loss=0.43346744775772095, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/13, Loss=5.480164495110512
Loss made of: CE 0.463366836309433, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.443374156951904 EntMin 0.0
Epoch 6, Class Loss=0.42351117730140686, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.42351117730140686, Class Loss=0.42351117730140686, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3603479564189911, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.901926
Mean Acc: 0.686231
FreqW Acc: 0.826509
Mean IoU: 0.575903
Class IoU:
	class 0: 0.90155333
	class 1: 0.8449543
	class 2: 0.3467123
	class 3: 0.8428891
	class 4: 0.6944708
	class 5: 0.77618665
	class 6: 0.87783664
	class 7: 0.84360445
	class 8: 0.79177827
	class 9: 0.08017163
	class 10: 0.0
	class 11: 0.0
	class 12: 0.48658666
Class Acc:
	class 0: 0.9712675
	class 1: 0.8731591
	class 2: 0.92645824
	class 3: 0.8741353
	class 4: 0.8614093
	class 5: 0.90798706
	class 6: 0.8914145
	class 7: 0.92648923
	class 8: 0.96049017
	class 9: 0.08765494
	class 10: 0.0
	class 11: 0.0
	class 12: 0.64053184

federated global round: 14, step: 2
select part of clients to conduct local training
[17, 3, 12, 16]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.1829018971067855
Loss made of: CE 0.014046404510736465, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.06514310836792 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/29, Loss=7.309967378544388
Loss made of: CE 0.006296222563832998, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.973687171936035 EntMin 0.0
Epoch 1, Class Loss=0.009750870987772942, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.009750870987772942, Class Loss=0.009750870987772942, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=6.720008742902428
Loss made of: CE 0.012080039829015732, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.749391555786133 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.529979276936501
Loss made of: CE 0.011034945957362652, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.258460998535156 EntMin 0.0
Epoch 2, Class Loss=0.031334150582551956, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.031334150582551956, Class Loss=0.031334150582551956, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=6.276817353069783
Loss made of: CE 0.06615033745765686, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.685424327850342 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.596347269788384
Loss made of: CE 0.07927830517292023, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.035083293914795 EntMin 0.0
Epoch 3, Class Loss=0.0715278834104538, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.0715278834104538, Class Loss=0.0715278834104538, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=6.402773267775774
Loss made of: CE 0.1113717257976532, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.277414321899414 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.46330665871501
Loss made of: CE 0.1313210129737854, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.139145851135254 EntMin 0.0
Epoch 4, Class Loss=0.1273854374885559, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.1273854374885559, Class Loss=0.1273854374885559, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.616370858252049
Loss made of: CE 0.16327731311321259, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8908843994140625 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.324702741205693
Loss made of: CE 0.2049712836742401, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.61366081237793 EntMin 0.0
Epoch 5, Class Loss=0.1969520002603531, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.1969520002603531, Class Loss=0.1969520002603531, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.6199797064065935
Loss made of: CE 0.24502453207969666, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3253889083862305 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.236034816503524
Loss made of: CE 0.21547476947307587, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.931771278381348 EntMin 0.0
Epoch 6, Class Loss=0.2579900026321411, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.2579900026321411, Class Loss=0.2579900026321411, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=7.334657169645652
Loss made of: CE 0.006869377102702856, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.211175918579102 EntMin 0.0
Epoch 1, Batch 20/29, Loss=6.817987619701308
Loss made of: CE 0.0026406552642583847, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.66968297958374 EntMin 0.0
Epoch 1, Class Loss=0.008868548087775707, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.008868548087775707, Class Loss=0.008868548087775707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=6.455566447041929
Loss made of: CE 0.029773419722914696, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.469120025634766 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.548109652847051
Loss made of: CE 0.05321374163031578, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.033082962036133 EntMin 0.0
Epoch 2, Class Loss=0.026882601901888847, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.026882601901888847, Class Loss=0.026882601901888847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=6.319202237948775
Loss made of: CE 0.06003309041261673, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.680513381958008 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.503779017180205
Loss made of: CE 0.08565603941679001, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.019698143005371 EntMin 0.0
Epoch 3, Class Loss=0.06213317811489105, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.06213317811489105, Class Loss=0.06213317811489105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=6.562969277054071
Loss made of: CE 0.15478947758674622, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.167067527770996 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.294429827481508
Loss made of: CE 0.1879030466079712, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.172859191894531 EntMin 0.0
Epoch 4, Class Loss=0.12363333255052567, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.12363333255052567, Class Loss=0.12363333255052567, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.292053613066673
Loss made of: CE 0.1835494488477707, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.467329025268555 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.384870928525925
Loss made of: CE 0.19555029273033142, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.048375606536865 EntMin 0.0
Epoch 5, Class Loss=0.180137038230896, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.180137038230896, Class Loss=0.180137038230896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.393930868804455
Loss made of: CE 0.2465675324201584, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.810938835144043 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.08592319637537
Loss made of: CE 0.30630576610565186, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.762235164642334 EntMin 0.0
Epoch 6, Class Loss=0.2466493397951126, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.2466493397951126, Class Loss=0.2466493397951126, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.08955900222063
Loss made of: CE 0.04701191186904907, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.895830154418945 EntMin 0.0
Epoch 1, Class Loss=0.0538162887096405, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.0538162887096405, Class Loss=0.0538162887096405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.443168346583843
Loss made of: CE 0.1688256710767746, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.037806034088135 EntMin 0.0
Epoch 2, Class Loss=0.15366771817207336, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.15366771817207336, Class Loss=0.15366771817207336, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.330491429567337
Loss made of: CE 0.22329513728618622, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.830743312835693 EntMin 0.0
Epoch 3, Class Loss=0.253614604473114, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.253614604473114, Class Loss=0.253614604473114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.159739080071449
Loss made of: CE 0.2836599349975586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9455461502075195 EntMin 0.0
Epoch 4, Class Loss=0.341673344373703, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.341673344373703, Class Loss=0.341673344373703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.26408522427082
Loss made of: CE 0.5199084281921387, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0825910568237305 EntMin 0.0
Epoch 5, Class Loss=0.40049928426742554, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.40049928426742554, Class Loss=0.40049928426742554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.205080139636993
Loss made of: CE 0.4523940980434418, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.59099006652832 EntMin 0.0
Epoch 6, Class Loss=0.45595571398735046, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.45595571398735046, Class Loss=0.45595571398735046, Reg Loss=0.0
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=8.091320860385895
Loss made of: CE 0.6034501791000366, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.167721748352051 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.30383814573288
Loss made of: CE 0.5751842856407166, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.12524938583374 EntMin 0.0
Epoch 1, Class Loss=0.6115127801895142, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.6115127801895142, Class Loss=0.6115127801895142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/29, Loss=7.043733951449394
Loss made of: CE 0.4178081452846527, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.245810508728027 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.120088940858841
Loss made of: CE 0.39050740003585815, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.519998073577881 EntMin 0.0
Epoch 2, Class Loss=0.46647530794143677, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.46647530794143677, Class Loss=0.46647530794143677, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/29, Loss=7.185543832182884
Loss made of: CE 0.4544081687927246, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.070858001708984 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.73640920817852
Loss made of: CE 0.36540907621383667, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1258697509765625 EntMin 0.0
Epoch 3, Class Loss=0.4204714298248291, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.4204714298248291, Class Loss=0.4204714298248291, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/29, Loss=6.792267370223999
Loss made of: CE 0.399314820766449, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.022311210632324 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.819301494956017
Loss made of: CE 0.35649293661117554, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.856999397277832 EntMin 0.0
Epoch 4, Class Loss=0.4031650722026825, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.4031650722026825, Class Loss=0.4031650722026825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/29, Loss=6.558099746704102
Loss made of: CE 0.48291105031967163, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.637457847595215 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.644830292463302
Loss made of: CE 0.28182709217071533, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.23624324798584 EntMin 0.0
Epoch 5, Class Loss=0.4117735028266907, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.4117735028266907, Class Loss=0.4117735028266907, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/29, Loss=6.481758660078048
Loss made of: CE 0.3546140789985657, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.209397792816162 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.446667984127998
Loss made of: CE 0.30016252398490906, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.399602890014648 EntMin 0.0
Epoch 6, Class Loss=0.41799747943878174, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.41799747943878174, Class Loss=0.41799747943878174, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.3589975833892822, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.905763
Mean Acc: 0.699628
FreqW Acc: 0.835705
Mean IoU: 0.581465
Class IoU:
	class 0: 0.9097794
	class 1: 0.84413415
	class 2: 0.3384032
	class 3: 0.839413
	class 4: 0.68308663
	class 5: 0.78340405
	class 6: 0.88232815
	class 7: 0.8424349
	class 8: 0.8231995
	class 9: 0.07928362
	class 10: 0.0
	class 11: 0.0
	class 12: 0.53357947
Class Acc:
	class 0: 0.96796405
	class 1: 0.8721923
	class 2: 0.93882823
	class 3: 0.8661351
	class 4: 0.85698247
	class 5: 0.91196847
	class 6: 0.8955367
	class 7: 0.9325585
	class 8: 0.95021105
	class 9: 0.08611229
	class 10: 0.0
	class 11: 0.0
	class 12: 0.8166799

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[13, 9, 6, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=8.940561527758836
Loss made of: CE 0.08729275315999985, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.23059368133545 EntMin 0.0
Epoch 1, Class Loss=0.13204267621040344, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.13204267621040344, Class Loss=0.13204267621040344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=7.76043686568737
Loss made of: CE 0.3056439161300659, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.277087211608887 EntMin 0.0
Epoch 2, Class Loss=0.307931512594223, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.307931512594223, Class Loss=0.307931512594223, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.33754016160965
Loss made of: CE 0.3955664038658142, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.406865119934082 EntMin 0.0
Epoch 3, Class Loss=0.42946499586105347, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.42946499586105347, Class Loss=0.42946499586105347, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/11, Loss=7.096229666471482
Loss made of: CE 0.4236069619655609, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.224078178405762 EntMin 0.0
Epoch 4, Class Loss=0.511073887348175, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.511073887348175, Class Loss=0.511073887348175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/11, Loss=7.005764192342758
Loss made of: CE 0.5483013391494751, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.581775665283203 EntMin 0.0
Epoch 5, Class Loss=0.5447052121162415, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5447052121162415, Class Loss=0.5447052121162415, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.800841793417931
Loss made of: CE 0.532089114189148, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3170485496521 EntMin 0.0
Epoch 6, Class Loss=0.5466976165771484, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.5466976165771484, Class Loss=0.5466976165771484, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=11.424763804487885
Loss made of: CE 0.029072683304548264, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.433418273925781 EntMin 0.0
Epoch 1, Class Loss=0.050721779465675354, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.050721779465675354, Class Loss=0.050721779465675354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=9.604379753768445
Loss made of: CE 0.3328232169151306, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.579495429992676 EntMin 0.0
Epoch 2, Class Loss=0.1828046292066574, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.1828046292066574, Class Loss=0.1828046292066574, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=8.976894095540047
Loss made of: CE 0.4008057117462158, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.735627174377441 EntMin 0.0
Epoch 3, Class Loss=0.37313327193260193, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.37313327193260193, Class Loss=0.37313327193260193, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.642257571220398
Loss made of: CE 0.3632594048976898, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.983569145202637 EntMin 0.0
Epoch 4, Class Loss=0.5222710371017456, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.5222710371017456, Class Loss=0.5222710371017456, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.492188054323197
Loss made of: CE 0.5667999386787415, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.344801902770996 EntMin 0.0
Epoch 5, Class Loss=0.5802841186523438, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5802841186523438, Class Loss=0.5802841186523438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=8.218659257888794
Loss made of: CE 0.5229634046554565, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.7083258628845215 EntMin 0.0
Epoch 6, Class Loss=0.5781441926956177, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.5781441926956177, Class Loss=0.5781441926956177, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=9.085321975126863
Loss made of: CE 0.10633575171232224, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.168664932250977 EntMin 0.0
Epoch 1, Class Loss=0.1416027545928955, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.1416027545928955, Class Loss=0.1416027545928955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=7.9583457708358765
Loss made of: CE 0.19954806566238403, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.695583343505859 EntMin 0.0
Epoch 2, Class Loss=0.31982675194740295, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.31982675194740295, Class Loss=0.31982675194740295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.439103016257286
Loss made of: CE 0.3204127252101898, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.148808479309082 EntMin 0.0
Epoch 3, Class Loss=0.4460298418998718, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4460298418998718, Class Loss=0.4460298418998718, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/11, Loss=7.224972981214523
Loss made of: CE 0.4488825500011444, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.52626895904541 EntMin 0.0
Epoch 4, Class Loss=0.5278937816619873, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.5278937816619873, Class Loss=0.5278937816619873, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/11, Loss=6.943479540944099
Loss made of: CE 0.4825509786605835, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.487029075622559 EntMin 0.0
Epoch 5, Class Loss=0.5304645895957947, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.5304645895957947, Class Loss=0.5304645895957947, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.816352313756942
Loss made of: CE 0.4523962736129761, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7851881980896 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Class Loss=0.520871639251709, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.520871639251709, Class Loss=0.520871639251709, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=11.300037574209274
Loss made of: CE 0.04164089262485504, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.738032341003418 EntMin 0.0
Epoch 1, Class Loss=0.057684119790792465, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.057684119790792465, Class Loss=0.057684119790792465, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=9.605410515516997
Loss made of: CE 0.20352301001548767, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.665583610534668 EntMin 0.0
Epoch 2, Class Loss=0.17443476617336273, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.17443476617336273, Class Loss=0.17443476617336273, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=8.899092909693717
Loss made of: CE 0.30465802550315857, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.455455780029297 EntMin 0.0
Epoch 3, Class Loss=0.3543252646923065, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.3543252646923065, Class Loss=0.3543252646923065, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.605255749821662
Loss made of: CE 0.5333334803581238, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.916784286499023 EntMin 0.0
Epoch 4, Class Loss=0.5153575539588928, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.5153575539588928, Class Loss=0.5153575539588928, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.350267153978347
Loss made of: CE 0.5401262044906616, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.970127105712891 EntMin 0.0
Epoch 5, Class Loss=0.5943370461463928, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.5943370461463928, Class Loss=0.5943370461463928, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=8.106953608989716
Loss made of: CE 0.6045101881027222, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.342256546020508 EntMin 0.0
Epoch 6, Class Loss=0.5758727192878723, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.5758727192878723, Class Loss=0.5758727192878723, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5998966693878174, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.867670
Mean Acc: 0.555035
FreqW Acc: 0.772316
Mean IoU: 0.445974
Class IoU:
	class 0: 0.87875855
	class 1: 0.7821453
	class 2: 0.27862617
	class 3: 0.76605195
	class 4: 0.61077106
	class 5: 0.72194606
	class 6: 0.65529656
	class 7: 0.78618413
	class 8: 0.7610691
	class 9: 0.069064066
	class 10: 0.0
	class 11: 0.0
	class 12: 0.37969017
	class 13: 0.0
	class 14: 0.0
Class Acc:
	class 0: 0.9739961
	class 1: 0.81411767
	class 2: 0.88472337
	class 3: 0.81668645
	class 4: 0.83379704
	class 5: 0.8057008
	class 6: 0.6604506
	class 7: 0.9036249
	class 8: 0.9386801
	class 9: 0.084705934
	class 10: 0.0
	class 11: 0.0
	class 12: 0.6090412
	class 13: 0.0
	class 14: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[18, 5, 20, 0]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.37141437381506
Loss made of: CE 0.031044401228427887, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.736447811126709 EntMin 0.0
Epoch 1, Class Loss=0.036316029727458954, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.036316029727458954, Class Loss=0.036316029727458954, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=7.888443244993686
Loss made of: CE 0.14852690696716309, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.107759475708008 EntMin 0.0
Epoch 2, Class Loss=0.10333673655986786, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.10333673655986786, Class Loss=0.10333673655986786, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.625891415774822
Loss made of: CE 0.28756338357925415, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.560976982116699 EntMin 0.0
Epoch 3, Class Loss=0.27801066637039185, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.27801066637039185, Class Loss=0.27801066637039185, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.678935581445694
Loss made of: CE 0.33862948417663574, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5569658279418945 EntMin 0.0
Epoch 4, Class Loss=0.40266144275665283, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.40266144275665283, Class Loss=0.40266144275665283, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.551172906160355
Loss made of: CE 0.42154747247695923, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.768357276916504 EntMin 0.0
Epoch 5, Class Loss=0.436235636472702, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.436235636472702, Class Loss=0.436235636472702, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.370767518877983
Loss made of: CE 0.44966816902160645, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.612936973571777 EntMin 0.0
Epoch 6, Class Loss=0.4374496638774872, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.4374496638774872, Class Loss=0.4374496638774872, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/12, Loss=9.11619666814804
Loss made of: CE 0.7943000793457031, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.341360092163086 EntMin 0.0
Epoch 1, Class Loss=0.8408412933349609, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.8408412933349609, Class Loss=0.8408412933349609, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=8.468674254417419
Loss made of: CE 0.6788161993026733, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.262914657592773 EntMin 0.0
Epoch 2, Class Loss=0.6624499559402466, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.6624499559402466, Class Loss=0.6624499559402466, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=8.1362499833107
Loss made of: CE 0.575671374797821, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.449589729309082 EntMin 0.0
Epoch 3, Class Loss=0.5586152076721191, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.5586152076721191, Class Loss=0.5586152076721191, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=7.793862953782082
Loss made of: CE 0.45110806822776794, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.165348052978516 EntMin 0.0
Epoch 4, Class Loss=0.44709205627441406, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.44709205627441406, Class Loss=0.44709205627441406, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.618101081252098
Loss made of: CE 0.355947345495224, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.272603511810303 EntMin 0.0
Epoch 5, Class Loss=0.3816467225551605, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.3816467225551605, Class Loss=0.3816467225551605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=7.4070221990346905
Loss made of: CE 0.33395788073539734, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.864671230316162 EntMin 0.0
Epoch 6, Class Loss=0.3606179356575012, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.3606179356575012, Class Loss=0.3606179356575012, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.304030279070139
Loss made of: CE 0.017086338251829147, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.82799243927002 EntMin 0.0
Epoch 1, Class Loss=0.026655886322259903, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.026655886322259903, Class Loss=0.026655886322259903, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=7.872381389141083
Loss made of: CE 0.10858672857284546, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.253052711486816 EntMin 0.0
Epoch 2, Class Loss=0.1134629100561142, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.1134629100561142, Class Loss=0.1134629100561142, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.8410786762833595
Loss made of: CE 0.21678362786769867, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.656383037567139 EntMin 0.0
Epoch 3, Class Loss=0.2663458287715912, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.2663458287715912, Class Loss=0.2663458287715912, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.670484718680382
Loss made of: CE 0.37270110845565796, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.144908428192139 EntMin 0.0
Epoch 4, Class Loss=0.37822121381759644, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.37822121381759644, Class Loss=0.37822121381759644, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.537557590007782
Loss made of: CE 0.33065855503082275, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.285607814788818 EntMin 0.0
Epoch 5, Class Loss=0.4464126229286194, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.4464126229286194, Class Loss=0.4464126229286194, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.47803503870964
Loss made of: CE 0.4172147512435913, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.745151042938232 EntMin 0.0
Epoch 6, Class Loss=0.4451943337917328, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.4451943337917328, Class Loss=0.4451943337917328, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=7.332700071483851
Loss made of: CE 0.06721585988998413, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.824819564819336 EntMin 0.0
Epoch 1, Class Loss=0.0802939161658287, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.0802939161658287, Class Loss=0.0802939161658287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/11, Loss=6.741163478791714
Loss made of: CE 0.17195332050323486, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.527028560638428 EntMin 0.0
Epoch 2, Class Loss=0.1963910013437271, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.1963910013437271, Class Loss=0.1963910013437271, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/11, Loss=6.522479921579361
Loss made of: CE 0.3198361396789551, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.453116416931152 EntMin 0.0
Epoch 3, Class Loss=0.32864752411842346, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.32864752411842346, Class Loss=0.32864752411842346, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/11, Loss=6.569140467047691
Loss made of: CE 0.3676236569881439, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.201825141906738 EntMin 0.0
Epoch 4, Class Loss=0.4132547080516815, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.4132547080516815, Class Loss=0.4132547080516815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/11, Loss=6.486695930361748
Loss made of: CE 0.44009730219841003, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.556600093841553 EntMin 0.0
Epoch 5, Class Loss=0.44707298278808594, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.44707298278808594, Class Loss=0.44707298278808594, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/11, Loss=6.330527901649475
Loss made of: CE 0.4115275740623474, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.663251876831055 EntMin 0.0
Epoch 6, Class Loss=0.44927462935447693, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.44927462935447693, Class Loss=0.44927462935447693, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5204196572303772, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.876932
Mean Acc: 0.589046
FreqW Acc: 0.784044
Mean IoU: 0.490141
Class IoU:
	class 0: 0.88004124
	class 1: 0.7957345
	class 2: 0.31527343
	class 3: 0.7704123
	class 4: 0.63763905
	class 5: 0.7287295
	class 6: 0.7192601
	class 7: 0.80452234
	class 8: 0.7650639
	class 9: 0.06154636
	class 10: 0.0
	class 11: 0.0
	class 12: 0.3604747
	class 13: 0.0
	class 14: 0.51342297
Class Acc:
	class 0: 0.97687787
	class 1: 0.82444865
	class 2: 0.8838855
	class 3: 0.80256236
	class 4: 0.8248021
	class 5: 0.81648976
	class 6: 0.7260301
	class 7: 0.89831334
	class 8: 0.9252386
	class 9: 0.067891404
	class 10: 0.0
	class 11: 0.0
	class 12: 0.4978206
	class 13: 0.0
	class 14: 0.59133464

federated global round: 17, step: 3
select part of clients to conduct local training
[1, 16, 6, 21]
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.12434842735529
Loss made of: CE 0.07932138442993164, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.170881271362305 EntMin 0.0
Epoch 1, Class Loss=0.08091417700052261, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.08091417700052261, Class Loss=0.08091417700052261, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=6.819826829433441
Loss made of: CE 0.1867140233516693, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5194501876831055 EntMin 0.0
Epoch 2, Class Loss=0.2073187530040741, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.2073187530040741, Class Loss=0.2073187530040741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.610333448648452
Loss made of: CE 0.31353524327278137, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.657242774963379 EntMin 0.0
Epoch 3, Class Loss=0.3305321931838989, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3305321931838989, Class Loss=0.3305321931838989, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.357242426276207
Loss made of: CE 0.31641340255737305, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.543354034423828 EntMin 0.0
Epoch 4, Class Loss=0.4040725529193878, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.4040725529193878, Class Loss=0.4040725529193878, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.3109486401081085
Loss made of: CE 0.44021886587142944, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.592909812927246 EntMin 0.0
Epoch 5, Class Loss=0.4407676160335541, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4407676160335541, Class Loss=0.4407676160335541, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=6.147186481952668
Loss made of: CE 0.40524348616600037, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.525123596191406 EntMin 0.0
Epoch 6, Class Loss=0.43307214975357056, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.43307214975357056, Class Loss=0.43307214975357056, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.04678060412407
Loss made of: CE 0.0887133777141571, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.716717720031738 EntMin 0.0
Epoch 1, Class Loss=0.08156589418649673, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.08156589418649673, Class Loss=0.08156589418649673, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=6.651248025894165
Loss made of: CE 0.19417111575603485, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.930265426635742 EntMin 0.0
Epoch 2, Class Loss=0.19817417860031128, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.19817417860031128, Class Loss=0.19817417860031128, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.519700637459755
Loss made of: CE 0.21318048238754272, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.990500450134277 EntMin 0.0
Epoch 3, Class Loss=0.32405275106430054, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.32405275106430054, Class Loss=0.32405275106430054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.426360419392585
Loss made of: CE 0.41851457953453064, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.596808433532715 EntMin 0.0
Epoch 4, Class Loss=0.40675246715545654, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.40675246715545654, Class Loss=0.40675246715545654, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.235711246728897
Loss made of: CE 0.3685009181499481, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.374650955200195 EntMin 0.0
Epoch 5, Class Loss=0.44064709544181824, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.44064709544181824, Class Loss=0.44064709544181824, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=6.022425439953804
Loss made of: CE 0.4307398200035095, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.973104000091553 EntMin 0.0
Epoch 6, Class Loss=0.42628511786460876, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.42628511786460876, Class Loss=0.42628511786460876, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.855922496318817
Loss made of: CE 0.784092366695404, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.18961238861084 EntMin 0.0
Epoch 1, Class Loss=0.8173167705535889, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.8173167705535889, Class Loss=0.8173167705535889, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/11, Loss=7.326192903518677
Loss made of: CE 0.6968790292739868, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1503095626831055 EntMin 0.0
Epoch 2, Class Loss=0.6464725136756897, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.6464725136756897, Class Loss=0.6464725136756897, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/11, Loss=6.777858683466912
Loss made of: CE 0.5287584662437439, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.613252639770508 EntMin 0.0
Epoch 3, Class Loss=0.5161350965499878, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.5161350965499878, Class Loss=0.5161350965499878, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/11, Loss=6.529170772433281
Loss made of: CE 0.4254533648490906, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.266112804412842 EntMin 0.0
Epoch 4, Class Loss=0.43298596143722534, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.43298596143722534, Class Loss=0.43298596143722534, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/11, Loss=6.299913796782493
Loss made of: CE 0.37500545382499695, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.11434268951416 EntMin 0.0
Epoch 5, Class Loss=0.3759062588214874, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.3759062588214874, Class Loss=0.3759062588214874, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/11, Loss=6.193641930818558
Loss made of: CE 0.2711387574672699, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.315384864807129 EntMin 0.0
Epoch 6, Class Loss=0.3315058648586273, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.3315058648586273, Class Loss=0.3315058648586273, Reg Loss=0.0
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=7.172494271816686
Loss made of: CE 0.00704491650685668, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.438675880432129 EntMin 0.0
Epoch 1, Class Loss=0.014014048501849174, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.014014048501849174, Class Loss=0.014014048501849174, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=6.988547595962882
Loss made of: CE 0.07023981213569641, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.322112560272217 EntMin 0.0
Epoch 2, Class Loss=0.058896392583847046, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.058896392583847046, Class Loss=0.058896392583847046, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=7.0036351680755615
Loss made of: CE 0.14858905971050262, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.630240440368652 EntMin 0.0
Epoch 3, Class Loss=0.148830384016037, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.148830384016037, Class Loss=0.148830384016037, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=7.110958136618137
Loss made of: CE 0.2817533016204834, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.120791912078857 EntMin 0.0
Epoch 4, Class Loss=0.24702301621437073, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.24702301621437073, Class Loss=0.24702301621437073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=6.8906045138835905
Loss made of: CE 0.28577113151550293, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.878594875335693 EntMin 0.0
Epoch 5, Class Loss=0.30187007784843445, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.30187007784843445, Class Loss=0.30187007784843445, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=6.973503315448761
Loss made of: CE 0.3009371757507324, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1236982345581055 EntMin 0.0
Epoch 6, Class Loss=0.3276224434375763, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.3276224434375763, Class Loss=0.3276224434375763, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.47772711515426636, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.877692
Mean Acc: 0.600499
FreqW Acc: 0.791458
Mean IoU: 0.491505
Class IoU:
	class 0: 0.8886884
	class 1: 0.8136174
	class 2: 0.27199492
	class 3: 0.79411536
	class 4: 0.647354
	class 5: 0.7399923
	class 6: 0.7563284
	class 7: 0.7874995
	class 8: 0.7784666
	class 9: 0.081139855
	class 10: 0.0
	class 11: 0.0
	class 12: 0.35505766
	class 13: 0.16810417
	class 14: 0.29021487
Class Acc:
	class 0: 0.9747771
	class 1: 0.84692913
	class 2: 0.9341466
	class 3: 0.83457124
	class 4: 0.8417008
	class 5: 0.8494685
	class 6: 0.7655262
	class 7: 0.92501056
	class 8: 0.9277115
	class 9: 0.09515668
	class 10: 0.0
	class 11: 0.0
	class 12: 0.48930588
	class 13: 0.23020774
	class 14: 0.29297507

federated global round: 18, step: 3
select part of clients to conduct local training
[8, 2, 17, 14]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=5.867584718763828
Loss made of: CE 0.05137057229876518, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.611888408660889 EntMin 0.0
Epoch 1, Class Loss=0.043802496045827866, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.043802496045827866, Class Loss=0.043802496045827866, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=5.618051953613758
Loss made of: CE 0.09654588997364044, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.299893856048584 EntMin 0.0
Epoch 2, Class Loss=0.11645741015672684, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.11645741015672684, Class Loss=0.11645741015672684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.667255420982838
Loss made of: CE 0.179709792137146, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.486368179321289 EntMin 0.0
Epoch 3, Class Loss=0.19076475501060486, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.19076475501060486, Class Loss=0.19076475501060486, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.5812877580523494
Loss made of: CE 0.24309596419334412, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.228819370269775 EntMin 0.0
Epoch 4, Class Loss=0.2640903890132904, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.2640903890132904, Class Loss=0.2640903890132904, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.573882764577865
Loss made of: CE 0.27643221616744995, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5649542808532715 EntMin 0.0
Epoch 5, Class Loss=0.30180761218070984, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.30180761218070984, Class Loss=0.30180761218070984, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.559839835762977
Loss made of: CE 0.26594892144203186, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.316049575805664 EntMin 0.0
Epoch 6, Class Loss=0.3261188566684723, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.3261188566684723, Class Loss=0.3261188566684723, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=6.207850992493332
Loss made of: CE 0.032078817486763, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.73596715927124 EntMin 0.0
Epoch 1, Class Loss=0.04292683303356171, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.04292683303356171, Class Loss=0.04292683303356171, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=5.8805830657482145
Loss made of: CE 0.11446284502744675, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.548954010009766 EntMin 0.0
Epoch 2, Class Loss=0.10665849596261978, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.10665849596261978, Class Loss=0.10665849596261978, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.80566040277481
Loss made of: CE 0.17806005477905273, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.906190395355225 EntMin 0.0
Epoch 3, Class Loss=0.18004664778709412, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.18004664778709412, Class Loss=0.18004664778709412, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.829508952796459
Loss made of: CE 0.2417338341474533, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.738964080810547 EntMin 0.0
Epoch 4, Class Loss=0.25092729926109314, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.25092729926109314, Class Loss=0.25092729926109314, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.7271925747394565
Loss made of: CE 0.27424055337905884, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.671486854553223 EntMin 0.0
Epoch 5, Class Loss=0.2896571457386017, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.2896571457386017, Class Loss=0.2896571457386017, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.705839887261391
Loss made of: CE 0.31165000796318054, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.989980697631836 EntMin 0.0
Epoch 6, Class Loss=0.3193203806877136, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.3193203806877136, Class Loss=0.3193203806877136, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.446131531056016
Loss made of: CE 0.007509845308959484, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.339335918426514 EntMin 0.0
Epoch 1, Class Loss=0.023725753650069237, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.023725753650069237, Class Loss=0.023725753650069237, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=7.5399409051984545
Loss made of: CE 0.0633947104215622, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.643641948699951 EntMin 0.0
Epoch 2, Class Loss=0.08349500596523285, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.08349500596523285, Class Loss=0.08349500596523285, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=7.3060082957148555
Loss made of: CE 0.1919090449810028, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.68275260925293 EntMin 0.0
Epoch 3, Class Loss=0.19596537947654724, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.19596537947654724, Class Loss=0.19596537947654724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=7.07941585034132
Loss made of: CE 0.283828467130661, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5492753982543945 EntMin 0.0
Epoch 4, Class Loss=0.257132351398468, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.257132351398468, Class Loss=0.257132351398468, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=6.931492638587952
Loss made of: CE 0.28027868270874023, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.248683452606201 EntMin 0.0
Epoch 5, Class Loss=0.33459365367889404, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.33459365367889404, Class Loss=0.33459365367889404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=7.041309428215027
Loss made of: CE 0.2895195186138153, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.650472164154053 EntMin 0.0
Epoch 6, Class Loss=0.37111395597457886, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.37111395597457886, Class Loss=0.37111395597457886, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=6.158749603666365
Loss made of: CE 0.035930950194597244, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4219970703125 EntMin 0.0
Epoch 1, Class Loss=0.035666171461343765, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.035666171461343765, Class Loss=0.035666171461343765, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=5.85709782987833
Loss made of: CE 0.12230950593948364, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7769880294799805 EntMin 0.0
Epoch 2, Class Loss=0.10618829727172852, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.10618829727172852, Class Loss=0.10618829727172852, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.980423976480961
Loss made of: CE 0.16582731902599335, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.61098051071167 EntMin 0.0
Epoch 3, Class Loss=0.17980286478996277, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.17980286478996277, Class Loss=0.17980286478996277, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/11, Loss=5.872027802467346
Loss made of: CE 0.24504613876342773, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.372812271118164 EntMin 0.0
Epoch 4, Class Loss=0.2570718824863434, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.2570718824863434, Class Loss=0.2570718824863434, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.766656213998795
Loss made of: CE 0.2720791697502136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.996844291687012 EntMin 0.0
Epoch 5, Class Loss=0.2997903525829315, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.2997903525829315, Class Loss=0.2997903525829315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.875253584980965
Loss made of: CE 0.28830045461654663, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.975054740905762 EntMin 0.0
Epoch 6, Class Loss=0.3338223397731781, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.3338223397731781, Class Loss=0.3338223397731781, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4754604995250702, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.876515
Mean Acc: 0.611492
FreqW Acc: 0.796486
Mean IoU: 0.481500
Class IoU:
	class 0: 0.8981877
	class 1: 0.8296084
	class 2: 0.2709309
	class 3: 0.8108992
	class 4: 0.6452054
	class 5: 0.7360232
	class 6: 0.8100681
	class 7: 0.7681808
	class 8: 0.7638562
	class 9: 0.08586562
	class 10: 0.0
	class 11: 0.0
	class 12: 0.37726182
	class 13: 0.22577819
	class 14: 0.00064007647
Class Acc:
	class 0: 0.9695389
	class 1: 0.8703147
	class 2: 0.9440646
	class 3: 0.8748009
	class 4: 0.8632927
	class 5: 0.87224096
	class 6: 0.82295066
	class 7: 0.93478423
	class 8: 0.9391265
	class 9: 0.09671058
	class 10: 0.0
	class 11: 0.0
	class 12: 0.45584813
	class 13: 0.52806747
	class 14: 0.00064011395

federated global round: 19, step: 3
select part of clients to conduct local training
[1, 9, 8, 0]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=6.169187623262405
Loss made of: CE 0.39221882820129395, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7594099044799805 EntMin 0.0
Epoch 1, Class Loss=0.41725724935531616, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.41725724935531616, Class Loss=0.41725724935531616, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/11, Loss=5.665595042705536
Loss made of: CE 0.36980900168418884, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.507106304168701 EntMin 0.0
Epoch 2, Class Loss=0.37388864159584045, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.37388864159584045, Class Loss=0.37388864159584045, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/11, Loss=5.744440233707428
Loss made of: CE 0.33279097080230713, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.893514633178711 EntMin 0.0
Epoch 3, Class Loss=0.33756783604621887, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.33756783604621887, Class Loss=0.33756783604621887, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/11, Loss=5.65350082218647
Loss made of: CE 0.288509339094162, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.763449668884277 EntMin 0.0
Epoch 4, Class Loss=0.3181664049625397, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.3181664049625397, Class Loss=0.3181664049625397, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/11, Loss=5.607150822877884
Loss made of: CE 0.31547611951828003, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.985942840576172 EntMin 0.0
Epoch 5, Class Loss=0.31166937947273254, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.31166937947273254, Class Loss=0.31166937947273254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/11, Loss=5.516189888119698
Loss made of: CE 0.31187552213668823, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.669044494628906 EntMin 0.0
Epoch 6, Class Loss=0.3027849793434143, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.3027849793434143, Class Loss=0.3027849793434143, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=9.030530714988709
Loss made of: CE 0.7476620078086853, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.602319717407227 EntMin 0.0
Epoch 1, Class Loss=0.7491859793663025, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.7491859793663025, Class Loss=0.7491859793663025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/12, Loss=7.833375930786133
Loss made of: CE 0.46718594431877136, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.802105903625488 EntMin 0.0
Epoch 2, Class Loss=0.5716277360916138, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.5716277360916138, Class Loss=0.5716277360916138, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/12, Loss=7.377344578504562
Loss made of: CE 0.44395923614501953, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.149594306945801 EntMin 0.0
Epoch 3, Class Loss=0.429252952337265, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.429252952337265, Class Loss=0.429252952337265, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/12, Loss=7.083728224039078
Loss made of: CE 0.32232600450515747, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.752462387084961 EntMin 0.0
Epoch 4, Class Loss=0.3688502311706543, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.3688502311706543, Class Loss=0.3688502311706543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/12, Loss=7.167962187528611
Loss made of: CE 0.40045785903930664, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.375349044799805 EntMin 0.0
Epoch 5, Class Loss=0.3428088426589966, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.3428088426589966, Class Loss=0.3428088426589966, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/12, Loss=7.0632964968681335
Loss made of: CE 0.2986903786659241, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.990890026092529 EntMin 0.0
Epoch 6, Class Loss=0.3435238003730774, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3435238003730774, Class Loss=0.3435238003730774, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=5.8984213918447495
Loss made of: CE 0.4040326476097107, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.415486812591553 EntMin 0.0
Epoch 1, Class Loss=0.4029022753238678, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.4029022753238678, Class Loss=0.4029022753238678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/11, Loss=5.55208909213543
Loss made of: CE 0.35436636209487915, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.064089298248291 EntMin 0.0
Epoch 2, Class Loss=0.3660014867782593, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.3660014867782593, Class Loss=0.3660014867782593, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/11, Loss=5.567589592933655
Loss made of: CE 0.314647376537323, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.275054931640625 EntMin 0.0
Epoch 3, Class Loss=0.3417738378047943, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.3417738378047943, Class Loss=0.3417738378047943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/11, Loss=5.45508936047554
Loss made of: CE 0.303133487701416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.999733924865723 EntMin 0.0
Epoch 4, Class Loss=0.32708215713500977, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.32708215713500977, Class Loss=0.32708215713500977, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/11, Loss=5.44317616224289
Loss made of: CE 0.27554452419281006, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5435285568237305 EntMin 0.0
Epoch 5, Class Loss=0.31971797347068787, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.31971797347068787, Class Loss=0.31971797347068787, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/11, Loss=5.35927931368351
Loss made of: CE 0.31003567576408386, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3054938316345215 EntMin 0.0
Epoch 6, Class Loss=0.3116222620010376, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.3116222620010376, Class Loss=0.3116222620010376, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=5.92710727751255
Loss made of: CE 0.4021032452583313, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.480935573577881 EntMin 0.0
Epoch 1, Class Loss=0.4250739514827728, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.4250739514827728, Class Loss=0.4250739514827728, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/11, Loss=5.8835632532835005
Loss made of: CE 0.3516828715801239, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.881737232208252 EntMin 0.0
Epoch 2, Class Loss=0.3726850152015686, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.3726850152015686, Class Loss=0.3726850152015686, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/11, Loss=5.701938566565514
Loss made of: CE 0.3165712356567383, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.694209098815918 EntMin 0.0
Epoch 3, Class Loss=0.3301050662994385, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.3301050662994385, Class Loss=0.3301050662994385, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/11, Loss=5.7673330187797545
Loss made of: CE 0.31828081607818604, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.402156352996826 EntMin 0.0
Epoch 4, Class Loss=0.3235362470149994, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3235362470149994, Class Loss=0.3235362470149994, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/11, Loss=5.62739160656929
Loss made of: CE 0.3073040843009949, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.776299953460693 EntMin 0.0
Epoch 5, Class Loss=0.2957351803779602, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.2957351803779602, Class Loss=0.2957351803779602, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/11, Loss=5.61972624361515
Loss made of: CE 0.27663683891296387, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.939164161682129 EntMin 0.0
Epoch 6, Class Loss=0.284334272146225, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.284334272146225, Class Loss=0.284334272146225, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4538536071777344, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.872171
Mean Acc: 0.590886
FreqW Acc: 0.790142
Mean IoU: 0.473182
Class IoU:
	class 0: 0.8953246
	class 1: 0.82261175
	class 2: 0.30305448
	class 3: 0.79859
	class 4: 0.6569794
	class 5: 0.735284
	class 6: 0.71398795
	class 7: 0.79335797
	class 8: 0.7815788
	class 9: 0.07126169
	class 10: 0.0
	class 11: 0.0
	class 12: 0.29699582
	class 13: 0.22133684
	class 14: 0.0073660617
Class Acc:
	class 0: 0.9741315
	class 1: 0.85338205
	class 2: 0.93098795
	class 3: 0.83777905
	class 4: 0.82528055
	class 5: 0.8303843
	class 6: 0.720184
	class 7: 0.9299005
	class 8: 0.9163204
	class 9: 0.077073365
	class 10: 0.0
	class 11: 0.0
	class 12: 0.32966232
	class 13: 0.6308297
	class 14: 0.0073699667

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[11, 2, 15, 6]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=8.679564386978745
Loss made of: CE 0.07751777023077011, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.214787483215332 EntMin 0.0
Epoch 1, Class Loss=0.10092450678348541, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.10092450678348541, Class Loss=0.10092450678348541, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.223084059357643
Loss made of: CE 0.22992579638957977, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.151395797729492 EntMin 0.0
Epoch 2, Class Loss=0.27784478664398193, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.27784478664398193, Class Loss=0.27784478664398193, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.580744358897209
Loss made of: CE 0.39070838689804077, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8951640129089355 EntMin 0.0
Epoch 3, Class Loss=0.4346669316291809, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.4346669316291809, Class Loss=0.4346669316291809, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.277753722667694
Loss made of: CE 0.435068815946579, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.839864253997803 EntMin 0.0
Epoch 4, Class Loss=0.5442442893981934, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5442442893981934, Class Loss=0.5442442893981934, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.141476100683212
Loss made of: CE 0.5101398229598999, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.228436470031738 EntMin 0.0
Epoch 5, Class Loss=0.5860787630081177, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5860787630081177, Class Loss=0.5860787630081177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.1448086977005
Loss made of: CE 0.7357912063598633, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.552262783050537 EntMin 0.0
Epoch 6, Class Loss=0.6521998643875122, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.6521998643875122, Class Loss=0.6521998643875122, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=8.654605744034052
Loss made of: CE 0.05196361988782883, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.317160606384277 EntMin 0.0
Epoch 1, Class Loss=0.07376863807439804, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.07376863807439804, Class Loss=0.07376863807439804, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.102238090336323
Loss made of: CE 0.25584131479263306, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.103984832763672 EntMin 0.0
Epoch 2, Class Loss=0.27497559785842896, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.27497559785842896, Class Loss=0.27497559785842896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.535897555947304
Loss made of: CE 0.3903600871562958, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.323053359985352 EntMin 0.0
Epoch 3, Class Loss=0.4146197736263275, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.4146197736263275, Class Loss=0.4146197736263275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.368385508656502
Loss made of: CE 0.4537191390991211, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.596019268035889 EntMin 0.0
Epoch 4, Class Loss=0.5369229912757874, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.5369229912757874, Class Loss=0.5369229912757874, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.075583434104919
Loss made of: CE 0.6054143309593201, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.687932968139648 EntMin 0.0
Epoch 5, Class Loss=0.5872599482536316, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5872599482536316, Class Loss=0.5872599482536316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.150713348388672
Loss made of: CE 0.7202377319335938, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2584614753723145 EntMin 0.0
Epoch 6, Class Loss=0.6417129039764404, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.6417129039764404, Class Loss=0.6417129039764404, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.443206499516965
Loss made of: CE 0.031879156827926636, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0119171142578125 EntMin 0.0
Epoch 1, Class Loss=0.07749079912900925, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.07749079912900925, Class Loss=0.07749079912900925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=6.912638252973556
Loss made of: CE 0.27370914816856384, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.033607006072998 EntMin 0.0
Epoch 2, Class Loss=0.24685624241828918, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.24685624241828918, Class Loss=0.24685624241828918, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.5359329879283905
Loss made of: CE 0.5131603479385376, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.056802749633789 EntMin 0.0
Epoch 3, Class Loss=0.4164995849132538, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.4164995849132538, Class Loss=0.4164995849132538, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.484120973944664
Loss made of: CE 0.431936115026474, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8592848777771 EntMin 0.0
Epoch 4, Class Loss=0.5390271544456482, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.5390271544456482, Class Loss=0.5390271544456482, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.199061715602875
Loss made of: CE 0.6435145139694214, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.819402694702148 EntMin 0.0
Epoch 5, Class Loss=0.5983712673187256, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.5983712673187256, Class Loss=0.5983712673187256, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 10/12, Loss=6.189015585184097
Loss made of: CE 0.6174649000167847, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.044772148132324 EntMin 0.0
Epoch 6, Class Loss=0.6334328651428223, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.6334328651428223, Class Loss=0.6334328651428223, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=7.923347068950534
Loss made of: CE 0.1629539430141449, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.8386077880859375 EntMin 0.0
Epoch 1, Batch 20/97, Loss=6.987604437023402
Loss made of: CE 0.1465991735458374, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.552577018737793 EntMin 0.0
Epoch 1, Batch 30/97, Loss=6.084784049913287
Loss made of: CE 0.14958813786506653, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.720465660095215 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.843412379547954
Loss made of: CE 0.13840369880199432, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.392995834350586 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.886723203212023
Loss made of: CE 0.08618329465389252, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.491069793701172 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.80327674113214
Loss made of: CE 0.043152954429388046, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.406680107116699 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.657692255824804
Loss made of: CE 0.09852981567382812, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.749309062957764 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.472534593194723
Loss made of: CE 0.13967905938625336, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8054518699646 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.254276867955923
Loss made of: CE 0.10949988663196564, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.093103408813477 EntMin 0.0
Epoch 1, Class Loss=0.09818559885025024, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.09818559885025024, Class Loss=0.09818559885025024, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/97, Loss=5.51849028468132
Loss made of: CE 0.1648397594690323, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.5427093505859375 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.448786789178849
Loss made of: CE 0.18899807333946228, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.647166728973389 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.180535382032394
Loss made of: CE 0.15599684417247772, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.904012680053711 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.255279135704041
Loss made of: CE 0.12237030267715454, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.751593589782715 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.220710471272469
Loss made of: CE 0.2005145400762558, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.918513298034668 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.185320076346398
Loss made of: CE 0.1589328795671463, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.073366641998291 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.136578024923802
Loss made of: CE 0.1628192663192749, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.219983100891113 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.336999119073153
Loss made of: CE 0.22207650542259216, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.311907768249512 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.318820437043906
Loss made of: CE 0.16879522800445557, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.058283805847168 EntMin 0.0
Epoch 2, Class Loss=0.17923970520496368, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.17923970520496368, Class Loss=0.17923970520496368, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/97, Loss=5.216917350888252
Loss made of: CE 0.1806856095790863, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.834004878997803 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.11871138215065
Loss made of: CE 0.27555346488952637, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.577603816986084 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.361263179779053
Loss made of: CE 0.2852246165275574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.867181301116943 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.33494146168232
Loss made of: CE 0.26267680525779724, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.670792579650879 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.320115348696708
Loss made of: CE 0.2013569176197052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.774653911590576 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.3713006868958475
Loss made of: CE 0.1546913981437683, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.008712291717529 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.1298098638653755
Loss made of: CE 0.2340995967388153, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.858209133148193 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.209383773803711
Loss made of: CE 0.1862487643957138, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.910903453826904 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.2382953584194185
Loss made of: CE 0.2576059103012085, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.901514053344727 EntMin 0.0
Epoch 3, Class Loss=0.2210695594549179, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.2210695594549179, Class Loss=0.2210695594549179, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/97, Loss=5.1574298843741415
Loss made of: CE 0.26248735189437866, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.866954803466797 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.144456593692302
Loss made of: CE 0.24279336631298065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.597748756408691 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.911942665278912
Loss made of: CE 0.1854126751422882, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.459414005279541 EntMin 0.0
Epoch 4, Batch 40/97, Loss=5.042169466614723
Loss made of: CE 0.30046841502189636, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.801704406738281 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.004088099300861
Loss made of: CE 0.20269875228405, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.753792762756348 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.95796196013689
Loss made of: CE 0.2633889615535736, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.777598857879639 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.055834037065506
Loss made of: CE 0.17703837156295776, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.000186920166016 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.297615697979927
Loss made of: CE 0.2524056136608124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.874407768249512 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.968880693614483
Loss made of: CE 0.21707245707511902, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.778899192810059 EntMin 0.0
Epoch 4, Class Loss=0.24872376024723053, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.24872376024723053, Class Loss=0.24872376024723053, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/97, Loss=4.981043612957
Loss made of: CE 0.254774272441864, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.852944374084473 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.940767601132393
Loss made of: CE 0.2664731442928314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.559942245483398 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.92998048812151
Loss made of: CE 0.2959425747394562, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.341081619262695 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.96216696202755
Loss made of: CE 0.3183577358722687, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.413985729217529 EntMin 0.0
Epoch 5, Batch 50/97, Loss=5.118310996890068
Loss made of: CE 0.23588864505290985, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.947373867034912 EntMin 0.0
Epoch 5, Batch 60/97, Loss=5.133416414260864
Loss made of: CE 0.27745521068573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.991825580596924 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.89457093924284
Loss made of: CE 0.2614123821258545, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363513469696045 EntMin 0.0
Epoch 5, Batch 80/97, Loss=5.168017818033695
Loss made of: CE 0.212581068277359, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3950300216674805 EntMin 0.0
Epoch 5, Batch 90/97, Loss=5.053145833313465
Loss made of: CE 0.24344035983085632, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.377523899078369 EntMin 0.0
Epoch 5, Class Loss=0.27822497487068176, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.27822497487068176, Class Loss=0.27822497487068176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/97, Loss=5.149959468841553
Loss made of: CE 0.3343600630760193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.202658653259277 EntMin 0.0
Epoch 6, Batch 20/97, Loss=5.254979461431503
Loss made of: CE 0.2509649991989136, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.437030792236328 EntMin 0.0
Epoch 6, Batch 30/97, Loss=5.102571552991867
Loss made of: CE 0.353694885969162, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.168670654296875 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.103946989774704
Loss made of: CE 0.372531533241272, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.977146148681641 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.966284325718879
Loss made of: CE 0.3190537393093109, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.565296173095703 EntMin 0.0
Epoch 6, Batch 60/97, Loss=5.0757450491189955
Loss made of: CE 0.38752779364585876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.791822910308838 EntMin 0.0
Epoch 6, Batch 70/97, Loss=5.066661834716797
Loss made of: CE 0.39680182933807373, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.453813552856445 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.895742373168469
Loss made of: CE 0.29488110542297363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.900360107421875 EntMin 0.0
Epoch 6, Batch 90/97, Loss=5.189804747700691
Loss made of: CE 0.3817019462585449, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.898471832275391 EntMin 0.0
Epoch 6, Class Loss=0.3405819535255432, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.3405819535255432, Class Loss=0.3405819535255432, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.6568813323974609, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.808108
Mean Acc: 0.535859
FreqW Acc: 0.679452
Mean IoU: 0.414330
Class IoU:
	class 0: 0.8165585
	class 1: 0.82651645
	class 2: 0.28226826
	class 3: 0.75584114
	class 4: 0.64368653
	class 5: 0.70637167
	class 6: 0.70023364
	class 7: 0.7463995
	class 8: 0.79224443
	class 9: 0.1498078
	class 10: 0.0
	class 11: 0.0
	class 12: 0.41171688
	class 13: 0.21195832
	class 14: 0.0
	class 15: 0.0
	class 16: 0.0
Class Acc:
	class 0: 0.9674326
	class 1: 0.88181
	class 2: 0.961509
	class 3: 0.78282094
	class 4: 0.8499132
	class 5: 0.8523363
	class 6: 0.7217937
	class 7: 0.9222885
	class 8: 0.8914074
	class 9: 0.1869262
	class 10: 0.0
	class 11: 0.0
	class 12: 0.47784156
	class 13: 0.61352885
	class 14: 0.0
	class 15: 0.0
	class 16: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[20, 2, 24, 4]
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=6.49621820487082
Loss made of: CE 0.0504114143550396, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.007033824920654 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.252839395403862
Loss made of: CE 0.053326331079006195, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.875868797302246 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.345876180380583
Loss made of: CE 0.06020452827215195, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.864885330200195 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.390237717330455
Loss made of: CE 0.07990160584449768, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478611946105957 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.061915129050613
Loss made of: CE 0.04238688945770264, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.122866153717041 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.188892612233758
Loss made of: CE 0.043686073273420334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.885683536529541 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.16328703649342
Loss made of: CE 0.04796717315912247, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.478730201721191 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.210486799106002
Loss made of: CE 0.07150242477655411, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8624162673950195 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/97, Loss=5.223130977526307
Loss made of: CE 0.04795408993959427, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.027572154998779 EntMin 0.0
Epoch 1, Class Loss=0.06439434736967087, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.06439434736967087, Class Loss=0.06439434736967087, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/97, Loss=4.956325925886631
Loss made of: CE 0.14677146077156067, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.059240341186523 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.9599452368915085
Loss made of: CE 0.09330792725086212, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.989904880523682 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.929943908005953
Loss made of: CE 0.0761316642165184, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.813436985015869 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.112734741717577
Loss made of: CE 0.0691811740398407, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.413397789001465 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.972442886978388
Loss made of: CE 0.09436500072479248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.518942356109619 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.917072509229183
Loss made of: CE 0.15237554907798767, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.775601863861084 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.3614794090390205
Loss made of: CE 0.17959782481193542, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.091487884521484 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.862659201025963
Loss made of: CE 0.07757893204689026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.48303747177124 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.833954416960478
Loss made of: CE 0.08164118975400925, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.48120641708374 EntMin 0.0
Epoch 2, Class Loss=0.13201120495796204, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.13201120495796204, Class Loss=0.13201120495796204, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/97, Loss=5.0604739636182785
Loss made of: CE 0.1761784553527832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.782105445861816 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.04444582760334
Loss made of: CE 0.17157569527626038, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.957691669464111 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.856900703907013
Loss made of: CE 0.16607950627803802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.615704536437988 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.826199086010456
Loss made of: CE 0.30065202713012695, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.314500331878662 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.011887760460377
Loss made of: CE 0.19401952624320984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623438835144043 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.317286173999309
Loss made of: CE 0.15448647737503052, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.826037883758545 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.768633010983467
Loss made of: CE 0.13759498298168182, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.374480247497559 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.833266873657704
Loss made of: CE 0.13770656287670135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.156754493713379 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.077843574434519
Loss made of: CE 0.11284011602401733, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.346639633178711 EntMin 0.0
Epoch 3, Class Loss=0.18072627484798431, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.18072627484798431, Class Loss=0.18072627484798431, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/97, Loss=5.131311652064324
Loss made of: CE 0.22277995944023132, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.756260871887207 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.118918953835964
Loss made of: CE 0.27196818590164185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.831721305847168 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.89655400365591
Loss made of: CE 0.2292461097240448, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2602362632751465 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.841649940609932
Loss made of: CE 0.17615672945976257, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.290712833404541 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.01906591206789
Loss made of: CE 0.24466678500175476, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.851934909820557 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.743199644982814
Loss made of: CE 0.2306264489889145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.581655502319336 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.809019182622433
Loss made of: CE 0.240925595164299, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.814652442932129 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.122305126488209
Loss made of: CE 0.22563688457012177, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.67922306060791 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.969291292130947
Loss made of: CE 0.17459736764431, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.796603202819824 EntMin 0.0
Epoch 4, Class Loss=0.21366915106773376, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.21366915106773376, Class Loss=0.21366915106773376, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/97, Loss=4.931902661919594
Loss made of: CE 0.17707067728042603, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.724884033203125 EntMin 0.0
Epoch 5, Batch 20/97, Loss=5.041086435317993
Loss made of: CE 0.2058277577161789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.747386932373047 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.973561601340771
Loss made of: CE 0.24965351819992065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746473789215088 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.894628235697747
Loss made of: CE 0.17247653007507324, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.443901538848877 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.91401608735323
Loss made of: CE 0.20153486728668213, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.206839084625244 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.9235386505723
Loss made of: CE 0.2722034454345703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.750829219818115 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.7996562063694
Loss made of: CE 0.23554512858390808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.425527572631836 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.817648079991341
Loss made of: CE 0.3311725854873657, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.502839088439941 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.78829894810915
Loss made of: CE 0.21464364230632782, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.039153575897217 EntMin 0.0
Epoch 5, Class Loss=0.2526141107082367, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.2526141107082367, Class Loss=0.2526141107082367, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/97, Loss=4.679025024175644
Loss made of: CE 0.4089805483818054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.351846218109131 EntMin 0.0
Epoch 6, Batch 20/97, Loss=5.082037186622619
Loss made of: CE 0.3614962100982666, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.401010036468506 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.706059682369232
Loss made of: CE 0.29912206530570984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.719966411590576 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.0650910347700115
Loss made of: CE 0.3309614360332489, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.711240291595459 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.946312960982323
Loss made of: CE 0.2978355586528778, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.273947238922119 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.777613867819309
Loss made of: CE 0.29865601658821106, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464438438415527 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.7795739531517025
Loss made of: CE 0.4088923931121826, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3654069900512695 EntMin 0.0
Epoch 6, Batch 80/97, Loss=5.027148866653443
Loss made of: CE 0.36462315917015076, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.964768409729004 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.806245651841164
Loss made of: CE 0.30632925033569336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.311369895935059 EntMin 0.0
Epoch 6, Class Loss=0.3240461051464081, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.3240461051464081, Class Loss=0.3240461051464081, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.070365166664123
Loss made of: CE 0.8902630805969238, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.93098783493042 EntMin 0.0
Epoch 1, Class Loss=0.7614912986755371, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.7614912986755371, Class Loss=0.7614912986755371, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=5.774859392642975
Loss made of: CE 0.6004942655563354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.718313694000244 EntMin 0.0
Epoch 2, Class Loss=0.6832042932510376, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.6832042932510376, Class Loss=0.6832042932510376, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=5.683815050125122
Loss made of: CE 0.6398590803146362, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.307170867919922 EntMin 0.0
Epoch 3, Class Loss=0.6511085033416748, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.6511085033416748, Class Loss=0.6511085033416748, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=5.6872892618179325
Loss made of: CE 0.6185441613197327, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0726637840271 EntMin 0.0
Epoch 4, Class Loss=0.6192454695701599, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.6192454695701599, Class Loss=0.6192454695701599, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.453482216596603
Loss made of: CE 0.6056799292564392, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.074479103088379 EntMin 0.0
Epoch 5, Class Loss=0.5888035297393799, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5888035297393799, Class Loss=0.5888035297393799, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=5.526704135537147
Loss made of: CE 0.62996906042099, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.830007553100586 EntMin 0.0
Epoch 6, Class Loss=0.5647997856140137, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.5647997856140137, Class Loss=0.5647997856140137, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.214863300137222
Loss made of: CE 0.040320128202438354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7773332595825195 EntMin 0.0
Epoch 1, Class Loss=0.059536442160606384, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.059536442160606384, Class Loss=0.059536442160606384, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.220206730812788
Loss made of: CE 0.2449268102645874, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.505162715911865 EntMin 0.0
Epoch 2, Class Loss=0.15860384702682495, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.15860384702682495, Class Loss=0.15860384702682495, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.289064529538154
Loss made of: CE 0.29712510108947754, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.22906494140625 EntMin 0.0
Epoch 3, Class Loss=0.26946157217025757, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.26946157217025757, Class Loss=0.26946157217025757, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 10/12, Loss=5.476312920451164
Loss made of: CE 0.41775164008140564, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.339156627655029 EntMin 0.0
Epoch 4, Class Loss=0.4035568833351135, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.4035568833351135, Class Loss=0.4035568833351135, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.494346341490745
Loss made of: CE 0.3678780794143677, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.030308723449707 EntMin 0.0
Epoch 5, Class Loss=0.4945862889289856, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.4945862889289856, Class Loss=0.4945862889289856, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.458340638875962
Loss made of: CE 0.532710075378418, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.736898422241211 EntMin 0.0
Epoch 6, Class Loss=0.5396754741668701, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.5396754741668701, Class Loss=0.5396754741668701, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.341215533763171
Loss made of: CE 0.040938977152109146, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.239545822143555 EntMin 0.0
Epoch 1, Class Loss=0.07996885478496552, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.07996885478496552, Class Loss=0.07996885478496552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.469147641211748
Loss made of: CE 0.23630361258983612, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.059512615203857 EntMin 0.0
Epoch 2, Class Loss=0.2003183215856552, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.2003183215856552, Class Loss=0.2003183215856552, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.464230459928513
Loss made of: CE 0.44899433851242065, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.090132236480713 EntMin 0.0
Epoch 3, Class Loss=0.308999240398407, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.308999240398407, Class Loss=0.308999240398407, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=5.460385572910309
Loss made of: CE 0.5302929878234863, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.146340370178223 EntMin 0.0
Epoch 4, Class Loss=0.40635523200035095, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.40635523200035095, Class Loss=0.40635523200035095, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 10/12, Loss=5.547848945856094
Loss made of: CE 0.4311959445476532, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.982797622680664 EntMin 0.0
Epoch 5, Class Loss=0.4988783001899719, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.4988783001899719, Class Loss=0.4988783001899719, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.447149577736854
Loss made of: CE 0.561998724937439, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.055322170257568 EntMin 0.0
Epoch 6, Class Loss=0.5677487850189209, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5677487850189209, Class Loss=0.5677487850189209, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5987071990966797, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.809652
Mean Acc: 0.531653
FreqW Acc: 0.681460
Mean IoU: 0.409334
Class IoU:
	class 0: 0.81907094
	class 1: 0.82755
	class 2: 0.27892306
	class 3: 0.7035753
	class 4: 0.6410642
	class 5: 0.6952379
	class 6: 0.66853845
	class 7: 0.7408951
	class 8: 0.7903884
	class 9: 0.13545468
	class 10: 0.0
	class 11: 0.0
	class 12: 0.40750617
	class 13: 0.2202339
	class 14: 0.0
	class 15: 0.030232938
	class 16: 0.0
Class Acc:
	class 0: 0.9691568
	class 1: 0.87985235
	class 2: 0.96339256
	class 3: 0.7183032
	class 4: 0.841751
	class 5: 0.8697691
	class 6: 0.6826522
	class 7: 0.92610055
	class 8: 0.89232916
	class 9: 0.16229555
	class 10: 0.0
	class 11: 0.0
	class 12: 0.46590456
	class 13: 0.63634855
	class 14: 0.0
	class 15: 0.030245068
	class 16: 0.0

federated global round: 22, step: 4
select part of clients to conduct local training
[21, 0, 25, 23]
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=5.708084815740586
Loss made of: CE 0.09288349002599716, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.444087028503418 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.248294218629598
Loss made of: CE 0.08981980383396149, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.080788612365723 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.954702947288752
Loss made of: CE 0.06589359045028687, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.810112476348877 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.999766844511032
Loss made of: CE 0.05522412806749344, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4757914543151855 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.976857142522931
Loss made of: CE 0.03192710876464844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.57557487487793 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.942010269872844
Loss made of: CE 0.03508689999580383, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.659802436828613 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.921850880235434
Loss made of: CE 0.04362563043832779, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.833070755004883 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.7894693244248625
Loss made of: CE 0.021810155361890793, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6223320960998535 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.652453265339136
Loss made of: CE 0.028422093018889427, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.825322151184082 EntMin 0.0
Epoch 1, Class Loss=0.05148496851325035, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.05148496851325035, Class Loss=0.05148496851325035, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.732898367941379
Loss made of: CE 0.12625698745250702, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.555561542510986 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.835219153761864
Loss made of: CE 0.06807699799537659, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.257831573486328 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.7995862260460855
Loss made of: CE 0.12470050901174545, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.429688453674316 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.66925695836544
Loss made of: CE 0.07957682013511658, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8882532119750977 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.748745953291655
Loss made of: CE 0.07578095048666, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.339423656463623 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.809094672650099
Loss made of: CE 0.10707836598157883, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.066091060638428 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.720328779146075
Loss made of: CE 0.0622672401368618, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.184440612792969 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.964460202306509
Loss made of: CE 0.1218174546957016, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.797735691070557 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.610625718906522
Loss made of: CE 0.08460613340139389, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0002241134643555 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.10271495580673218, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.10271495580673218, Class Loss=0.10271495580673218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.7636733025312425
Loss made of: CE 0.17422768473625183, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.644341468811035 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.708627401292324
Loss made of: CE 0.14082513749599457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5890212059021 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.803489352762699
Loss made of: CE 0.16261708736419678, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.261065483093262 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.595173701643944
Loss made of: CE 0.10324045270681381, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.569948673248291 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.808785307407379
Loss made of: CE 0.18391983211040497, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.458420753479004 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.82163675352931
Loss made of: CE 0.12964120507240295, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.383972644805908 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.764359910041094
Loss made of: CE 0.13898228108882904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.381511688232422 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.786985374987125
Loss made of: CE 0.18790149688720703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.444268703460693 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.6847293227910995
Loss made of: CE 0.12727844715118408, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.08590841293335 EntMin 0.0
Epoch 3, Class Loss=0.15207567811012268, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.15207567811012268, Class Loss=0.15207567811012268, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.863294258713722
Loss made of: CE 0.22199222445487976, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.719548225402832 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.693000069260597
Loss made of: CE 0.20092496275901794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.451931953430176 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.939445002377033
Loss made of: CE 0.22298574447631836, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7793426513671875 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.668737012147903
Loss made of: CE 0.23653170466423035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.511343002319336 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.7911860078573225
Loss made of: CE 0.1657639741897583, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.302370548248291 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.826293878257275
Loss made of: CE 0.20317909121513367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.828776836395264 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.744307364523411
Loss made of: CE 0.18860147893428802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.368903160095215 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.607087527960539
Loss made of: CE 0.1562352478504181, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1750593185424805 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.9443314030766485
Loss made of: CE 0.1849808394908905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.550473213195801 EntMin 0.0
Epoch 4, Class Loss=0.19873256981372833, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.19873256981372833, Class Loss=0.19873256981372833, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.818325184285641
Loss made of: CE 0.2538363039493561, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.861264705657959 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.706833647191525
Loss made of: CE 0.25024914741516113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.504151344299316 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.790432433784008
Loss made of: CE 0.2992824912071228, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.704733371734619 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.924181641638279
Loss made of: CE 0.26915666460990906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.972302436828613 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.637304833531379
Loss made of: CE 0.2431504726409912, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400116920471191 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.933489753305912
Loss made of: CE 0.22244159877300262, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.540777683258057 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.852812850475312
Loss made of: CE 0.23297475278377533, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726801872253418 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.7118499964475635
Loss made of: CE 0.22893983125686646, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.633423328399658 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.888826295733452
Loss made of: CE 0.2769888639450073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.771177291870117 EntMin 0.0
Epoch 5, Class Loss=0.24975435435771942, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.24975435435771942, Class Loss=0.24975435435771942, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.700167196989059
Loss made of: CE 0.36050471663475037, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.697886943817139 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.841202440857887
Loss made of: CE 0.3223250210285187, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.002901077270508 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.987804663181305
Loss made of: CE 0.2831481695175171, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.280508995056152 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.05646502673626
Loss made of: CE 0.3507133424282074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.621060371398926 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.730636066198349
Loss made of: CE 0.34297487139701843, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.116961479187012 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.7802654027938845
Loss made of: CE 0.3019448220729828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.694972991943359 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.702036648988724
Loss made of: CE 0.3101605176925659, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.334018707275391 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.731103149056435
Loss made of: CE 0.2510823607444763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.463225364685059 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.6398633271455765
Loss made of: CE 0.34136271476745605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.155364036560059 EntMin 0.0
Epoch 6, Class Loss=0.3214208483695984, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.3214208483695984, Class Loss=0.3214208483695984, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.599753750115633
Loss made of: CE 0.03704164922237396, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.348325252532959 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.008645789138972
Loss made of: CE 0.05274366959929466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.490474700927734 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.049443780072034
Loss made of: CE 0.06168448179960251, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8375725746154785 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.021441653370857
Loss made of: CE 0.04509005695581436, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.742486953735352 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.924642799794674
Loss made of: CE 0.06099105626344681, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.679247856140137 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.6475800579413775
Loss made of: CE 0.025317704305052757, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726536750793457 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.903857015445828
Loss made of: CE 0.07563293725252151, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.370439529418945 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.793820897117257
Loss made of: CE 0.097126804292202, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5672149658203125 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.913877915218473
Loss made of: CE 0.09822492301464081, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.378446578979492 EntMin 0.0
Epoch 1, Class Loss=0.05240778252482414, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.05240778252482414, Class Loss=0.05240778252482414, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.772760321199894
Loss made of: CE 0.09266281127929688, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.403970718383789 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.865013983100653
Loss made of: CE 0.12152574956417084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.332179546356201 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.992044142633676
Loss made of: CE 0.07460376620292664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.264325141906738 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.6876388669013975
Loss made of: CE 0.09753754734992981, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.533857345581055 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.9029102575033905
Loss made of: CE 0.12168258428573608, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.417804718017578 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.80004334077239
Loss made of: CE 0.06737925857305527, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.901432991027832 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.886421161517501
Loss made of: CE 0.11161645501852036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.926326751708984 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.811840391159057
Loss made of: CE 0.1608487218618393, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.106110572814941 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.858507277071476
Loss made of: CE 0.12933580577373505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.787105560302734 EntMin 0.0
Epoch 2, Class Loss=0.10514631867408752, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.10514631867408752, Class Loss=0.10514631867408752, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.8198565736413
Loss made of: CE 0.15196546912193298, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.214458465576172 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.670037991553545
Loss made of: CE 0.11397169530391693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5149641036987305 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.968509939312935
Loss made of: CE 0.2136019468307495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.673614501953125 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.916089588403702
Loss made of: CE 0.16593477129936218, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.541282653808594 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.907736232876777
Loss made of: CE 0.133962482213974, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.851263046264648 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.856664562225342
Loss made of: CE 0.12796956300735474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.743749141693115 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.987111841887236
Loss made of: CE 0.14084607362747192, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2765326499938965 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.902549882978201
Loss made of: CE 0.13328547775745392, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.248520851135254 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.861887731403113
Loss made of: CE 0.17122384905815125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.817287445068359 EntMin 0.0
Epoch 3, Class Loss=0.15441669523715973, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.15441669523715973, Class Loss=0.15441669523715973, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.657264120876789
Loss made of: CE 0.1882844865322113, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.232610702514648 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.752296480536461
Loss made of: CE 0.2099589705467224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.903796672821045 EntMin 0.0
Epoch 4, Batch 30/97, Loss=5.177012088894844
Loss made of: CE 0.21210770308971405, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.007021427154541 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.811913335323334
Loss made of: CE 0.1621539443731308, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.047310829162598 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.712830641865731
Loss made of: CE 0.15677624940872192, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7884297370910645 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.6238326072692875
Loss made of: CE 0.17923665046691895, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.52872371673584 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.758540484309196
Loss made of: CE 0.16729381680488586, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.321600914001465 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.661263380944729
Loss made of: CE 0.1819671094417572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.022653579711914 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.004345783591271
Loss made of: CE 0.19664236903190613, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.548608779907227 EntMin 0.0
Epoch 4, Class Loss=0.19774378836154938, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.19774378836154938, Class Loss=0.19774378836154938, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.623805625736713
Loss made of: CE 0.20941439270973206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.151778697967529 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.887734061479568
Loss made of: CE 0.2284911870956421, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.391318321228027 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.751785843074321
Loss made of: CE 0.24269084632396698, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2278923988342285 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.86316750049591
Loss made of: CE 0.21368508040905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.994934558868408 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.6996766299009325
Loss made of: CE 0.29643094539642334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.278882026672363 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.72174232751131
Loss made of: CE 0.2689693868160248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.617902755737305 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.817607100307941
Loss made of: CE 0.23418252170085907, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.038221836090088 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.504600425064564
Loss made of: CE 0.24936120212078094, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1972737312316895 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.5764920368790625
Loss made of: CE 0.21658030152320862, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18397331237793 EntMin 0.0
Epoch 5, Class Loss=0.2407856434583664, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.2407856434583664, Class Loss=0.2407856434583664, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.845429614186287
Loss made of: CE 0.24980542063713074, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.170670509338379 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.861185665428638
Loss made of: CE 0.2987011671066284, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.040520191192627 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.906193068623542
Loss made of: CE 0.32242345809936523, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.516010284423828 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.0510363727808
Loss made of: CE 0.26703009009361267, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.765821933746338 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.909862400591374
Loss made of: CE 0.34510502219200134, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.385238170623779 EntMin 0.0
Epoch 6, Batch 60/97, Loss=5.003084203600883
Loss made of: CE 0.3040892779827118, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.006207466125488 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.776001581549645
Loss made of: CE 0.29261839389801025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.539633274078369 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.7616769388318065
Loss made of: CE 0.2882566452026367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.799933433532715 EntMin 0.0
Epoch 6, Batch 90/97, Loss=5.063867580890656
Loss made of: CE 0.27849364280700684, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.652979373931885 EntMin 0.0
Epoch 6, Class Loss=0.31904950737953186, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.31904950737953186, Class Loss=0.31904950737953186, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.454526507854462
Loss made of: CE 0.08094343543052673, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.037672996520996 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.38744925186038
Loss made of: CE 0.0950915515422821, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.642004013061523 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.0509937655180694
Loss made of: CE 0.028068795800209045, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9126739501953125 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.928491675108671
Loss made of: CE 0.02938443422317505, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.705133438110352 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.821328604966402
Loss made of: CE 0.03536234050989151, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.070394039154053 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.942850766330958
Loss made of: CE 0.03816603869199753, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.828537464141846 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.9858257997781035
Loss made of: CE 0.04090660437941551, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.122922897338867 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.864608806557953
Loss made of: CE 0.029662257060408592, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.278748512268066 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.874959632381797
Loss made of: CE 0.02453209087252617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.792733192443848 EntMin 0.0
Epoch 1, Class Loss=0.050827015191316605, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.050827015191316605, Class Loss=0.050827015191316605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.757895035296679
Loss made of: CE 0.09727558493614197, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.795220375061035 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.967919563502074
Loss made of: CE 0.045964181423187256, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464415550231934 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.843881633505225
Loss made of: CE 0.09444916248321533, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.361468315124512 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.934447190910578
Loss made of: CE 0.11690059304237366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.904115676879883 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.8319710195064545
Loss made of: CE 0.07189180701971054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.404557228088379 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.084916172921657
Loss made of: CE 0.1259591281414032, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.183856010437012 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.729185716807843
Loss made of: CE 0.14170414209365845, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.173633575439453 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.747470296546817
Loss made of: CE 0.08966578543186188, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.29243278503418 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.6627628721296785
Loss made of: CE 0.07015043497085571, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.548652648925781 EntMin 0.0
Epoch 2, Class Loss=0.10597633570432663, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.10597633570432663, Class Loss=0.10597633570432663, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.681412225961685
Loss made of: CE 0.2353106439113617, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.34150505065918 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.765237344801426
Loss made of: CE 0.12739190459251404, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.54863166809082 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.676859080791473
Loss made of: CE 0.15430216491222382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.408397197723389 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.969702008366585
Loss made of: CE 0.14152243733406067, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9771552085876465 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.9849735729396345
Loss made of: CE 0.10089705884456635, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.603217124938965 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.7788098372519014
Loss made of: CE 0.12225287407636642, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.415541172027588 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.7435915350914
Loss made of: CE 0.22233013808727264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.302574634552002 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.819606160372496
Loss made of: CE 0.17399835586547852, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.284337997436523 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.729860306531191
Loss made of: CE 0.1672431081533432, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.573032855987549 EntMin 0.0
Epoch 3, Class Loss=0.15458601713180542, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.15458601713180542, Class Loss=0.15458601713180542, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.698995220661163
Loss made of: CE 0.18861405551433563, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.334785461425781 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.850935080647469
Loss made of: CE 0.23386113345623016, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.497530460357666 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.561474213004113
Loss made of: CE 0.1867794692516327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623517990112305 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.7165422976017
Loss made of: CE 0.23151183128356934, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.697609901428223 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.843838982284069
Loss made of: CE 0.323269248008728, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.512825012207031 EntMin 0.0
Epoch 4, Batch 60/97, Loss=5.08116148263216
Loss made of: CE 0.20691677927970886, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.172237873077393 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.688107696175575
Loss made of: CE 0.20222537219524384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128281593322754 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.700450199097395
Loss made of: CE 0.18409574031829834, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.004225730895996 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.632924146205187
Loss made of: CE 0.24730831384658813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.570799350738525 EntMin 0.0
Epoch 4, Class Loss=0.19973468780517578, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.19973468780517578, Class Loss=0.19973468780517578, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.555017305910587
Loss made of: CE 0.24612638354301453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.381077766418457 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.479891182482243
Loss made of: CE 0.22675685584545135, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.169305801391602 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.963966549932957
Loss made of: CE 0.25225406885147095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.361611366271973 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.914280399680138
Loss made of: CE 0.19119296967983246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807150840759277 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.948535221815109
Loss made of: CE 0.25071224570274353, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.68203067779541 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.843348979949951
Loss made of: CE 0.25371378660202026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.311507225036621 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.775502452254296
Loss made of: CE 0.22348438203334808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251845836639404 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.565591645240784
Loss made of: CE 0.22217826545238495, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303352355957031 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.715036289393902
Loss made of: CE 0.2893463373184204, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.628908157348633 EntMin 0.0
Epoch 5, Class Loss=0.24360942840576172, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.24360942840576172, Class Loss=0.24360942840576172, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.59551437497139
Loss made of: CE 0.2743552327156067, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.307229995727539 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.672763451933861
Loss made of: CE 0.2927591800689697, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.895723342895508 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.804115402698517
Loss made of: CE 0.2754766643047333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.665345668792725 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.739064437150955
Loss made of: CE 0.29059064388275146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.33880615234375 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.843260717391968
Loss made of: CE 0.36101073026657104, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.596846103668213 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.787389865517616
Loss made of: CE 0.27514153718948364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.665950298309326 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.9268750011920925
Loss made of: CE 0.30924320220947266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.388124465942383 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.776805818080902
Loss made of: CE 0.27441737055778503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1626877784729 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.510439193248748
Loss made of: CE 0.2785947322845459, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3438496589660645 EntMin 0.0
Epoch 6, Class Loss=0.31511637568473816, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.31511637568473816, Class Loss=0.31511637568473816, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=6.00560843013227
Loss made of: CE 0.04217962548136711, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.417233467102051 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.909395048581064
Loss made of: CE 0.043790560215711594, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.600099563598633 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.855223712325096
Loss made of: CE 0.062235839664936066, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.088371753692627 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.012085230462253
Loss made of: CE 0.06844519078731537, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.680294990539551 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.925250613875687
Loss made of: CE 0.06379406899213791, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.931284427642822 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.639556347951293
Loss made of: CE 0.06542147696018219, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.631278038024902 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.774355039559305
Loss made of: CE 0.038881566375494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.842439651489258 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.901639676839113
Loss made of: CE 0.04860153794288635, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.285333633422852 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.950066450424492
Loss made of: CE 0.03865307196974754, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.074946403503418 EntMin 0.0
Epoch 1, Class Loss=0.053699467331171036, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.053699467331171036, Class Loss=0.053699467331171036, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.8177909038960935
Loss made of: CE 0.073440782725811, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.20610237121582 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.886651013046503
Loss made of: CE 0.1312170922756195, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.002885818481445 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.678892551362514
Loss made of: CE 0.10752616822719574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2960405349731445 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.795894362777472
Loss made of: CE 0.07471756637096405, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.413865089416504 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.928996207565069
Loss made of: CE 0.07605190575122833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.686229228973389 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.1526383016258475
Loss made of: CE 0.11113753914833069, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.038764953613281 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.788813604414463
Loss made of: CE 0.10630383342504501, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.852315425872803 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.846628176793456
Loss made of: CE 0.12819209694862366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.872609615325928 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.7717319868505
Loss made of: CE 0.10849130153656006, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.77120304107666 EntMin 0.0
Epoch 2, Class Loss=0.10675427317619324, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.10675427317619324, Class Loss=0.10675427317619324, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.935217668116093
Loss made of: CE 0.14436852931976318, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.036065101623535 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.973191233724355
Loss made of: CE 0.10805895924568176, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.836214065551758 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.907932830601931
Loss made of: CE 0.19670788943767548, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.205906391143799 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.873284494876861
Loss made of: CE 0.22075456380844116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.808064937591553 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.778451189398766
Loss made of: CE 0.13605499267578125, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.833658218383789 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.727463776618242
Loss made of: CE 0.12933634221553802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.413863182067871 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.6145762272179125
Loss made of: CE 0.11614467948675156, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358043670654297 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.626746033132076
Loss made of: CE 0.16797113418579102, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.922649383544922 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.942862424254417
Loss made of: CE 0.17399531602859497, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.957036018371582 EntMin 0.0
Epoch 3, Class Loss=0.15391023457050323, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.15391023457050323, Class Loss=0.15391023457050323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.685825526714325
Loss made of: CE 0.21514098346233368, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.586670875549316 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.928336134552955
Loss made of: CE 0.21382328867912292, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.674661636352539 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.56991035938263
Loss made of: CE 0.23197047412395477, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.827390670776367 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.8999269381165504
Loss made of: CE 0.23959903419017792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.166008949279785 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.852451808750629
Loss made of: CE 0.2160676121711731, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7044267654418945 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.73057706952095
Loss made of: CE 0.20133499801158905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.455909252166748 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.900188954174519
Loss made of: CE 0.2538220286369324, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.570695877075195 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.037402513623237
Loss made of: CE 0.1675962656736374, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.306056022644043 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.595621307194233
Loss made of: CE 0.2173459529876709, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5989203453063965 EntMin 0.0
Epoch 4, Class Loss=0.19989752769470215, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.19989752769470215, Class Loss=0.19989752769470215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.806914922595024
Loss made of: CE 0.2333780825138092, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.446957588195801 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.806070590019226
Loss made of: CE 0.34681615233421326, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.511786460876465 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.894971925020218
Loss made of: CE 0.27098363637924194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3564605712890625 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.684093828499317
Loss made of: CE 0.23404192924499512, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.654521465301514 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.723221142590046
Loss made of: CE 0.24932286143302917, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.588858604431152 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.60989465713501
Loss made of: CE 0.21343466639518738, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.015314102172852 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.690765139460564
Loss made of: CE 0.23346328735351562, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.262050151824951 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.5682784646749495
Loss made of: CE 0.1768265664577484, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.103039264678955 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.852017311751842
Loss made of: CE 0.3239305019378662, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.57460355758667 EntMin 0.0
Epoch 5, Class Loss=0.25217023491859436, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.25217023491859436, Class Loss=0.25217023491859436, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.712012857198715
Loss made of: CE 0.291128933429718, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.289279460906982 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.829233431816101
Loss made of: CE 0.35449039936065674, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.24270486831665 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.78166628330946
Loss made of: CE 0.3972974717617035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.303639888763428 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.586507251858711
Loss made of: CE 0.2523820996284485, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.204455375671387 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.9237277001142505
Loss made of: CE 0.30289602279663086, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.383718967437744 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.807919152081013
Loss made of: CE 0.27466124296188354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.70114278793335 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.802183739840984
Loss made of: CE 0.3943621516227722, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.250192642211914 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.781076782941819
Loss made of: CE 0.3134223222732544, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.709858417510986 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.870275521278382
Loss made of: CE 0.30365538597106934, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.583195209503174 EntMin 0.0
Epoch 6, Class Loss=0.31996995210647583, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.31996995210647583, Class Loss=0.31996995210647583, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5185617208480835, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.856734
Mean Acc: 0.573292
FreqW Acc: 0.761453
Mean IoU: 0.449907
Class IoU:
	class 0: 0.8749723
	class 1: 0.84420913
	class 2: 0.2637394
	class 3: 0.76570094
	class 4: 0.6396814
	class 5: 0.7296785
	class 6: 0.6839645
	class 7: 0.7785689
	class 8: 0.76583785
	class 9: 0.08083116
	class 10: 0.0
	class 11: 0.0
	class 12: 0.35826835
	class 13: 0.2498433
	class 14: 0.0
	class 15: 0.61311513
	class 16: 0.0
Class Acc:
	class 0: 0.96032
	class 1: 0.90914774
	class 2: 0.9635954
	class 3: 0.78721005
	class 4: 0.84963125
	class 5: 0.851558
	class 6: 0.69027954
	class 7: 0.9334151
	class 8: 0.9356668
	class 9: 0.08932617
	class 10: 0.0
	class 11: 0.0
	class 12: 0.42525715
	class 13: 0.54941887
	class 14: 0.0
	class 15: 0.80113727
	class 16: 0.0

federated global round: 23, step: 4
select part of clients to conduct local training
[17, 3, 5, 22]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.257636006549001
Loss made of: CE 0.01328842993825674, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.241959571838379 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/97, Loss=4.325311002135277
Loss made of: CE 0.02336372248828411, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.858323574066162 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.4200893769040706
Loss made of: CE 0.01623663678765297, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.210126876831055 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.341426895931363
Loss made of: CE 0.03082147054374218, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8558995723724365 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.458931104652583
Loss made of: CE 0.017269033938646317, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.005621910095215 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.344641673658043
Loss made of: CE 0.024008547887206078, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.187844753265381 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.2230626830365505
Loss made of: CE 0.020399754866957664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3092145919799805 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.4867941055446865
Loss made of: CE 0.019912278279662132, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.392244338989258 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.517779694963247
Loss made of: CE 0.025071877986192703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.105603218078613 EntMin 0.0
Epoch 1, Class Loss=0.022724522277712822, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.022724522277712822, Class Loss=0.022724522277712822, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.4413867078721525
Loss made of: CE 0.045751627534627914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.877442359924316 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.336249085515737
Loss made of: CE 0.0685785785317421, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9860150814056396 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.804230353236198
Loss made of: CE 0.03957683965563774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.795243263244629 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.416950826346874
Loss made of: CE 0.0366390161216259, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.090577602386475 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.3281012289226055
Loss made of: CE 0.06654434651136398, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.570974349975586 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.474764649197459
Loss made of: CE 0.06079646199941635, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.563939094543457 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.662806687131524
Loss made of: CE 0.0419875904917717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.377060413360596 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.551240465044975
Loss made of: CE 0.048481814563274384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.245419502258301 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.292066925391555
Loss made of: CE 0.05167052149772644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.135927200317383 EntMin 0.0
Epoch 2, Class Loss=0.06114412471652031, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.06114412471652031, Class Loss=0.06114412471652031, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.455892553925514
Loss made of: CE 0.09210845082998276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.200457572937012 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.509142901748419
Loss made of: CE 0.10893659293651581, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.048198699951172 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.240491212159395
Loss made of: CE 0.10166919231414795, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.57418155670166 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.563722272217274
Loss made of: CE 0.10695105791091919, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.170368194580078 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.352863759547472
Loss made of: CE 0.15243205428123474, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.298893928527832 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.507197888940572
Loss made of: CE 0.1399833858013153, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.387396812438965 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.518730866909027
Loss made of: CE 0.09937772154808044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5491437911987305 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.435574105381965
Loss made of: CE 0.08102484792470932, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.070803642272949 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.5146865881979465
Loss made of: CE 0.10998621582984924, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.106614112854004 EntMin 0.0
Epoch 3, Class Loss=0.11257655918598175, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.11257655918598175, Class Loss=0.11257655918598175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.348387521505356
Loss made of: CE 0.15152500569820404, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.948579788208008 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.779453410208225
Loss made of: CE 0.17755913734436035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.730231285095215 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.463428853452205
Loss made of: CE 0.12671470642089844, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1752519607543945 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.421927823126316
Loss made of: CE 0.13548290729522705, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8868603706359863 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.635820752382278
Loss made of: CE 0.16481655836105347, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.398881435394287 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.593256551772356
Loss made of: CE 0.15812860429286957, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.385007858276367 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.427079460769892
Loss made of: CE 0.13369742035865784, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.208143711090088 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.499804998934269
Loss made of: CE 0.1477627009153366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.192962646484375 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.277027245610952
Loss made of: CE 0.10559026896953583, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.35893440246582 EntMin 0.0
Epoch 4, Class Loss=0.16464105248451233, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.16464105248451233, Class Loss=0.16464105248451233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.309974925220013
Loss made of: CE 0.2601095736026764, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.463130474090576 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.486810725927353
Loss made of: CE 0.24904590845108032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.571591854095459 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.422177529335022
Loss made of: CE 0.15669476985931396, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.357945442199707 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.641778653860092
Loss made of: CE 0.2467951476573944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.618684768676758 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.664028671383858
Loss made of: CE 0.16681423783302307, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9442291259765625 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.555639885365963
Loss made of: CE 0.20447087287902832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.641984939575195 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.663090482354164
Loss made of: CE 0.237967848777771, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623409271240234 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.581605693697929
Loss made of: CE 0.2370031476020813, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.562409400939941 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.633728002011776
Loss made of: CE 0.20674704015254974, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.283035755157471 EntMin 0.0
Epoch 5, Class Loss=0.21828728914260864, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.21828728914260864, Class Loss=0.21828728914260864, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.703817270696163
Loss made of: CE 0.29823094606399536, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.600768089294434 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.566096493601799
Loss made of: CE 0.35631775856018066, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.140141010284424 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.563755875825882
Loss made of: CE 0.2932787239551544, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.613847732543945 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.538785666227341
Loss made of: CE 0.2881728708744049, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.890437602996826 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.831898327171802
Loss made of: CE 0.31763404607772827, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6796770095825195 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.716797652840614
Loss made of: CE 0.2694855332374573, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18475341796875 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.65102271437645
Loss made of: CE 0.3194049000740051, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8726511001586914 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.643562832474709
Loss made of: CE 0.3531959056854248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.175809860229492 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.878727272152901
Loss made of: CE 0.43655043840408325, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1791815757751465 EntMin 0.0
Epoch 6, Class Loss=0.3182928264141083, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.3182928264141083, Class Loss=0.3182928264141083, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.329174638539553
Loss made of: CE 0.09030820429325104, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.372095108032227 EntMin 0.0
Epoch 1, Class Loss=0.12659910321235657, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.12659910321235657, Class Loss=0.12659910321235657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=5.762282805144787
Loss made of: CE 0.2107025384902954, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.198694229125977 EntMin 0.0
Epoch 2, Class Loss=0.29645365476608276, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.29645365476608276, Class Loss=0.29645365476608276, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.537381060421467
Loss made of: CE 0.17338643968105316, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.617417335510254 EntMin 0.0
Epoch 3, Class Loss=0.4477170705795288, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.4477170705795288, Class Loss=0.4477170705795288, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.713035610318184
Loss made of: CE 0.26946181058883667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.825434684753418 EntMin 0.0
Epoch 4, Class Loss=0.5741304159164429, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.5741304159164429, Class Loss=0.5741304159164429, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.392886248230934
Loss made of: CE 0.6246280074119568, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.778742790222168 EntMin 0.0
Epoch 5, Class Loss=0.6196495294570923, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.6196495294570923, Class Loss=0.6196495294570923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.543361487984657
Loss made of: CE 0.46648845076560974, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.655539512634277 EntMin 0.0
Epoch 6, Class Loss=0.6844428777694702, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.6844428777694702, Class Loss=0.6844428777694702, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.151987775880843
Loss made of: CE 0.012946339324116707, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8167033195495605 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.290727257076651
Loss made of: CE 0.03653227910399437, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.358855247497559 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.294664786662906
Loss made of: CE 0.010615520179271698, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.471286773681641 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.26001209532842
Loss made of: CE 0.011548253707587719, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.155697822570801 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.344097308721393
Loss made of: CE 0.010857226327061653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.279801368713379 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.289954996109008
Loss made of: CE 0.010303026996552944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.018924236297607 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.383683974854648
Loss made of: CE 0.0160799752920866, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.739941120147705 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.650542359799147
Loss made of: CE 0.014615945518016815, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.461484909057617 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/97, Loss=4.555667026713491
Loss made of: CE 0.024959584698081017, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5239410400390625 EntMin 0.0
Epoch 1, Class Loss=0.02258831076323986, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.02258831076323986, Class Loss=0.02258831076323986, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.396602030470968
Loss made of: CE 0.04940413683652878, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.314301013946533 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.484638383612037
Loss made of: CE 0.07442240417003632, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.180389404296875 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.338492863997817
Loss made of: CE 0.08172938227653503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.517843723297119 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.313776130229234
Loss made of: CE 0.09119906276464462, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.355143070220947 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.495603630505502
Loss made of: CE 0.08330558985471725, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.844229698181152 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.514395127445459
Loss made of: CE 0.056712955236434937, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9360218048095703 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.41344216838479
Loss made of: CE 0.08785693347454071, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0572829246521 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.468009723722934
Loss made of: CE 0.05856912583112717, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.269656658172607 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.5542812321335076
Loss made of: CE 0.051323987543582916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.806052207946777 EntMin 0.0
Epoch 2, Class Loss=0.06140335276722908, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.06140335276722908, Class Loss=0.06140335276722908, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.568477649986744
Loss made of: CE 0.10505396127700806, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.190737724304199 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.582204157859087
Loss made of: CE 0.10004658997058868, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4574127197265625 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.316904055327177
Loss made of: CE 0.11367765069007874, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9550282955169678 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.507444869726896
Loss made of: CE 0.09078150987625122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.71381950378418 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.486998369544745
Loss made of: CE 0.10986419767141342, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.281810283660889 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.426163590699434
Loss made of: CE 0.14372050762176514, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0886945724487305 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.42746639251709
Loss made of: CE 0.11567386239767075, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.132991313934326 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.350630977004767
Loss made of: CE 0.09794537723064423, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.013206958770752 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.383094258606434
Loss made of: CE 0.11000987887382507, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4696502685546875 EntMin 0.0
Epoch 3, Class Loss=0.10960213840007782, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.10960213840007782, Class Loss=0.10960213840007782, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.519177639484406
Loss made of: CE 0.174630805850029, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.401683807373047 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.365078201889991
Loss made of: CE 0.13683098554611206, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9613263607025146 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.420524192601443
Loss made of: CE 0.177176833152771, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.05142879486084 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.383543206751346
Loss made of: CE 0.20201191306114197, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9549336433410645 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.284513580799103
Loss made of: CE 0.16627532243728638, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.204511642456055 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.441192664206028
Loss made of: CE 0.18796306848526, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.819988250732422 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.384428355842829
Loss made of: CE 0.1622634381055832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.550392150878906 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.140734697878361
Loss made of: CE 0.13324537873268127, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.701936721801758 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.413298226892948
Loss made of: CE 0.17036795616149902, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.977792739868164 EntMin 0.0
Epoch 4, Class Loss=0.16144591569900513, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.16144591569900513, Class Loss=0.16144591569900513, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.56977918446064
Loss made of: CE 0.21956495940685272, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.15458345413208 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.444012136757374
Loss made of: CE 0.20420220494270325, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.811126708984375 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.406095857918262
Loss made of: CE 0.26302430033683777, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.430640697479248 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.330154599249363
Loss made of: CE 0.1813010275363922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.406801223754883 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.407447393238544
Loss made of: CE 0.21132436394691467, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8717703819274902 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.552493241429329
Loss made of: CE 0.24953702092170715, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2789812088012695 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.395250822603702
Loss made of: CE 0.18567141890525818, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.475768089294434 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.488045288622379
Loss made of: CE 0.15925166010856628, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.866334915161133 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.395705673098564
Loss made of: CE 0.18653377890586853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.148342609405518 EntMin 0.0
Epoch 5, Class Loss=0.21427014470100403, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.21427014470100403, Class Loss=0.21427014470100403, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.6006544679403305
Loss made of: CE 0.32550495862960815, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.135481834411621 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.382259809970856
Loss made of: CE 0.2591133713722229, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7262609004974365 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.458812189102173
Loss made of: CE 0.3547205626964569, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143339157104492 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.847290050983429
Loss made of: CE 0.3618234097957611, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.908675193786621 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.4516938656568525
Loss made of: CE 0.25903838872909546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611814022064209 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.595921769738197
Loss made of: CE 0.3123239278793335, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726576328277588 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.355934804677963
Loss made of: CE 0.2889128625392914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.098903179168701 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.5344069972634315
Loss made of: CE 0.4236980378627777, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.511769771575928 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.501109538972377
Loss made of: CE 0.27684539556503296, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.974393367767334 EntMin 0.0
Epoch 6, Class Loss=0.3043484091758728, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.3043484091758728, Class Loss=0.3043484091758728, Reg Loss=0.0
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.286108245700598
Loss made of: CE 0.09033612906932831, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.800429344177246 EntMin 0.0
Epoch 1, Class Loss=0.1361517608165741, Reg Loss=0.0
Clinet index 22, End of Epoch 1/6, Average Loss=0.1361517608165741, Class Loss=0.1361517608165741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=5.669324150681495
Loss made of: CE 0.17761707305908203, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.883118152618408 EntMin 0.0
Epoch 2, Class Loss=0.32255819439888, Reg Loss=0.0
Clinet index 22, End of Epoch 2/6, Average Loss=0.32255819439888, Class Loss=0.32255819439888, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.389062131941318
Loss made of: CE 0.6404546499252319, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.193615913391113 EntMin 0.0
Epoch 3, Class Loss=0.4583517014980316, Reg Loss=0.0
Clinet index 22, End of Epoch 3/6, Average Loss=0.4583517014980316, Class Loss=0.4583517014980316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.531567773222923
Loss made of: CE 0.5124189853668213, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.081841945648193 EntMin 0.0
Epoch 4, Class Loss=0.5991659760475159, Reg Loss=0.0
Clinet index 22, End of Epoch 4/6, Average Loss=0.5991659760475159, Class Loss=0.5991659760475159, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.4674531608819965
Loss made of: CE 0.4954237639904022, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.541843414306641 EntMin 0.0
Epoch 5, Class Loss=0.6407416462898254, Reg Loss=0.0
Clinet index 22, End of Epoch 5/6, Average Loss=0.6407416462898254, Class Loss=0.6407416462898254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.330885776877404
Loss made of: CE 0.5044316053390503, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.328075885772705 EntMin 0.0
Epoch 6, Class Loss=0.6745070815086365, Reg Loss=0.0
Clinet index 22, End of Epoch 6/6, Average Loss=0.6745070815086365, Class Loss=0.6745070815086365, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5210701823234558, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.858865
Mean Acc: 0.596317
FreqW Acc: 0.768118
Mean IoU: 0.458052
Class IoU:
	class 0: 0.87970054
	class 1: 0.841811
	class 2: 0.26024708
	class 3: 0.8034915
	class 4: 0.6385751
	class 5: 0.7126668
	class 6: 0.7529267
	class 7: 0.729268
	class 8: 0.7787573
	class 9: 0.113023154
	class 10: 0.0
	class 11: 0.0
	class 12: 0.4005123
	class 13: 0.25796875
	class 14: 0.0
	class 15: 0.6179394
	class 16: 0.0
Class Acc:
	class 0: 0.9507237
	class 1: 0.93479747
	class 2: 0.96787065
	class 3: 0.8466569
	class 4: 0.8538222
	class 5: 0.883555
	class 6: 0.7668821
	class 7: 0.94315785
	class 8: 0.9387023
	class 9: 0.12891433
	class 10: 0.0
	class 11: 0.0
	class 12: 0.46410197
	class 13: 0.6183005
	class 14: 0.0
	class 15: 0.8398981
	class 16: 0.0

federated global round: 24, step: 4
select part of clients to conduct local training
[16, 9, 12, 2]
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=4.496516170352697
Loss made of: CE 0.018735822290182114, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.394696235656738 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.106728438660502
Loss made of: CE 0.017759833484888077, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.926466703414917 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.260917175747454
Loss made of: CE 0.021660491824150085, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.129059314727783 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.258919630479068
Loss made of: CE 0.025629175826907158, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400960922241211 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.2002281080931425
Loss made of: CE 0.023481830954551697, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.839686393737793 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.447370027378201
Loss made of: CE 0.032452039420604706, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.700342655181885 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.499168355204165
Loss made of: CE 0.01568954437971115, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.891620635986328 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.393953741528094
Loss made of: CE 0.04214519262313843, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9872729778289795 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.249595376290381
Loss made of: CE 0.02867836132645607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.728593826293945 EntMin 0.0
Epoch 1, Class Loss=0.025797823444008827, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.025797823444008827, Class Loss=0.025797823444008827, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/97, Loss=4.494463676959276
Loss made of: CE 0.05423203110694885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.692437171936035 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.39517761208117
Loss made of: CE 0.04459630697965622, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.443729400634766 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.476781587302685
Loss made of: CE 0.055376194417476654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.888978004455566 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.4090703933499755
Loss made of: CE 0.062485724687576294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.13270902633667 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.265349542349577
Loss made of: CE 0.046022962778806686, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.315999507904053 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.272796554118395
Loss made of: CE 0.08465748280286789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.669999599456787 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.1827551033347845
Loss made of: CE 0.08546307682991028, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.107359886169434 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.243111828155816
Loss made of: CE 0.02665150724351406, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0596604347229 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.38443402312696
Loss made of: CE 0.06109428405761719, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.424831390380859 EntMin 0.0
Epoch 2, Class Loss=0.06326287984848022, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.06326287984848022, Class Loss=0.06326287984848022, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/97, Loss=4.1273791015148165
Loss made of: CE 0.1330317258834839, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9193177223205566 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.249005788564682
Loss made of: CE 0.10008728504180908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2184882164001465 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.544913569837808
Loss made of: CE 0.11632110923528671, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7071123123168945 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.5729881070554255
Loss made of: CE 0.10047124326229095, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.070919990539551 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.279105817526579
Loss made of: CE 0.08982694149017334, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.221732139587402 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.349452213943005
Loss made of: CE 0.11207161843776703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3089494705200195 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.460942508280278
Loss made of: CE 0.13920001685619354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143226146697998 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.440815611183643
Loss made of: CE 0.08402623981237411, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.597329616546631 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.190695890039206
Loss made of: CE 0.07632482051849365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.336498260498047 EntMin 0.0
Epoch 3, Class Loss=0.11631904542446136, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.11631904542446136, Class Loss=0.11631904542446136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/97, Loss=4.230126668512821
Loss made of: CE 0.19972273707389832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3110575675964355 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.214389321208
Loss made of: CE 0.11974292993545532, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9168708324432373 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.232418768107891
Loss made of: CE 0.13992954790592194, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.378174781799316 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.280943097174168
Loss made of: CE 0.20105509459972382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.768123149871826 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.320158919692039
Loss made of: CE 0.13063792884349823, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7944517135620117 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.210134309530258
Loss made of: CE 0.1443301886320114, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2276611328125 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.304744869470596
Loss made of: CE 0.13861750066280365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.293931484222412 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.322047382593155
Loss made of: CE 0.19068056344985962, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.122341156005859 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.192692391574383
Loss made of: CE 0.12816239893436432, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.173585891723633 EntMin 0.0
Epoch 4, Class Loss=0.16946282982826233, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.16946282982826233, Class Loss=0.16946282982826233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/97, Loss=4.318319880962372
Loss made of: CE 0.2969522178173065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.59529447555542 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.5017250895500185
Loss made of: CE 0.26037079095840454, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.286571502685547 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.244264084100723
Loss made of: CE 0.2635655999183655, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195883750915527 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.414051404595375
Loss made of: CE 0.265598863363266, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.923609495162964 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.299084450304508
Loss made of: CE 0.29994410276412964, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9497392177581787 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.441940411925316
Loss made of: CE 0.2684668004512787, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.305647850036621 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.313508336246014
Loss made of: CE 0.15594279766082764, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.842071294784546 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.225344970822334
Loss made of: CE 0.18194720149040222, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.79099440574646 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.4681016385555266
Loss made of: CE 0.31778544187545776, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.598691940307617 EntMin 0.0
Epoch 5, Class Loss=0.23258696496486664, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.23258696496486664, Class Loss=0.23258696496486664, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/97, Loss=4.409206113219261
Loss made of: CE 0.26761478185653687, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.935051441192627 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.358675098419189
Loss made of: CE 0.36857888102531433, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.612792491912842 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.173040369153023
Loss made of: CE 0.25087419152259827, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4543628692626953 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.2231051415205005
Loss made of: CE 0.4513414800167084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.324434280395508 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.115691344439983
Loss made of: CE 0.1914496123790741, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4698033332824707 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.2686695665121075
Loss made of: CE 0.25031301379203796, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.964509963989258 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.210213170945645
Loss made of: CE 0.32967904210090637, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.909902572631836 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.25185759961605
Loss made of: CE 0.2646690607070923, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.704387664794922 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.5159254431724545
Loss made of: CE 0.32836586236953735, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.134186267852783 EntMin 0.0
Epoch 6, Class Loss=0.30787426233291626, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.30787426233291626, Class Loss=0.30787426233291626, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.246941076219082
Loss made of: CE 0.40801703929901123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.345679759979248 EntMin 0.0
Epoch 1, Class Loss=0.11846242845058441, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.11846242845058441, Class Loss=0.11846242845058441, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.15949766933918
Loss made of: CE 0.305420458316803, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8017072677612305 EntMin 0.0
Epoch 2, Class Loss=0.26233217120170593, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.26233217120170593, Class Loss=0.26233217120170593, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.137633436918259
Loss made of: CE 0.24389126896858215, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.01131534576416 EntMin 0.0
Epoch 3, Class Loss=0.3709414005279541, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.3709414005279541, Class Loss=0.3709414005279541, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=5.300457727909088
Loss made of: CE 0.35210394859313965, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9431538581848145 EntMin 0.0
Epoch 4, Class Loss=0.4852200448513031, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.4852200448513031, Class Loss=0.4852200448513031, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=5.193038982152939
Loss made of: CE 0.4165956377983093, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.586115837097168 EntMin 0.0
Epoch 5, Class Loss=0.5591474771499634, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5591474771499634, Class Loss=0.5591474771499634, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=5.247195851802826
Loss made of: CE 0.4575884938240051, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.033834934234619 EntMin 0.0
Epoch 6, Class Loss=0.6411467790603638, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.6411467790603638, Class Loss=0.6411467790603638, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.158151310496033
Loss made of: CE 0.04899848252534866, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.952299118041992 EntMin 0.0
Epoch 1, Class Loss=0.08014067262411118, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.08014067262411118, Class Loss=0.08014067262411118, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=5.14158036261797
Loss made of: CE 0.2040189802646637, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.779252052307129 EntMin 0.0
Epoch 2, Class Loss=0.23751698434352875, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.23751698434352875, Class Loss=0.23751698434352875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.135189345479011
Loss made of: CE 0.3781853914260864, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0065412521362305 EntMin 0.0
Epoch 3, Class Loss=0.36468154191970825, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.36468154191970825, Class Loss=0.36468154191970825, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.988405475020409
Loss made of: CE 0.5124134421348572, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.575565338134766 EntMin 0.0
Epoch 4, Class Loss=0.46713051199913025, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.46713051199913025, Class Loss=0.46713051199913025, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=5.145641797780991
Loss made of: CE 0.5553684830665588, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.73628044128418 EntMin 0.0
Epoch 5, Class Loss=0.5277338027954102, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5277338027954102, Class Loss=0.5277338027954102, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=5.116085201501846
Loss made of: CE 0.5499897003173828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.682554721832275 EntMin 0.0
Epoch 6, Class Loss=0.667083203792572, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.667083203792572, Class Loss=0.667083203792572, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.005203151702881
Loss made of: CE 1.0326430797576904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758280277252197 EntMin 0.0
Epoch 1, Class Loss=0.8975870609283447, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.8975870609283447, Class Loss=0.8975870609283447, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/12, Loss=5.5841619074344635
Loss made of: CE 0.5260599851608276, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.348492622375488 EntMin 0.0
Epoch 2, Class Loss=0.7987631559371948, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.7987631559371948, Class Loss=0.7987631559371948, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/12, Loss=5.472450882196426
Loss made of: CE 0.6635679602622986, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.955389499664307 EntMin 0.0
Epoch 3, Class Loss=0.7448320388793945, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.7448320388793945, Class Loss=0.7448320388793945, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/12, Loss=5.3919735789299015
Loss made of: CE 0.7300459742546082, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.675928115844727 EntMin 0.0
Epoch 4, Class Loss=0.6987791061401367, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.6987791061401367, Class Loss=0.6987791061401367, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/12, Loss=5.238489407300949
Loss made of: CE 0.7529788017272949, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.748926162719727 EntMin 0.0
Epoch 5, Class Loss=0.6745266318321228, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.6745266318321228, Class Loss=0.6745266318321228, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/12, Loss=5.306192943453789
Loss made of: CE 0.7400276064872742, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.310643196105957 EntMin 0.0
Epoch 6, Class Loss=0.6734030246734619, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.6734030246734619, Class Loss=0.6734030246734619, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5265950560569763, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.857575
Mean Acc: 0.600793
FreqW Acc: 0.768067
Mean IoU: 0.458100
Class IoU:
	class 0: 0.87978095
	class 1: 0.83980423
	class 2: 0.27869487
	class 3: 0.8034673
	class 4: 0.62110144
	class 5: 0.69758755
	class 6: 0.76594394
	class 7: 0.72323817
	class 8: 0.7834593
	class 9: 0.1221208
	class 10: 0.0
	class 11: 0.0
	class 12: 0.41460302
	class 13: 0.24803765
	class 14: 0.0
	class 15: 0.60985583
	class 16: 0.0
Class Acc:
	class 0: 0.94502777
	class 1: 0.9297758
	class 2: 0.961127
	class 3: 0.8538062
	class 4: 0.8698191
	class 5: 0.8701467
	class 6: 0.7847889
	class 7: 0.94009465
	class 8: 0.93990606
	class 9: 0.14002822
	class 10: 0.0
	class 11: 0.0
	class 12: 0.4778456
	class 13: 0.64009976
	class 14: 0.0
	class 15: 0.8610181
	class 16: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 25, step: 5
select part of clients to conduct local training
[24, 7, 4, 12]
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.958222249895334
Loss made of: CE 0.12298720329999924, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.809698104858398 EntMin 0.0
Epoch 1, Class Loss=0.20312291383743286, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.20312291383743286, Class Loss=0.20312291383743286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=6.025051304697991
Loss made of: CE 0.30588671565055847, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5921950340271 EntMin 0.0
Epoch 2, Class Loss=0.352975457906723, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.352975457906723, Class Loss=0.352975457906723, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.869764369726181
Loss made of: CE 0.4592680335044861, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.282167911529541 EntMin 0.0
Epoch 3, Class Loss=0.4844823479652405, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.4844823479652405, Class Loss=0.4844823479652405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.760328105092048
Loss made of: CE 0.6633208990097046, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.040349960327148 EntMin 0.0
Epoch 4, Class Loss=0.5699447989463806, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.5699447989463806, Class Loss=0.5699447989463806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.4907735049724575
Loss made of: CE 0.569583535194397, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.525440216064453 EntMin 0.0
Epoch 5, Class Loss=0.6116664409637451, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.6116664409637451, Class Loss=0.6116664409637451, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.450159502029419
Loss made of: CE 0.5715124011039734, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.691823482513428 EntMin 0.0
Epoch 6, Class Loss=0.660764217376709, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.660764217376709, Class Loss=0.660764217376709, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4020773470401764, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.4020773470401764, Class Loss=0.4020773470401764, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.5903424024581909, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.5903424024581909, Class Loss=0.5903424024581909, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6743168234825134, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.6743168234825134, Class Loss=0.6743168234825134, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.7029842734336853, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.7029842734336853, Class Loss=0.7029842734336853, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.706363320350647, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.706363320350647, Class Loss=0.706363320350647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6959803104400635, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.6959803104400635, Class Loss=0.6959803104400635, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.811329130828381
Loss made of: CE 0.17229518294334412, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.7217116355896 EntMin 0.0
Epoch 1, Class Loss=0.2180253118276596, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.2180253118276596, Class Loss=0.2180253118276596, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=5.941420957446098
Loss made of: CE 0.3121597170829773, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.781602382659912 EntMin 0.0
Epoch 2, Class Loss=0.36974722146987915, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.36974722146987915, Class Loss=0.36974722146987915, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.838489827513695
Loss made of: CE 0.4677458703517914, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.918891906738281 EntMin 0.0
Epoch 3, Class Loss=0.4888279139995575, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.4888279139995575, Class Loss=0.4888279139995575, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.728641587495804
Loss made of: CE 0.5981258153915405, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.448071479797363 EntMin 0.0
Epoch 4, Class Loss=0.558964729309082, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.558964729309082, Class Loss=0.558964729309082, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.508225730061531
Loss made of: CE 0.734329104423523, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5904459953308105 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 5, Class Loss=0.6324471235275269, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.6324471235275269, Class Loss=0.6324471235275269, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.392535310983658
Loss made of: CE 0.650071382522583, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.25076961517334 EntMin 0.0
Epoch 6, Class Loss=0.6546634435653687, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6546634435653687, Class Loss=0.6546634435653687, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.6807839050889015
Loss made of: CE 0.2366424798965454, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.758893966674805 EntMin 0.0
Epoch 1, Class Loss=0.21868960559368134, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.21868960559368134, Class Loss=0.21868960559368134, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=6.066566777229309
Loss made of: CE 0.4383111000061035, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.270750999450684 EntMin 0.0
Epoch 2, Class Loss=0.38183194398880005, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.38183194398880005, Class Loss=0.38183194398880005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.751339295506478
Loss made of: CE 0.5858039855957031, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3172149658203125 EntMin 0.0
Epoch 3, Class Loss=0.49297627806663513, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.49297627806663513, Class Loss=0.49297627806663513, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.813106834888458
Loss made of: CE 0.5181396007537842, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.910152435302734 EntMin 0.0
Epoch 4, Class Loss=0.5973321795463562, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.5973321795463562, Class Loss=0.5973321795463562, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 5, Batch 10/12, Loss=5.482267260551453
Loss made of: CE 0.5860840082168579, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6654510498046875 EntMin 0.0
Epoch 5, Class Loss=0.6260048151016235, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.6260048151016235, Class Loss=0.6260048151016235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.5637765526771545
Loss made of: CE 0.6574426889419556, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.670295238494873 EntMin 0.0
Epoch 6, Class Loss=0.6923372745513916, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.6923372745513916, Class Loss=0.6923372745513916, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7774629592895508, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.835766
Mean Acc: 0.524719
FreqW Acc: 0.730612
Mean IoU: 0.389706
Class IoU:
	class 0: 0.8541258
	class 1: 0.8041952
	class 2: 0.25674197
	class 3: 0.72787946
	class 4: 0.5860288
	class 5: 0.6709884
	class 6: 0.6367935
	class 7: 0.693305
	class 8: 0.7721301
	class 9: 0.117896184
	class 10: 0.0
	class 11: 0.0
	class 12: 0.38810107
	class 13: 0.23421091
	class 14: 0.0
	class 15: 0.6620238
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
Class Acc:
	class 0: 0.94939953
	class 1: 0.9309827
	class 2: 0.96674466
	class 3: 0.75755006
	class 4: 0.8976371
	class 5: 0.8417633
	class 6: 0.6562658
	class 7: 0.9327073
	class 8: 0.88202256
	class 9: 0.14512807
	class 10: 0.0
	class 11: 0.0
	class 12: 0.48940283
	class 13: 0.65970486
	class 14: 0.0
	class 15: 0.8603478
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0

federated global round: 26, step: 5
select part of clients to conduct local training
[6, 28, 20, 4]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.057101812213659
Loss made of: CE 0.15283408761024475, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.404642581939697 EntMin 0.0
Epoch 1, Class Loss=0.1368260681629181, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.1368260681629181, Class Loss=0.1368260681629181, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=4.975534699857235
Loss made of: CE 0.28040486574172974, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.530274868011475 EntMin 0.0
Epoch 2, Class Loss=0.259166419506073, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.259166419506073, Class Loss=0.259166419506073, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.048976323008537
Loss made of: CE 0.317657470703125, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.05510139465332 EntMin 0.0
Epoch 3, Class Loss=0.36217960715293884, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.36217960715293884, Class Loss=0.36217960715293884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=4.916790917515755
Loss made of: CE 0.4852849841117859, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.610380172729492 EntMin 0.0
Epoch 4, Class Loss=0.4620458483695984, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.4620458483695984, Class Loss=0.4620458483695984, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.051289731264115
Loss made of: CE 0.6126108169555664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.768527030944824 EntMin 0.0
Epoch 5, Class Loss=0.5431943535804749, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.5431943535804749, Class Loss=0.5431943535804749, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 6, Batch 10/12, Loss=4.956249177455902
Loss made of: CE 0.5993019342422485, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9808614253997803 EntMin 0.0
Epoch 6, Class Loss=0.5994446277618408, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5994446277618408, Class Loss=0.5994446277618408, Reg Loss=0.0
Current Client Index:  28
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.28523585200309753, Reg Loss=0.0
Clinet index 28, End of Epoch 1/6, Average Loss=0.28523585200309753, Class Loss=0.28523585200309753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.47669923305511475, Reg Loss=0.0
Clinet index 28, End of Epoch 2/6, Average Loss=0.47669923305511475, Class Loss=0.47669923305511475, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.6189063787460327, Reg Loss=0.0
Clinet index 28, End of Epoch 3/6, Average Loss=0.6189063787460327, Class Loss=0.6189063787460327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.752636730670929, Reg Loss=0.0
Clinet index 28, End of Epoch 4/6, Average Loss=0.752636730670929, Class Loss=0.752636730670929, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.8020946383476257, Reg Loss=0.0
Clinet index 28, End of Epoch 5/6, Average Loss=0.8020946383476257, Class Loss=0.8020946383476257, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7704318165779114, Reg Loss=0.0
Clinet index 28, End of Epoch 6/6, Average Loss=0.7704318165779114, Class Loss=0.7704318165779114, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.2820279002189636, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.2820279002189636, Class Loss=0.2820279002189636, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.4436991512775421, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.4436991512775421, Class Loss=0.4436991512775421, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.5952039957046509, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.5952039957046509, Class Loss=0.5952039957046509, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.6842569708824158, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.6842569708824158, Class Loss=0.6842569708824158, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7256432771682739, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.7256432771682739, Class Loss=0.7256432771682739, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.7824200391769409, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.7824200391769409, Class Loss=0.7824200391769409, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.652590453624725
Loss made of: CE 0.8537005186080933, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.168622970581055 EntMin 0.0
Epoch 1, Class Loss=0.8816121816635132, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.8816121816635132, Class Loss=0.8816121816635132, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=5.451723408699036
Loss made of: CE 0.7969080209732056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.905431747436523 EntMin 0.0
Epoch 2, Class Loss=0.8325011134147644, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.8325011134147644, Class Loss=0.8325011134147644, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=5.344271796941757
Loss made of: CE 0.7546893954277039, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.25288200378418 EntMin 0.0
Epoch 3, Class Loss=0.7658910155296326, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.7658910155296326, Class Loss=0.7658910155296326, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=5.379344975948333
Loss made of: CE 0.738776683807373, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.963663101196289 EntMin 0.0
Epoch 4, Class Loss=0.7321399450302124, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.7321399450302124, Class Loss=0.7321399450302124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.1422719120979306
Loss made of: CE 0.7199036478996277, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.017218589782715 EntMin 0.0
Epoch 5, Class Loss=0.7028304934501648, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.7028304934501648, Class Loss=0.7028304934501648, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=5.074881392717361
Loss made of: CE 0.6923180818557739, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.388317584991455 EntMin 0.0
Epoch 6, Class Loss=0.6813237071037292, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6813237071037292, Class Loss=0.6813237071037292, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6976889371871948, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.831290
Mean Acc: 0.490640
FreqW Acc: 0.718621
Mean IoU: 0.375054
Class IoU:
	class 0: 0.844886
	class 1: 0.8029541
	class 2: 0.2918878
	class 3: 0.62543195
	class 4: 0.5871071
	class 5: 0.6556556
	class 6: 0.4820337
	class 7: 0.7091695
	class 8: 0.7436394
	class 9: 0.10906088
	class 10: 0.0
	class 11: 0.0
	class 12: 0.37407407
	class 13: 0.23521294
	class 14: 0.0
	class 15: 0.6642773
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0006286322
Class Acc:
	class 0: 0.9602903
	class 1: 0.8660654
	class 2: 0.9425645
	class 3: 0.6390558
	class 4: 0.86080325
	class 5: 0.7742475
	class 6: 0.48953906
	class 7: 0.9234408
	class 8: 0.8072743
	class 9: 0.13439445
	class 10: 0.0
	class 11: 0.0
	class 12: 0.45305204
	class 13: 0.6276716
	class 14: 0.0
	class 15: 0.8431286
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0006286522

federated global round: 27, step: 5
select part of clients to conduct local training
[24, 8, 26, 0]
Current Client Index:  24
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 1, Batch 10/12, Loss=6.421503734588623
Loss made of: CE 0.8091286420822144, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.481175899505615 EntMin 0.0
Epoch 1, Class Loss=0.8590409755706787, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.8590409755706787, Class Loss=0.8590409755706787, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/12, Loss=5.590701425075531
Loss made of: CE 0.8094786405563354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.656064033508301 EntMin 0.0
Epoch 2, Class Loss=0.8082172274589539, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.8082172274589539, Class Loss=0.8082172274589539, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/12, Loss=5.453364670276642
Loss made of: CE 0.7222329378128052, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3703932762146 EntMin 0.0
Epoch 3, Class Loss=0.7662184238433838, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.7662184238433838, Class Loss=0.7662184238433838, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/12, Loss=5.38433518409729
Loss made of: CE 0.7080820798873901, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.381978988647461 EntMin 0.0
Epoch 4, Class Loss=0.7325322031974792, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.7325322031974792, Class Loss=0.7325322031974792, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/12, Loss=5.154341131448746
Loss made of: CE 0.7280270457267761, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.145310401916504 EntMin 0.0
Epoch 5, Class Loss=0.6820752620697021, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.6820752620697021, Class Loss=0.6820752620697021, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/12, Loss=4.999052441120147
Loss made of: CE 0.6029008030891418, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.226616859436035 EntMin 0.0
Epoch 6, Class Loss=0.6827091574668884, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.6827091574668884, Class Loss=0.6827091574668884, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.2421514242887497, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.2421514242887497, Class Loss=0.2421514242887497, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.39636602997779846, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.39636602997779846, Class Loss=0.39636602997779846, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.5648139119148254, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.5648139119148254, Class Loss=0.5648139119148254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.6339424848556519, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.6339424848556519, Class Loss=0.6339424848556519, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Class Loss=0.6818439364433289, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.6818439364433289, Class Loss=0.6818439364433289, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.7145125269889832, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.7145125269889832, Class Loss=0.7145125269889832, Reg Loss=0.0
Current Client Index:  26
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 1, Batch 10/12, Loss=5.510894300788641
Loss made of: CE 0.1736365556716919, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.727657794952393 EntMin 0.0
Epoch 1, Class Loss=0.15110978484153748, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.15110978484153748, Class Loss=0.15110978484153748, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=5.016616652905941
Loss made of: CE 0.32637912034988403, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.544610023498535 EntMin 0.0
Epoch 2, Class Loss=0.26106196641921997, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.26106196641921997, Class Loss=0.26106196641921997, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=5.034004637598992
Loss made of: CE 0.39812156558036804, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8602705001831055 EntMin 0.0
Epoch 3, Class Loss=0.3641209602355957, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.3641209602355957, Class Loss=0.3641209602355957, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=5.057656142115593
Loss made of: CE 0.3606782853603363, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.603668689727783 EntMin 0.0
Epoch 4, Class Loss=0.4393119812011719, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.4393119812011719, Class Loss=0.4393119812011719, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.792783436179161
Loss made of: CE 0.5178539156913757, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.838662147521973 EntMin 0.0
Epoch 5, Class Loss=0.49331387877464294, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.49331387877464294, Class Loss=0.49331387877464294, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.915392100811005
Loss made of: CE 0.5511884689331055, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.728066444396973 EntMin 0.0
Epoch 6, Class Loss=0.5480676889419556, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.5480676889419556, Class Loss=0.5480676889419556, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.2262610666453835
Loss made of: CE 0.1361292004585266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.460988998413086 EntMin 0.0
Epoch 1, Class Loss=0.13543710112571716, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.13543710112571716, Class Loss=0.13543710112571716, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=4.865635550022125
Loss made of: CE 0.20425307750701904, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.979184150695801 EntMin 0.0
Epoch 2, Class Loss=0.2548092007637024, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.2548092007637024, Class Loss=0.2548092007637024, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=4.924826917052269
Loss made of: CE 0.4890499711036682, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3648200035095215 EntMin 0.0
Epoch 3, Class Loss=0.34868454933166504, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.34868454933166504, Class Loss=0.34868454933166504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=4.9034519135952
Loss made of: CE 0.3702196478843689, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.469513893127441 EntMin 0.0
Epoch 4, Class Loss=0.4224422872066498, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.4224422872066498, Class Loss=0.4224422872066498, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.8798174381256105
Loss made of: CE 0.5027268528938293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.507633686065674 EntMin 0.0
Epoch 5, Class Loss=0.48303669691085815, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.48303669691085815, Class Loss=0.48303669691085815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.791709178686142
Loss made of: CE 0.5328110456466675, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.898982048034668 EntMin 0.0
Epoch 6, Class Loss=0.5367165803909302, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.5367165803909302, Class Loss=0.5367165803909302, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7134305238723755, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.836097
Mean Acc: 0.526824
FreqW Acc: 0.734538
Mean IoU: 0.392402
Class IoU:
	class 0: 0.8586044
	class 1: 0.8191314
	class 2: 0.2735481
	class 3: 0.7285258
	class 4: 0.5774825
	class 5: 0.6553169
	class 6: 0.47497836
	class 7: 0.6910899
	class 8: 0.7749203
	class 9: 0.09198378
	class 10: 0.0
	class 11: 0.0
	class 12: 0.41542032
	class 13: 0.2330599
	class 14: 0.0
	class 15: 0.6775291
	class 16: 0.0
	class 17: 0.0
	class 18: 0.1840541
Class Acc:
	class 0: 0.94976336
	class 1: 0.8988004
	class 2: 0.9568503
	class 3: 0.7615803
	class 4: 0.87875843
	class 5: 0.82594496
	class 6: 0.48348242
	class 7: 0.93104565
	class 8: 0.9098607
	class 9: 0.10760781
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5396595
	class 13: 0.6368268
	class 14: 0.0
	class 15: 0.83772486
	class 16: 0.0
	class 17: 0.0
	class 18: 0.2917439

federated global round: 28, step: 5
select part of clients to conduct local training
[8, 9, 16, 3]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.1976380348205566, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.1976380348205566, Class Loss=1.1976380348205566, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Epoch 2, Class Loss=1.0953315496444702, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=1.0953315496444702, Class Loss=1.0953315496444702, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.9704405665397644, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.9704405665397644, Class Loss=0.9704405665397644, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.8657910227775574, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.8657910227775574, Class Loss=0.8657910227775574, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.7865222692489624, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.7865222692489624, Class Loss=0.7865222692489624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.7157142758369446, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.7157142758369446, Class Loss=0.7157142758369446, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.2641119956970215, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.2641119956970215, Class Loss=0.2641119956970215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.47117647528648376, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.47117647528648376, Class Loss=0.47117647528648376, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.579484224319458, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.579484224319458, Class Loss=0.579484224319458, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.6994876265525818, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.6994876265525818, Class Loss=0.6994876265525818, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.707304060459137, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.707304060459137, Class Loss=0.707304060459137, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.7218781113624573, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.7218781113624573, Class Loss=0.7218781113624573, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=4.53192736171186
Loss made of: CE 0.09126801788806915, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.713467121124268 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.0797884613275528, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.0797884613275528, Class Loss=0.0797884613275528, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=4.33833429813385
Loss made of: CE 0.23216550052165985, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.395766258239746 EntMin 0.0
Epoch 2, Class Loss=0.18020550906658173, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.18020550906658173, Class Loss=0.18020550906658173, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.385316807031631
Loss made of: CE 0.24764308333396912, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8984975814819336 EntMin 0.0
Epoch 3, Class Loss=0.2667354941368103, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.2667354941368103, Class Loss=0.2667354941368103, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.391880968213082
Loss made of: CE 0.33953356742858887, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7460927963256836 EntMin 0.0
Epoch 4, Class Loss=0.3666338622570038, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.3666338622570038, Class Loss=0.3666338622570038, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.435839396715164
Loss made of: CE 0.4296163022518158, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7568111419677734 EntMin 0.0
Epoch 5, Class Loss=0.44096893072128296, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.44096893072128296, Class Loss=0.44096893072128296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.676790332794189
Loss made of: CE 0.5009271502494812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.445655345916748 EntMin 0.0
Epoch 6, Class Loss=0.5242469310760498, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.5242469310760498, Class Loss=0.5242469310760498, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=4.582283928990364
Loss made of: CE 0.08135345578193665, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5120530128479 EntMin 0.0
Epoch 1, Class Loss=0.10671897977590561, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.10671897977590561, Class Loss=0.10671897977590561, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=4.630700270831585
Loss made of: CE 0.21727555990219116, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.784389495849609 EntMin 0.0
Epoch 2, Class Loss=0.19025033712387085, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.19025033712387085, Class Loss=0.19025033712387085, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.6479514092206955
Loss made of: CE 0.35202306509017944, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5944366455078125 EntMin 0.0
Epoch 3, Class Loss=0.29428282380104065, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.29428282380104065, Class Loss=0.29428282380104065, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.804134035110474
Loss made of: CE 0.3767473101615906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.088032245635986 EntMin 0.0
Epoch 4, Class Loss=0.3932342529296875, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.3932342529296875, Class Loss=0.3932342529296875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.751312565803528
Loss made of: CE 0.46722039580345154, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.229144096374512 EntMin 0.0
Epoch 5, Class Loss=0.4749668836593628, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.4749668836593628, Class Loss=0.4749668836593628, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.708520004153252
Loss made of: CE 0.3947318494319916, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9221513271331787 EntMin 0.0
Epoch 6, Class Loss=0.5267592668533325, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.5267592668533325, Class Loss=0.5267592668533325, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6696251630783081, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.834637
Mean Acc: 0.507850
FreqW Acc: 0.727716
Mean IoU: 0.384020
Class IoU:
	class 0: 0.8533931
	class 1: 0.79810053
	class 2: 0.2777397
	class 3: 0.69781065
	class 4: 0.5841157
	class 5: 0.65195537
	class 6: 0.41417506
	class 7: 0.6951088
	class 8: 0.7682624
	class 9: 0.09345638
	class 10: 0.0
	class 11: 0.0
	class 12: 0.39917907
	class 13: 0.23876293
	class 14: 0.0
	class 15: 0.6736302
	class 16: 0.0
	class 17: 0.0
	class 18: 0.15069279
Class Acc:
	class 0: 0.95802355
	class 1: 0.8614707
	class 2: 0.95298284
	class 3: 0.72065985
	class 4: 0.855624
	class 5: 0.7955973
	class 6: 0.41877347
	class 7: 0.9319329
	class 8: 0.8820126
	class 9: 0.11270412
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5082926
	class 13: 0.6399016
	class 14: 0.0
	class 15: 0.82044184
	class 16: 0.0
	class 17: 0.0
	class 18: 0.19073631

federated global round: 29, step: 5
select part of clients to conduct local training
[29, 27, 26, 18]
Current Client Index:  29
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.911864985898137
Loss made of: CE 0.17661218345165253, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.086937427520752 EntMin 0.0
Epoch 1, Class Loss=0.1168995201587677, Reg Loss=0.0
Clinet index 29, End of Epoch 1/6, Average Loss=0.1168995201587677, Class Loss=0.1168995201587677, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=4.826126791536808
Loss made of: CE 0.20240232348442078, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9322097301483154 EntMin 0.0
Epoch 2, Class Loss=0.2015012800693512, Reg Loss=0.0
Clinet index 29, End of Epoch 2/6, Average Loss=0.2015012800693512, Class Loss=0.2015012800693512, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.447119471430779
Loss made of: CE 0.3011421859264374, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9608888626098633 EntMin 0.0
Epoch 3, Class Loss=0.3036652207374573, Reg Loss=0.0
Clinet index 29, End of Epoch 3/6, Average Loss=0.3036652207374573, Class Loss=0.3036652207374573, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.753741207718849
Loss made of: CE 0.40244540572166443, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.341806411743164 EntMin 0.0
Epoch 4, Class Loss=0.38933882117271423, Reg Loss=0.0
Clinet index 29, End of Epoch 4/6, Average Loss=0.38933882117271423, Class Loss=0.38933882117271423, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.638404339551926
Loss made of: CE 0.4960351586341858, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.946737051010132 EntMin 0.0
Epoch 5, Class Loss=0.46226227283477783, Reg Loss=0.0
Clinet index 29, End of Epoch 5/6, Average Loss=0.46226227283477783, Class Loss=0.46226227283477783, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.600870817899704
Loss made of: CE 0.5768862962722778, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.829660177230835 EntMin 0.0
Epoch 6, Class Loss=0.5366696119308472, Reg Loss=0.0
Clinet index 29, End of Epoch 6/6, Average Loss=0.5366696119308472, Class Loss=0.5366696119308472, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.209432452917099, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.209432452917099, Class Loss=0.209432452917099, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.39713427424430847, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.39713427424430847, Class Loss=0.39713427424430847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.5136145353317261, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.5136145353317261, Class Loss=0.5136145353317261, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.6213423609733582, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.6213423609733582, Class Loss=0.6213423609733582, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.7167556285858154, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.7167556285858154, Class Loss=0.7167556285858154, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.7911211252212524, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.7911211252212524, Class Loss=0.7911211252212524, Reg Loss=0.0
Current Client Index:  26
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.629756999015808
Loss made of: CE 0.720496654510498, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195913791656494 EntMin 0.0
Epoch 1, Class Loss=0.7695948481559753, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.7695948481559753, Class Loss=0.7695948481559753, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/12, Loss=5.1199972331523895
Loss made of: CE 0.7385754585266113, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.378320217132568 EntMin 0.0
Epoch 2, Class Loss=0.7210087776184082, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.7210087776184082, Class Loss=0.7210087776184082, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/12, Loss=5.015845507383347
Loss made of: CE 0.672102689743042, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.447973251342773 EntMin 0.0
Epoch 3, Class Loss=0.6863300204277039, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.6863300204277039, Class Loss=0.6863300204277039, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/12, Loss=4.905312901735305
Loss made of: CE 0.6084272861480713, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2037458419799805 EntMin 0.0
Epoch 4, Class Loss=0.6758195757865906, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.6758195757865906, Class Loss=0.6758195757865906, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/12, Loss=4.644171619415284
Loss made of: CE 0.6413488388061523, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.416093826293945 EntMin 0.0
Epoch 5, Class Loss=0.6557042002677917, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.6557042002677917, Class Loss=0.6557042002677917, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/12, Loss=4.767844051122665
Loss made of: CE 0.6269693374633789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.123678207397461 EntMin 0.0
Epoch 6, Class Loss=0.65082186460495, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.65082186460495, Class Loss=0.65082186460495, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.076750849187374
Loss made of: CE 0.12663929164409637, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.577315330505371 EntMin 0.0
Epoch 1, Class Loss=0.12118306010961533, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.12118306010961533, Class Loss=0.12118306010961533, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=4.680756519734859
Loss made of: CE 0.19304227828979492, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.635221481323242 EntMin 0.0
Epoch 2, Class Loss=0.20837953686714172, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.20837953686714172, Class Loss=0.20837953686714172, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.580721926689148
Loss made of: CE 0.2873935401439667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.217655181884766 EntMin 0.0
Epoch 3, Class Loss=0.3127294182777405, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.3127294182777405, Class Loss=0.3127294182777405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.76445497572422
Loss made of: CE 0.32943278551101685, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.116813659667969 EntMin 0.0
Epoch 4, Class Loss=0.3778255581855774, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.3778255581855774, Class Loss=0.3778255581855774, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.64943074285984
Loss made of: CE 0.4268612861633301, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7520856857299805 EntMin 0.0
Epoch 5, Class Loss=0.4656420648097992, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.4656420648097992, Class Loss=0.4656420648097992, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.667015227675438
Loss made of: CE 0.5393074750900269, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.478060245513916 EntMin 0.0
Epoch 6, Class Loss=0.5422542095184326, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.5422542095184326, Class Loss=0.5422542095184326, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.693976640701294, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.834157
Mean Acc: 0.537902
FreqW Acc: 0.737301
Mean IoU: 0.393611
Class IoU:
	class 0: 0.8610945
	class 1: 0.8074897
	class 2: 0.26026025
	class 3: 0.75292027
	class 4: 0.57747966
	class 5: 0.64138687
	class 6: 0.4844077
	class 7: 0.6801191
	class 8: 0.7772725
	class 9: 0.09660403
	class 10: 0.0
	class 11: 0.0
	class 12: 0.44302705
	class 13: 0.23832132
	class 14: 0.0
	class 15: 0.6809632
	class 16: 0.0
	class 17: 0.0
	class 18: 0.17725635
Class Acc:
	class 0: 0.9423069
	class 1: 0.91834855
	class 2: 0.9647321
	class 3: 0.8000248
	class 4: 0.86935025
	class 5: 0.83310205
	class 6: 0.49428284
	class 7: 0.93110096
	class 8: 0.92113996
	class 9: 0.11332452
	class 10: 0.0
	class 11: 0.0
	class 12: 0.57859486
	class 13: 0.6417291
	class 14: 0.0
	class 15: 0.8262438
	class 16: 0.0
	class 17: 0.0
	class 18: 0.3858653

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 30, step: 6
select part of clients to conduct local training
[10, 30, 4, 33]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=13.181749637983739
Loss made of: CE 0.1136983260512352, LKD 0.0, LDE 0.0, LReg 0.0, POD 12.417192459106445 EntMin 0.0
Epoch 1, Class Loss=0.06701409071683884, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.06701409071683884, Class Loss=0.06701409071683884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 2, Batch 10/12, Loss=11.4551036298275
Loss made of: CE 0.344681978225708, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.826713562011719 EntMin 0.0
Epoch 2, Class Loss=0.31054848432540894, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.31054848432540894, Class Loss=0.31054848432540894, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=11.085284823179245
Loss made of: CE 0.6212599277496338, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.649888038635254 EntMin 0.0
Epoch 3, Class Loss=0.6681232452392578, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.6681232452392578, Class Loss=0.6681232452392578, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.582107990980148
Loss made of: CE 0.7702716588973999, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.298175811767578 EntMin 0.0
Epoch 4, Class Loss=0.7893168926239014, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.7893168926239014, Class Loss=0.7893168926239014, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.360720372200012
Loss made of: CE 0.6852596998214722, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.459406852722168 EntMin 0.0
Epoch 5, Class Loss=0.7596983909606934, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.7596983909606934, Class Loss=0.7596983909606934, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.953144055604934
Loss made of: CE 0.6843294501304626, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.823182106018066 EntMin 0.0
Epoch 6, Class Loss=0.681678295135498, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.681678295135498, Class Loss=0.681678295135498, Reg Loss=0.0
Current Client Index:  30
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=8.79077642308548
Loss made of: CE 0.07219715416431427, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.988784790039062 EntMin 0.0
Epoch 1, Class Loss=0.04322436824440956, Reg Loss=0.0
Clinet index 30, End of Epoch 1/6, Average Loss=0.04322436824440956, Class Loss=0.04322436824440956, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/13, Loss=7.667005589604377
Loss made of: CE 0.20122897624969482, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.038786888122559 EntMin 0.0
Epoch 2, Class Loss=0.21459852159023285, Reg Loss=0.0
Clinet index 30, End of Epoch 2/6, Average Loss=0.21459852159023285, Class Loss=0.21459852159023285, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=7.0799338042736055
Loss made of: CE 0.33565017580986023, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.405293941497803 EntMin 0.0
Epoch 3, Class Loss=0.41669967770576477, Reg Loss=0.0
Clinet index 30, End of Epoch 3/6, Average Loss=0.41669967770576477, Class Loss=0.41669967770576477, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=6.781049063801765
Loss made of: CE 0.5817317366600037, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.815563201904297 EntMin 0.0
Epoch 4, Class Loss=0.5362234115600586, Reg Loss=0.0
Clinet index 30, End of Epoch 4/6, Average Loss=0.5362234115600586, Class Loss=0.5362234115600586, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=6.50668458044529
Loss made of: CE 0.7105903625488281, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.041500091552734 EntMin 0.0
Epoch 5, Class Loss=0.6086676716804504, Reg Loss=0.0
Clinet index 30, End of Epoch 5/6, Average Loss=0.6086676716804504, Class Loss=0.6086676716804504, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=6.427581983804703
Loss made of: CE 0.7266899943351746, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.326594352722168 EntMin 0.0
Epoch 6, Class Loss=0.6927387118339539, Reg Loss=0.0
Clinet index 30, End of Epoch 6/6, Average Loss=0.6927387118339539, Class Loss=0.6927387118339539, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=12.578901810199023
Loss made of: CE 0.037925295531749725, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.30205249786377 EntMin 0.0
Epoch 1, Class Loss=0.04894758760929108, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.04894758760929108, Class Loss=0.04894758760929108, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=11.148260506987572
Loss made of: CE 0.1446075439453125, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.370399475097656 EntMin 0.0
Epoch 2, Class Loss=0.2999953627586365, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.2999953627586365, Class Loss=0.2999953627586365, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 3, Batch 10/12, Loss=11.157277977466583
Loss made of: CE 0.7436110377311707, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.940380096435547 EntMin 0.0
Epoch 3, Class Loss=0.6679717898368835, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.6679717898368835, Class Loss=0.6679717898368835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.877141851186753
Loss made of: CE 0.9245302081108093, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.058759689331055 EntMin 0.0
Epoch 4, Class Loss=0.8564581871032715, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.8564581871032715, Class Loss=0.8564581871032715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.318697774410248
Loss made of: CE 0.7125439643859863, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.698915481567383 EntMin 0.0
Epoch 5, Class Loss=0.7962877750396729, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.7962877750396729, Class Loss=0.7962877750396729, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.540502369403839
Loss made of: CE 0.636199414730072, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.188488960266113 EntMin 0.0
Epoch 6, Class Loss=0.6827234625816345, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6827234625816345, Class Loss=0.6827234625816345, Reg Loss=0.0
Current Client Index:  33
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=13.046863332018257
Loss made of: CE 0.015494180843234062, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.755876541137695 EntMin 0.0
Epoch 1, Class Loss=0.060323767364025116, Reg Loss=0.0
Clinet index 33, End of Epoch 1/6, Average Loss=0.060323767364025116, Class Loss=0.060323767364025116, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=11.592322909832001
Loss made of: CE 0.2498188316822052, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.611989974975586 EntMin 0.0
Epoch 2, Class Loss=0.3305087983608246, Reg Loss=0.0
Clinet index 33, End of Epoch 2/6, Average Loss=0.3305087983608246, Class Loss=0.3305087983608246, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=11.46035721898079
Loss made of: CE 0.7374491691589355, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.982903480529785 EntMin 0.0
Epoch 3, Class Loss=0.6771999597549438, Reg Loss=0.0
Clinet index 33, End of Epoch 3/6, Average Loss=0.6771999597549438, Class Loss=0.6771999597549438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=11.09464602470398
Loss made of: CE 0.8453025817871094, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.080705642700195 EntMin 0.0
Epoch 4, Class Loss=0.799419641494751, Reg Loss=0.0
Clinet index 33, End of Epoch 4/6, Average Loss=0.799419641494751, Class Loss=0.799419641494751, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.58826161623001
Loss made of: CE 0.6531301736831665, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.9960551261901855 EntMin 0.0
Epoch 5, Class Loss=0.7713608145713806, Reg Loss=0.0
Clinet index 33, End of Epoch 5/6, Average Loss=0.7713608145713806, Class Loss=0.7713608145713806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.99085275530815
Loss made of: CE 0.6487342119216919, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.208459854125977 EntMin 0.0
Epoch 6, Class Loss=0.677123486995697, Reg Loss=0.0
Clinet index 33, End of Epoch 6/6, Average Loss=0.677123486995697, Class Loss=0.677123486995697, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8433844447135925, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.806656
Mean Acc: 0.413569
FreqW Acc: 0.675445
Mean IoU: 0.319355
Class IoU:
	class 0: 0.81252253
	class 1: 0.74838847
	class 2: 0.28446308
	class 3: 0.5310253
	class 4: 0.57466656
	class 5: 0.59039575
	class 6: 0.39583087
	class 7: 0.7076155
	class 8: 0.59418815
	class 9: 0.07722092
	class 10: 0.0
	class 11: 0.0
	class 12: 0.34464365
	class 13: 0.2331928
	class 14: 0.0
	class 15: 0.672591
	class 16: 0.0
	class 17: 0.0
	class 18: 0.01700324
	class 19: 0.12270616
	class 20: 0.0
Class Acc:
	class 0: 0.9641911
	class 1: 0.7987992
	class 2: 0.93405914
	class 3: 0.53938055
	class 4: 0.75160736
	class 5: 0.66958684
	class 6: 0.4022472
	class 7: 0.90954274
	class 8: 0.62017
	class 9: 0.094575815
	class 10: 0.0
	class 11: 0.0
	class 12: 0.4121685
	class 13: 0.59100705
	class 14: 0.0
	class 15: 0.8476952
	class 16: 0.0
	class 17: 0.0
	class 18: 0.017419798
	class 19: 0.13248906
	class 20: 0.0

federated global round: 31, step: 6
select part of clients to conduct local training
[9, 17, 14, 1]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=8.110062926262618
Loss made of: CE 0.05709024891257286, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.705968379974365 EntMin 0.0
Epoch 1, Class Loss=0.04981913045048714, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.04981913045048714, Class Loss=0.04981913045048714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=6.891183453798294
Loss made of: CE 0.11467640101909637, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.751359939575195 EntMin 0.0
Epoch 2, Class Loss=0.19438615441322327, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.19438615441322327, Class Loss=0.19438615441322327, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=6.499722996354103
Loss made of: CE 0.5197947025299072, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.685784339904785 EntMin 0.0
Epoch 3, Class Loss=0.3598514795303345, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.3598514795303345, Class Loss=0.3598514795303345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.154077476263046
Loss made of: CE 0.4418451189994812, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.757350921630859 EntMin 0.0
Epoch 4, Class Loss=0.5169942378997803, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.5169942378997803, Class Loss=0.5169942378997803, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=6.122565197944641
Loss made of: CE 0.6087610125541687, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.632068157196045 EntMin 0.0
Epoch 5, Class Loss=0.6130280494689941, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.6130280494689941, Class Loss=0.6130280494689941, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=6.032305270433426
Loss made of: CE 0.6080554127693176, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.887999534606934 EntMin 0.0
Epoch 6, Class Loss=0.6848462224006653, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.6848462224006653, Class Loss=0.6848462224006653, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=9.356462272116914
Loss made of: CE 0.010066772811114788, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.259708404541016 EntMin 0.0
Epoch 1, Class Loss=0.0359993539750576, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.0359993539750576, Class Loss=0.0359993539750576, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=9.448842917382716
Loss made of: CE 0.10308387130498886, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.550634384155273 EntMin 0.0
Epoch 2, Class Loss=0.16792082786560059, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.16792082786560059, Class Loss=0.16792082786560059, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=8.758237728476525
Loss made of: CE 0.3291415572166443, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.397735595703125 EntMin 0.0
Epoch 3, Class Loss=0.34662145376205444, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.34662145376205444, Class Loss=0.34662145376205444, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.963608038425445
Loss made of: CE 0.4338073134422302, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0436859130859375 EntMin 0.0
Epoch 4, Class Loss=0.4633610248565674, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.4633610248565674, Class Loss=0.4633610248565674, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.33841614127159
Loss made of: CE 0.527747392654419, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.030379295349121 EntMin 0.0
Epoch 5, Class Loss=0.5109081864356995, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.5109081864356995, Class Loss=0.5109081864356995, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=8.122754490375518
Loss made of: CE 0.4643382430076599, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.99846076965332 EntMin 0.0
Epoch 6, Class Loss=0.5024804472923279, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.5024804472923279, Class Loss=0.5024804472923279, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=8.207260341010988
Loss made of: CE 0.05065611004829407, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.233180046081543 EntMin 0.0
Epoch 1, Class Loss=0.054928429424762726, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.054928429424762726, Class Loss=0.054928429424762726, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=6.8843005038797855
Loss made of: CE 0.092435322701931, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.068164348602295 EntMin 0.0
Epoch 2, Class Loss=0.18567322194576263, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.18567322194576263, Class Loss=0.18567322194576263, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=6.501692226529121
Loss made of: CE 0.24346709251403809, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.180639266967773 EntMin 0.0
Epoch 3, Class Loss=0.37695592641830444, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.37695592641830444, Class Loss=0.37695592641830444, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.30352013707161
Loss made of: CE 0.330186665058136, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.013767242431641 EntMin 0.0
Epoch 4, Class Loss=0.4977799355983734, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.4977799355983734, Class Loss=0.4977799355983734, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=6.143185430765152
Loss made of: CE 0.5311633348464966, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.564849853515625 EntMin 0.0
Epoch 5, Class Loss=0.5923786163330078, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5923786163330078, Class Loss=0.5923786163330078, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=6.022124409675598
Loss made of: CE 0.8177924156188965, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.856634140014648 EntMin 0.0
Epoch 6, Class Loss=0.671241044998169, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.671241044998169, Class Loss=0.671241044998169, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=9.501426230650395
Loss made of: CE 0.019191093742847443, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.753281593322754 EntMin 0.0
Epoch 1, Class Loss=0.027919409796595573, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.027919409796595573, Class Loss=0.027919409796595573, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=8.894487661868334
Loss made of: CE 0.13661108911037445, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.076017379760742 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.15142790973186493, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.15142790973186493, Class Loss=0.15142790973186493, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=8.954830759763718
Loss made of: CE 0.32091546058654785, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.463786125183105 EntMin 0.0
Epoch 3, Class Loss=0.3515006899833679, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3515006899833679, Class Loss=0.3515006899833679, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.799955868721009
Loss made of: CE 0.4106464087963104, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.755870342254639 EntMin 0.0
Epoch 4, Class Loss=0.4784396290779114, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.4784396290779114, Class Loss=0.4784396290779114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.517661690711975
Loss made of: CE 0.5112336874008179, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.61972713470459 EntMin 0.0
Epoch 5, Class Loss=0.5203561782836914, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.5203561782836914, Class Loss=0.5203561782836914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=8.187822023034096
Loss made of: CE 0.5541182160377502, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.32605504989624 EntMin 0.0
Epoch 6, Class Loss=0.5026420950889587, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.5026420950889587, Class Loss=0.5026420950889587, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7609719038009644, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.818188
Mean Acc: 0.459839
FreqW Acc: 0.699336
Mean IoU: 0.348598
Class IoU:
	class 0: 0.8300595
	class 1: 0.7796193
	class 2: 0.27736378
	class 3: 0.6152977
	class 4: 0.56627285
	class 5: 0.619014
	class 6: 0.6136205
	class 7: 0.65129054
	class 8: 0.6341799
	class 9: 0.086399294
	class 10: 0.0
	class 11: 8.45934e-06
	class 12: 0.37585807
	class 13: 0.22752479
	class 14: 0.0
	class 15: 0.691769
	class 16: 0.0
	class 17: 0.0
	class 18: 0.116484776
	class 19: 0.23579748
	class 20: 0.0
Class Acc:
	class 0: 0.95813143
	class 1: 0.8472015
	class 2: 0.9403599
	class 3: 0.62829715
	class 4: 0.8389495
	class 5: 0.7441241
	class 6: 0.65656435
	class 7: 0.93507594
	class 8: 0.67165923
	class 9: 0.1123891
	class 10: 0.0
	class 11: 8.45934e-06
	class 12: 0.46627823
	class 13: 0.6258312
	class 14: 0.0
	class 15: 0.83769035
	class 16: 0.0
	class 17: 0.0
	class 18: 0.13375601
	class 19: 0.26031253
	class 20: 0.0

federated global round: 32, step: 6
select part of clients to conduct local training
[3, 8, 27, 7]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=8.716530320979654
Loss made of: CE 0.02806750498712063, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.540151596069336 EntMin 0.0
Epoch 1, Class Loss=0.03445186838507652, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.03445186838507652, Class Loss=0.03445186838507652, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=8.257015856355428
Loss made of: CE 0.14293962717056274, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.965099334716797 EntMin 0.0
Epoch 2, Class Loss=0.17044734954833984, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.17044734954833984, Class Loss=0.17044734954833984, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=8.077114361524583
Loss made of: CE 0.4462538957595825, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.398367881774902 EntMin 0.0
Epoch 3, Class Loss=0.32691994309425354, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.32691994309425354, Class Loss=0.32691994309425354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=7.521668696403504
Loss made of: CE 0.4136408269405365, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.011722087860107 EntMin 0.0
Epoch 4, Class Loss=0.3964465856552124, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.3964465856552124, Class Loss=0.3964465856552124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=7.701179850101471
Loss made of: CE 0.34872645139694214, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.503213882446289 EntMin 0.0
Epoch 5, Class Loss=0.4578935503959656, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.4578935503959656, Class Loss=0.4578935503959656, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=7.323547342419625
Loss made of: CE 0.4241732060909271, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.112794399261475 EntMin 0.0
Epoch 6, Class Loss=0.46725136041641235, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.46725136041641235, Class Loss=0.46725136041641235, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.69338311906904
Loss made of: CE 0.05924966186285019, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.721324920654297 EntMin 0.0
Epoch 1, Class Loss=0.061732884496450424, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.061732884496450424, Class Loss=0.061732884496450424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.04894972294569
Loss made of: CE 0.2775493860244751, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.507307052612305 EntMin 0.0
Epoch 2, Class Loss=0.1830078661441803, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.1830078661441803, Class Loss=0.1830078661441803, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.819684086740017
Loss made of: CE 0.3315722346305847, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.235345840454102 EntMin 0.0
Epoch 3, Class Loss=0.33276617527008057, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.33276617527008057, Class Loss=0.33276617527008057, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.762267962098122
Loss made of: CE 0.4457671642303467, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.584664821624756 EntMin 0.0
Epoch 4, Class Loss=0.46044760942459106, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.46044760942459106, Class Loss=0.46044760942459106, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.629407417774201
Loss made of: CE 0.46535205841064453, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.793985366821289 EntMin 0.0
Epoch 5, Class Loss=0.5659334659576416, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.5659334659576416, Class Loss=0.5659334659576416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.497142970561981
Loss made of: CE 0.7155641913414001, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.308346748352051 EntMin 0.0
Epoch 6, Class Loss=0.6273002028465271, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6273002028465271, Class Loss=0.6273002028465271, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.547531043551862
Loss made of: CE 0.12987887859344482, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.40639591217041 EntMin 0.0
Epoch 1, Class Loss=0.05381913110613823, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.05381913110613823, Class Loss=0.05381913110613823, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=5.724191074073315
Loss made of: CE 0.2517569661140442, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.53486967086792 EntMin 0.0
Epoch 2, Class Loss=0.18005217611789703, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.18005217611789703, Class Loss=0.18005217611789703, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.822114887833595
Loss made of: CE 0.28795674443244934, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2648091316223145 EntMin 0.0
Epoch 3, Class Loss=0.35194966197013855, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.35194966197013855, Class Loss=0.35194966197013855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.585013973712921
Loss made of: CE 0.2804923951625824, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.40308141708374 EntMin 0.0
Epoch 4, Class Loss=0.4533163607120514, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.4533163607120514, Class Loss=0.4533163607120514, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.675290927290916
Loss made of: CE 0.49904561042785645, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9858598709106445 EntMin 0.0
Epoch 5, Class Loss=0.5456809997558594, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.5456809997558594, Class Loss=0.5456809997558594, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.590868973731995
Loss made of: CE 0.6034237742424011, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.62985897064209 EntMin 0.0
Epoch 6, Class Loss=0.6262639164924622, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.6262639164924622, Class Loss=0.6262639164924622, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.9840976934880015
Loss made of: CE 0.08596988767385483, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.28278112411499 EntMin 0.0
Epoch 1, Class Loss=0.0669630914926529, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.0669630914926529, Class Loss=0.0669630914926529, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.260251896083355
Loss made of: CE 0.20914892852306366, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.346504211425781 EntMin 0.0
Epoch 2, Class Loss=0.18615131080150604, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.18615131080150604, Class Loss=0.18615131080150604, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.732173252105713
Loss made of: CE 0.2243189513683319, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.536099433898926 EntMin 0.0
Epoch 3, Class Loss=0.33864206075668335, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.33864206075668335, Class Loss=0.33864206075668335, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=6.127101224660874
Loss made of: CE 0.4965636134147644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.991617202758789 EntMin 0.0
Epoch 4, Class Loss=0.47370508313179016, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.47370508313179016, Class Loss=0.47370508313179016, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.723703330755233
Loss made of: CE 0.49181875586509705, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734808444976807 EntMin 0.0
Epoch 5, Class Loss=0.549505889415741, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.549505889415741, Class Loss=0.549505889415741, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.5591479480266575
Loss made of: CE 0.5303449630737305, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.585646152496338 EntMin 0.0
Epoch 6, Class Loss=0.6279397010803223, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.6279397010803223, Class Loss=0.6279397010803223, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7511622905731201, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.819359
Mean Acc: 0.477149
FreqW Acc: 0.705643
Mean IoU: 0.352137
Class IoU:
	class 0: 0.83654016
	class 1: 0.79356176
	class 2: 0.28402758
	class 3: 0.6206618
	class 4: 0.55001867
	class 5: 0.6312745
	class 6: 0.6873441
	class 7: 0.6440363
	class 8: 0.70101076
	class 9: 0.09064426
	class 10: 0.0
	class 11: 7.0494286e-07
	class 12: 0.38146955
	class 13: 0.21873042
	class 14: 0.0
	class 15: 0.70151126
	class 16: 0.0
	class 17: 0.0
	class 18: 0.20116737
	class 19: 0.052870505
	class 20: 0.0
Class Acc:
	class 0: 0.95074123
	class 1: 0.89490175
	class 2: 0.9383552
	class 3: 0.6362063
	class 4: 0.87139887
	class 5: 0.7874631
	class 6: 0.7675011
	class 7: 0.9339175
	class 8: 0.77251035
	class 9: 0.12342971
	class 10: 0.0
	class 11: 7.04945e-07
	class 12: 0.48050085
	class 13: 0.7005406
	class 14: 0.0
	class 15: 0.8475735
	class 16: 0.0
	class 17: 0.0
	class 18: 0.26175463
	class 19: 0.053342365
	class 20: 0.0

federated global round: 33, step: 6
select part of clients to conduct local training
[6, 20, 13, 10]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=5.195179082080722
Loss made of: CE 0.00916101410984993, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.401914596557617 EntMin 0.0
Epoch 1, Class Loss=0.02756042592227459, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.02756042592227459, Class Loss=0.02756042592227459, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.3454207681119446
Loss made of: CE 0.08421342074871063, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143672943115234 EntMin 0.0
Epoch 2, Class Loss=0.14912115037441254, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.14912115037441254, Class Loss=0.14912115037441254, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.3481373086571695
Loss made of: CE 0.3026779890060425, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.64306640625 EntMin 0.0
Epoch 3, Class Loss=0.27203989028930664, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.27203989028930664, Class Loss=0.27203989028930664, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.5195869415998455
Loss made of: CE 0.3765903115272522, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.418593406677246 EntMin 0.0
Epoch 4, Class Loss=0.37337014079093933, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.37337014079093933, Class Loss=0.37337014079093933, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.335243180394173
Loss made of: CE 0.35336145758628845, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.765498161315918 EntMin 0.0
Epoch 5, Class Loss=0.47167643904685974, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.47167643904685974, Class Loss=0.47167643904685974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.15704157948494
Loss made of: CE 0.5767765045166016, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3601179122924805 EntMin 0.0
Epoch 6, Class Loss=0.5710451006889343, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5710451006889343, Class Loss=0.5710451006889343, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=5.0901646027341485
Loss made of: CE 0.012424841523170471, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.406991004943848 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.03404417634010315, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.03404417634010315, Class Loss=0.03404417634010315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.252711311727762
Loss made of: CE 0.0852268636226654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.604466438293457 EntMin 0.0
Epoch 2, Class Loss=0.13615159690380096, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.13615159690380096, Class Loss=0.13615159690380096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.281181514263153
Loss made of: CE 0.20658457279205322, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.159870147705078 EntMin 0.0
Epoch 3, Class Loss=0.2489755004644394, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.2489755004644394, Class Loss=0.2489755004644394, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.242426279187202
Loss made of: CE 0.32240843772888184, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0964837074279785 EntMin 0.0
Epoch 4, Class Loss=0.37853965163230896, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.37853965163230896, Class Loss=0.37853965163230896, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.506836393475533
Loss made of: CE 0.5347720980644226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.609645843505859 EntMin 0.0
Epoch 5, Class Loss=0.48111864924430847, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.48111864924430847, Class Loss=0.48111864924430847, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.329591774940491
Loss made of: CE 0.5553869605064392, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.388633728027344 EntMin 0.0
Epoch 6, Class Loss=0.563998818397522, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.563998818397522, Class Loss=0.563998818397522, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=9.699327934347092
Loss made of: CE 0.03681854531168938, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.87769603729248 EntMin 0.0
Epoch 1, Class Loss=0.04467201232910156, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.04467201232910156, Class Loss=0.04467201232910156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=8.086698077619076
Loss made of: CE 0.33519530296325684, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.9451751708984375 EntMin 0.0
Epoch 2, Class Loss=0.21699683368206024, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.21699683368206024, Class Loss=0.21699683368206024, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=7.9477553337812425
Loss made of: CE 0.2755002975463867, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.57728910446167 EntMin 0.0
Epoch 3, Class Loss=0.3903248906135559, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.3903248906135559, Class Loss=0.3903248906135559, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=7.912549129128456
Loss made of: CE 0.43994688987731934, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.811336040496826 EntMin 0.0
Epoch 4, Class Loss=0.43826407194137573, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.43826407194137573, Class Loss=0.43826407194137573, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.683057022094727
Loss made of: CE 0.44191059470176697, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.014273643493652 EntMin 0.0
Epoch 5, Class Loss=0.4677008092403412, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.4677008092403412, Class Loss=0.4677008092403412, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=7.485042828321457
Loss made of: CE 0.4953654408454895, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.040895462036133 EntMin 0.0
Epoch 6, Class Loss=0.49143585562705994, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.49143585562705994, Class Loss=0.49143585562705994, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=11.168970733880997
Loss made of: CE 0.9313420057296753, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.044598579406738 EntMin 0.0
Epoch 1, Class Loss=0.969176173210144, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.969176173210144, Class Loss=0.969176173210144, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/12, Loss=8.92226409316063
Loss made of: CE 0.7425059080123901, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.405426979064941 EntMin 0.0
Epoch 2, Class Loss=0.7182602286338806, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.7182602286338806, Class Loss=0.7182602286338806, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=8.62192353606224
Loss made of: CE 0.5109156966209412, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.291561126708984 EntMin 0.0
Epoch 3, Class Loss=0.534493088722229, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.534493088722229, Class Loss=0.534493088722229, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/12, Loss=8.105877834558488
Loss made of: CE 0.45961666107177734, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.090351104736328 EntMin 0.0
Epoch 4, Class Loss=0.47167128324508667, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.47167128324508667, Class Loss=0.47167128324508667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/12, Loss=7.814404460787773
Loss made of: CE 0.4070812463760376, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.9407758712768555 EntMin 0.0
Epoch 5, Class Loss=0.4400617480278015, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.4400617480278015, Class Loss=0.4400617480278015, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/12, Loss=7.725667038559914
Loss made of: CE 0.4132058024406433, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.286113739013672 EntMin 0.0
Epoch 6, Class Loss=0.4378127455711365, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.4378127455711365, Class Loss=0.4378127455711365, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7097289562225342, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.822884
Mean Acc: 0.473793
FreqW Acc: 0.709636
Mean IoU: 0.358213
Class IoU:
	class 0: 0.83984244
	class 1: 0.79402936
	class 2: 0.2874939
	class 3: 0.57588214
	class 4: 0.555558
	class 5: 0.64273816
	class 6: 0.6466098
	class 7: 0.65748894
	class 8: 0.65496063
	class 9: 0.07846789
	class 10: 0.0
	class 11: 0.0
	class 12: 0.35052794
	class 13: 0.22660658
	class 14: 0.0
	class 15: 0.6907566
	class 16: 0.0
	class 17: 0.0
	class 18: 0.16441058
	class 19: 0.35710108
	class 20: 0.0
Class Acc:
	class 0: 0.9579959
	class 1: 0.84967166
	class 2: 0.93449706
	class 3: 0.5838486
	class 4: 0.8473242
	class 5: 0.7700551
	class 6: 0.6876898
	class 7: 0.93015087
	class 8: 0.7056043
	class 9: 0.097167686
	class 10: 0.0
	class 11: 0.0
	class 12: 0.42177045
	class 13: 0.67079544
	class 14: 0.0
	class 15: 0.82138854
	class 16: 0.0
	class 17: 0.0
	class 18: 0.19855064
	class 19: 0.47314343
	class 20: 0.0

federated global round: 34, step: 6
select part of clients to conduct local training
[19, 18, 27, 14]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.075682637002319
Loss made of: CE 0.00766811054199934, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.616265296936035 EntMin 0.0
Epoch 1, Class Loss=0.037749726325273514, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.037749726325273514, Class Loss=0.037749726325273514, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.495608399808407
Loss made of: CE 0.11529861390590668, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.843050003051758 EntMin 0.0
Epoch 2, Class Loss=0.1391955465078354, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.1391955465078354, Class Loss=0.1391955465078354, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.206039343774319
Loss made of: CE 0.2916225790977478, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679993629455566 EntMin 0.0
Epoch 3, Class Loss=0.2873813211917877, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.2873813211917877, Class Loss=0.2873813211917877, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.219942286610603
Loss made of: CE 0.3696030080318451, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.811582565307617 EntMin 0.0
Epoch 4, Class Loss=0.4042576849460602, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.4042576849460602, Class Loss=0.4042576849460602, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.4055379152297975
Loss made of: CE 0.43167775869369507, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.726568222045898 EntMin 0.0
Epoch 5, Class Loss=0.5208767652511597, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.5208767652511597, Class Loss=0.5208767652511597, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.497167998552323
Loss made of: CE 0.5290641784667969, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.053010940551758 EntMin 0.0
Epoch 6, Class Loss=0.6255013346672058, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.6255013346672058, Class Loss=0.6255013346672058, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=5.957400356465951
Loss made of: CE 0.04157393425703049, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.961380958557129 EntMin 0.0
Epoch 1, Class Loss=0.038634251803159714, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.038634251803159714, Class Loss=0.038634251803159714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.152809641510248
Loss made of: CE 0.10035163164138794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.707765102386475 EntMin 0.0
Epoch 2, Class Loss=0.1318107694387436, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.1318107694387436, Class Loss=0.1318107694387436, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.239930261671543
Loss made of: CE 0.24738292396068573, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.877793312072754 EntMin 0.0
Epoch 3, Class Loss=0.26443541049957275, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.26443541049957275, Class Loss=0.26443541049957275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.0740269929170605
Loss made of: CE 0.3723013401031494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.252275466918945 EntMin 0.0
Epoch 4, Class Loss=0.38777628540992737, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.38777628540992737, Class Loss=0.38777628540992737, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.190468740463257
Loss made of: CE 0.5199393630027771, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.748145580291748 EntMin 0.0
Epoch 5, Class Loss=0.4937330186367035, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.4937330186367035, Class Loss=0.4937330186367035, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.194673067331314
Loss made of: CE 0.5544953346252441, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.153397560119629 EntMin 0.0
Epoch 6, Class Loss=0.6055947542190552, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.6055947542190552, Class Loss=0.6055947542190552, Reg Loss=0.0
Current Client Index:  27
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.844943445920944
Loss made of: CE 0.8281225562095642, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.644044399261475 EntMin 0.0
Epoch 1, Class Loss=0.9299418926239014, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.9299418926239014, Class Loss=0.9299418926239014, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/13, Loss=5.656458467245102
Loss made of: CE 0.9199670553207397, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.079629898071289 EntMin 0.0
Epoch 2, Class Loss=0.8315833210945129, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.8315833210945129, Class Loss=0.8315833210945129, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/13, Loss=5.642092055082321
Loss made of: CE 0.8737563490867615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.862364292144775 EntMin 0.0
Epoch 3, Class Loss=0.7686800360679626, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.7686800360679626, Class Loss=0.7686800360679626, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/13, Loss=5.378994423151016
Loss made of: CE 0.7353550791740417, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.101780891418457 EntMin 0.0
Epoch 4, Class Loss=0.7472264766693115, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.7472264766693115, Class Loss=0.7472264766693115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/13, Loss=5.288711911439895
Loss made of: CE 0.6835464835166931, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.245547771453857 EntMin 0.0
Epoch 5, Class Loss=0.7162294387817383, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.7162294387817383, Class Loss=0.7162294387817383, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/13, Loss=5.288416022062302
Loss made of: CE 0.6889857053756714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.215892791748047 EntMin 0.0
Epoch 6, Class Loss=0.7095910906791687, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.7095910906791687, Class Loss=0.7095910906791687, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 1, Batch 10/13, Loss=6.893151664733887
Loss made of: CE 0.9757828712463379, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.186089515686035 EntMin 0.0
Epoch 1, Class Loss=0.8990315198898315, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.8990315198898315, Class Loss=0.8990315198898315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/13, Loss=5.8841053485870365
Loss made of: CE 0.7959046363830566, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.713476181030273 EntMin 0.0
Epoch 2, Class Loss=0.8085421323776245, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.8085421323776245, Class Loss=0.8085421323776245, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/13, Loss=5.592394864559173
Loss made of: CE 0.6644692420959473, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2119598388671875 EntMin 0.0
Epoch 3, Class Loss=0.7422380447387695, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.7422380447387695, Class Loss=0.7422380447387695, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/13, Loss=5.398224383592606
Loss made of: CE 0.6344753503799438, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.33580207824707 EntMin 0.0
Epoch 4, Class Loss=0.7152416706085205, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.7152416706085205, Class Loss=0.7152416706085205, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/13, Loss=5.339780485630035
Loss made of: CE 0.6834672689437866, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.387626647949219 EntMin 0.0
Epoch 5, Class Loss=0.7003084421157837, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.7003084421157837, Class Loss=0.7003084421157837, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/13, Loss=5.314530998468399
Loss made of: CE 0.7507933378219604, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.620089530944824 EntMin 0.0
Epoch 6, Class Loss=0.6880989074707031, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.6880989074707031, Class Loss=0.6880989074707031, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7277784943580627, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.819739
Mean Acc: 0.485010
FreqW Acc: 0.709041
Mean IoU: 0.356606
Class IoU:
	class 0: 0.842498
	class 1: 0.80644166
	class 2: 0.2866166
	class 3: 0.64750475
	class 4: 0.5420525
	class 5: 0.62670505
	class 6: 0.58301413
	class 7: 0.66607755
	class 8: 0.72513944
	class 9: 0.08354459
	class 10: 0.0
	class 11: 0.0
	class 12: 0.3740723
	class 13: 0.22102052
	class 14: 0.0
	class 15: 0.6947185
	class 16: 0.0
	class 17: 0.0
	class 18: 0.2226171
	class 19: 0.01332275
	class 20: 0.15337655
Class Acc:
	class 0: 0.9528557
	class 1: 0.90517235
	class 2: 0.93296397
	class 3: 0.6700685
	class 4: 0.8687797
	class 5: 0.79692364
	class 6: 0.6331328
	class 7: 0.9286714
	class 8: 0.8076033
	class 9: 0.11064908
	class 10: 0.0
	class 11: 0.0
	class 12: 0.47074905
	class 13: 0.69749635
	class 14: 0.0
	class 15: 0.8302536
	class 16: 0.0
	class 17: 0.0
	class 18: 0.28875846
	class 19: 0.013327288
	class 20: 0.27779558

voc_8-2_OURS On GPUs 2
Run in 106426s
