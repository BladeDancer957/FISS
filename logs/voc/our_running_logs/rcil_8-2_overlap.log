nohup: ignoring input
35
kvoc_8-2_RCIL On GPUs 2\Writing in results/seed_2023-ov/2023-03-15_voc_8-2_RCIL.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: None
Epoch 1, lr = 0.010000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/72, Loss=1.3388906061649322
Loss made of: CE 0.8479092121124268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/72, Loss=0.5651932477951049
Loss made of: CE 0.4969347417354584, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/72, Loss=0.3834062576293945
Loss made of: CE 0.2922220826148987, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/72, Loss=0.3724807262420654
Loss made of: CE 0.3208848237991333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/72, Loss=0.3232965856790543
Loss made of: CE 0.347342848777771, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/72, Loss=0.2812351375818253
Loss made of: CE 0.2607148289680481, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/72, Loss=0.27519139349460603
Loss made of: CE 0.3190236985683441, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.49896156787872314, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.49896156787872314, Class Loss=0.49896156787872314, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/72, Loss=0.24918241947889327
Loss made of: CE 0.21301482617855072, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/72, Loss=0.25948717147111894
Loss made of: CE 0.2098589837551117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/72, Loss=0.27360016107559204
Loss made of: CE 0.15481650829315186, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/72, Loss=0.23798072189092637
Loss made of: CE 0.1727456897497177, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/72, Loss=0.23555446565151214
Loss made of: CE 0.27753788232803345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/72, Loss=0.230945160984993
Loss made of: CE 0.3614562451839447, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/72, Loss=0.21005117148160934
Loss made of: CE 0.15605032444000244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2408791184425354, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.2408791184425354, Class Loss=0.2408791184425354, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/72, Loss=0.18985944241285324
Loss made of: CE 0.24650225043296814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/72, Loss=0.20537357330322265
Loss made of: CE 0.25439760088920593, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/72, Loss=0.19488425850868224
Loss made of: CE 0.16991405189037323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/72, Loss=0.15715253949165345
Loss made of: CE 0.15413902699947357, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/72, Loss=0.16010114997625352
Loss made of: CE 0.17305336892604828, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/72, Loss=0.18441115841269493
Loss made of: CE 0.17464813590049744, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/72, Loss=0.1662002146244049
Loss made of: CE 0.19677512347698212, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.18001337349414825, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.18001337349414825, Class Loss=0.18001337349414825, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/72, Loss=0.15717890113592148
Loss made of: CE 0.13020889461040497, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/72, Loss=0.14501830115914344
Loss made of: CE 0.1249045729637146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/72, Loss=0.13954352885484694
Loss made of: CE 0.17879265546798706, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/72, Loss=0.16447215378284455
Loss made of: CE 0.20259492099285126, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/72, Loss=0.14773373529314995
Loss made of: CE 0.14268085360527039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/72, Loss=0.17085910737514495
Loss made of: CE 0.18819275498390198, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/72, Loss=0.14057914838194846
Loss made of: CE 0.11160634458065033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.15241801738739014, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.15241801738739014, Class Loss=0.15241801738739014, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/72, Loss=0.153669985383749
Loss made of: CE 0.14879201352596283, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/72, Loss=0.1513858363032341
Loss made of: CE 0.20530912280082703, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/72, Loss=0.1195748694241047
Loss made of: CE 0.14454615116119385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/72, Loss=0.1278662711381912
Loss made of: CE 0.16812317073345184, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/72, Loss=0.1268318310379982
Loss made of: CE 0.14589957892894745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/72, Loss=0.11818521767854691
Loss made of: CE 0.15522260963916779, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/72, Loss=0.11614041179418563
Loss made of: CE 0.1460990309715271, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.12988628447055817, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.12988628447055817, Class Loss=0.12988628447055817, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/72, Loss=0.11631943434476852
Loss made of: CE 0.11378645151853561, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/72, Loss=0.10806186646223068
Loss made of: CE 0.14243873953819275, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/72, Loss=0.11746126115322113
Loss made of: CE 0.10045450925827026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/72, Loss=0.1124738797545433
Loss made of: CE 0.1371108442544937, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/72, Loss=0.132464936375618
Loss made of: CE 0.1032738983631134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/72, Loss=0.1180480308830738
Loss made of: CE 0.11133068799972534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/72, Loss=0.1255435712635517
Loss made of: CE 0.18464301526546478, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11932124197483063, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.11932124197483063, Class Loss=0.11932124197483063, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/50, Loss=1.1772181510925293
Loss made of: CE 1.0532593727111816, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/50, Loss=0.6831991791725158
Loss made of: CE 0.5470215678215027, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/50, Loss=0.44934925734996795
Loss made of: CE 0.3934897780418396, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/50, Loss=0.3340898841619492
Loss made of: CE 0.3015037477016449, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/50, Loss=0.28779410421848295
Loss made of: CE 0.28863582015037537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5863301157951355, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.5863301157951355, Class Loss=0.5863301157951355, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/50, Loss=0.24620795696973802
Loss made of: CE 0.2520599365234375, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/50, Loss=0.2637610465288162
Loss made of: CE 0.24749790132045746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/50, Loss=0.21873183101415633
Loss made of: CE 0.23045003414154053, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/50, Loss=0.26620966792106626
Loss made of: CE 0.29676663875579834, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/50, Loss=0.22963337749242782
Loss made of: CE 0.24402311444282532, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.24490876495838165, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.24490876495838165, Class Loss=0.24490876495838165, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/50, Loss=0.22365807145833969
Loss made of: CE 0.13942335546016693, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/50, Loss=0.2166964143514633
Loss made of: CE 0.2760385274887085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/50, Loss=0.1860978826880455
Loss made of: CE 0.17898786067962646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/50, Loss=0.18226213455200196
Loss made of: CE 0.15338647365570068, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/50, Loss=0.17751211673021317
Loss made of: CE 0.20634347200393677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.19724532961845398, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.19724532961845398, Class Loss=0.19724532961845398, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/50, Loss=0.1543257251381874
Loss made of: CE 0.1356528401374817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/50, Loss=0.17830854058265685
Loss made of: CE 0.17077021300792694, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/50, Loss=0.15871278196573257
Loss made of: CE 0.14450009167194366, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/50, Loss=0.16597265675663947
Loss made of: CE 0.2438901662826538, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/50, Loss=0.16338966563344
Loss made of: CE 0.19960474967956543, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.16414186358451843, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.16414186358451843, Class Loss=0.16414186358451843, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/50, Loss=0.15412695556879044
Loss made of: CE 0.1260605901479721, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/50, Loss=0.1670721299946308
Loss made of: CE 0.1293897032737732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/50, Loss=0.17011175081133842
Loss made of: CE 0.11854247748851776, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/50, Loss=0.1508725754916668
Loss made of: CE 0.12354470789432526, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/50, Loss=0.15351326987147332
Loss made of: CE 0.1450861394405365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.15913933515548706, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.15913933515548706, Class Loss=0.15913933515548706, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/50, Loss=0.14675964191555976
Loss made of: CE 0.1274341344833374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/50, Loss=0.12399458214640617
Loss made of: CE 0.10990091413259506, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/50, Loss=0.12179474011063576
Loss made of: CE 0.12677501142024994, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/50, Loss=0.1421540029346943
Loss made of: CE 0.11029194295406342, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/50, Loss=0.12475620359182357
Loss made of: CE 0.13483518362045288, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.13189183175563812, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.13189183175563812, Class Loss=0.13189183175563812, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/61, Loss=1.2362619280815124
Loss made of: CE 0.7255339622497559, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.5990858167409897
Loss made of: CE 0.5453583002090454, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.4196604579687119
Loss made of: CE 0.4538000524044037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.35613304376602173
Loss made of: CE 0.3313075006008148, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.28361968994140624
Loss made of: CE 0.26667898893356323, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.3146918550133705
Loss made of: CE 0.27261823415756226, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5294411778450012, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5294411778450012, Class Loss=0.5294411778450012, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/61, Loss=0.25084042102098464
Loss made of: CE 0.34087324142456055, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.22382188141345977
Loss made of: CE 0.21917250752449036, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.19242076873779296
Loss made of: CE 0.2059108465909958, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.26560617983341217
Loss made of: CE 0.19025085866451263, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.276051239669323
Loss made of: CE 0.36311930418014526, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.20006767511367798
Loss made of: CE 0.14075149595737457, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.23437923192977905, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.23437923192977905, Class Loss=0.23437923192977905, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/61, Loss=0.2047937646508217
Loss made of: CE 0.18461614847183228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.18223576620221138
Loss made of: CE 0.12166102975606918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.19391334652900696
Loss made of: CE 0.15644487738609314, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.1746232807636261
Loss made of: CE 0.15254591405391693, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.18869380354881288
Loss made of: CE 0.14134925603866577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.18827250301837922
Loss made of: CE 0.17233459651470184, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.18796105682849884, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.18796105682849884, Class Loss=0.18796105682849884, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/61, Loss=0.15816477239131926
Loss made of: CE 0.1433728188276291, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.16248028427362443
Loss made of: CE 0.16826453804969788, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.18758043199777602
Loss made of: CE 0.3894955813884735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.1717291958630085
Loss made of: CE 0.11976993083953857, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.14889717549085618
Loss made of: CE 0.13631942868232727, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.17483631223440171
Loss made of: CE 0.14787587523460388, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.16715559363365173, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.16715559363365173, Class Loss=0.16715559363365173, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/61, Loss=0.1364536814391613
Loss made of: CE 0.1180625781416893, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.14323420599102973
Loss made of: CE 0.11841710656881332, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.1311626151204109
Loss made of: CE 0.14213518798351288, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.14308900162577629
Loss made of: CE 0.11977994441986084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.11504250690340996
Loss made of: CE 0.09959070384502411, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.1621682070195675
Loss made of: CE 0.23577694594860077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.13845466077327728, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.13845466077327728, Class Loss=0.13845466077327728, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/61, Loss=0.130177304148674
Loss made of: CE 0.15323208272457123, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.1292868360877037
Loss made of: CE 0.09696163237094879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.12058701813220978
Loss made of: CE 0.08575838059186935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.12669359296560287
Loss made of: CE 0.22452829778194427, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.11290049627423286
Loss made of: CE 0.10226783156394958, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.13898451924324035
Loss made of: CE 0.14149326086044312, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.12655484676361084, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.12655484676361084, Class Loss=0.12655484676361084, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/66, Loss=1.3070074260234832
Loss made of: CE 0.7876971960067749, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/66, Loss=0.6342305183410645
Loss made of: CE 0.44836950302124023, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/66, Loss=0.42119947373867034
Loss made of: CE 0.33005771040916443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/66, Loss=0.3136217802762985
Loss made of: CE 0.2753300070762634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/66, Loss=0.2889087736606598
Loss made of: CE 0.30528178811073303, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/66, Loss=0.2811176598072052
Loss made of: CE 0.21242865920066833, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5125429034233093, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.5125429034233093, Class Loss=0.5125429034233093, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/66, Loss=0.23409589678049086
Loss made of: CE 0.2214151918888092, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/66, Loss=0.22923839837312698
Loss made of: CE 0.2400520145893097, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/66, Loss=0.22808797508478165
Loss made of: CE 0.15149584412574768, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/66, Loss=0.19748070240020751
Loss made of: CE 0.17602324485778809, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/66, Loss=0.19786084145307542
Loss made of: CE 0.16296613216400146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/66, Loss=0.19939319640398026
Loss made of: CE 0.1828150749206543, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.215201735496521, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.215201735496521, Class Loss=0.215201735496521, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/66, Loss=0.1987747833132744
Loss made of: CE 0.13383065164089203, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/66, Loss=0.16992676258087158
Loss made of: CE 0.14755530655384064, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/66, Loss=0.1990233600139618
Loss made of: CE 0.13487392663955688, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/66, Loss=0.18089938163757324
Loss made of: CE 0.14124709367752075, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/66, Loss=0.18290236592292786
Loss made of: CE 0.2215784788131714, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/66, Loss=0.16525174975395202
Loss made of: CE 0.1625327318906784, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.18090538680553436, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.18090538680553436, Class Loss=0.18090538680553436, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/66, Loss=0.15211049020290374
Loss made of: CE 0.17619141936302185, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/66, Loss=0.1468912996351719
Loss made of: CE 0.15552875399589539, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/66, Loss=0.16119751259684562
Loss made of: CE 0.14113029837608337, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/66, Loss=0.1581874445080757
Loss made of: CE 0.11078660190105438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/66, Loss=0.1351217061281204
Loss made of: CE 0.15664955973625183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/66, Loss=0.1801887758076191
Loss made of: CE 0.2540554702281952, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1555246263742447, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.1555246263742447, Class Loss=0.1555246263742447, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/66, Loss=0.15063635557889937
Loss made of: CE 0.1769694685935974, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/66, Loss=0.13286680430173875
Loss made of: CE 0.13058297336101532, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/66, Loss=0.12478793859481811
Loss made of: CE 0.12770536541938782, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/66, Loss=0.13372273221611977
Loss made of: CE 0.08860199898481369, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/66, Loss=0.1667195349931717
Loss made of: CE 0.13771861791610718, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/66, Loss=0.11404569670557976
Loss made of: CE 0.13884589076042175, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.13689665496349335, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.13689665496349335, Class Loss=0.13689665496349335, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/66, Loss=0.12211700603365898
Loss made of: CE 0.11689615249633789, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/66, Loss=0.11225669458508492
Loss made of: CE 0.08596783876419067, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/66, Loss=0.1265587940812111
Loss made of: CE 0.11910488456487656, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/66, Loss=0.11977752968668938
Loss made of: CE 0.08324422687292099, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/66, Loss=0.132962603867054
Loss made of: CE 0.09620468318462372, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/66, Loss=0.1161008358001709
Loss made of: CE 0.16767190396785736, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11963187903165817, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.11963187903165817, Class Loss=0.11963187903165817, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2959594428539276, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.893714
Mean Acc: 0.437941
FreqW Acc: 0.801686
Mean IoU: 0.397601
Class IoU:
	class 0: 0.89043576
	class 1: 0.15644802
	class 2: 0.043999832
	class 3: 0.03897764
	class 4: 0.22703509
	class 5: 0.0
	class 6: 0.81809694
	class 7: 0.68104637
	class 8: 0.7223723
Class Acc:
	class 0: 0.9932877
	class 1: 0.15651923
	class 2: 0.047976147
	class 3: 0.038978975
	class 4: 0.22884016
	class 5: 0.0
	class 6: 0.9684645
	class 7: 0.7064472
	class 8: 0.8009554

federated global round: 1, step: 0
select part of clients to conduct local training
[4, 3, 1, 2]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/60, Loss=0.4192676395177841
Loss made of: CE 0.3746945858001709, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/60, Loss=0.32319219410419464
Loss made of: CE 0.20734438300132751, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/60, Loss=0.2639474615454674
Loss made of: CE 0.1800314038991928, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/60, Loss=0.20496893525123597
Loss made of: CE 0.25103339552879333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/60, Loss=0.23333661556243895
Loss made of: CE 0.21997812390327454, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/60, Loss=0.19095414280891418
Loss made of: CE 0.16215357184410095, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.27261120080947876, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.27261120080947876, Class Loss=0.27261120080947876, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/60, Loss=0.16353770345449448
Loss made of: CE 0.12199728190898895, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/60, Loss=0.18314651399850845
Loss made of: CE 0.13762304186820984, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/60, Loss=0.167555882781744
Loss made of: CE 0.11422204226255417, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/60, Loss=0.1769683875143528
Loss made of: CE 0.19044339656829834, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/60, Loss=0.16997004970908164
Loss made of: CE 0.1221320852637291, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/60, Loss=0.1677600622177124
Loss made of: CE 0.12799343466758728, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.17148977518081665, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.17148977518081665, Class Loss=0.17148977518081665, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/60, Loss=0.15169909298419954
Loss made of: CE 0.13850370049476624, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/60, Loss=0.12051490023732185
Loss made of: CE 0.10391983389854431, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/60, Loss=0.12259376943111419
Loss made of: CE 0.10259266942739487, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/60, Loss=0.1367439068853855
Loss made of: CE 0.11021994799375534, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/60, Loss=0.12876338437199591
Loss made of: CE 0.12847872078418732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/60, Loss=0.12742699682712555
Loss made of: CE 0.08859076350927353, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.13129034638404846, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.13129034638404846, Class Loss=0.13129034638404846, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/60, Loss=0.12940226793289183
Loss made of: CE 0.2111952006816864, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/60, Loss=0.11658338680863381
Loss made of: CE 0.09065324068069458, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/60, Loss=0.11965448558330535
Loss made of: CE 0.11047667264938354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/60, Loss=0.10658918023109436
Loss made of: CE 0.09714248776435852, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/60, Loss=0.1236420065164566
Loss made of: CE 0.09106439352035522, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/60, Loss=0.1178931675851345
Loss made of: CE 0.18601791560649872, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.11896075308322906, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.11896075308322906, Class Loss=0.11896075308322906, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/60, Loss=0.09957758262753487
Loss made of: CE 0.07741236686706543, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/60, Loss=0.10592829808592796
Loss made of: CE 0.09237223118543625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/60, Loss=0.10933405384421349
Loss made of: CE 0.09889887273311615, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/60, Loss=0.09782831817865371
Loss made of: CE 0.09463965892791748, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/60, Loss=0.12022694945335388
Loss made of: CE 0.09273211658000946, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/60, Loss=0.10490166246891022
Loss made of: CE 0.13895440101623535, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10629948228597641, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.10629948228597641, Class Loss=0.10629948228597641, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/60, Loss=0.09788886979222297
Loss made of: CE 0.0649861991405487, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/60, Loss=0.10826407447457313
Loss made of: CE 0.0633004754781723, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/60, Loss=0.10666905418038368
Loss made of: CE 0.15866242349147797, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/60, Loss=0.0908891201019287
Loss made of: CE 0.07339516282081604, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/60, Loss=0.10237820073962212
Loss made of: CE 0.12816652655601501, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/60, Loss=0.08906273990869522
Loss made of: CE 0.09748432040214539, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09919201582670212, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.09919201582670212, Class Loss=0.09919201582670212, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/61, Loss=0.4265773996710777
Loss made of: CE 0.21879276633262634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.24185035079717637
Loss made of: CE 0.21430444717407227, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.18419842571020126
Loss made of: CE 0.3835643529891968, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.217384535074234
Loss made of: CE 0.2484864443540573, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.21390831246972083
Loss made of: CE 0.21946623921394348, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.17988502085208893
Loss made of: CE 0.13311408460140228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2433244287967682, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.2433244287967682, Class Loss=0.2433244287967682, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/61, Loss=0.19028772041201591
Loss made of: CE 0.22448572516441345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.15471643432974816
Loss made of: CE 0.23179267346858978, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.15886743143200874
Loss made of: CE 0.12250306457281113, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.1431029662489891
Loss made of: CE 0.14982962608337402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.1557253494858742
Loss made of: CE 0.14464926719665527, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.15212400406599044
Loss made of: CE 0.14168602228164673, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.15823762118816376, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.15823762118816376, Class Loss=0.15823762118816376, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/61, Loss=0.14906057566404343
Loss made of: CE 0.12957724928855896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.13250824585556983
Loss made of: CE 0.17258822917938232, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.12388628497719764
Loss made of: CE 0.11918561160564423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.12234031632542611
Loss made of: CE 0.20978306233882904, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.127614925801754
Loss made of: CE 0.11246234178543091, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.1281539812684059
Loss made of: CE 0.0820893868803978, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.13042612373828888, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.13042612373828888, Class Loss=0.13042612373828888, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/61, Loss=0.11554711833596229
Loss made of: CE 0.09849449247121811, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.13227949514985085
Loss made of: CE 0.132491797208786, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.1039244331419468
Loss made of: CE 0.09700827300548553, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.11361047774553298
Loss made of: CE 0.10049831122159958, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.1078428640961647
Loss made of: CE 0.06791273504495621, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.10645814538002014
Loss made of: CE 0.14015573263168335, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.11311057955026627, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.11311057955026627, Class Loss=0.11311057955026627, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/61, Loss=0.0971399910748005
Loss made of: CE 0.09411662817001343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.10018093585968017
Loss made of: CE 0.08175038546323776, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.0925949715077877
Loss made of: CE 0.06890745460987091, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.1066570058465004
Loss made of: CE 0.07058052718639374, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.1010161243379116
Loss made of: CE 0.08111115545034409, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.107698255777359
Loss made of: CE 0.12885238230228424, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10044277459383011, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.10044277459383011, Class Loss=0.10044277459383011, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/61, Loss=0.10046018138527871
Loss made of: CE 0.12339387834072113, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.0913651742041111
Loss made of: CE 0.12212908267974854, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.09583839550614356
Loss made of: CE 0.09252341836690903, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.09257158488035203
Loss made of: CE 0.13511833548545837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.09798994436860084
Loss made of: CE 0.09872154891490936, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.10326179042458535
Loss made of: CE 0.07284477353096008, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09688563644886017, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.09688563644886017, Class Loss=0.09688563644886017, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/83, Loss=0.27798948287963865
Loss made of: CE 0.21564029157161713, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/83, Loss=0.22881981879472732
Loss made of: CE 0.22510044276714325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/83, Loss=0.18799329400062562
Loss made of: CE 0.18496167659759521, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/83, Loss=0.18012224212288858
Loss made of: CE 0.20265702903270721, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/83, Loss=0.1664292462170124
Loss made of: CE 0.12421548366546631, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/83, Loss=0.157838424295187
Loss made of: CE 0.18391674757003784, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/83, Loss=0.18509208858013154
Loss made of: CE 0.12630930542945862, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/83, Loss=0.19076734483242036
Loss made of: CE 0.16993413865566254, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1987246721982956, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.1987246721982956, Class Loss=0.1987246721982956, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/83, Loss=0.14360596761107444
Loss made of: CE 0.11128183454275131, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/83, Loss=0.14360080286860466
Loss made of: CE 0.10577418655157089, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/83, Loss=0.1402749590575695
Loss made of: CE 0.12753301858901978, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/83, Loss=0.12769040167331697
Loss made of: CE 0.1145973652601242, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/83, Loss=0.1449865408241749
Loss made of: CE 0.12634888291358948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/83, Loss=0.12818826735019684
Loss made of: CE 0.11148794740438461, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/83, Loss=0.13169706091284752
Loss made of: CE 0.122861348092556, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/83, Loss=0.12602003142237664
Loss made of: CE 0.14658157527446747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.13489703834056854, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.13489703834056854, Class Loss=0.13489703834056854, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/83, Loss=0.10601365566253662
Loss made of: CE 0.10352921485900879, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/83, Loss=0.11811916157603264
Loss made of: CE 0.09509631991386414, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/83, Loss=0.12735037580132486
Loss made of: CE 0.12139052152633667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/83, Loss=0.12129138633608819
Loss made of: CE 0.12257860600948334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/83, Loss=0.12150447145104408
Loss made of: CE 0.1363517940044403, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/83, Loss=0.11349586024880409
Loss made of: CE 0.10670749843120575, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/83, Loss=0.10101192742586136
Loss made of: CE 0.08275822550058365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/83, Loss=0.11023439317941666
Loss made of: CE 0.10538416355848312, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.11429823189973831, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.11429823189973831, Class Loss=0.11429823189973831, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/83, Loss=0.11658534854650497
Loss made of: CE 0.0888819694519043, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/83, Loss=0.10022697448730469
Loss made of: CE 0.11853818595409393, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/83, Loss=0.09024716690182685
Loss made of: CE 0.07939790934324265, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/83, Loss=0.1010841704905033
Loss made of: CE 0.08295141905546188, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/83, Loss=0.09179420843720436
Loss made of: CE 0.10121303051710129, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/83, Loss=0.09688220098614693
Loss made of: CE 0.09924685955047607, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/83, Loss=0.10634665116667748
Loss made of: CE 0.06267301738262177, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/83, Loss=0.09332558587193489
Loss made of: CE 0.0944049283862114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.0988406389951706, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.0988406389951706, Class Loss=0.0988406389951706, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/83, Loss=0.08763068839907646
Loss made of: CE 0.07556477189064026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/83, Loss=0.08703777454793453
Loss made of: CE 0.0846729576587677, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/83, Loss=0.08468893878161907
Loss made of: CE 0.08477868139743805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/83, Loss=0.08878653571009636
Loss made of: CE 0.10974656790494919, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/83, Loss=0.08814636543393135
Loss made of: CE 0.11327292770147324, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/83, Loss=0.10347937867045402
Loss made of: CE 0.12742330133914948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/83, Loss=0.08148908168077469
Loss made of: CE 0.07244665920734406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/83, Loss=0.090398520976305
Loss made of: CE 0.0998232364654541, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08891989290714264, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.08891989290714264, Class Loss=0.08891989290714264, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/83, Loss=0.08090262524783612
Loss made of: CE 0.08146634697914124, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/83, Loss=0.10044564157724381
Loss made of: CE 0.16898874938488007, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/83, Loss=0.08532828688621522
Loss made of: CE 0.09230174869298935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/83, Loss=0.09930842369794846
Loss made of: CE 0.0952109694480896, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/83, Loss=0.08452625721693038
Loss made of: CE 0.10204964876174927, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/83, Loss=0.08542842343449593
Loss made of: CE 0.10966601222753525, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/83, Loss=0.10988617837429046
Loss made of: CE 0.11780790984630585, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/83, Loss=0.0888296939432621
Loss made of: CE 0.12010817229747772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0919007807970047, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.0919007807970047, Class Loss=0.0919007807970047, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/66, Loss=0.2798370286822319
Loss made of: CE 0.15542882680892944, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/66, Loss=0.15376766473054887
Loss made of: CE 0.1340181529521942, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/66, Loss=0.14311549365520476
Loss made of: CE 0.12733040750026703, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/66, Loss=0.14195243790745735
Loss made of: CE 0.12702971696853638, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/66, Loss=0.13995005637407304
Loss made of: CE 0.17285972833633423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/66, Loss=0.15627582296729087
Loss made of: CE 0.129970520734787, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.16765394806861877, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.16765394806861877, Class Loss=0.16765394806861877, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/66, Loss=0.12755399644374849
Loss made of: CE 0.14006957411766052, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/66, Loss=0.12356961742043496
Loss made of: CE 0.1548185646533966, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/66, Loss=0.1246128499507904
Loss made of: CE 0.09069845080375671, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/66, Loss=0.12288018167018891
Loss made of: CE 0.10705786943435669, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/66, Loss=0.11315523535013199
Loss made of: CE 0.09423881769180298, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/66, Loss=0.1250482365489006
Loss made of: CE 0.11562031507492065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.12246546894311905, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.12246546894311905, Class Loss=0.12246546894311905, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/66, Loss=0.11198198050260544
Loss made of: CE 0.09198801964521408, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/66, Loss=0.10124984383583069
Loss made of: CE 0.105710968375206, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/66, Loss=0.1173403225839138
Loss made of: CE 0.09199835360050201, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/66, Loss=0.10552681311964988
Loss made of: CE 0.08067885041236877, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/66, Loss=0.10944703742861747
Loss made of: CE 0.13878141343593597, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/66, Loss=0.09877863228321075
Loss made of: CE 0.09073799848556519, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10682444274425507, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.10682444274425507, Class Loss=0.10682444274425507, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/66, Loss=0.08574483469128609
Loss made of: CE 0.06863591074943542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/66, Loss=0.08737959265708924
Loss made of: CE 0.09257881343364716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/66, Loss=0.09915833547711372
Loss made of: CE 0.07513897120952606, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/66, Loss=0.10042149052023888
Loss made of: CE 0.08243004977703094, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/66, Loss=0.08994097672402859
Loss made of: CE 0.1084853857755661, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/66, Loss=0.11657176837325096
Loss made of: CE 0.14364707469940186, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09601371735334396, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.09601371735334396, Class Loss=0.09601371735334396, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/66, Loss=0.10423710644245147
Loss made of: CE 0.11222213506698608, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/66, Loss=0.09005403593182564
Loss made of: CE 0.07493370026350021, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/66, Loss=0.08106790408492089
Loss made of: CE 0.07826443016529083, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/66, Loss=0.08662654384970665
Loss made of: CE 0.07753713428974152, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/66, Loss=0.101967191696167
Loss made of: CE 0.1390710175037384, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/66, Loss=0.07894543036818505
Loss made of: CE 0.09346466511487961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09008766710758209, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.09008766710758209, Class Loss=0.09008766710758209, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/66, Loss=0.09499393217265606
Loss made of: CE 0.08694535493850708, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/66, Loss=0.08322888165712357
Loss made of: CE 0.07159334421157837, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/66, Loss=0.08563254177570342
Loss made of: CE 0.075099416077137, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/66, Loss=0.0788701169192791
Loss made of: CE 0.06456280499696732, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/66, Loss=0.08784912079572678
Loss made of: CE 0.08120156079530716, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/66, Loss=0.08008549511432647
Loss made of: CE 0.11604899168014526, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08413126319646835, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.08413126319646835, Class Loss=0.08413126319646835, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.17380991578102112, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.942202
Mean Acc: 0.792503
FreqW Acc: 0.895755
Mean IoU: 0.702155
Class IoU:
	class 0: 0.9320854
	class 1: 0.8534345
	class 2: 0.3258267
	class 3: 0.8444996
	class 4: 0.6952724
	class 5: 0.028819814
	class 6: 0.9186818
	class 7: 0.8343234
	class 8: 0.8864549
Class Acc:
	class 0: 0.97432846
	class 1: 0.90814894
	class 2: 0.7439227
	class 3: 0.8873922
	class 4: 0.7832946
	class 5: 0.02883751
	class 6: 0.9724858
	class 7: 0.9284891
	class 8: 0.9056233

federated global round: 2, step: 0
select part of clients to conduct local training
[0, 4, 7, 2]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/57, Loss=0.227725400775671
Loss made of: CE 0.12047003954648972, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/57, Loss=0.16162052527070045
Loss made of: CE 0.17987430095672607, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/57, Loss=0.17468500584363938
Loss made of: CE 0.1868486851453781, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/57, Loss=0.13988533467054368
Loss made of: CE 0.14515143632888794, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/57, Loss=0.150857263058424
Loss made of: CE 0.17649321258068085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.17083008587360382, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.17083008587360382, Class Loss=0.17083008587360382, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/57, Loss=0.14273675084114074
Loss made of: CE 0.13471081852912903, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/57, Loss=0.13074520975351334
Loss made of: CE 0.12982116639614105, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/57, Loss=0.12799582481384278
Loss made of: CE 0.14473244547843933, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/57, Loss=0.12132862731814384
Loss made of: CE 0.13804510235786438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/57, Loss=0.13632674962282182
Loss made of: CE 0.11942442506551743, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.13059374690055847, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.13059374690055847, Class Loss=0.13059374690055847, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/57, Loss=0.12104160338640213
Loss made of: CE 0.12309279292821884, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/57, Loss=0.10585353150963783
Loss made of: CE 0.07853720337152481, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/57, Loss=0.11684051975607872
Loss made of: CE 0.10368013381958008, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/57, Loss=0.11312428116798401
Loss made of: CE 0.1015884280204773, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/57, Loss=0.11267064586281776
Loss made of: CE 0.11020828783512115, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.11192793399095535, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.11192793399095535, Class Loss=0.11192793399095535, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/57, Loss=0.1066050574183464
Loss made of: CE 0.12712369859218597, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/57, Loss=0.09940438941121102
Loss made of: CE 0.1054973229765892, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/57, Loss=0.10220399349927903
Loss made of: CE 0.06937387585639954, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/57, Loss=0.10468432903289795
Loss made of: CE 0.1343957483768463, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/57, Loss=0.09894602559506893
Loss made of: CE 0.10492297261953354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10180754214525223, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.10180754214525223, Class Loss=0.10180754214525223, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/57, Loss=0.09814760461449623
Loss made of: CE 0.11719643324613571, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/57, Loss=0.09939862042665482
Loss made of: CE 0.10295969247817993, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/57, Loss=0.0933573417365551
Loss made of: CE 0.08825132250785828, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/57, Loss=0.08497921265661716
Loss made of: CE 0.0991826057434082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/57, Loss=0.09538982585072517
Loss made of: CE 0.104130819439888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09402401745319366, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.09402401745319366, Class Loss=0.09402401745319366, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/57, Loss=0.08456619121134282
Loss made of: CE 0.08344784379005432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/57, Loss=0.08984609022736549
Loss made of: CE 0.08778882026672363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/57, Loss=0.09108374863862992
Loss made of: CE 0.08627516031265259, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/57, Loss=0.08014601543545723
Loss made of: CE 0.06403880566358566, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/57, Loss=0.09676760137081146
Loss made of: CE 0.15066370368003845, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09028960019350052, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.09028960019350052, Class Loss=0.09028960019350052, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/60, Loss=0.1995388872921467
Loss made of: CE 0.25712788105010986, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/60, Loss=0.16747843772172927
Loss made of: CE 0.09109044075012207, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/60, Loss=0.1360232003033161
Loss made of: CE 0.11702969670295715, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/60, Loss=0.13072867095470428
Loss made of: CE 0.15348626673221588, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/60, Loss=0.14114540591835975
Loss made of: CE 0.14240214228630066, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/60, Loss=0.11560209468007088
Loss made of: CE 0.11330398172140121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.14841945469379425, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.14841945469379425, Class Loss=0.14841945469379425, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007332
Epoch 2, Batch 10/60, Loss=0.10963360741734504
Loss made of: CE 0.1149623841047287, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/60, Loss=0.11999590843915939
Loss made of: CE 0.11104634404182434, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/60, Loss=0.10785114392638206
Loss made of: CE 0.06842266023159027, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/60, Loss=0.11318111196160316
Loss made of: CE 0.11957228183746338, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/60, Loss=0.1161733664572239
Loss made of: CE 0.10858067125082016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/60, Loss=0.10867889821529389
Loss made of: CE 0.10184435546398163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.11258567869663239, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.11258567869663239, Class Loss=0.11258567869663239, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.006943
Epoch 3, Batch 10/60, Loss=0.10423641577363014
Loss made of: CE 0.09397698938846588, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/60, Loss=0.09852499663829803
Loss made of: CE 0.07121013849973679, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/60, Loss=0.09778544530272484
Loss made of: CE 0.07969307899475098, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/60, Loss=0.09373017102479934
Loss made of: CE 0.07142411172389984, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/60, Loss=0.09982893466949463
Loss made of: CE 0.0874396488070488, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/60, Loss=0.09604396969079972
Loss made of: CE 0.07361506670713425, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09835832566022873, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.09835832566022873, Class Loss=0.09835832566022873, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.006551
Epoch 4, Batch 10/60, Loss=0.09052083790302276
Loss made of: CE 0.1286173313856125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/60, Loss=0.08597997948527336
Loss made of: CE 0.06998100876808167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/60, Loss=0.0832341454923153
Loss made of: CE 0.09713703393936157, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/60, Loss=0.0833661824464798
Loss made of: CE 0.0848216712474823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/60, Loss=0.09250538274645806
Loss made of: CE 0.07772555202245712, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/60, Loss=0.087333158031106
Loss made of: CE 0.1384606957435608, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08715661615133286, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.08715661615133286, Class Loss=0.08715661615133286, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006156
Epoch 5, Batch 10/60, Loss=0.07638969011604786
Loss made of: CE 0.08034726232290268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/60, Loss=0.08946304656565189
Loss made of: CE 0.0833495482802391, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/60, Loss=0.09069539979100227
Loss made of: CE 0.08125217258930206, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/60, Loss=0.0795473575592041
Loss made of: CE 0.06474708020687103, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/60, Loss=0.09228503704071045
Loss made of: CE 0.06855820119380951, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/60, Loss=0.08688001483678817
Loss made of: CE 0.12753739953041077, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08587676286697388, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.08587676286697388, Class Loss=0.08587676286697388, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.005759
Epoch 6, Batch 10/60, Loss=0.08512862101197242
Loss made of: CE 0.05585160851478577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/60, Loss=0.08297695741057395
Loss made of: CE 0.06444866210222244, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/60, Loss=0.08641419634222984
Loss made of: CE 0.08259998261928558, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/60, Loss=0.07575413212180138
Loss made of: CE 0.06471803039312363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/60, Loss=0.08506340682506561
Loss made of: CE 0.08955832570791245, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/60, Loss=0.06790459863841533
Loss made of: CE 0.07652600109577179, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08054032176733017, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.08054032176733017, Class Loss=0.08054032176733017, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/50, Loss=0.19392853379249572
Loss made of: CE 0.2074165791273117, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/50, Loss=0.19119327291846275
Loss made of: CE 0.1486150473356247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/50, Loss=0.15055908411741256
Loss made of: CE 0.18065059185028076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/50, Loss=0.1384314067661762
Loss made of: CE 0.1373233199119568, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/50, Loss=0.13936327546834945
Loss made of: CE 0.12842245399951935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.16269512474536896, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.16269512474536896, Class Loss=0.16269512474536896, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007770
Epoch 2, Batch 10/50, Loss=0.12842777967453003
Loss made of: CE 0.12073656916618347, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/50, Loss=0.12821802198886872
Loss made of: CE 0.1368686556816101, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/50, Loss=0.10811728164553643
Loss made of: CE 0.13040128350257874, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/50, Loss=0.12258992716670036
Loss made of: CE 0.16485536098480225, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/50, Loss=0.11182567924261093
Loss made of: CE 0.13094975054264069, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1198357343673706, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.1198357343673706, Class Loss=0.1198357343673706, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.007358
Epoch 3, Batch 10/50, Loss=0.11077962517738342
Loss made of: CE 0.0777902752161026, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/50, Loss=0.11211302205920219
Loss made of: CE 0.14190945029258728, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/50, Loss=0.10083412230014802
Loss made of: CE 0.09801161289215088, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/50, Loss=0.09667358249425888
Loss made of: CE 0.08915548026561737, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/50, Loss=0.10322454273700714
Loss made of: CE 0.12454168498516083, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10472497344017029, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.10472497344017029, Class Loss=0.10472497344017029, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.006943
Epoch 4, Batch 10/50, Loss=0.08898035511374473
Loss made of: CE 0.07399813830852509, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/50, Loss=0.10112509801983834
Loss made of: CE 0.11430154740810394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/50, Loss=0.09126976057887078
Loss made of: CE 0.08829512447118759, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/50, Loss=0.0978834092617035
Loss made of: CE 0.12067397683858871, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/50, Loss=0.10026240050792694
Loss made of: CE 0.12860818207263947, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09590420871973038, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.09590420871973038, Class Loss=0.09590420871973038, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006525
Epoch 5, Batch 10/50, Loss=0.09043954461812972
Loss made of: CE 0.07085085660219193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/50, Loss=0.09921373128890991
Loss made of: CE 0.07744966447353363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/50, Loss=0.09887564554810524
Loss made of: CE 0.07532745599746704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/50, Loss=0.09887066408991814
Loss made of: CE 0.08507483452558517, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/50, Loss=0.09660335630178452
Loss made of: CE 0.10080980509519577, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09680058062076569, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.09680058062076569, Class Loss=0.09680058062076569, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006104
Epoch 6, Batch 10/50, Loss=0.09498368501663208
Loss made of: CE 0.07565777748823166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/50, Loss=0.08018835112452508
Loss made of: CE 0.07086905837059021, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/50, Loss=0.079542026668787
Loss made of: CE 0.08145402371883392, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/50, Loss=0.09096795469522476
Loss made of: CE 0.07494481652975082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/50, Loss=0.08447754569351673
Loss made of: CE 0.08322479575872421, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08603191375732422, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.08603191375732422, Class Loss=0.08603191375732422, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/66, Loss=0.16218410134315492
Loss made of: CE 0.0937461107969284, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/66, Loss=0.1033486358821392
Loss made of: CE 0.09930338710546494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/66, Loss=0.09760801941156387
Loss made of: CE 0.08361178636550903, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/66, Loss=0.10186779126524925
Loss made of: CE 0.09125910699367523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/66, Loss=0.09806917980313301
Loss made of: CE 0.08288180828094482, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/66, Loss=0.10782514289021491
Loss made of: CE 0.07715722173452377, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.11041896045207977, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.11041896045207977, Class Loss=0.11041896045207977, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.005998
Epoch 2, Batch 10/66, Loss=0.09359203614294528
Loss made of: CE 0.09242033958435059, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/66, Loss=0.09073820933699608
Loss made of: CE 0.09446742385625839, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/66, Loss=0.09064239412546157
Loss made of: CE 0.06752327084541321, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/66, Loss=0.09052814394235612
Loss made of: CE 0.07634338736534119, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/66, Loss=0.08697032779455185
Loss made of: CE 0.07279381155967712, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/66, Loss=0.08932939469814301
Loss made of: CE 0.08885635435581207, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08993548154830933, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.08993548154830933, Class Loss=0.08993548154830933, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.005679
Epoch 3, Batch 10/66, Loss=0.07500979378819465
Loss made of: CE 0.06221665441989899, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/66, Loss=0.0797152154147625
Loss made of: CE 0.07472354173660278, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/66, Loss=0.08992023095488548
Loss made of: CE 0.07496672868728638, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/66, Loss=0.07832863405346871
Loss made of: CE 0.07000420987606049, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/66, Loss=0.08434685468673705
Loss made of: CE 0.1041082963347435, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/66, Loss=0.07640784084796906
Loss made of: CE 0.06181759759783745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08001194149255753, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.08001194149255753, Class Loss=0.08001194149255753, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/66, Loss=0.07180264927446842
Loss made of: CE 0.0641421228647232, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/66, Loss=0.07107513882219792
Loss made of: CE 0.06846790015697479, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/66, Loss=0.08049837499856949
Loss made of: CE 0.05457364767789841, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/66, Loss=0.07719228863716125
Loss made of: CE 0.07727904617786407, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/66, Loss=0.07888090908527375
Loss made of: CE 0.10439687967300415, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/66, Loss=0.09500551223754883
Loss made of: CE 0.1568094789981842, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07875723391771317, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.07875723391771317, Class Loss=0.07875723391771317, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.005036
Epoch 5, Batch 10/66, Loss=0.08198407404124737
Loss made of: CE 0.08065847307443619, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/66, Loss=0.07222164608538151
Loss made of: CE 0.061419881880283356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/66, Loss=0.06880608275532722
Loss made of: CE 0.05980402231216431, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/66, Loss=0.07686517760157585
Loss made of: CE 0.05574272572994232, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/66, Loss=0.08166942745447159
Loss made of: CE 0.11723177134990692, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/66, Loss=0.06311297118663788
Loss made of: CE 0.07458088546991348, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0738908126950264, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.0738908126950264, Class Loss=0.0738908126950264, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.004711
Epoch 6, Batch 10/66, Loss=0.0721781000494957
Loss made of: CE 0.06865194439888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/66, Loss=0.06543271914124489
Loss made of: CE 0.057301681488752365, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/66, Loss=0.06963216215372085
Loss made of: CE 0.06850333511829376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/66, Loss=0.0667361281812191
Loss made of: CE 0.05222306773066521, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/66, Loss=0.07398030534386635
Loss made of: CE 0.06743236631155014, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/66, Loss=0.0749122578650713
Loss made of: CE 0.1271522343158722, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07006995379924774, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.07006995379924774, Class Loss=0.07006995379924774, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.1320953071117401, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.953162
Mean Acc: 0.855710
FreqW Acc: 0.915967
Mean IoU: 0.763380
Class IoU:
	class 0: 0.94421136
	class 1: 0.8853175
	class 2: 0.3713981
	class 3: 0.8670341
	class 4: 0.7174849
	class 5: 0.4033785
	class 6: 0.92362493
	class 7: 0.8715555
	class 8: 0.88641745
Class Acc:
	class 0: 0.9759772
	class 1: 0.92983943
	class 2: 0.84354895
	class 3: 0.887891
	class 4: 0.8456499
	class 5: 0.4096711
	class 6: 0.9719739
	class 7: 0.9324079
	class 8: 0.9044292

federated global round: 3, step: 0
select part of clients to conduct local training
[1, 9, 3, 8]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/83, Loss=0.15172927603125572
Loss made of: CE 0.09474669396877289, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/83, Loss=0.120738485455513
Loss made of: CE 0.11267207562923431, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/83, Loss=0.09090945646166801
Loss made of: CE 0.08948095142841339, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/83, Loss=0.09351747557520866
Loss made of: CE 0.11022301763296127, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/83, Loss=0.09435542449355125
Loss made of: CE 0.0747818797826767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/83, Loss=0.08426137007772923
Loss made of: CE 0.09785344451665878, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/83, Loss=0.10430301874876022
Loss made of: CE 0.08933718502521515, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/83, Loss=0.09573893994092941
Loss made of: CE 0.10572190582752228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.10514581948518753, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.10514581948518753, Class Loss=0.10514581948518753, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007137
Epoch 2, Batch 10/83, Loss=0.08456564992666245
Loss made of: CE 0.06473241746425629, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/83, Loss=0.08451379872858525
Loss made of: CE 0.06485778093338013, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/83, Loss=0.0812837801873684
Loss made of: CE 0.0730951577425003, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/83, Loss=0.08690190687775612
Loss made of: CE 0.07894940674304962, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/83, Loss=0.07945069074630737
Loss made of: CE 0.07718256115913391, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/83, Loss=0.08147542998194694
Loss made of: CE 0.059972018003463745, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/83, Loss=0.08140549287199975
Loss made of: CE 0.07352171838283539, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/83, Loss=0.08940757066011429
Loss made of: CE 0.08923124521970749, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08374438434839249, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.08374438434839249, Class Loss=0.08374438434839249, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.006551
Epoch 3, Batch 10/83, Loss=0.07494791559875011
Loss made of: CE 0.08525468409061432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/83, Loss=0.07635783702135086
Loss made of: CE 0.05926959961652756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/83, Loss=0.08452647589147091
Loss made of: CE 0.08148053288459778, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/83, Loss=0.0762625053524971
Loss made of: CE 0.07919728010892868, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/83, Loss=0.09039929434657097
Loss made of: CE 0.10044294595718384, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/83, Loss=0.08041448369622231
Loss made of: CE 0.07224273681640625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/83, Loss=0.07195533998310566
Loss made of: CE 0.0674077495932579, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/83, Loss=0.07552295364439487
Loss made of: CE 0.0874343067407608, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07880919426679611, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.07880919426679611, Class Loss=0.07880919426679611, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005958
Epoch 4, Batch 10/83, Loss=0.08602469563484191
Loss made of: CE 0.0743773803114891, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/83, Loss=0.06993349194526673
Loss made of: CE 0.08452441543340683, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/83, Loss=0.0712975099682808
Loss made of: CE 0.0645105168223381, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/83, Loss=0.06954253613948821
Loss made of: CE 0.060645610094070435, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/83, Loss=0.06499623768031597
Loss made of: CE 0.07466235756874084, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/83, Loss=0.06881161406636238
Loss made of: CE 0.07650183141231537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/83, Loss=0.08322823829948903
Loss made of: CE 0.057299934327602386, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/83, Loss=0.07237539663910866
Loss made of: CE 0.06983977556228638, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07320693880319595, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.07320693880319595, Class Loss=0.07320693880319595, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.005359
Epoch 5, Batch 10/83, Loss=0.06739592626690864
Loss made of: CE 0.06362360715866089, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/83, Loss=0.06623080894351005
Loss made of: CE 0.05454862862825394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/83, Loss=0.06620143949985505
Loss made of: CE 0.05724514275789261, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/83, Loss=0.06730375736951828
Loss made of: CE 0.07042428851127625, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/83, Loss=0.07180765606462955
Loss made of: CE 0.12529852986335754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/83, Loss=0.07689048536121845
Loss made of: CE 0.10122279822826385, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/83, Loss=0.0682903178036213
Loss made of: CE 0.07833118736743927, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/83, Loss=0.06592372544109822
Loss made of: CE 0.07356072962284088, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.06886687874794006, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.06886687874794006, Class Loss=0.06886687874794006, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.004752
Epoch 6, Batch 10/83, Loss=0.06233085803687573
Loss made of: CE 0.06723221391439438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/83, Loss=0.07646668702363968
Loss made of: CE 0.10923131555318832, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/83, Loss=0.066521006077528
Loss made of: CE 0.06178629398345947, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/83, Loss=0.0753830898553133
Loss made of: CE 0.08929882943630219, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/83, Loss=0.057400451973080635
Loss made of: CE 0.06310749053955078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/83, Loss=0.05999194644391537
Loss made of: CE 0.05854546278715134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/83, Loss=0.07213470339775085
Loss made of: CE 0.06524147093296051, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/83, Loss=0.06359718032181264
Loss made of: CE 0.09492333978414536, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06665080785751343, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.06665080785751343, Class Loss=0.06665080785751343, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/61, Loss=0.21536660343408584
Loss made of: CE 0.10086452960968018, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.15636920630931855
Loss made of: CE 0.11944564431905746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.11587533727288246
Loss made of: CE 0.15468066930770874, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.1281922549009323
Loss made of: CE 0.11325942724943161, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.11537056565284728
Loss made of: CE 0.10099035501480103, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.12665557116270065
Loss made of: CE 0.11883090436458588, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1420208215713501, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.1420208215713501, Class Loss=0.1420208215713501, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007564
Epoch 2, Batch 10/61, Loss=0.10702779814600945
Loss made of: CE 0.11515115946531296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.09552920237183571
Loss made of: CE 0.06875428557395935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.10002813935279846
Loss made of: CE 0.09026643633842468, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.09786002635955811
Loss made of: CE 0.08271317929029465, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.10561763420701027
Loss made of: CE 0.13456983864307404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.08641741015017032
Loss made of: CE 0.052811142057180405, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09860910475254059, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.09860910475254059, Class Loss=0.09860910475254059, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.006943
Epoch 3, Batch 10/61, Loss=0.08756217658519745
Loss made of: CE 0.09851686656475067, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.08098283708095551
Loss made of: CE 0.058480046689510345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.08880646526813507
Loss made of: CE 0.0691182017326355, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.07594978027045726
Loss made of: CE 0.06138923019170761, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.08244220390915871
Loss made of: CE 0.07157447189092636, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.102239228785038
Loss made of: CE 0.10214635729789734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08599550276994705, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.08599550276994705, Class Loss=0.08599550276994705, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.006314
Epoch 4, Batch 10/61, Loss=0.07913641892373562
Loss made of: CE 0.07298346608877182, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.07818499356508254
Loss made of: CE 0.0997929573059082, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.0909376110881567
Loss made of: CE 0.22416365146636963, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.0848108060657978
Loss made of: CE 0.07369047403335571, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.07369446605443955
Loss made of: CE 0.05991523712873459, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.08790591284632683
Loss made of: CE 0.08267438411712646, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.0823911726474762, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.0823911726474762, Class Loss=0.0823911726474762, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.005679
Epoch 5, Batch 10/61, Loss=0.07275302149355412
Loss made of: CE 0.0654652863740921, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.07853871658444404
Loss made of: CE 0.07260087132453918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.07207644060254097
Loss made of: CE 0.08122976869344711, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.07304657213389873
Loss made of: CE 0.05706101655960083, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.06999479420483112
Loss made of: CE 0.07612989097833633, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.08708934597671032
Loss made of: CE 0.10568223893642426, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07541254162788391, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.07541254162788391, Class Loss=0.07541254162788391, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.005036
Epoch 6, Batch 10/61, Loss=0.07598950527608395
Loss made of: CE 0.09007479250431061, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.06941749192774296
Loss made of: CE 0.05801188200712204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.0631871048361063
Loss made of: CE 0.04877399653196335, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.06938573382794858
Loss made of: CE 0.1191227063536644, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.07246333435177803
Loss made of: CE 0.05550210922956467, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.08068559616804123
Loss made of: CE 0.07756444811820984, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07199620455503464, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.07199620455503464, Class Loss=0.07199620455503464, Reg Loss=0.0
Current Client Index:  3
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/61, Loss=0.1470158390700817
Loss made of: CE 0.10937031358480453, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.10622794181108475
Loss made of: CE 0.0960950255393982, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.08502508848905563
Loss made of: CE 0.11490951478481293, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.0990455511957407
Loss made of: CE 0.09065693616867065, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.101382265239954
Loss made of: CE 0.11056359112262726, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.0895796537399292
Loss made of: CE 0.07216842472553253, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.10412607342004776, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.10412607342004776, Class Loss=0.10412607342004776, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007137
Epoch 2, Batch 10/61, Loss=0.0996931992471218
Loss made of: CE 0.12452014535665512, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.09053940176963807
Loss made of: CE 0.1162145584821701, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.08797908574342728
Loss made of: CE 0.0694885402917862, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.0793061874806881
Loss made of: CE 0.07619334757328033, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.08967130966484546
Loss made of: CE 0.06671576201915741, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.08732283897697926
Loss made of: CE 0.10653388500213623, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.0885966345667839, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.0885966345667839, Class Loss=0.0885966345667839, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.006551
Epoch 3, Batch 10/61, Loss=0.0876232661306858
Loss made of: CE 0.07459141314029694, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.08350643217563629
Loss made of: CE 0.08742483705282211, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.07907171957194806
Loss made of: CE 0.07914412021636963, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.07546823471784592
Loss made of: CE 0.07055138796567917, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.0765736684203148
Loss made of: CE 0.0703287348151207, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.07826466001570224
Loss made of: CE 0.05459992587566376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07999380677938461, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.07999380677938461, Class Loss=0.07999380677938461, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005958
Epoch 4, Batch 10/61, Loss=0.07245148420333862
Loss made of: CE 0.08427586406469345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.08844530284404754
Loss made of: CE 0.09900711476802826, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.07052419371902943
Loss made of: CE 0.06682047247886658, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.07697966434061528
Loss made of: CE 0.08259733766317368, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.07290504798293114
Loss made of: CE 0.05237016826868057, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.06950899213552475
Loss made of: CE 0.08941348642110825, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07474611699581146, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.07474611699581146, Class Loss=0.07474611699581146, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.005359
Epoch 5, Batch 10/61, Loss=0.06883809082210064
Loss made of: CE 0.06892333179712296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.07123194336891174
Loss made of: CE 0.05899517983198166, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.06967737823724747
Loss made of: CE 0.055629879236221313, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.07071955166757107
Loss made of: CE 0.05598603934049606, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.0769539088010788
Loss made of: CE 0.06019681692123413, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.07535618580877781
Loss made of: CE 0.08925096690654755, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07183337211608887, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.07183337211608887, Class Loss=0.07183337211608887, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.004752
Epoch 6, Batch 10/61, Loss=0.0697716798633337
Loss made of: CE 0.08504470437765121, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.06773104444146157
Loss made of: CE 0.07152476906776428, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.07340731658041477
Loss made of: CE 0.08335484564304352, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.06538790427148342
Loss made of: CE 0.06612960994243622, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.06867700926959515
Loss made of: CE 0.06991687417030334, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.07982851713895797
Loss made of: CE 0.050008855760097504, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0707383006811142, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.0707383006811142, Class Loss=0.0707383006811142, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/60, Loss=0.145052620023489
Loss made of: CE 0.10328038781881332, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/60, Loss=0.1261196918785572
Loss made of: CE 0.07888077199459076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/60, Loss=0.11293874308466911
Loss made of: CE 0.11331530660390854, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/60, Loss=0.11279891580343246
Loss made of: CE 0.16818079352378845, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/60, Loss=0.1256939433515072
Loss made of: CE 0.13052695989608765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/60, Loss=0.1289915196597576
Loss made of: CE 0.14061763882637024, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1252659112215042, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.1252659112215042, Class Loss=0.1252659112215042, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009247
Epoch 2, Batch 10/60, Loss=0.11410130113363266
Loss made of: CE 0.11545416712760925, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/60, Loss=0.10875832587480545
Loss made of: CE 0.09393783658742905, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/60, Loss=0.10037706345319748
Loss made of: CE 0.10063743591308594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/60, Loss=0.09974470846354962
Loss made of: CE 0.10920747369527817, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/60, Loss=0.11352095901966094
Loss made of: CE 0.08702617883682251, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/60, Loss=0.10838889628648758
Loss made of: CE 0.08729615807533264, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10748188197612762, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.10748188197612762, Class Loss=0.10748188197612762, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.008487
Epoch 3, Batch 10/60, Loss=0.08613050654530525
Loss made of: CE 0.06066036969423294, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/60, Loss=0.0871830478310585
Loss made of: CE 0.08151781558990479, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/60, Loss=0.10611102320253848
Loss made of: CE 0.11071610450744629, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/60, Loss=0.08578936010599136
Loss made of: CE 0.09441244602203369, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/60, Loss=0.10674978345632553
Loss made of: CE 0.06485752016305923, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/60, Loss=0.09904416799545288
Loss made of: CE 0.06796640157699585, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09516798704862595, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.09516798704862595, Class Loss=0.09516798704862595, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007719
Epoch 4, Batch 10/60, Loss=0.09467316195368766
Loss made of: CE 0.08630450814962387, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/60, Loss=0.08148020654916763
Loss made of: CE 0.09005579352378845, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/60, Loss=0.0880938459187746
Loss made of: CE 0.09831823408603668, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/60, Loss=0.08457038663327694
Loss made of: CE 0.1197139322757721, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/60, Loss=0.0857237696647644
Loss made of: CE 0.0918346643447876, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/60, Loss=0.0750046819448471
Loss made of: CE 0.07149666547775269, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08492434769868851, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.08492434769868851, Class Loss=0.08492434769868851, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/60, Loss=0.07581081092357636
Loss made of: CE 0.0772404745221138, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/60, Loss=0.07245888151228427
Loss made of: CE 0.07465289533138275, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/60, Loss=0.08148444145917892
Loss made of: CE 0.11217713356018066, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/60, Loss=0.08765137642621994
Loss made of: CE 0.09169864654541016, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/60, Loss=0.07492709644138813
Loss made of: CE 0.05592785403132439, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/60, Loss=0.0753161694854498
Loss made of: CE 0.11323709785938263, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07794146984815598, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.07794146984815598, Class Loss=0.07794146984815598, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006156
Epoch 6, Batch 10/60, Loss=0.08221632242202759
Loss made of: CE 0.12595134973526, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/60, Loss=0.07485682666301727
Loss made of: CE 0.05069384723901749, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/60, Loss=0.07914396524429321
Loss made of: CE 0.09662914276123047, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/60, Loss=0.06829178556799889
Loss made of: CE 0.06585633754730225, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/60, Loss=0.08128832280635834
Loss made of: CE 0.0752047523856163, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/60, Loss=0.06930546499788762
Loss made of: CE 0.0565284863114357, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07585044950246811, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.07585044950246811, Class Loss=0.07585044950246811, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.11502161622047424, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.961718
Mean Acc: 0.911077
FreqW Acc: 0.932884
Mean IoU: 0.808608
Class IoU:
	class 0: 0.9534324
	class 1: 0.8722394
	class 2: 0.36889607
	class 3: 0.8728311
	class 4: 0.700805
	class 5: 0.7726478
	class 6: 0.93952984
	class 7: 0.8734739
	class 8: 0.9236206
Class Acc:
	class 0: 0.97168916
	class 1: 0.9196183
	class 2: 0.8466574
	class 3: 0.89099056
	class 4: 0.8778245
	class 5: 0.8183908
	class 6: 0.97786516
	class 7: 0.9391298
	class 8: 0.95752925

federated global round: 4, step: 0
select part of clients to conduct local training
[5, 9, 0, 4]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/58, Loss=0.12002304270863533
Loss made of: CE 0.10805746167898178, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/58, Loss=0.10946853011846543
Loss made of: CE 0.11717360466718674, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/58, Loss=0.09783041328191758
Loss made of: CE 0.08456109464168549, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/58, Loss=0.10801380723714829
Loss made of: CE 0.20818567276000977, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/58, Loss=0.12263819053769112
Loss made of: CE 0.08734624087810516, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.11141669750213623, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.11141669750213623, Class Loss=0.11141669750213623, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.008487
Epoch 2, Batch 10/58, Loss=0.1090035803616047
Loss made of: CE 0.18923167884349823, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/58, Loss=0.09471591599285603
Loss made of: CE 0.10926499962806702, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/58, Loss=0.11251537427306176
Loss made of: CE 0.09245306253433228, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/58, Loss=0.09849530532956123
Loss made of: CE 0.08178584277629852, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/58, Loss=0.12655347138643264
Loss made of: CE 0.09143920987844467, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.11700501292943954, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.11700501292943954, Class Loss=0.11700501292943954, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.006943
Epoch 3, Batch 10/58, Loss=0.10405751913785935
Loss made of: CE 0.1529236137866974, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/58, Loss=0.11638069897890091
Loss made of: CE 0.19516555964946747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/58, Loss=0.10503647252917289
Loss made of: CE 0.07009260356426239, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/58, Loss=0.10214533358812332
Loss made of: CE 0.08931776136159897, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/58, Loss=0.0939582958817482
Loss made of: CE 0.2037537693977356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10196095705032349, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.10196095705032349, Class Loss=0.10196095705032349, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/58, Loss=0.093141695484519
Loss made of: CE 0.09697191417217255, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/58, Loss=0.07418965511023998
Loss made of: CE 0.06355346739292145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/58, Loss=0.08823297061026096
Loss made of: CE 0.10109911859035492, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/58, Loss=0.08689573332667351
Loss made of: CE 0.0672135129570961, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/58, Loss=0.08234709426760674
Loss made of: CE 0.08013521879911423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08344922959804535, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.08344922959804535, Class Loss=0.08344922959804535, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.003720
Epoch 5, Batch 10/58, Loss=0.07118465565145016
Loss made of: CE 0.06596049666404724, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/58, Loss=0.07353704646229745
Loss made of: CE 0.07214199751615524, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/58, Loss=0.07915555387735367
Loss made of: CE 0.06707058846950531, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/58, Loss=0.08485052697360515
Loss made of: CE 0.059504132717847824, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/58, Loss=0.08128545284271241
Loss made of: CE 0.06912204623222351, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07776015996932983, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.07776015996932983, Class Loss=0.07776015996932983, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.001994
Epoch 6, Batch 10/58, Loss=0.07464246153831482
Loss made of: CE 0.0615258514881134, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/58, Loss=0.07380312234163285
Loss made of: CE 0.07792791724205017, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/58, Loss=0.0706217709928751
Loss made of: CE 0.057801879942417145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/58, Loss=0.06823615320026874
Loss made of: CE 0.07654687762260437, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/58, Loss=0.07663698866963387
Loss made of: CE 0.06780380755662918, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07156375795602798, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.07156375795602798, Class Loss=0.07156375795602798, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.004384
Epoch 1, Batch 10/61, Loss=0.12672263607382775
Loss made of: CE 0.09159132093191147, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/61, Loss=0.12207950875163079
Loss made of: CE 0.09100811183452606, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/61, Loss=0.07537468709051609
Loss made of: CE 0.0619499534368515, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/61, Loss=0.08701138719916343
Loss made of: CE 0.1070604920387268, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/61, Loss=0.08339603245258331
Loss made of: CE 0.08314426243305206, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/61, Loss=0.09033258184790612
Loss made of: CE 0.08981333673000336, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.09720191359519958, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.09720191359519958, Class Loss=0.09720191359519958, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.003720
Epoch 2, Batch 10/61, Loss=0.08273337855935096
Loss made of: CE 0.08230316638946533, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/61, Loss=0.07391400225460529
Loss made of: CE 0.05515890568494797, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/61, Loss=0.07624933272600173
Loss made of: CE 0.08708717674016953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/61, Loss=0.0877820897847414
Loss made of: CE 0.0770505741238594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/61, Loss=0.09029944092035294
Loss made of: CE 0.10018324851989746, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/61, Loss=0.08048936426639557
Loss made of: CE 0.06880404055118561, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08186858147382736, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.08186858147382736, Class Loss=0.08186858147382736, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.003043
Epoch 3, Batch 10/61, Loss=0.07859333343803883
Loss made of: CE 0.08415015786886215, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/61, Loss=0.06855216808617115
Loss made of: CE 0.04453619569540024, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/61, Loss=0.07948219329118729
Loss made of: CE 0.07115785032510757, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/61, Loss=0.06599856987595558
Loss made of: CE 0.06157566234469414, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/61, Loss=0.0725153498351574
Loss made of: CE 0.060164034366607666, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/61, Loss=0.08569972589612007
Loss made of: CE 0.08887279033660889, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07477457821369171, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.07477457821369171, Class Loss=0.07477457821369171, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.002349
Epoch 4, Batch 10/61, Loss=0.07210425548255443
Loss made of: CE 0.07223808765411377, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/61, Loss=0.07201197110116482
Loss made of: CE 0.07505723834037781, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/61, Loss=0.0887385856360197
Loss made of: CE 0.2118384838104248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/61, Loss=0.07655596733093262
Loss made of: CE 0.050069503486156464, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/61, Loss=0.06641525775194168
Loss made of: CE 0.0471932590007782, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/61, Loss=0.07803624458611011
Loss made of: CE 0.07890485972166061, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07554405927658081, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.07554405927658081, Class Loss=0.07554405927658081, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.001631
Epoch 5, Batch 10/61, Loss=0.06800962500274181
Loss made of: CE 0.05927732586860657, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/61, Loss=0.07486934661865234
Loss made of: CE 0.06862957030534744, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/61, Loss=0.06297023221850395
Loss made of: CE 0.07287973165512085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/61, Loss=0.07341780811548233
Loss made of: CE 0.06486497819423676, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/61, Loss=0.06196779310703278
Loss made of: CE 0.05751625820994377, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/61, Loss=0.08002321794629097
Loss made of: CE 0.11498751491308212, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07010357826948166, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.07010357826948166, Class Loss=0.07010357826948166, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.000874
Epoch 6, Batch 10/61, Loss=0.07270539663732052
Loss made of: CE 0.08570462465286255, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/61, Loss=0.07062656469643117
Loss made of: CE 0.0664234608411789, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/61, Loss=0.058906561508774755
Loss made of: CE 0.05003208667039871, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/61, Loss=0.06494434587657452
Loss made of: CE 0.09992535412311554, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/61, Loss=0.06804294735193253
Loss made of: CE 0.05998675525188446, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/61, Loss=0.08023204281926155
Loss made of: CE 0.07276295125484467, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.069218710064888, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.069218710064888, Class Loss=0.069218710064888, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006943
Epoch 1, Batch 10/57, Loss=0.1575745902955532
Loss made of: CE 0.0814686268568039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/57, Loss=0.11250677853822708
Loss made of: CE 0.1004815548658371, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/57, Loss=0.1223466455936432
Loss made of: CE 0.1381896436214447, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/57, Loss=0.10810875967144966
Loss made of: CE 0.09068478643894196, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/57, Loss=0.09649686589837074
Loss made of: CE 0.10089485347270966, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.11968433111906052, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.11968433111906052, Class Loss=0.11968433111906052, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.005892
Epoch 2, Batch 10/57, Loss=0.09820504114031792
Loss made of: CE 0.0842411145567894, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/57, Loss=0.09651481136679649
Loss made of: CE 0.10432827472686768, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/57, Loss=0.09647870734333992
Loss made of: CE 0.10036149621009827, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/57, Loss=0.1036047764122486
Loss made of: CE 0.18963013589382172, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/57, Loss=0.09684616103768348
Loss made of: CE 0.09600468724966049, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09828853607177734, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.09828853607177734, Class Loss=0.09828853607177734, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.004820
Epoch 3, Batch 10/57, Loss=0.08445056676864623
Loss made of: CE 0.0698232352733612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/57, Loss=0.08369637168943882
Loss made of: CE 0.06527048349380493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/57, Loss=0.08723496422171592
Loss made of: CE 0.07628963887691498, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/57, Loss=0.08976848497986793
Loss made of: CE 0.08649861067533493, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/57, Loss=0.08674529790878296
Loss made of: CE 0.10237067937850952, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08514392375946045, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.08514392375946045, Class Loss=0.08514392375946045, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.003720
Epoch 4, Batch 10/57, Loss=0.08384675979614258
Loss made of: CE 0.0965016782283783, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/57, Loss=0.08250135071575641
Loss made of: CE 0.06657801568508148, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/57, Loss=0.0861026681959629
Loss made of: CE 0.05803167074918747, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/57, Loss=0.08727465681731701
Loss made of: CE 0.09481561183929443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/57, Loss=0.0892858438193798
Loss made of: CE 0.08451618999242783, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08479049056768417, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.08479049056768417, Class Loss=0.08479049056768417, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.002583
Epoch 5, Batch 10/57, Loss=0.0802028350532055
Loss made of: CE 0.09207846969366074, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/57, Loss=0.0873430922627449
Loss made of: CE 0.10171230882406235, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/57, Loss=0.08115927577018738
Loss made of: CE 0.08336928486824036, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/57, Loss=0.07252595201134682
Loss made of: CE 0.07524650543928146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/57, Loss=0.08404267355799674
Loss made of: CE 0.10580432415008545, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08148812502622604, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.08148812502622604, Class Loss=0.08148812502622604, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.001384
Epoch 6, Batch 10/57, Loss=0.07216344624757767
Loss made of: CE 0.06645964086055756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/57, Loss=0.0884102437645197
Loss made of: CE 0.08387088775634766, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/57, Loss=0.08230931982398033
Loss made of: CE 0.07309248298406601, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/57, Loss=0.07289201468229294
Loss made of: CE 0.06436830759048462, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/57, Loss=0.09561135433614254
Loss made of: CE 0.114099882543087, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08293624222278595, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.08293624222278595, Class Loss=0.08293624222278595, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.005359
Epoch 1, Batch 10/60, Loss=0.09650269001722336
Loss made of: CE 0.093225859105587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/60, Loss=0.09868875108659267
Loss made of: CE 0.06755369901657104, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/60, Loss=0.08288675919175148
Loss made of: CE 0.0679776594042778, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/60, Loss=0.08174762055277825
Loss made of: CE 0.09069319814443588, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/60, Loss=0.08790939524769784
Loss made of: CE 0.0767272412776947, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/60, Loss=0.07857422456145287
Loss made of: CE 0.0684160590171814, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.08771824836730957, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.08771824836730957, Class Loss=0.08771824836730957, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.004548
Epoch 2, Batch 10/60, Loss=0.07500101923942566
Loss made of: CE 0.06202120706439018, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/60, Loss=0.08879446461796761
Loss made of: CE 0.08654341101646423, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/60, Loss=0.07513712719082832
Loss made of: CE 0.052009761333465576, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/60, Loss=0.0824886303395033
Loss made of: CE 0.08075708150863647, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/60, Loss=0.0839643083512783
Loss made of: CE 0.07742103189229965, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/60, Loss=0.07623765543103218
Loss made of: CE 0.06420180201530457, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08027054369449615, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.08027054369449615, Class Loss=0.08027054369449615, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.003720
Epoch 3, Batch 10/60, Loss=0.08259512707591057
Loss made of: CE 0.07308465987443924, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/60, Loss=0.07005710303783416
Loss made of: CE 0.05439651757478714, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/60, Loss=0.07356706224381923
Loss made of: CE 0.053193747997283936, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/60, Loss=0.07066997699439526
Loss made of: CE 0.0759868323802948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/60, Loss=0.07492838129401207
Loss made of: CE 0.07954791188240051, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/60, Loss=0.07711452059447765
Loss made of: CE 0.06907153874635696, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07482203096151352, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.07482203096151352, Class Loss=0.07482203096151352, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.002872
Epoch 4, Batch 10/60, Loss=0.07133847661316395
Loss made of: CE 0.10964249074459076, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/60, Loss=0.06843496523797513
Loss made of: CE 0.055432260036468506, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/60, Loss=0.07119598723948002
Loss made of: CE 0.08020970225334167, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/60, Loss=0.06522507220506668
Loss made of: CE 0.06400992721319199, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/60, Loss=0.07507769353687763
Loss made of: CE 0.05839700624346733, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/60, Loss=0.07088264562189579
Loss made of: CE 0.11743056774139404, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07035914808511734, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.07035914808511734, Class Loss=0.07035914808511734, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.001994
Epoch 5, Batch 10/60, Loss=0.06690494157373905
Loss made of: CE 0.05726880952715874, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/60, Loss=0.07066636607050895
Loss made of: CE 0.07026408612728119, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/60, Loss=0.07043899409472942
Loss made of: CE 0.057780638337135315, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/60, Loss=0.06556904837489128
Loss made of: CE 0.05791490525007248, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/60, Loss=0.07615312710404396
Loss made of: CE 0.05416778475046158, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/60, Loss=0.07295789755880833
Loss made of: CE 0.11098645627498627, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07044839859008789, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.07044839859008789, Class Loss=0.07044839859008789, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.001068
Epoch 6, Batch 10/60, Loss=0.07421375401318073
Loss made of: CE 0.04479178786277771, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/60, Loss=0.07122910991311074
Loss made of: CE 0.05293542146682739, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/60, Loss=0.06990736797451973
Loss made of: CE 0.06449258327484131, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/60, Loss=0.0677597712725401
Loss made of: CE 0.057639628648757935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/60, Loss=0.07240894138813019
Loss made of: CE 0.08662991225719452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/60, Loss=0.06350254788994789
Loss made of: CE 0.08058324456214905, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06983692198991776, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.06983692198991776, Class Loss=0.06983692198991776, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.11985168606042862, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.960495
Mean Acc: 0.917198
FreqW Acc: 0.930468
Mean IoU: 0.809589
Class IoU:
	class 0: 0.95145637
	class 1: 0.89248437
	class 2: 0.38144636
	class 3: 0.82845116
	class 4: 0.7165798
	class 5: 0.8009295
	class 6: 0.9376719
	class 7: 0.8720975
	class 8: 0.9051812
Class Acc:
	class 0: 0.97057354
	class 1: 0.95049864
	class 2: 0.8844503
	class 3: 0.8407339
	class 4: 0.87501675
	class 5: 0.88057727
	class 6: 0.98002476
	class 7: 0.94420916
	class 8: 0.92869526

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[5, 11, 7, 1]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/27, Loss=25.138544809818267
Loss made of: CE 0.6727415323257446, LKD 0.7186292409896851, LDE 0.0, LReg 0.0, POD 20.297412872314453 EntMin 0.0
Epoch 1, Batch 20/27, Loss=21.103374442458154
Loss made of: CE 0.39673298597335815, LKD 1.2717936038970947, LDE 0.0, LReg 0.0, POD 19.138858795166016 EntMin 0.0
Epoch 1, Class Loss=0.668190598487854, Reg Loss=1.215545892715454
Clinet index 5, End of Epoch 1/6, Average Loss=1.883736491203308, Class Loss=0.668190598487854, Reg Loss=1.215545892715454
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=19.016299188137054
Loss made of: CE 0.41653189063072205, LKD 1.3539035320281982, LDE 0.0, LReg 0.0, POD 17.179716110229492 EntMin 0.0
Epoch 2, Batch 20/27, Loss=17.997967731952667
Loss made of: CE 0.3327244520187378, LKD 0.769586443901062, LDE 0.0, LReg 0.0, POD 15.833904266357422 EntMin 0.0
Epoch 2, Class Loss=0.42542022466659546, Reg Loss=0.8529006242752075
Clinet index 5, End of Epoch 2/6, Average Loss=1.2783207893371582, Class Loss=0.42542022466659546, Reg Loss=0.8529006242752075
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=17.592414382100106
Loss made of: CE 0.4652162790298462, LKD 0.8523780107498169, LDE 0.0, LReg 0.0, POD 16.36502456665039 EntMin 0.0
Epoch 3, Batch 20/27, Loss=16.888888269662857
Loss made of: CE 0.3222637176513672, LKD 0.8233045339584351, LDE 0.0, LReg 0.0, POD 15.390350341796875 EntMin 0.0
Epoch 3, Class Loss=0.39502277970314026, Reg Loss=0.8835979700088501
Clinet index 5, End of Epoch 3/6, Average Loss=1.278620719909668, Class Loss=0.39502277970314026, Reg Loss=0.8835979700088501
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=16.746247959136962
Loss made of: CE 0.28593164682388306, LKD 0.8033501505851746, LDE 0.0, LReg 0.0, POD 16.978342056274414 EntMin 0.0
Epoch 4, Batch 20/27, Loss=15.871731823682785
Loss made of: CE 0.3998715579509735, LKD 0.8536931276321411, LDE 0.0, LReg 0.0, POD 14.415413856506348 EntMin 0.0
Epoch 4, Class Loss=0.3634189963340759, Reg Loss=0.8015028238296509
Clinet index 5, End of Epoch 4/6, Average Loss=1.164921760559082, Class Loss=0.3634189963340759, Reg Loss=0.8015028238296509
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=15.996847069263458
Loss made of: CE 0.32927370071411133, LKD 0.6745082139968872, LDE 0.0, LReg 0.0, POD 15.508909225463867 EntMin 0.0
Epoch 5, Batch 20/27, Loss=15.445855695009232
Loss made of: CE 0.36145395040512085, LKD 0.7065269351005554, LDE 0.0, LReg 0.0, POD 14.197080612182617 EntMin 0.0
Epoch 5, Class Loss=0.34008902311325073, Reg Loss=0.7784979343414307
Clinet index 5, End of Epoch 5/6, Average Loss=1.1185870170593262, Class Loss=0.34008902311325073, Reg Loss=0.7784979343414307
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=15.673574176430701
Loss made of: CE 0.362649142742157, LKD 1.0602374076843262, LDE 0.0, LReg 0.0, POD 14.34610652923584 EntMin 0.0
Epoch 6, Batch 20/27, Loss=15.285311302542686
Loss made of: CE 0.31084033846855164, LKD 0.4767029285430908, LDE 0.0, LReg 0.0, POD 13.568347930908203 EntMin 0.0
Epoch 6, Class Loss=0.3224354088306427, Reg Loss=0.7708132266998291
Clinet index 5, End of Epoch 6/6, Average Loss=1.0932486057281494, Class Loss=0.3224354088306427, Reg Loss=0.7708132266998291
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.077347755432129, Reg Loss=2.068861484527588
Clinet index 11, End of Epoch 1/6, Average Loss=3.146209239959717, Class Loss=1.077347755432129, Reg Loss=2.068861484527588
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.8016293048858643, Reg Loss=1.129049301147461
Clinet index 11, End of Epoch 2/6, Average Loss=1.9306786060333252, Class Loss=0.8016293048858643, Reg Loss=1.129049301147461
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6568572521209717, Reg Loss=0.9188523292541504
Clinet index 11, End of Epoch 3/6, Average Loss=1.575709581375122, Class Loss=0.6568572521209717, Reg Loss=0.9188523292541504
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5679597854614258, Reg Loss=0.8849049806594849
Clinet index 11, End of Epoch 4/6, Average Loss=1.4528647661209106, Class Loss=0.5679597854614258, Reg Loss=0.8849049806594849
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.48343998193740845, Reg Loss=0.7660812735557556
Clinet index 11, End of Epoch 5/6, Average Loss=1.249521255493164, Class Loss=0.48343998193740845, Reg Loss=0.7660812735557556
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.4335777759552002, Reg Loss=0.7910711765289307
Clinet index 11, End of Epoch 6/6, Average Loss=1.2246489524841309, Class Loss=0.4335777759552002, Reg Loss=0.7910711765289307
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/27, Loss=25.008490467071532
Loss made of: CE 0.703744113445282, LKD 0.7853370904922485, LDE 0.0, LReg 0.0, POD 21.287517547607422 EntMin 0.0
Epoch 1, Batch 20/27, Loss=21.035140568017958
Loss made of: CE 0.5280865430831909, LKD 0.6975233554840088, LDE 0.0, LReg 0.0, POD 18.458837509155273 EntMin 0.0
Epoch 1, Class Loss=0.6596789360046387, Reg Loss=1.1804665327072144
Clinet index 7, End of Epoch 1/6, Average Loss=1.840145468711853, Class Loss=0.6596789360046387, Reg Loss=1.1804665327072144
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=19.204778063297272
Loss made of: CE 0.6331067085266113, LKD 1.2176382541656494, LDE 0.0, LReg 0.0, POD 19.247949600219727 EntMin 0.0
Epoch 2, Batch 20/27, Loss=18.39346488416195
Loss made of: CE 0.48065075278282166, LKD 0.8913097381591797, LDE 0.0, LReg 0.0, POD 18.34377098083496 EntMin 0.0
Epoch 2, Class Loss=0.43610480427742004, Reg Loss=0.9452254176139832
Clinet index 7, End of Epoch 2/6, Average Loss=1.3813302516937256, Class Loss=0.43610480427742004, Reg Loss=0.9452254176139832
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=17.0499821215868
Loss made of: CE 0.4586496949195862, LKD 0.6262507438659668, LDE 0.0, LReg 0.0, POD 15.740875244140625 EntMin 0.0
Epoch 3, Batch 20/27, Loss=17.280992579460143
Loss made of: CE 0.3764122724533081, LKD 0.8787433505058289, LDE 0.0, LReg 0.0, POD 15.839761734008789 EntMin 0.0
Epoch 3, Class Loss=0.40706998109817505, Reg Loss=0.9372288584709167
Clinet index 7, End of Epoch 3/6, Average Loss=1.3442988395690918, Class Loss=0.40706998109817505, Reg Loss=0.9372288584709167
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=16.25451873242855
Loss made of: CE 0.3233485817909241, LKD 0.6571171879768372, LDE 0.0, LReg 0.0, POD 14.778200149536133 EntMin 0.0
Epoch 4, Batch 20/27, Loss=16.510780438780785
Loss made of: CE 0.5749958157539368, LKD 1.4522945880889893, LDE 0.0, LReg 0.0, POD 15.043628692626953 EntMin 0.0
Epoch 4, Class Loss=0.3738115727901459, Reg Loss=0.9056496620178223
Clinet index 7, End of Epoch 4/6, Average Loss=1.2794612646102905, Class Loss=0.3738115727901459, Reg Loss=0.9056496620178223
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=16.10349332392216
Loss made of: CE 0.40428242087364197, LKD 0.8447958827018738, LDE 0.0, LReg 0.0, POD 14.035469055175781 EntMin 0.0
Epoch 5, Batch 20/27, Loss=15.421308624744416
Loss made of: CE 0.2329036295413971, LKD 0.9057106375694275, LDE 0.0, LReg 0.0, POD 13.83076286315918 EntMin 0.0
Epoch 5, Class Loss=0.3585473299026489, Reg Loss=0.8683047294616699
Clinet index 7, End of Epoch 5/6, Average Loss=1.2268520593643188, Class Loss=0.3585473299026489, Reg Loss=0.8683047294616699
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=15.342188167572022
Loss made of: CE 0.40556037425994873, LKD 1.0302855968475342, LDE 0.0, LReg 0.0, POD 14.174636840820312 EntMin 0.0
Epoch 6, Batch 20/27, Loss=16.17160206735134
Loss made of: CE 0.25428351759910583, LKD 0.9528238773345947, LDE 0.0, LReg 0.0, POD 14.842463493347168 EntMin 0.0
Epoch 6, Class Loss=0.33611711859703064, Reg Loss=0.8349334001541138
Clinet index 7, End of Epoch 6/6, Average Loss=1.1710505485534668, Class Loss=0.33611711859703064, Reg Loss=0.8349334001541138
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=24.677039438486098
Loss made of: CE 0.6291782855987549, LKD 0.6072975397109985, LDE 0.0, LReg 0.0, POD 20.000507354736328 EntMin 0.0
Epoch 1, Batch 20/27, Loss=21.27328948378563
Loss made of: CE 0.482114315032959, LKD 0.9522747993469238, LDE 0.0, LReg 0.0, POD 19.422908782958984 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.6332713961601257, Reg Loss=1.1773464679718018
Clinet index 1, End of Epoch 1/6, Average Loss=1.8106179237365723, Class Loss=0.6332713961601257, Reg Loss=1.1773464679718018
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=18.647446778416633
Loss made of: CE 0.3453296422958374, LKD 0.638178825378418, LDE 0.0, LReg 0.0, POD 17.464797973632812 EntMin 0.0
Epoch 2, Batch 20/27, Loss=18.47769241929054
Loss made of: CE 0.42125916481018066, LKD 0.684847354888916, LDE 0.0, LReg 0.0, POD 16.82550621032715 EntMin 0.0
Epoch 2, Class Loss=0.4249902069568634, Reg Loss=0.9281597137451172
Clinet index 1, End of Epoch 2/6, Average Loss=1.3531498908996582, Class Loss=0.4249902069568634, Reg Loss=0.9281597137451172
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=17.005802154541016
Loss made of: CE 0.3718406558036804, LKD 0.6712039709091187, LDE 0.0, LReg 0.0, POD 15.56374740600586 EntMin 0.0
Epoch 3, Batch 20/27, Loss=16.953899899125098
Loss made of: CE 0.36929017305374146, LKD 0.818254828453064, LDE 0.0, LReg 0.0, POD 14.916730880737305 EntMin 0.0
Epoch 3, Class Loss=0.3984695374965668, Reg Loss=0.9516469240188599
Clinet index 1, End of Epoch 3/6, Average Loss=1.350116491317749, Class Loss=0.3984695374965668, Reg Loss=0.9516469240188599
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=16.490232676267624
Loss made of: CE 0.4135891795158386, LKD 0.8577051162719727, LDE 0.0, LReg 0.0, POD 14.51473617553711 EntMin 0.0
Epoch 4, Batch 20/27, Loss=16.256658758223058
Loss made of: CE 0.3420727252960205, LKD 0.4795404076576233, LDE 0.0, LReg 0.0, POD 16.08210563659668 EntMin 0.0
Epoch 4, Class Loss=0.3690303862094879, Reg Loss=0.90920090675354
Clinet index 1, End of Epoch 4/6, Average Loss=1.2782312631607056, Class Loss=0.3690303862094879, Reg Loss=0.90920090675354
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=15.775401255488395
Loss made of: CE 0.3304373621940613, LKD 0.610177755355835, LDE 0.0, LReg 0.0, POD 14.466936111450195 EntMin 0.0
Epoch 5, Batch 20/27, Loss=15.621612030267716
Loss made of: CE 0.26855725049972534, LKD 0.6473333835601807, LDE 0.0, LReg 0.0, POD 14.468679428100586 EntMin 0.0
Epoch 5, Class Loss=0.3533743917942047, Reg Loss=0.833259105682373
Clinet index 1, End of Epoch 5/6, Average Loss=1.1866334676742554, Class Loss=0.3533743917942047, Reg Loss=0.833259105682373
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=15.46962029337883
Loss made of: CE 0.3353908956050873, LKD 0.49812209606170654, LDE 0.0, LReg 0.0, POD 14.384872436523438 EntMin 0.0
Epoch 6, Batch 20/27, Loss=15.259560023248195
Loss made of: CE 0.406521737575531, LKD 2.1263246536254883, LDE 0.0, LReg 0.0, POD 14.099895477294922 EntMin 0.0
Epoch 6, Class Loss=0.33486801385879517, Reg Loss=0.8036724925041199
Clinet index 1, End of Epoch 6/6, Average Loss=1.138540506362915, Class Loss=0.33486801385879517, Reg Loss=0.8036724925041199
federated aggregation...
Validation, Class Loss=0.3144928514957428, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923369
Mean Acc: 0.724971
FreqW Acc: 0.860220
Mean IoU: 0.639664
Class IoU:
	class 0: 0.91058975
	class 1: 0.87918985
	class 2: 0.35139632
	class 3: 0.8107142
	class 4: 0.717243
	class 5: 0.77872354
	class 6: 0.8383404
	class 7: 0.8688799
	class 8: 0.8812253
	class 9: 0.0
	class 10: 0.0
Class Acc:
	class 0: 0.9785457
	class 1: 0.92122734
	class 2: 0.9284314
	class 3: 0.82415116
	class 4: 0.8259732
	class 5: 0.81976616
	class 6: 0.84463465
	class 7: 0.90323025
	class 8: 0.92871976
	class 9: 0.0
	class 10: 0.0

federated global round: 6, step: 1
select part of clients to conduct local training
[1, 6, 7, 3]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=15.21609336733818
Loss made of: CE 0.26960545778274536, LKD 0.5870722532272339, LDE 0.0, LReg 0.0, POD 14.184314727783203 EntMin 0.0
Epoch 1, Batch 20/27, Loss=14.875009682774543
Loss made of: CE 0.3490382134914398, LKD 0.7894114851951599, LDE 0.0, LReg 0.0, POD 13.933446884155273 EntMin 0.0
Epoch 1, Class Loss=0.3524296283721924, Reg Loss=0.8048338294029236
Clinet index 1, End of Epoch 1/6, Average Loss=1.1572635173797607, Class Loss=0.3524296283721924, Reg Loss=0.8048338294029236
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=14.63999414741993
Loss made of: CE 0.3073677718639374, LKD 0.5314435958862305, LDE 0.0, LReg 0.0, POD 14.24248218536377 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/27, Loss=15.079069882631302
Loss made of: CE 0.3063596487045288, LKD 0.5480067729949951, LDE 0.0, LReg 0.0, POD 13.73580265045166 EntMin 0.0
Epoch 2, Class Loss=0.33688661456108093, Reg Loss=0.8082354068756104
Clinet index 1, End of Epoch 2/6, Average Loss=1.1451220512390137, Class Loss=0.33688661456108093, Reg Loss=0.8082354068756104
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=14.556412535905839
Loss made of: CE 0.28116029500961304, LKD 0.6928246021270752, LDE 0.0, LReg 0.0, POD 12.913002967834473 EntMin 0.0
Epoch 3, Batch 20/27, Loss=14.551053151488304
Loss made of: CE 0.32235777378082275, LKD 0.744565486907959, LDE 0.0, LReg 0.0, POD 13.105364799499512 EntMin 0.0
Epoch 3, Class Loss=0.3256154954433441, Reg Loss=0.8050132989883423
Clinet index 1, End of Epoch 3/6, Average Loss=1.1306288242340088, Class Loss=0.3256154954433441, Reg Loss=0.8050132989883423
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=14.591722133755685
Loss made of: CE 0.328093022108078, LKD 0.6958020925521851, LDE 0.0, LReg 0.0, POD 13.088269233703613 EntMin 0.0
Epoch 4, Batch 20/27, Loss=14.484812106192113
Loss made of: CE 0.3018832504749298, LKD 0.4421304166316986, LDE 0.0, LReg 0.0, POD 13.819355010986328 EntMin 0.0
Epoch 4, Class Loss=0.31443625688552856, Reg Loss=0.7790520787239075
Clinet index 1, End of Epoch 4/6, Average Loss=1.093488335609436, Class Loss=0.31443625688552856, Reg Loss=0.7790520787239075
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=14.233620741963387
Loss made of: CE 0.29746922850608826, LKD 0.6740850806236267, LDE 0.0, LReg 0.0, POD 12.977718353271484 EntMin 0.0
Epoch 5, Batch 20/27, Loss=14.250651323795319
Loss made of: CE 0.23951661586761475, LKD 0.7089888453483582, LDE 0.0, LReg 0.0, POD 13.2516508102417 EntMin 0.0
Epoch 5, Class Loss=0.311014860868454, Reg Loss=0.7801979184150696
Clinet index 1, End of Epoch 5/6, Average Loss=1.0912127494812012, Class Loss=0.311014860868454, Reg Loss=0.7801979184150696
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=13.930502872169019
Loss made of: CE 0.3000779151916504, LKD 0.40771353244781494, LDE 0.0, LReg 0.0, POD 12.68586540222168 EntMin 0.0
Epoch 6, Batch 20/27, Loss=13.919657680392266
Loss made of: CE 0.39723309874534607, LKD 1.8659851551055908, LDE 0.0, LReg 0.0, POD 12.894184112548828 EntMin 0.0
Epoch 6, Class Loss=0.3034266531467438, Reg Loss=0.7523778080940247
Clinet index 1, End of Epoch 6/6, Average Loss=1.0558044910430908, Class Loss=0.3034266531467438, Reg Loss=0.7523778080940247
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=15.280376222729682
Loss made of: CE 0.3402610719203949, LKD 1.0697846412658691, LDE 0.0, LReg 0.0, POD 13.470169067382812 EntMin 0.0
Epoch 1, Batch 20/27, Loss=14.923487895727158
Loss made of: CE 0.276386022567749, LKD 0.9331416487693787, LDE 0.0, LReg 0.0, POD 14.085613250732422 EntMin 0.0
Epoch 1, Class Loss=0.3656825125217438, Reg Loss=0.8321912884712219
Clinet index 6, End of Epoch 1/6, Average Loss=1.197873830795288, Class Loss=0.3656825125217438, Reg Loss=0.8321912884712219
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/27, Loss=15.158806443214417
Loss made of: CE 0.2992022633552551, LKD 0.8401839733123779, LDE 0.0, LReg 0.0, POD 14.696701049804688 EntMin 0.0
Epoch 2, Batch 20/27, Loss=15.102565011382103
Loss made of: CE 0.3665028214454651, LKD 0.8179558515548706, LDE 0.0, LReg 0.0, POD 13.64076042175293 EntMin 0.0
Epoch 2, Class Loss=0.3423244059085846, Reg Loss=0.8761234283447266
Clinet index 6, End of Epoch 2/6, Average Loss=1.2184478044509888, Class Loss=0.3423244059085846, Reg Loss=0.8761234283447266
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/27, Loss=14.738184928894043
Loss made of: CE 0.3305285573005676, LKD 0.5474255084991455, LDE 0.0, LReg 0.0, POD 14.039361000061035 EntMin 0.0
Epoch 3, Batch 20/27, Loss=14.832371640205384
Loss made of: CE 0.3425068259239197, LKD 0.8289610743522644, LDE 0.0, LReg 0.0, POD 13.295205116271973 EntMin 0.0
Epoch 3, Class Loss=0.3300955891609192, Reg Loss=0.8336760997772217
Clinet index 6, End of Epoch 3/6, Average Loss=1.163771629333496, Class Loss=0.3300955891609192, Reg Loss=0.8336760997772217
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/27, Loss=14.86711919605732
Loss made of: CE 0.4070868492126465, LKD 1.3617002964019775, LDE 0.0, LReg 0.0, POD 12.756017684936523 EntMin 0.0
Epoch 4, Batch 20/27, Loss=14.509473310410977
Loss made of: CE 0.24283866584300995, LKD 0.7095890045166016, LDE 0.0, LReg 0.0, POD 13.405174255371094 EntMin 0.0
Epoch 4, Class Loss=0.32284414768218994, Reg Loss=0.8193055391311646
Clinet index 6, End of Epoch 4/6, Average Loss=1.1421496868133545, Class Loss=0.32284414768218994, Reg Loss=0.8193055391311646
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/27, Loss=15.000936782360077
Loss made of: CE 0.348647803068161, LKD 1.0265549421310425, LDE 0.0, LReg 0.0, POD 13.170865058898926 EntMin 0.0
Epoch 5, Batch 20/27, Loss=14.352492389082908
Loss made of: CE 0.21449574828147888, LKD 0.7832416296005249, LDE 0.0, LReg 0.0, POD 13.158247947692871 EntMin 0.0
Epoch 5, Class Loss=0.30927586555480957, Reg Loss=0.8288324475288391
Clinet index 6, End of Epoch 5/6, Average Loss=1.138108253479004, Class Loss=0.30927586555480957, Reg Loss=0.8288324475288391
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/27, Loss=14.469444258511066
Loss made of: CE 0.261333703994751, LKD 0.819930911064148, LDE 0.0, LReg 0.0, POD 13.215843200683594 EntMin 0.0
Epoch 6, Batch 20/27, Loss=14.20877169072628
Loss made of: CE 0.2463933527469635, LKD 0.8176209330558777, LDE 0.0, LReg 0.0, POD 12.472991943359375 EntMin 0.0
Epoch 6, Class Loss=0.3066781163215637, Reg Loss=0.7924086451530457
Clinet index 6, End of Epoch 6/6, Average Loss=1.0990867614746094, Class Loss=0.3066781163215637, Reg Loss=0.7924086451530457
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=15.240753743052483
Loss made of: CE 0.3304528594017029, LKD 0.6238979697227478, LDE 0.0, LReg 0.0, POD 14.069878578186035 EntMin 0.0
Epoch 1, Batch 20/27, Loss=14.813444530963897
Loss made of: CE 0.3694443702697754, LKD 0.634997546672821, LDE 0.0, LReg 0.0, POD 13.206840515136719 EntMin 0.0
Epoch 1, Class Loss=0.35847756266593933, Reg Loss=0.8352115154266357
Clinet index 7, End of Epoch 1/6, Average Loss=1.1936891078948975, Class Loss=0.35847756266593933, Reg Loss=0.8352115154266357
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=15.246125197410583
Loss made of: CE 0.5340573191642761, LKD 1.1797831058502197, LDE 0.0, LReg 0.0, POD 15.695667266845703 EntMin 0.0
Epoch 2, Batch 20/27, Loss=14.871857777237892
Loss made of: CE 0.38797783851623535, LKD 0.5462988615036011, LDE 0.0, LReg 0.0, POD 15.04002571105957 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.34085729718208313, Reg Loss=0.8239659070968628
Clinet index 7, End of Epoch 2/6, Average Loss=1.1648231744766235, Class Loss=0.34085729718208313, Reg Loss=0.8239659070968628
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=14.601765036582947
Loss made of: CE 0.3429493308067322, LKD 0.7699512839317322, LDE 0.0, LReg 0.0, POD 13.564691543579102 EntMin 0.0
Epoch 3, Batch 20/27, Loss=14.867415031790733
Loss made of: CE 0.35206693410873413, LKD 0.7315231561660767, LDE 0.0, LReg 0.0, POD 13.21605110168457 EntMin 0.0
Epoch 3, Class Loss=0.33914434909820557, Reg Loss=0.8348524570465088
Clinet index 7, End of Epoch 3/6, Average Loss=1.1739968061447144, Class Loss=0.33914434909820557, Reg Loss=0.8348524570465088
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=14.415802201628685
Loss made of: CE 0.28041404485702515, LKD 0.5700581073760986, LDE 0.0, LReg 0.0, POD 13.593666076660156 EntMin 0.0
Epoch 4, Batch 20/27, Loss=14.833743178844452
Loss made of: CE 0.45838794112205505, LKD 1.104385495185852, LDE 0.0, LReg 0.0, POD 13.710318565368652 EntMin 0.0
Epoch 4, Class Loss=0.31760475039482117, Reg Loss=0.8311069011688232
Clinet index 7, End of Epoch 4/6, Average Loss=1.1487116813659668, Class Loss=0.31760475039482117, Reg Loss=0.8311069011688232
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=14.50374813079834
Loss made of: CE 0.3077767491340637, LKD 0.7640228271484375, LDE 0.0, LReg 0.0, POD 13.228877067565918 EntMin 0.0
Epoch 5, Batch 20/27, Loss=13.961727806925774
Loss made of: CE 0.21413254737854004, LKD 0.8081983327865601, LDE 0.0, LReg 0.0, POD 12.999090194702148 EntMin 0.0
Epoch 5, Class Loss=0.32030531764030457, Reg Loss=0.7995789051055908
Clinet index 7, End of Epoch 5/6, Average Loss=1.1198842525482178, Class Loss=0.32030531764030457, Reg Loss=0.7995789051055908
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=14.472444273531437
Loss made of: CE 0.39611098170280457, LKD 0.8803698420524597, LDE 0.0, LReg 0.0, POD 13.46340560913086 EntMin 0.0
Epoch 6, Batch 20/27, Loss=14.83529260903597
Loss made of: CE 0.24544589221477509, LKD 0.7548432350158691, LDE 0.0, LReg 0.0, POD 13.744362831115723 EntMin 0.0
Epoch 6, Class Loss=0.3111478388309479, Reg Loss=0.801161527633667
Clinet index 7, End of Epoch 6/6, Average Loss=1.1123093366622925, Class Loss=0.3111478388309479, Reg Loss=0.801161527633667
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2898409366607666, Reg Loss=1.0189218521118164
Clinet index 3, End of Epoch 1/6, Average Loss=2.308762788772583, Class Loss=1.2898409366607666, Reg Loss=1.0189218521118164
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=1.1761709451675415, Reg Loss=0.9533018469810486
Clinet index 3, End of Epoch 2/6, Average Loss=2.1294727325439453, Class Loss=1.1761709451675415, Reg Loss=0.9533018469810486
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=1.0157328844070435, Reg Loss=0.868991494178772
Clinet index 3, End of Epoch 3/6, Average Loss=1.8847243785858154, Class Loss=1.0157328844070435, Reg Loss=0.868991494178772
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.871263861656189, Reg Loss=0.9208189845085144
Clinet index 3, End of Epoch 4/6, Average Loss=1.7920827865600586, Class Loss=0.871263861656189, Reg Loss=0.9208189845085144
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7314161658287048, Reg Loss=0.8986163139343262
Clinet index 3, End of Epoch 5/6, Average Loss=1.6300325393676758, Class Loss=0.7314161658287048, Reg Loss=0.8986163139343262
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.556227445602417, Reg Loss=0.8627025485038757
Clinet index 3, End of Epoch 6/6, Average Loss=1.4189300537109375, Class Loss=0.556227445602417, Reg Loss=0.8627025485038757
federated aggregation...
Validation, Class Loss=0.3058936595916748, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.925076
Mean Acc: 0.734196
FreqW Acc: 0.863932
Mean IoU: 0.647681
Class IoU:
	class 0: 0.9124658
	class 1: 0.8810333
	class 2: 0.35893595
	class 3: 0.78939337
	class 4: 0.7163538
	class 5: 0.7810283
	class 6: 0.8756429
	class 7: 0.8769009
	class 8: 0.88805234
	class 9: 0.044682577
	class 10: 0.0
Class Acc:
	class 0: 0.97737795
	class 1: 0.923727
	class 2: 0.920918
	class 3: 0.7999916
	class 4: 0.8453312
	class 5: 0.82257843
	class 6: 0.88641506
	class 7: 0.9210964
	class 8: 0.9309326
	class 9: 0.047783908
	class 10: 0.0

federated global round: 7, step: 1
select part of clients to conduct local training
[13, 8, 0, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.3240768909454346, Reg Loss=0.9009446501731873
Clinet index 13, End of Epoch 1/6, Average Loss=2.2250216007232666, Class Loss=1.3240768909454346, Reg Loss=0.9009446501731873
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.2473065853118896, Reg Loss=0.8529769778251648
Clinet index 13, End of Epoch 2/6, Average Loss=2.100283622741699, Class Loss=1.2473065853118896, Reg Loss=0.8529769778251648
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=1.0484912395477295, Reg Loss=0.8807751536369324
Clinet index 13, End of Epoch 3/6, Average Loss=1.9292664527893066, Class Loss=1.0484912395477295, Reg Loss=0.8807751536369324
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.9426903128623962, Reg Loss=0.8282512426376343
Clinet index 13, End of Epoch 4/6, Average Loss=1.7709414958953857, Class Loss=0.9426903128623962, Reg Loss=0.8282512426376343
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.7710649967193604, Reg Loss=0.7733989953994751
Clinet index 13, End of Epoch 5/6, Average Loss=1.5444639921188354, Class Loss=0.7710649967193604, Reg Loss=0.7733989953994751
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.7000484466552734, Reg Loss=0.8367620706558228
Clinet index 13, End of Epoch 6/6, Average Loss=1.5368105173110962, Class Loss=0.7000484466552734, Reg Loss=0.8367620706558228
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.5118467807769775, Reg Loss=1.092020034790039
Clinet index 8, End of Epoch 1/6, Average Loss=2.6038668155670166, Class Loss=1.5118467807769775, Reg Loss=1.092020034790039
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.285717248916626, Reg Loss=0.8414250612258911
Clinet index 8, End of Epoch 2/6, Average Loss=2.1271424293518066, Class Loss=1.285717248916626, Reg Loss=0.8414250612258911
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=1.1658740043640137, Reg Loss=0.8801531195640564
Clinet index 8, End of Epoch 3/6, Average Loss=2.046027183532715, Class Loss=1.1658740043640137, Reg Loss=0.8801531195640564
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=1.040327787399292, Reg Loss=1.0284571647644043
Clinet index 8, End of Epoch 4/6, Average Loss=2.0687849521636963, Class Loss=1.040327787399292, Reg Loss=1.0284571647644043
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.8145725131034851, Reg Loss=0.9695705771446228
Clinet index 8, End of Epoch 5/6, Average Loss=1.784143090248108, Class Loss=0.8145725131034851, Reg Loss=0.9695705771446228
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6351882219314575, Reg Loss=0.9075059294700623
Clinet index 8, End of Epoch 6/6, Average Loss=1.542694091796875, Class Loss=0.6351882219314575, Reg Loss=0.9075059294700623
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.3834866285324097, Reg Loss=0.8245736956596375
Clinet index 0, End of Epoch 1/6, Average Loss=2.2080602645874023, Class Loss=1.3834866285324097, Reg Loss=0.8245736956596375
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.3262418508529663, Reg Loss=0.8908454775810242
Clinet index 0, End of Epoch 2/6, Average Loss=2.2170872688293457, Class Loss=1.3262418508529663, Reg Loss=0.8908454775810242
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=1.1573145389556885, Reg Loss=0.9906390905380249
Clinet index 0, End of Epoch 3/6, Average Loss=2.147953510284424, Class Loss=1.1573145389556885, Reg Loss=0.9906390905380249
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.9314157366752625, Reg Loss=0.84987872838974
Clinet index 0, End of Epoch 4/6, Average Loss=1.7812944650650024, Class Loss=0.9314157366752625, Reg Loss=0.84987872838974
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.776665210723877, Reg Loss=0.9085297584533691
Clinet index 0, End of Epoch 5/6, Average Loss=1.685194969177246, Class Loss=0.776665210723877, Reg Loss=0.9085297584533691
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6281360387802124, Reg Loss=0.8591267466545105
Clinet index 0, End of Epoch 6/6, Average Loss=1.4872627258300781, Class Loss=0.6281360387802124, Reg Loss=0.8591267466545105
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=14.604482313990593
Loss made of: CE 0.23511509597301483, LKD 0.5284286737442017, LDE 0.0, LReg 0.0, POD 13.157747268676758 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/27, Loss=14.003182294964791
Loss made of: CE 0.2628408670425415, LKD 1.4121222496032715, LDE 0.0, LReg 0.0, POD 12.714912414550781 EntMin 0.0
Epoch 1, Class Loss=0.30413058400154114, Reg Loss=0.7314556837081909
Clinet index 5, End of Epoch 1/6, Average Loss=1.0355862379074097, Class Loss=0.30413058400154114, Reg Loss=0.7314556837081909
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/27, Loss=14.049524572491645
Loss made of: CE 0.299521803855896, LKD 1.107627511024475, LDE 0.0, LReg 0.0, POD 13.21717643737793 EntMin 0.0
Epoch 2, Batch 20/27, Loss=14.01032065153122
Loss made of: CE 0.24645660817623138, LKD 0.6448892951011658, LDE 0.0, LReg 0.0, POD 12.517356872558594 EntMin 0.0
Epoch 2, Class Loss=0.2984465956687927, Reg Loss=0.7632544636726379
Clinet index 5, End of Epoch 2/6, Average Loss=1.0617010593414307, Class Loss=0.2984465956687927, Reg Loss=0.7632544636726379
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/27, Loss=14.264773839712143
Loss made of: CE 0.35329970717430115, LKD 0.6638524532318115, LDE 0.0, LReg 0.0, POD 13.499652862548828 EntMin 0.0
Epoch 3, Batch 20/27, Loss=13.907093447446822
Loss made of: CE 0.22982299327850342, LKD 0.6970998644828796, LDE 0.0, LReg 0.0, POD 13.257172584533691 EntMin 0.0
Epoch 3, Class Loss=0.290275901556015, Reg Loss=0.7349505424499512
Clinet index 5, End of Epoch 3/6, Average Loss=1.0252264738082886, Class Loss=0.290275901556015, Reg Loss=0.7349505424499512
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/27, Loss=14.05947154611349
Loss made of: CE 0.2814069986343384, LKD 0.7026993632316589, LDE 0.0, LReg 0.0, POD 13.195243835449219 EntMin 0.0
Epoch 4, Batch 20/27, Loss=13.837473191320896
Loss made of: CE 0.31064993143081665, LKD 0.7346298694610596, LDE 0.0, LReg 0.0, POD 12.428201675415039 EntMin 0.0
Epoch 4, Class Loss=0.28563570976257324, Reg Loss=0.7389287352561951
Clinet index 5, End of Epoch 4/6, Average Loss=1.024564504623413, Class Loss=0.28563570976257324, Reg Loss=0.7389287352561951
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/27, Loss=14.00328799188137
Loss made of: CE 0.26567959785461426, LKD 0.7003235816955566, LDE 0.0, LReg 0.0, POD 13.0618257522583 EntMin 0.0
Epoch 5, Batch 20/27, Loss=13.48193655461073
Loss made of: CE 0.278942346572876, LKD 0.7494829893112183, LDE 0.0, LReg 0.0, POD 12.032535552978516 EntMin 0.0
Epoch 5, Class Loss=0.28184276819229126, Reg Loss=0.7389541268348694
Clinet index 5, End of Epoch 5/6, Average Loss=1.0207968950271606, Class Loss=0.28184276819229126, Reg Loss=0.7389541268348694
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/27, Loss=13.903458714485168
Loss made of: CE 0.30664724111557007, LKD 1.0428040027618408, LDE 0.0, LReg 0.0, POD 12.263607025146484 EntMin 0.0
Epoch 6, Batch 20/27, Loss=13.591518807411195
Loss made of: CE 0.28570008277893066, LKD 0.4437866806983948, LDE 0.0, LReg 0.0, POD 11.711457252502441 EntMin 0.0
Epoch 6, Class Loss=0.27610039710998535, Reg Loss=0.7130679488182068
Clinet index 5, End of Epoch 6/6, Average Loss=0.9891683459281921, Class Loss=0.27610039710998535, Reg Loss=0.7130679488182068
federated aggregation...
Validation, Class Loss=0.25603899359703064, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.920352
Mean Acc: 0.710516
FreqW Acc: 0.855661
Mean IoU: 0.629854
Class IoU:
	class 0: 0.9083329
	class 1: 0.8818527
	class 2: 0.35068998
	class 3: 0.7595875
	class 4: 0.7064207
	class 5: 0.72550327
	class 6: 0.8529352
	class 7: 0.8732501
	class 8: 0.83603084
	class 9: 0.033787232
	class 10: 0.0
Class Acc:
	class 0: 0.97973084
	class 1: 0.91874576
	class 2: 0.9037546
	class 3: 0.7687247
	class 4: 0.8163157
	class 5: 0.74876857
	class 6: 0.8600202
	class 7: 0.92175806
	class 8: 0.8607738
	class 9: 0.037088864
	class 10: 0.0

federated global round: 8, step: 1
select part of clients to conduct local training
[12, 9, 4, 13]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=17.173115241527558
Loss made of: CE 0.40608128905296326, LKD 0.9173272848129272, LDE 0.0, LReg 0.0, POD 14.301844596862793 EntMin 0.0
Epoch 1, Batch 20/27, Loss=14.535465888679028
Loss made of: CE 0.2934332489967346, LKD 0.6969174146652222, LDE 0.0, LReg 0.0, POD 12.561885833740234 EntMin 0.0
Epoch 1, Class Loss=0.31045374274253845, Reg Loss=0.8369376063346863
Clinet index 12, End of Epoch 1/6, Average Loss=1.1473913192749023, Class Loss=0.31045374274253845, Reg Loss=0.8369376063346863
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=14.131106887757777
Loss made of: CE 0.3267093300819397, LKD 0.6654313802719116, LDE 0.0, LReg 0.0, POD 13.10997200012207 EntMin 0.0
Epoch 2, Batch 20/27, Loss=14.746091309189797
Loss made of: CE 0.38426947593688965, LKD 0.9687907099723816, LDE 0.0, LReg 0.0, POD 13.898473739624023 EntMin 0.0
Epoch 2, Class Loss=0.3005807399749756, Reg Loss=0.7908076643943787
Clinet index 12, End of Epoch 2/6, Average Loss=1.091388463973999, Class Loss=0.3005807399749756, Reg Loss=0.7908076643943787
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=14.078222519159317
Loss made of: CE 0.2349470853805542, LKD 0.8545114398002625, LDE 0.0, LReg 0.0, POD 13.086833953857422 EntMin 0.0
Epoch 3, Batch 20/27, Loss=14.324163156747819
Loss made of: CE 0.24120685458183289, LKD 0.7884265780448914, LDE 0.0, LReg 0.0, POD 12.108125686645508 EntMin 0.0
Epoch 3, Class Loss=0.2857179641723633, Reg Loss=0.8224430680274963
Clinet index 12, End of Epoch 3/6, Average Loss=1.1081609725952148, Class Loss=0.2857179641723633, Reg Loss=0.8224430680274963
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=13.701338182389737
Loss made of: CE 0.23209786415100098, LKD 0.7481908798217773, LDE 0.0, LReg 0.0, POD 12.75836181640625 EntMin 0.0
Epoch 4, Batch 20/27, Loss=13.443732395768166
Loss made of: CE 0.2596674859523773, LKD 0.7965288162231445, LDE 0.0, LReg 0.0, POD 12.269261360168457 EntMin 0.0
Epoch 4, Class Loss=0.27490201592445374, Reg Loss=0.8293204307556152
Clinet index 12, End of Epoch 4/6, Average Loss=1.1042224168777466, Class Loss=0.27490201592445374, Reg Loss=0.8293204307556152
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/27, Loss=13.831666253507137
Loss made of: CE 0.2411157786846161, LKD 0.5958377122879028, LDE 0.0, LReg 0.0, POD 13.261680603027344 EntMin 0.0
Epoch 5, Batch 20/27, Loss=14.106077605485916
Loss made of: CE 0.26006221771240234, LKD 0.6204025745391846, LDE 0.0, LReg 0.0, POD 13.432191848754883 EntMin 0.0
Epoch 5, Class Loss=0.27489715814590454, Reg Loss=0.8204839825630188
Clinet index 12, End of Epoch 5/6, Average Loss=1.0953811407089233, Class Loss=0.27489715814590454, Reg Loss=0.8204839825630188
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=13.290813781321049
Loss made of: CE 0.3387727439403534, LKD 0.7592841386795044, LDE 0.0, LReg 0.0, POD 12.285116195678711 EntMin 0.0
Epoch 6, Batch 20/27, Loss=13.467302817106248
Loss made of: CE 0.2920178771018982, LKD 0.824690580368042, LDE 0.0, LReg 0.0, POD 12.913440704345703 EntMin 0.0
Epoch 6, Class Loss=0.27000439167022705, Reg Loss=0.7803930640220642
Clinet index 12, End of Epoch 6/6, Average Loss=1.0503973960876465, Class Loss=0.27000439167022705, Reg Loss=0.7803930640220642
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.8141052722930908, Reg Loss=1.0180060863494873
Clinet index 9, End of Epoch 1/6, Average Loss=1.8321113586425781, Class Loss=0.8141052722930908, Reg Loss=1.0180060863494873
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.6891580820083618, Reg Loss=1.0034232139587402
Clinet index 9, End of Epoch 2/6, Average Loss=1.692581295967102, Class Loss=0.6891580820083618, Reg Loss=1.0034232139587402
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.6137239933013916, Reg Loss=0.9201106429100037
Clinet index 9, End of Epoch 3/6, Average Loss=1.53383469581604, Class Loss=0.6137239933013916, Reg Loss=0.9201106429100037
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.4989742040634155, Reg Loss=0.9860360026359558
Clinet index 9, End of Epoch 4/6, Average Loss=1.4850101470947266, Class Loss=0.4989742040634155, Reg Loss=0.9860360026359558
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.42113929986953735, Reg Loss=0.9527840614318848
Clinet index 9, End of Epoch 5/6, Average Loss=1.3739233016967773, Class Loss=0.42113929986953735, Reg Loss=0.9527840614318848
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.3448309302330017, Reg Loss=0.8944925665855408
Clinet index 9, End of Epoch 6/6, Average Loss=1.2393234968185425, Class Loss=0.3448309302330017, Reg Loss=0.8944925665855408
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/27, Loss=17.30943110138178
Loss made of: CE 0.3323380649089813, LKD 0.7738088965415955, LDE 0.0, LReg 0.0, POD 13.850105285644531 EntMin 0.0
Epoch 1, Batch 20/27, Loss=14.900561937689782
Loss made of: CE 0.3224840760231018, LKD 0.9640986323356628, LDE 0.0, LReg 0.0, POD 13.100311279296875 EntMin 0.0
Epoch 1, Class Loss=0.3265586793422699, Reg Loss=0.8547084331512451
Clinet index 4, End of Epoch 1/6, Average Loss=1.1812671422958374, Class Loss=0.3265586793422699, Reg Loss=0.8547084331512451
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=14.425716565549374
Loss made of: CE 0.26654335856437683, LKD 0.8501667976379395, LDE 0.0, LReg 0.0, POD 12.457128524780273 EntMin 0.0
Epoch 2, Batch 20/27, Loss=14.567760114371776
Loss made of: CE 0.3180086612701416, LKD 0.8928982615470886, LDE 0.0, LReg 0.0, POD 12.740382194519043 EntMin 0.0
Epoch 2, Class Loss=0.3113788068294525, Reg Loss=0.8314517140388489
Clinet index 4, End of Epoch 2/6, Average Loss=1.142830491065979, Class Loss=0.3113788068294525, Reg Loss=0.8314517140388489
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=13.876665499806403
Loss made of: CE 0.2707645297050476, LKD 0.784390926361084, LDE 0.0, LReg 0.0, POD 12.488666534423828 EntMin 0.0
Epoch 3, Batch 20/27, Loss=14.43249783962965
Loss made of: CE 0.40623506903648376, LKD 0.8544649481773376, LDE 0.0, LReg 0.0, POD 13.005904197692871 EntMin 0.0
Epoch 3, Class Loss=0.2950643301010132, Reg Loss=0.7908012866973877
Clinet index 4, End of Epoch 3/6, Average Loss=1.0858656167984009, Class Loss=0.2950643301010132, Reg Loss=0.7908012866973877
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=13.899904963374137
Loss made of: CE 0.41488105058670044, LKD 1.3930177688598633, LDE 0.0, LReg 0.0, POD 13.748418807983398 EntMin 0.0
Epoch 4, Batch 20/27, Loss=14.063564452528954
Loss made of: CE 0.2646232545375824, LKD 0.7606347799301147, LDE 0.0, LReg 0.0, POD 12.959327697753906 EntMin 0.0
Epoch 4, Class Loss=0.2911956310272217, Reg Loss=0.778840959072113
Clinet index 4, End of Epoch 4/6, Average Loss=1.0700366497039795, Class Loss=0.2911956310272217, Reg Loss=0.778840959072113
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=13.830059139430523
Loss made of: CE 0.26331859827041626, LKD 0.9600261449813843, LDE 0.0, LReg 0.0, POD 13.676009178161621 EntMin 0.0
Epoch 5, Batch 20/27, Loss=13.7184893399477
Loss made of: CE 0.2655467689037323, LKD 0.598706841468811, LDE 0.0, LReg 0.0, POD 12.465465545654297 EntMin 0.0
Epoch 5, Class Loss=0.2857283055782318, Reg Loss=0.8235785365104675
Clinet index 4, End of Epoch 5/6, Average Loss=1.109306812286377, Class Loss=0.2857283055782318, Reg Loss=0.8235785365104675
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=13.616489164531231
Loss made of: CE 0.31481337547302246, LKD 0.5262545347213745, LDE 0.0, LReg 0.0, POD 12.843687057495117 EntMin 0.0
Epoch 6, Batch 20/27, Loss=13.516385114192962
Loss made of: CE 0.29472577571868896, LKD 0.8669780492782593, LDE 0.0, LReg 0.0, POD 11.558439254760742 EntMin 0.0
Epoch 6, Class Loss=0.2818624973297119, Reg Loss=0.8364245891571045
Clinet index 4, End of Epoch 6/6, Average Loss=1.1182870864868164, Class Loss=0.2818624973297119, Reg Loss=0.8364245891571045
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.774346113204956, Reg Loss=0.8668464422225952
Clinet index 13, End of Epoch 1/6, Average Loss=1.6411925554275513, Class Loss=0.774346113204956, Reg Loss=0.8668464422225952
Pseudo labeling is: None
Epoch 2, lr = 0.000642
Epoch 2, Class Loss=0.7032698392868042, Reg Loss=0.8463007807731628
Clinet index 13, End of Epoch 2/6, Average Loss=1.5495705604553223, Class Loss=0.7032698392868042, Reg Loss=0.8463007807731628
Pseudo labeling is: None
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.6056218147277832, Reg Loss=0.8374926447868347
Clinet index 13, End of Epoch 3/6, Average Loss=1.4431145191192627, Class Loss=0.6056218147277832, Reg Loss=0.8374926447868347
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5674992203712463, Reg Loss=0.7946101427078247
Clinet index 13, End of Epoch 4/6, Average Loss=1.3621094226837158, Class Loss=0.5674992203712463, Reg Loss=0.7946101427078247
Pseudo labeling is: None
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.510421633720398, Reg Loss=0.8184458613395691
Clinet index 13, End of Epoch 5/6, Average Loss=1.3288674354553223, Class Loss=0.510421633720398, Reg Loss=0.8184458613395691
Pseudo labeling is: None
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.47642284631729126, Reg Loss=0.8223023414611816
Clinet index 13, End of Epoch 6/6, Average Loss=1.2987251281738281, Class Loss=0.47642284631729126, Reg Loss=0.8223023414611816
federated aggregation...
Validation, Class Loss=0.24096713960170746, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923722
Mean Acc: 0.731159
FreqW Acc: 0.863324
Mean IoU: 0.644256
Class IoU:
	class 0: 0.9127897
	class 1: 0.8824629
	class 2: 0.34853455
	class 3: 0.78336877
	class 4: 0.70882314
	class 5: 0.767966
	class 6: 0.8762771
	class 7: 0.87991446
	class 8: 0.87408215
	class 9: 0.05259177
	class 10: 0.0
Class Acc:
	class 0: 0.976937
	class 1: 0.91854906
	class 2: 0.92735916
	class 3: 0.7921693
	class 4: 0.83413374
	class 5: 0.8059989
	class 6: 0.8843105
	class 7: 0.92405915
	class 8: 0.9165834
	class 9: 0.06265237
	class 10: 0.0

federated global round: 9, step: 1
select part of clients to conduct local training
[4, 6, 13, 12]
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=14.955529843270778
Loss made of: CE 0.2719615697860718, LKD 0.6695770025253296, LDE 0.0, LReg 0.0, POD 12.883771896362305 EntMin 0.0
Epoch 1, Batch 20/27, Loss=13.804578050971031
Loss made of: CE 0.29356878995895386, LKD 0.745756983757019, LDE 0.0, LReg 0.0, POD 12.234857559204102 EntMin 0.0
Epoch 1, Class Loss=0.3134637475013733, Reg Loss=0.8075786828994751
Clinet index 4, End of Epoch 1/6, Average Loss=1.1210424900054932, Class Loss=0.3134637475013733, Reg Loss=0.8075786828994751
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=13.074061065912247
Loss made of: CE 0.28283461928367615, LKD 0.8331848382949829, LDE 0.0, LReg 0.0, POD 10.801727294921875 EntMin 0.0
Epoch 2, Batch 20/27, Loss=13.230595281720161
Loss made of: CE 0.31982970237731934, LKD 0.8965396881103516, LDE 0.0, LReg 0.0, POD 11.099197387695312 EntMin 0.0
Epoch 2, Class Loss=0.3118237555027008, Reg Loss=0.7609545588493347
Clinet index 4, End of Epoch 2/6, Average Loss=1.072778344154358, Class Loss=0.3118237555027008, Reg Loss=0.7609545588493347
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=12.754423472285271
Loss made of: CE 0.28115689754486084, LKD 0.7891548871994019, LDE 0.0, LReg 0.0, POD 11.552022933959961 EntMin 0.0
Epoch 3, Batch 20/27, Loss=13.091879208385944
Loss made of: CE 0.41615206003189087, LKD 0.6515963673591614, LDE 0.0, LReg 0.0, POD 11.591608047485352 EntMin 0.0
Epoch 3, Class Loss=0.3007199764251709, Reg Loss=0.7857581377029419
Clinet index 4, End of Epoch 3/6, Average Loss=1.0864781141281128, Class Loss=0.3007199764251709, Reg Loss=0.7857581377029419
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=12.667044481635093
Loss made of: CE 0.4238165020942688, LKD 1.2691882848739624, LDE 0.0, LReg 0.0, POD 12.135429382324219 EntMin 0.0
Epoch 4, Batch 20/27, Loss=12.914860726892949
Loss made of: CE 0.25901100039482117, LKD 0.5933643579483032, LDE 0.0, LReg 0.0, POD 11.791986465454102 EntMin 0.0
Epoch 4, Class Loss=0.2991095781326294, Reg Loss=0.7817813158035278
Clinet index 4, End of Epoch 4/6, Average Loss=1.0808908939361572, Class Loss=0.2991095781326294, Reg Loss=0.7817813158035278
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=12.587197221815586
Loss made of: CE 0.2537885904312134, LKD 1.0041589736938477, LDE 0.0, LReg 0.0, POD 12.318307876586914 EntMin 0.0
Epoch 5, Batch 20/27, Loss=12.552871063351631
Loss made of: CE 0.2864062190055847, LKD 0.6217007040977478, LDE 0.0, LReg 0.0, POD 11.572969436645508 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.29695624113082886, Reg Loss=0.8003405332565308
Clinet index 4, End of Epoch 5/6, Average Loss=1.0972967147827148, Class Loss=0.29695624113082886, Reg Loss=0.8003405332565308
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=12.390427888929844
Loss made of: CE 0.3546774387359619, LKD 0.5239807367324829, LDE 0.0, LReg 0.0, POD 11.62594985961914 EntMin 0.0
Epoch 6, Batch 20/27, Loss=12.406592828035354
Loss made of: CE 0.3383009433746338, LKD 0.9138206243515015, LDE 0.0, LReg 0.0, POD 10.26687240600586 EntMin 0.0
Epoch 6, Class Loss=0.29972442984580994, Reg Loss=0.7955349683761597
Clinet index 4, End of Epoch 6/6, Average Loss=1.095259428024292, Class Loss=0.29972442984580994, Reg Loss=0.7955349683761597
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/27, Loss=15.067285153269768
Loss made of: CE 0.3007503151893616, LKD 1.1608216762542725, LDE 0.0, LReg 0.0, POD 12.03150463104248 EntMin 0.0
Epoch 1, Batch 20/27, Loss=13.559727221727371
Loss made of: CE 0.26046642661094666, LKD 1.0403286218643188, LDE 0.0, LReg 0.0, POD 12.69637393951416 EntMin 0.0
Epoch 1, Class Loss=0.3315833806991577, Reg Loss=0.8366729021072388
Clinet index 6, End of Epoch 1/6, Average Loss=1.1682562828063965, Class Loss=0.3315833806991577, Reg Loss=0.8366729021072388
Pseudo labeling is: None
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/27, Loss=13.437533083558083
Loss made of: CE 0.2854391932487488, LKD 0.9762557744979858, LDE 0.0, LReg 0.0, POD 12.717203140258789 EntMin 0.0
Epoch 2, Batch 20/27, Loss=13.385788932442665
Loss made of: CE 0.3351558446884155, LKD 0.7535523176193237, LDE 0.0, LReg 0.0, POD 11.920291900634766 EntMin 0.0
Epoch 2, Class Loss=0.3105615973472595, Reg Loss=0.8450950384140015
Clinet index 6, End of Epoch 2/6, Average Loss=1.1556565761566162, Class Loss=0.3105615973472595, Reg Loss=0.8450950384140015
Pseudo labeling is: None
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/27, Loss=12.937467858195305
Loss made of: CE 0.3126929998397827, LKD 0.8243867754936218, LDE 0.0, LReg 0.0, POD 12.144508361816406 EntMin 0.0
Epoch 3, Batch 20/27, Loss=13.251334965229034
Loss made of: CE 0.33742016553878784, LKD 0.6770895719528198, LDE 0.0, LReg 0.0, POD 11.85308837890625 EntMin 0.0
Epoch 3, Class Loss=0.3027472198009491, Reg Loss=0.7924998998641968
Clinet index 6, End of Epoch 3/6, Average Loss=1.0952471494674683, Class Loss=0.3027472198009491, Reg Loss=0.7924998998641968
Pseudo labeling is: None
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/27, Loss=13.197269290685654
Loss made of: CE 0.3894880414009094, LKD 1.4684903621673584, LDE 0.0, LReg 0.0, POD 11.291006088256836 EntMin 0.0
Epoch 4, Batch 20/27, Loss=12.743738958239556
Loss made of: CE 0.23074206709861755, LKD 0.6465007066726685, LDE 0.0, LReg 0.0, POD 11.95657730102539 EntMin 0.0
Epoch 4, Class Loss=0.30174437165260315, Reg Loss=0.8002369403839111
Clinet index 6, End of Epoch 4/6, Average Loss=1.101981282234192, Class Loss=0.30174437165260315, Reg Loss=0.8002369403839111
Pseudo labeling is: None
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/27, Loss=13.042675539851189
Loss made of: CE 0.29975247383117676, LKD 0.9787033796310425, LDE 0.0, LReg 0.0, POD 10.945148468017578 EntMin 0.0
Epoch 5, Batch 20/27, Loss=12.757051216065884
Loss made of: CE 0.21368591487407684, LKD 0.7061523199081421, LDE 0.0, LReg 0.0, POD 11.803232192993164 EntMin 0.0
Epoch 5, Class Loss=0.29518720507621765, Reg Loss=0.7987074851989746
Clinet index 6, End of Epoch 5/6, Average Loss=1.0938947200775146, Class Loss=0.29518720507621765, Reg Loss=0.7987074851989746
Pseudo labeling is: None
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/27, Loss=12.835399743914603
Loss made of: CE 0.23635411262512207, LKD 0.5922540426254272, LDE 0.0, LReg 0.0, POD 11.652585983276367 EntMin 0.0
Epoch 6, Batch 20/27, Loss=12.23691234588623
Loss made of: CE 0.22266000509262085, LKD 0.6962385177612305, LDE 0.0, LReg 0.0, POD 10.39586067199707 EntMin 0.0
Epoch 6, Class Loss=0.29324984550476074, Reg Loss=0.781558632850647
Clinet index 6, End of Epoch 6/6, Average Loss=1.0748084783554077, Class Loss=0.29324984550476074, Reg Loss=0.781558632850647
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000372
Epoch 1, Class Loss=0.7268441915512085, Reg Loss=0.8608724474906921
Clinet index 13, End of Epoch 1/6, Average Loss=1.5877165794372559, Class Loss=0.7268441915512085, Reg Loss=0.8608724474906921
Pseudo labeling is: None
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.7188374400138855, Reg Loss=0.8849008083343506
Clinet index 13, End of Epoch 2/6, Average Loss=1.6037383079528809, Class Loss=0.7188374400138855, Reg Loss=0.8849008083343506
Pseudo labeling is: None
Epoch 3, lr = 0.000258
Epoch 3, Class Loss=0.6607269644737244, Reg Loss=0.896195650100708
Clinet index 13, End of Epoch 3/6, Average Loss=1.5569226741790771, Class Loss=0.6607269644737244, Reg Loss=0.896195650100708
Pseudo labeling is: None
Epoch 4, lr = 0.000199
Epoch 4, Class Loss=0.6515079736709595, Reg Loss=0.7848069071769714
Clinet index 13, End of Epoch 4/6, Average Loss=1.4363148212432861, Class Loss=0.6515079736709595, Reg Loss=0.7848069071769714
Pseudo labeling is: None
Epoch 5, lr = 0.000138
Epoch 5, Class Loss=0.6392590403556824, Reg Loss=0.7282533049583435
Clinet index 13, End of Epoch 5/6, Average Loss=1.3675123453140259, Class Loss=0.6392590403556824, Reg Loss=0.7282533049583435
Pseudo labeling is: None
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.6228387355804443, Reg Loss=0.7709454298019409
Clinet index 13, End of Epoch 6/6, Average Loss=1.3937841653823853, Class Loss=0.6228387355804443, Reg Loss=0.7709454298019409
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=15.037788850069045
Loss made of: CE 0.406371533870697, LKD 0.8844588994979858, LDE 0.0, LReg 0.0, POD 12.636922836303711 EntMin 0.0
Epoch 1, Batch 20/27, Loss=13.405977892875672
Loss made of: CE 0.29549640417099, LKD 0.6240170001983643, LDE 0.0, LReg 0.0, POD 12.094894409179688 EntMin 0.0
Epoch 1, Class Loss=0.2983909249305725, Reg Loss=0.8361125588417053
Clinet index 12, End of Epoch 1/6, Average Loss=1.1345034837722778, Class Loss=0.2983909249305725, Reg Loss=0.8361125588417053
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=12.944662722945214
Loss made of: CE 0.2844144105911255, LKD 0.770164966583252, LDE 0.0, LReg 0.0, POD 11.596506118774414 EntMin 0.0
Epoch 2, Batch 20/27, Loss=13.70217990130186
Loss made of: CE 0.3807945251464844, LKD 1.1503784656524658, LDE 0.0, LReg 0.0, POD 13.329258918762207 EntMin 0.0
Epoch 2, Class Loss=0.29368287324905396, Reg Loss=0.8109033703804016
Clinet index 12, End of Epoch 2/6, Average Loss=1.1045862436294556, Class Loss=0.29368287324905396, Reg Loss=0.8109033703804016
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=12.821634550392627
Loss made of: CE 0.24941089749336243, LKD 0.8330925703048706, LDE 0.0, LReg 0.0, POD 11.772514343261719 EntMin 0.0
Epoch 3, Batch 20/27, Loss=13.076006227731705
Loss made of: CE 0.2436474859714508, LKD 0.8219882845878601, LDE 0.0, LReg 0.0, POD 11.278606414794922 EntMin 0.0
Epoch 3, Class Loss=0.2922196686267853, Reg Loss=0.7713939547538757
Clinet index 12, End of Epoch 3/6, Average Loss=1.0636136531829834, Class Loss=0.2922196686267853, Reg Loss=0.7713939547538757
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=12.786934813857078
Loss made of: CE 0.22056615352630615, LKD 0.6387040019035339, LDE 0.0, LReg 0.0, POD 11.937858581542969 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 20/27, Loss=12.350866430997849
Loss made of: CE 0.2945552170276642, LKD 0.8002356290817261, LDE 0.0, LReg 0.0, POD 11.712739944458008 EntMin 0.0
Epoch 4, Class Loss=0.28155404329299927, Reg Loss=0.7610958218574524
Clinet index 12, End of Epoch 4/6, Average Loss=1.0426498651504517, Class Loss=0.28155404329299927, Reg Loss=0.7610958218574524
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=12.703442384302615
Loss made of: CE 0.2606986463069916, LKD 0.6736282110214233, LDE 0.0, LReg 0.0, POD 12.001035690307617 EntMin 0.0
Epoch 5, Batch 20/27, Loss=12.862359356880187
Loss made of: CE 0.2819978892803192, LKD 0.6690058708190918, LDE 0.0, LReg 0.0, POD 11.671329498291016 EntMin 0.0
Epoch 5, Class Loss=0.28261733055114746, Reg Loss=0.7912686467170715
Clinet index 12, End of Epoch 5/6, Average Loss=1.0738859176635742, Class Loss=0.28261733055114746, Reg Loss=0.7912686467170715
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=12.161676245927811
Loss made of: CE 0.3327522277832031, LKD 1.0069974660873413, LDE 0.0, LReg 0.0, POD 10.894218444824219 EntMin 0.0
Epoch 6, Batch 20/27, Loss=12.353281217813493
Loss made of: CE 0.36633533239364624, LKD 0.9185265302658081, LDE 0.0, LReg 0.0, POD 12.030912399291992 EntMin 0.0
Epoch 6, Class Loss=0.28191444277763367, Reg Loss=0.7754033207893372
Clinet index 12, End of Epoch 6/6, Average Loss=1.0573177337646484, Class Loss=0.28191444277763367, Reg Loss=0.7754033207893372
federated aggregation...
Validation, Class Loss=0.24858741462230682, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923206
Mean Acc: 0.740763
FreqW Acc: 0.864748
Mean IoU: 0.650985
Class IoU:
	class 0: 0.9127325
	class 1: 0.8809577
	class 2: 0.3548771
	class 3: 0.7886256
	class 4: 0.7101632
	class 5: 0.7836607
	class 6: 0.8766636
	class 7: 0.87708867
	class 8: 0.8881634
	class 9: 0.08790765
	class 10: 0.0
Class Acc:
	class 0: 0.97341657
	class 1: 0.9214437
	class 2: 0.91452837
	class 3: 0.7980216
	class 4: 0.8387065
	class 5: 0.83384097
	class 6: 0.88401437
	class 7: 0.92797047
	class 8: 0.9307019
	class 9: 0.12574834
	class 10: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[10, 14, 16, 7]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=32.31075947284698
Loss made of: CE 0.8331829905509949, LKD 5.295892238616943, LDE 0.0, LReg 0.0, POD 22.515869140625 EntMin 0.0
Epoch 1, Batch 20/29, Loss=28.431877040863036
Loss made of: CE 0.7928172945976257, LKD 5.752503871917725, LDE 0.0, LReg 0.0, POD 21.561466217041016 EntMin 0.0
Epoch 1, Class Loss=0.8594238758087158, Reg Loss=5.77952241897583
Clinet index 10, End of Epoch 1/6, Average Loss=6.638946533203125, Class Loss=0.8594238758087158, Reg Loss=5.77952241897583
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=25.551160663366318
Loss made of: CE 0.4749261140823364, LKD 5.194357395172119, LDE 0.0, LReg 0.0, POD 18.688987731933594 EntMin 0.0
Epoch 2, Batch 20/29, Loss=25.255863842368125
Loss made of: CE 0.43328794836997986, LKD 5.292851448059082, LDE 0.0, LReg 0.0, POD 18.42319107055664 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.5176876783370972, Reg Loss=5.498571395874023
Clinet index 10, End of Epoch 2/6, Average Loss=6.01625919342041, Class Loss=0.5176876783370972, Reg Loss=5.498571395874023
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=24.244344547390938
Loss made of: CE 0.4156724214553833, LKD 5.350362777709961, LDE 0.0, LReg 0.0, POD 17.981273651123047 EntMin 0.0
Epoch 3, Batch 20/29, Loss=22.891441252827644
Loss made of: CE 0.3994743824005127, LKD 5.407670974731445, LDE 0.0, LReg 0.0, POD 16.423099517822266 EntMin 0.0
Epoch 3, Class Loss=0.4265238344669342, Reg Loss=5.507987022399902
Clinet index 10, End of Epoch 3/6, Average Loss=5.934510707855225, Class Loss=0.4265238344669342, Reg Loss=5.507987022399902
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=23.086867171525956
Loss made of: CE 0.45202964544296265, LKD 5.753871917724609, LDE 0.0, LReg 0.0, POD 17.469411849975586 EntMin 0.0
Epoch 4, Batch 20/29, Loss=22.778123632073402
Loss made of: CE 0.41360896825790405, LKD 5.47636604309082, LDE 0.0, LReg 0.0, POD 16.116043090820312 EntMin 0.0
Epoch 4, Class Loss=0.38591694831848145, Reg Loss=5.5296711921691895
Clinet index 10, End of Epoch 4/6, Average Loss=5.91558837890625, Class Loss=0.38591694831848145, Reg Loss=5.5296711921691895
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=22.427062234282495
Loss made of: CE 0.29963481426239014, LKD 4.779819488525391, LDE 0.0, LReg 0.0, POD 15.481210708618164 EntMin 0.0
Epoch 5, Batch 20/29, Loss=22.40857472717762
Loss made of: CE 0.3804336488246918, LKD 5.517547607421875, LDE 0.0, LReg 0.0, POD 17.704769134521484 EntMin 0.0
Epoch 5, Class Loss=0.3622080683708191, Reg Loss=5.458588600158691
Clinet index 10, End of Epoch 5/6, Average Loss=5.820796489715576, Class Loss=0.3622080683708191, Reg Loss=5.458588600158691
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=21.79273917675018
Loss made of: CE 0.3782188594341278, LKD 5.273690223693848, LDE 0.0, LReg 0.0, POD 16.31847381591797 EntMin 0.0
Epoch 6, Batch 20/29, Loss=21.51568385064602
Loss made of: CE 0.3457597494125366, LKD 5.92171049118042, LDE 0.0, LReg 0.0, POD 15.453727722167969 EntMin 0.0
Epoch 6, Class Loss=0.3558792173862457, Reg Loss=5.476196765899658
Clinet index 10, End of Epoch 6/6, Average Loss=5.832076072692871, Class Loss=0.3558792173862457, Reg Loss=5.476196765899658
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=18.440980619192125
Loss made of: CE 0.7837785482406616, LKD 3.175175428390503, LDE 0.0, LReg 0.0, POD 13.658478736877441 EntMin 0.0
Epoch 1, Class Loss=0.863997757434845, Reg Loss=2.988039255142212
Clinet index 14, End of Epoch 1/6, Average Loss=3.852036952972412, Class Loss=0.863997757434845, Reg Loss=2.988039255142212
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=16.99872455596924
Loss made of: CE 0.5671299695968628, LKD 3.0197601318359375, LDE 0.0, LReg 0.0, POD 11.929901123046875 EntMin 0.0
Epoch 2, Class Loss=0.6269081234931946, Reg Loss=2.958502769470215
Clinet index 14, End of Epoch 2/6, Average Loss=3.5854108333587646, Class Loss=0.6269081234931946, Reg Loss=2.958502769470215
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/13, Loss=16.095993918180465
Loss made of: CE 0.5086917877197266, LKD 2.951404571533203, LDE 0.0, LReg 0.0, POD 12.955720901489258 EntMin 0.0
Epoch 3, Class Loss=0.5443172454833984, Reg Loss=2.9531545639038086
Clinet index 14, End of Epoch 3/6, Average Loss=3.497471809387207, Class Loss=0.5443172454833984, Reg Loss=2.9531545639038086
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=15.421161940693855
Loss made of: CE 0.47062695026397705, LKD 2.4728126525878906, LDE 0.0, LReg 0.0, POD 12.011157989501953 EntMin 0.0
Epoch 4, Class Loss=0.4895719885826111, Reg Loss=2.920485496520996
Clinet index 14, End of Epoch 4/6, Average Loss=3.410057544708252, Class Loss=0.4895719885826111, Reg Loss=2.920485496520996
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=15.075713294744492
Loss made of: CE 0.48493242263793945, LKD 3.033130168914795, LDE 0.0, LReg 0.0, POD 11.87009334564209 EntMin 0.0
Epoch 5, Class Loss=0.4524686634540558, Reg Loss=2.9569976329803467
Clinet index 14, End of Epoch 5/6, Average Loss=3.40946626663208, Class Loss=0.4524686634540558, Reg Loss=2.9569976329803467
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=14.85400377213955
Loss made of: CE 0.4066891074180603, LKD 2.7953782081604004, LDE 0.0, LReg 0.0, POD 10.818330764770508 EntMin 0.0
Epoch 6, Class Loss=0.42721787095069885, Reg Loss=3.0057826042175293
Clinet index 14, End of Epoch 6/6, Average Loss=3.4330005645751953, Class Loss=0.42721787095069885, Reg Loss=3.0057826042175293
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=33.188148856163025
Loss made of: CE 1.003891944885254, LKD 5.484342098236084, LDE 0.0, LReg 0.0, POD 22.964893341064453 EntMin 0.0
Epoch 1, Batch 20/29, Loss=28.589916312694548
Loss made of: CE 0.7240399122238159, LKD 5.890354156494141, LDE 0.0, LReg 0.0, POD 19.503503799438477 EntMin 0.0
Epoch 1, Class Loss=0.8602303266525269, Reg Loss=5.839199542999268
Clinet index 16, End of Epoch 1/6, Average Loss=6.699429988861084, Class Loss=0.8602303266525269, Reg Loss=5.839199542999268
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=25.059036406874657
Loss made of: CE 0.4674355685710907, LKD 5.207130432128906, LDE 0.0, LReg 0.0, POD 19.164112091064453 EntMin 0.0
Epoch 2, Batch 20/29, Loss=25.218947571516036
Loss made of: CE 0.5059596300125122, LKD 5.482891082763672, LDE 0.0, LReg 0.0, POD 18.891454696655273 EntMin 0.0
Epoch 2, Class Loss=0.5198254585266113, Reg Loss=5.54171085357666
Clinet index 16, End of Epoch 2/6, Average Loss=6.0615363121032715, Class Loss=0.5198254585266113, Reg Loss=5.54171085357666
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=24.116993057727814
Loss made of: CE 0.45422881841659546, LKD 6.186064720153809, LDE 0.0, LReg 0.0, POD 19.098712921142578 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 20/29, Loss=23.17246743142605
Loss made of: CE 0.3784725069999695, LKD 4.87905740737915, LDE 0.0, LReg 0.0, POD 18.155014038085938 EntMin 0.0
Epoch 3, Class Loss=0.41587039828300476, Reg Loss=5.525060653686523
Clinet index 16, End of Epoch 3/6, Average Loss=5.9409308433532715, Class Loss=0.41587039828300476, Reg Loss=5.525060653686523
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=22.656401893496515
Loss made of: CE 0.33545559644699097, LKD 4.957700729370117, LDE 0.0, LReg 0.0, POD 16.138776779174805 EntMin 0.0
Epoch 4, Batch 20/29, Loss=22.67530664205551
Loss made of: CE 0.3171117305755615, LKD 5.724851131439209, LDE 0.0, LReg 0.0, POD 14.993972778320312 EntMin 0.0
Epoch 4, Class Loss=0.37975990772247314, Reg Loss=5.524672985076904
Clinet index 16, End of Epoch 4/6, Average Loss=5.904432773590088, Class Loss=0.37975990772247314, Reg Loss=5.524672985076904
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=21.9503093034029
Loss made of: CE 0.3854161500930786, LKD 5.555182456970215, LDE 0.0, LReg 0.0, POD 16.752826690673828 EntMin 0.0
Epoch 5, Batch 20/29, Loss=21.983496820926668
Loss made of: CE 0.29248857498168945, LKD 4.544431209564209, LDE 0.0, LReg 0.0, POD 15.982284545898438 EntMin 0.0
Epoch 5, Class Loss=0.36691659688949585, Reg Loss=5.532693862915039
Clinet index 16, End of Epoch 5/6, Average Loss=5.89961051940918, Class Loss=0.36691659688949585, Reg Loss=5.532693862915039
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=21.96663916707039
Loss made of: CE 0.36202311515808105, LKD 5.313858985900879, LDE 0.0, LReg 0.0, POD 15.091523170471191 EntMin 0.0
Epoch 6, Batch 20/29, Loss=21.63476521074772
Loss made of: CE 0.3455886244773865, LKD 5.09907341003418, LDE 0.0, LReg 0.0, POD 14.541584014892578 EntMin 0.0
Epoch 6, Class Loss=0.35392889380455017, Reg Loss=5.569846153259277
Clinet index 16, End of Epoch 6/6, Average Loss=5.9237751960754395, Class Loss=0.35392889380455017, Reg Loss=5.569846153259277
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=18.54881172180176
Loss made of: CE 0.7506370544433594, LKD 2.5644989013671875, LDE 0.0, LReg 0.0, POD 14.28909683227539 EntMin 0.0
Epoch 1, Class Loss=0.8586198687553406, Reg Loss=3.010953187942505
Clinet index 7, End of Epoch 1/6, Average Loss=3.8695731163024902, Class Loss=0.8586198687553406, Reg Loss=3.010953187942505
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=17.0493572473526
Loss made of: CE 0.594179093837738, LKD 3.068906307220459, LDE 0.0, LReg 0.0, POD 12.41806697845459 EntMin 0.0
Epoch 2, Class Loss=0.6292296648025513, Reg Loss=2.945887804031372
Clinet index 7, End of Epoch 2/6, Average Loss=3.575117588043213, Class Loss=0.6292296648025513, Reg Loss=2.945887804031372
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=16.316625097393988
Loss made of: CE 0.493943452835083, LKD 3.34887957572937, LDE 0.0, LReg 0.0, POD 12.585548400878906 EntMin 0.0
Epoch 3, Class Loss=0.5348533987998962, Reg Loss=2.978750705718994
Clinet index 7, End of Epoch 3/6, Average Loss=3.513604164123535, Class Loss=0.5348533987998962, Reg Loss=2.978750705718994
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=15.892570647597314
Loss made of: CE 0.5004292130470276, LKD 3.1653361320495605, LDE 0.0, LReg 0.0, POD 11.873201370239258 EntMin 0.0
Epoch 4, Class Loss=0.4827672839164734, Reg Loss=2.946561098098755
Clinet index 7, End of Epoch 4/6, Average Loss=3.429328441619873, Class Loss=0.4827672839164734, Reg Loss=2.946561098098755
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/13, Loss=15.809417137503624
Loss made of: CE 0.43788522481918335, LKD 2.564742088317871, LDE 0.0, LReg 0.0, POD 13.550573348999023 EntMin 0.0
Epoch 5, Class Loss=0.4552929997444153, Reg Loss=2.9730217456817627
Clinet index 7, End of Epoch 5/6, Average Loss=3.428314685821533, Class Loss=0.4552929997444153, Reg Loss=2.9730217456817627
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=15.529374444484711
Loss made of: CE 0.4134795665740967, LKD 2.58255672454834, LDE 0.0, LReg 0.0, POD 13.439057350158691 EntMin 0.0
Epoch 6, Class Loss=0.4324772357940674, Reg Loss=2.9546961784362793
Clinet index 7, End of Epoch 6/6, Average Loss=3.3871734142303467, Class Loss=0.4324772357940674, Reg Loss=2.9546961784362793
federated aggregation...
Validation, Class Loss=0.4097934663295746, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.875001
Mean Acc: 0.632764
FreqW Acc: 0.795014
Mean IoU: 0.527620
Class IoU:
	class 0: 0.8895153
	class 1: 0.86153275
	class 2: 0.33448234
	class 3: 0.79591495
	class 4: 0.69643736
	class 5: 0.7944009
	class 6: 0.83914363
	class 7: 0.87060726
	class 8: 0.70703566
	class 9: 0.06999246
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.9693481
	class 1: 0.8943054
	class 2: 0.9245719
	class 3: 0.81781346
	class 4: 0.80020756
	class 5: 0.8683166
	class 6: 0.8439303
	class 7: 0.93163466
	class 8: 0.9520812
	class 9: 0.22371837
	class 10: 0.0
	class 11: 0.0
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[10, 13, 6, 1]
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/29, Loss=25.306134516000746
Loss made of: CE 0.4624941349029541, LKD 5.245509147644043, LDE 0.0, LReg 0.0, POD 17.816146850585938 EntMin 0.0
Epoch 1, Batch 20/29, Loss=22.957059475779534
Loss made of: CE 0.5206533074378967, LKD 5.621430397033691, LDE 0.0, LReg 0.0, POD 16.326993942260742 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.6024472713470459, Reg Loss=5.520501613616943
Clinet index 10, End of Epoch 1/6, Average Loss=6.12294864654541, Class Loss=0.6024472713470459, Reg Loss=5.520501613616943
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/29, Loss=22.698601821064948
Loss made of: CE 0.3562541902065277, LKD 5.271053791046143, LDE 0.0, LReg 0.0, POD 16.37725067138672 EntMin 0.0
Epoch 2, Batch 20/29, Loss=22.66475376188755
Loss made of: CE 0.35916468501091003, LKD 5.4691596031188965, LDE 0.0, LReg 0.0, POD 15.570236206054688 EntMin 0.0
Epoch 2, Class Loss=0.41062673926353455, Reg Loss=5.52052116394043
Clinet index 10, End of Epoch 2/6, Average Loss=5.931148052215576, Class Loss=0.41062673926353455, Reg Loss=5.52052116394043
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/29, Loss=22.108754938840868
Loss made of: CE 0.37192320823669434, LKD 5.111844539642334, LDE 0.0, LReg 0.0, POD 15.69123649597168 EntMin 0.0
Epoch 3, Batch 20/29, Loss=21.249694761633872
Loss made of: CE 0.3425470292568207, LKD 5.630134105682373, LDE 0.0, LReg 0.0, POD 14.375715255737305 EntMin 0.0
Epoch 3, Class Loss=0.38037407398223877, Reg Loss=5.483275890350342
Clinet index 10, End of Epoch 3/6, Average Loss=5.863649845123291, Class Loss=0.38037407398223877, Reg Loss=5.483275890350342
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/29, Loss=21.47806380689144
Loss made of: CE 0.4397624433040619, LKD 5.887973785400391, LDE 0.0, LReg 0.0, POD 16.36833381652832 EntMin 0.0
Epoch 4, Batch 20/29, Loss=21.533627653121947
Loss made of: CE 0.4212861657142639, LKD 5.381865501403809, LDE 0.0, LReg 0.0, POD 15.094888687133789 EntMin 0.0
Epoch 4, Class Loss=0.36240193247795105, Reg Loss=5.502466678619385
Clinet index 10, End of Epoch 4/6, Average Loss=5.864868640899658, Class Loss=0.36240193247795105, Reg Loss=5.502466678619385
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=21.518180957436563
Loss made of: CE 0.3077738881111145, LKD 4.793517589569092, LDE 0.0, LReg 0.0, POD 14.769832611083984 EntMin 0.0
Epoch 5, Batch 20/29, Loss=21.44001704454422
Loss made of: CE 0.3775905966758728, LKD 5.468531608581543, LDE 0.0, LReg 0.0, POD 16.563941955566406 EntMin 0.0
Epoch 5, Class Loss=0.354106605052948, Reg Loss=5.43972110748291
Clinet index 10, End of Epoch 5/6, Average Loss=5.793827533721924, Class Loss=0.354106605052948, Reg Loss=5.43972110748291
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/29, Loss=20.78897544145584
Loss made of: CE 0.3678480088710785, LKD 5.364991188049316, LDE 0.0, LReg 0.0, POD 15.652334213256836 EntMin 0.0
Epoch 6, Batch 20/29, Loss=20.424408280849455
Loss made of: CE 0.3613014221191406, LKD 5.986971855163574, LDE 0.0, LReg 0.0, POD 14.164648056030273 EntMin 0.0
Epoch 6, Class Loss=0.3507543206214905, Reg Loss=5.459620952606201
Clinet index 10, End of Epoch 6/6, Average Loss=5.810375213623047, Class Loss=0.3507543206214905, Reg Loss=5.459620952606201
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=26.20585631728172
Loss made of: CE 0.44142621755599976, LKD 4.986800670623779, LDE 0.0, LReg 0.0, POD 16.494327545166016 EntMin 0.0
Epoch 1, Batch 20/29, Loss=23.76512075662613
Loss made of: CE 0.4113472104072571, LKD 5.546910762786865, LDE 0.0, LReg 0.0, POD 17.563243865966797 EntMin 0.0
Epoch 1, Class Loss=0.589512288570404, Reg Loss=5.63723611831665
Clinet index 13, End of Epoch 1/6, Average Loss=6.226748466491699, Class Loss=0.589512288570404, Reg Loss=5.63723611831665
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/29, Loss=23.392536705732347
Loss made of: CE 0.43912380933761597, LKD 6.1639909744262695, LDE 0.0, LReg 0.0, POD 16.016319274902344 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/29, Loss=22.613923665881156
Loss made of: CE 0.4144127070903778, LKD 6.049813747406006, LDE 0.0, LReg 0.0, POD 16.086780548095703 EntMin 0.0
Epoch 2, Class Loss=0.4124913811683655, Reg Loss=5.580657482147217
Clinet index 13, End of Epoch 2/6, Average Loss=5.9931488037109375, Class Loss=0.4124913811683655, Reg Loss=5.580657482147217
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/29, Loss=22.035442277789116
Loss made of: CE 0.40143680572509766, LKD 5.795612335205078, LDE 0.0, LReg 0.0, POD 16.548145294189453 EntMin 0.0
Epoch 3, Batch 20/29, Loss=22.997395959496497
Loss made of: CE 0.35664230585098267, LKD 5.782301902770996, LDE 0.0, LReg 0.0, POD 17.652740478515625 EntMin 0.0
Epoch 3, Class Loss=0.3785775899887085, Reg Loss=5.516952991485596
Clinet index 13, End of Epoch 3/6, Average Loss=5.895530700683594, Class Loss=0.3785775899887085, Reg Loss=5.516952991485596
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/29, Loss=22.78898223042488
Loss made of: CE 0.3955689072608948, LKD 5.3986968994140625, LDE 0.0, LReg 0.0, POD 16.314741134643555 EntMin 0.0
Epoch 4, Batch 20/29, Loss=21.851201131939888
Loss made of: CE 0.37390658259391785, LKD 5.641358375549316, LDE 0.0, LReg 0.0, POD 14.166605949401855 EntMin 0.0
Epoch 4, Class Loss=0.367906391620636, Reg Loss=5.561517715454102
Clinet index 13, End of Epoch 4/6, Average Loss=5.929424285888672, Class Loss=0.367906391620636, Reg Loss=5.561517715454102
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/29, Loss=21.665688493847846
Loss made of: CE 0.3381820619106293, LKD 6.099143981933594, LDE 0.0, LReg 0.0, POD 15.640115737915039 EntMin 0.0
Epoch 5, Batch 20/29, Loss=21.638856756687165
Loss made of: CE 0.362740159034729, LKD 5.7856597900390625, LDE 0.0, LReg 0.0, POD 16.87201499938965 EntMin 0.0
Epoch 5, Class Loss=0.35422325134277344, Reg Loss=5.528411865234375
Clinet index 13, End of Epoch 5/6, Average Loss=5.882635116577148, Class Loss=0.35422325134277344, Reg Loss=5.528411865234375
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/29, Loss=21.47514404654503
Loss made of: CE 0.31876736879348755, LKD 5.544669151306152, LDE 0.0, LReg 0.0, POD 17.186800003051758 EntMin 0.0
Epoch 6, Batch 20/29, Loss=21.846252965927125
Loss made of: CE 0.3443501889705658, LKD 6.256059646606445, LDE 0.0, LReg 0.0, POD 15.404115676879883 EntMin 0.0
Epoch 6, Class Loss=0.35536372661590576, Reg Loss=5.552917003631592
Clinet index 13, End of Epoch 6/6, Average Loss=5.908280849456787, Class Loss=0.35536372661590576, Reg Loss=5.552917003631592
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=18.805190271139146
Loss made of: CE 1.029673457145691, LKD 3.0387816429138184, LDE 0.0, LReg 0.0, POD 12.964332580566406 EntMin 0.0
Epoch 1, Class Loss=0.7682792544364929, Reg Loss=3.1739916801452637
Clinet index 6, End of Epoch 1/6, Average Loss=3.9422709941864014, Class Loss=0.7682792544364929, Reg Loss=3.1739916801452637
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=16.685870838165282
Loss made of: CE 0.5290237069129944, LKD 2.9069793224334717, LDE 0.0, LReg 0.0, POD 12.24873161315918 EntMin 0.0
Epoch 2, Class Loss=0.6472271680831909, Reg Loss=3.000082015991211
Clinet index 6, End of Epoch 2/6, Average Loss=3.6473093032836914, Class Loss=0.6472271680831909, Reg Loss=3.000082015991211
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=15.765345668792724
Loss made of: CE 0.34073323011398315, LKD 2.949510097503662, LDE 0.0, LReg 0.0, POD 11.455596923828125 EntMin 0.0
Epoch 3, Class Loss=0.49126294255256653, Reg Loss=3.0632739067077637
Clinet index 6, End of Epoch 3/6, Average Loss=3.554536819458008, Class Loss=0.49126294255256653, Reg Loss=3.0632739067077637
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=15.673388701677322
Loss made of: CE 0.39334914088249207, LKD 2.569047451019287, LDE 0.0, LReg 0.0, POD 12.383760452270508 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.4335757791996002, Reg Loss=3.0056393146514893
Clinet index 6, End of Epoch 4/6, Average Loss=3.4392151832580566, Class Loss=0.4335757791996002, Reg Loss=3.0056393146514893
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=15.113998258113861
Loss made of: CE 0.4092447757720947, LKD 3.313774824142456, LDE 0.0, LReg 0.0, POD 10.688117980957031 EntMin 0.0
Epoch 5, Class Loss=0.38565170764923096, Reg Loss=2.9699783325195312
Clinet index 6, End of Epoch 5/6, Average Loss=3.3556299209594727, Class Loss=0.38565170764923096, Reg Loss=2.9699783325195312
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=14.94053474664688
Loss made of: CE 0.3427259922027588, LKD 2.897592067718506, LDE 0.0, LReg 0.0, POD 12.093822479248047 EntMin 0.0
Epoch 6, Class Loss=0.3755112290382385, Reg Loss=2.992492198944092
Clinet index 6, End of Epoch 6/6, Average Loss=3.3680033683776855, Class Loss=0.3755112290382385, Reg Loss=2.992492198944092
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=18.440293711423873
Loss made of: CE 0.8783706426620483, LKD 3.0017008781433105, LDE 0.0, LReg 0.0, POD 14.513792037963867 EntMin 0.0
Epoch 1, Class Loss=0.7452414631843567, Reg Loss=3.1024982929229736
Clinet index 1, End of Epoch 1/6, Average Loss=3.8477396965026855, Class Loss=0.7452414631843567, Reg Loss=3.1024982929229736
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=16.165422269701956
Loss made of: CE 0.4681716859340668, LKD 3.310797691345215, LDE 0.0, LReg 0.0, POD 11.773086547851562 EntMin 0.0
Epoch 2, Class Loss=0.6365857720375061, Reg Loss=2.996565103530884
Clinet index 1, End of Epoch 2/6, Average Loss=3.633150815963745, Class Loss=0.6365857720375061, Reg Loss=2.996565103530884
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=15.141008991003037
Loss made of: CE 0.5475896596908569, LKD 2.8958957195281982, LDE 0.0, LReg 0.0, POD 10.649888038635254 EntMin 0.0
Epoch 3, Class Loss=0.4832410514354706, Reg Loss=2.961113452911377
Clinet index 1, End of Epoch 3/6, Average Loss=3.44435453414917, Class Loss=0.4832410514354706, Reg Loss=2.961113452911377
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=15.38669425547123
Loss made of: CE 0.4445429742336273, LKD 3.298365354537964, LDE 0.0, LReg 0.0, POD 11.379356384277344 EntMin 0.0
Epoch 4, Class Loss=0.4240949749946594, Reg Loss=2.9389448165893555
Clinet index 1, End of Epoch 4/6, Average Loss=3.36303973197937, Class Loss=0.4240949749946594, Reg Loss=2.9389448165893555
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/13, Loss=14.572417375445365
Loss made of: CE 0.32322394847869873, LKD 2.9558818340301514, LDE 0.0, LReg 0.0, POD 10.26887321472168 EntMin 0.0
Epoch 5, Class Loss=0.3874378204345703, Reg Loss=2.986532211303711
Clinet index 1, End of Epoch 5/6, Average Loss=3.3739700317382812, Class Loss=0.3874378204345703, Reg Loss=2.986532211303711
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=14.275269463658333
Loss made of: CE 0.373529851436615, LKD 2.975950241088867, LDE 0.0, LReg 0.0, POD 11.8978853225708 EntMin 0.0
Epoch 6, Class Loss=0.3615000247955322, Reg Loss=2.945560932159424
Clinet index 1, End of Epoch 6/6, Average Loss=3.307060956954956, Class Loss=0.3615000247955322, Reg Loss=2.945560932159424
federated aggregation...
Validation, Class Loss=0.39065417647361755, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.872411
Mean Acc: 0.629759
FreqW Acc: 0.796495
Mean IoU: 0.524486
Class IoU:
	class 0: 0.8923099
	class 1: 0.8472285
	class 2: 0.33821627
	class 3: 0.77799976
	class 4: 0.69619846
	class 5: 0.79520524
	class 6: 0.80578226
	class 7: 0.868846
	class 8: 0.69896924
	class 9: 0.061999816
	class 10: 0.0
	class 11: 0.0
	class 12: 0.035566863
Class Acc:
	class 0: 0.96650153
	class 1: 0.8750362
	class 2: 0.92261374
	class 3: 0.7995327
	class 4: 0.78668666
	class 5: 0.8809011
	class 6: 0.8094801
	class 7: 0.93035644
	class 8: 0.94537973
	class 9: 0.23457897
	class 10: 0.0
	class 11: 0.0
	class 12: 0.03579445

federated global round: 12, step: 2
select part of clients to conduct local training
[11, 0, 8, 14]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=18.206608521938325
Loss made of: CE 0.7358726263046265, LKD 2.9975647926330566, LDE 0.0, LReg 0.0, POD 15.261425018310547 EntMin 0.0
Epoch 1, Class Loss=0.6964259147644043, Reg Loss=3.1959168910980225
Clinet index 11, End of Epoch 1/6, Average Loss=3.8923428058624268, Class Loss=0.6964259147644043, Reg Loss=3.1959168910980225
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=16.35059602856636
Loss made of: CE 0.48012757301330566, LKD 2.9763894081115723, LDE 0.0, LReg 0.0, POD 12.784200668334961 EntMin 0.0
Epoch 2, Class Loss=0.5560364723205566, Reg Loss=3.035449743270874
Clinet index 11, End of Epoch 2/6, Average Loss=3.5914862155914307, Class Loss=0.5560364723205566, Reg Loss=3.035449743270874
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=15.23425358235836
Loss made of: CE 0.4789696931838989, LKD 3.140864849090576, LDE 0.0, LReg 0.0, POD 10.65544319152832 EntMin 0.0
Epoch 3, Class Loss=0.434609979391098, Reg Loss=3.0020229816436768
Clinet index 11, End of Epoch 3/6, Average Loss=3.4366328716278076, Class Loss=0.434609979391098, Reg Loss=3.0020229816436768
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=15.139276003837585
Loss made of: CE 0.3861618936061859, LKD 3.072782039642334, LDE 0.0, LReg 0.0, POD 12.35675048828125 EntMin 0.0
Epoch 4, Class Loss=0.3884669840335846, Reg Loss=2.9974091053009033
Clinet index 11, End of Epoch 4/6, Average Loss=3.385876178741455, Class Loss=0.3884669840335846, Reg Loss=2.9974091053009033
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=14.836916291713715
Loss made of: CE 0.3388487696647644, LKD 2.6439895629882812, LDE 0.0, LReg 0.0, POD 11.86278247833252 EntMin 0.0
Epoch 5, Class Loss=0.36722874641418457, Reg Loss=3.001016855239868
Clinet index 11, End of Epoch 5/6, Average Loss=3.3682456016540527, Class Loss=0.36722874641418457, Reg Loss=3.001016855239868
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=14.322241896390915
Loss made of: CE 0.3387376070022583, LKD 2.8116350173950195, LDE 0.0, LReg 0.0, POD 10.174895286560059 EntMin 0.0
Epoch 6, Class Loss=0.3589594066143036, Reg Loss=2.963639497756958
Clinet index 11, End of Epoch 6/6, Average Loss=3.322598934173584, Class Loss=0.3589594066143036, Reg Loss=2.963639497756958
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=23.41794463992119
Loss made of: CE 0.5892821550369263, LKD 5.542593955993652, LDE 0.0, LReg 0.0, POD 17.406835556030273 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/29, Loss=22.054424127936365
Loss made of: CE 0.381134033203125, LKD 5.266570091247559, LDE 0.0, LReg 0.0, POD 14.539068222045898 EntMin 0.0
Epoch 1, Class Loss=0.4830915033817291, Reg Loss=5.511048316955566
Clinet index 0, End of Epoch 1/6, Average Loss=5.994139671325684, Class Loss=0.4830915033817291, Reg Loss=5.511048316955566
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=21.875039657950403
Loss made of: CE 0.3357860743999481, LKD 5.157303333282471, LDE 0.0, LReg 0.0, POD 15.272056579589844 EntMin 0.0
Epoch 2, Batch 20/29, Loss=21.612227872014046
Loss made of: CE 0.36163514852523804, LKD 5.63226842880249, LDE 0.0, LReg 0.0, POD 15.297250747680664 EntMin 0.0
Epoch 2, Class Loss=0.3731536269187927, Reg Loss=5.498099327087402
Clinet index 0, End of Epoch 2/6, Average Loss=5.87125301361084, Class Loss=0.3731536269187927, Reg Loss=5.498099327087402
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=21.623099157214163
Loss made of: CE 0.3159317374229431, LKD 4.956941604614258, LDE 0.0, LReg 0.0, POD 15.045433044433594 EntMin 0.0
Epoch 3, Batch 20/29, Loss=21.082059094309805
Loss made of: CE 0.32782381772994995, LKD 5.4418230056762695, LDE 0.0, LReg 0.0, POD 14.735038757324219 EntMin 0.0
Epoch 3, Class Loss=0.3562777042388916, Reg Loss=5.480067729949951
Clinet index 0, End of Epoch 3/6, Average Loss=5.836345672607422, Class Loss=0.3562777042388916, Reg Loss=5.480067729949951
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=20.848035824298858
Loss made of: CE 0.441238135099411, LKD 6.292601585388184, LDE 0.0, LReg 0.0, POD 17.283775329589844 EntMin 0.0
Epoch 4, Batch 20/29, Loss=21.027394923567773
Loss made of: CE 0.34457433223724365, LKD 5.795741081237793, LDE 0.0, LReg 0.0, POD 14.165953636169434 EntMin 0.0
Epoch 4, Class Loss=0.34360471367836, Reg Loss=5.452146530151367
Clinet index 0, End of Epoch 4/6, Average Loss=5.795751094818115, Class Loss=0.34360471367836, Reg Loss=5.452146530151367
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=20.533932182192803
Loss made of: CE 0.3905492424964905, LKD 5.146986961364746, LDE 0.0, LReg 0.0, POD 16.420690536499023 EntMin 0.0
Epoch 5, Batch 20/29, Loss=20.574211287498475
Loss made of: CE 0.3252088725566864, LKD 5.003638744354248, LDE 0.0, LReg 0.0, POD 15.499473571777344 EntMin 0.0
Epoch 5, Class Loss=0.3431178033351898, Reg Loss=5.519779205322266
Clinet index 0, End of Epoch 5/6, Average Loss=5.862896919250488, Class Loss=0.3431178033351898, Reg Loss=5.519779205322266
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=20.106349119544028
Loss made of: CE 0.27496442198753357, LKD 5.5201849937438965, LDE 0.0, LReg 0.0, POD 14.19285774230957 EntMin 0.0
Epoch 6, Batch 20/29, Loss=20.80693746805191
Loss made of: CE 0.39891692996025085, LKD 5.815319061279297, LDE 0.0, LReg 0.0, POD 14.369171142578125 EntMin 0.0
Epoch 6, Class Loss=0.3408358097076416, Reg Loss=5.488844871520996
Clinet index 0, End of Epoch 6/6, Average Loss=5.829680442810059, Class Loss=0.3408358097076416, Reg Loss=5.488844871520996
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=24.07795832157135
Loss made of: CE 0.5248602032661438, LKD 5.467292785644531, LDE 0.0, LReg 0.0, POD 16.150196075439453 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/29, Loss=22.06103059053421
Loss made of: CE 0.3413184881210327, LKD 5.233821868896484, LDE 0.0, LReg 0.0, POD 16.174640655517578 EntMin 0.0
Epoch 1, Class Loss=0.48356398940086365, Reg Loss=5.52324104309082
Clinet index 8, End of Epoch 1/6, Average Loss=6.006804943084717, Class Loss=0.48356398940086365, Reg Loss=5.52324104309082
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=21.208047938346862
Loss made of: CE 0.3824557662010193, LKD 5.582435131072998, LDE 0.0, LReg 0.0, POD 15.209760665893555 EntMin 0.0
Epoch 2, Batch 20/29, Loss=21.914355185627937
Loss made of: CE 0.4486687183380127, LKD 5.563910961151123, LDE 0.0, LReg 0.0, POD 16.96124267578125 EntMin 0.0
Epoch 2, Class Loss=0.3690836429595947, Reg Loss=5.450634479522705
Clinet index 8, End of Epoch 2/6, Average Loss=5.819718360900879, Class Loss=0.3690836429595947, Reg Loss=5.450634479522705
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=22.084027510881423
Loss made of: CE 0.31757354736328125, LKD 5.64493465423584, LDE 0.0, LReg 0.0, POD 15.688055038452148 EntMin 0.0
Epoch 3, Batch 20/29, Loss=21.561347794532775
Loss made of: CE 0.4011642634868622, LKD 5.771121025085449, LDE 0.0, LReg 0.0, POD 16.133892059326172 EntMin 0.0
Epoch 3, Class Loss=0.35570982098579407, Reg Loss=5.4682769775390625
Clinet index 8, End of Epoch 3/6, Average Loss=5.823987007141113, Class Loss=0.35570982098579407, Reg Loss=5.4682769775390625
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=20.907537984848023
Loss made of: CE 0.29304879903793335, LKD 4.862742900848389, LDE 0.0, LReg 0.0, POD 14.17474365234375 EntMin 0.0
Epoch 4, Batch 20/29, Loss=21.940799191594124
Loss made of: CE 0.36055657267570496, LKD 6.480851173400879, LDE 0.0, LReg 0.0, POD 16.32083511352539 EntMin 0.0
Epoch 4, Class Loss=0.34716469049453735, Reg Loss=5.536868572235107
Clinet index 8, End of Epoch 4/6, Average Loss=5.884033203125, Class Loss=0.34716469049453735, Reg Loss=5.536868572235107
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=20.872132444381712
Loss made of: CE 0.3795888423919678, LKD 5.933588981628418, LDE 0.0, LReg 0.0, POD 14.75153923034668 EntMin 0.0
Epoch 5, Batch 20/29, Loss=20.696516725420953
Loss made of: CE 0.3493983745574951, LKD 5.209021091461182, LDE 0.0, LReg 0.0, POD 14.33489990234375 EntMin 0.0
Epoch 5, Class Loss=0.33232754468917847, Reg Loss=5.429486274719238
Clinet index 8, End of Epoch 5/6, Average Loss=5.761813640594482, Class Loss=0.33232754468917847, Reg Loss=5.429486274719238
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=20.658284613490103
Loss made of: CE 0.2792203426361084, LKD 5.4420576095581055, LDE 0.0, LReg 0.0, POD 14.876827239990234 EntMin 0.0
Epoch 6, Batch 20/29, Loss=21.279982540011407
Loss made of: CE 0.3441269099712372, LKD 5.81977653503418, LDE 0.0, LReg 0.0, POD 16.52490234375 EntMin 0.0
Epoch 6, Class Loss=0.33379653096199036, Reg Loss=5.44609260559082
Clinet index 8, End of Epoch 6/6, Average Loss=5.779889106750488, Class Loss=0.33379653096199036, Reg Loss=5.44609260559082
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=18.62914260625839
Loss made of: CE 0.7152438163757324, LKD 3.412313222885132, LDE 0.0, LReg 0.0, POD 13.409025192260742 EntMin 0.0
Epoch 1, Class Loss=0.7154978513717651, Reg Loss=3.190645217895508
Clinet index 14, End of Epoch 1/6, Average Loss=3.9061431884765625, Class Loss=0.7154978513717651, Reg Loss=3.190645217895508
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/13, Loss=16.216912952065467
Loss made of: CE 0.5104804039001465, LKD 2.9946305751800537, LDE 0.0, LReg 0.0, POD 11.2432861328125 EntMin 0.0
Epoch 2, Class Loss=0.5958513021469116, Reg Loss=2.9970884323120117
Clinet index 14, End of Epoch 2/6, Average Loss=3.592939853668213, Class Loss=0.5958513021469116, Reg Loss=2.9970884323120117
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/13, Loss=15.235689830780029
Loss made of: CE 0.4152284562587738, LKD 3.2544782161712646, LDE 0.0, LReg 0.0, POD 11.817068099975586 EntMin 0.0
Epoch 3, Class Loss=0.4645311236381531, Reg Loss=2.992037296295166
Clinet index 14, End of Epoch 3/6, Average Loss=3.456568479537964, Class Loss=0.4645311236381531, Reg Loss=2.992037296295166
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/13, Loss=14.527310574054718
Loss made of: CE 0.37949252128601074, LKD 2.5262203216552734, LDE 0.0, LReg 0.0, POD 11.219768524169922 EntMin 0.0
Epoch 4, Class Loss=0.39849987626075745, Reg Loss=2.9720945358276367
Clinet index 14, End of Epoch 4/6, Average Loss=3.3705945014953613, Class Loss=0.39849987626075745, Reg Loss=2.9720945358276367
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/13, Loss=14.277971571683883
Loss made of: CE 0.4059779644012451, LKD 3.1030712127685547, LDE 0.0, LReg 0.0, POD 11.34634780883789 EntMin 0.0
Epoch 5, Class Loss=0.37756258249282837, Reg Loss=2.9345784187316895
Clinet index 14, End of Epoch 5/6, Average Loss=3.312140941619873, Class Loss=0.37756258249282837, Reg Loss=2.9345784187316895
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/13, Loss=14.29201794564724
Loss made of: CE 0.36857134103775024, LKD 2.9758825302124023, LDE 0.0, LReg 0.0, POD 9.844602584838867 EntMin 0.0
Epoch 6, Class Loss=0.36457154154777527, Reg Loss=2.9743053913116455
Clinet index 14, End of Epoch 6/6, Average Loss=3.338876962661743, Class Loss=0.36457154154777527, Reg Loss=2.9743053913116455
federated aggregation...
Validation, Class Loss=0.3670925796031952, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.883953
Mean Acc: 0.650042
FreqW Acc: 0.811416
Mean IoU: 0.548228
Class IoU:
	class 0: 0.8979828
	class 1: 0.8459062
	class 2: 0.34766966
	class 3: 0.7905452
	class 4: 0.6992307
	class 5: 0.7986011
	class 6: 0.78384376
	class 7: 0.86405367
	class 8: 0.7351016
	class 9: 0.06869911
	class 10: 0.0
	class 11: 0.0029775298
	class 12: 0.29235798
Class Acc:
	class 0: 0.96852124
	class 1: 0.8708234
	class 2: 0.9213867
	class 3: 0.81236166
	class 4: 0.78436327
	class 5: 0.8913234
	class 6: 0.787071
	class 7: 0.9307623
	class 8: 0.94703734
	class 9: 0.21307348
	class 10: 0.0
	class 11: 0.0029776879
	class 12: 0.32084966

federated global round: 13, step: 2
select part of clients to conduct local training
[13, 5, 15, 7]
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/29, Loss=22.90126188993454
Loss made of: CE 0.3427445888519287, LKD 4.934171676635742, LDE 0.0, LReg 0.0, POD 14.15827751159668 EntMin 0.0
Epoch 1, Batch 20/29, Loss=21.334361004829407
Loss made of: CE 0.3459988832473755, LKD 5.291438579559326, LDE 0.0, LReg 0.0, POD 16.531465530395508 EntMin 0.0
Epoch 1, Class Loss=0.455424964427948, Reg Loss=5.52883768081665
Clinet index 13, End of Epoch 1/6, Average Loss=5.984262466430664, Class Loss=0.455424964427948, Reg Loss=5.52883768081665
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/29, Loss=21.20503624379635
Loss made of: CE 0.4387582242488861, LKD 5.8286967277526855, LDE 0.0, LReg 0.0, POD 13.954898834228516 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/29, Loss=20.581578540802003
Loss made of: CE 0.33525365591049194, LKD 5.611804962158203, LDE 0.0, LReg 0.0, POD 13.755542755126953 EntMin 0.0
Epoch 2, Class Loss=0.3757936656475067, Reg Loss=5.5336079597473145
Clinet index 13, End of Epoch 2/6, Average Loss=5.9094014167785645, Class Loss=0.3757936656475067, Reg Loss=5.5336079597473145
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/29, Loss=20.60847483575344
Loss made of: CE 0.44836553931236267, LKD 5.976369380950928, LDE 0.0, LReg 0.0, POD 15.470264434814453 EntMin 0.0
Epoch 3, Batch 20/29, Loss=21.230233377218248
Loss made of: CE 0.367520809173584, LKD 5.698537349700928, LDE 0.0, LReg 0.0, POD 16.275177001953125 EntMin 0.0
Epoch 3, Class Loss=0.36450937390327454, Reg Loss=5.565019130706787
Clinet index 13, End of Epoch 3/6, Average Loss=5.929528713226318, Class Loss=0.36450937390327454, Reg Loss=5.565019130706787
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/29, Loss=20.78522635400295
Loss made of: CE 0.3791567087173462, LKD 5.575675964355469, LDE 0.0, LReg 0.0, POD 13.613142013549805 EntMin 0.0
Epoch 4, Batch 20/29, Loss=20.343704423308374
Loss made of: CE 0.385926216840744, LKD 5.640250205993652, LDE 0.0, LReg 0.0, POD 12.717111587524414 EntMin 0.0
Epoch 4, Class Loss=0.3506939709186554, Reg Loss=5.548670768737793
Clinet index 13, End of Epoch 4/6, Average Loss=5.899364948272705, Class Loss=0.3506939709186554, Reg Loss=5.548670768737793
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/29, Loss=20.19702053666115
Loss made of: CE 0.32865163683891296, LKD 6.056163787841797, LDE 0.0, LReg 0.0, POD 14.738369941711426 EntMin 0.0
Epoch 5, Batch 20/29, Loss=20.080610060691832
Loss made of: CE 0.35658571124076843, LKD 5.6587629318237305, LDE 0.0, LReg 0.0, POD 14.466270446777344 EntMin 0.0
Epoch 5, Class Loss=0.34709635376930237, Reg Loss=5.494636058807373
Clinet index 13, End of Epoch 5/6, Average Loss=5.841732501983643, Class Loss=0.34709635376930237, Reg Loss=5.494636058807373
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/29, Loss=20.095165807008744
Loss made of: CE 0.3026880919933319, LKD 5.336244583129883, LDE 0.0, LReg 0.0, POD 15.156994819641113 EntMin 0.0
Epoch 6, Batch 20/29, Loss=19.985693764686584
Loss made of: CE 0.33809250593185425, LKD 5.938241004943848, LDE 0.0, LReg 0.0, POD 13.974502563476562 EntMin 0.0
Epoch 6, Class Loss=0.3417057394981384, Reg Loss=5.493618011474609
Clinet index 13, End of Epoch 6/6, Average Loss=5.835323810577393, Class Loss=0.3417057394981384, Reg Loss=5.493618011474609
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=21.34406424462795
Loss made of: CE 0.3830687999725342, LKD 4.965856075286865, LDE 0.0, LReg 0.0, POD 13.973249435424805 EntMin 0.0
Epoch 1, Batch 20/29, Loss=21.476916217803954
Loss made of: CE 0.28969985246658325, LKD 5.415515899658203, LDE 0.0, LReg 0.0, POD 16.15798568725586 EntMin 0.0
Epoch 1, Class Loss=0.4169923961162567, Reg Loss=5.423355579376221
Clinet index 5, End of Epoch 1/6, Average Loss=5.840347766876221, Class Loss=0.4169923961162567, Reg Loss=5.423355579376221
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/29, Loss=21.679332959651948
Loss made of: CE 0.32583919167518616, LKD 5.857387542724609, LDE 0.0, LReg 0.0, POD 15.50722885131836 EntMin 0.0
Epoch 2, Batch 20/29, Loss=21.21048731803894
Loss made of: CE 0.3099921941757202, LKD 5.792586326599121, LDE 0.0, LReg 0.0, POD 15.432483673095703 EntMin 0.0
Epoch 2, Class Loss=0.34511974453926086, Reg Loss=5.431703567504883
Clinet index 5, End of Epoch 2/6, Average Loss=5.7768235206604, Class Loss=0.34511974453926086, Reg Loss=5.431703567504883
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/29, Loss=21.831026473641394
Loss made of: CE 0.3523467779159546, LKD 5.118565559387207, LDE 0.0, LReg 0.0, POD 15.652708053588867 EntMin 0.0
Epoch 3, Batch 20/29, Loss=20.64302692115307
Loss made of: CE 0.2977609634399414, LKD 4.632855415344238, LDE 0.0, LReg 0.0, POD 14.210338592529297 EntMin 0.0
Epoch 3, Class Loss=0.3379999101161957, Reg Loss=5.479936599731445
Clinet index 5, End of Epoch 3/6, Average Loss=5.817936420440674, Class Loss=0.3379999101161957, Reg Loss=5.479936599731445
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/29, Loss=20.708932989835738
Loss made of: CE 0.3232607841491699, LKD 5.498193264007568, LDE 0.0, LReg 0.0, POD 13.570880889892578 EntMin 0.0
Epoch 4, Batch 20/29, Loss=20.400210419297217
Loss made of: CE 0.4039667546749115, LKD 5.5405683517456055, LDE 0.0, LReg 0.0, POD 15.352519989013672 EntMin 0.0
Epoch 4, Class Loss=0.3302268385887146, Reg Loss=5.451114177703857
Clinet index 5, End of Epoch 4/6, Average Loss=5.781341075897217, Class Loss=0.3302268385887146, Reg Loss=5.451114177703857
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=19.803390546143056
Loss made of: CE 0.2878803014755249, LKD 5.709565162658691, LDE 0.0, LReg 0.0, POD 14.305642127990723 EntMin 0.0
Epoch 5, Batch 20/29, Loss=20.591575506329537
Loss made of: CE 0.3501875102519989, LKD 5.368300437927246, LDE 0.0, LReg 0.0, POD 13.522897720336914 EntMin 0.0
Epoch 5, Class Loss=0.3264908194541931, Reg Loss=5.41831111907959
Clinet index 5, End of Epoch 5/6, Average Loss=5.744801998138428, Class Loss=0.3264908194541931, Reg Loss=5.41831111907959
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/29, Loss=20.13730881512165
Loss made of: CE 0.3195927143096924, LKD 5.9380621910095215, LDE 0.0, LReg 0.0, POD 14.458317756652832 EntMin 0.0
Epoch 6, Batch 20/29, Loss=19.979065656661987
Loss made of: CE 0.30378568172454834, LKD 5.216621398925781, LDE 0.0, LReg 0.0, POD 14.251143455505371 EntMin 0.0
Epoch 6, Class Loss=0.3226774334907532, Reg Loss=5.467456817626953
Clinet index 5, End of Epoch 6/6, Average Loss=5.790134429931641, Class Loss=0.3226774334907532, Reg Loss=5.467456817626953
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=18.12434891760349
Loss made of: CE 0.5077046751976013, LKD 2.5819568634033203, LDE 0.0, LReg 0.0, POD 12.715189933776855 EntMin 0.0
Epoch 1, Class Loss=0.5699458122253418, Reg Loss=3.0744307041168213
Clinet index 15, End of Epoch 1/6, Average Loss=3.644376516342163, Class Loss=0.5699458122253418, Reg Loss=3.0744307041168213
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=16.14443733692169
Loss made of: CE 0.43928226828575134, LKD 2.6367223262786865, LDE 0.0, LReg 0.0, POD 12.834497451782227 EntMin 0.0
Epoch 2, Class Loss=0.463138222694397, Reg Loss=2.9659981727600098
Clinet index 15, End of Epoch 2/6, Average Loss=3.429136276245117, Class Loss=0.463138222694397, Reg Loss=2.9659981727600098
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=15.331298705935477
Loss made of: CE 0.4068703055381775, LKD 3.259990692138672, LDE 0.0, LReg 0.0, POD 10.551589965820312 EntMin 0.0
Epoch 3, Class Loss=0.39258265495300293, Reg Loss=2.948868989944458
Clinet index 15, End of Epoch 3/6, Average Loss=3.341451644897461, Class Loss=0.39258265495300293, Reg Loss=2.948868989944458
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=14.700690430402755
Loss made of: CE 0.385738343000412, LKD 3.2079501152038574, LDE 0.0, LReg 0.0, POD 11.421897888183594 EntMin 0.0
Epoch 4, Class Loss=0.3495296835899353, Reg Loss=2.946786642074585
Clinet index 15, End of Epoch 4/6, Average Loss=3.296316385269165, Class Loss=0.3495296835899353, Reg Loss=2.946786642074585
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=14.767796355485917
Loss made of: CE 0.2802494764328003, LKD 2.9330880641937256, LDE 0.0, LReg 0.0, POD 12.19626235961914 EntMin 0.0
Epoch 5, Class Loss=0.3396836817264557, Reg Loss=2.95402193069458
Clinet index 15, End of Epoch 5/6, Average Loss=3.293705701828003, Class Loss=0.3396836817264557, Reg Loss=2.95402193069458
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=14.110809782147408
Loss made of: CE 0.2772172689437866, LKD 2.5595216751098633, LDE 0.0, LReg 0.0, POD 10.43815803527832 EntMin 0.0
Epoch 6, Class Loss=0.3287125527858734, Reg Loss=2.943455696105957
Clinet index 15, End of Epoch 6/6, Average Loss=3.2721681594848633, Class Loss=0.3287125527858734, Reg Loss=2.943455696105957
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=18.166100484132766
Loss made of: CE 0.5164574384689331, LKD 2.64675235748291, LDE 0.0, LReg 0.0, POD 14.231403350830078 EntMin 0.0
Epoch 1, Class Loss=0.6086705327033997, Reg Loss=3.128783702850342
Clinet index 7, End of Epoch 1/6, Average Loss=3.7374541759490967, Class Loss=0.6086705327033997, Reg Loss=3.128783702850342
Pseudo labeling is: None
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/13, Loss=16.154178446531297
Loss made of: CE 0.47646206617355347, LKD 3.219482421875, LDE 0.0, LReg 0.0, POD 11.604799270629883 EntMin 0.0
Epoch 2, Class Loss=0.5251590013504028, Reg Loss=2.99859356880188
Clinet index 7, End of Epoch 2/6, Average Loss=3.5237526893615723, Class Loss=0.5251590013504028, Reg Loss=2.99859356880188
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=15.095010524988174
Loss made of: CE 0.36669549345970154, LKD 3.2907447814941406, LDE 0.0, LReg 0.0, POD 12.22916030883789 EntMin 0.0
Epoch 3, Class Loss=0.43278101086616516, Reg Loss=2.9162962436676025
Clinet index 7, End of Epoch 3/6, Average Loss=3.3490772247314453, Class Loss=0.43278101086616516, Reg Loss=2.9162962436676025
Pseudo labeling is: None
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/13, Loss=14.892175301909447
Loss made of: CE 0.41276809573173523, LKD 3.026334762573242, LDE 0.0, LReg 0.0, POD 11.037704467773438 EntMin 0.0
Epoch 4, Class Loss=0.3798382878303528, Reg Loss=2.951763868331909
Clinet index 7, End of Epoch 4/6, Average Loss=3.331602096557617, Class Loss=0.3798382878303528, Reg Loss=2.951763868331909
Pseudo labeling is: None
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/13, Loss=14.87089662849903
Loss made of: CE 0.3470732569694519, LKD 2.5474250316619873, LDE 0.0, LReg 0.0, POD 11.664566040039062 EntMin 0.0
Epoch 5, Class Loss=0.3753828704357147, Reg Loss=2.992703437805176
Clinet index 7, End of Epoch 5/6, Average Loss=3.368086338043213, Class Loss=0.3753828704357147, Reg Loss=2.992703437805176
Pseudo labeling is: None
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/13, Loss=14.819180977344512
Loss made of: CE 0.3611738085746765, LKD 2.4836597442626953, LDE 0.0, LReg 0.0, POD 11.78853702545166 EntMin 0.0
Epoch 6, Class Loss=0.3580152094364166, Reg Loss=2.9456374645233154
Clinet index 7, End of Epoch 6/6, Average Loss=3.303652763366699, Class Loss=0.3580152094364166, Reg Loss=2.9456374645233154
federated aggregation...
Validation, Class Loss=0.3669140338897705, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.886777
Mean Acc: 0.659418
FreqW Acc: 0.817751
Mean IoU: 0.556769
Class IoU:
	class 0: 0.90157604
	class 1: 0.8414702
	class 2: 0.34847626
	class 3: 0.7851294
	class 4: 0.6953009
	class 5: 0.79896235
	class 6: 0.7649619
	class 7: 0.8635037
	class 8: 0.742274
	class 9: 0.07201779
	class 10: 0.0
	class 11: 0.039682228
	class 12: 0.38464695
Class Acc:
	class 0: 0.9664441
	class 1: 0.8637925
	class 2: 0.9223564
	class 3: 0.80362874
	class 4: 0.7705371
	class 5: 0.8882009
	class 6: 0.76785
	class 7: 0.93103683
	class 8: 0.94539773
	class 9: 0.2276372
	class 10: 0.0
	class 11: 0.039745506
	class 12: 0.44580075

federated global round: 14, step: 2
select part of clients to conduct local training
[17, 3, 12, 16]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=21.992849904298783
Loss made of: CE 0.3361278176307678, LKD 4.922486782073975, LDE 0.0, LReg 0.0, POD 13.677005767822266 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/29, Loss=22.51964690685272
Loss made of: CE 0.3464275300502777, LKD 5.457475185394287, LDE 0.0, LReg 0.0, POD 15.045408248901367 EntMin 0.0
Epoch 1, Class Loss=0.43325677514076233, Reg Loss=5.665682315826416
Clinet index 17, End of Epoch 1/6, Average Loss=6.098938941955566, Class Loss=0.43325677514076233, Reg Loss=5.665682315826416
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=20.941553762555124
Loss made of: CE 0.39596426486968994, LKD 5.833177089691162, LDE 0.0, LReg 0.0, POD 15.386004447937012 EntMin 0.0
Epoch 2, Batch 20/29, Loss=21.01873580813408
Loss made of: CE 0.4162599742412567, LKD 5.6491522789001465, LDE 0.0, LReg 0.0, POD 14.457016944885254 EntMin 0.0
Epoch 2, Class Loss=0.3553841710090637, Reg Loss=5.547010898590088
Clinet index 17, End of Epoch 2/6, Average Loss=5.902395248413086, Class Loss=0.3553841710090637, Reg Loss=5.547010898590088
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=19.821582245826722
Loss made of: CE 0.28897592425346375, LKD 5.387122631072998, LDE 0.0, LReg 0.0, POD 13.09567642211914 EntMin 0.0
Epoch 3, Batch 20/29, Loss=20.899169450998308
Loss made of: CE 0.3897603750228882, LKD 5.361049652099609, LDE 0.0, LReg 0.0, POD 14.25316047668457 EntMin 0.0
Epoch 3, Class Loss=0.34886208176612854, Reg Loss=5.577966690063477
Clinet index 17, End of Epoch 3/6, Average Loss=5.926828861236572, Class Loss=0.34886208176612854, Reg Loss=5.577966690063477
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=19.600535666942598
Loss made of: CE 0.37260788679122925, LKD 6.016287803649902, LDE 0.0, LReg 0.0, POD 13.045635223388672 EntMin 0.0
Epoch 4, Batch 20/29, Loss=20.24374462366104
Loss made of: CE 0.3526742458343506, LKD 5.886831283569336, LDE 0.0, LReg 0.0, POD 15.30459976196289 EntMin 0.0
Epoch 4, Class Loss=0.3425944745540619, Reg Loss=5.54299783706665
Clinet index 17, End of Epoch 4/6, Average Loss=5.885592460632324, Class Loss=0.3425944745540619, Reg Loss=5.54299783706665
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=19.849032083153723
Loss made of: CE 0.27982044219970703, LKD 5.373315334320068, LDE 0.0, LReg 0.0, POD 13.136533737182617 EntMin 0.0
Epoch 5, Batch 20/29, Loss=19.777658665180205
Loss made of: CE 0.33813905715942383, LKD 5.882476806640625, LDE 0.0, LReg 0.0, POD 13.930895805358887 EntMin 0.0
Epoch 5, Class Loss=0.344388872385025, Reg Loss=5.54072380065918
Clinet index 17, End of Epoch 5/6, Average Loss=5.885112762451172, Class Loss=0.344388872385025, Reg Loss=5.54072380065918
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=19.701365822553633
Loss made of: CE 0.33928483724594116, LKD 5.3593878746032715, LDE 0.0, LReg 0.0, POD 12.970497131347656 EntMin 0.0
Epoch 6, Batch 20/29, Loss=19.02870403826237
Loss made of: CE 0.2948262393474579, LKD 5.3456878662109375, LDE 0.0, LReg 0.0, POD 12.766328811645508 EntMin 0.0
Epoch 6, Class Loss=0.3417282700538635, Reg Loss=5.506338596343994
Clinet index 17, End of Epoch 6/6, Average Loss=5.848066806793213, Class Loss=0.3417282700538635, Reg Loss=5.506338596343994
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=22.525405371189116
Loss made of: CE 0.3770049512386322, LKD 5.126756191253662, LDE 0.0, LReg 0.0, POD 15.41776180267334 EntMin 0.0
Epoch 1, Batch 20/29, Loss=20.508929699659348
Loss made of: CE 0.3827141225337982, LKD 5.820313930511475, LDE 0.0, LReg 0.0, POD 14.547746658325195 EntMin 0.0
Epoch 1, Class Loss=0.4124034643173218, Reg Loss=5.548851490020752
Clinet index 3, End of Epoch 1/6, Average Loss=5.961255073547363, Class Loss=0.4124034643173218, Reg Loss=5.548851490020752
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/29, Loss=20.83276882469654
Loss made of: CE 0.33210277557373047, LKD 4.917238235473633, LDE 0.0, LReg 0.0, POD 14.043546676635742 EntMin 0.0
Epoch 2, Batch 20/29, Loss=20.504159995913504
Loss made of: CE 0.4182935655117035, LKD 5.542172908782959, LDE 0.0, LReg 0.0, POD 15.393071174621582 EntMin 0.0
Epoch 2, Class Loss=0.34789130091667175, Reg Loss=5.523310661315918
Clinet index 3, End of Epoch 2/6, Average Loss=5.871201992034912, Class Loss=0.34789130091667175, Reg Loss=5.523310661315918
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=20.911924028396605
Loss made of: CE 0.3198389708995819, LKD 5.539212703704834, LDE 0.0, LReg 0.0, POD 13.58340835571289 EntMin 0.0
Epoch 3, Batch 20/29, Loss=20.50308048427105
Loss made of: CE 0.3552199602127075, LKD 5.540007591247559, LDE 0.0, LReg 0.0, POD 15.526182174682617 EntMin 0.0
Epoch 3, Class Loss=0.3500148355960846, Reg Loss=5.604142665863037
Clinet index 3, End of Epoch 3/6, Average Loss=5.95415735244751, Class Loss=0.3500148355960846, Reg Loss=5.604142665863037
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=20.8122415214777
Loss made of: CE 0.38458549976348877, LKD 5.013695240020752, LDE 0.0, LReg 0.0, POD 14.392589569091797 EntMin 0.0
Epoch 4, Batch 20/29, Loss=19.933655509352683
Loss made of: CE 0.3862341344356537, LKD 5.631108283996582, LDE 0.0, LReg 0.0, POD 14.986916542053223 EntMin 0.0
Epoch 4, Class Loss=0.33709007501602173, Reg Loss=5.519458770751953
Clinet index 3, End of Epoch 4/6, Average Loss=5.85654878616333, Class Loss=0.33709007501602173, Reg Loss=5.519458770751953
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=19.540512242913245
Loss made of: CE 0.27547863125801086, LKD 5.032057762145996, LDE 0.0, LReg 0.0, POD 13.405813217163086 EntMin 0.0
Epoch 5, Batch 20/29, Loss=19.800387439131736
Loss made of: CE 0.3192339539527893, LKD 5.481797218322754, LDE 0.0, LReg 0.0, POD 14.796010971069336 EntMin 0.0
Epoch 5, Class Loss=0.3320585787296295, Reg Loss=5.495728969573975
Clinet index 3, End of Epoch 5/6, Average Loss=5.827787399291992, Class Loss=0.3320585787296295, Reg Loss=5.495728969573975
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=19.486892476677895
Loss made of: CE 0.36304590106010437, LKD 5.702960014343262, LDE 0.0, LReg 0.0, POD 14.516234397888184 EntMin 0.0
Epoch 6, Batch 20/29, Loss=18.888609778881072
Loss made of: CE 0.3627603054046631, LKD 4.854567527770996, LDE 0.0, LReg 0.0, POD 14.446277618408203 EntMin 0.0
Epoch 6, Class Loss=0.3394920527935028, Reg Loss=5.417774677276611
Clinet index 3, End of Epoch 6/6, Average Loss=5.757266521453857, Class Loss=0.3394920527935028, Reg Loss=5.417774677276611
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=17.689111828804016
Loss made of: CE 0.4304956793785095, LKD 2.6247646808624268, LDE 0.0, LReg 0.0, POD 11.377357482910156 EntMin 0.0
Epoch 1, Class Loss=0.5630707740783691, Reg Loss=3.0834875106811523
Clinet index 12, End of Epoch 1/6, Average Loss=3.6465582847595215, Class Loss=0.5630707740783691, Reg Loss=3.0834875106811523
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/13, Loss=15.64804679453373
Loss made of: CE 0.45171546936035156, LKD 2.525104522705078, LDE 0.0, LReg 0.0, POD 12.190011024475098 EntMin 0.0
Epoch 2, Class Loss=0.44646188616752625, Reg Loss=2.9686279296875
Clinet index 12, End of Epoch 2/6, Average Loss=3.4150898456573486, Class Loss=0.44646188616752625, Reg Loss=2.9686279296875
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=15.27479692697525
Loss made of: CE 0.3368435800075531, LKD 2.4248223304748535, LDE 0.0, LReg 0.0, POD 11.823741912841797 EntMin 0.0
Epoch 3, Class Loss=0.3894332945346832, Reg Loss=2.9575371742248535
Clinet index 12, End of Epoch 3/6, Average Loss=3.346970558166504, Class Loss=0.3894332945346832, Reg Loss=2.9575371742248535
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=14.678942042589188
Loss made of: CE 0.3353583812713623, LKD 2.9168155193328857, LDE 0.0, LReg 0.0, POD 11.017927169799805 EntMin 0.0
Epoch 4, Class Loss=0.3748149275779724, Reg Loss=2.947404623031616
Clinet index 12, End of Epoch 4/6, Average Loss=3.3222196102142334, Class Loss=0.3748149275779724, Reg Loss=2.947404623031616
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=14.510563147068023
Loss made of: CE 0.3924250602722168, LKD 2.4629745483398438, LDE 0.0, LReg 0.0, POD 13.311990737915039 EntMin 0.0
Epoch 5, Class Loss=0.3598286211490631, Reg Loss=2.93326735496521
Clinet index 12, End of Epoch 5/6, Average Loss=3.2930960655212402, Class Loss=0.3598286211490631, Reg Loss=2.93326735496521
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=14.226498186588287
Loss made of: CE 0.3495757579803467, LKD 2.8752315044403076, LDE 0.0, LReg 0.0, POD 10.597386360168457 EntMin 0.0
Epoch 6, Class Loss=0.3486262261867523, Reg Loss=2.919530153274536
Clinet index 12, End of Epoch 6/6, Average Loss=3.2681562900543213, Class Loss=0.3486262261867523, Reg Loss=2.919530153274536
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=22.83856794834137
Loss made of: CE 0.4077284336090088, LKD 5.475844383239746, LDE 0.0, LReg 0.0, POD 14.921039581298828 EntMin 0.0
Epoch 1, Batch 20/29, Loss=20.6713888913393
Loss made of: CE 0.38696926832199097, LKD 5.742195129394531, LDE 0.0, LReg 0.0, POD 14.228870391845703 EntMin 0.0
Epoch 1, Class Loss=0.4201247990131378, Reg Loss=5.506837844848633
Clinet index 16, End of Epoch 1/6, Average Loss=5.926962852478027, Class Loss=0.4201247990131378, Reg Loss=5.506837844848633
Pseudo labeling is: None
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/29, Loss=20.2926023542881
Loss made of: CE 0.30281001329421997, LKD 5.184356689453125, LDE 0.0, LReg 0.0, POD 16.012962341308594 EntMin 0.0
Epoch 2, Batch 20/29, Loss=20.72027108967304
Loss made of: CE 0.33539146184921265, LKD 5.481074333190918, LDE 0.0, LReg 0.0, POD 14.479391098022461 EntMin 0.0
Epoch 2, Class Loss=0.3572981357574463, Reg Loss=5.484966278076172
Clinet index 16, End of Epoch 2/6, Average Loss=5.842264175415039, Class Loss=0.3572981357574463, Reg Loss=5.484966278076172
Pseudo labeling is: None
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/29, Loss=20.474387723207474
Loss made of: CE 0.4050980508327484, LKD 6.4672064781188965, LDE 0.0, LReg 0.0, POD 15.284795761108398 EntMin 0.0
Epoch 3, Batch 20/29, Loss=19.69825038611889
Loss made of: CE 0.3070259690284729, LKD 4.705895900726318, LDE 0.0, LReg 0.0, POD 15.59582805633545 EntMin 0.0
Epoch 3, Class Loss=0.3458089232444763, Reg Loss=5.483623504638672
Clinet index 16, End of Epoch 3/6, Average Loss=5.829432487487793, Class Loss=0.3458089232444763, Reg Loss=5.483623504638672
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/29, Loss=19.676089164614677
Loss made of: CE 0.2916634976863861, LKD 4.824409484863281, LDE 0.0, LReg 0.0, POD 13.02591323852539 EntMin 0.0
Epoch 4, Batch 20/29, Loss=19.706588000059128
Loss made of: CE 0.304794043302536, LKD 5.572612762451172, LDE 0.0, LReg 0.0, POD 13.126688957214355 EntMin 0.0
Epoch 4, Class Loss=0.3363201916217804, Reg Loss=5.468863487243652
Clinet index 16, End of Epoch 4/6, Average Loss=5.8051838874816895, Class Loss=0.3363201916217804, Reg Loss=5.468863487243652
Pseudo labeling is: None
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/29, Loss=19.34885740876198
Loss made of: CE 0.3524765372276306, LKD 5.817529201507568, LDE 0.0, LReg 0.0, POD 14.481941223144531 EntMin 0.0
Epoch 5, Batch 20/29, Loss=19.51135499179363
Loss made of: CE 0.2754037082195282, LKD 4.589719295501709, LDE 0.0, LReg 0.0, POD 13.881078720092773 EntMin 0.0
Epoch 5, Class Loss=0.34236791729927063, Reg Loss=5.486912250518799
Clinet index 16, End of Epoch 5/6, Average Loss=5.829280376434326, Class Loss=0.34236791729927063, Reg Loss=5.486912250518799
Pseudo labeling is: None
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/29, Loss=19.210591742396353
Loss made of: CE 0.33655235171318054, LKD 5.301144123077393, LDE 0.0, LReg 0.0, POD 12.953279495239258 EntMin 0.0
Epoch 6, Batch 20/29, Loss=18.962757542729378
Loss made of: CE 0.3033568263053894, LKD 5.003991603851318, LDE 0.0, LReg 0.0, POD 11.67140007019043 EntMin 0.0
Epoch 6, Class Loss=0.3341882526874542, Reg Loss=5.480253219604492
Clinet index 16, End of Epoch 6/6, Average Loss=5.814441680908203, Class Loss=0.3341882526874542, Reg Loss=5.480253219604492
federated aggregation...
Validation, Class Loss=0.3634676933288574, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.895231
Mean Acc: 0.675136
FreqW Acc: 0.827955
Mean IoU: 0.568113
Class IoU:
	class 0: 0.9071913
	class 1: 0.83672816
	class 2: 0.34519184
	class 3: 0.78196794
	class 4: 0.6903354
	class 5: 0.7940846
	class 6: 0.79469424
	class 7: 0.86542386
	class 8: 0.78852767
	class 9: 0.08882517
	class 10: 0.0
	class 11: 0.006588194
	class 12: 0.4859105
Class Acc:
	class 0: 0.9652881
	class 1: 0.8550214
	class 2: 0.91868824
	class 3: 0.80033404
	class 4: 0.7631085
	class 5: 0.88046235
	class 6: 0.7980088
	class 7: 0.93278074
	class 8: 0.9356962
	class 9: 0.20712852
	class 10: 0.0
	class 11: 0.006588886
	class 12: 0.7136632

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[13, 9, 6, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=28.367558932304384
Loss made of: CE 1.0762219429016113, LKD 7.175281047821045, LDE 0.0, LReg 0.0, POD 18.466293334960938 EntMin 0.0
Epoch 1, Class Loss=1.2818979024887085, Reg Loss=6.875521183013916
Clinet index 13, End of Epoch 1/6, Average Loss=8.157419204711914, Class Loss=1.2818979024887085, Reg Loss=6.875521183013916
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=24.681854170560836
Loss made of: CE 0.8125011920928955, LKD 6.259517669677734, LDE 0.0, LReg 0.0, POD 16.202720642089844 EntMin 0.0
Epoch 2, Class Loss=0.9364520907402039, Reg Loss=6.761689186096191
Clinet index 13, End of Epoch 2/6, Average Loss=7.698141098022461, Class Loss=0.9364520907402039, Reg Loss=6.761689186096191
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=23.036734694242476
Loss made of: CE 0.7992995977401733, LKD 6.89460563659668, LDE 0.0, LReg 0.0, POD 16.18114471435547 EntMin 0.0
Epoch 3, Class Loss=0.7776789665222168, Reg Loss=6.687732696533203
Clinet index 13, End of Epoch 3/6, Average Loss=7.46541166305542, Class Loss=0.7776789665222168, Reg Loss=6.687732696533203
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/11, Loss=22.09713362455368
Loss made of: CE 0.755189061164856, LKD 7.531031608581543, LDE 0.0, LReg 0.0, POD 14.312105178833008 EntMin 0.0
Epoch 4, Class Loss=0.7059793472290039, Reg Loss=6.705532073974609
Clinet index 13, End of Epoch 4/6, Average Loss=7.411511421203613, Class Loss=0.7059793472290039, Reg Loss=6.705532073974609
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/11, Loss=21.297214597463608
Loss made of: CE 0.6482062339782715, LKD 6.300683498382568, LDE 0.0, LReg 0.0, POD 14.645389556884766 EntMin 0.0
Epoch 5, Class Loss=0.6439022421836853, Reg Loss=6.562941551208496
Clinet index 13, End of Epoch 5/6, Average Loss=7.206843852996826, Class Loss=0.6439022421836853, Reg Loss=6.562941551208496
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 10/11, Loss=20.739719605445863
Loss made of: CE 0.588623046875, LKD 6.9070892333984375, LDE 0.0, LReg 0.0, POD 13.328853607177734 EntMin 0.0
Epoch 6, Class Loss=0.5900559425354004, Reg Loss=6.6113080978393555
Clinet index 13, End of Epoch 6/6, Average Loss=7.201364040374756, Class Loss=0.5900559425354004, Reg Loss=6.6113080978393555
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=33.9262554526329
Loss made of: CE 1.1701374053955078, LKD 7.673834800720215, LDE 0.0, LReg 0.0, POD 21.896991729736328 EntMin 0.0
Epoch 1, Class Loss=1.1974422931671143, Reg Loss=7.713794231414795
Clinet index 9, End of Epoch 1/6, Average Loss=8.911236763000488, Class Loss=1.1974422931671143, Reg Loss=7.713794231414795
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=27.23199067115784
Loss made of: CE 0.7520900964736938, LKD 6.414559841156006, LDE 0.0, LReg 0.0, POD 17.974275588989258 EntMin 0.0
Epoch 2, Class Loss=0.8881685137748718, Reg Loss=7.003053665161133
Clinet index 9, End of Epoch 2/6, Average Loss=7.89122200012207, Class Loss=0.8881685137748718, Reg Loss=7.003053665161133
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=24.81813383102417
Loss made of: CE 0.7314152717590332, LKD 7.208014965057373, LDE 0.0, LReg 0.0, POD 16.942981719970703 EntMin 0.0
Epoch 3, Class Loss=0.725849986076355, Reg Loss=6.907870292663574
Clinet index 9, End of Epoch 3/6, Average Loss=7.633720397949219, Class Loss=0.725849986076355, Reg Loss=6.907870292663574
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=23.160114842653275
Loss made of: CE 0.5225185751914978, LKD 6.118167400360107, LDE 0.0, LReg 0.0, POD 13.857551574707031 EntMin 0.0
Epoch 4, Class Loss=0.6345682740211487, Reg Loss=6.816848278045654
Clinet index 9, End of Epoch 4/6, Average Loss=7.451416492462158, Class Loss=0.6345682740211487, Reg Loss=6.816848278045654
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=22.574455088377
Loss made of: CE 0.586527407169342, LKD 6.928229331970215, LDE 0.0, LReg 0.0, POD 15.925951957702637 EntMin 0.0
Epoch 5, Class Loss=0.5714298486709595, Reg Loss=6.753989219665527
Clinet index 9, End of Epoch 5/6, Average Loss=7.325418949127197, Class Loss=0.5714298486709595, Reg Loss=6.753989219665527
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=22.073080205917357
Loss made of: CE 0.4279240071773529, LKD 6.061483383178711, LDE 0.0, LReg 0.0, POD 13.58092212677002 EntMin 0.0
Epoch 6, Class Loss=0.5388129353523254, Reg Loss=6.763962745666504
Clinet index 9, End of Epoch 6/6, Average Loss=7.302775859832764, Class Loss=0.5388129353523254, Reg Loss=6.763962745666504
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=28.416492235660552
Loss made of: CE 1.1804296970367432, LKD 7.105679512023926, LDE 0.0, LReg 0.0, POD 18.75665283203125 EntMin 0.0
Epoch 1, Class Loss=1.2542023658752441, Reg Loss=6.822048187255859
Clinet index 6, End of Epoch 1/6, Average Loss=8.076250076293945, Class Loss=1.2542023658752441, Reg Loss=6.822048187255859
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=24.824426090717317
Loss made of: CE 0.9874187707901001, LKD 7.500335216522217, LDE 0.0, LReg 0.0, POD 17.376384735107422 EntMin 0.0
Epoch 2, Class Loss=0.9437541961669922, Reg Loss=6.698599815368652
Clinet index 6, End of Epoch 2/6, Average Loss=7.6423540115356445, Class Loss=0.9437541961669922, Reg Loss=6.698599815368652
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=23.0727488219738
Loss made of: CE 0.8042025566101074, LKD 6.804740905761719, LDE 0.0, LReg 0.0, POD 15.633292198181152 EntMin 0.0
Epoch 3, Class Loss=0.7947219014167786, Reg Loss=6.63995885848999
Clinet index 6, End of Epoch 3/6, Average Loss=7.434680938720703, Class Loss=0.7947219014167786, Reg Loss=6.63995885848999
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/11, Loss=21.77321856021881
Loss made of: CE 0.733087420463562, LKD 6.716472625732422, LDE 0.0, LReg 0.0, POD 16.792095184326172 EntMin 0.0
Epoch 4, Class Loss=0.7074813842773438, Reg Loss=6.567243576049805
Clinet index 6, End of Epoch 4/6, Average Loss=7.274724960327148, Class Loss=0.7074813842773438, Reg Loss=6.567243576049805
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/11, Loss=21.311864531040193
Loss made of: CE 0.4790348410606384, LKD 5.66945219039917, LDE 0.0, LReg 0.0, POD 13.567346572875977 EntMin 0.0
Epoch 5, Class Loss=0.6256530284881592, Reg Loss=6.597621917724609
Clinet index 6, End of Epoch 5/6, Average Loss=7.223275184631348, Class Loss=0.6256530284881592, Reg Loss=6.597621917724609
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=20.744294899702073
Loss made of: CE 0.5513795614242554, LKD 6.3819990158081055, LDE 0.0, LReg 0.0, POD 12.962715148925781 EntMin 0.0
Epoch 6, Class Loss=0.5983856320381165, Reg Loss=6.603873252868652
Clinet index 6, End of Epoch 6/6, Average Loss=7.202259063720703, Class Loss=0.5983856320381165, Reg Loss=6.603873252868652
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=34.033887159824374
Loss made of: CE 1.2196495532989502, LKD 6.9782915115356445, LDE 0.0, LReg 0.0, POD 22.35152816772461 EntMin 0.0
Epoch 1, Class Loss=1.210709571838379, Reg Loss=7.740298748016357
Clinet index 5, End of Epoch 1/6, Average Loss=8.951007843017578, Class Loss=1.210709571838379, Reg Loss=7.740298748016357
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=27.635786575078964
Loss made of: CE 0.8308104872703552, LKD 7.256131649017334, LDE 0.0, LReg 0.0, POD 18.869617462158203 EntMin 0.0
Epoch 2, Class Loss=0.901774525642395, Reg Loss=7.126805305480957
Clinet index 5, End of Epoch 2/6, Average Loss=8.028579711914062, Class Loss=0.901774525642395, Reg Loss=7.126805305480957
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=24.90528177022934
Loss made of: CE 0.7665928602218628, LKD 7.477347373962402, LDE 0.0, LReg 0.0, POD 16.901439666748047 EntMin 0.0
Epoch 3, Class Loss=0.7347289323806763, Reg Loss=6.9839301109313965
Clinet index 5, End of Epoch 3/6, Average Loss=7.718658924102783, Class Loss=0.7347289323806763, Reg Loss=6.9839301109313965
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=23.631283628940583
Loss made of: CE 0.5528368353843689, LKD 6.953124046325684, LDE 0.0, LReg 0.0, POD 15.328226089477539 EntMin 0.0
Epoch 4, Class Loss=0.6417880058288574, Reg Loss=6.871827602386475
Clinet index 5, End of Epoch 4/6, Average Loss=7.513615608215332, Class Loss=0.6417880058288574, Reg Loss=6.871827602386475
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=22.20191955268383
Loss made of: CE 0.4775521755218506, LKD 5.958440780639648, LDE 0.0, LReg 0.0, POD 13.319189071655273 EntMin 0.0
Epoch 5, Class Loss=0.5785109996795654, Reg Loss=6.8179144859313965
Clinet index 5, End of Epoch 5/6, Average Loss=7.396425247192383, Class Loss=0.5785109996795654, Reg Loss=6.8179144859313965
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=21.622853368520737
Loss made of: CE 0.625458300113678, LKD 7.731547832489014, LDE 0.0, LReg 0.0, POD 14.144814491271973 EntMin 0.0
Epoch 6, Class Loss=0.5432798862457275, Reg Loss=6.871862411499023
Clinet index 5, End of Epoch 6/6, Average Loss=7.415142059326172, Class Loss=0.5432798862457275, Reg Loss=6.871862411499023
federated aggregation...
Validation, Class Loss=0.5325416326522827, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.859136
Mean Acc: 0.535052
FreqW Acc: 0.783614
Mean IoU: 0.440541
Class IoU:
	class 0: 0.8967152
	class 1: 0.73331684
	class 2: 0.30717665
	class 3: 0.680107
	class 4: 0.62877744
	class 5: 0.7339909
	class 6: 0.6835809
	class 7: 0.8081648
	class 8: 0.708132
	class 9: 0.04778948
	class 10: 0.0
	class 11: 0.0003698589
	class 12: 0.37998626
	class 13: 0.0
	class 14: 0.0
Class Acc:
	class 0: 0.9710444
	class 1: 0.7411682
	class 2: 0.9121969
	class 3: 0.6877636
	class 4: 0.6774042
	class 5: 0.79555804
	class 6: 0.68562955
	class 7: 0.9386948
	class 8: 0.78217465
	class 9: 0.21495815
	class 10: 0.0
	class 11: 0.00036986117
	class 12: 0.6188138
	class 13: 0.0
	class 14: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[18, 5, 20, 0]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=24.248043370246886
Loss made of: CE 0.7019674777984619, LKD 6.418496131896973, LDE 0.0, LReg 0.0, POD 14.075370788574219 EntMin 0.0
Epoch 1, Class Loss=0.8660092353820801, Reg Loss=6.963822841644287
Clinet index 18, End of Epoch 1/6, Average Loss=7.829832077026367, Class Loss=0.8660092353820801, Reg Loss=6.963822841644287
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=22.717561054229737
Loss made of: CE 0.6379344463348389, LKD 7.074668884277344, LDE 0.0, LReg 0.0, POD 14.976712226867676 EntMin 0.0
Epoch 2, Class Loss=0.6487647891044617, Reg Loss=6.905456066131592
Clinet index 18, End of Epoch 2/6, Average Loss=7.554220676422119, Class Loss=0.6487647891044617, Reg Loss=6.905456066131592
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=21.207144576311112
Loss made of: CE 0.4769293963909149, LKD 6.433997631072998, LDE 0.0, LReg 0.0, POD 13.53398323059082 EntMin 0.0
Epoch 3, Class Loss=0.5467265844345093, Reg Loss=6.725926399230957
Clinet index 18, End of Epoch 3/6, Average Loss=7.272653102874756, Class Loss=0.5467265844345093, Reg Loss=6.725926399230957
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=21.012848433852195
Loss made of: CE 0.4723101258277893, LKD 6.652011871337891, LDE 0.0, LReg 0.0, POD 12.827398300170898 EntMin 0.0
Epoch 4, Class Loss=0.5229021310806274, Reg Loss=6.72666072845459
Clinet index 18, End of Epoch 4/6, Average Loss=7.249562740325928, Class Loss=0.5229021310806274, Reg Loss=6.72666072845459
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=19.961842036247255
Loss made of: CE 0.4116978049278259, LKD 6.183177947998047, LDE 0.0, LReg 0.0, POD 12.297691345214844 EntMin 0.0
Epoch 5, Class Loss=0.4995783567428589, Reg Loss=6.734912395477295
Clinet index 18, End of Epoch 5/6, Average Loss=7.234490871429443, Class Loss=0.4995783567428589, Reg Loss=6.734912395477295
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=19.8749945551157
Loss made of: CE 0.41832661628723145, LKD 6.473052024841309, LDE 0.0, LReg 0.0, POD 13.395766258239746 EntMin 0.0
Epoch 6, Class Loss=0.49424082040786743, Reg Loss=6.7970991134643555
Clinet index 18, End of Epoch 6/6, Average Loss=7.291339874267578, Class Loss=0.49424082040786743, Reg Loss=6.7970991134643555
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/12, Loss=24.27157855629921
Loss made of: CE 0.8672207593917847, LKD 6.601559638977051, LDE 0.0, LReg 0.0, POD 16.272663116455078 EntMin 0.0
Epoch 1, Class Loss=0.8694261908531189, Reg Loss=6.972091197967529
Clinet index 5, End of Epoch 1/6, Average Loss=7.841517448425293, Class Loss=0.8694261908531189, Reg Loss=6.972091197967529
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=22.573455727100374
Loss made of: CE 0.6549975872039795, LKD 7.099793910980225, LDE 0.0, LReg 0.0, POD 15.165643692016602 EntMin 0.0
Epoch 2, Class Loss=0.6781845092773438, Reg Loss=6.870386123657227
Clinet index 5, End of Epoch 2/6, Average Loss=7.54857063293457, Class Loss=0.6781845092773438, Reg Loss=6.870386123657227
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=21.42991453409195
Loss made of: CE 0.6346317529678345, LKD 7.184582710266113, LDE 0.0, LReg 0.0, POD 14.584980964660645 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.5647854804992676, Reg Loss=6.846261501312256
Clinet index 5, End of Epoch 3/6, Average Loss=7.411046981811523, Class Loss=0.5647854804992676, Reg Loss=6.846261501312256
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=21.246397519111632
Loss made of: CE 0.48516178131103516, LKD 6.966363906860352, LDE 0.0, LReg 0.0, POD 13.338886260986328 EntMin 0.0
Epoch 4, Class Loss=0.5384644269943237, Reg Loss=6.832747459411621
Clinet index 5, End of Epoch 4/6, Average Loss=7.371212005615234, Class Loss=0.5384644269943237, Reg Loss=6.832747459411621
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=20.588061824440956
Loss made of: CE 0.41819819808006287, LKD 5.98939847946167, LDE 0.0, LReg 0.0, POD 12.282258987426758 EntMin 0.0
Epoch 5, Class Loss=0.5278579592704773, Reg Loss=6.8543548583984375
Clinet index 5, End of Epoch 5/6, Average Loss=7.3822126388549805, Class Loss=0.5278579592704773, Reg Loss=6.8543548583984375
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=20.391419106721877
Loss made of: CE 0.5938523411750793, LKD 7.501277923583984, LDE 0.0, LReg 0.0, POD 12.818843841552734 EntMin 0.0
Epoch 6, Class Loss=0.5097979307174683, Reg Loss=6.857314586639404
Clinet index 5, End of Epoch 6/6, Average Loss=7.367112636566162, Class Loss=0.5097979307174683, Reg Loss=6.857314586639404
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=24.485590356588364
Loss made of: CE 1.0051816701889038, LKD 7.806395530700684, LDE 0.0, LReg 0.0, POD 16.791027069091797 EntMin 0.0
Epoch 1, Class Loss=0.8724190592765808, Reg Loss=6.98240852355957
Clinet index 20, End of Epoch 1/6, Average Loss=7.854827404022217, Class Loss=0.8724190592765808, Reg Loss=6.98240852355957
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=22.687015533447266
Loss made of: CE 0.6449899673461914, LKD 7.430611610412598, LDE 0.0, LReg 0.0, POD 15.627395629882812 EntMin 0.0
Epoch 2, Class Loss=0.6424110531806946, Reg Loss=6.916406154632568
Clinet index 20, End of Epoch 2/6, Average Loss=7.558817386627197, Class Loss=0.6424110531806946, Reg Loss=6.916406154632568
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=21.86929978430271
Loss made of: CE 0.663362443447113, LKD 7.586084365844727, LDE 0.0, LReg 0.0, POD 15.321212768554688 EntMin 0.0
Epoch 3, Class Loss=0.5684204697608948, Reg Loss=6.881289482116699
Clinet index 20, End of Epoch 3/6, Average Loss=7.449709892272949, Class Loss=0.5684204697608948, Reg Loss=6.881289482116699
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=21.17404587864876
Loss made of: CE 0.5515655875205994, LKD 6.956957817077637, LDE 0.0, LReg 0.0, POD 13.826862335205078 EntMin 0.0
Epoch 4, Class Loss=0.5342183709144592, Reg Loss=6.873294830322266
Clinet index 20, End of Epoch 4/6, Average Loss=7.40751314163208, Class Loss=0.5342183709144592, Reg Loss=6.873294830322266
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=20.5874803096056
Loss made of: CE 0.4639243483543396, LKD 6.8810529708862305, LDE 0.0, LReg 0.0, POD 13.175243377685547 EntMin 0.0
Epoch 5, Class Loss=0.516776442527771, Reg Loss=6.8329973220825195
Clinet index 20, End of Epoch 5/6, Average Loss=7.34977388381958, Class Loss=0.516776442527771, Reg Loss=6.8329973220825195
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=19.715901333093644
Loss made of: CE 0.5404753684997559, LKD 7.2672529220581055, LDE 0.0, LReg 0.0, POD 13.578624725341797 EntMin 0.0
Epoch 6, Class Loss=0.4986853003501892, Reg Loss=6.78041410446167
Clinet index 20, End of Epoch 6/6, Average Loss=7.279099464416504, Class Loss=0.4986853003501892, Reg Loss=6.78041410446167
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=23.153262162208556
Loss made of: CE 0.839130163192749, LKD 6.529967784881592, LDE 0.0, LReg 0.0, POD 14.300464630126953 EntMin 0.0
Epoch 1, Class Loss=0.8462987542152405, Reg Loss=6.552885055541992
Clinet index 0, End of Epoch 1/6, Average Loss=7.399183750152588, Class Loss=0.8462987542152405, Reg Loss=6.552885055541992
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/11, Loss=21.455505514144896
Loss made of: CE 0.5656700134277344, LKD 5.735259056091309, LDE 0.0, LReg 0.0, POD 12.001922607421875 EntMin 0.0
Epoch 2, Class Loss=0.6728660464286804, Reg Loss=6.44420862197876
Clinet index 0, End of Epoch 2/6, Average Loss=7.117074489593506, Class Loss=0.6728660464286804, Reg Loss=6.44420862197876
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/11, Loss=20.676408296823503
Loss made of: CE 0.512704074382782, LKD 6.547447204589844, LDE 0.0, LReg 0.0, POD 13.732988357543945 EntMin 0.0
Epoch 3, Class Loss=0.5805296897888184, Reg Loss=6.443362236022949
Clinet index 0, End of Epoch 3/6, Average Loss=7.023891925811768, Class Loss=0.5805296897888184, Reg Loss=6.443362236022949
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/11, Loss=20.225785753130914
Loss made of: CE 0.6030340194702148, LKD 7.301383018493652, LDE 0.0, LReg 0.0, POD 14.304597854614258 EntMin 0.0
Epoch 4, Class Loss=0.5458240509033203, Reg Loss=6.526388645172119
Clinet index 0, End of Epoch 4/6, Average Loss=7.0722126960754395, Class Loss=0.5458240509033203, Reg Loss=6.526388645172119
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/11, Loss=19.621068012714385
Loss made of: CE 0.5743459463119507, LKD 6.537132263183594, LDE 0.0, LReg 0.0, POD 13.189589500427246 EntMin 0.0
Epoch 5, Class Loss=0.5289854407310486, Reg Loss=6.453835487365723
Clinet index 0, End of Epoch 5/6, Average Loss=6.982820987701416, Class Loss=0.5289854407310486, Reg Loss=6.453835487365723
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 10/11, Loss=19.29235001206398
Loss made of: CE 0.5025849938392639, LKD 6.30424690246582, LDE 0.0, LReg 0.0, POD 11.96481704711914 EntMin 0.0
Epoch 6, Class Loss=0.49755340814590454, Reg Loss=6.421228885650635
Clinet index 0, End of Epoch 6/6, Average Loss=6.9187822341918945, Class Loss=0.49755340814590454, Reg Loss=6.421228885650635
federated aggregation...
Validation, Class Loss=0.5079543590545654, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.858995
Mean Acc: 0.544339
FreqW Acc: 0.785973
Mean IoU: 0.452110
Class IoU:
	class 0: 0.89623594
	class 1: 0.7378211
	class 2: 0.3235845
	class 3: 0.68310565
	class 4: 0.6234604
	class 5: 0.7534432
	class 6: 0.6900317
	class 7: 0.82299626
	class 8: 0.69273776
	class 9: 0.05076006
	class 10: 0.0
	class 11: 0.0014370517
	class 12: 0.39572755
	class 13: 0.0
	class 14: 0.110302866
Class Acc:
	class 0: 0.96929187
	class 1: 0.7457156
	class 2: 0.91398007
	class 3: 0.6912408
	class 4: 0.6612274
	class 5: 0.8341196
	class 6: 0.69213265
	class 7: 0.93576515
	class 8: 0.7384871
	class 9: 0.25393328
	class 10: 0.0
	class 11: 0.001437148
	class 12: 0.61325705
	class 13: 0.0
	class 14: 0.1144986

federated global round: 17, step: 3
select part of clients to conduct local training
[1, 16, 6, 21]
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=25.973805922269822
Loss made of: CE 0.8689025640487671, LKD 6.623851299285889, LDE 0.0, LReg 0.0, POD 16.171894073486328 EntMin 0.0
Epoch 1, Class Loss=0.9660760760307312, Reg Loss=6.687394142150879
Clinet index 1, End of Epoch 1/6, Average Loss=7.653470039367676, Class Loss=0.9660760760307312, Reg Loss=6.687394142150879
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=21.54545325636864
Loss made of: CE 0.6731652021408081, LKD 6.735899925231934, LDE 0.0, LReg 0.0, POD 13.806787490844727 EntMin 0.0
Epoch 2, Class Loss=0.7659348845481873, Reg Loss=6.438872814178467
Clinet index 1, End of Epoch 2/6, Average Loss=7.204807758331299, Class Loss=0.7659348845481873, Reg Loss=6.438872814178467
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=20.90867211818695
Loss made of: CE 0.5614930987358093, LKD 6.28521728515625, LDE 0.0, LReg 0.0, POD 12.368173599243164 EntMin 0.0
Epoch 3, Class Loss=0.6142246723175049, Reg Loss=6.555144309997559
Clinet index 1, End of Epoch 3/6, Average Loss=7.169368743896484, Class Loss=0.6142246723175049, Reg Loss=6.555144309997559
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=20.096290704607963
Loss made of: CE 0.5419458746910095, LKD 6.602495193481445, LDE 0.0, LReg 0.0, POD 12.530708312988281 EntMin 0.0
Epoch 4, Class Loss=0.539476752281189, Reg Loss=6.546627521514893
Clinet index 1, End of Epoch 4/6, Average Loss=7.086104393005371, Class Loss=0.539476752281189, Reg Loss=6.546627521514893
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=20.084564608335494
Loss made of: CE 0.5497902631759644, LKD 6.375095367431641, LDE 0.0, LReg 0.0, POD 11.861509323120117 EntMin 0.0
Epoch 5, Class Loss=0.5331728458404541, Reg Loss=6.52559757232666
Clinet index 1, End of Epoch 5/6, Average Loss=7.058770179748535, Class Loss=0.5331728458404541, Reg Loss=6.52559757232666
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=19.554354730248452
Loss made of: CE 0.5086240768432617, LKD 6.513617515563965, LDE 0.0, LReg 0.0, POD 13.469379425048828 EntMin 0.0
Epoch 6, Class Loss=0.5145878791809082, Reg Loss=6.500138759613037
Clinet index 1, End of Epoch 6/6, Average Loss=7.014726638793945, Class Loss=0.5145878791809082, Reg Loss=6.500138759613037
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=26.13255133628845
Loss made of: CE 0.8860892057418823, LKD 6.472392559051514, LDE 0.0, LReg 0.0, POD 15.941740036010742 EntMin 0.0
Epoch 1, Class Loss=0.9982471466064453, Reg Loss=6.831948757171631
Clinet index 16, End of Epoch 1/6, Average Loss=7.830195903778076, Class Loss=0.9982471466064453, Reg Loss=6.831948757171631
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=21.78515037894249
Loss made of: CE 0.7257488965988159, LKD 6.865940093994141, LDE 0.0, LReg 0.0, POD 13.088390350341797 EntMin 0.0
Epoch 2, Class Loss=0.7676482796669006, Reg Loss=6.538569450378418
Clinet index 16, End of Epoch 2/6, Average Loss=7.306217670440674, Class Loss=0.7676482796669006, Reg Loss=6.538569450378418
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=20.83796257376671
Loss made of: CE 0.5528281927108765, LKD 6.62705135345459, LDE 0.0, LReg 0.0, POD 13.116411209106445 EntMin 0.0
Epoch 3, Class Loss=0.5963840484619141, Reg Loss=6.612750053405762
Clinet index 16, End of Epoch 3/6, Average Loss=7.209134101867676, Class Loss=0.5963840484619141, Reg Loss=6.612750053405762
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=20.347315409779547
Loss made of: CE 0.37729912996292114, LKD 5.7966437339782715, LDE 0.0, LReg 0.0, POD 13.667238235473633 EntMin 0.0
Epoch 4, Class Loss=0.5390684604644775, Reg Loss=6.590531349182129
Clinet index 16, End of Epoch 4/6, Average Loss=7.129599571228027, Class Loss=0.5390684604644775, Reg Loss=6.590531349182129
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=19.8110528588295
Loss made of: CE 0.5818499326705933, LKD 6.791591644287109, LDE 0.0, LReg 0.0, POD 12.250442504882812 EntMin 0.0
Epoch 5, Class Loss=0.5230744481086731, Reg Loss=6.545018672943115
Clinet index 16, End of Epoch 5/6, Average Loss=7.068093299865723, Class Loss=0.5230744481086731, Reg Loss=6.545018672943115
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=19.183147180080415
Loss made of: CE 0.5293110609054565, LKD 6.479343414306641, LDE 0.0, LReg 0.0, POD 12.608846664428711 EntMin 0.0
Epoch 6, Class Loss=0.5017814040184021, Reg Loss=6.470099925994873
Clinet index 16, End of Epoch 6/6, Average Loss=6.97188138961792, Class Loss=0.5017814040184021, Reg Loss=6.470099925994873
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=26.359148228168486
Loss made of: CE 1.121574878692627, LKD 7.007891654968262, LDE 0.0, LReg 0.0, POD 16.374692916870117 EntMin 0.0
Epoch 1, Class Loss=1.0130969285964966, Reg Loss=6.823648929595947
Clinet index 6, End of Epoch 1/6, Average Loss=7.836745738983154, Class Loss=1.0130969285964966, Reg Loss=6.823648929595947
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/11, Loss=22.096405881643296
Loss made of: CE 1.021370530128479, LKD 7.523024559020996, LDE 0.0, LReg 0.0, POD 15.748668670654297 EntMin 0.0
Epoch 2, Class Loss=0.8262238502502441, Reg Loss=6.593021869659424
Clinet index 6, End of Epoch 2/6, Average Loss=7.419245719909668, Class Loss=0.8262238502502441, Reg Loss=6.593021869659424
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/11, Loss=20.89450563788414
Loss made of: CE 0.649077296257019, LKD 6.721586227416992, LDE 0.0, LReg 0.0, POD 14.165989875793457 EntMin 0.0
Epoch 3, Class Loss=0.6743203401565552, Reg Loss=6.67795991897583
Clinet index 6, End of Epoch 3/6, Average Loss=7.352280139923096, Class Loss=0.6743203401565552, Reg Loss=6.67795991897583
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/11, Loss=20.064159202575684
Loss made of: CE 0.6033391952514648, LKD 6.575969696044922, LDE 0.0, LReg 0.0, POD 14.967488288879395 EntMin 0.0
Epoch 4, Class Loss=0.5773999691009521, Reg Loss=6.592884063720703
Clinet index 6, End of Epoch 4/6, Average Loss=7.170284271240234, Class Loss=0.5773999691009521, Reg Loss=6.592884063720703
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/11, Loss=20.17066003382206
Loss made of: CE 0.42880114912986755, LKD 5.978357315063477, LDE 0.0, LReg 0.0, POD 12.777982711791992 EntMin 0.0
Epoch 5, Class Loss=0.5635095834732056, Reg Loss=6.702371120452881
Clinet index 6, End of Epoch 5/6, Average Loss=7.265880584716797, Class Loss=0.5635095834732056, Reg Loss=6.702371120452881
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/11, Loss=19.67441768348217
Loss made of: CE 0.5126861333847046, LKD 6.465203285217285, LDE 0.0, LReg 0.0, POD 11.827384948730469 EntMin 0.0
Epoch 6, Class Loss=0.5429113507270813, Reg Loss=6.600118160247803
Clinet index 6, End of Epoch 6/6, Average Loss=7.143029689788818, Class Loss=0.5429113507270813, Reg Loss=6.600118160247803
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=20.85655311644077
Loss made of: CE 0.4962042570114136, LKD 6.366689205169678, LDE 0.0, LReg 0.0, POD 13.940644264221191 EntMin 0.0
Epoch 1, Class Loss=0.5835268497467041, Reg Loss=6.926180839538574
Clinet index 21, End of Epoch 1/6, Average Loss=7.509707450866699, Class Loss=0.5835268497467041, Reg Loss=6.926180839538574
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=20.05153027176857
Loss made of: CE 0.5323364734649658, LKD 7.470536708831787, LDE 0.0, LReg 0.0, POD 12.935135841369629 EntMin 0.0
Epoch 2, Class Loss=0.5108481645584106, Reg Loss=6.947212219238281
Clinet index 21, End of Epoch 2/6, Average Loss=7.458060264587402, Class Loss=0.5108481645584106, Reg Loss=6.947212219238281
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/12, Loss=20.367623215913774
Loss made of: CE 0.5665923953056335, LKD 7.625250816345215, LDE 0.0, LReg 0.0, POD 13.268038749694824 EntMin 0.0
Epoch 3, Class Loss=0.5002421140670776, Reg Loss=6.9962687492370605
Clinet index 21, End of Epoch 3/6, Average Loss=7.496510982513428, Class Loss=0.5002421140670776, Reg Loss=6.9962687492370605
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=19.543799290060996
Loss made of: CE 0.507926344871521, LKD 6.795772075653076, LDE 0.0, LReg 0.0, POD 12.857187271118164 EntMin 0.0
Epoch 4, Class Loss=0.48569080233573914, Reg Loss=6.840216636657715
Clinet index 21, End of Epoch 4/6, Average Loss=7.325907230377197, Class Loss=0.48569080233573914, Reg Loss=6.840216636657715
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=19.052844566106796
Loss made of: CE 0.4710785746574402, LKD 6.993691921234131, LDE 0.0, LReg 0.0, POD 11.303966522216797 EntMin 0.0
Epoch 5, Class Loss=0.4802812933921814, Reg Loss=6.89430570602417
Clinet index 21, End of Epoch 5/6, Average Loss=7.374587059020996, Class Loss=0.4802812933921814, Reg Loss=6.89430570602417
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=19.858150374889373
Loss made of: CE 0.42910036444664, LKD 6.549323081970215, LDE 0.0, LReg 0.0, POD 11.113849639892578 EntMin 0.0
Epoch 6, Class Loss=0.47987139225006104, Reg Loss=6.959602355957031
Clinet index 21, End of Epoch 6/6, Average Loss=7.439473628997803, Class Loss=0.47987139225006104, Reg Loss=6.959602355957031
federated aggregation...
Validation, Class Loss=0.49702876806259155, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.859458
Mean Acc: 0.553763
FreqW Acc: 0.791843
Mean IoU: 0.451755
Class IoU:
	class 0: 0.904944
	class 1: 0.77104694
	class 2: 0.3386885
	class 3: 0.7085311
	class 4: 0.64120525
	class 5: 0.7329108
	class 6: 0.70419186
	class 7: 0.8330979
	class 8: 0.6911127
	class 9: 0.063538305
	class 10: 0.0
	class 11: 0.0006243463
	class 12: 0.3861737
	class 13: 5.3579526e-05
	class 14: 0.00021228395
Class Acc:
	class 0: 0.96299654
	class 1: 0.7807609
	class 2: 0.8828054
	class 3: 0.723544
	class 4: 0.68177915
	class 5: 0.796796
	class 6: 0.7065716
	class 7: 0.93833154
	class 8: 0.7354105
	class 9: 0.31095445
	class 10: 0.0
	class 11: 0.0006243463
	class 12: 0.7856033
	class 13: 5.3597603e-05
	class 14: 0.00021228932

federated global round: 18, step: 3
select part of clients to conduct local training
[8, 2, 17, 14]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=19.706286370754242
Loss made of: CE 0.5145130157470703, LKD 6.235093593597412, LDE 0.0, LReg 0.0, POD 14.117378234863281 EntMin 0.0
Epoch 1, Class Loss=0.5879287719726562, Reg Loss=6.4081597328186035
Clinet index 8, End of Epoch 1/6, Average Loss=6.99608850479126, Class Loss=0.5879287719726562, Reg Loss=6.4081597328186035
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=18.8399497628212
Loss made of: CE 0.5555617809295654, LKD 7.14577579498291, LDE 0.0, LReg 0.0, POD 11.836265563964844 EntMin 0.0
Epoch 2, Class Loss=0.5034347176551819, Reg Loss=6.405343055725098
Clinet index 8, End of Epoch 2/6, Average Loss=6.908777713775635, Class Loss=0.5034347176551819, Reg Loss=6.405343055725098
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=18.65805951654911
Loss made of: CE 0.4033088684082031, LKD 5.985459804534912, LDE 0.0, LReg 0.0, POD 11.830098152160645 EntMin 0.0
Epoch 3, Class Loss=0.4867297112941742, Reg Loss=6.371891021728516
Clinet index 8, End of Epoch 3/6, Average Loss=6.858620643615723, Class Loss=0.4867297112941742, Reg Loss=6.371891021728516
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=18.438832050561906
Loss made of: CE 0.41630110144615173, LKD 5.682158470153809, LDE 0.0, LReg 0.0, POD 11.56865119934082 EntMin 0.0
Epoch 4, Class Loss=0.4807417690753937, Reg Loss=6.327247142791748
Clinet index 8, End of Epoch 4/6, Average Loss=6.807989120483398, Class Loss=0.4807417690753937, Reg Loss=6.327247142791748
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=18.387895771861075
Loss made of: CE 0.4399772584438324, LKD 6.7212018966674805, LDE 0.0, LReg 0.0, POD 11.955411911010742 EntMin 0.0
Epoch 5, Class Loss=0.48135173320770264, Reg Loss=6.382122039794922
Clinet index 8, End of Epoch 5/6, Average Loss=6.863473892211914, Class Loss=0.48135173320770264, Reg Loss=6.382122039794922
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=18.266475275158882
Loss made of: CE 0.48938173055648804, LKD 6.693822383880615, LDE 0.0, LReg 0.0, POD 12.012462615966797 EntMin 0.0
Epoch 6, Class Loss=0.453914999961853, Reg Loss=6.292295455932617
Clinet index 8, End of Epoch 6/6, Average Loss=6.74621057510376, Class Loss=0.453914999961853, Reg Loss=6.292295455932617
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=20.052037644386292
Loss made of: CE 0.4732103943824768, LKD 6.25769567489624, LDE 0.0, LReg 0.0, POD 12.158552169799805 EntMin 0.0
Epoch 1, Class Loss=0.6111656427383423, Reg Loss=6.40057373046875
Clinet index 2, End of Epoch 1/6, Average Loss=7.011739253997803, Class Loss=0.6111656427383423, Reg Loss=6.40057373046875
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=19.601896822452545
Loss made of: CE 0.522165060043335, LKD 6.2448625564575195, LDE 0.0, LReg 0.0, POD 11.877163887023926 EntMin 0.0
Epoch 2, Class Loss=0.5248934030532837, Reg Loss=6.465447902679443
Clinet index 2, End of Epoch 2/6, Average Loss=6.9903411865234375, Class Loss=0.5248934030532837, Reg Loss=6.465447902679443
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=19.379762706160545
Loss made of: CE 0.4670557975769043, LKD 6.356320858001709, LDE 0.0, LReg 0.0, POD 11.075216293334961 EntMin 0.0
Epoch 3, Class Loss=0.4943326711654663, Reg Loss=6.408919811248779
Clinet index 2, End of Epoch 3/6, Average Loss=6.903252601623535, Class Loss=0.4943326711654663, Reg Loss=6.408919811248779
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=19.198536720871925
Loss made of: CE 0.444124311208725, LKD 6.47616720199585, LDE 0.0, LReg 0.0, POD 12.099016189575195 EntMin 0.0
Epoch 4, Class Loss=0.508197546005249, Reg Loss=6.426513195037842
Clinet index 2, End of Epoch 4/6, Average Loss=6.934710502624512, Class Loss=0.508197546005249, Reg Loss=6.426513195037842
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=18.679785791039468
Loss made of: CE 0.43670153617858887, LKD 6.3516082763671875, LDE 0.0, LReg 0.0, POD 11.573253631591797 EntMin 0.0
Epoch 5, Class Loss=0.48115867376327515, Reg Loss=6.405558109283447
Clinet index 2, End of Epoch 5/6, Average Loss=6.886716842651367, Class Loss=0.48115867376327515, Reg Loss=6.405558109283447
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=18.501786971092223
Loss made of: CE 0.45197340846061707, LKD 6.310138702392578, LDE 0.0, LReg 0.0, POD 13.227677345275879 EntMin 0.0
Epoch 6, Class Loss=0.4785703420639038, Reg Loss=6.348775863647461
Clinet index 2, End of Epoch 6/6, Average Loss=6.827346324920654, Class Loss=0.4785703420639038, Reg Loss=6.348775863647461
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=24.958393520116807
Loss made of: CE 0.8721429705619812, LKD 7.386735439300537, LDE 0.0, LReg 0.0, POD 14.423147201538086 EntMin 0.0
Epoch 1, Class Loss=0.8470451235771179, Reg Loss=7.049846649169922
Clinet index 17, End of Epoch 1/6, Average Loss=7.8968915939331055, Class Loss=0.8470451235771179, Reg Loss=7.049846649169922
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=21.706501030921935
Loss made of: CE 0.5915464758872986, LKD 7.0342020988464355, LDE 0.0, LReg 0.0, POD 12.952150344848633 EntMin 0.0
Epoch 2, Class Loss=0.603563129901886, Reg Loss=6.9817914962768555
Clinet index 17, End of Epoch 2/6, Average Loss=7.585354804992676, Class Loss=0.603563129901886, Reg Loss=6.9817914962768555
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=20.460731673240662
Loss made of: CE 0.4287016987800598, LKD 7.099285125732422, LDE 0.0, LReg 0.0, POD 12.288656234741211 EntMin 0.0
Epoch 3, Class Loss=0.5207734704017639, Reg Loss=6.917471885681152
Clinet index 17, End of Epoch 3/6, Average Loss=7.4382452964782715, Class Loss=0.5207734704017639, Reg Loss=6.917471885681152
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=20.30323769748211
Loss made of: CE 0.4798634946346283, LKD 6.352160930633545, LDE 0.0, LReg 0.0, POD 12.940134048461914 EntMin 0.0
Epoch 4, Class Loss=0.5148613452911377, Reg Loss=6.857339859008789
Clinet index 17, End of Epoch 4/6, Average Loss=7.372200965881348, Class Loss=0.5148613452911377, Reg Loss=6.857339859008789
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=19.042777314782143
Loss made of: CE 0.3787139356136322, LKD 6.49751091003418, LDE 0.0, LReg 0.0, POD 11.234434127807617 EntMin 0.0
Epoch 5, Class Loss=0.48901423811912537, Reg Loss=6.907794952392578
Clinet index 17, End of Epoch 5/6, Average Loss=7.396809101104736, Class Loss=0.48901423811912537, Reg Loss=6.907794952392578
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=19.420787528157234
Loss made of: CE 0.37889212369918823, LKD 6.360298156738281, LDE 0.0, LReg 0.0, POD 10.79590892791748 EntMin 0.0
Epoch 6, Class Loss=0.4880238175392151, Reg Loss=6.829083442687988
Clinet index 17, End of Epoch 6/6, Average Loss=7.317107200622559, Class Loss=0.4880238175392151, Reg Loss=6.829083442687988
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=20.290218406915663
Loss made of: CE 0.5428087711334229, LKD 6.265594005584717, LDE 0.0, LReg 0.0, POD 13.366933822631836 EntMin 0.0
Epoch 1, Class Loss=0.6260757446289062, Reg Loss=6.549163341522217
Clinet index 14, End of Epoch 1/6, Average Loss=7.175239086151123, Class Loss=0.6260757446289062, Reg Loss=6.549163341522217
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=19.56885231733322
Loss made of: CE 0.5073142051696777, LKD 6.54551887512207, LDE 0.0, LReg 0.0, POD 11.708160400390625 EntMin 0.0
Epoch 2, Class Loss=0.5302696228027344, Reg Loss=6.564082145690918
Clinet index 14, End of Epoch 2/6, Average Loss=7.094351768493652, Class Loss=0.5302696228027344, Reg Loss=6.564082145690918
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/11, Loss=19.729535853862764
Loss made of: CE 0.47953280806541443, LKD 6.563159465789795, LDE 0.0, LReg 0.0, POD 12.398616790771484 EntMin 0.0
Epoch 3, Class Loss=0.5216978788375854, Reg Loss=6.646807670593262
Clinet index 14, End of Epoch 3/6, Average Loss=7.168505668640137, Class Loss=0.5216978788375854, Reg Loss=6.646807670593262
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=19.558103805780412
Loss made of: CE 0.5340632200241089, LKD 6.582098007202148, LDE 0.0, LReg 0.0, POD 11.554771423339844 EntMin 0.0
Epoch 4, Class Loss=0.5209866762161255, Reg Loss=6.598241329193115
Clinet index 14, End of Epoch 4/6, Average Loss=7.119227886199951, Class Loss=0.5209866762161255, Reg Loss=6.598241329193115
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=18.832800969481468
Loss made of: CE 0.4668162763118744, LKD 6.715754508972168, LDE 0.0, LReg 0.0, POD 10.83566665649414 EntMin 0.0
Epoch 5, Class Loss=0.5088397264480591, Reg Loss=6.692126274108887
Clinet index 14, End of Epoch 5/6, Average Loss=7.200965881347656, Class Loss=0.5088397264480591, Reg Loss=6.692126274108887
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=18.748152834177017
Loss made of: CE 0.4289887845516205, LKD 6.021203517913818, LDE 0.0, LReg 0.0, POD 12.136313438415527 EntMin 0.0
Epoch 6, Class Loss=0.5016577243804932, Reg Loss=6.541214942932129
Clinet index 14, End of Epoch 6/6, Average Loss=7.042872428894043, Class Loss=0.5016577243804932, Reg Loss=6.541214942932129
federated aggregation...
Validation, Class Loss=0.4881659746170044, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.861549
Mean Acc: 0.558174
FreqW Acc: 0.793765
Mean IoU: 0.455186
Class IoU:
	class 0: 0.9051953
	class 1: 0.77241427
	class 2: 0.3328562
	class 3: 0.72029495
	class 4: 0.6415095
	class 5: 0.73222244
	class 6: 0.69932055
	class 7: 0.8303051
	class 8: 0.72634166
	class 9: 0.061408088
	class 10: 0.0
	class 11: 0.00063022086
	class 12: 0.40189493
	class 13: 0.0034034182
	class 14: 0.0
Class Acc:
	class 0: 0.96360284
	class 1: 0.7816105
	class 2: 0.9020158
	class 3: 0.73583764
	class 4: 0.6852067
	class 5: 0.79833746
	class 6: 0.7016325
	class 7: 0.9396428
	class 8: 0.78249776
	class 9: 0.29656142
	class 10: 0.0
	class 11: 0.00063022086
	class 12: 0.7816046
	class 13: 0.0034256764
	class 14: 0.0

federated global round: 19, step: 3
select part of clients to conduct local training
[1, 9, 8, 0]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/11, Loss=18.80154446065426
Loss made of: CE 0.4971172511577606, LKD 6.155844688415527, LDE 0.0, LReg 0.0, POD 11.292451858520508 EntMin 0.0
Epoch 1, Class Loss=0.5686780214309692, Reg Loss=6.49975061416626
Clinet index 1, End of Epoch 1/6, Average Loss=7.0684285163879395, Class Loss=0.5686780214309692, Reg Loss=6.49975061416626
Pseudo labeling is: None
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/11, Loss=18.38203746378422
Loss made of: CE 0.5073072910308838, LKD 6.705255031585693, LDE 0.0, LReg 0.0, POD 11.508498191833496 EntMin 0.0
Epoch 2, Class Loss=0.5169769525527954, Reg Loss=6.446746826171875
Clinet index 1, End of Epoch 2/6, Average Loss=6.963723659515381, Class Loss=0.5169769525527954, Reg Loss=6.446746826171875
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/11, Loss=18.465249356627464
Loss made of: CE 0.5065288543701172, LKD 6.304053783416748, LDE 0.0, LReg 0.0, POD 10.085729598999023 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.501733660697937, Reg Loss=6.520453929901123
Clinet index 1, End of Epoch 3/6, Average Loss=7.02218770980835, Class Loss=0.501733660697937, Reg Loss=6.520453929901123
Pseudo labeling is: None
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/11, Loss=18.144304093718528
Loss made of: CE 0.5091463923454285, LKD 6.700163841247559, LDE 0.0, LReg 0.0, POD 10.647241592407227 EntMin 0.0
Epoch 4, Class Loss=0.4905659556388855, Reg Loss=6.4910993576049805
Clinet index 1, End of Epoch 4/6, Average Loss=6.981665134429932, Class Loss=0.4905659556388855, Reg Loss=6.4910993576049805
Pseudo labeling is: None
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/11, Loss=18.23381859064102
Loss made of: CE 0.5101412534713745, LKD 6.246385097503662, LDE 0.0, LReg 0.0, POD 10.704286575317383 EntMin 0.0
Epoch 5, Class Loss=0.48826757073402405, Reg Loss=6.482793807983398
Clinet index 1, End of Epoch 5/6, Average Loss=6.9710612297058105, Class Loss=0.48826757073402405, Reg Loss=6.482793807983398
Pseudo labeling is: None
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/11, Loss=17.824568313360214
Loss made of: CE 0.47766751050949097, LKD 6.487725734710693, LDE 0.0, LReg 0.0, POD 11.867345809936523 EntMin 0.0
Epoch 6, Class Loss=0.5006436705589294, Reg Loss=6.479031085968018
Clinet index 1, End of Epoch 6/6, Average Loss=6.979674816131592, Class Loss=0.5006436705589294, Reg Loss=6.479031085968018
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/12, Loss=24.11438482403755
Loss made of: CE 0.8913015127182007, LKD 6.916280269622803, LDE 0.0, LReg 0.0, POD 14.025043487548828 EntMin 0.0
Epoch 1, Class Loss=0.8707166910171509, Reg Loss=6.877730369567871
Clinet index 9, End of Epoch 1/6, Average Loss=7.748446941375732, Class Loss=0.8707166910171509, Reg Loss=6.877730369567871
Pseudo labeling is: None
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/12, Loss=21.251110243797303
Loss made of: CE 0.5404583811759949, LKD 6.640692710876465, LDE 0.0, LReg 0.0, POD 13.232551574707031 EntMin 0.0
Epoch 2, Class Loss=0.6611640453338623, Reg Loss=6.857497692108154
Clinet index 9, End of Epoch 2/6, Average Loss=7.5186614990234375, Class Loss=0.6611640453338623, Reg Loss=6.857497692108154
Pseudo labeling is: None
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/12, Loss=19.85052353143692
Loss made of: CE 0.556175708770752, LKD 6.78062629699707, LDE 0.0, LReg 0.0, POD 12.847054481506348 EntMin 0.0
Epoch 3, Class Loss=0.5262231826782227, Reg Loss=6.68068790435791
Clinet index 9, End of Epoch 3/6, Average Loss=7.206911087036133, Class Loss=0.5262231826782227, Reg Loss=6.68068790435791
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/12, Loss=19.526364746689797
Loss made of: CE 0.41934236884117126, LKD 6.1928558349609375, LDE 0.0, LReg 0.0, POD 11.429655075073242 EntMin 0.0
Epoch 4, Class Loss=0.5004781484603882, Reg Loss=6.808528900146484
Clinet index 9, End of Epoch 4/6, Average Loss=7.309007167816162, Class Loss=0.5004781484603882, Reg Loss=6.808528900146484
Pseudo labeling is: None
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/12, Loss=19.88140705227852
Loss made of: CE 0.5547633171081543, LKD 7.023329734802246, LDE 0.0, LReg 0.0, POD 13.123756408691406 EntMin 0.0
Epoch 5, Class Loss=0.505759596824646, Reg Loss=6.828840255737305
Clinet index 9, End of Epoch 5/6, Average Loss=7.33459997177124, Class Loss=0.505759596824646, Reg Loss=6.828840255737305
Pseudo labeling is: None
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/12, Loss=19.444074422121048
Loss made of: CE 0.3689751625061035, LKD 6.0273847579956055, LDE 0.0, LReg 0.0, POD 11.168256759643555 EntMin 0.0
Epoch 6, Class Loss=0.4931206703186035, Reg Loss=6.725534439086914
Clinet index 9, End of Epoch 6/6, Average Loss=7.218655109405518, Class Loss=0.4931206703186035, Reg Loss=6.725534439086914
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=18.758933827281
Loss made of: CE 0.46715760231018066, LKD 6.101217269897461, LDE 0.0, LReg 0.0, POD 12.09506893157959 EntMin 0.0
Epoch 1, Class Loss=0.5460033416748047, Reg Loss=6.359755992889404
Clinet index 8, End of Epoch 1/6, Average Loss=6.905759334564209, Class Loss=0.5460033416748047, Reg Loss=6.359755992889404
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/11, Loss=17.78315961062908
Loss made of: CE 0.5104280710220337, LKD 6.865188121795654, LDE 0.0, LReg 0.0, POD 10.819107055664062 EntMin 0.0
Epoch 2, Class Loss=0.49672651290893555, Reg Loss=6.341821193695068
Clinet index 8, End of Epoch 2/6, Average Loss=6.838547706604004, Class Loss=0.49672651290893555, Reg Loss=6.341821193695068
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/11, Loss=17.875043791532516
Loss made of: CE 0.39442992210388184, LKD 5.9757208824157715, LDE 0.0, LReg 0.0, POD 10.789763450622559 EntMin 0.0
Epoch 3, Class Loss=0.4819031357765198, Reg Loss=6.400914669036865
Clinet index 8, End of Epoch 3/6, Average Loss=6.88281774520874, Class Loss=0.4819031357765198, Reg Loss=6.400914669036865
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/11, Loss=17.76650184392929
Loss made of: CE 0.3926924467086792, LKD 5.540352821350098, LDE 0.0, LReg 0.0, POD 10.94281005859375 EntMin 0.0
Epoch 4, Class Loss=0.476798951625824, Reg Loss=6.40986442565918
Clinet index 8, End of Epoch 4/6, Average Loss=6.886663436889648, Class Loss=0.476798951625824, Reg Loss=6.40986442565918
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/11, Loss=17.706046241521836
Loss made of: CE 0.45451948046684265, LKD 6.635425567626953, LDE 0.0, LReg 0.0, POD 11.518292427062988 EntMin 0.0
Epoch 5, Class Loss=0.4814278185367584, Reg Loss=6.339051723480225
Clinet index 8, End of Epoch 5/6, Average Loss=6.820479393005371, Class Loss=0.4814278185367584, Reg Loss=6.339051723480225
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/11, Loss=17.649952054023743
Loss made of: CE 0.5163896083831787, LKD 6.847938537597656, LDE 0.0, LReg 0.0, POD 11.287503242492676 EntMin 0.0
Epoch 6, Class Loss=0.4700019657611847, Reg Loss=6.33219575881958
Clinet index 8, End of Epoch 6/6, Average Loss=6.8021979331970215, Class Loss=0.4700019657611847, Reg Loss=6.33219575881958
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/11, Loss=18.86290906071663
Loss made of: CE 0.544018566608429, LKD 6.3776679039001465, LDE 0.0, LReg 0.0, POD 10.764371871948242 EntMin 0.0
Epoch 1, Class Loss=0.5416359901428223, Reg Loss=6.436032772064209
Clinet index 0, End of Epoch 1/6, Average Loss=6.977668762207031, Class Loss=0.5416359901428223, Reg Loss=6.436032772064209
Pseudo labeling is: None
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/11, Loss=18.79655548930168
Loss made of: CE 0.4391481876373291, LKD 5.895755290985107, LDE 0.0, LReg 0.0, POD 10.367816925048828 EntMin 0.0
Epoch 2, Class Loss=0.5112188458442688, Reg Loss=6.55291748046875
Clinet index 0, End of Epoch 2/6, Average Loss=7.064136505126953, Class Loss=0.5112188458442688, Reg Loss=6.55291748046875
Pseudo labeling is: None
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/11, Loss=18.487927961349488
Loss made of: CE 0.43862447142601013, LKD 6.37213659286499, LDE 0.0, LReg 0.0, POD 11.46126937866211 EntMin 0.0
Epoch 3, Class Loss=0.48466548323631287, Reg Loss=6.5073957443237305
Clinet index 0, End of Epoch 3/6, Average Loss=6.992061138153076, Class Loss=0.48466548323631287, Reg Loss=6.5073957443237305
Pseudo labeling is: None
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/11, Loss=18.226250204443932
Loss made of: CE 0.5312365293502808, LKD 7.130677700042725, LDE 0.0, LReg 0.0, POD 12.30207347869873 EntMin 0.0
Epoch 4, Class Loss=0.4852568209171295, Reg Loss=6.472477436065674
Clinet index 0, End of Epoch 4/6, Average Loss=6.957734107971191, Class Loss=0.4852568209171295, Reg Loss=6.472477436065674
Pseudo labeling is: None
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/11, Loss=18.114622992277145
Loss made of: CE 0.5196587443351746, LKD 6.709979057312012, LDE 0.0, LReg 0.0, POD 11.867985725402832 EntMin 0.0
Epoch 5, Class Loss=0.48272547125816345, Reg Loss=6.486764430999756
Clinet index 0, End of Epoch 5/6, Average Loss=6.969490051269531, Class Loss=0.48272547125816345, Reg Loss=6.486764430999756
Pseudo labeling is: None
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/11, Loss=17.888989090919495
Loss made of: CE 0.4814658761024475, LKD 6.289254188537598, LDE 0.0, LReg 0.0, POD 10.709959030151367 EntMin 0.0
Epoch 6, Class Loss=0.475585401058197, Reg Loss=6.41943883895874
Clinet index 0, End of Epoch 6/6, Average Loss=6.895024299621582, Class Loss=0.475585401058197, Reg Loss=6.41943883895874
federated aggregation...
Validation, Class Loss=0.4880240857601166, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.862804
Mean Acc: 0.566649
FreqW Acc: 0.796675
Mean IoU: 0.462301
Class IoU:
	class 0: 0.9061996
	class 1: 0.78331935
	class 2: 0.33022377
	class 3: 0.73352027
	class 4: 0.6525824
	class 5: 0.74616075
	class 6: 0.72144437
	class 7: 0.8362546
	class 8: 0.7424721
	class 9: 0.06283728
	class 10: 0.0
	class 11: 0.0010205208
	class 12: 0.40419415
	class 13: 0.01103073
	class 14: 0.003253885
Class Acc:
	class 0: 0.9620827
	class 1: 0.7938483
	class 2: 0.90593195
	class 3: 0.751564
	class 4: 0.69869745
	class 5: 0.81494504
	class 6: 0.72404224
	class 7: 0.94050777
	class 8: 0.8037677
	class 9: 0.30712628
	class 10: 0.0
	class 11: 0.0010205255
	class 12: 0.78165555
	class 13: 0.011288736
	class 14: 0.0032553193

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[11, 2, 15, 6]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=27.39256615638733
Loss made of: CE 0.9014143943786621, LKD 6.433380126953125, LDE 0.0, LReg 0.0, POD 18.384984970092773 EntMin 0.0
Epoch 1, Class Loss=0.8627746105194092, Reg Loss=6.699843406677246
Clinet index 11, End of Epoch 1/6, Average Loss=7.562618255615234, Class Loss=0.8627746105194092, Reg Loss=6.699843406677246
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=22.259244048595427
Loss made of: CE 0.6540930271148682, LKD 6.393122673034668, LDE 0.0, LReg 0.0, POD 13.894486427307129 EntMin 0.0
Epoch 2, Class Loss=0.711883008480072, Reg Loss=6.131382942199707
Clinet index 11, End of Epoch 2/6, Average Loss=6.843266010284424, Class Loss=0.711883008480072, Reg Loss=6.131382942199707
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=20.376079243421554
Loss made of: CE 0.6404792666435242, LKD 5.769458770751953, LDE 0.0, LReg 0.0, POD 12.988920211791992 EntMin 0.0
Epoch 3, Class Loss=0.6412175893783569, Reg Loss=6.069211006164551
Clinet index 11, End of Epoch 3/6, Average Loss=6.710428714752197, Class Loss=0.6412175893783569, Reg Loss=6.069211006164551
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=19.640984213352205
Loss made of: CE 0.6383240818977356, LKD 6.327079772949219, LDE 0.0, LReg 0.0, POD 12.411226272583008 EntMin 0.0
Epoch 4, Class Loss=0.6164841651916504, Reg Loss=6.0395660400390625
Clinet index 11, End of Epoch 4/6, Average Loss=6.656050205230713, Class Loss=0.6164841651916504, Reg Loss=6.0395660400390625
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=19.09415819942951
Loss made of: CE 0.5359045267105103, LKD 6.254406929016113, LDE 0.0, LReg 0.0, POD 12.152135848999023 EntMin 0.0
Epoch 5, Class Loss=0.5873305797576904, Reg Loss=6.004617214202881
Clinet index 11, End of Epoch 5/6, Average Loss=6.591947555541992, Class Loss=0.5873305797576904, Reg Loss=6.004617214202881
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=18.69027629494667
Loss made of: CE 0.6844582557678223, LKD 5.707431793212891, LDE 0.0, LReg 0.0, POD 12.241887092590332 EntMin 0.0
Epoch 6, Class Loss=0.5642955899238586, Reg Loss=6.0121073722839355
Clinet index 11, End of Epoch 6/6, Average Loss=6.5764031410217285, Class Loss=0.5642955899238586, Reg Loss=6.0121073722839355
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=26.90137137770653
Loss made of: CE 0.870661735534668, LKD 6.246916770935059, LDE 0.0, LReg 0.0, POD 16.08104133605957 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.8607145547866821, Reg Loss=6.729118347167969
Clinet index 2, End of Epoch 1/6, Average Loss=7.589832782745361, Class Loss=0.8607145547866821, Reg Loss=6.729118347167969
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=22.152387541532516
Loss made of: CE 0.6200422644615173, LKD 5.955896377563477, LDE 0.0, LReg 0.0, POD 13.737771987915039 EntMin 0.0
Epoch 2, Class Loss=0.7141507863998413, Reg Loss=6.206895351409912
Clinet index 2, End of Epoch 2/6, Average Loss=6.921046257019043, Class Loss=0.7141507863998413, Reg Loss=6.206895351409912
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=20.502897518873215
Loss made of: CE 0.5732043981552124, LKD 5.971184730529785, LDE 0.0, LReg 0.0, POD 13.57347297668457 EntMin 0.0
Epoch 3, Class Loss=0.6450973153114319, Reg Loss=6.08186674118042
Clinet index 2, End of Epoch 3/6, Average Loss=6.726963996887207, Class Loss=0.6450973153114319, Reg Loss=6.08186674118042
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=19.509100091457366
Loss made of: CE 0.5972387790679932, LKD 6.580350399017334, LDE 0.0, LReg 0.0, POD 12.643832206726074 EntMin 0.0
Epoch 4, Class Loss=0.5948922038078308, Reg Loss=6.114406585693359
Clinet index 2, End of Epoch 4/6, Average Loss=6.709298610687256, Class Loss=0.5948922038078308, Reg Loss=6.114406585693359
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=18.910291981697082
Loss made of: CE 0.6806537508964539, LKD 6.361438751220703, LDE 0.0, LReg 0.0, POD 12.643654823303223 EntMin 0.0
Epoch 5, Class Loss=0.5830711722373962, Reg Loss=5.982133388519287
Clinet index 2, End of Epoch 5/6, Average Loss=6.565204620361328, Class Loss=0.5830711722373962, Reg Loss=5.982133388519287
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=18.77613169550896
Loss made of: CE 0.6238242387771606, LKD 6.059301376342773, LDE 0.0, LReg 0.0, POD 12.149150848388672 EntMin 0.0
Epoch 6, Class Loss=0.5508689880371094, Reg Loss=6.104648113250732
Clinet index 2, End of Epoch 6/6, Average Loss=6.655517101287842, Class Loss=0.5508689880371094, Reg Loss=6.104648113250732
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=26.773067104816437
Loss made of: CE 0.8338577151298523, LKD 6.854731559753418, LDE 0.0, LReg 0.0, POD 16.225866317749023 EntMin 0.0
Epoch 1, Class Loss=0.837357223033905, Reg Loss=6.6504316329956055
Clinet index 15, End of Epoch 1/6, Average Loss=7.487788677215576, Class Loss=0.837357223033905, Reg Loss=6.6504316329956055
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=21.987878704071044
Loss made of: CE 0.7767093181610107, LKD 6.437560558319092, LDE 0.0, LReg 0.0, POD 13.402735710144043 EntMin 0.0
Epoch 2, Class Loss=0.7030460238456726, Reg Loss=6.047372817993164
Clinet index 15, End of Epoch 2/6, Average Loss=6.750418663024902, Class Loss=0.7030460238456726, Reg Loss=6.047372817993164
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/12, Loss=20.225913962721826
Loss made of: CE 0.5876625776290894, LKD 5.702517509460449, LDE 0.0, LReg 0.0, POD 13.174398422241211 EntMin 0.0
Epoch 3, Class Loss=0.6301635503768921, Reg Loss=5.955324172973633
Clinet index 15, End of Epoch 3/6, Average Loss=6.5854878425598145, Class Loss=0.6301635503768921, Reg Loss=5.955324172973633
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=19.78802522420883
Loss made of: CE 0.472974956035614, LKD 5.803517818450928, LDE 0.0, LReg 0.0, POD 13.12631607055664 EntMin 0.0
Epoch 4, Class Loss=0.5872747302055359, Reg Loss=5.960707664489746
Clinet index 15, End of Epoch 4/6, Average Loss=6.547982215881348, Class Loss=0.5872747302055359, Reg Loss=5.960707664489746
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=19.135366383194924
Loss made of: CE 0.7229968905448914, LKD 5.4835524559021, LDE 0.0, LReg 0.0, POD 12.1740083694458 EntMin 0.0
Epoch 5, Class Loss=0.5809427499771118, Reg Loss=5.981366157531738
Clinet index 15, End of Epoch 5/6, Average Loss=6.5623087882995605, Class Loss=0.5809427499771118, Reg Loss=5.981366157531738
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=18.766461426019667
Loss made of: CE 0.4845782220363617, LKD 5.543580532073975, LDE 0.0, LReg 0.0, POD 12.520866394042969 EntMin 0.0
Epoch 6, Class Loss=0.5418789386749268, Reg Loss=5.874324321746826
Clinet index 15, End of Epoch 6/6, Average Loss=6.416203498840332, Class Loss=0.5418789386749268, Reg Loss=5.874324321746826
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=25.233367526531218
Loss made of: CE 0.7277123332023621, LKD 6.044611930847168, LDE 0.0, LReg 0.0, POD 15.545995712280273 EntMin 0.0
Epoch 1, Batch 20/97, Loss=22.000258576869964
Loss made of: CE 0.7364453077316284, LKD 6.382850646972656, LDE 0.0, LReg 0.0, POD 15.200127601623535 EntMin 0.0
Epoch 1, Batch 30/97, Loss=20.398980098962785
Loss made of: CE 0.6268796324729919, LKD 6.053763389587402, LDE 0.0, LReg 0.0, POD 12.86524486541748 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 40/97, Loss=19.482999578118324
Loss made of: CE 0.4675099551677704, LKD 5.357054710388184, LDE 0.0, LReg 0.0, POD 12.281986236572266 EntMin 0.0
Epoch 1, Batch 50/97, Loss=19.00971910953522
Loss made of: CE 0.5096477270126343, LKD 5.30908727645874, LDE 0.0, LReg 0.0, POD 12.412725448608398 EntMin 0.0
Epoch 1, Batch 60/97, Loss=19.212928423285483
Loss made of: CE 0.5785125494003296, LKD 6.682979583740234, LDE 0.0, LReg 0.0, POD 12.640613555908203 EntMin 0.0
Epoch 1, Batch 70/97, Loss=19.007190430164336
Loss made of: CE 0.5445166826248169, LKD 5.724936485290527, LDE 0.0, LReg 0.0, POD 12.64748764038086 EntMin 0.0
Epoch 1, Batch 80/97, Loss=18.32415145635605
Loss made of: CE 0.4252813756465912, LKD 5.5777692794799805, LDE 0.0, LReg 0.0, POD 13.26249885559082 EntMin 0.0
Epoch 1, Batch 90/97, Loss=18.402952310442924
Loss made of: CE 0.4245474934577942, LKD 6.166001319885254, LDE 0.0, LReg 0.0, POD 12.351079940795898 EntMin 0.0
Epoch 1, Class Loss=0.563178300857544, Reg Loss=5.941226959228516
Clinet index 6, End of Epoch 1/6, Average Loss=6.5044050216674805, Class Loss=0.563178300857544, Reg Loss=5.941226959228516
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/97, Loss=18.41094801723957
Loss made of: CE 0.419230580329895, LKD 5.556793212890625, LDE 0.0, LReg 0.0, POD 14.221797943115234 EntMin 0.0
Epoch 2, Batch 20/97, Loss=17.955104172229767
Loss made of: CE 0.46821749210357666, LKD 5.521929740905762, LDE 0.0, LReg 0.0, POD 12.682441711425781 EntMin 0.0
Epoch 2, Batch 30/97, Loss=17.936359375715256
Loss made of: CE 0.3431626260280609, LKD 5.990411281585693, LDE 0.0, LReg 0.0, POD 12.276838302612305 EntMin 0.0
Epoch 2, Batch 40/97, Loss=18.242331114411353
Loss made of: CE 0.38152265548706055, LKD 5.9985151290893555, LDE 0.0, LReg 0.0, POD 11.407341003417969 EntMin 0.0
Epoch 2, Batch 50/97, Loss=18.131302484869956
Loss made of: CE 0.2876693308353424, LKD 5.653491020202637, LDE 0.0, LReg 0.0, POD 11.613581657409668 EntMin 0.0
Epoch 2, Batch 60/97, Loss=18.154961720108986
Loss made of: CE 0.417825311422348, LKD 5.666646957397461, LDE 0.0, LReg 0.0, POD 11.969558715820312 EntMin 0.0
Epoch 2, Batch 70/97, Loss=17.494441306591035
Loss made of: CE 0.2873435914516449, LKD 5.350505828857422, LDE 0.0, LReg 0.0, POD 12.059778213500977 EntMin 0.0
Epoch 2, Batch 80/97, Loss=18.251804265379906
Loss made of: CE 0.400080144405365, LKD 5.705294609069824, LDE 0.0, LReg 0.0, POD 12.460945129394531 EntMin 0.0
Epoch 2, Batch 90/97, Loss=18.499482882022857
Loss made of: CE 0.3290069103240967, LKD 5.837533950805664, LDE 0.0, LReg 0.0, POD 11.572662353515625 EntMin 0.0
Epoch 2, Class Loss=0.36947619915008545, Reg Loss=5.847122669219971
Clinet index 6, End of Epoch 2/6, Average Loss=6.216598987579346, Class Loss=0.36947619915008545, Reg Loss=5.847122669219971
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/97, Loss=17.792369601130485
Loss made of: CE 0.3320835530757904, LKD 6.3834662437438965, LDE 0.0, LReg 0.0, POD 11.639259338378906 EntMin 0.0
Epoch 3, Batch 20/97, Loss=17.679238814115525
Loss made of: CE 0.3726820945739746, LKD 5.983677864074707, LDE 0.0, LReg 0.0, POD 11.789628982543945 EntMin 0.0
Epoch 3, Batch 30/97, Loss=17.838634368777274
Loss made of: CE 0.32673293352127075, LKD 5.712344169616699, LDE 0.0, LReg 0.0, POD 11.93909740447998 EntMin 0.0
Epoch 3, Batch 40/97, Loss=17.86743338406086
Loss made of: CE 0.35078489780426025, LKD 5.36145544052124, LDE 0.0, LReg 0.0, POD 11.318717002868652 EntMin 0.0
Epoch 3, Batch 50/97, Loss=18.280081602931023
Loss made of: CE 0.35022449493408203, LKD 6.209101676940918, LDE 0.0, LReg 0.0, POD 11.483541488647461 EntMin 0.0
Epoch 3, Batch 60/97, Loss=18.26071753501892
Loss made of: CE 0.412611186504364, LKD 7.104849815368652, LDE 0.0, LReg 0.0, POD 13.565366744995117 EntMin 0.0
Epoch 3, Batch 70/97, Loss=17.640652987360955
Loss made of: CE 0.25238654017448425, LKD 5.49554443359375, LDE 0.0, LReg 0.0, POD 11.078306198120117 EntMin 0.0
Epoch 3, Batch 80/97, Loss=17.792697444558144
Loss made of: CE 0.26682478189468384, LKD 5.315596103668213, LDE 0.0, LReg 0.0, POD 11.512734413146973 EntMin 0.0
Epoch 3, Batch 90/97, Loss=17.9945259809494
Loss made of: CE 0.3424789607524872, LKD 5.732062339782715, LDE 0.0, LReg 0.0, POD 11.826730728149414 EntMin 0.0
Epoch 3, Class Loss=0.32860472798347473, Reg Loss=5.829110622406006
Clinet index 6, End of Epoch 3/6, Average Loss=6.157715320587158, Class Loss=0.32860472798347473, Reg Loss=5.829110622406006
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/97, Loss=17.637317740917204
Loss made of: CE 0.32712024450302124, LKD 6.087795257568359, LDE 0.0, LReg 0.0, POD 11.621929168701172 EntMin 0.0
Epoch 4, Batch 20/97, Loss=17.8288152217865
Loss made of: CE 0.3079162836074829, LKD 5.656957149505615, LDE 0.0, LReg 0.0, POD 10.987987518310547 EntMin 0.0
Epoch 4, Batch 30/97, Loss=16.93965834379196
Loss made of: CE 0.2793700098991394, LKD 5.757249355316162, LDE 0.0, LReg 0.0, POD 10.492002487182617 EntMin 0.0
Epoch 4, Batch 40/97, Loss=17.236930711567403
Loss made of: CE 0.3523682951927185, LKD 5.766493797302246, LDE 0.0, LReg 0.0, POD 11.526691436767578 EntMin 0.0
Epoch 4, Batch 50/97, Loss=17.404044356942176
Loss made of: CE 0.29778194427490234, LKD 5.771026611328125, LDE 0.0, LReg 0.0, POD 10.869735717773438 EntMin 0.0
Epoch 4, Batch 60/97, Loss=16.897503598034383
Loss made of: CE 0.336090087890625, LKD 5.9733076095581055, LDE 0.0, LReg 0.0, POD 10.748086929321289 EntMin 0.0
Epoch 4, Batch 70/97, Loss=17.19990020394325
Loss made of: CE 0.2767103612422943, LKD 5.39259672164917, LDE 0.0, LReg 0.0, POD 11.306827545166016 EntMin 0.0
Epoch 4, Batch 80/97, Loss=17.752765613794327
Loss made of: CE 0.2792437970638275, LKD 5.778876304626465, LDE 0.0, LReg 0.0, POD 11.311562538146973 EntMin 0.0
Epoch 4, Batch 90/97, Loss=17.24202981889248
Loss made of: CE 0.4417211711406708, LKD 6.631625175476074, LDE 0.0, LReg 0.0, POD 11.432849884033203 EntMin 0.0
Epoch 4, Class Loss=0.30937978625297546, Reg Loss=5.79680061340332
Clinet index 6, End of Epoch 4/6, Average Loss=6.106180191040039, Class Loss=0.30937978625297546, Reg Loss=5.79680061340332
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/97, Loss=17.53194643706083
Loss made of: CE 0.2523777186870575, LKD 5.975148677825928, LDE 0.0, LReg 0.0, POD 11.958698272705078 EntMin 0.0
Epoch 5, Batch 20/97, Loss=17.184036634862423
Loss made of: CE 0.2641586661338806, LKD 5.552325248718262, LDE 0.0, LReg 0.0, POD 10.562871932983398 EntMin 0.0
Epoch 5, Batch 30/97, Loss=17.053772535920142
Loss made of: CE 0.31479543447494507, LKD 6.495275020599365, LDE 0.0, LReg 0.0, POD 11.341463088989258 EntMin 0.0
Epoch 5, Batch 40/97, Loss=16.961929413676263
Loss made of: CE 0.39241188764572144, LKD 6.391766548156738, LDE 0.0, LReg 0.0, POD 11.851224899291992 EntMin 0.0
Epoch 5, Batch 50/97, Loss=17.05649304538965
Loss made of: CE 0.2542189955711365, LKD 5.338132381439209, LDE 0.0, LReg 0.0, POD 10.571664810180664 EntMin 0.0
Epoch 5, Batch 60/97, Loss=17.30079915970564
Loss made of: CE 0.2417115420103073, LKD 5.335099697113037, LDE 0.0, LReg 0.0, POD 11.721696853637695 EntMin 0.0
Epoch 5, Batch 70/97, Loss=16.882023671269415
Loss made of: CE 0.23197773098945618, LKD 6.143180847167969, LDE 0.0, LReg 0.0, POD 10.962620735168457 EntMin 0.0
Epoch 5, Batch 80/97, Loss=17.527442137897015
Loss made of: CE 0.29631736874580383, LKD 6.087270259857178, LDE 0.0, LReg 0.0, POD 10.831660270690918 EntMin 0.0
Epoch 5, Batch 90/97, Loss=17.102469600737095
Loss made of: CE 0.2608806788921356, LKD 5.690074443817139, LDE 0.0, LReg 0.0, POD 10.457047462463379 EntMin 0.0
Epoch 5, Class Loss=0.2972067594528198, Reg Loss=5.776161193847656
Clinet index 6, End of Epoch 5/6, Average Loss=6.073368072509766, Class Loss=0.2972067594528198, Reg Loss=5.776161193847656
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/97, Loss=16.92930480837822
Loss made of: CE 0.2787175476551056, LKD 6.0100202560424805, LDE 0.0, LReg 0.0, POD 10.073498725891113 EntMin 0.0
Epoch 6, Batch 20/97, Loss=16.961586138606073
Loss made of: CE 0.20678779482841492, LKD 5.3591413497924805, LDE 0.0, LReg 0.0, POD 10.65186882019043 EntMin 0.0
Epoch 6, Batch 30/97, Loss=16.870411904156207
Loss made of: CE 0.24449893832206726, LKD 7.040380001068115, LDE 0.0, LReg 0.0, POD 11.585525512695312 EntMin 0.0
Epoch 6, Batch 40/97, Loss=17.312685182690622
Loss made of: CE 0.32803961634635925, LKD 5.766265869140625, LDE 0.0, LReg 0.0, POD 10.72064208984375 EntMin 0.0
Epoch 6, Batch 50/97, Loss=16.618455062806607
Loss made of: CE 0.27278411388397217, LKD 5.51402473449707, LDE 0.0, LReg 0.0, POD 9.810956001281738 EntMin 0.0
Epoch 6, Batch 60/97, Loss=17.408727353811265
Loss made of: CE 0.3384495973587036, LKD 6.194970607757568, LDE 0.0, LReg 0.0, POD 11.400154113769531 EntMin 0.0
Epoch 6, Batch 70/97, Loss=16.737875939905642
Loss made of: CE 0.322624534368515, LKD 6.276302337646484, LDE 0.0, LReg 0.0, POD 9.971478462219238 EntMin 0.0
Epoch 6, Batch 80/97, Loss=16.529977245628835
Loss made of: CE 0.2913541793823242, LKD 5.022782802581787, LDE 0.0, LReg 0.0, POD 10.626748085021973 EntMin 0.0
Epoch 6, Batch 90/97, Loss=17.00305579453707
Loss made of: CE 0.27295106649398804, LKD 5.944036483764648, LDE 0.0, LReg 0.0, POD 11.254181861877441 EntMin 0.0
Epoch 6, Class Loss=0.28642505407333374, Reg Loss=5.781813621520996
Clinet index 6, End of Epoch 6/6, Average Loss=6.068238735198975, Class Loss=0.28642505407333374, Reg Loss=5.781813621520996
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.67326819896698, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.796104
Mean Acc: 0.488788
FreqW Acc: 0.676709
Mean IoU: 0.391140
Class IoU:
	class 0: 0.82199746
	class 1: 0.7410105
	class 2: 0.3338292
	class 3: 0.6703913
	class 4: 0.62206864
	class 5: 0.7465964
	class 6: 0.6216416
	class 7: 0.83350986
	class 8: 0.76190436
	class 9: 0.058744017
	class 10: 0.0
	class 11: 0.0065809255
	class 12: 0.40314305
	class 13: 0.021331998
	class 14: 0.0064039347
	class 15: 0.00022974107
	class 16: 0.0
Class Acc:
	class 0: 0.9625721
	class 1: 0.7542485
	class 2: 0.91504085
	class 3: 0.68706715
	class 4: 0.6706369
	class 5: 0.8508794
	class 6: 0.6251236
	class 7: 0.9327227
	class 8: 0.8342335
	class 9: 0.31850955
	class 10: 0.0
	class 11: 0.0065823067
	class 12: 0.72252023
	class 13: 0.022613412
	class 14: 0.0064178025
	class 15: 0.00022974577
	class 16: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[20, 2, 24, 4]
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=20.909412011504173
Loss made of: CE 0.48589619994163513, LKD 6.654569149017334, LDE 0.0, LReg 0.0, POD 11.653169631958008 EntMin 0.0
Epoch 1, Batch 20/97, Loss=18.222205859422683
Loss made of: CE 0.4957464933395386, LKD 5.531362056732178, LDE 0.0, LReg 0.0, POD 11.665838241577148 EntMin 0.0
Epoch 1, Batch 30/97, Loss=18.767662760615348
Loss made of: CE 0.36289986968040466, LKD 5.638526916503906, LDE 0.0, LReg 0.0, POD 10.652299880981445 EntMin 0.0
Epoch 1, Batch 40/97, Loss=18.91292234957218
Loss made of: CE 0.3724249303340912, LKD 6.01655387878418, LDE 0.0, LReg 0.0, POD 10.55052375793457 EntMin 0.0
Epoch 1, Batch 50/97, Loss=18.048523154854774
Loss made of: CE 0.42304229736328125, LKD 6.778262138366699, LDE 0.0, LReg 0.0, POD 11.634649276733398 EntMin 0.0
Epoch 1, Batch 60/97, Loss=18.127032589912414
Loss made of: CE 0.3301871418952942, LKD 5.402766227722168, LDE 0.0, LReg 0.0, POD 11.793745994567871 EntMin 0.0
Epoch 1, Batch 70/97, Loss=18.068060132861138
Loss made of: CE 0.38011687994003296, LKD 5.923614025115967, LDE 0.0, LReg 0.0, POD 12.15383529663086 EntMin 0.0
Epoch 1, Batch 80/97, Loss=17.667023521661758
Loss made of: CE 0.3309669494628906, LKD 6.434023857116699, LDE 0.0, LReg 0.0, POD 11.623379707336426 EntMin 0.0
Epoch 1, Batch 90/97, Loss=18.16399086713791
Loss made of: CE 0.3595733642578125, LKD 6.8127336502075195, LDE 0.0, LReg 0.0, POD 11.50074577331543 EntMin 0.0
Epoch 1, Class Loss=0.4388873279094696, Reg Loss=5.984559059143066
Clinet index 20, End of Epoch 1/6, Average Loss=6.423446178436279, Class Loss=0.4388873279094696, Reg Loss=5.984559059143066
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/97, Loss=17.47506987154484
Loss made of: CE 0.4251689314842224, LKD 6.311339378356934, LDE 0.0, LReg 0.0, POD 11.93464469909668 EntMin 0.0
Epoch 2, Batch 20/97, Loss=17.524664379656315
Loss made of: CE 0.3770138621330261, LKD 6.2034502029418945, LDE 0.0, LReg 0.0, POD 11.817037582397461 EntMin 0.0
Epoch 2, Batch 30/97, Loss=17.6317112326622
Loss made of: CE 0.3049892783164978, LKD 7.049535751342773, LDE 0.0, LReg 0.0, POD 11.775856018066406 EntMin 0.0
Epoch 2, Batch 40/97, Loss=17.749202319979666
Loss made of: CE 0.33160266280174255, LKD 6.904143333435059, LDE 0.0, LReg 0.0, POD 11.121569633483887 EntMin 0.0
Epoch 2, Batch 50/97, Loss=17.66026631295681
Loss made of: CE 0.30518895387649536, LKD 6.269801139831543, LDE 0.0, LReg 0.0, POD 11.269058227539062 EntMin 0.0
Epoch 2, Batch 60/97, Loss=17.777000948786736
Loss made of: CE 0.3599011301994324, LKD 5.457670211791992, LDE 0.0, LReg 0.0, POD 11.064695358276367 EntMin 0.0
Epoch 2, Batch 70/97, Loss=18.148868829011917
Loss made of: CE 0.40563124418258667, LKD 5.876086235046387, LDE 0.0, LReg 0.0, POD 12.28919792175293 EntMin 0.0
Epoch 2, Batch 80/97, Loss=17.65387890934944
Loss made of: CE 0.32181939482688904, LKD 6.199004173278809, LDE 0.0, LReg 0.0, POD 10.565135955810547 EntMin 0.0
Epoch 2, Batch 90/97, Loss=17.171084922552108
Loss made of: CE 0.3079836964607239, LKD 5.9361114501953125, LDE 0.0, LReg 0.0, POD 10.426445007324219 EntMin 0.0
Epoch 2, Class Loss=0.33619266748428345, Reg Loss=5.970325946807861
Clinet index 20, End of Epoch 2/6, Average Loss=6.3065185546875, Class Loss=0.33619266748428345, Reg Loss=5.970325946807861
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/97, Loss=17.934415060281754
Loss made of: CE 0.3119974732398987, LKD 5.913379669189453, LDE 0.0, LReg 0.0, POD 11.189352035522461 EntMin 0.0
Epoch 3, Batch 20/97, Loss=17.53354847729206
Loss made of: CE 0.3205258846282959, LKD 6.09051513671875, LDE 0.0, LReg 0.0, POD 12.039308547973633 EntMin 0.0
Epoch 3, Batch 30/97, Loss=17.22663441300392
Loss made of: CE 0.2789004445075989, LKD 6.268521308898926, LDE 0.0, LReg 0.0, POD 10.849960327148438 EntMin 0.0
Epoch 3, Batch 40/97, Loss=17.3330258667469
Loss made of: CE 0.37077972292900085, LKD 4.942262172698975, LDE 0.0, LReg 0.0, POD 12.178159713745117 EntMin 0.0
Epoch 3, Batch 50/97, Loss=17.250115552544592
Loss made of: CE 0.3044981360435486, LKD 6.110136985778809, LDE 0.0, LReg 0.0, POD 10.545755386352539 EntMin 0.0
Epoch 3, Batch 60/97, Loss=17.897575601935387
Loss made of: CE 0.30830317735671997, LKD 6.732414245605469, LDE 0.0, LReg 0.0, POD 10.52335262298584 EntMin 0.0
Epoch 3, Batch 70/97, Loss=17.564462900161743
Loss made of: CE 0.28016895055770874, LKD 6.115645408630371, LDE 0.0, LReg 0.0, POD 10.804750442504883 EntMin 0.0
Epoch 3, Batch 80/97, Loss=17.413743314146995
Loss made of: CE 0.24586373567581177, LKD 5.743590831756592, LDE 0.0, LReg 0.0, POD 10.371063232421875 EntMin 0.0
Epoch 3, Batch 90/97, Loss=17.496918727457523
Loss made of: CE 0.28653571009635925, LKD 5.834484100341797, LDE 0.0, LReg 0.0, POD 10.14061164855957 EntMin 0.0
Epoch 3, Class Loss=0.31166693568229675, Reg Loss=5.95988655090332
Clinet index 20, End of Epoch 3/6, Average Loss=6.2715535163879395, Class Loss=0.31166693568229675, Reg Loss=5.95988655090332
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/97, Loss=17.639977449178694
Loss made of: CE 0.2685413956642151, LKD 6.270936489105225, LDE 0.0, LReg 0.0, POD 10.82169246673584 EntMin 0.0
Epoch 4, Batch 20/97, Loss=17.54876550734043
Loss made of: CE 0.31038743257522583, LKD 5.901496887207031, LDE 0.0, LReg 0.0, POD 11.279068946838379 EntMin 0.0
Epoch 4, Batch 30/97, Loss=17.52598692178726
Loss made of: CE 0.24919211864471436, LKD 5.882615566253662, LDE 0.0, LReg 0.0, POD 10.964760780334473 EntMin 0.0
Epoch 4, Batch 40/97, Loss=17.36227218210697
Loss made of: CE 0.34400102496147156, LKD 6.026256084442139, LDE 0.0, LReg 0.0, POD 10.747642517089844 EntMin 0.0
Epoch 4, Batch 50/97, Loss=17.265076807141305
Loss made of: CE 0.2606896162033081, LKD 4.957733154296875, LDE 0.0, LReg 0.0, POD 10.747650146484375 EntMin 0.0
Epoch 4, Batch 60/97, Loss=16.868884509801866
Loss made of: CE 0.28665459156036377, LKD 5.74969482421875, LDE 0.0, LReg 0.0, POD 11.08711051940918 EntMin 0.0
Epoch 4, Batch 70/97, Loss=17.00076832920313
Loss made of: CE 0.3260050415992737, LKD 5.767541885375977, LDE 0.0, LReg 0.0, POD 11.706476211547852 EntMin 0.0
Epoch 4, Batch 80/97, Loss=17.88120513409376
Loss made of: CE 0.28068098425865173, LKD 5.745309352874756, LDE 0.0, LReg 0.0, POD 10.904777526855469 EntMin 0.0
Epoch 4, Batch 90/97, Loss=17.587567932903767
Loss made of: CE 0.3574008345603943, LKD 6.472177028656006, LDE 0.0, LReg 0.0, POD 11.890165328979492 EntMin 0.0
Epoch 4, Class Loss=0.2989102005958557, Reg Loss=5.995813369750977
Clinet index 20, End of Epoch 4/6, Average Loss=6.2947235107421875, Class Loss=0.2989102005958557, Reg Loss=5.995813369750977
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/97, Loss=17.49987373948097
Loss made of: CE 0.3046252727508545, LKD 7.5203070640563965, LDE 0.0, LReg 0.0, POD 11.340822219848633 EntMin 0.0
Epoch 5, Batch 20/97, Loss=17.20635429620743
Loss made of: CE 0.25033628940582275, LKD 6.116693019866943, LDE 0.0, LReg 0.0, POD 11.035354614257812 EntMin 0.0
Epoch 5, Batch 30/97, Loss=17.055501526594163
Loss made of: CE 0.2646489441394806, LKD 5.379473686218262, LDE 0.0, LReg 0.0, POD 10.341115951538086 EntMin 0.0
Epoch 5, Batch 40/97, Loss=16.88281556367874
Loss made of: CE 0.26087671518325806, LKD 5.704080581665039, LDE 0.0, LReg 0.0, POD 9.946006774902344 EntMin 0.0
Epoch 5, Batch 50/97, Loss=16.953646194934844
Loss made of: CE 0.29707086086273193, LKD 6.626420021057129, LDE 0.0, LReg 0.0, POD 11.262537002563477 EntMin 0.0
Epoch 5, Batch 60/97, Loss=16.796010527014733
Loss made of: CE 0.30725592374801636, LKD 6.138996124267578, LDE 0.0, LReg 0.0, POD 10.497200012207031 EntMin 0.0
Epoch 5, Batch 70/97, Loss=16.844242933392525
Loss made of: CE 0.2722387909889221, LKD 5.919715881347656, LDE 0.0, LReg 0.0, POD 10.517775535583496 EntMin 0.0
Epoch 5, Batch 80/97, Loss=17.02942831814289
Loss made of: CE 0.27582553029060364, LKD 5.893729209899902, LDE 0.0, LReg 0.0, POD 10.328408241271973 EntMin 0.0
Epoch 5, Batch 90/97, Loss=16.918190851807594
Loss made of: CE 0.28032785654067993, LKD 6.007678985595703, LDE 0.0, LReg 0.0, POD 9.632235527038574 EntMin 0.0
Epoch 5, Class Loss=0.2909557521343231, Reg Loss=5.968093395233154
Clinet index 20, End of Epoch 5/6, Average Loss=6.259048938751221, Class Loss=0.2909557521343231, Reg Loss=5.968093395233154
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/97, Loss=16.470000287890436
Loss made of: CE 0.26504993438720703, LKD 5.65072774887085, LDE 0.0, LReg 0.0, POD 10.431906700134277 EntMin 0.0
Epoch 6, Batch 20/97, Loss=17.24074432104826
Loss made of: CE 0.35133057832717896, LKD 5.654202461242676, LDE 0.0, LReg 0.0, POD 11.01324462890625 EntMin 0.0
Epoch 6, Batch 30/97, Loss=16.541160832345486
Loss made of: CE 0.23246753215789795, LKD 5.33167028427124, LDE 0.0, LReg 0.0, POD 10.62080192565918 EntMin 0.0
Epoch 6, Batch 40/97, Loss=16.637738133966923
Loss made of: CE 0.3028796315193176, LKD 5.9371232986450195, LDE 0.0, LReg 0.0, POD 10.503561019897461 EntMin 0.0
Epoch 6, Batch 50/97, Loss=16.764467139542102
Loss made of: CE 0.25186043977737427, LKD 5.362823009490967, LDE 0.0, LReg 0.0, POD 9.714313507080078 EntMin 0.0
Epoch 6, Batch 60/97, Loss=16.98512502759695
Loss made of: CE 0.2419297993183136, LKD 6.037395000457764, LDE 0.0, LReg 0.0, POD 10.41267204284668 EntMin 0.0
Epoch 6, Batch 70/97, Loss=16.477474129199983
Loss made of: CE 0.33965441584587097, LKD 5.648365020751953, LDE 0.0, LReg 0.0, POD 9.69678783416748 EntMin 0.0
Epoch 6, Batch 80/97, Loss=16.817834036052226
Loss made of: CE 0.3245306611061096, LKD 6.664271831512451, LDE 0.0, LReg 0.0, POD 11.816020965576172 EntMin 0.0
Epoch 6, Batch 90/97, Loss=16.990607775747776
Loss made of: CE 0.24983546137809753, LKD 5.573591232299805, LDE 0.0, LReg 0.0, POD 9.746129989624023 EntMin 0.0
Epoch 6, Class Loss=0.28290191292762756, Reg Loss=5.957311630249023
Clinet index 20, End of Epoch 6/6, Average Loss=6.240213394165039, Class Loss=0.28290191292762756, Reg Loss=5.957311630249023
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/12, Loss=18.33229325711727
Loss made of: CE 0.7423069477081299, LKD 5.986199855804443, LDE 0.0, LReg 0.0, POD 10.626683235168457 EntMin 0.0
Epoch 1, Class Loss=0.6142030358314514, Reg Loss=6.02399206161499
Clinet index 2, End of Epoch 1/6, Average Loss=6.638195037841797, Class Loss=0.6142030358314514, Reg Loss=6.02399206161499
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=17.594314643740653
Loss made of: CE 0.5077978372573853, LKD 5.813249111175537, LDE 0.0, LReg 0.0, POD 10.013189315795898 EntMin 0.0
Epoch 2, Class Loss=0.567203938961029, Reg Loss=5.978282928466797
Clinet index 2, End of Epoch 2/6, Average Loss=6.545486927032471, Class Loss=0.567203938961029, Reg Loss=5.978282928466797
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=17.89040410220623
Loss made of: CE 0.5250046253204346, LKD 6.05288028717041, LDE 0.0, LReg 0.0, POD 11.450815200805664 EntMin 0.0
Epoch 3, Class Loss=0.5498230457305908, Reg Loss=6.0302019119262695
Clinet index 2, End of Epoch 3/6, Average Loss=6.580024719238281, Class Loss=0.5498230457305908, Reg Loss=6.0302019119262695
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=17.59420185983181
Loss made of: CE 0.5376911163330078, LKD 6.643369197845459, LDE 0.0, LReg 0.0, POD 11.099766731262207 EntMin 0.0
Epoch 4, Class Loss=0.5142003297805786, Reg Loss=6.0818281173706055
Clinet index 2, End of Epoch 4/6, Average Loss=6.5960283279418945, Class Loss=0.5142003297805786, Reg Loss=6.0818281173706055
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=17.520996329188346
Loss made of: CE 0.5664201974868774, LKD 6.529475688934326, LDE 0.0, LReg 0.0, POD 12.424741744995117 EntMin 0.0
Epoch 5, Class Loss=0.49359971284866333, Reg Loss=6.047212600708008
Clinet index 2, End of Epoch 5/6, Average Loss=6.5408124923706055, Class Loss=0.49359971284866333, Reg Loss=6.047212600708008
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=17.38846148252487
Loss made of: CE 0.506047248840332, LKD 6.183303356170654, LDE 0.0, LReg 0.0, POD 10.292880058288574 EntMin 0.0
Epoch 6, Class Loss=0.4892587661743164, Reg Loss=6.054723262786865
Clinet index 2, End of Epoch 6/6, Average Loss=6.543982028961182, Class Loss=0.4892587661743164, Reg Loss=6.054723262786865
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=17.765167659521104
Loss made of: CE 0.514744758605957, LKD 5.72983455657959, LDE 0.0, LReg 0.0, POD 10.526254653930664 EntMin 0.0
Epoch 1, Class Loss=0.5340012907981873, Reg Loss=5.794843673706055
Clinet index 24, End of Epoch 1/6, Average Loss=6.328845024108887, Class Loss=0.5340012907981873, Reg Loss=5.794843673706055
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=17.78015015721321
Loss made of: CE 0.6379048824310303, LKD 6.00801420211792, LDE 0.0, LReg 0.0, POD 12.728214263916016 EntMin 0.0
Epoch 2, Class Loss=0.5042069554328918, Reg Loss=5.830968379974365
Clinet index 24, End of Epoch 2/6, Average Loss=6.335175514221191, Class Loss=0.5042069554328918, Reg Loss=5.830968379974365
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=17.8826106518507
Loss made of: CE 0.41439181566238403, LKD 5.847285270690918, LDE 0.0, LReg 0.0, POD 12.155494689941406 EntMin 0.0
Epoch 3, Class Loss=0.4699018597602844, Reg Loss=5.858850002288818
Clinet index 24, End of Epoch 3/6, Average Loss=6.328752040863037, Class Loss=0.4699018597602844, Reg Loss=5.858850002288818
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=17.951817172765733
Loss made of: CE 0.3989858031272888, LKD 5.592032432556152, LDE 0.0, LReg 0.0, POD 11.841323852539062 EntMin 0.0
Epoch 4, Class Loss=0.4713687598705292, Reg Loss=5.868664741516113
Clinet index 24, End of Epoch 4/6, Average Loss=6.340033531188965, Class Loss=0.4713687598705292, Reg Loss=5.868664741516113
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=17.844658935070036
Loss made of: CE 0.35059255361557007, LKD 5.73148250579834, LDE 0.0, LReg 0.0, POD 12.401466369628906 EntMin 0.0
Epoch 5, Class Loss=0.4615267217159271, Reg Loss=5.817962646484375
Clinet index 24, End of Epoch 5/6, Average Loss=6.279489517211914, Class Loss=0.4615267217159271, Reg Loss=5.817962646484375
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=17.472878548502923
Loss made of: CE 0.45625942945480347, LKD 5.663022518157959, LDE 0.0, LReg 0.0, POD 10.613041877746582 EntMin 0.0
Epoch 6, Class Loss=0.4478263258934021, Reg Loss=5.905934810638428
Clinet index 24, End of Epoch 6/6, Average Loss=6.353761196136475, Class Loss=0.4478263258934021, Reg Loss=5.905934810638428
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=17.900797668099404
Loss made of: CE 0.394363135099411, LKD 5.799195766448975, LDE 0.0, LReg 0.0, POD 11.185970306396484 EntMin 0.0
Epoch 1, Class Loss=0.5651485919952393, Reg Loss=5.9407172203063965
Clinet index 4, End of Epoch 1/6, Average Loss=6.505866050720215, Class Loss=0.5651485919952393, Reg Loss=5.9407172203063965
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=17.887381029129028
Loss made of: CE 0.5427844524383545, LKD 6.19024658203125, LDE 0.0, LReg 0.0, POD 13.428495407104492 EntMin 0.0
Epoch 2, Class Loss=0.5428786873817444, Reg Loss=5.941993713378906
Clinet index 4, End of Epoch 2/6, Average Loss=6.484872341156006, Class Loss=0.5428786873817444, Reg Loss=5.941993713378906
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=17.9557193338871
Loss made of: CE 0.7296766638755798, LKD 6.559331893920898, LDE 0.0, LReg 0.0, POD 11.912178993225098 EntMin 0.0
Epoch 3, Class Loss=0.5093393325805664, Reg Loss=5.959575176239014
Clinet index 4, End of Epoch 3/6, Average Loss=6.46891450881958, Class Loss=0.5093393325805664, Reg Loss=5.959575176239014
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=17.73689824640751
Loss made of: CE 0.6063370108604431, LKD 5.557969093322754, LDE 0.0, LReg 0.0, POD 11.668660163879395 EntMin 0.0
Epoch 4, Class Loss=0.4838530421257019, Reg Loss=5.891707897186279
Clinet index 4, End of Epoch 4/6, Average Loss=6.375560760498047, Class Loss=0.4838530421257019, Reg Loss=5.891707897186279
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=17.216091164946555
Loss made of: CE 0.5042976140975952, LKD 5.877044677734375, LDE 0.0, LReg 0.0, POD 11.286596298217773 EntMin 0.0
Epoch 5, Class Loss=0.4767197072505951, Reg Loss=5.858474254608154
Clinet index 4, End of Epoch 5/6, Average Loss=6.335194110870361, Class Loss=0.4767197072505951, Reg Loss=5.858474254608154
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=17.09462462961674
Loss made of: CE 0.4494903087615967, LKD 5.636147975921631, LDE 0.0, LReg 0.0, POD 10.476572036743164 EntMin 0.0
Epoch 6, Class Loss=0.4699472188949585, Reg Loss=5.881434917449951
Clinet index 4, End of Epoch 6/6, Average Loss=6.351382255554199, Class Loss=0.4699472188949585, Reg Loss=5.881434917449951
federated aggregation...
Validation, Class Loss=0.6411017179489136, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.811549
Mean Acc: 0.510801
FreqW Acc: 0.710041
Mean IoU: 0.407472
Class IoU:
	class 0: 0.84291315
	class 1: 0.7458005
	class 2: 0.31705663
	class 3: 0.683565
	class 4: 0.6250321
	class 5: 0.74389213
	class 6: 0.6229912
	class 7: 0.8386373
	class 8: 0.76076955
	class 9: 0.05770207
	class 10: 0.0
	class 11: 0.009365488
	class 12: 0.39501154
	class 13: 0.010669795
	class 14: 0.0033418173
	class 15: 0.27012995
	class 16: 0.00014955817
Class Acc:
	class 0: 0.9547413
	class 1: 0.75946575
	class 2: 0.9346053
	class 3: 0.70816594
	class 4: 0.66888773
	class 5: 0.85332435
	class 6: 0.6266612
	class 7: 0.9365453
	class 8: 0.85739154
	class 9: 0.34826094
	class 10: 0.0
	class 11: 0.009369189
	class 12: 0.73661935
	class 13: 0.0113248825
	class 14: 0.0033488045
	class 15: 0.27476132
	class 16: 0.00014955859

federated global round: 22, step: 4
select part of clients to conduct local training
[21, 0, 25, 23]
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=19.1452006816864
Loss made of: CE 0.5507996082305908, LKD 6.396121025085449, LDE 0.0, LReg 0.0, POD 12.342632293701172 EntMin 0.0
Epoch 1, Batch 20/97, Loss=18.722782936692237
Loss made of: CE 0.4405903220176697, LKD 6.645555019378662, LDE 0.0, LReg 0.0, POD 12.520732879638672 EntMin 0.0
Epoch 1, Batch 30/97, Loss=17.572525051236152
Loss made of: CE 0.3380850553512573, LKD 5.232034683227539, LDE 0.0, LReg 0.0, POD 11.463184356689453 EntMin 0.0
Epoch 1, Batch 40/97, Loss=17.626784417033196
Loss made of: CE 0.29108476638793945, LKD 5.0060014724731445, LDE 0.0, LReg 0.0, POD 12.37448501586914 EntMin 0.0
Epoch 1, Batch 50/97, Loss=17.52300065755844
Loss made of: CE 0.33466315269470215, LKD 6.640468597412109, LDE 0.0, LReg 0.0, POD 10.9119873046875 EntMin 0.0
Epoch 1, Batch 60/97, Loss=17.616816622018813
Loss made of: CE 0.3031468391418457, LKD 6.967625617980957, LDE 0.0, LReg 0.0, POD 10.773633003234863 EntMin 0.0
Epoch 1, Batch 70/97, Loss=17.794138124585153
Loss made of: CE 0.3343586325645447, LKD 5.447926998138428, LDE 0.0, LReg 0.0, POD 11.654027938842773 EntMin 0.0
Epoch 1, Batch 80/97, Loss=17.230675354599953
Loss made of: CE 0.2975095212459564, LKD 6.2313032150268555, LDE 0.0, LReg 0.0, POD 11.245559692382812 EntMin 0.0
Epoch 1, Batch 90/97, Loss=17.189860412478446
Loss made of: CE 0.28518834710121155, LKD 5.988686561584473, LDE 0.0, LReg 0.0, POD 11.614056587219238 EntMin 0.0
Epoch 1, Class Loss=0.3815479278564453, Reg Loss=5.88252592086792
Clinet index 21, End of Epoch 1/6, Average Loss=6.264073848724365, Class Loss=0.3815479278564453, Reg Loss=5.88252592086792
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=17.41405262351036
Loss made of: CE 0.35130128264427185, LKD 5.8439435958862305, LDE 0.0, LReg 0.0, POD 11.087785720825195 EntMin 0.0
Epoch 2, Batch 20/97, Loss=17.387085315585136
Loss made of: CE 0.29520440101623535, LKD 6.6936845779418945, LDE 0.0, LReg 0.0, POD 10.032618522644043 EntMin 0.0
Epoch 2, Batch 30/97, Loss=17.262122684717177
Loss made of: CE 0.3198460340499878, LKD 6.13356351852417, LDE 0.0, LReg 0.0, POD 10.96465015411377 EntMin 0.0
Epoch 2, Batch 40/97, Loss=17.688256913423537
Loss made of: CE 0.321210116147995, LKD 6.170513153076172, LDE 0.0, LReg 0.0, POD 10.189306259155273 EntMin 0.0
Epoch 2, Batch 50/97, Loss=16.892726942896843
Loss made of: CE 0.31437110900878906, LKD 6.0415849685668945, LDE 0.0, LReg 0.0, POD 9.962241172790527 EntMin 0.0
Epoch 2, Batch 60/97, Loss=17.128504107892514
Loss made of: CE 0.2886769473552704, LKD 5.65045690536499, LDE 0.0, LReg 0.0, POD 11.276548385620117 EntMin 0.0
Epoch 2, Batch 70/97, Loss=16.941343307495117
Loss made of: CE 0.2765457034111023, LKD 6.212649822235107, LDE 0.0, LReg 0.0, POD 10.886048316955566 EntMin 0.0
Epoch 2, Batch 80/97, Loss=17.143109200894834
Loss made of: CE 0.3577272295951843, LKD 6.469094753265381, LDE 0.0, LReg 0.0, POD 10.916912078857422 EntMin 0.0
Epoch 2, Batch 90/97, Loss=16.9748623251915
Loss made of: CE 0.3556339740753174, LKD 6.501044273376465, LDE 0.0, LReg 0.0, POD 10.955049514770508 EntMin 0.0
Epoch 2, Class Loss=0.3104681670665741, Reg Loss=5.863883972167969
Clinet index 21, End of Epoch 2/6, Average Loss=6.174352169036865, Class Loss=0.3104681670665741, Reg Loss=5.863883972167969
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=16.968278652429582
Loss made of: CE 0.38506340980529785, LKD 6.251354217529297, LDE 0.0, LReg 0.0, POD 11.241769790649414 EntMin 0.0
Epoch 3, Batch 20/97, Loss=17.092118540406226
Loss made of: CE 0.3064907193183899, LKD 6.079152584075928, LDE 0.0, LReg 0.0, POD 10.528909683227539 EntMin 0.0
Epoch 3, Batch 30/97, Loss=16.870109218358994
Loss made of: CE 0.31627804040908813, LKD 5.523523330688477, LDE 0.0, LReg 0.0, POD 10.053091049194336 EntMin 0.0
Epoch 3, Batch 40/97, Loss=16.66843623816967
Loss made of: CE 0.2955646514892578, LKD 6.027403831481934, LDE 0.0, LReg 0.0, POD 11.091949462890625 EntMin 0.0
Epoch 3, Batch 50/97, Loss=16.882245540618896
Loss made of: CE 0.34523218870162964, LKD 5.960458278656006, LDE 0.0, LReg 0.0, POD 10.360298156738281 EntMin 0.0
Epoch 3, Batch 60/97, Loss=16.99579436033964
Loss made of: CE 0.30236297845840454, LKD 5.7167768478393555, LDE 0.0, LReg 0.0, POD 9.85839557647705 EntMin 0.0
Epoch 3, Batch 70/97, Loss=17.159624469280242
Loss made of: CE 0.3549513816833496, LKD 6.198727607727051, LDE 0.0, LReg 0.0, POD 10.799446105957031 EntMin 0.0
Epoch 3, Batch 80/97, Loss=17.16989612132311
Loss made of: CE 0.3011969029903412, LKD 5.637724876403809, LDE 0.0, LReg 0.0, POD 11.040228843688965 EntMin 0.0
Epoch 3, Batch 90/97, Loss=16.771665409207344
Loss made of: CE 0.22976279258728027, LKD 5.4618330001831055, LDE 0.0, LReg 0.0, POD 10.26556396484375 EntMin 0.0
Epoch 3, Class Loss=0.29597845673561096, Reg Loss=5.868803024291992
Clinet index 21, End of Epoch 3/6, Average Loss=6.16478157043457, Class Loss=0.29597845673561096, Reg Loss=5.868803024291992
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=17.13362355530262
Loss made of: CE 0.334497332572937, LKD 6.34562873840332, LDE 0.0, LReg 0.0, POD 10.579584121704102 EntMin 0.0
Epoch 4, Batch 20/97, Loss=16.51933944821358
Loss made of: CE 0.2602713406085968, LKD 5.7098002433776855, LDE 0.0, LReg 0.0, POD 10.381503105163574 EntMin 0.0
Epoch 4, Batch 30/97, Loss=17.209436333179475
Loss made of: CE 0.31756171584129333, LKD 5.510993957519531, LDE 0.0, LReg 0.0, POD 11.077751159667969 EntMin 0.0
Epoch 4, Batch 40/97, Loss=16.666029545664788
Loss made of: CE 0.3243527412414551, LKD 5.83728551864624, LDE 0.0, LReg 0.0, POD 10.692362785339355 EntMin 0.0
Epoch 4, Batch 50/97, Loss=16.724136097729208
Loss made of: CE 0.24508552253246307, LKD 5.8881049156188965, LDE 0.0, LReg 0.0, POD 10.085052490234375 EntMin 0.0
Epoch 4, Batch 60/97, Loss=16.980209651589394
Loss made of: CE 0.3352696895599365, LKD 5.8711957931518555, LDE 0.0, LReg 0.0, POD 11.59205436706543 EntMin 0.0
Epoch 4, Batch 70/97, Loss=17.029011756181717
Loss made of: CE 0.30707424879074097, LKD 6.402705192565918, LDE 0.0, LReg 0.0, POD 10.530919075012207 EntMin 0.0
Epoch 4, Batch 80/97, Loss=16.596442694962025
Loss made of: CE 0.23983222246170044, LKD 5.660874843597412, LDE 0.0, LReg 0.0, POD 9.680662155151367 EntMin 0.0
Epoch 4, Batch 90/97, Loss=17.26129378974438
Loss made of: CE 0.24157387018203735, LKD 6.374614715576172, LDE 0.0, LReg 0.0, POD 10.787611961364746 EntMin 0.0
Epoch 4, Class Loss=0.2889821529388428, Reg Loss=5.869562149047852
Clinet index 21, End of Epoch 4/6, Average Loss=6.158544540405273, Class Loss=0.2889821529388428, Reg Loss=5.869562149047852
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=16.953717841207983
Loss made of: CE 0.20869295299053192, LKD 5.627309799194336, LDE 0.0, LReg 0.0, POD 12.119197845458984 EntMin 0.0
Epoch 5, Batch 20/97, Loss=16.786126139760018
Loss made of: CE 0.3175870180130005, LKD 6.5367751121521, LDE 0.0, LReg 0.0, POD 10.075376510620117 EntMin 0.0
Epoch 5, Batch 30/97, Loss=16.484798361361026
Loss made of: CE 0.2601549029350281, LKD 5.24930477142334, LDE 0.0, LReg 0.0, POD 11.354225158691406 EntMin 0.0
Epoch 5, Batch 40/97, Loss=16.971352069079877
Loss made of: CE 0.3096700608730316, LKD 5.197296619415283, LDE 0.0, LReg 0.0, POD 11.278700828552246 EntMin 0.0
Epoch 5, Batch 50/97, Loss=16.543951615691185
Loss made of: CE 0.24942903220653534, LKD 5.92333984375, LDE 0.0, LReg 0.0, POD 10.942018508911133 EntMin 0.0
Epoch 5, Batch 60/97, Loss=16.913190600275993
Loss made of: CE 0.2964773178100586, LKD 6.125644683837891, LDE 0.0, LReg 0.0, POD 10.27461051940918 EntMin 0.0
Epoch 5, Batch 70/97, Loss=16.823738852143286
Loss made of: CE 0.2528291940689087, LKD 5.431167125701904, LDE 0.0, LReg 0.0, POD 10.04702091217041 EntMin 0.0
Epoch 5, Batch 80/97, Loss=16.621784989535808
Loss made of: CE 0.2979709208011627, LKD 6.286717891693115, LDE 0.0, LReg 0.0, POD 10.9984130859375 EntMin 0.0
Epoch 5, Batch 90/97, Loss=16.75451252311468
Loss made of: CE 0.31823650002479553, LKD 6.515694618225098, LDE 0.0, LReg 0.0, POD 10.1918306350708 EntMin 0.0
Epoch 5, Class Loss=0.2782767713069916, Reg Loss=5.866307735443115
Clinet index 21, End of Epoch 5/6, Average Loss=6.144584655761719, Class Loss=0.2782767713069916, Reg Loss=5.866307735443115
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=16.153390154242516
Loss made of: CE 0.277596116065979, LKD 5.235677719116211, LDE 0.0, LReg 0.0, POD 10.99323844909668 EntMin 0.0
Epoch 6, Batch 20/97, Loss=16.11829338222742
Loss made of: CE 0.22351698577404022, LKD 5.7607622146606445, LDE 0.0, LReg 0.0, POD 8.91004753112793 EntMin 0.0
Epoch 6, Batch 30/97, Loss=17.08191685974598
Loss made of: CE 0.28545472025871277, LKD 5.676791191101074, LDE 0.0, LReg 0.0, POD 10.088888168334961 EntMin 0.0
Epoch 6, Batch 40/97, Loss=16.683194833993912
Loss made of: CE 0.2840346097946167, LKD 6.565847396850586, LDE 0.0, LReg 0.0, POD 10.270135879516602 EntMin 0.0
Epoch 6, Batch 50/97, Loss=16.096461476385592
Loss made of: CE 0.26978835463523865, LKD 5.9740095138549805, LDE 0.0, LReg 0.0, POD 9.556538581848145 EntMin 0.0
Epoch 6, Batch 60/97, Loss=16.090350940823555
Loss made of: CE 0.2711288332939148, LKD 5.894266128540039, LDE 0.0, LReg 0.0, POD 10.980551719665527 EntMin 0.0
Epoch 6, Batch 70/97, Loss=16.176898524165154
Loss made of: CE 0.2908802032470703, LKD 5.7536139488220215, LDE 0.0, LReg 0.0, POD 9.71821403503418 EntMin 0.0
Epoch 6, Batch 80/97, Loss=16.380774465203285
Loss made of: CE 0.340606153011322, LKD 6.446696758270264, LDE 0.0, LReg 0.0, POD 10.465353965759277 EntMin 0.0
Epoch 6, Batch 90/97, Loss=16.220179855823517
Loss made of: CE 0.3215775191783905, LKD 6.8689422607421875, LDE 0.0, LReg 0.0, POD 9.499284744262695 EntMin 0.0
Epoch 6, Class Loss=0.2729733884334564, Reg Loss=5.84420919418335
Clinet index 21, End of Epoch 6/6, Average Loss=6.117182731628418, Class Loss=0.2729733884334564, Reg Loss=5.84420919418335
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=18.541149631142616
Loss made of: CE 0.6252739429473877, LKD 5.609783172607422, LDE 0.0, LReg 0.0, POD 11.649633407592773 EntMin 0.0
Epoch 1, Batch 20/97, Loss=18.129163986444475
Loss made of: CE 0.35443049669265747, LKD 5.961036682128906, LDE 0.0, LReg 0.0, POD 9.804336547851562 EntMin 0.0
Epoch 1, Batch 30/97, Loss=17.680644235014917
Loss made of: CE 0.32847854495048523, LKD 5.6887006759643555, LDE 0.0, LReg 0.0, POD 11.451143264770508 EntMin 0.0
Epoch 1, Batch 40/97, Loss=17.817976287007333
Loss made of: CE 0.42337942123413086, LKD 5.5094428062438965, LDE 0.0, LReg 0.0, POD 12.928363800048828 EntMin 0.0
Epoch 1, Batch 50/97, Loss=17.18000027537346
Loss made of: CE 0.46276921033859253, LKD 5.556241512298584, LDE 0.0, LReg 0.0, POD 13.080741882324219 EntMin 0.0
Epoch 1, Batch 60/97, Loss=17.065071080625057
Loss made of: CE 0.3148517310619354, LKD 5.752778053283691, LDE 0.0, LReg 0.0, POD 10.912933349609375 EntMin 0.0
Epoch 1, Batch 70/97, Loss=17.5313272356987
Loss made of: CE 0.4423554539680481, LKD 5.689996242523193, LDE 0.0, LReg 0.0, POD 11.932985305786133 EntMin 0.0
Epoch 1, Batch 80/97, Loss=17.242070239782333
Loss made of: CE 0.3604828119277954, LKD 5.518424034118652, LDE 0.0, LReg 0.0, POD 10.478835105895996 EntMin 0.0
Epoch 1, Batch 90/97, Loss=17.571509131789206
Loss made of: CE 0.27227434515953064, LKD 5.138415813446045, LDE 0.0, LReg 0.0, POD 11.805176734924316 EntMin 0.0
Epoch 1, Class Loss=0.3817938566207886, Reg Loss=5.85231351852417
Clinet index 0, End of Epoch 1/6, Average Loss=6.234107494354248, Class Loss=0.3817938566207886, Reg Loss=5.85231351852417
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=17.3874313890934
Loss made of: CE 0.2876431941986084, LKD 5.88472843170166, LDE 0.0, LReg 0.0, POD 10.568666458129883 EntMin 0.0
Epoch 2, Batch 20/97, Loss=17.117557379603387
Loss made of: CE 0.31578537821769714, LKD 5.519262313842773, LDE 0.0, LReg 0.0, POD 11.297338485717773 EntMin 0.0
Epoch 2, Batch 30/97, Loss=17.523419216275215
Loss made of: CE 0.2863192558288574, LKD 5.73114013671875, LDE 0.0, LReg 0.0, POD 10.379413604736328 EntMin 0.0
Epoch 2, Batch 40/97, Loss=17.069465702772142
Loss made of: CE 0.3239714503288269, LKD 5.867700099945068, LDE 0.0, LReg 0.0, POD 11.479625701904297 EntMin 0.0
Epoch 2, Batch 50/97, Loss=17.45459173619747
Loss made of: CE 0.2896704375743866, LKD 5.515329360961914, LDE 0.0, LReg 0.0, POD 10.914813995361328 EntMin 0.0
Epoch 2, Batch 60/97, Loss=17.271843510866166
Loss made of: CE 0.3171839118003845, LKD 6.207845687866211, LDE 0.0, LReg 0.0, POD 11.614644050598145 EntMin 0.0
Epoch 2, Batch 70/97, Loss=17.5395178347826
Loss made of: CE 0.31651633977890015, LKD 6.067559719085693, LDE 0.0, LReg 0.0, POD 11.070259094238281 EntMin 0.0
Epoch 2, Batch 80/97, Loss=17.203579950332642
Loss made of: CE 0.29923003911972046, LKD 5.526979446411133, LDE 0.0, LReg 0.0, POD 11.509557723999023 EntMin 0.0
Epoch 2, Batch 90/97, Loss=17.82911636829376
Loss made of: CE 0.3153919577598572, LKD 6.8835577964782715, LDE 0.0, LReg 0.0, POD 10.286596298217773 EntMin 0.0
Epoch 2, Class Loss=0.3100297451019287, Reg Loss=5.861651420593262
Clinet index 0, End of Epoch 2/6, Average Loss=6.1716814041137695, Class Loss=0.3100297451019287, Reg Loss=5.861651420593262
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=17.093479585647582
Loss made of: CE 0.2582011818885803, LKD 6.398387908935547, LDE 0.0, LReg 0.0, POD 10.095012664794922 EntMin 0.0
Epoch 3, Batch 20/97, Loss=16.87027971893549
Loss made of: CE 0.27392035722732544, LKD 6.599048614501953, LDE 0.0, LReg 0.0, POD 9.930730819702148 EntMin 0.0
Epoch 3, Batch 30/97, Loss=17.6646099999547
Loss made of: CE 0.36227941513061523, LKD 5.90496826171875, LDE 0.0, LReg 0.0, POD 12.206184387207031 EntMin 0.0
Epoch 3, Batch 40/97, Loss=16.998855775594713
Loss made of: CE 0.3032183051109314, LKD 5.544046401977539, LDE 0.0, LReg 0.0, POD 9.829973220825195 EntMin 0.0
Epoch 3, Batch 50/97, Loss=17.053107064962386
Loss made of: CE 0.23463010787963867, LKD 5.725890159606934, LDE 0.0, LReg 0.0, POD 11.129243850708008 EntMin 0.0
Epoch 3, Batch 60/97, Loss=17.225542002916335
Loss made of: CE 0.2759791612625122, LKD 5.813327789306641, LDE 0.0, LReg 0.0, POD 10.770652770996094 EntMin 0.0
Epoch 3, Batch 70/97, Loss=17.202993881702422
Loss made of: CE 0.23597848415374756, LKD 5.646240234375, LDE 0.0, LReg 0.0, POD 12.375358581542969 EntMin 0.0
Epoch 3, Batch 80/97, Loss=17.357833552360535
Loss made of: CE 0.308735728263855, LKD 5.786106109619141, LDE 0.0, LReg 0.0, POD 9.59315299987793 EntMin 0.0
Epoch 3, Batch 90/97, Loss=17.15357612669468
Loss made of: CE 0.3070589005947113, LKD 5.607274532318115, LDE 0.0, LReg 0.0, POD 10.522574424743652 EntMin 0.0
Epoch 3, Class Loss=0.2987457513809204, Reg Loss=5.851593971252441
Clinet index 0, End of Epoch 3/6, Average Loss=6.150339603424072, Class Loss=0.2987457513809204, Reg Loss=5.851593971252441
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=16.578429384529592
Loss made of: CE 0.26116666197776794, LKD 5.977045059204102, LDE 0.0, LReg 0.0, POD 10.637962341308594 EntMin 0.0
Epoch 4, Batch 20/97, Loss=17.032367631793022
Loss made of: CE 0.29729118943214417, LKD 6.310854911804199, LDE 0.0, LReg 0.0, POD 11.699874877929688 EntMin 0.0
Epoch 4, Batch 30/97, Loss=17.42336835414171
Loss made of: CE 0.33275988698005676, LKD 5.8529205322265625, LDE 0.0, LReg 0.0, POD 10.668184280395508 EntMin 0.0
Epoch 4, Batch 40/97, Loss=17.05168647170067
Loss made of: CE 0.3057640790939331, LKD 7.277888298034668, LDE 0.0, LReg 0.0, POD 9.440863609313965 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 50/97, Loss=16.691026416420936
Loss made of: CE 0.30553412437438965, LKD 5.471771240234375, LDE 0.0, LReg 0.0, POD 11.1986665725708 EntMin 0.0
Epoch 4, Batch 60/97, Loss=16.475117142498494
Loss made of: CE 0.2595716714859009, LKD 5.4296722412109375, LDE 0.0, LReg 0.0, POD 11.035813331604004 EntMin 0.0
Epoch 4, Batch 70/97, Loss=16.59186257570982
Loss made of: CE 0.24928991496562958, LKD 6.01657772064209, LDE 0.0, LReg 0.0, POD 10.656120300292969 EntMin 0.0
Epoch 4, Batch 80/97, Loss=16.44295235723257
Loss made of: CE 0.24745723605155945, LKD 6.165694236755371, LDE 0.0, LReg 0.0, POD 9.884883880615234 EntMin 0.0
Epoch 4, Batch 90/97, Loss=17.15236154049635
Loss made of: CE 0.2933577597141266, LKD 5.890460014343262, LDE 0.0, LReg 0.0, POD 10.5122709274292 EntMin 0.0
Epoch 4, Class Loss=0.2846768796443939, Reg Loss=5.841108322143555
Clinet index 0, End of Epoch 4/6, Average Loss=6.1257853507995605, Class Loss=0.2846768796443939, Reg Loss=5.841108322143555
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=16.33059123158455
Loss made of: CE 0.2151157557964325, LKD 5.599076271057129, LDE 0.0, LReg 0.0, POD 10.589964866638184 EntMin 0.0
Epoch 5, Batch 20/97, Loss=17.04643116891384
Loss made of: CE 0.2679191827774048, LKD 5.670236587524414, LDE 0.0, LReg 0.0, POD 10.410323143005371 EntMin 0.0
Epoch 5, Batch 30/97, Loss=16.87248496413231
Loss made of: CE 0.2903323769569397, LKD 5.0659894943237305, LDE 0.0, LReg 0.0, POD 9.9199800491333 EntMin 0.0
Epoch 5, Batch 40/97, Loss=16.78266548514366
Loss made of: CE 0.2517186403274536, LKD 6.652366638183594, LDE 0.0, LReg 0.0, POD 10.948001861572266 EntMin 0.0
Epoch 5, Batch 50/97, Loss=16.173595644533634
Loss made of: CE 0.3561093211174011, LKD 5.410199165344238, LDE 0.0, LReg 0.0, POD 10.305337905883789 EntMin 0.0
Epoch 5, Batch 60/97, Loss=16.583690311014653
Loss made of: CE 0.338379442691803, LKD 6.127602577209473, LDE 0.0, LReg 0.0, POD 11.01523208618164 EntMin 0.0
Epoch 5, Batch 70/97, Loss=16.724124939739703
Loss made of: CE 0.25615406036376953, LKD 5.815367698669434, LDE 0.0, LReg 0.0, POD 10.80255126953125 EntMin 0.0
Epoch 5, Batch 80/97, Loss=16.374449487030507
Loss made of: CE 0.27771663665771484, LKD 5.5738630294799805, LDE 0.0, LReg 0.0, POD 10.203433990478516 EntMin 0.0
Epoch 5, Batch 90/97, Loss=16.596502351760865
Loss made of: CE 0.27055299282073975, LKD 6.379088401794434, LDE 0.0, LReg 0.0, POD 10.188575744628906 EntMin 0.0
Epoch 5, Class Loss=0.28005483746528625, Reg Loss=5.827507495880127
Clinet index 0, End of Epoch 5/6, Average Loss=6.10756254196167, Class Loss=0.28005483746528625, Reg Loss=5.827507495880127
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=16.1316121712327
Loss made of: CE 0.24925661087036133, LKD 6.390385627746582, LDE 0.0, LReg 0.0, POD 9.567989349365234 EntMin 0.0
Epoch 6, Batch 20/97, Loss=16.41099921911955
Loss made of: CE 0.22566303610801697, LKD 5.272359848022461, LDE 0.0, LReg 0.0, POD 9.05140495300293 EntMin 0.0
Epoch 6, Batch 30/97, Loss=16.82270843684673
Loss made of: CE 0.20334023237228394, LKD 5.76763916015625, LDE 0.0, LReg 0.0, POD 10.70747184753418 EntMin 0.0
Epoch 6, Batch 40/97, Loss=16.944102120399474
Loss made of: CE 0.26096224784851074, LKD 6.045435905456543, LDE 0.0, LReg 0.0, POD 9.827580451965332 EntMin 0.0
Epoch 6, Batch 50/97, Loss=16.53571126759052
Loss made of: CE 0.2380950152873993, LKD 5.253178596496582, LDE 0.0, LReg 0.0, POD 10.189647674560547 EntMin 0.0
Epoch 6, Batch 60/97, Loss=17.14067456573248
Loss made of: CE 0.26431113481521606, LKD 5.177184581756592, LDE 0.0, LReg 0.0, POD 9.488086700439453 EntMin 0.0
Epoch 6, Batch 70/97, Loss=16.45858663022518
Loss made of: CE 0.30859440565109253, LKD 5.711667537689209, LDE 0.0, LReg 0.0, POD 11.076484680175781 EntMin 0.0
Epoch 6, Batch 80/97, Loss=16.671045944094658
Loss made of: CE 0.22432491183280945, LKD 5.428373336791992, LDE 0.0, LReg 0.0, POD 10.458138465881348 EntMin 0.0
Epoch 6, Batch 90/97, Loss=16.634738460183144
Loss made of: CE 0.27547428011894226, LKD 5.865224361419678, LDE 0.0, LReg 0.0, POD 10.241060256958008 EntMin 0.0
Epoch 6, Class Loss=0.2759080231189728, Reg Loss=5.842716217041016
Clinet index 0, End of Epoch 6/6, Average Loss=6.118624210357666, Class Loss=0.2759080231189728, Reg Loss=5.842716217041016
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=18.86889446079731
Loss made of: CE 0.5150666236877441, LKD 5.853848934173584, LDE 0.0, LReg 0.0, POD 11.906243324279785 EntMin 0.0
Epoch 1, Batch 20/97, Loss=17.942210444808005
Loss made of: CE 0.46408140659332275, LKD 6.411791801452637, LDE 0.0, LReg 0.0, POD 12.558504104614258 EntMin 0.0
Epoch 1, Batch 30/97, Loss=17.674472817778586
Loss made of: CE 0.40048083662986755, LKD 5.793299674987793, LDE 0.0, LReg 0.0, POD 11.447216033935547 EntMin 0.0
Epoch 1, Batch 40/97, Loss=17.51318234205246
Loss made of: CE 0.36472949385643005, LKD 6.184793472290039, LDE 0.0, LReg 0.0, POD 11.063825607299805 EntMin 0.0
Epoch 1, Batch 50/97, Loss=17.571054369211197
Loss made of: CE 0.33904486894607544, LKD 5.64026403427124, LDE 0.0, LReg 0.0, POD 11.492927551269531 EntMin 0.0
Epoch 1, Batch 60/97, Loss=17.417851608991622
Loss made of: CE 0.2843061089515686, LKD 5.246452331542969, LDE 0.0, LReg 0.0, POD 11.059216499328613 EntMin 0.0
Epoch 1, Batch 70/97, Loss=17.84752113521099
Loss made of: CE 0.30353981256484985, LKD 5.744189739227295, LDE 0.0, LReg 0.0, POD 12.421440124511719 EntMin 0.0
Epoch 1, Batch 80/97, Loss=17.819098372757434
Loss made of: CE 0.39713168144226074, LKD 5.7499895095825195, LDE 0.0, LReg 0.0, POD 12.239404678344727 EntMin 0.0
Epoch 1, Batch 90/97, Loss=17.573605784773825
Loss made of: CE 0.2622568607330322, LKD 5.637790679931641, LDE 0.0, LReg 0.0, POD 10.948319435119629 EntMin 0.0
Epoch 1, Class Loss=0.38422077894210815, Reg Loss=5.8957366943359375
Clinet index 25, End of Epoch 1/6, Average Loss=6.279957294464111, Class Loss=0.38422077894210815, Reg Loss=5.8957366943359375
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=17.620433017611504
Loss made of: CE 0.2686006724834442, LKD 5.9280290603637695, LDE 0.0, LReg 0.0, POD 11.348962783813477 EntMin 0.0
Epoch 2, Batch 20/97, Loss=17.547942417860032
Loss made of: CE 0.3009489178657532, LKD 5.907176971435547, LDE 0.0, LReg 0.0, POD 10.389785766601562 EntMin 0.0
Epoch 2, Batch 30/97, Loss=17.672786298394204
Loss made of: CE 0.3074377775192261, LKD 6.944061279296875, LDE 0.0, LReg 0.0, POD 11.161008834838867 EntMin 0.0
Epoch 2, Batch 40/97, Loss=17.793466821312904
Loss made of: CE 0.25821298360824585, LKD 5.859433650970459, LDE 0.0, LReg 0.0, POD 11.533170700073242 EntMin 0.0
Epoch 2, Batch 50/97, Loss=17.120654502511023
Loss made of: CE 0.2651776671409607, LKD 5.58638858795166, LDE 0.0, LReg 0.0, POD 10.878686904907227 EntMin 0.0
Epoch 2, Batch 60/97, Loss=17.5137827783823
Loss made of: CE 0.2134263515472412, LKD 5.073014259338379, LDE 0.0, LReg 0.0, POD 11.581094741821289 EntMin 0.0
Epoch 2, Batch 70/97, Loss=17.09476629495621
Loss made of: CE 0.3494310975074768, LKD 6.648138046264648, LDE 0.0, LReg 0.0, POD 12.020069122314453 EntMin 0.0
Epoch 2, Batch 80/97, Loss=17.56790261864662
Loss made of: CE 0.3235880732536316, LKD 6.030797481536865, LDE 0.0, LReg 0.0, POD 10.217405319213867 EntMin 0.0
Epoch 2, Batch 90/97, Loss=17.20392895489931
Loss made of: CE 0.2884604334831238, LKD 5.515260219573975, LDE 0.0, LReg 0.0, POD 11.0322904586792 EntMin 0.0
Epoch 2, Class Loss=0.3145248293876648, Reg Loss=5.893736362457275
Clinet index 25, End of Epoch 2/6, Average Loss=6.208261013031006, Class Loss=0.3145248293876648, Reg Loss=5.893736362457275
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=17.05834918320179
Loss made of: CE 0.31219857931137085, LKD 5.46531867980957, LDE 0.0, LReg 0.0, POD 10.568379402160645 EntMin 0.0
Epoch 3, Batch 20/97, Loss=17.277076524496078
Loss made of: CE 0.26024094223976135, LKD 6.119784355163574, LDE 0.0, LReg 0.0, POD 11.456228256225586 EntMin 0.0
Epoch 3, Batch 30/97, Loss=16.829111507534982
Loss made of: CE 0.3317145109176636, LKD 5.854661464691162, LDE 0.0, LReg 0.0, POD 10.664752006530762 EntMin 0.0
Epoch 3, Batch 40/97, Loss=17.4435589954257
Loss made of: CE 0.36412590742111206, LKD 5.914245128631592, LDE 0.0, LReg 0.0, POD 12.390604972839355 EntMin 0.0
Epoch 3, Batch 50/97, Loss=17.48941712975502
Loss made of: CE 0.25279030203819275, LKD 5.694457530975342, LDE 0.0, LReg 0.0, POD 10.973098754882812 EntMin 0.0
Epoch 3, Batch 60/97, Loss=17.015160590410233
Loss made of: CE 0.3624313771724701, LKD 5.909000396728516, LDE 0.0, LReg 0.0, POD 10.62210464477539 EntMin 0.0
Epoch 3, Batch 70/97, Loss=16.88707529902458
Loss made of: CE 0.2690172493457794, LKD 5.6879096031188965, LDE 0.0, LReg 0.0, POD 10.35969352722168 EntMin 0.0
Epoch 3, Batch 80/97, Loss=17.42608521580696
Loss made of: CE 0.317901611328125, LKD 5.884818077087402, LDE 0.0, LReg 0.0, POD 11.76070785522461 EntMin 0.0
Epoch 3, Batch 90/97, Loss=17.24931224733591
Loss made of: CE 0.28914347290992737, LKD 6.288032054901123, LDE 0.0, LReg 0.0, POD 10.871602058410645 EntMin 0.0
Epoch 3, Class Loss=0.2955072522163391, Reg Loss=5.863836765289307
Clinet index 25, End of Epoch 3/6, Average Loss=6.15934419631958, Class Loss=0.2955072522163391, Reg Loss=5.863836765289307
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=16.987567216157913
Loss made of: CE 0.264352947473526, LKD 6.15701961517334, LDE 0.0, LReg 0.0, POD 10.000022888183594 EntMin 0.0
Epoch 4, Batch 20/97, Loss=17.444888724386693
Loss made of: CE 0.31844115257263184, LKD 5.473983287811279, LDE 0.0, LReg 0.0, POD 10.986242294311523 EntMin 0.0
Epoch 4, Batch 30/97, Loss=16.771131901443006
Loss made of: CE 0.24405412375926971, LKD 5.4422221183776855, LDE 0.0, LReg 0.0, POD 10.487161636352539 EntMin 0.0
Epoch 4, Batch 40/97, Loss=16.520969414710997
Loss made of: CE 0.3023517429828644, LKD 5.31095027923584, LDE 0.0, LReg 0.0, POD 11.47598648071289 EntMin 0.0
Epoch 4, Batch 50/97, Loss=17.21573227792978
Loss made of: CE 0.40272587537765503, LKD 5.260136127471924, LDE 0.0, LReg 0.0, POD 12.503538131713867 EntMin 0.0
Epoch 4, Batch 60/97, Loss=17.826540042459964
Loss made of: CE 0.3296564817428589, LKD 6.02351188659668, LDE 0.0, LReg 0.0, POD 12.094057083129883 EntMin 0.0
Epoch 4, Batch 70/97, Loss=16.88330697417259
Loss made of: CE 0.35187146067619324, LKD 6.291810035705566, LDE 0.0, LReg 0.0, POD 10.46630859375 EntMin 0.0
Epoch 4, Batch 80/97, Loss=16.848756888508795
Loss made of: CE 0.34162816405296326, LKD 6.141192436218262, LDE 0.0, LReg 0.0, POD 9.815509796142578 EntMin 0.0
Epoch 4, Batch 90/97, Loss=16.6140670388937
Loss made of: CE 0.28293484449386597, LKD 5.1409993171691895, LDE 0.0, LReg 0.0, POD 10.969513893127441 EntMin 0.0
Epoch 4, Class Loss=0.28998005390167236, Reg Loss=5.887596130371094
Clinet index 25, End of Epoch 4/6, Average Loss=6.177576065063477, Class Loss=0.28998005390167236, Reg Loss=5.887596130371094
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=16.38239240497351
Loss made of: CE 0.22893647849559784, LKD 5.329675674438477, LDE 0.0, LReg 0.0, POD 9.899919509887695 EntMin 0.0
Epoch 5, Batch 20/97, Loss=16.347791785001753
Loss made of: CE 0.29792696237564087, LKD 6.434056758880615, LDE 0.0, LReg 0.0, POD 9.925495147705078 EntMin 0.0
Epoch 5, Batch 30/97, Loss=17.12107471525669
Loss made of: CE 0.29300805926322937, LKD 6.512611389160156, LDE 0.0, LReg 0.0, POD 10.190603256225586 EntMin 0.0
Epoch 5, Batch 40/97, Loss=16.84093970656395
Loss made of: CE 0.2581231892108917, LKD 5.856869697570801, LDE 0.0, LReg 0.0, POD 10.935400009155273 EntMin 0.0
Epoch 5, Batch 50/97, Loss=16.992745727300644
Loss made of: CE 0.21151337027549744, LKD 5.881545543670654, LDE 0.0, LReg 0.0, POD 11.222146987915039 EntMin 0.0
Epoch 5, Batch 60/97, Loss=16.883501979708672
Loss made of: CE 0.31213510036468506, LKD 6.1516571044921875, LDE 0.0, LReg 0.0, POD 10.194616317749023 EntMin 0.0
Epoch 5, Batch 70/97, Loss=16.50352603942156
Loss made of: CE 0.3052994906902313, LKD 6.412759780883789, LDE 0.0, LReg 0.0, POD 10.37498664855957 EntMin 0.0
Epoch 5, Batch 80/97, Loss=16.468680375814436
Loss made of: CE 0.23997071385383606, LKD 5.5363898277282715, LDE 0.0, LReg 0.0, POD 10.49287223815918 EntMin 0.0
Epoch 5, Batch 90/97, Loss=16.73442312479019
Loss made of: CE 0.3154478073120117, LKD 5.148938179016113, LDE 0.0, LReg 0.0, POD 10.757522583007812 EntMin 0.0
Epoch 5, Class Loss=0.28081512451171875, Reg Loss=5.876211643218994
Clinet index 25, End of Epoch 5/6, Average Loss=6.157026767730713, Class Loss=0.28081512451171875, Reg Loss=5.876211643218994
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=16.170354437828063
Loss made of: CE 0.2563115358352661, LKD 6.076605796813965, LDE 0.0, LReg 0.0, POD 9.783084869384766 EntMin 0.0
Epoch 6, Batch 20/97, Loss=16.49040923267603
Loss made of: CE 0.25950923562049866, LKD 5.500058650970459, LDE 0.0, LReg 0.0, POD 9.77530288696289 EntMin 0.0
Epoch 6, Batch 30/97, Loss=16.55926906466484
Loss made of: CE 0.2965162694454193, LKD 5.545105934143066, LDE 0.0, LReg 0.0, POD 10.552947044372559 EntMin 0.0
Epoch 6, Batch 40/97, Loss=16.144043590128422
Loss made of: CE 0.24229006469249725, LKD 5.839546203613281, LDE 0.0, LReg 0.0, POD 10.358376502990723 EntMin 0.0
Epoch 6, Batch 50/97, Loss=16.57985222041607
Loss made of: CE 0.3660217225551605, LKD 5.823253631591797, LDE 0.0, LReg 0.0, POD 10.764862060546875 EntMin 0.0
Epoch 6, Batch 60/97, Loss=16.638255693018436
Loss made of: CE 0.23137466609477997, LKD 6.137172222137451, LDE 0.0, LReg 0.0, POD 10.503337860107422 EntMin 0.0
Epoch 6, Batch 70/97, Loss=16.7838564902544
Loss made of: CE 0.292803019285202, LKD 5.616782188415527, LDE 0.0, LReg 0.0, POD 10.732391357421875 EntMin 0.0
Epoch 6, Batch 80/97, Loss=16.341272217035293
Loss made of: CE 0.255293607711792, LKD 6.647242546081543, LDE 0.0, LReg 0.0, POD 9.98934555053711 EntMin 0.0
Epoch 6, Batch 90/97, Loss=15.959387589991092
Loss made of: CE 0.25786256790161133, LKD 5.927958965301514, LDE 0.0, LReg 0.0, POD 10.236276626586914 EntMin 0.0
Epoch 6, Class Loss=0.27712735533714294, Reg Loss=5.86595344543457
Clinet index 25, End of Epoch 6/6, Average Loss=6.143080711364746, Class Loss=0.27712735533714294, Reg Loss=5.86595344543457
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/97, Loss=19.993697541952134
Loss made of: CE 0.36463648080825806, LKD 6.110354423522949, LDE 0.0, LReg 0.0, POD 13.051375389099121 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/97, Loss=17.565562406182288
Loss made of: CE 0.3884187936782837, LKD 5.693234443664551, LDE 0.0, LReg 0.0, POD 10.256353378295898 EntMin 0.0
Epoch 1, Batch 30/97, Loss=17.129043728113174
Loss made of: CE 0.387384295463562, LKD 6.227329254150391, LDE 0.0, LReg 0.0, POD 11.135169982910156 EntMin 0.0
Epoch 1, Batch 40/97, Loss=17.864075151085853
Loss made of: CE 0.2529796361923218, LKD 4.9551191329956055, LDE 0.0, LReg 0.0, POD 13.860727310180664 EntMin 0.0
Epoch 1, Batch 50/97, Loss=17.15991234779358
Loss made of: CE 0.35328659415245056, LKD 5.2782301902771, LDE 0.0, LReg 0.0, POD 11.218598365783691 EntMin 0.0
Epoch 1, Batch 60/97, Loss=17.210345017910004
Loss made of: CE 0.3114136755466461, LKD 5.111043930053711, LDE 0.0, LReg 0.0, POD 11.210404396057129 EntMin 0.0
Epoch 1, Batch 70/97, Loss=16.97068950533867
Loss made of: CE 0.37025901675224304, LKD 5.961661338806152, LDE 0.0, LReg 0.0, POD 10.542152404785156 EntMin 0.0
Epoch 1, Batch 80/97, Loss=17.691374123096466
Loss made of: CE 0.3454601764678955, LKD 6.069211483001709, LDE 0.0, LReg 0.0, POD 11.749332427978516 EntMin 0.0
Epoch 1, Batch 90/97, Loss=17.68792778849602
Loss made of: CE 0.3383445739746094, LKD 6.27380895614624, LDE 0.0, LReg 0.0, POD 10.478998184204102 EntMin 0.0
Epoch 1, Class Loss=0.384196937084198, Reg Loss=5.923976898193359
Clinet index 23, End of Epoch 1/6, Average Loss=6.308173656463623, Class Loss=0.384196937084198, Reg Loss=5.923976898193359
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=17.35726717710495
Loss made of: CE 0.28903698921203613, LKD 6.444734573364258, LDE 0.0, LReg 0.0, POD 12.087871551513672 EntMin 0.0
Epoch 2, Batch 20/97, Loss=17.420607030391693
Loss made of: CE 0.3473207354545593, LKD 5.817206382751465, LDE 0.0, LReg 0.0, POD 11.983327865600586 EntMin 0.0
Epoch 2, Batch 30/97, Loss=17.04832654297352
Loss made of: CE 0.28137990832328796, LKD 5.556743144989014, LDE 0.0, LReg 0.0, POD 10.843899726867676 EntMin 0.0
Epoch 2, Batch 40/97, Loss=17.34797431230545
Loss made of: CE 0.39009594917297363, LKD 6.514721870422363, LDE 0.0, LReg 0.0, POD 12.260552406311035 EntMin 0.0
Epoch 2, Batch 50/97, Loss=17.425747260451317
Loss made of: CE 0.3152393400669098, LKD 6.438243865966797, LDE 0.0, LReg 0.0, POD 11.242621421813965 EntMin 0.0
Epoch 2, Batch 60/97, Loss=17.474859024584294
Loss made of: CE 0.2985854148864746, LKD 5.847869873046875, LDE 0.0, LReg 0.0, POD 11.202048301696777 EntMin 0.0
Epoch 2, Batch 70/97, Loss=17.249828711152077
Loss made of: CE 0.39025288820266724, LKD 5.779557704925537, LDE 0.0, LReg 0.0, POD 11.751776695251465 EntMin 0.0
Epoch 2, Batch 80/97, Loss=17.797853934764863
Loss made of: CE 0.28738629817962646, LKD 5.352323532104492, LDE 0.0, LReg 0.0, POD 10.986749649047852 EntMin 0.0
Epoch 2, Batch 90/97, Loss=17.336834406852724
Loss made of: CE 0.33021479845046997, LKD 5.4518537521362305, LDE 0.0, LReg 0.0, POD 12.362465858459473 EntMin 0.0
Epoch 2, Class Loss=0.31473439931869507, Reg Loss=5.906806468963623
Clinet index 23, End of Epoch 2/6, Average Loss=6.221540927886963, Class Loss=0.31473439931869507, Reg Loss=5.906806468963623
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=17.374813283979893
Loss made of: CE 0.29566720128059387, LKD 5.772505760192871, LDE 0.0, LReg 0.0, POD 10.382596969604492 EntMin 0.0
Epoch 3, Batch 20/97, Loss=17.557216945290566
Loss made of: CE 0.38017624616622925, LKD 6.892322540283203, LDE 0.0, LReg 0.0, POD 12.161016464233398 EntMin 0.0
Epoch 3, Batch 30/97, Loss=17.520190551877022
Loss made of: CE 0.2823053002357483, LKD 5.819510459899902, LDE 0.0, LReg 0.0, POD 10.479497909545898 EntMin 0.0
Epoch 3, Batch 40/97, Loss=17.215025095641614
Loss made of: CE 0.27903053164482117, LKD 5.141798973083496, LDE 0.0, LReg 0.0, POD 10.863133430480957 EntMin 0.0
Epoch 3, Batch 50/97, Loss=16.723059779405595
Loss made of: CE 0.30695095658302307, LKD 6.1662116050720215, LDE 0.0, LReg 0.0, POD 11.144371032714844 EntMin 0.0
Epoch 3, Batch 60/97, Loss=17.14343976825476
Loss made of: CE 0.29846298694610596, LKD 6.0999860763549805, LDE 0.0, LReg 0.0, POD 10.971389770507812 EntMin 0.0
Epoch 3, Batch 70/97, Loss=16.56174905002117
Loss made of: CE 0.2784683406352997, LKD 6.19148588180542, LDE 0.0, LReg 0.0, POD 10.614618301391602 EntMin 0.0
Epoch 3, Batch 80/97, Loss=16.807066428661347
Loss made of: CE 0.34915274381637573, LKD 6.168759822845459, LDE 0.0, LReg 0.0, POD 11.00475025177002 EntMin 0.0
Epoch 3, Batch 90/97, Loss=17.696630689501763
Loss made of: CE 0.2773427963256836, LKD 5.388078689575195, LDE 0.0, LReg 0.0, POD 11.820716857910156 EntMin 0.0
Epoch 3, Class Loss=0.29838845133781433, Reg Loss=5.906559944152832
Clinet index 23, End of Epoch 3/6, Average Loss=6.204948425292969, Class Loss=0.29838845133781433, Reg Loss=5.906559944152832
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=16.888974906504153
Loss made of: CE 0.30659326910972595, LKD 5.792560577392578, LDE 0.0, LReg 0.0, POD 10.989730834960938 EntMin 0.0
Epoch 4, Batch 20/97, Loss=17.413782769441603
Loss made of: CE 0.30677303671836853, LKD 6.453242778778076, LDE 0.0, LReg 0.0, POD 10.330297470092773 EntMin 0.0
Epoch 4, Batch 30/97, Loss=16.662098804116248
Loss made of: CE 0.20098289847373962, LKD 5.200823783874512, LDE 0.0, LReg 0.0, POD 10.651415824890137 EntMin 0.0
Epoch 4, Batch 40/97, Loss=17.085445657372475
Loss made of: CE 0.3288946747779846, LKD 5.794742107391357, LDE 0.0, LReg 0.0, POD 10.720527648925781 EntMin 0.0
Epoch 4, Batch 50/97, Loss=16.81011518239975
Loss made of: CE 0.2778812646865845, LKD 5.374841213226318, LDE 0.0, LReg 0.0, POD 9.977856636047363 EntMin 0.0
Epoch 4, Batch 60/97, Loss=16.75570038408041
Loss made of: CE 0.26506471633911133, LKD 5.947050094604492, LDE 0.0, LReg 0.0, POD 10.627967834472656 EntMin 0.0
Epoch 4, Batch 70/97, Loss=16.896679693460463
Loss made of: CE 0.2888479232788086, LKD 5.507246017456055, LDE 0.0, LReg 0.0, POD 10.37057113647461 EntMin 0.0
Epoch 4, Batch 80/97, Loss=17.326323169469834
Loss made of: CE 0.24424657225608826, LKD 6.051337718963623, LDE 0.0, LReg 0.0, POD 9.966129302978516 EntMin 0.0
Epoch 4, Batch 90/97, Loss=16.584716200828552
Loss made of: CE 0.2833566963672638, LKD 5.807994842529297, LDE 0.0, LReg 0.0, POD 10.580185890197754 EntMin 0.0
Epoch 4, Class Loss=0.28714439272880554, Reg Loss=5.892324924468994
Clinet index 23, End of Epoch 4/6, Average Loss=6.179469108581543, Class Loss=0.28714439272880554, Reg Loss=5.892324924468994
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=17.170649391412734
Loss made of: CE 0.26231440901756287, LKD 6.387441158294678, LDE 0.0, LReg 0.0, POD 10.206388473510742 EntMin 0.0
Epoch 5, Batch 20/97, Loss=16.81924545019865
Loss made of: CE 0.37469935417175293, LKD 6.531132698059082, LDE 0.0, LReg 0.0, POD 10.787809371948242 EntMin 0.0
Epoch 5, Batch 30/97, Loss=17.270954963564872
Loss made of: CE 0.29606422781944275, LKD 6.663737773895264, LDE 0.0, LReg 0.0, POD 10.046897888183594 EntMin 0.0
Epoch 5, Batch 40/97, Loss=16.703164307773115
Loss made of: CE 0.2441624402999878, LKD 5.577260494232178, LDE 0.0, LReg 0.0, POD 11.555179595947266 EntMin 0.0
Epoch 5, Batch 50/97, Loss=16.438235433399676
Loss made of: CE 0.27942177653312683, LKD 5.538372993469238, LDE 0.0, LReg 0.0, POD 10.575695991516113 EntMin 0.0
Epoch 5, Batch 60/97, Loss=16.39735602736473
Loss made of: CE 0.2609884738922119, LKD 5.722369194030762, LDE 0.0, LReg 0.0, POD 9.247185707092285 EntMin 0.0
Epoch 5, Batch 70/97, Loss=16.478539945185183
Loss made of: CE 0.27066224813461304, LKD 6.223868370056152, LDE 0.0, LReg 0.0, POD 9.874216079711914 EntMin 0.0
Epoch 5, Batch 80/97, Loss=16.123964866995813
Loss made of: CE 0.23833692073822021, LKD 5.582140922546387, LDE 0.0, LReg 0.0, POD 9.90092658996582 EntMin 0.0
Epoch 5, Batch 90/97, Loss=16.472963255643844
Loss made of: CE 0.2740340232849121, LKD 5.327725887298584, LDE 0.0, LReg 0.0, POD 11.310757637023926 EntMin 0.0
Epoch 5, Class Loss=0.2824621796607971, Reg Loss=5.892284870147705
Clinet index 23, End of Epoch 5/6, Average Loss=6.174746990203857, Class Loss=0.2824621796607971, Reg Loss=5.892284870147705
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=16.322347816824912
Loss made of: CE 0.23205196857452393, LKD 6.083602428436279, LDE 0.0, LReg 0.0, POD 9.802736282348633 EntMin 0.0
Epoch 6, Batch 20/97, Loss=16.8473377764225
Loss made of: CE 0.2579628825187683, LKD 5.373298168182373, LDE 0.0, LReg 0.0, POD 9.679500579833984 EntMin 0.0
Epoch 6, Batch 30/97, Loss=16.65251336991787
Loss made of: CE 0.33518171310424805, LKD 5.47412633895874, LDE 0.0, LReg 0.0, POD 10.704906463623047 EntMin 0.0
Epoch 6, Batch 40/97, Loss=15.985943546891212
Loss made of: CE 0.2576408386230469, LKD 5.388932704925537, LDE 0.0, LReg 0.0, POD 9.666364669799805 EntMin 0.0
Epoch 6, Batch 50/97, Loss=16.72537801414728
Loss made of: CE 0.26166534423828125, LKD 5.688864707946777, LDE 0.0, LReg 0.0, POD 10.100502014160156 EntMin 0.0
Epoch 6, Batch 60/97, Loss=15.918609477579594
Loss made of: CE 0.2581416666507721, LKD 5.933887004852295, LDE 0.0, LReg 0.0, POD 11.316041946411133 EntMin 0.0
Epoch 6, Batch 70/97, Loss=16.849869014322756
Loss made of: CE 0.27875709533691406, LKD 7.284442901611328, LDE 0.0, LReg 0.0, POD 10.646343231201172 EntMin 0.0
Epoch 6, Batch 80/97, Loss=16.409892569482327
Loss made of: CE 0.3384939134120941, LKD 5.749972343444824, LDE 0.0, LReg 0.0, POD 10.171387672424316 EntMin 0.0
Epoch 6, Batch 90/97, Loss=16.618289686739445
Loss made of: CE 0.39092954993247986, LKD 6.3971710205078125, LDE 0.0, LReg 0.0, POD 10.76877212524414 EntMin 0.0
Epoch 6, Class Loss=0.2837574779987335, Reg Loss=5.9018874168396
Clinet index 23, End of Epoch 6/6, Average Loss=6.18564510345459, Class Loss=0.2837574779987335, Reg Loss=5.9018874168396
federated aggregation...
Validation, Class Loss=0.5705830454826355, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.846067
Mean Acc: 0.558608
FreqW Acc: 0.771850
Mean IoU: 0.447983
Class IoU:
	class 0: 0.88201207
	class 1: 0.78029054
	class 2: 0.32991683
	class 3: 0.6805288
	class 4: 0.66916615
	class 5: 0.7352542
	class 6: 0.7545546
	class 7: 0.8311987
	class 8: 0.7546033
	class 9: 0.062238064
	class 10: 0.0
	class 11: 0.0035837325
	class 12: 0.39951545
	class 13: 0.022367025
	class 14: 0.0018216308
	class 15: 0.7086543
	class 16: 0.0
Class Acc:
	class 0: 0.93754995
	class 1: 0.79434115
	class 2: 0.90911496
	class 3: 0.7003994
	class 4: 0.73000664
	class 5: 0.8263007
	class 6: 0.7639832
	class 7: 0.9480593
	class 8: 0.8105191
	class 9: 0.35412294
	class 10: 0.0
	class 11: 0.0035846455
	class 12: 0.8254319
	class 13: 0.0230692
	class 14: 0.0018225288
	class 15: 0.8680342
	class 16: 0.0

federated global round: 23, step: 4
select part of clients to conduct local training
[17, 3, 5, 22]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/97, Loss=15.682107976078987
Loss made of: CE 0.3501926362514496, LKD 5.793062210083008, LDE 0.0, LReg 0.0, POD 9.541695594787598 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/97, Loss=16.099465048313142
Loss made of: CE 0.27970966696739197, LKD 6.254347801208496, LDE 0.0, LReg 0.0, POD 8.81927490234375 EntMin 0.0
Epoch 1, Batch 30/97, Loss=16.281650820374487
Loss made of: CE 0.2681947946548462, LKD 7.121568202972412, LDE 0.0, LReg 0.0, POD 9.51053524017334 EntMin 0.0
Epoch 1, Batch 40/97, Loss=16.412224221229554
Loss made of: CE 0.252716064453125, LKD 5.340950012207031, LDE 0.0, LReg 0.0, POD 9.541613578796387 EntMin 0.0
Epoch 1, Batch 50/97, Loss=16.442897188663483
Loss made of: CE 0.29960647225379944, LKD 5.622087001800537, LDE 0.0, LReg 0.0, POD 9.236760139465332 EntMin 0.0
Epoch 1, Batch 60/97, Loss=16.296221885085107
Loss made of: CE 0.2142915427684784, LKD 6.1848578453063965, LDE 0.0, LReg 0.0, POD 10.404914855957031 EntMin 0.0
Epoch 1, Batch 70/97, Loss=16.10442792624235
Loss made of: CE 0.23456399142742157, LKD 5.518365859985352, LDE 0.0, LReg 0.0, POD 10.127128601074219 EntMin 0.0
Epoch 1, Batch 80/97, Loss=16.826408027112485
Loss made of: CE 0.3465506434440613, LKD 6.353672027587891, LDE 0.0, LReg 0.0, POD 11.490726470947266 EntMin 0.0
Epoch 1, Batch 90/97, Loss=16.69197435975075
Loss made of: CE 0.26505422592163086, LKD 5.974092483520508, LDE 0.0, LReg 0.0, POD 10.09737777709961 EntMin 0.0
Epoch 1, Class Loss=0.2748302221298218, Reg Loss=5.891646862030029
Clinet index 17, End of Epoch 1/6, Average Loss=6.166477203369141, Class Loss=0.2748302221298218, Reg Loss=5.891646862030029
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=16.067963099479677
Loss made of: CE 0.3053601086139679, LKD 6.110733985900879, LDE 0.0, LReg 0.0, POD 10.771156311035156 EntMin 0.0
Epoch 2, Batch 20/97, Loss=16.413235881924628
Loss made of: CE 0.2776525020599365, LKD 5.771799087524414, LDE 0.0, LReg 0.0, POD 9.989697456359863 EntMin 0.0
Epoch 2, Batch 30/97, Loss=17.167285674810408
Loss made of: CE 0.26899009943008423, LKD 6.859279632568359, LDE 0.0, LReg 0.0, POD 12.01692008972168 EntMin 0.0
Epoch 2, Batch 40/97, Loss=16.868795162439348
Loss made of: CE 0.2771340012550354, LKD 6.233013153076172, LDE 0.0, LReg 0.0, POD 10.15333080291748 EntMin 0.0
Epoch 2, Batch 50/97, Loss=16.462951247394084
Loss made of: CE 0.24490055441856384, LKD 6.1212158203125, LDE 0.0, LReg 0.0, POD 11.257987022399902 EntMin 0.0
Epoch 2, Batch 60/97, Loss=16.788679242134094
Loss made of: CE 0.24734985828399658, LKD 5.830216884613037, LDE 0.0, LReg 0.0, POD 10.924967765808105 EntMin 0.0
Epoch 2, Batch 70/97, Loss=17.309580399096014
Loss made of: CE 0.24725769460201263, LKD 6.546168804168701, LDE 0.0, LReg 0.0, POD 10.891007423400879 EntMin 0.0
Epoch 2, Batch 80/97, Loss=16.928224098682403
Loss made of: CE 0.30077970027923584, LKD 6.783702850341797, LDE 0.0, LReg 0.0, POD 10.86456298828125 EntMin 0.0
Epoch 2, Batch 90/97, Loss=16.258163914084435
Loss made of: CE 0.25294792652130127, LKD 5.273986339569092, LDE 0.0, LReg 0.0, POD 10.755441665649414 EntMin 0.0
Epoch 2, Class Loss=0.27157729864120483, Reg Loss=5.920355319976807
Clinet index 17, End of Epoch 2/6, Average Loss=6.191932678222656, Class Loss=0.27157729864120483, Reg Loss=5.920355319976807
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=16.440711022913455
Loss made of: CE 0.2816735804080963, LKD 6.3034987449646, LDE 0.0, LReg 0.0, POD 9.978450775146484 EntMin 0.0
Epoch 3, Batch 20/97, Loss=16.639984057843684
Loss made of: CE 0.38705500960350037, LKD 6.477970123291016, LDE 0.0, LReg 0.0, POD 12.39869499206543 EntMin 0.0
Epoch 3, Batch 30/97, Loss=16.08435209840536
Loss made of: CE 0.2155848741531372, LKD 5.827872276306152, LDE 0.0, LReg 0.0, POD 10.006550788879395 EntMin 0.0
Epoch 3, Batch 40/97, Loss=16.49493411630392
Loss made of: CE 0.34006938338279724, LKD 5.790126800537109, LDE 0.0, LReg 0.0, POD 10.053515434265137 EntMin 0.0
Epoch 3, Batch 50/97, Loss=16.51786461174488
Loss made of: CE 0.27763545513153076, LKD 5.9404191970825195, LDE 0.0, LReg 0.0, POD 10.541227340698242 EntMin 0.0
Epoch 3, Batch 60/97, Loss=16.545508405566217
Loss made of: CE 0.2588701546192169, LKD 5.201144218444824, LDE 0.0, LReg 0.0, POD 10.566086769104004 EntMin 0.0
Epoch 3, Batch 70/97, Loss=16.705681110918523
Loss made of: CE 0.22062882781028748, LKD 5.644388198852539, LDE 0.0, LReg 0.0, POD 10.755525588989258 EntMin 0.0
Epoch 3, Batch 80/97, Loss=16.252802270650864
Loss made of: CE 0.2520650029182434, LKD 5.649944305419922, LDE 0.0, LReg 0.0, POD 10.364402770996094 EntMin 0.0
Epoch 3, Batch 90/97, Loss=16.665866492688657
Loss made of: CE 0.2728477120399475, LKD 5.934736251831055, LDE 0.0, LReg 0.0, POD 9.744752883911133 EntMin 0.0
Epoch 3, Class Loss=0.2670074701309204, Reg Loss=5.892852306365967
Clinet index 17, End of Epoch 3/6, Average Loss=6.159859657287598, Class Loss=0.2670074701309204, Reg Loss=5.892852306365967
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=15.993777360022069
Loss made of: CE 0.24323372542858124, LKD 6.5602874755859375, LDE 0.0, LReg 0.0, POD 9.596291542053223 EntMin 0.0
Epoch 4, Batch 20/97, Loss=17.076927292346955
Loss made of: CE 0.24746106564998627, LKD 5.423646450042725, LDE 0.0, LReg 0.0, POD 11.233884811401367 EntMin 0.0
Epoch 4, Batch 30/97, Loss=16.180573666095732
Loss made of: CE 0.21835285425186157, LKD 5.476356029510498, LDE 0.0, LReg 0.0, POD 10.204290390014648 EntMin 0.0
Epoch 4, Batch 40/97, Loss=16.359917056560516
Loss made of: CE 0.21509015560150146, LKD 6.4366302490234375, LDE 0.0, LReg 0.0, POD 9.571329116821289 EntMin 0.0
Epoch 4, Batch 50/97, Loss=16.62783500254154
Loss made of: CE 0.23219406604766846, LKD 5.590911865234375, LDE 0.0, LReg 0.0, POD 10.208555221557617 EntMin 0.0
Epoch 4, Batch 60/97, Loss=16.487120164930822
Loss made of: CE 0.23707249760627747, LKD 5.7816972732543945, LDE 0.0, LReg 0.0, POD 11.001352310180664 EntMin 0.0
Epoch 4, Batch 70/97, Loss=16.212657971680166
Loss made of: CE 0.2585795521736145, LKD 5.941066741943359, LDE 0.0, LReg 0.0, POD 9.794960975646973 EntMin 0.0
Epoch 4, Batch 80/97, Loss=16.134163077175618
Loss made of: CE 0.23336851596832275, LKD 5.584323883056641, LDE 0.0, LReg 0.0, POD 10.076937675476074 EntMin 0.0
Epoch 4, Batch 90/97, Loss=15.935254646837711
Loss made of: CE 0.23855558037757874, LKD 6.081334590911865, LDE 0.0, LReg 0.0, POD 9.672686576843262 EntMin 0.0
Epoch 4, Class Loss=0.2637033462524414, Reg Loss=5.887475967407227
Clinet index 17, End of Epoch 4/6, Average Loss=6.151179313659668, Class Loss=0.2637033462524414, Reg Loss=5.887475967407227
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=15.833818954229354
Loss made of: CE 0.3544233441352844, LKD 5.875678062438965, LDE 0.0, LReg 0.0, POD 10.192729949951172 EntMin 0.0
Epoch 5, Batch 20/97, Loss=16.360697612166405
Loss made of: CE 0.271609902381897, LKD 6.4053144454956055, LDE 0.0, LReg 0.0, POD 10.511119842529297 EntMin 0.0
Epoch 5, Batch 30/97, Loss=16.455672617256642
Loss made of: CE 0.2650114595890045, LKD 6.643923759460449, LDE 0.0, LReg 0.0, POD 10.116250038146973 EntMin 0.0
Epoch 5, Batch 40/97, Loss=16.348447808623312
Loss made of: CE 0.2646327614784241, LKD 5.849278926849365, LDE 0.0, LReg 0.0, POD 10.656656265258789 EntMin 0.0
Epoch 5, Batch 50/97, Loss=16.22831892967224
Loss made of: CE 0.2723914384841919, LKD 6.221823692321777, LDE 0.0, LReg 0.0, POD 10.269035339355469 EntMin 0.0
Epoch 5, Batch 60/97, Loss=16.573863482475282
Loss made of: CE 0.326684832572937, LKD 6.721529960632324, LDE 0.0, LReg 0.0, POD 10.84195327758789 EntMin 0.0
Epoch 5, Batch 70/97, Loss=16.277337892353536
Loss made of: CE 0.26314735412597656, LKD 5.672359466552734, LDE 0.0, LReg 0.0, POD 10.472867012023926 EntMin 0.0
Epoch 5, Batch 80/97, Loss=16.222434736788273
Loss made of: CE 0.2776656150817871, LKD 5.735862731933594, LDE 0.0, LReg 0.0, POD 10.049516677856445 EntMin 0.0
Epoch 5, Batch 90/97, Loss=16.00320871770382
Loss made of: CE 0.2321501225233078, LKD 5.288501262664795, LDE 0.0, LReg 0.0, POD 10.122539520263672 EntMin 0.0
Epoch 5, Class Loss=0.2654268741607666, Reg Loss=5.907790660858154
Clinet index 17, End of Epoch 5/6, Average Loss=6.1732177734375, Class Loss=0.2654268741607666, Reg Loss=5.907790660858154
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=16.445594310760498
Loss made of: CE 0.31982046365737915, LKD 6.029233455657959, LDE 0.0, LReg 0.0, POD 10.478050231933594 EntMin 0.0
Epoch 6, Batch 20/97, Loss=16.000022676587104
Loss made of: CE 0.271522581577301, LKD 5.721576690673828, LDE 0.0, LReg 0.0, POD 9.759207725524902 EntMin 0.0
Epoch 6, Batch 30/97, Loss=15.992059864103794
Loss made of: CE 0.24217775464057922, LKD 5.882375717163086, LDE 0.0, LReg 0.0, POD 10.126757621765137 EntMin 0.0
Epoch 6, Batch 40/97, Loss=15.72142568975687
Loss made of: CE 0.31562596559524536, LKD 6.037538051605225, LDE 0.0, LReg 0.0, POD 9.444372177124023 EntMin 0.0
Epoch 6, Batch 50/97, Loss=16.196108581125735
Loss made of: CE 0.2396961897611618, LKD 6.294797897338867, LDE 0.0, LReg 0.0, POD 10.263660430908203 EntMin 0.0
Epoch 6, Batch 60/97, Loss=16.18764311671257
Loss made of: CE 0.23021921515464783, LKD 4.954466819763184, LDE 0.0, LReg 0.0, POD 9.63271713256836 EntMin 0.0
Epoch 6, Batch 70/97, Loss=16.346927952766418
Loss made of: CE 0.26500779390335083, LKD 5.752292156219482, LDE 0.0, LReg 0.0, POD 9.111674308776855 EntMin 0.0
Epoch 6, Batch 80/97, Loss=15.95528788715601
Loss made of: CE 0.2851881980895996, LKD 6.082684516906738, LDE 0.0, LReg 0.0, POD 9.158105850219727 EntMin 0.0
Epoch 6, Batch 90/97, Loss=15.846234327554702
Loss made of: CE 0.25464046001434326, LKD 5.396966934204102, LDE 0.0, LReg 0.0, POD 10.561903953552246 EntMin 0.0
Epoch 6, Class Loss=0.2635553181171417, Reg Loss=5.896355152130127
Clinet index 17, End of Epoch 6/6, Average Loss=6.159910678863525, Class Loss=0.2635553181171417, Reg Loss=5.896355152130127
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=21.11800075173378
Loss made of: CE 0.6308316588401794, LKD 6.0096659660339355, LDE 0.0, LReg 0.0, POD 13.518476486206055 EntMin 0.0
Epoch 1, Class Loss=0.9017670154571533, Reg Loss=6.09503698348999
Clinet index 3, End of Epoch 1/6, Average Loss=6.996804237365723, Class Loss=0.9017670154571533, Reg Loss=6.09503698348999
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=19.05834091901779
Loss made of: CE 0.649996280670166, LKD 5.806567192077637, LDE 0.0, LReg 0.0, POD 11.70851993560791 EntMin 0.0
Epoch 2, Class Loss=0.7910496592521667, Reg Loss=5.972044944763184
Clinet index 3, End of Epoch 2/6, Average Loss=6.763094425201416, Class Loss=0.7910496592521667, Reg Loss=5.972044944763184
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=18.13420794904232
Loss made of: CE 0.33634787797927856, LKD 5.585313320159912, LDE 0.0, LReg 0.0, POD 10.412962913513184 EntMin 0.0
Epoch 3, Class Loss=0.6622400879859924, Reg Loss=6.011998653411865
Clinet index 3, End of Epoch 3/6, Average Loss=6.674238681793213, Class Loss=0.6622400879859924, Reg Loss=6.011998653411865
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/12, Loss=18.102634257078172
Loss made of: CE 0.3261979818344116, LKD 5.949665069580078, LDE 0.0, LReg 0.0, POD 11.560224533081055 EntMin 0.0
Epoch 4, Class Loss=0.5822955369949341, Reg Loss=6.003269195556641
Clinet index 3, End of Epoch 4/6, Average Loss=6.585564613342285, Class Loss=0.5822955369949341, Reg Loss=6.003269195556641
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=17.16481677889824
Loss made of: CE 0.5006411075592041, LKD 5.604602336883545, LDE 0.0, LReg 0.0, POD 10.620944023132324 EntMin 0.0
Epoch 5, Class Loss=0.514334499835968, Reg Loss=5.967684745788574
Clinet index 3, End of Epoch 5/6, Average Loss=6.482019424438477, Class Loss=0.514334499835968, Reg Loss=5.967684745788574
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=17.62685236632824
Loss made of: CE 0.36239883303642273, LKD 6.284149169921875, LDE 0.0, LReg 0.0, POD 10.72690200805664 EntMin 0.0
Epoch 6, Class Loss=0.468286395072937, Reg Loss=6.049941539764404
Clinet index 3, End of Epoch 6/6, Average Loss=6.518228054046631, Class Loss=0.468286395072937, Reg Loss=6.049941539764404
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=15.791440527141095
Loss made of: CE 0.2761250138282776, LKD 6.250971794128418, LDE 0.0, LReg 0.0, POD 9.51691722869873 EntMin 0.0
Epoch 1, Batch 20/97, Loss=16.423608388006688
Loss made of: CE 0.4710875153541565, LKD 6.403323650360107, LDE 0.0, LReg 0.0, POD 9.745468139648438 EntMin 0.0
Epoch 1, Batch 30/97, Loss=16.150371813774107
Loss made of: CE 0.3048695921897888, LKD 6.077754020690918, LDE 0.0, LReg 0.0, POD 10.708656311035156 EntMin 0.0
Epoch 1, Batch 40/97, Loss=16.101795715093612
Loss made of: CE 0.24395617842674255, LKD 6.5358405113220215, LDE 0.0, LReg 0.0, POD 9.975677490234375 EntMin 0.0
Epoch 1, Batch 50/97, Loss=16.693105885386466
Loss made of: CE 0.19768089056015015, LKD 5.890995025634766, LDE 0.0, LReg 0.0, POD 10.882562637329102 EntMin 0.0
Epoch 1, Batch 60/97, Loss=16.273359522223473
Loss made of: CE 0.2922995686531067, LKD 6.100089073181152, LDE 0.0, LReg 0.0, POD 9.697368621826172 EntMin 0.0
Epoch 1, Batch 70/97, Loss=16.578761841356755
Loss made of: CE 0.2860994338989258, LKD 5.707713603973389, LDE 0.0, LReg 0.0, POD 9.239542961120605 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 80/97, Loss=17.261084631085396
Loss made of: CE 0.26325663924217224, LKD 5.619611740112305, LDE 0.0, LReg 0.0, POD 10.526679992675781 EntMin 0.0
Epoch 1, Batch 90/97, Loss=16.471822883188725
Loss made of: CE 0.29666221141815186, LKD 5.694591045379639, LDE 0.0, LReg 0.0, POD 10.529318809509277 EntMin 0.0
Epoch 1, Class Loss=0.2758933901786804, Reg Loss=5.885733127593994
Clinet index 5, End of Epoch 1/6, Average Loss=6.16162633895874, Class Loss=0.2758933901786804, Reg Loss=5.885733127593994
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=16.200166538357735
Loss made of: CE 0.25383591651916504, LKD 5.478659629821777, LDE 0.0, LReg 0.0, POD 10.349235534667969 EntMin 0.0
Epoch 2, Batch 20/97, Loss=16.48548069149256
Loss made of: CE 0.28524187207221985, LKD 5.44736385345459, LDE 0.0, LReg 0.0, POD 9.633060455322266 EntMin 0.0
Epoch 2, Batch 30/97, Loss=16.334137754142283
Loss made of: CE 0.3110588490962982, LKD 6.579505920410156, LDE 0.0, LReg 0.0, POD 10.202898025512695 EntMin 0.0
Epoch 2, Batch 40/97, Loss=16.40893430262804
Loss made of: CE 0.31804364919662476, LKD 6.011844635009766, LDE 0.0, LReg 0.0, POD 10.45228385925293 EntMin 0.0
Epoch 2, Batch 50/97, Loss=17.172969162464142
Loss made of: CE 0.2873653173446655, LKD 5.3962602615356445, LDE 0.0, LReg 0.0, POD 11.709199905395508 EntMin 0.0
Epoch 2, Batch 60/97, Loss=16.615655875205995
Loss made of: CE 0.27299565076828003, LKD 5.961691856384277, LDE 0.0, LReg 0.0, POD 9.303791046142578 EntMin 0.0
Epoch 2, Batch 70/97, Loss=16.518794052302837
Loss made of: CE 0.21620023250579834, LKD 5.060695648193359, LDE 0.0, LReg 0.0, POD 11.010504722595215 EntMin 0.0
Epoch 2, Batch 80/97, Loss=16.52961147725582
Loss made of: CE 0.23854365944862366, LKD 5.823443412780762, LDE 0.0, LReg 0.0, POD 11.679178237915039 EntMin 0.0
Epoch 2, Batch 90/97, Loss=16.8532501026988
Loss made of: CE 0.20402145385742188, LKD 6.035083770751953, LDE 0.0, LReg 0.0, POD 12.018842697143555 EntMin 0.0
Epoch 2, Class Loss=0.2705652713775635, Reg Loss=5.892995357513428
Clinet index 5, End of Epoch 2/6, Average Loss=6.16356086730957, Class Loss=0.2705652713775635, Reg Loss=5.892995357513428
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=16.855088974535466
Loss made of: CE 0.27472397685050964, LKD 6.539234161376953, LDE 0.0, LReg 0.0, POD 9.993318557739258 EntMin 0.0
Epoch 3, Batch 20/97, Loss=16.770586302876474
Loss made of: CE 0.21442750096321106, LKD 5.713315010070801, LDE 0.0, LReg 0.0, POD 10.3228759765625 EntMin 0.0
Epoch 3, Batch 30/97, Loss=16.60192490518093
Loss made of: CE 0.2862940728664398, LKD 5.20377254486084, LDE 0.0, LReg 0.0, POD 10.316803932189941 EntMin 0.0
Epoch 3, Batch 40/97, Loss=16.90458839535713
Loss made of: CE 0.2752903699874878, LKD 6.219339370727539, LDE 0.0, LReg 0.0, POD 10.555059432983398 EntMin 0.0
Epoch 3, Batch 50/97, Loss=16.627252320945264
Loss made of: CE 0.34469303488731384, LKD 5.691649436950684, LDE 0.0, LReg 0.0, POD 10.09716796875 EntMin 0.0
Epoch 3, Batch 60/97, Loss=16.69668641835451
Loss made of: CE 0.5296204090118408, LKD 6.937701225280762, LDE 0.0, LReg 0.0, POD 12.54636001586914 EntMin 0.0
Epoch 3, Batch 70/97, Loss=16.17819198668003
Loss made of: CE 0.24593323469161987, LKD 4.693702220916748, LDE 0.0, LReg 0.0, POD 10.074466705322266 EntMin 0.0
Epoch 3, Batch 80/97, Loss=16.355949173867703
Loss made of: CE 0.2322964370250702, LKD 5.387310028076172, LDE 0.0, LReg 0.0, POD 9.665487289428711 EntMin 0.0
Epoch 3, Batch 90/97, Loss=16.195567750930785
Loss made of: CE 0.3475939631462097, LKD 6.092996120452881, LDE 0.0, LReg 0.0, POD 9.790754318237305 EntMin 0.0
Epoch 3, Class Loss=0.2700040340423584, Reg Loss=5.892068386077881
Clinet index 5, End of Epoch 3/6, Average Loss=6.16207218170166, Class Loss=0.2700040340423584, Reg Loss=5.892068386077881
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=16.569080789387225
Loss made of: CE 0.2690543830394745, LKD 5.661816596984863, LDE 0.0, LReg 0.0, POD 11.376472473144531 EntMin 0.0
Epoch 4, Batch 20/97, Loss=16.14466549605131
Loss made of: CE 0.2444646805524826, LKD 6.448966026306152, LDE 0.0, LReg 0.0, POD 9.530604362487793 EntMin 0.0
Epoch 4, Batch 30/97, Loss=16.190457823872567
Loss made of: CE 0.27026331424713135, LKD 5.626858711242676, LDE 0.0, LReg 0.0, POD 10.189129829406738 EntMin 0.0
Epoch 4, Batch 40/97, Loss=16.073136678338052
Loss made of: CE 0.3046574592590332, LKD 6.093143463134766, LDE 0.0, LReg 0.0, POD 9.810142517089844 EntMin 0.0
Epoch 4, Batch 50/97, Loss=15.92349004894495
Loss made of: CE 0.2100290060043335, LKD 6.18707799911499, LDE 0.0, LReg 0.0, POD 10.161279678344727 EntMin 0.0
Epoch 4, Batch 60/97, Loss=16.281970044970514
Loss made of: CE 0.3145304322242737, LKD 5.746181488037109, LDE 0.0, LReg 0.0, POD 10.270912170410156 EntMin 0.0
Epoch 4, Batch 70/97, Loss=16.229288406670094
Loss made of: CE 0.3499205708503723, LKD 5.858144283294678, LDE 0.0, LReg 0.0, POD 11.00982666015625 EntMin 0.0
Epoch 4, Batch 80/97, Loss=16.06154556274414
Loss made of: CE 0.2837250828742981, LKD 5.671597480773926, LDE 0.0, LReg 0.0, POD 9.261909484863281 EntMin 0.0
Epoch 4, Batch 90/97, Loss=16.42308363169432
Loss made of: CE 0.27495962381362915, LKD 6.15553617477417, LDE 0.0, LReg 0.0, POD 10.129335403442383 EntMin 0.0
Epoch 4, Class Loss=0.26543664932250977, Reg Loss=5.881342887878418
Clinet index 5, End of Epoch 4/6, Average Loss=6.146779537200928, Class Loss=0.26543664932250977, Reg Loss=5.881342887878418
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=16.478703036904335
Loss made of: CE 0.27249833941459656, LKD 5.504859924316406, LDE 0.0, LReg 0.0, POD 9.76171875 EntMin 0.0
Epoch 5, Batch 20/97, Loss=15.949330580234527
Loss made of: CE 0.2941396236419678, LKD 5.780421733856201, LDE 0.0, LReg 0.0, POD 9.619688987731934 EntMin 0.0
Epoch 5, Batch 30/97, Loss=16.163430540263654
Loss made of: CE 0.25811582803726196, LKD 6.10943603515625, LDE 0.0, LReg 0.0, POD 10.93724536895752 EntMin 0.0
Epoch 5, Batch 40/97, Loss=15.851173338294029
Loss made of: CE 0.2754139304161072, LKD 5.889176845550537, LDE 0.0, LReg 0.0, POD 10.118733406066895 EntMin 0.0
Epoch 5, Batch 50/97, Loss=15.988005983829499
Loss made of: CE 0.24242745339870453, LKD 5.211771011352539, LDE 0.0, LReg 0.0, POD 9.145977020263672 EntMin 0.0
Epoch 5, Batch 60/97, Loss=16.432233899831772
Loss made of: CE 0.3394496440887451, LKD 5.826840400695801, LDE 0.0, LReg 0.0, POD 11.582462310791016 EntMin 0.0
Epoch 5, Batch 70/97, Loss=16.190845863521098
Loss made of: CE 0.24473261833190918, LKD 6.245072841644287, LDE 0.0, LReg 0.0, POD 10.203729629516602 EntMin 0.0
Epoch 5, Batch 80/97, Loss=16.09475357681513
Loss made of: CE 0.2382383644580841, LKD 5.49606466293335, LDE 0.0, LReg 0.0, POD 9.005131721496582 EntMin 0.0
Epoch 5, Batch 90/97, Loss=16.485036566853523
Loss made of: CE 0.2381294220685959, LKD 6.029576778411865, LDE 0.0, LReg 0.0, POD 9.902935028076172 EntMin 0.0
Epoch 5, Class Loss=0.26063093543052673, Reg Loss=5.851946830749512
Clinet index 5, End of Epoch 5/6, Average Loss=6.11257791519165, Class Loss=0.26063093543052673, Reg Loss=5.851946830749512
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=15.806584222614765
Loss made of: CE 0.32992348074913025, LKD 5.706443786621094, LDE 0.0, LReg 0.0, POD 9.809832572937012 EntMin 0.0
Epoch 6, Batch 20/97, Loss=16.125052104890347
Loss made of: CE 0.23829708993434906, LKD 6.1942644119262695, LDE 0.0, LReg 0.0, POD 8.938053131103516 EntMin 0.0
Epoch 6, Batch 30/97, Loss=15.823065979778766
Loss made of: CE 0.3453774154186249, LKD 5.901422023773193, LDE 0.0, LReg 0.0, POD 9.666473388671875 EntMin 0.0
Epoch 6, Batch 40/97, Loss=16.239504657685757
Loss made of: CE 0.24741172790527344, LKD 5.797183036804199, LDE 0.0, LReg 0.0, POD 10.613285064697266 EntMin 0.0
Epoch 6, Batch 50/97, Loss=15.962854665517806
Loss made of: CE 0.24044105410575867, LKD 5.789480209350586, LDE 0.0, LReg 0.0, POD 10.135690689086914 EntMin 0.0
Epoch 6, Batch 60/97, Loss=16.091072535514833
Loss made of: CE 0.24159082770347595, LKD 5.548434257507324, LDE 0.0, LReg 0.0, POD 10.490406036376953 EntMin 0.0
Epoch 6, Batch 70/97, Loss=15.433120264112949
Loss made of: CE 0.28267329931259155, LKD 5.759879112243652, LDE 0.0, LReg 0.0, POD 9.530118942260742 EntMin 0.0
Epoch 6, Batch 80/97, Loss=16.140466879308224
Loss made of: CE 0.29197168350219727, LKD 6.1967363357543945, LDE 0.0, LReg 0.0, POD 10.93673038482666 EntMin 0.0
Epoch 6, Batch 90/97, Loss=15.88712918460369
Loss made of: CE 0.229879230260849, LKD 5.53626012802124, LDE 0.0, LReg 0.0, POD 9.086669921875 EntMin 0.0
Epoch 6, Class Loss=0.25872960686683655, Reg Loss=5.864936351776123
Clinet index 5, End of Epoch 6/6, Average Loss=6.123665809631348, Class Loss=0.25872960686683655, Reg Loss=5.864936351776123
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=20.822758078575134
Loss made of: CE 1.1063504219055176, LKD 6.128689765930176, LDE 0.0, LReg 0.0, POD 13.372719764709473 EntMin 0.0
Epoch 1, Class Loss=0.9016436338424683, Reg Loss=6.045255661010742
Clinet index 22, End of Epoch 1/6, Average Loss=6.9468994140625, Class Loss=0.9016436338424683, Reg Loss=6.045255661010742
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=18.746119058132173
Loss made of: CE 0.514509916305542, LKD 5.980792999267578, LDE 0.0, LReg 0.0, POD 10.984678268432617 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.7941055297851562, Reg Loss=5.937467098236084
Clinet index 22, End of Epoch 2/6, Average Loss=6.73157262802124, Class Loss=0.7941055297851562, Reg Loss=5.937467098236084
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=17.90699732005596
Loss made of: CE 0.8817180395126343, LKD 5.806282043457031, LDE 0.0, LReg 0.0, POD 11.988113403320312 EntMin 0.0
Epoch 3, Class Loss=0.6641058921813965, Reg Loss=5.894975662231445
Clinet index 22, End of Epoch 3/6, Average Loss=6.559081554412842, Class Loss=0.6641058921813965, Reg Loss=5.894975662231445
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=17.781739369034767
Loss made of: CE 0.5497397780418396, LKD 5.706696510314941, LDE 0.0, LReg 0.0, POD 11.196741104125977 EntMin 0.0
Epoch 4, Class Loss=0.5916516780853271, Reg Loss=5.942397117614746
Clinet index 22, End of Epoch 4/6, Average Loss=6.534049034118652, Class Loss=0.5916516780853271, Reg Loss=5.942397117614746
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=17.169463860988618
Loss made of: CE 0.3322363495826721, LKD 5.908222198486328, LDE 0.0, LReg 0.0, POD 12.18499755859375 EntMin 0.0
Epoch 5, Class Loss=0.5103726387023926, Reg Loss=5.9144744873046875
Clinet index 22, End of Epoch 5/6, Average Loss=6.42484712600708, Class Loss=0.5103726387023926, Reg Loss=5.9144744873046875
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=16.854471853375436
Loss made of: CE 0.3930494785308838, LKD 5.590611457824707, LDE 0.0, LReg 0.0, POD 10.155292510986328 EntMin 0.0
Epoch 6, Class Loss=0.47178971767425537, Reg Loss=5.944423675537109
Clinet index 22, End of Epoch 6/6, Average Loss=6.416213512420654, Class Loss=0.47178971767425537, Reg Loss=5.944423675537109
federated aggregation...
Validation, Class Loss=0.570404589176178, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.845240
Mean Acc: 0.553363
FreqW Acc: 0.771375
Mean IoU: 0.444117
Class IoU:
	class 0: 0.88146746
	class 1: 0.76164514
	class 2: 0.31961423
	class 3: 0.6842052
	class 4: 0.63906336
	class 5: 0.73772484
	class 6: 0.72182524
	class 7: 0.8338085
	class 8: 0.76434547
	class 9: 0.059102204
	class 10: 0.0
	class 11: 0.004513696
	class 12: 0.4058863
	class 13: 0.017322278
	class 14: 0.0009909713
	class 15: 0.7184671
	class 16: 0.0
Class Acc:
	class 0: 0.94029313
	class 1: 0.7735833
	class 2: 0.93213546
	class 3: 0.7074055
	class 4: 0.6865012
	class 5: 0.8335378
	class 6: 0.7298908
	class 7: 0.94407
	class 8: 0.8391932
	class 9: 0.3572436
	class 10: 0.0
	class 11: 0.0045149378
	class 12: 0.7923866
	class 13: 0.01776179
	class 14: 0.0009915491
	class 15: 0.84765583
	class 16: 0.0

federated global round: 24, step: 4
select part of clients to conduct local training
[16, 9, 12, 2]
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=16.570548820495606
Loss made of: CE 0.3536083698272705, LKD 6.3147430419921875, LDE 0.0, LReg 0.0, POD 10.122581481933594 EntMin 0.0
Epoch 1, Batch 20/97, Loss=15.965535473823547
Loss made of: CE 0.2903500199317932, LKD 6.317127227783203, LDE 0.0, LReg 0.0, POD 9.78316879272461 EntMin 0.0
Epoch 1, Batch 30/97, Loss=16.141873133182525
Loss made of: CE 0.24729132652282715, LKD 6.357001304626465, LDE 0.0, LReg 0.0, POD 10.062932968139648 EntMin 0.0
Epoch 1, Batch 40/97, Loss=16.09577175527811
Loss made of: CE 0.25619733333587646, LKD 5.272080421447754, LDE 0.0, LReg 0.0, POD 10.719730377197266 EntMin 0.0
Epoch 1, Batch 50/97, Loss=16.146440163254738
Loss made of: CE 0.43992066383361816, LKD 6.369537353515625, LDE 0.0, LReg 0.0, POD 10.739419937133789 EntMin 0.0
Epoch 1, Batch 60/97, Loss=16.650592613220216
Loss made of: CE 0.2365705817937851, LKD 5.438320159912109, LDE 0.0, LReg 0.0, POD 10.180436134338379 EntMin 0.0
Epoch 1, Batch 70/97, Loss=16.240048369765283
Loss made of: CE 0.20540472865104675, LKD 6.008902549743652, LDE 0.0, LReg 0.0, POD 8.982755661010742 EntMin 0.0
Epoch 1, Batch 80/97, Loss=16.22961404621601
Loss made of: CE 0.29068493843078613, LKD 5.8135881423950195, LDE 0.0, LReg 0.0, POD 9.803180694580078 EntMin 0.0
Epoch 1, Batch 90/97, Loss=16.289952786266802
Loss made of: CE 0.24079099297523499, LKD 5.639117240905762, LDE 0.0, LReg 0.0, POD 10.744250297546387 EntMin 0.0
Epoch 1, Class Loss=0.27598926424980164, Reg Loss=5.8252153396606445
Clinet index 16, End of Epoch 1/6, Average Loss=6.1012043952941895, Class Loss=0.27598926424980164, Reg Loss=5.8252153396606445
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/97, Loss=16.494395369291304
Loss made of: CE 0.22144624590873718, LKD 6.248025894165039, LDE 0.0, LReg 0.0, POD 11.06035327911377 EntMin 0.0
Epoch 2, Batch 20/97, Loss=16.35214200168848
Loss made of: CE 0.22473391890525818, LKD 5.411465644836426, LDE 0.0, LReg 0.0, POD 9.817577362060547 EntMin 0.0
Epoch 2, Batch 30/97, Loss=16.47263200879097
Loss made of: CE 0.2567586302757263, LKD 5.5124711990356445, LDE 0.0, LReg 0.0, POD 12.132169723510742 EntMin 0.0
Epoch 2, Batch 40/97, Loss=16.234319606423377
Loss made of: CE 0.3177199363708496, LKD 5.6793975830078125, LDE 0.0, LReg 0.0, POD 10.73483657836914 EntMin 0.0
Epoch 2, Batch 50/97, Loss=16.089351309835912
Loss made of: CE 0.22866348922252655, LKD 5.946272850036621, LDE 0.0, LReg 0.0, POD 10.421530723571777 EntMin 0.0
Epoch 2, Batch 60/97, Loss=16.213195617496968
Loss made of: CE 0.3533637225627899, LKD 6.218331336975098, LDE 0.0, LReg 0.0, POD 10.450662612915039 EntMin 0.0
Epoch 2, Batch 70/97, Loss=16.174940030276776
Loss made of: CE 0.278072714805603, LKD 5.787603378295898, LDE 0.0, LReg 0.0, POD 9.52786636352539 EntMin 0.0
Epoch 2, Batch 80/97, Loss=15.935736498236656
Loss made of: CE 0.22341804206371307, LKD 6.124631404876709, LDE 0.0, LReg 0.0, POD 10.43437385559082 EntMin 0.0
Epoch 2, Batch 90/97, Loss=16.20365278571844
Loss made of: CE 0.239200159907341, LKD 5.5268073081970215, LDE 0.0, LReg 0.0, POD 10.35999870300293 EntMin 0.0
Epoch 2, Class Loss=0.26533612608909607, Reg Loss=5.82442569732666
Clinet index 16, End of Epoch 2/6, Average Loss=6.089761734008789, Class Loss=0.26533612608909607, Reg Loss=5.82442569732666
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/97, Loss=15.596024399995803
Loss made of: CE 0.2581406235694885, LKD 5.358192443847656, LDE 0.0, LReg 0.0, POD 9.182830810546875 EntMin 0.0
Epoch 3, Batch 20/97, Loss=16.06499689370394
Loss made of: CE 0.20389489829540253, LKD 5.8163251876831055, LDE 0.0, LReg 0.0, POD 9.788236618041992 EntMin 0.0
Epoch 3, Batch 30/97, Loss=16.43147069066763
Loss made of: CE 0.30412134528160095, LKD 5.910739898681641, LDE 0.0, LReg 0.0, POD 10.874419212341309 EntMin 0.0
Epoch 3, Batch 40/97, Loss=16.595365130901335
Loss made of: CE 0.2601875066757202, LKD 5.969998359680176, LDE 0.0, LReg 0.0, POD 9.491683959960938 EntMin 0.0
Epoch 3, Batch 50/97, Loss=15.768772976100445
Loss made of: CE 0.22580914199352264, LKD 5.561580657958984, LDE 0.0, LReg 0.0, POD 9.548348426818848 EntMin 0.0
Epoch 3, Batch 60/97, Loss=15.855906292796135
Loss made of: CE 0.23107272386550903, LKD 5.328772068023682, LDE 0.0, LReg 0.0, POD 9.772207260131836 EntMin 0.0
Epoch 3, Batch 70/97, Loss=16.463717538118363
Loss made of: CE 0.3097468614578247, LKD 5.483067512512207, LDE 0.0, LReg 0.0, POD 10.397151947021484 EntMin 0.0
Epoch 3, Batch 80/97, Loss=16.624897366762163
Loss made of: CE 0.24584530293941498, LKD 5.387407302856445, LDE 0.0, LReg 0.0, POD 10.707921981811523 EntMin 0.0
Epoch 3, Batch 90/97, Loss=15.630073767900466
Loss made of: CE 0.21692335605621338, LKD 5.967558860778809, LDE 0.0, LReg 0.0, POD 9.396740913391113 EntMin 0.0
Epoch 3, Class Loss=0.2600628435611725, Reg Loss=5.809550762176514
Clinet index 16, End of Epoch 3/6, Average Loss=6.069613456726074, Class Loss=0.2600628435611725, Reg Loss=5.809550762176514
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/97, Loss=15.699267645180225
Loss made of: CE 0.29706570506095886, LKD 5.582200527191162, LDE 0.0, LReg 0.0, POD 9.642050743103027 EntMin 0.0
Epoch 4, Batch 20/97, Loss=15.672195276618003
Loss made of: CE 0.2349381148815155, LKD 5.835351467132568, LDE 0.0, LReg 0.0, POD 9.016718864440918 EntMin 0.0
Epoch 4, Batch 30/97, Loss=15.642429094016553
Loss made of: CE 0.27331554889678955, LKD 5.628931999206543, LDE 0.0, LReg 0.0, POD 10.213563919067383 EntMin 0.0
Epoch 4, Batch 40/97, Loss=15.58001497387886
Loss made of: CE 0.25702109932899475, LKD 5.528713226318359, LDE 0.0, LReg 0.0, POD 10.999780654907227 EntMin 0.0
Epoch 4, Batch 50/97, Loss=15.449612966179847
Loss made of: CE 0.24278739094734192, LKD 6.237454891204834, LDE 0.0, LReg 0.0, POD 8.843429565429688 EntMin 0.0
Epoch 4, Batch 60/97, Loss=15.547229704260825
Loss made of: CE 0.19565348327159882, LKD 5.892966270446777, LDE 0.0, LReg 0.0, POD 9.581892013549805 EntMin 0.0
Epoch 4, Batch 70/97, Loss=15.660639856755733
Loss made of: CE 0.2780141830444336, LKD 5.804185390472412, LDE 0.0, LReg 0.0, POD 10.80876350402832 EntMin 0.0
Epoch 4, Batch 80/97, Loss=15.8883405148983
Loss made of: CE 0.3045468032360077, LKD 5.139500617980957, LDE 0.0, LReg 0.0, POD 11.58056640625 EntMin 0.0
Epoch 4, Batch 90/97, Loss=15.276518648862838
Loss made of: CE 0.25717732310295105, LKD 6.4633002281188965, LDE 0.0, LReg 0.0, POD 9.299013137817383 EntMin 0.0
Epoch 4, Class Loss=0.26211977005004883, Reg Loss=5.808596611022949
Clinet index 16, End of Epoch 4/6, Average Loss=6.070716381072998, Class Loss=0.26211977005004883, Reg Loss=5.808596611022949
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/97, Loss=15.739519946277142
Loss made of: CE 0.21768927574157715, LKD 4.849652290344238, LDE 0.0, LReg 0.0, POD 9.454046249389648 EntMin 0.0
Epoch 5, Batch 20/97, Loss=15.891856087744236
Loss made of: CE 0.3975500762462616, LKD 6.27435827255249, LDE 0.0, LReg 0.0, POD 11.595367431640625 EntMin 0.0
Epoch 5, Batch 30/97, Loss=15.762498189508914
Loss made of: CE 0.3063284754753113, LKD 6.562686443328857, LDE 0.0, LReg 0.0, POD 9.074897766113281 EntMin 0.0
Epoch 5, Batch 40/97, Loss=15.315799742937088
Loss made of: CE 0.24076005816459656, LKD 5.701610565185547, LDE 0.0, LReg 0.0, POD 8.915648460388184 EntMin 0.0
Epoch 5, Batch 50/97, Loss=15.02816918939352
Loss made of: CE 0.20650982856750488, LKD 5.11395263671875, LDE 0.0, LReg 0.0, POD 9.023735046386719 EntMin 0.0
Epoch 5, Batch 60/97, Loss=15.682583528757096
Loss made of: CE 0.24444815516471863, LKD 5.661702632904053, LDE 0.0, LReg 0.0, POD 9.43533706665039 EntMin 0.0
Epoch 5, Batch 70/97, Loss=15.394100965559483
Loss made of: CE 0.23363563418388367, LKD 5.7690253257751465, LDE 0.0, LReg 0.0, POD 9.23503303527832 EntMin 0.0
Epoch 5, Batch 80/97, Loss=15.93717277497053
Loss made of: CE 0.24272707104682922, LKD 6.658608913421631, LDE 0.0, LReg 0.0, POD 8.902678489685059 EntMin 0.0
Epoch 5, Batch 90/97, Loss=15.610403338074684
Loss made of: CE 0.29405301809310913, LKD 5.782050132751465, LDE 0.0, LReg 0.0, POD 11.037897109985352 EntMin 0.0
Epoch 5, Class Loss=0.2577510476112366, Reg Loss=5.813971042633057
Clinet index 16, End of Epoch 5/6, Average Loss=6.071722030639648, Class Loss=0.2577510476112366, Reg Loss=5.813971042633057
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/97, Loss=15.315305718779564
Loss made of: CE 0.23735783994197845, LKD 5.284315586090088, LDE 0.0, LReg 0.0, POD 9.067445755004883 EntMin 0.0
Epoch 6, Batch 20/97, Loss=15.471884025633335
Loss made of: CE 0.26911354064941406, LKD 5.1535964012146, LDE 0.0, LReg 0.0, POD 10.497283935546875 EntMin 0.0
Epoch 6, Batch 30/97, Loss=15.033003729581832
Loss made of: CE 0.2246551513671875, LKD 5.642204284667969, LDE 0.0, LReg 0.0, POD 7.663128852844238 EntMin 0.0
Epoch 6, Batch 40/97, Loss=15.01617331057787
Loss made of: CE 0.24267661571502686, LKD 5.385153770446777, LDE 0.0, LReg 0.0, POD 9.884346008300781 EntMin 0.0
Epoch 6, Batch 50/97, Loss=14.951583623886108
Loss made of: CE 0.1861383616924286, LKD 5.693660259246826, LDE 0.0, LReg 0.0, POD 8.476716995239258 EntMin 0.0
Epoch 6, Batch 60/97, Loss=14.958503676950931
Loss made of: CE 0.2529827356338501, LKD 5.686277389526367, LDE 0.0, LReg 0.0, POD 8.758028030395508 EntMin 0.0
Epoch 6, Batch 70/97, Loss=15.326767972111702
Loss made of: CE 0.29873594641685486, LKD 5.598809719085693, LDE 0.0, LReg 0.0, POD 8.90036678314209 EntMin 0.0
Epoch 6, Batch 80/97, Loss=15.12032663822174
Loss made of: CE 0.28181788325309753, LKD 5.355766296386719, LDE 0.0, LReg 0.0, POD 8.932272911071777 EntMin 0.0
Epoch 6, Batch 90/97, Loss=15.518818590044976
Loss made of: CE 0.2655622959136963, LKD 5.590883255004883, LDE 0.0, LReg 0.0, POD 9.188647270202637 EntMin 0.0
Epoch 6, Class Loss=0.25936177372932434, Reg Loss=5.803703308105469
Clinet index 16, End of Epoch 6/6, Average Loss=6.063065052032471, Class Loss=0.25936177372932434, Reg Loss=5.803703308105469
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=18.16066179871559
Loss made of: CE 0.7979955077171326, LKD 6.127964019775391, LDE 0.0, LReg 0.0, POD 10.415830612182617 EntMin 0.0
Epoch 1, Class Loss=0.635270893573761, Reg Loss=6.0378265380859375
Clinet index 9, End of Epoch 1/6, Average Loss=6.673097610473633, Class Loss=0.635270893573761, Reg Loss=6.0378265380859375
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=17.56534193456173
Loss made of: CE 0.5883723497390747, LKD 5.665050029754639, LDE 0.0, LReg 0.0, POD 11.826007843017578 EntMin 0.0
Epoch 2, Class Loss=0.559624433517456, Reg Loss=6.0072526931762695
Clinet index 9, End of Epoch 2/6, Average Loss=6.566877365112305, Class Loss=0.559624433517456, Reg Loss=6.0072526931762695
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=17.303197818994523
Loss made of: CE 0.37311530113220215, LKD 5.466508388519287, LDE 0.0, LReg 0.0, POD 10.834721565246582 EntMin 0.0
Epoch 3, Class Loss=0.4984000623226166, Reg Loss=6.063408851623535
Clinet index 9, End of Epoch 3/6, Average Loss=6.561809062957764, Class Loss=0.4984000623226166, Reg Loss=6.063408851623535
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=17.207445734739302
Loss made of: CE 0.38540685176849365, LKD 6.251121520996094, LDE 0.0, LReg 0.0, POD 10.394447326660156 EntMin 0.0
Epoch 4, Class Loss=0.4572311043739319, Reg Loss=5.99887228012085
Clinet index 9, End of Epoch 4/6, Average Loss=6.456103324890137, Class Loss=0.4572311043739319, Reg Loss=5.99887228012085
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=16.89417272210121
Loss made of: CE 0.3293713331222534, LKD 5.846248149871826, LDE 0.0, LReg 0.0, POD 10.158015251159668 EntMin 0.0
Epoch 5, Class Loss=0.4383220672607422, Reg Loss=6.033421516418457
Clinet index 9, End of Epoch 5/6, Average Loss=6.471743583679199, Class Loss=0.4383220672607422, Reg Loss=6.033421516418457
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=16.71096007823944
Loss made of: CE 0.25815659761428833, LKD 6.6718034744262695, LDE 0.0, LReg 0.0, POD 10.73148250579834 EntMin 0.0
Epoch 6, Class Loss=0.43046972155570984, Reg Loss=6.068166732788086
Clinet index 9, End of Epoch 6/6, Average Loss=6.498636245727539, Class Loss=0.43046972155570984, Reg Loss=6.068166732788086
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=17.755812972784042
Loss made of: CE 0.5249838829040527, LKD 5.659297943115234, LDE 0.0, LReg 0.0, POD 10.764179229736328 EntMin 0.0
Epoch 1, Class Loss=0.6019158959388733, Reg Loss=5.974956512451172
Clinet index 12, End of Epoch 1/6, Average Loss=6.5768723487854, Class Loss=0.6019158959388733, Reg Loss=5.974956512451172
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=17.416477325558663
Loss made of: CE 0.4795697331428528, LKD 5.658498287200928, LDE 0.0, LReg 0.0, POD 11.04281234741211 EntMin 0.0
Epoch 2, Class Loss=0.5454158782958984, Reg Loss=5.933654308319092
Clinet index 12, End of Epoch 2/6, Average Loss=6.47907018661499, Class Loss=0.5454158782958984, Reg Loss=5.933654308319092
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=16.90669548213482
Loss made of: CE 0.5586780309677124, LKD 6.2534003257751465, LDE 0.0, LReg 0.0, POD 10.846510887145996 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.47668886184692383, Reg Loss=5.903239727020264
Clinet index 12, End of Epoch 3/6, Average Loss=6.3799285888671875, Class Loss=0.47668886184692383, Reg Loss=5.903239727020264
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=16.588368520140648
Loss made of: CE 0.48069998621940613, LKD 5.668993949890137, LDE 0.0, LReg 0.0, POD 10.153890609741211 EntMin 0.0
Epoch 4, Class Loss=0.4589291214942932, Reg Loss=5.894643306732178
Clinet index 12, End of Epoch 4/6, Average Loss=6.353572368621826, Class Loss=0.4589291214942932, Reg Loss=5.894643306732178
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=16.356525507569312
Loss made of: CE 0.436985582113266, LKD 5.766194820404053, LDE 0.0, LReg 0.0, POD 9.772846221923828 EntMin 0.0
Epoch 5, Class Loss=0.4389938712120056, Reg Loss=5.855609893798828
Clinet index 12, End of Epoch 5/6, Average Loss=6.2946038246154785, Class Loss=0.4389938712120056, Reg Loss=5.855609893798828
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=16.19387192428112
Loss made of: CE 0.3172076940536499, LKD 5.764043807983398, LDE 0.0, LReg 0.0, POD 10.012849807739258 EntMin 0.0
Epoch 6, Class Loss=0.4419253468513489, Reg Loss=5.9107489585876465
Clinet index 12, End of Epoch 6/6, Average Loss=6.35267448425293, Class Loss=0.4419253468513489, Reg Loss=5.9107489585876465
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/12, Loss=18.156622821092604
Loss made of: CE 0.7721893787384033, LKD 5.95985746383667, LDE 0.0, LReg 0.0, POD 10.07204532623291 EntMin 0.0
Epoch 1, Class Loss=0.654579758644104, Reg Loss=6.02138614654541
Clinet index 2, End of Epoch 1/6, Average Loss=6.675965785980225, Class Loss=0.654579758644104, Reg Loss=6.02138614654541
Pseudo labeling is: None
Epoch 2, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=17.237117248773576
Loss made of: CE 0.47225475311279297, LKD 5.973862648010254, LDE 0.0, LReg 0.0, POD 9.516075134277344 EntMin 0.0
Epoch 2, Class Loss=0.603378415107727, Reg Loss=6.000377178192139
Clinet index 2, End of Epoch 2/6, Average Loss=6.603755474090576, Class Loss=0.603378415107727, Reg Loss=6.000377178192139
Pseudo labeling is: None
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/12, Loss=17.167437490820884
Loss made of: CE 0.5336371064186096, LKD 5.910281658172607, LDE 0.0, LReg 0.0, POD 10.690837860107422 EntMin 0.0
Epoch 3, Class Loss=0.5569585561752319, Reg Loss=6.0036211013793945
Clinet index 2, End of Epoch 3/6, Average Loss=6.560579776763916, Class Loss=0.5569585561752319, Reg Loss=6.0036211013793945
Pseudo labeling is: None
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/12, Loss=16.7690300822258
Loss made of: CE 0.6000535488128662, LKD 6.61702299118042, LDE 0.0, LReg 0.0, POD 10.094501495361328 EntMin 0.0
Epoch 4, Class Loss=0.5178363919258118, Reg Loss=6.03243350982666
Clinet index 2, End of Epoch 4/6, Average Loss=6.550270080566406, Class Loss=0.5178363919258118, Reg Loss=6.03243350982666
Pseudo labeling is: None
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/12, Loss=16.416369447112082
Loss made of: CE 0.6719125509262085, LKD 6.469266414642334, LDE 0.0, LReg 0.0, POD 10.895886421203613 EntMin 0.0
Epoch 5, Class Loss=0.5122029781341553, Reg Loss=5.968928337097168
Clinet index 2, End of Epoch 5/6, Average Loss=6.481131553649902, Class Loss=0.5122029781341553, Reg Loss=5.968928337097168
Pseudo labeling is: None
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/12, Loss=16.69457266032696
Loss made of: CE 0.493068665266037, LKD 6.089699745178223, LDE 0.0, LReg 0.0, POD 9.48724365234375 EntMin 0.0
Epoch 6, Class Loss=0.4902437925338745, Reg Loss=6.03999662399292
Clinet index 2, End of Epoch 6/6, Average Loss=6.530240535736084, Class Loss=0.4902437925338745, Reg Loss=6.03999662399292
federated aggregation...
Validation, Class Loss=0.581470251083374, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.844392
Mean Acc: 0.554020
FreqW Acc: 0.770250
Mean IoU: 0.444306
Class IoU:
	class 0: 0.88010556
	class 1: 0.76194376
	class 2: 0.32591495
	class 3: 0.69018394
	class 4: 0.6343288
	class 5: 0.7371402
	class 6: 0.70148796
	class 7: 0.8337855
	class 8: 0.7730449
	class 9: 0.060490105
	class 10: 0.0
	class 11: 0.005363776
	class 12: 0.4051601
	class 13: 0.017798847
	class 14: 0.0017524966
	class 15: 0.7172541
	class 16: 0.007446131
Class Acc:
	class 0: 0.938846
	class 1: 0.7747681
	class 2: 0.9276939
	class 3: 0.71831584
	class 4: 0.68225676
	class 5: 0.8346249
	class 6: 0.70833784
	class 7: 0.9431386
	class 8: 0.85140234
	class 9: 0.37127534
	class 10: 0.0
	class 11: 0.0053651016
	class 12: 0.7833854
	class 13: 0.018323941
	class 14: 0.0017537132
	class 15: 0.85137504
	class 16: 0.007474637

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 25, step: 5
select part of clients to conduct local training
[24, 7, 4, 12]
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=23.749842703342438
Loss made of: CE 0.7944197654724121, LKD 8.013120651245117, LDE 0.0, LReg 0.0, POD 13.46628475189209 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.9091277122497559, Reg Loss=7.311500072479248
Clinet index 24, End of Epoch 1/6, Average Loss=8.220627784729004, Class Loss=0.9091277122497559, Reg Loss=7.311500072479248
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=20.970639729499815
Loss made of: CE 0.7861847877502441, LKD 7.500844955444336, LDE 0.0, LReg 0.0, POD 14.205913543701172 EntMin 0.0
Epoch 2, Class Loss=0.7712219953536987, Reg Loss=7.2903265953063965
Clinet index 24, End of Epoch 2/6, Average Loss=8.061548233032227, Class Loss=0.7712219953536987, Reg Loss=7.2903265953063965
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=20.20662875175476
Loss made of: CE 0.6650944948196411, LKD 7.559373378753662, LDE 0.0, LReg 0.0, POD 12.090721130371094 EntMin 0.0
Epoch 3, Class Loss=0.6966537237167358, Reg Loss=7.193017482757568
Clinet index 24, End of Epoch 3/6, Average Loss=7.889671325683594, Class Loss=0.6966537237167358, Reg Loss=7.193017482757568
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=19.69733938574791
Loss made of: CE 0.6844326853752136, LKD 6.9349212646484375, LDE 0.0, LReg 0.0, POD 11.500659942626953 EntMin 0.0
Epoch 4, Class Loss=0.6349455118179321, Reg Loss=7.212983131408691
Clinet index 24, End of Epoch 4/6, Average Loss=7.847928524017334, Class Loss=0.6349455118179321, Reg Loss=7.212983131408691
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=18.927749288082122
Loss made of: CE 0.6143357753753662, LKD 7.199741363525391, LDE 0.0, LReg 0.0, POD 10.782938957214355 EntMin 0.0
Epoch 5, Class Loss=0.6010388135910034, Reg Loss=7.1252570152282715
Clinet index 24, End of Epoch 5/6, Average Loss=7.7262959480285645, Class Loss=0.6010388135910034, Reg Loss=7.1252570152282715
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=18.626553666591644
Loss made of: CE 0.5287940502166748, LKD 7.219476222991943, LDE 0.0, LReg 0.0, POD 10.604612350463867 EntMin 0.0
Epoch 6, Class Loss=0.5773220658302307, Reg Loss=7.166801452636719
Clinet index 24, End of Epoch 6/6, Average Loss=7.744123458862305, Class Loss=0.5773220658302307, Reg Loss=7.166801452636719
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.3396260738372803, Reg Loss=6.749871730804443
Clinet index 7, End of Epoch 1/6, Average Loss=8.089497566223145, Class Loss=1.3396260738372803, Reg Loss=6.749871730804443
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=1.1080230474472046, Reg Loss=6.391214847564697
Clinet index 7, End of Epoch 2/6, Average Loss=7.499238014221191, Class Loss=1.1080230474472046, Reg Loss=6.391214847564697
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.9252522587776184, Reg Loss=6.564364433288574
Clinet index 7, End of Epoch 3/6, Average Loss=7.489616870880127, Class Loss=0.9252522587776184, Reg Loss=6.564364433288574
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.8170620203018188, Reg Loss=6.453289985656738
Clinet index 7, End of Epoch 4/6, Average Loss=7.270351886749268, Class Loss=0.8170620203018188, Reg Loss=6.453289985656738
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.7352952361106873, Reg Loss=6.351247310638428
Clinet index 7, End of Epoch 5/6, Average Loss=7.08654260635376, Class Loss=0.7352952361106873, Reg Loss=6.351247310638428
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6734370589256287, Reg Loss=6.416976451873779
Clinet index 7, End of Epoch 6/6, Average Loss=7.090413570404053, Class Loss=0.6734370589256287, Reg Loss=6.416976451873779
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=23.43016700744629
Loss made of: CE 0.976237952709198, LKD 7.64605712890625, LDE 0.0, LReg 0.0, POD 13.678424835205078 EntMin 0.0
Epoch 1, Class Loss=0.9700642824172974, Reg Loss=7.261847496032715
Clinet index 4, End of Epoch 1/6, Average Loss=8.231911659240723, Class Loss=0.9700642824172974, Reg Loss=7.261847496032715
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=20.789402258396148
Loss made of: CE 0.712408721446991, LKD 7.47357177734375, LDE 0.0, LReg 0.0, POD 12.801414489746094 EntMin 0.0
Epoch 2, Class Loss=0.8111027479171753, Reg Loss=7.278989315032959
Clinet index 4, End of Epoch 2/6, Average Loss=8.090091705322266, Class Loss=0.8111027479171753, Reg Loss=7.278989315032959
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=19.494969111680984
Loss made of: CE 0.6793405413627625, LKD 6.841909408569336, LDE 0.0, LReg 0.0, POD 10.818781852722168 EntMin 0.0
Epoch 3, Class Loss=0.7222919464111328, Reg Loss=7.159824371337891
Clinet index 4, End of Epoch 3/6, Average Loss=7.882116317749023, Class Loss=0.7222919464111328, Reg Loss=7.159824371337891
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=19.57776321172714
Loss made of: CE 0.7126264572143555, LKD 7.308280944824219, LDE 0.0, LReg 0.0, POD 12.892152786254883 EntMin 0.0
Epoch 4, Class Loss=0.6735259294509888, Reg Loss=7.239221572875977
Clinet index 4, End of Epoch 4/6, Average Loss=7.912747383117676, Class Loss=0.6735259294509888, Reg Loss=7.239221572875977
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=18.698406249284744
Loss made of: CE 0.7293271422386169, LKD 6.915615081787109, LDE 0.0, LReg 0.0, POD 11.554244995117188 EntMin 0.0
Epoch 5, Class Loss=0.6289520859718323, Reg Loss=7.142123222351074
Clinet index 4, End of Epoch 5/6, Average Loss=7.771075248718262, Class Loss=0.6289520859718323, Reg Loss=7.142123222351074
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=18.23181371688843
Loss made of: CE 0.6096189022064209, LKD 6.925388336181641, LDE 0.0, LReg 0.0, POD 10.717846870422363 EntMin 0.0
Epoch 6, Class Loss=0.6019684076309204, Reg Loss=7.196597099304199
Clinet index 4, End of Epoch 6/6, Average Loss=7.79856538772583, Class Loss=0.6019684076309204, Reg Loss=7.196597099304199
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=22.95258425474167
Loss made of: CE 0.9252104759216309, LKD 7.579996109008789, LDE 0.0, LReg 0.0, POD 16.599571228027344 EntMin 0.0
Epoch 1, Class Loss=0.9078336954116821, Reg Loss=7.293604850769043
Clinet index 12, End of Epoch 1/6, Average Loss=8.201438903808594, Class Loss=0.9078336954116821, Reg Loss=7.293604850769043
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=21.035856342315675
Loss made of: CE 0.7982778549194336, LKD 7.132552146911621, LDE 0.0, LReg 0.0, POD 12.89362907409668 EntMin 0.0
Epoch 2, Class Loss=0.7800058126449585, Reg Loss=7.33793830871582
Clinet index 12, End of Epoch 2/6, Average Loss=8.11794376373291, Class Loss=0.7800058126449585, Reg Loss=7.33793830871582
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=19.618855440616606
Loss made of: CE 0.7367551326751709, LKD 6.977726936340332, LDE 0.0, LReg 0.0, POD 10.951040267944336 EntMin 0.0
Epoch 3, Class Loss=0.6804420948028564, Reg Loss=7.202389717102051
Clinet index 12, End of Epoch 3/6, Average Loss=7.882831573486328, Class Loss=0.6804420948028564, Reg Loss=7.202389717102051
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=19.252738416194916
Loss made of: CE 0.5529190897941589, LKD 6.88767147064209, LDE 0.0, LReg 0.0, POD 10.651973724365234 EntMin 0.0
Epoch 4, Class Loss=0.6267299056053162, Reg Loss=7.194840908050537
Clinet index 12, End of Epoch 4/6, Average Loss=7.821570873260498, Class Loss=0.6267299056053162, Reg Loss=7.194840908050537
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=18.705007082223894
Loss made of: CE 0.5398135781288147, LKD 7.012751579284668, LDE 0.0, LReg 0.0, POD 10.131776809692383 EntMin 0.0
Epoch 5, Class Loss=0.5985736846923828, Reg Loss=7.214227199554443
Clinet index 12, End of Epoch 5/6, Average Loss=7.812800884246826, Class Loss=0.5985736846923828, Reg Loss=7.214227199554443
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=18.737446135282518
Loss made of: CE 0.572898805141449, LKD 6.961516380310059, LDE 0.0, LReg 0.0, POD 10.78165054321289 EntMin 0.0
Epoch 6, Class Loss=0.5724527835845947, Reg Loss=7.2282257080078125
Clinet index 12, End of Epoch 6/6, Average Loss=7.800678253173828, Class Loss=0.5724527835845947, Reg Loss=7.2282257080078125
federated aggregation...
Validation, Class Loss=0.8611597418785095, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.812195
Mean Acc: 0.471514
FreqW Acc: 0.729089
Mean IoU: 0.365946
Class IoU:
	class 0: 0.8590826
	class 1: 0.6269304
	class 2: 0.33088526
	class 3: 0.5603763
	class 4: 0.56694406
	class 5: 0.66705996
	class 6: 0.5915947
	class 7: 0.83624816
	class 8: 0.7471168
	class 9: 0.05995024
	class 10: 0.0
	class 11: 0.0017978342
	class 12: 0.34834164
	class 13: 0.025338845
	class 14: 0.00018196992
	class 15: 0.6966812
	class 16: 0.030089324
	class 17: 0.0
	class 18: 0.004351404
Class Acc:
	class 0: 0.9248743
	class 1: 0.63272536
	class 2: 0.90266657
	class 3: 0.57151484
	class 4: 0.59387237
	class 5: 0.724988
	class 6: 0.59608537
	class 7: 0.9411524
	class 8: 0.8606991
	class 9: 0.46018523
	class 10: 0.0
	class 11: 0.0017978448
	class 12: 0.7988231
	class 13: 0.02684596
	class 14: 0.0001819932
	class 15: 0.8868229
	class 16: 0.030655278
	class 17: 0.0
	class 18: 0.004870832

federated global round: 26, step: 5
select part of clients to conduct local training
[6, 28, 20, 4]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=19.052855849266052
Loss made of: CE 0.7492744326591492, LKD 6.8714752197265625, LDE 0.0, LReg 0.0, POD 12.154475212097168 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.67315673828125, Reg Loss=7.317958354949951
Clinet index 6, End of Epoch 1/6, Average Loss=7.991115093231201, Class Loss=0.67315673828125, Reg Loss=7.317958354949951
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=18.651861047744752
Loss made of: CE 0.5705673694610596, LKD 7.165187358856201, LDE 0.0, LReg 0.0, POD 11.806547164916992 EntMin 0.0
Epoch 2, Class Loss=0.6387194395065308, Reg Loss=7.317049980163574
Clinet index 6, End of Epoch 2/6, Average Loss=7.9557695388793945, Class Loss=0.6387194395065308, Reg Loss=7.317049980163574
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=18.249257451295854
Loss made of: CE 0.6745073795318604, LKD 7.629251480102539, LDE 0.0, LReg 0.0, POD 11.156532287597656 EntMin 0.0
Epoch 3, Class Loss=0.5973986983299255, Reg Loss=7.321797847747803
Clinet index 6, End of Epoch 3/6, Average Loss=7.919196605682373, Class Loss=0.5973986983299255, Reg Loss=7.321797847747803
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=17.928136575222016
Loss made of: CE 0.5944380760192871, LKD 7.0014262199401855, LDE 0.0, LReg 0.0, POD 10.900762557983398 EntMin 0.0
Epoch 4, Class Loss=0.5754215717315674, Reg Loss=7.247751235961914
Clinet index 6, End of Epoch 4/6, Average Loss=7.823172569274902, Class Loss=0.5754215717315674, Reg Loss=7.247751235961914
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=18.659194058179857
Loss made of: CE 0.6513997912406921, LKD 7.969346523284912, LDE 0.0, LReg 0.0, POD 11.627766609191895 EntMin 0.0
Epoch 5, Class Loss=0.5590865612030029, Reg Loss=7.33602237701416
Clinet index 6, End of Epoch 5/6, Average Loss=7.895109176635742, Class Loss=0.5590865612030029, Reg Loss=7.33602237701416
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=17.683708575367927
Loss made of: CE 0.4830600917339325, LKD 6.864904403686523, LDE 0.0, LReg 0.0, POD 8.90705394744873 EntMin 0.0
Epoch 6, Class Loss=0.5487109422683716, Reg Loss=7.273782730102539
Clinet index 6, End of Epoch 6/6, Average Loss=7.822493553161621, Class Loss=0.5487109422683716, Reg Loss=7.273782730102539
Current Client Index:  28
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.3118362426757812, Reg Loss=6.6807122230529785
Clinet index 28, End of Epoch 1/6, Average Loss=7.99254846572876, Class Loss=1.3118362426757812, Reg Loss=6.6807122230529785
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=1.173201084136963, Reg Loss=6.497995376586914
Clinet index 28, End of Epoch 2/6, Average Loss=7.671196460723877, Class Loss=1.173201084136963, Reg Loss=6.497995376586914
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.9810671806335449, Reg Loss=6.670219898223877
Clinet index 28, End of Epoch 3/6, Average Loss=7.651287078857422, Class Loss=0.9810671806335449, Reg Loss=6.670219898223877
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.8505576252937317, Reg Loss=6.636971473693848
Clinet index 28, End of Epoch 4/6, Average Loss=7.487529277801514, Class Loss=0.8505576252937317, Reg Loss=6.636971473693848
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7567664384841919, Reg Loss=6.494730472564697
Clinet index 28, End of Epoch 5/6, Average Loss=7.2514967918396, Class Loss=0.7567664384841919, Reg Loss=6.494730472564697
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6737473011016846, Reg Loss=6.566999435424805
Clinet index 28, End of Epoch 6/6, Average Loss=7.24074649810791, Class Loss=0.6737473011016846, Reg Loss=6.566999435424805
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.2712180614471436, Reg Loss=6.511836528778076
Clinet index 20, End of Epoch 1/6, Average Loss=7.783054351806641, Class Loss=1.2712180614471436, Reg Loss=6.511836528778076
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=1.1515424251556396, Reg Loss=6.421718120574951
Clinet index 20, End of Epoch 2/6, Average Loss=7.573260307312012, Class Loss=1.1515424251556396, Reg Loss=6.421718120574951
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.9110889434814453, Reg Loss=6.257603168487549
Clinet index 20, End of Epoch 3/6, Average Loss=7.168692111968994, Class Loss=0.9110889434814453, Reg Loss=6.257603168487549
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.8117159605026245, Reg Loss=6.452780723571777
Clinet index 20, End of Epoch 4/6, Average Loss=7.264496803283691, Class Loss=0.8117159605026245, Reg Loss=6.452780723571777
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7361177802085876, Reg Loss=6.427275657653809
Clinet index 20, End of Epoch 5/6, Average Loss=7.163393497467041, Class Loss=0.7361177802085876, Reg Loss=6.427275657653809
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6564266681671143, Reg Loss=6.396855354309082
Clinet index 20, End of Epoch 6/6, Average Loss=7.053281784057617, Class Loss=0.6564266681671143, Reg Loss=6.396855354309082
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=18.737309718132018
Loss made of: CE 0.7032380104064941, LKD 7.492276668548584, LDE 0.0, LReg 0.0, POD 10.55473518371582 EntMin 0.0
Epoch 1, Class Loss=0.6781866550445557, Reg Loss=7.182307720184326
Clinet index 4, End of Epoch 1/6, Average Loss=7.860494613647461, Class Loss=0.6781866550445557, Reg Loss=7.182307720184326
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=18.243543219566345
Loss made of: CE 0.6010538339614868, LKD 7.63749885559082, LDE 0.0, LReg 0.0, POD 10.818746566772461 EntMin 0.0
Epoch 2, Class Loss=0.6292303800582886, Reg Loss=7.1919050216674805
Clinet index 4, End of Epoch 2/6, Average Loss=7.821135520935059, Class Loss=0.6292303800582886, Reg Loss=7.1919050216674805
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=17.929006534814835
Loss made of: CE 0.5759052634239197, LKD 6.873568534851074, LDE 0.0, LReg 0.0, POD 9.91975212097168 EntMin 0.0
Epoch 3, Class Loss=0.5950784683227539, Reg Loss=7.151205062866211
Clinet index 4, End of Epoch 3/6, Average Loss=7.746283531188965, Class Loss=0.5950784683227539, Reg Loss=7.151205062866211
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=18.258512169122696
Loss made of: CE 0.6219165325164795, LKD 7.215722560882568, LDE 0.0, LReg 0.0, POD 10.907793045043945 EntMin 0.0
Epoch 4, Class Loss=0.5697352886199951, Reg Loss=7.180332183837891
Clinet index 4, End of Epoch 4/6, Average Loss=7.750067710876465, Class Loss=0.5697352886199951, Reg Loss=7.180332183837891
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=17.885114455223082
Loss made of: CE 0.6746152639389038, LKD 7.021840572357178, LDE 0.0, LReg 0.0, POD 11.583717346191406 EntMin 0.0
Epoch 5, Class Loss=0.5638927221298218, Reg Loss=7.210511684417725
Clinet index 4, End of Epoch 5/6, Average Loss=7.774404525756836, Class Loss=0.5638927221298218, Reg Loss=7.210511684417725
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=17.39310993552208
Loss made of: CE 0.5487353801727295, LKD 6.889548301696777, LDE 0.0, LReg 0.0, POD 9.431530952453613 EntMin 0.0
Epoch 6, Class Loss=0.5467730760574341, Reg Loss=7.121075630187988
Clinet index 4, End of Epoch 6/6, Average Loss=7.667848587036133, Class Loss=0.5467730760574341, Reg Loss=7.121075630187988
federated aggregation...
Validation, Class Loss=0.8110001683235168, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.808519
Mean Acc: 0.459085
FreqW Acc: 0.722313
Mean IoU: 0.353195
Class IoU:
	class 0: 0.8543144
	class 1: 0.6055155
	class 2: 0.32017753
	class 3: 0.5394606
	class 4: 0.5411579
	class 5: 0.623194
	class 6: 0.5069072
	class 7: 0.82923275
	class 8: 0.7451645
	class 9: 0.061505258
	class 10: 0.0
	class 11: 0.00052588875
	class 12: 0.33725557
	class 13: 0.022711057
	class 14: 9.0228845e-05
	class 15: 0.6996967
	class 16: 0.016852863
	class 17: 0.0
	class 18: 0.0069411667
Class Acc:
	class 0: 0.92499405
	class 1: 0.6099695
	class 2: 0.906051
	class 3: 0.54992324
	class 4: 0.5673852
	class 5: 0.66954756
	class 6: 0.5085751
	class 7: 0.93628955
	class 8: 0.8664972
	class 9: 0.45080236
	class 10: 0.0
	class 11: 0.000525889
	class 12: 0.8048273
	class 13: 0.024016296
	class 14: 9.023919e-05
	class 15: 0.87796366
	class 16: 0.01703651
	class 17: 0.0
	class 18: 0.008110967

federated global round: 27, step: 5
select part of clients to conduct local training
[24, 8, 26, 0]
Current Client Index:  24
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/12, Loss=20.301047733426095
Loss made of: CE 0.5998378992080688, LKD 7.927341461181641, LDE 0.0, LReg 0.0, POD 11.558486938476562 EntMin 0.0
Epoch 1, Class Loss=0.619297206401825, Reg Loss=7.159709930419922
Clinet index 24, End of Epoch 1/6, Average Loss=7.7790069580078125, Class Loss=0.619297206401825, Reg Loss=7.159709930419922
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=18.465555679798126
Loss made of: CE 0.6098566055297852, LKD 7.482138633728027, LDE 0.0, LReg 0.0, POD 11.867688179016113 EntMin 0.0
Epoch 2, Class Loss=0.5787056684494019, Reg Loss=7.175551414489746
Clinet index 24, End of Epoch 2/6, Average Loss=7.7542572021484375, Class Loss=0.5787056684494019, Reg Loss=7.175551414489746
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/12, Loss=18.168729597330092
Loss made of: CE 0.5062142610549927, LKD 7.651942729949951, LDE 0.0, LReg 0.0, POD 9.890336990356445 EntMin 0.0
Epoch 3, Class Loss=0.5456863641738892, Reg Loss=7.201711654663086
Clinet index 24, End of Epoch 3/6, Average Loss=7.7473978996276855, Class Loss=0.5456863641738892, Reg Loss=7.201711654663086
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/12, Loss=17.8191518843174
Loss made of: CE 0.5448676347732544, LKD 7.04376745223999, LDE 0.0, LReg 0.0, POD 9.508419036865234 EntMin 0.0
Epoch 4, Class Loss=0.5165575742721558, Reg Loss=7.141025066375732
Clinet index 24, End of Epoch 4/6, Average Loss=7.657582759857178, Class Loss=0.5165575742721558, Reg Loss=7.141025066375732
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/12, Loss=17.573098149895667
Loss made of: CE 0.5317397713661194, LKD 7.354913234710693, LDE 0.0, LReg 0.0, POD 10.191926956176758 EntMin 0.0
Epoch 5, Class Loss=0.5052571892738342, Reg Loss=7.162876129150391
Clinet index 24, End of Epoch 5/6, Average Loss=7.66813325881958, Class Loss=0.5052571892738342, Reg Loss=7.162876129150391
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/12, Loss=17.618979123234748
Loss made of: CE 0.4465615749359131, LKD 7.147521018981934, LDE 0.0, LReg 0.0, POD 9.051546096801758 EntMin 0.0
Epoch 6, Class Loss=0.4996160864830017, Reg Loss=7.146910667419434
Clinet index 24, End of Epoch 6/6, Average Loss=7.64652681350708, Class Loss=0.4996160864830017, Reg Loss=7.146910667419434
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.9704128503799438, Reg Loss=6.445584297180176
Clinet index 8, End of Epoch 1/6, Average Loss=7.41599702835083, Class Loss=0.9704128503799438, Reg Loss=6.445584297180176
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.8722662925720215, Reg Loss=6.555662631988525
Clinet index 8, End of Epoch 2/6, Average Loss=7.427928924560547, Class Loss=0.8722662925720215, Reg Loss=6.555662631988525
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.766076385974884, Reg Loss=6.454056739807129
Clinet index 8, End of Epoch 3/6, Average Loss=7.220133304595947, Class Loss=0.766076385974884, Reg Loss=6.454056739807129
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.6774299144744873, Reg Loss=6.403189659118652
Clinet index 8, End of Epoch 4/6, Average Loss=7.080619812011719, Class Loss=0.6774299144744873, Reg Loss=6.403189659118652
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.6180180311203003, Reg Loss=6.440163612365723
Clinet index 8, End of Epoch 5/6, Average Loss=7.0581817626953125, Class Loss=0.6180180311203003, Reg Loss=6.440163612365723
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.5672202110290527, Reg Loss=6.525084495544434
Clinet index 8, End of Epoch 6/6, Average Loss=7.092304706573486, Class Loss=0.5672202110290527, Reg Loss=6.525084495544434
Current Client Index:  26
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=19.92838539481163
Loss made of: CE 0.5055062770843506, LKD 6.941546440124512, LDE 0.0, LReg 0.0, POD 10.397647857666016 EntMin 0.0
Epoch 1, Class Loss=0.6339550018310547, Reg Loss=7.209846496582031
Clinet index 26, End of Epoch 1/6, Average Loss=7.843801498413086, Class Loss=0.6339550018310547, Reg Loss=7.209846496582031
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=18.65557719171047
Loss made of: CE 0.7327028512954712, LKD 7.0105814933776855, LDE 0.0, LReg 0.0, POD 12.188536643981934 EntMin 0.0
Epoch 2, Class Loss=0.588703989982605, Reg Loss=7.2064642906188965
Clinet index 26, End of Epoch 2/6, Average Loss=7.795168399810791, Class Loss=0.588703989982605, Reg Loss=7.2064642906188965
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=18.31405308842659
Loss made of: CE 0.5022929906845093, LKD 6.761961936950684, LDE 0.0, LReg 0.0, POD 10.327438354492188 EntMin 0.0
Epoch 3, Class Loss=0.547103226184845, Reg Loss=7.2133331298828125
Clinet index 26, End of Epoch 3/6, Average Loss=7.760436534881592, Class Loss=0.547103226184845, Reg Loss=7.2133331298828125
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=18.127865406870843
Loss made of: CE 0.4880330264568329, LKD 6.964683532714844, LDE 0.0, LReg 0.0, POD 9.958841323852539 EntMin 0.0
Epoch 4, Class Loss=0.5211629867553711, Reg Loss=7.174919128417969
Clinet index 26, End of Epoch 4/6, Average Loss=7.69608211517334, Class Loss=0.5211629867553711, Reg Loss=7.174919128417969
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=17.56135963201523
Loss made of: CE 0.47165927290916443, LKD 6.638902187347412, LDE 0.0, LReg 0.0, POD 9.72359848022461 EntMin 0.0
Epoch 5, Class Loss=0.5040537118911743, Reg Loss=7.136416435241699
Clinet index 26, End of Epoch 5/6, Average Loss=7.640470027923584, Class Loss=0.5040537118911743, Reg Loss=7.136416435241699
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=17.749380961060524
Loss made of: CE 0.44120287895202637, LKD 6.820215225219727, LDE 0.0, LReg 0.0, POD 10.116780281066895 EntMin 0.0
Epoch 6, Class Loss=0.4990532398223877, Reg Loss=7.176946640014648
Clinet index 26, End of Epoch 6/6, Average Loss=7.675999641418457, Class Loss=0.4990532398223877, Reg Loss=7.176946640014648
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=19.6067182302475
Loss made of: CE 0.5740143060684204, LKD 6.931887626647949, LDE 0.0, LReg 0.0, POD 10.399169921875 EntMin 0.0
Epoch 1, Class Loss=0.661056637763977, Reg Loss=7.2703166007995605
Clinet index 0, End of Epoch 1/6, Average Loss=7.931373119354248, Class Loss=0.661056637763977, Reg Loss=7.2703166007995605
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=18.390157371759415
Loss made of: CE 0.5417046546936035, LKD 7.399495601654053, LDE 0.0, LReg 0.0, POD 9.890270233154297 EntMin 0.0
Epoch 2, Class Loss=0.6046738028526306, Reg Loss=7.2318830490112305
Clinet index 0, End of Epoch 2/6, Average Loss=7.836556911468506, Class Loss=0.6046738028526306, Reg Loss=7.2318830490112305
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=18.14200411438942
Loss made of: CE 0.5751515626907349, LKD 7.065995693206787, LDE 0.0, LReg 0.0, POD 11.333630561828613 EntMin 0.0
Epoch 3, Class Loss=0.5676463842391968, Reg Loss=7.2362470626831055
Clinet index 0, End of Epoch 3/6, Average Loss=7.803893566131592, Class Loss=0.5676463842391968, Reg Loss=7.2362470626831055
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=17.909417855739594
Loss made of: CE 0.5130054950714111, LKD 7.3211894035339355, LDE 0.0, LReg 0.0, POD 11.14125919342041 EntMin 0.0
Epoch 4, Class Loss=0.5355308055877686, Reg Loss=7.270453453063965
Clinet index 0, End of Epoch 4/6, Average Loss=7.8059844970703125, Class Loss=0.5355308055877686, Reg Loss=7.270453453063965
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=17.929458501935006
Loss made of: CE 0.5521673560142517, LKD 7.591387748718262, LDE 0.0, LReg 0.0, POD 10.2276611328125 EntMin 0.0
Epoch 5, Class Loss=0.5321636199951172, Reg Loss=7.296268463134766
Clinet index 0, End of Epoch 5/6, Average Loss=7.828432083129883, Class Loss=0.5321636199951172, Reg Loss=7.296268463134766
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=17.611067947745322
Loss made of: CE 0.49554193019866943, LKD 6.437828063964844, LDE 0.0, LReg 0.0, POD 9.402523040771484 EntMin 0.0
Epoch 6, Class Loss=0.5197489261627197, Reg Loss=7.251921653747559
Clinet index 0, End of Epoch 6/6, Average Loss=7.771670341491699, Class Loss=0.5197489261627197, Reg Loss=7.251921653747559
federated aggregation...
Validation, Class Loss=0.8473062515258789, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.788638
Mean Acc: 0.479391
FreqW Acc: 0.708156
Mean IoU: 0.362206
Class IoU:
	class 0: 0.8305519
	class 1: 0.65089875
	class 2: 0.32060245
	class 3: 0.56020486
	class 4: 0.55694515
	class 5: 0.6242977
	class 6: 0.53188664
	class 7: 0.8278757
	class 8: 0.749053
	class 9: 0.059953585
	class 10: 0.0
	class 11: 0.00043800523
	class 12: 0.3508655
	class 13: 0.023855025
	class 14: 0.0001142461
	class 15: 0.6835977
	class 16: 0.008449911
	class 17: 0.0
	class 18: 0.10232892
Class Acc:
	class 0: 0.88576406
	class 1: 0.657846
	class 2: 0.9075446
	class 3: 0.57281154
	class 4: 0.58363515
	class 5: 0.6683828
	class 6: 0.5344026
	class 7: 0.9435347
	class 8: 0.87901497
	class 9: 0.45531136
	class 10: 0.0
	class 11: 0.00043800584
	class 12: 0.79215753
	class 13: 0.025249666
	class 14: 0.000114259696
	class 15: 0.89293885
	class 16: 0.008492858
	class 17: 0.0
	class 18: 0.3007845

federated global round: 28, step: 5
select part of clients to conduct local training
[8, 9, 16, 3]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=1.069031834602356, Reg Loss=6.422018527984619
Clinet index 8, End of Epoch 1/6, Average Loss=7.4910502433776855, Class Loss=1.069031834602356, Reg Loss=6.422018527984619
Pseudo labeling is: None
Epoch 2, lr = 0.000642
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.9625387191772461, Reg Loss=6.444820880889893
Clinet index 8, End of Epoch 2/6, Average Loss=7.407359600067139, Class Loss=0.9625387191772461, Reg Loss=6.444820880889893
Pseudo labeling is: None
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.9004520177841187, Reg Loss=6.463644027709961
Clinet index 8, End of Epoch 3/6, Average Loss=7.364096164703369, Class Loss=0.9004520177841187, Reg Loss=6.463644027709961
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.8105745911598206, Reg Loss=6.502503395080566
Clinet index 8, End of Epoch 4/6, Average Loss=7.313077926635742, Class Loss=0.8105745911598206, Reg Loss=6.502503395080566
Pseudo labeling is: None
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.7108088731765747, Reg Loss=6.399328231811523
Clinet index 8, End of Epoch 5/6, Average Loss=7.110136985778809, Class Loss=0.7108088731765747, Reg Loss=6.399328231811523
Pseudo labeling is: None
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.6569048166275024, Reg Loss=6.451076984405518
Clinet index 8, End of Epoch 6/6, Average Loss=7.1079816818237305, Class Loss=0.6569048166275024, Reg Loss=6.451076984405518
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.1280550956726074, Reg Loss=6.591214179992676
Clinet index 9, End of Epoch 1/6, Average Loss=7.719269275665283, Class Loss=1.1280550956726074, Reg Loss=6.591214179992676
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=1.0229737758636475, Reg Loss=6.606240272521973
Clinet index 9, End of Epoch 2/6, Average Loss=7.629214286804199, Class Loss=1.0229737758636475, Reg Loss=6.606240272521973
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.8623493313789368, Reg Loss=6.571529865264893
Clinet index 9, End of Epoch 3/6, Average Loss=7.433879375457764, Class Loss=0.8623493313789368, Reg Loss=6.571529865264893
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.7321760058403015, Reg Loss=6.579165458679199
Clinet index 9, End of Epoch 4/6, Average Loss=7.311341285705566, Class Loss=0.7321760058403015, Reg Loss=6.579165458679199
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.6587910056114197, Reg Loss=6.4795026779174805
Clinet index 9, End of Epoch 5/6, Average Loss=7.138293743133545, Class Loss=0.6587910056114197, Reg Loss=6.4795026779174805
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.61781245470047, Reg Loss=6.6507978439331055
Clinet index 9, End of Epoch 6/6, Average Loss=7.26861047744751, Class Loss=0.61781245470047, Reg Loss=6.6507978439331055
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=18.00983901321888
Loss made of: CE 0.447437047958374, LKD 7.086399078369141, LDE 0.0, LReg 0.0, POD 10.469600677490234 EntMin 0.0
Epoch 1, Class Loss=0.5345929265022278, Reg Loss=7.27451229095459
Clinet index 16, End of Epoch 1/6, Average Loss=7.809105396270752, Class Loss=0.5345929265022278, Reg Loss=7.27451229095459
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=17.500462904572487
Loss made of: CE 0.49273428320884705, LKD 7.0303168296813965, LDE 0.0, LReg 0.0, POD 9.730937004089355 EntMin 0.0
Epoch 2, Class Loss=0.5229310989379883, Reg Loss=7.263725280761719
Clinet index 16, End of Epoch 2/6, Average Loss=7.786656379699707, Class Loss=0.5229310989379883, Reg Loss=7.263725280761719
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=17.058626839518546
Loss made of: CE 0.46691030263900757, LKD 7.086965560913086, LDE 0.0, LReg 0.0, POD 9.832075119018555 EntMin 0.0
Epoch 3, Class Loss=0.4870622754096985, Reg Loss=7.207886695861816
Clinet index 16, End of Epoch 3/6, Average Loss=7.694949150085449, Class Loss=0.4870622754096985, Reg Loss=7.207886695861816
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=16.996855786442758
Loss made of: CE 0.5087630748748779, LKD 7.451587200164795, LDE 0.0, LReg 0.0, POD 9.16936206817627 EntMin 0.0
Epoch 4, Class Loss=0.4813375473022461, Reg Loss=7.215638637542725
Clinet index 16, End of Epoch 4/6, Average Loss=7.696976184844971, Class Loss=0.4813375473022461, Reg Loss=7.215638637542725
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=17.032032451033594
Loss made of: CE 0.4859164357185364, LKD 7.0626540184021, LDE 0.0, LReg 0.0, POD 8.891660690307617 EntMin 0.0
Epoch 5, Class Loss=0.47884342074394226, Reg Loss=7.199880599975586
Clinet index 16, End of Epoch 5/6, Average Loss=7.6787238121032715, Class Loss=0.47884342074394226, Reg Loss=7.199880599975586
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=17.055671560764313
Loss made of: CE 0.4672665596008301, LKD 7.032291412353516, LDE 0.0, LReg 0.0, POD 9.602293014526367 EntMin 0.0
Epoch 6, Class Loss=0.47144097089767456, Reg Loss=7.18370246887207
Clinet index 16, End of Epoch 6/6, Average Loss=7.6551432609558105, Class Loss=0.47144097089767456, Reg Loss=7.18370246887207
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=17.79444578588009
Loss made of: CE 0.48667725920677185, LKD 7.374980926513672, LDE 0.0, LReg 0.0, POD 10.59677791595459 EntMin 0.0
Epoch 1, Class Loss=0.5432013273239136, Reg Loss=7.11970853805542
Clinet index 3, End of Epoch 1/6, Average Loss=7.662909984588623, Class Loss=0.5432013273239136, Reg Loss=7.11970853805542
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=17.77462291419506
Loss made of: CE 0.6215860247612, LKD 7.241204738616943, LDE 0.0, LReg 0.0, POD 10.074384689331055 EntMin 0.0
Epoch 2, Class Loss=0.5257842540740967, Reg Loss=7.186500549316406
Clinet index 3, End of Epoch 2/6, Average Loss=7.712285041809082, Class Loss=0.5257842540740967, Reg Loss=7.186500549316406
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=17.694061028957368
Loss made of: CE 0.5790081024169922, LKD 7.459774017333984, LDE 0.0, LReg 0.0, POD 10.660049438476562 EntMin 0.0
Epoch 3, Class Loss=0.5107356905937195, Reg Loss=7.258525848388672
Clinet index 3, End of Epoch 3/6, Average Loss=7.769261360168457, Class Loss=0.5107356905937195, Reg Loss=7.258525848388672
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=17.784669974446295
Loss made of: CE 0.46186313033103943, LKD 6.977891445159912, LDE 0.0, LReg 0.0, POD 9.404457092285156 EntMin 0.0
Epoch 4, Class Loss=0.49058985710144043, Reg Loss=7.161502838134766
Clinet index 3, End of Epoch 4/6, Average Loss=7.652092933654785, Class Loss=0.49058985710144043, Reg Loss=7.161502838134766
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=17.198017543554307
Loss made of: CE 0.4539293646812439, LKD 6.689132213592529, LDE 0.0, LReg 0.0, POD 9.920377731323242 EntMin 0.0
Epoch 5, Class Loss=0.4833999276161194, Reg Loss=7.151111602783203
Clinet index 3, End of Epoch 5/6, Average Loss=7.634511470794678, Class Loss=0.4833999276161194, Reg Loss=7.151111602783203
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=17.130098843574523
Loss made of: CE 0.4752471148967743, LKD 7.4449567794799805, LDE 0.0, LReg 0.0, POD 8.778787612915039 EntMin 0.0
Epoch 6, Class Loss=0.4768233299255371, Reg Loss=7.209012985229492
Clinet index 3, End of Epoch 6/6, Average Loss=7.685836315155029, Class Loss=0.4768233299255371, Reg Loss=7.209012985229492
federated aggregation...
Validation, Class Loss=0.8029682040214539, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.796887
Mean Acc: 0.456423
FreqW Acc: 0.711958
Mean IoU: 0.347538
Class IoU:
	class 0: 0.8415379
	class 1: 0.6094278
	class 2: 0.32106537
	class 3: 0.52657247
	class 4: 0.49372977
	class 5: 0.5903249
	class 6: 0.45454648
	class 7: 0.8185306
	class 8: 0.7408219
	class 9: 0.058371115
	class 10: 0.0
	class 11: 0.00038137526
	class 12: 0.33935082
	class 13: 0.022864904
	class 14: 0.00021658692
	class 15: 0.6936112
	class 16: 0.0077601373
	class 17: 3.010253e-07
	class 18: 0.0841133
Class Acc:
	class 0: 0.9082185
	class 1: 0.61401397
	class 2: 0.9019642
	class 3: 0.53604496
	class 4: 0.51141536
	class 5: 0.6297512
	class 6: 0.45555633
	class 7: 0.9406093
	class 8: 0.86330926
	class 9: 0.42764428
	class 10: 0.0
	class 11: 0.00038137526
	class 12: 0.80288357
	class 13: 0.024153199
	class 14: 0.00021661734
	class 15: 0.8759925
	class 16: 0.0077982107
	class 17: 3.010253e-07
	class 18: 0.17207953

federated global round: 29, step: 5
select part of clients to conduct local training
[29, 27, 26, 18]
Current Client Index:  29
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=18.863009878993033
Loss made of: CE 0.6042903661727905, LKD 7.15974760055542, LDE 0.0, LReg 0.0, POD 11.090264320373535 EntMin 0.0
Epoch 1, Class Loss=0.5771437883377075, Reg Loss=7.260770320892334
Clinet index 29, End of Epoch 1/6, Average Loss=7.837913990020752, Class Loss=0.5771437883377075, Reg Loss=7.260770320892334
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=18.05949263572693
Loss made of: CE 0.4677734076976776, LKD 7.198245048522949, LDE 0.0, LReg 0.0, POD 8.789655685424805 EntMin 0.0
Epoch 2, Class Loss=0.5140509009361267, Reg Loss=7.232381820678711
Clinet index 29, End of Epoch 2/6, Average Loss=7.746432781219482, Class Loss=0.5140509009361267, Reg Loss=7.232381820678711
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=17.158542144298554
Loss made of: CE 0.5308976769447327, LKD 7.213627815246582, LDE 0.0, LReg 0.0, POD 8.726273536682129 EntMin 0.0
Epoch 3, Class Loss=0.5001940727233887, Reg Loss=7.229814529418945
Clinet index 29, End of Epoch 3/6, Average Loss=7.730008602142334, Class Loss=0.5001940727233887, Reg Loss=7.229814529418945
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=17.518774142861368
Loss made of: CE 0.49125251173973083, LKD 7.083364009857178, LDE 0.0, LReg 0.0, POD 9.612237930297852 EntMin 0.0
Epoch 4, Class Loss=0.4958276152610779, Reg Loss=7.185781478881836
Clinet index 29, End of Epoch 4/6, Average Loss=7.681609153747559, Class Loss=0.4958276152610779, Reg Loss=7.185781478881836
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=17.024838039278983
Loss made of: CE 0.4702940881252289, LKD 6.744017601013184, LDE 0.0, LReg 0.0, POD 9.57486343383789 EntMin 0.0
Epoch 5, Class Loss=0.48783785104751587, Reg Loss=7.141196250915527
Clinet index 29, End of Epoch 5/6, Average Loss=7.629034042358398, Class Loss=0.48783785104751587, Reg Loss=7.141196250915527
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=16.93513505756855
Loss made of: CE 0.4297311305999756, LKD 7.356664180755615, LDE 0.0, LReg 0.0, POD 8.194459915161133 EntMin 0.0
Epoch 6, Class Loss=0.4815412163734436, Reg Loss=7.242746353149414
Clinet index 29, End of Epoch 6/6, Average Loss=7.724287509918213, Class Loss=0.4815412163734436, Reg Loss=7.242746353149414
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.9654911756515503, Reg Loss=6.607613563537598
Clinet index 27, End of Epoch 1/6, Average Loss=7.5731048583984375, Class Loss=0.9654911756515503, Reg Loss=6.607613563537598
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.8762262463569641, Reg Loss=6.5612359046936035
Clinet index 27, End of Epoch 2/6, Average Loss=7.437462329864502, Class Loss=0.8762262463569641, Reg Loss=6.5612359046936035
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.7359257936477661, Reg Loss=6.4236884117126465
Clinet index 27, End of Epoch 3/6, Average Loss=7.159614086151123, Class Loss=0.7359257936477661, Reg Loss=6.4236884117126465
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.711016058921814, Reg Loss=6.525905132293701
Clinet index 27, End of Epoch 4/6, Average Loss=7.236921310424805, Class Loss=0.711016058921814, Reg Loss=6.525905132293701
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.651019811630249, Reg Loss=6.526782035827637
Clinet index 27, End of Epoch 5/6, Average Loss=7.177802085876465, Class Loss=0.651019811630249, Reg Loss=6.526782035827637
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.6457093358039856, Reg Loss=6.482174873352051
Clinet index 27, End of Epoch 6/6, Average Loss=7.127884387969971, Class Loss=0.6457093358039856, Reg Loss=6.482174873352051
Current Client Index:  26
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=18.79971036911011
Loss made of: CE 0.48142558336257935, LKD 6.839201927185059, LDE 0.0, LReg 0.0, POD 9.619417190551758 EntMin 0.0
Epoch 1, Class Loss=0.5555288791656494, Reg Loss=7.222344398498535
Clinet index 26, End of Epoch 1/6, Average Loss=7.7778730392456055, Class Loss=0.5555288791656494, Reg Loss=7.222344398498535
Pseudo labeling is: None
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/12, Loss=17.509428471326828
Loss made of: CE 0.669521689414978, LKD 6.952060222625732, LDE 0.0, LReg 0.0, POD 10.581273078918457 EntMin 0.0
Epoch 2, Class Loss=0.5409129858016968, Reg Loss=7.164705276489258
Clinet index 26, End of Epoch 2/6, Average Loss=7.705618381500244, Class Loss=0.5409129858016968, Reg Loss=7.164705276489258
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/12, Loss=17.17356253862381
Loss made of: CE 0.4616283178329468, LKD 6.750713348388672, LDE 0.0, LReg 0.0, POD 9.216001510620117 EntMin 0.0
Epoch 3, Class Loss=0.5175259113311768, Reg Loss=7.186853408813477
Clinet index 26, End of Epoch 3/6, Average Loss=7.704379081726074, Class Loss=0.5175259113311768, Reg Loss=7.186853408813477
Pseudo labeling is: None
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/12, Loss=17.08981554210186
Loss made of: CE 0.44619864225387573, LKD 6.922355651855469, LDE 0.0, LReg 0.0, POD 8.864059448242188 EntMin 0.0
Epoch 4, Class Loss=0.4998585879802704, Reg Loss=7.177085876464844
Clinet index 26, End of Epoch 4/6, Average Loss=7.676944255828857, Class Loss=0.4998585879802704, Reg Loss=7.177085876464844
Pseudo labeling is: None
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/12, Loss=16.394994103908537
Loss made of: CE 0.44172197580337524, LKD 6.637317657470703, LDE 0.0, LReg 0.0, POD 9.056614875793457 EntMin 0.0
Epoch 5, Class Loss=0.48668432235717773, Reg Loss=7.15818977355957
Clinet index 26, End of Epoch 5/6, Average Loss=7.644874095916748, Class Loss=0.48668432235717773, Reg Loss=7.15818977355957
Pseudo labeling is: None
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/12, Loss=16.572932007908822
Loss made of: CE 0.4413480758666992, LKD 6.868086814880371, LDE 0.0, LReg 0.0, POD 9.056060791015625 EntMin 0.0
Epoch 6, Class Loss=0.486286461353302, Reg Loss=7.120734214782715
Clinet index 26, End of Epoch 6/6, Average Loss=7.607020854949951, Class Loss=0.486286461353302, Reg Loss=7.120734214782715
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=18.851302021741866
Loss made of: CE 0.5200862884521484, LKD 6.922415733337402, LDE 0.0, LReg 0.0, POD 10.515830039978027 EntMin 0.0
Epoch 1, Class Loss=0.5832038521766663, Reg Loss=7.269204139709473
Clinet index 18, End of Epoch 1/6, Average Loss=7.852407932281494, Class Loss=0.5832038521766663, Reg Loss=7.269204139709473
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=17.932211437821387
Loss made of: CE 0.45204704999923706, LKD 6.925098419189453, LDE 0.0, LReg 0.0, POD 9.811330795288086 EntMin 0.0
Epoch 2, Class Loss=0.5304016470909119, Reg Loss=7.277822494506836
Clinet index 18, End of Epoch 2/6, Average Loss=7.808224201202393, Class Loss=0.5304016470909119, Reg Loss=7.277822494506836
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=17.508175686001778
Loss made of: CE 0.4569738805294037, LKD 6.781589508056641, LDE 0.0, LReg 0.0, POD 10.1378812789917 EntMin 0.0
Epoch 3, Class Loss=0.5114104747772217, Reg Loss=7.260550498962402
Clinet index 18, End of Epoch 3/6, Average Loss=7.771961212158203, Class Loss=0.5114104747772217, Reg Loss=7.260550498962402
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=17.54962948560715
Loss made of: CE 0.5036095976829529, LKD 7.170832633972168, LDE 0.0, LReg 0.0, POD 10.288707733154297 EntMin 0.0
Epoch 4, Class Loss=0.5034972429275513, Reg Loss=7.274005889892578
Clinet index 18, End of Epoch 4/6, Average Loss=7.77750301361084, Class Loss=0.5034972429275513, Reg Loss=7.274005889892578
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=17.313255971670152
Loss made of: CE 0.4500202536582947, LKD 7.388436794281006, LDE 0.0, LReg 0.0, POD 10.275226593017578 EntMin 0.0
Epoch 5, Class Loss=0.49763593077659607, Reg Loss=7.318572998046875
Clinet index 18, End of Epoch 5/6, Average Loss=7.816208839416504, Class Loss=0.49763593077659607, Reg Loss=7.318572998046875
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=16.928712046146394
Loss made of: CE 0.4669356048107147, LKD 6.565410614013672, LDE 0.0, LReg 0.0, POD 9.840757369995117 EntMin 0.0
Epoch 6, Class Loss=0.4890749156475067, Reg Loss=7.269235610961914
Clinet index 18, End of Epoch 6/6, Average Loss=7.758310317993164, Class Loss=0.4890749156475067, Reg Loss=7.269235610961914
federated aggregation...
Validation, Class Loss=0.8567150235176086, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.774653
Mean Acc: 0.481953
FreqW Acc: 0.696215
Mean IoU: 0.360309
Class IoU:
	class 0: 0.81343025
	class 1: 0.6613664
	class 2: 0.32180694
	class 3: 0.55760455
	class 4: 0.5505975
	class 5: 0.61949915
	class 6: 0.52638745
	class 7: 0.81972677
	class 8: 0.7462329
	class 9: 0.05756084
	class 10: 0.0
	class 11: 0.00040769292
	class 12: 0.35288015
	class 13: 0.021561047
	class 14: 0.00012441781
	class 15: 0.68834156
	class 16: 0.0061884895
	class 17: 0.0
	class 18: 0.10216401
Class Acc:
	class 0: 0.86417276
	class 1: 0.66929096
	class 2: 0.903875
	class 3: 0.5705323
	class 4: 0.57559216
	class 5: 0.66312057
	class 6: 0.5287881
	class 7: 0.9448028
	class 8: 0.8759316
	class 9: 0.42436546
	class 10: 0.0
	class 11: 0.0004076932
	class 12: 0.7870847
	class 13: 0.022574563
	class 14: 0.00012443055
	class 15: 0.88624823
	class 16: 0.006211855
	class 17: 0.0
	class 18: 0.43399274

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 30, step: 6
select part of clients to conduct local training
[10, 30, 4, 33]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=33.579420709609984
Loss made of: CE 1.0784573554992676, LKD 7.054532527923584, LDE 0.0, LReg 0.0, POD 23.133007049560547 EntMin 0.0
Epoch 1, Class Loss=1.1129447221755981, Reg Loss=7.047893047332764
Clinet index 10, End of Epoch 1/6, Average Loss=8.16083812713623, Class Loss=1.1129447221755981, Reg Loss=7.047893047332764
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=27.159863072633744
Loss made of: CE 0.8372578620910645, LKD 6.517246246337891, LDE 0.0, LReg 0.0, POD 19.709962844848633 EntMin 0.0
Epoch 2, Class Loss=0.8138560652732849, Reg Loss=6.0901198387146
Clinet index 10, End of Epoch 2/6, Average Loss=6.903975963592529, Class Loss=0.8138560652732849, Reg Loss=6.0901198387146
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=25.089285796880723
Loss made of: CE 0.738166093826294, LKD 6.095964431762695, LDE 0.0, LReg 0.0, POD 17.88180923461914 EntMin 0.0
Epoch 3, Class Loss=0.7284847497940063, Reg Loss=5.880046367645264
Clinet index 10, End of Epoch 3/6, Average Loss=6.6085309982299805, Class Loss=0.7284847497940063, Reg Loss=5.880046367645264
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=23.353166139125825
Loss made of: CE 0.554031252861023, LKD 5.44659423828125, LDE 0.0, LReg 0.0, POD 15.514331817626953 EntMin 0.0
Epoch 4, Class Loss=0.625389575958252, Reg Loss=5.823262691497803
Clinet index 10, End of Epoch 4/6, Average Loss=6.448652267456055, Class Loss=0.625389575958252, Reg Loss=5.823262691497803
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=21.944243675470354
Loss made of: CE 0.5476943850517273, LKD 5.69008731842041, LDE 0.0, LReg 0.0, POD 14.646413803100586 EntMin 0.0
Epoch 5, Class Loss=0.5485084056854248, Reg Loss=5.798388481140137
Clinet index 10, End of Epoch 5/6, Average Loss=6.346897125244141, Class Loss=0.5485084056854248, Reg Loss=5.798388481140137
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=21.045035710930826
Loss made of: CE 0.4738582372665405, LKD 5.872065544128418, LDE 0.0, LReg 0.0, POD 15.251840591430664 EntMin 0.0
Epoch 6, Class Loss=0.5149389505386353, Reg Loss=5.8481597900390625
Clinet index 10, End of Epoch 6/6, Average Loss=6.363098621368408, Class Loss=0.5149389505386353, Reg Loss=5.8481597900390625
Current Client Index:  30
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=28.045416712760925
Loss made of: CE 0.6401905417442322, LKD 7.383836269378662, LDE 0.0, LReg 0.0, POD 17.877607345581055 EntMin 0.0
Epoch 1, Class Loss=0.663186252117157, Reg Loss=7.935490131378174
Clinet index 30, End of Epoch 1/6, Average Loss=8.598676681518555, Class Loss=0.663186252117157, Reg Loss=7.935490131378174
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=23.586990118026733
Loss made of: CE 0.5220859050750732, LKD 7.898347854614258, LDE 0.0, LReg 0.0, POD 11.695121765136719 EntMin 0.0
Epoch 2, Class Loss=0.5536367893218994, Reg Loss=7.703709125518799
Clinet index 30, End of Epoch 2/6, Average Loss=8.257346153259277, Class Loss=0.5536367893218994, Reg Loss=7.703709125518799
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=21.378228399157525
Loss made of: CE 0.4972991943359375, LKD 7.995588302612305, LDE 0.0, LReg 0.0, POD 13.309428215026855 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.5035924911499023, Reg Loss=7.6723127365112305
Clinet index 30, End of Epoch 3/6, Average Loss=8.175905227661133, Class Loss=0.5035924911499023, Reg Loss=7.6723127365112305
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=20.503552702069282
Loss made of: CE 0.5280930995941162, LKD 7.688225746154785, LDE 0.0, LReg 0.0, POD 10.609092712402344 EntMin 0.0
Epoch 4, Class Loss=0.4604850709438324, Reg Loss=7.566892623901367
Clinet index 30, End of Epoch 4/6, Average Loss=8.02737808227539, Class Loss=0.4604850709438324, Reg Loss=7.566892623901367
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=20.190339866280556
Loss made of: CE 0.5033172965049744, LKD 7.235654354095459, LDE 0.0, LReg 0.0, POD 16.89437484741211 EntMin 0.0
Epoch 5, Class Loss=0.4339662492275238, Reg Loss=7.601336479187012
Clinet index 30, End of Epoch 5/6, Average Loss=8.035303115844727, Class Loss=0.4339662492275238, Reg Loss=7.601336479187012
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=19.25618471503258
Loss made of: CE 0.5479334592819214, LKD 7.849790096282959, LDE 0.0, LReg 0.0, POD 10.72288703918457 EntMin 0.0
Epoch 6, Class Loss=0.4208719730377197, Reg Loss=7.588927745819092
Clinet index 30, End of Epoch 6/6, Average Loss=8.00979995727539, Class Loss=0.4208719730377197, Reg Loss=7.588927745819092
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=32.847191733121875
Loss made of: CE 0.9718035459518433, LKD 6.324059963226318, LDE 0.0, LReg 0.0, POD 20.213848114013672 EntMin 0.0
Epoch 1, Class Loss=1.0823355913162231, Reg Loss=6.930591583251953
Clinet index 4, End of Epoch 1/6, Average Loss=8.012927055358887, Class Loss=1.0823355913162231, Reg Loss=6.930591583251953
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=26.79177480340004
Loss made of: CE 0.7732571363449097, LKD 5.8907470703125, LDE 0.0, LReg 0.0, POD 20.55246925354004 EntMin 0.0
Epoch 2, Class Loss=0.8061746954917908, Reg Loss=5.973003387451172
Clinet index 4, End of Epoch 2/6, Average Loss=6.779178142547607, Class Loss=0.8061746954917908, Reg Loss=5.973003387451172
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=24.67268639206886
Loss made of: CE 0.7616121172904968, LKD 5.608607292175293, LDE 0.0, LReg 0.0, POD 18.147890090942383 EntMin 0.0
Epoch 3, Class Loss=0.7107416987419128, Reg Loss=5.787883758544922
Clinet index 4, End of Epoch 3/6, Average Loss=6.4986252784729, Class Loss=0.7107416987419128, Reg Loss=5.787883758544922
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=23.036494898796082
Loss made of: CE 0.5957072973251343, LKD 5.596316337585449, LDE 0.0, LReg 0.0, POD 17.926677703857422 EntMin 0.0
Epoch 4, Class Loss=0.616011381149292, Reg Loss=5.755500316619873
Clinet index 4, End of Epoch 4/6, Average Loss=6.371511459350586, Class Loss=0.616011381149292, Reg Loss=5.755500316619873
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=21.671899557113647
Loss made of: CE 0.5705331563949585, LKD 5.929933071136475, LDE 0.0, LReg 0.0, POD 15.178433418273926 EntMin 0.0
Epoch 5, Class Loss=0.5552114844322205, Reg Loss=5.698944091796875
Clinet index 4, End of Epoch 5/6, Average Loss=6.25415563583374, Class Loss=0.5552114844322205, Reg Loss=5.698944091796875
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=20.606351763010025
Loss made of: CE 0.39869505167007446, LKD 4.955150604248047, LDE 0.0, LReg 0.0, POD 16.283245086669922 EntMin 0.0
Epoch 6, Class Loss=0.5030305981636047, Reg Loss=5.7382402420043945
Clinet index 4, End of Epoch 6/6, Average Loss=6.241271018981934, Class Loss=0.5030305981636047, Reg Loss=5.7382402420043945
Current Client Index:  33
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=33.48403489589691
Loss made of: CE 0.9018759727478027, LKD 6.4413251876831055, LDE 0.0, LReg 0.0, POD 22.30276107788086 EntMin 0.0
Epoch 1, Class Loss=1.0866482257843018, Reg Loss=6.915849208831787
Clinet index 33, End of Epoch 1/6, Average Loss=8.002497673034668, Class Loss=1.0866482257843018, Reg Loss=6.915849208831787
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=27.62336493730545
Loss made of: CE 0.7933878302574158, LKD 5.923030853271484, LDE 0.0, LReg 0.0, POD 18.629037857055664 EntMin 0.0
Epoch 2, Class Loss=0.8254587054252625, Reg Loss=6.010785102844238
Clinet index 33, End of Epoch 2/6, Average Loss=6.836243629455566, Class Loss=0.8254587054252625, Reg Loss=6.010785102844238
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=25.06921457052231
Loss made of: CE 0.660649836063385, LKD 5.741961479187012, LDE 0.0, LReg 0.0, POD 17.164464950561523 EntMin 0.0
Epoch 3, Class Loss=0.7064792513847351, Reg Loss=5.877145767211914
Clinet index 33, End of Epoch 3/6, Average Loss=6.583624839782715, Class Loss=0.7064792513847351, Reg Loss=5.877145767211914
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=23.300425493717192
Loss made of: CE 0.542601466178894, LKD 5.872941493988037, LDE 0.0, LReg 0.0, POD 16.249738693237305 EntMin 0.0
Epoch 4, Class Loss=0.623596727848053, Reg Loss=5.770928382873535
Clinet index 33, End of Epoch 4/6, Average Loss=6.394525051116943, Class Loss=0.623596727848053, Reg Loss=5.770928382873535
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=22.327629166841508
Loss made of: CE 0.5551143884658813, LKD 5.951659202575684, LDE 0.0, LReg 0.0, POD 13.863580703735352 EntMin 0.0
Epoch 5, Class Loss=0.556678056716919, Reg Loss=5.742246150970459
Clinet index 33, End of Epoch 5/6, Average Loss=6.298924446105957, Class Loss=0.556678056716919, Reg Loss=5.742246150970459
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=21.181013482809067
Loss made of: CE 0.5362309217453003, LKD 5.726678371429443, LDE 0.0, LReg 0.0, POD 14.657052040100098 EntMin 0.0
Epoch 6, Class Loss=0.5035092830657959, Reg Loss=5.7535223960876465
Clinet index 33, End of Epoch 6/6, Average Loss=6.257031440734863, Class Loss=0.5035092830657959, Reg Loss=5.7535223960876465
federated aggregation...
Validation, Class Loss=0.9376904368400574, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.786872
Mean Acc: 0.413434
FreqW Acc: 0.697658
Mean IoU: 0.326216
Class IoU:
	class 0: 0.8300349
	class 1: 0.5044649
	class 2: 0.34222898
	class 3: 0.29989266
	class 4: 0.517575
	class 5: 0.5451351
	class 6: 0.6512847
	class 7: 0.80687535
	class 8: 0.6757165
	class 9: 0.039743334
	class 10: 0.0
	class 11: 0.0028838965
	class 12: 0.383575
	class 13: 0.071817316
	class 14: 0.003543429
	class 15: 0.7022551
	class 16: 0.020862898
	class 17: 0.0
	class 18: 0.07309322
	class 19: 0.37956288
	class 20: 0.0
Class Acc:
	class 0: 0.9167825
	class 1: 0.50779897
	class 2: 0.8729325
	class 3: 0.30067363
	class 4: 0.5515087
	class 5: 0.56913304
	class 6: 0.6676799
	class 7: 0.9149184
	class 8: 0.72761416
	class 9: 0.26653937
	class 10: 0.0
	class 11: 0.0028844
	class 12: 0.69338274
	class 13: 0.07995371
	class 14: 0.003550923
	class 15: 0.84306926
	class 16: 0.021088231
	class 17: 0.0
	class 18: 0.27059978
	class 19: 0.47200483
	class 20: 0.0

federated global round: 31, step: 6
select part of clients to conduct local training
[9, 17, 14, 1]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=26.411968049407005
Loss made of: CE 0.7669183611869812, LKD 7.583756923675537, LDE 0.0, LReg 0.0, POD 13.758973121643066 EntMin 0.0
Epoch 1, Class Loss=0.6857694983482361, Reg Loss=7.79286003112793
Clinet index 9, End of Epoch 1/6, Average Loss=8.478629112243652, Class Loss=0.6857694983482361, Reg Loss=7.79286003112793
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/13, Loss=22.103464731574057
Loss made of: CE 0.44399747252464294, LKD 7.6072797775268555, LDE 0.0, LReg 0.0, POD 13.68293285369873 EntMin 0.0
Epoch 2, Class Loss=0.587493896484375, Reg Loss=7.638662815093994
Clinet index 9, End of Epoch 2/6, Average Loss=8.226156234741211, Class Loss=0.587493896484375, Reg Loss=7.638662815093994
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=20.712199956178665
Loss made of: CE 0.5047218799591064, LKD 7.738462448120117, LDE 0.0, LReg 0.0, POD 14.126867294311523 EntMin 0.0
Epoch 3, Class Loss=0.5067349076271057, Reg Loss=7.548659324645996
Clinet index 9, End of Epoch 3/6, Average Loss=8.055394172668457, Class Loss=0.5067349076271057, Reg Loss=7.548659324645996
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=19.522937202453612
Loss made of: CE 0.39042162895202637, LKD 7.643239974975586, LDE 0.0, LReg 0.0, POD 12.050375938415527 EntMin 0.0
Epoch 4, Class Loss=0.4637846350669861, Reg Loss=7.549311637878418
Clinet index 9, End of Epoch 4/6, Average Loss=8.01309585571289, Class Loss=0.4637846350669861, Reg Loss=7.549311637878418
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=19.368071961402894
Loss made of: CE 0.39556440711021423, LKD 7.2927470207214355, LDE 0.0, LReg 0.0, POD 11.274521827697754 EntMin 0.0
Epoch 5, Class Loss=0.44216388463974, Reg Loss=7.609343528747559
Clinet index 9, End of Epoch 5/6, Average Loss=8.051506996154785, Class Loss=0.44216388463974, Reg Loss=7.609343528747559
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=19.292981758713722
Loss made of: CE 0.3830227255821228, LKD 7.01761531829834, LDE 0.0, LReg 0.0, POD 11.0040283203125 EntMin 0.0
Epoch 6, Class Loss=0.41052573919296265, Reg Loss=7.53199577331543
Clinet index 9, End of Epoch 6/6, Average Loss=7.942521572113037, Class Loss=0.41052573919296265, Reg Loss=7.53199577331543
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=21.517715415358545
Loss made of: CE 0.46260425448417664, LKD 5.522149562835693, LDE 0.0, LReg 0.0, POD 15.035106658935547 EntMin 0.0
Epoch 1, Class Loss=0.6196198463439941, Reg Loss=5.829068183898926
Clinet index 17, End of Epoch 1/6, Average Loss=6.44868803024292, Class Loss=0.6196198463439941, Reg Loss=5.829068183898926
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=20.923721101880073
Loss made of: CE 0.4552188813686371, LKD 5.5925140380859375, LDE 0.0, LReg 0.0, POD 13.992961883544922 EntMin 0.0
Epoch 2, Class Loss=0.5028702020645142, Reg Loss=5.784160137176514
Clinet index 17, End of Epoch 2/6, Average Loss=6.287030220031738, Class Loss=0.5028702020645142, Reg Loss=5.784160137176514
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=19.673095917701723
Loss made of: CE 0.5238845348358154, LKD 6.315967559814453, LDE 0.0, LReg 0.0, POD 12.34788703918457 EntMin 0.0
Epoch 3, Class Loss=0.46021005511283875, Reg Loss=5.802657127380371
Clinet index 17, End of Epoch 3/6, Average Loss=6.262866973876953, Class Loss=0.46021005511283875, Reg Loss=5.802657127380371
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=19.56145803630352
Loss made of: CE 0.44258570671081543, LKD 6.015555381774902, LDE 0.0, LReg 0.0, POD 11.68797779083252 EntMin 0.0
Epoch 4, Class Loss=0.4478662610054016, Reg Loss=5.8380584716796875
Clinet index 17, End of Epoch 4/6, Average Loss=6.285924911499023, Class Loss=0.4478662610054016, Reg Loss=5.8380584716796875
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=18.3920899361372
Loss made of: CE 0.46813535690307617, LKD 5.727560043334961, LDE 0.0, LReg 0.0, POD 12.08850383758545 EntMin 0.0
Epoch 5, Class Loss=0.4212535619735718, Reg Loss=5.76216459274292
Clinet index 17, End of Epoch 5/6, Average Loss=6.183418273925781, Class Loss=0.4212535619735718, Reg Loss=5.76216459274292
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=18.38998367190361
Loss made of: CE 0.39582714438438416, LKD 5.891383647918701, LDE 0.0, LReg 0.0, POD 11.637308120727539 EntMin 0.0
Epoch 6, Class Loss=0.4104192852973938, Reg Loss=5.788372039794922
Clinet index 17, End of Epoch 6/6, Average Loss=6.19879150390625, Class Loss=0.4104192852973938, Reg Loss=5.788372039794922
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=26.46144895553589
Loss made of: CE 0.6885585784912109, LKD 8.321619987487793, LDE 0.0, LReg 0.0, POD 14.967881202697754 EntMin 0.0
Epoch 1, Class Loss=0.6499063372612, Reg Loss=7.846989154815674
Clinet index 14, End of Epoch 1/6, Average Loss=8.496895790100098, Class Loss=0.6499063372612, Reg Loss=7.846989154815674
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=21.535237339138984
Loss made of: CE 0.41269347071647644, LKD 7.79585599899292, LDE 0.0, LReg 0.0, POD 12.073274612426758 EntMin 0.0
Epoch 2, Class Loss=0.573506772518158, Reg Loss=7.598989486694336
Clinet index 14, End of Epoch 2/6, Average Loss=8.17249584197998, Class Loss=0.573506772518158, Reg Loss=7.598989486694336
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=20.418583020567894
Loss made of: CE 0.4002777338027954, LKD 7.956639289855957, LDE 0.0, LReg 0.0, POD 11.179813385009766 EntMin 0.0
Epoch 3, Class Loss=0.48893555998802185, Reg Loss=7.589804649353027
Clinet index 14, End of Epoch 3/6, Average Loss=8.078740119934082, Class Loss=0.48893555998802185, Reg Loss=7.589804649353027
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=19.192720973491667
Loss made of: CE 0.30228087306022644, LKD 7.385763168334961, LDE 0.0, LReg 0.0, POD 11.462425231933594 EntMin 0.0
Epoch 4, Class Loss=0.4437161386013031, Reg Loss=7.497967720031738
Clinet index 14, End of Epoch 4/6, Average Loss=7.941683769226074, Class Loss=0.4437161386013031, Reg Loss=7.497967720031738
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=18.998503547906875
Loss made of: CE 0.38742679357528687, LKD 7.525302886962891, LDE 0.0, LReg 0.0, POD 10.066030502319336 EntMin 0.0
Epoch 5, Class Loss=0.4159556031227112, Reg Loss=7.5099263191223145
Clinet index 14, End of Epoch 5/6, Average Loss=7.925881862640381, Class Loss=0.4159556031227112, Reg Loss=7.5099263191223145
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=19.079117384552955
Loss made of: CE 0.5399433374404907, LKD 7.652606010437012, LDE 0.0, LReg 0.0, POD 11.795669555664062 EntMin 0.0
Epoch 6, Class Loss=0.40278300642967224, Reg Loss=7.590615272521973
Clinet index 14, End of Epoch 6/6, Average Loss=7.993398189544678, Class Loss=0.40278300642967224, Reg Loss=7.590615272521973
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=21.690264022350313
Loss made of: CE 0.5396357178688049, LKD 5.422976493835449, LDE 0.0, LReg 0.0, POD 14.870075225830078 EntMin 0.0
Epoch 1, Class Loss=0.6115709543228149, Reg Loss=5.772676467895508
Clinet index 1, End of Epoch 1/6, Average Loss=6.384247303009033, Class Loss=0.6115709543228149, Reg Loss=5.772676467895508
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=20.432712894678115
Loss made of: CE 0.5127875804901123, LKD 5.927146911621094, LDE 0.0, LReg 0.0, POD 12.784271240234375 EntMin 0.0
Epoch 2, Class Loss=0.5139777660369873, Reg Loss=5.8493781089782715
Clinet index 1, End of Epoch 2/6, Average Loss=6.36335563659668, Class Loss=0.5139777660369873, Reg Loss=5.8493781089782715
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=19.80681256055832
Loss made of: CE 0.5089144706726074, LKD 6.112728118896484, LDE 0.0, LReg 0.0, POD 12.931634902954102 EntMin 0.0
Epoch 3, Class Loss=0.48189982771873474, Reg Loss=5.772438049316406
Clinet index 1, End of Epoch 3/6, Average Loss=6.254337787628174, Class Loss=0.48189982771873474, Reg Loss=5.772438049316406
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=19.33809584081173
Loss made of: CE 0.3627135157585144, LKD 5.668369293212891, LDE 0.0, LReg 0.0, POD 12.372121810913086 EntMin 0.0
Epoch 4, Class Loss=0.44833409786224365, Reg Loss=5.798942565917969
Clinet index 1, End of Epoch 4/6, Average Loss=6.247276782989502, Class Loss=0.44833409786224365, Reg Loss=5.798942565917969
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=18.81368876993656
Loss made of: CE 0.4686501920223236, LKD 5.6466593742370605, LDE 0.0, LReg 0.0, POD 12.415254592895508 EntMin 0.0
Epoch 5, Class Loss=0.4289315342903137, Reg Loss=5.719866752624512
Clinet index 1, End of Epoch 5/6, Average Loss=6.14879846572876, Class Loss=0.4289315342903137, Reg Loss=5.719866752624512
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=18.73652077317238
Loss made of: CE 0.5106209516525269, LKD 5.5792999267578125, LDE 0.0, LReg 0.0, POD 12.319657325744629 EntMin 0.0
Epoch 6, Class Loss=0.43079039454460144, Reg Loss=5.796163082122803
Clinet index 1, End of Epoch 6/6, Average Loss=6.226953506469727, Class Loss=0.43079039454460144, Reg Loss=5.796163082122803
federated aggregation...
Validation, Class Loss=0.9297085404396057, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.780365
Mean Acc: 0.442285
FreqW Acc: 0.696305
Mean IoU: 0.338561
Class IoU:
	class 0: 0.8242924
	class 1: 0.5508799
	class 2: 0.32644135
	class 3: 0.3625822
	class 4: 0.5720733
	class 5: 0.6216948
	class 6: 0.7153265
	class 7: 0.80255705
	class 8: 0.7149663
	class 9: 0.048117377
	class 10: 0.0
	class 11: 0.002461857
	class 12: 0.3777086
	class 13: 0.07021108
	class 14: 0.003607807
	class 15: 0.6860884
	class 16: 0.018260576
	class 17: 0.0
	class 18: 0.0883699
	class 19: 0.3241425
	class 20: 0.0
Class Acc:
	class 0: 0.89175093
	class 1: 0.5559131
	class 2: 0.8899669
	class 3: 0.36430216
	class 4: 0.63484067
	class 5: 0.6620061
	class 6: 0.75422883
	class 7: 0.92618585
	class 8: 0.78300554
	class 9: 0.33714095
	class 10: 0.0
	class 11: 0.002462138
	class 12: 0.7526846
	class 13: 0.07744043
	class 14: 0.0036143286
	class 15: 0.88831824
	class 16: 0.018391004
	class 17: 0.0
	class 18: 0.3775175
	class 19: 0.36821088
	class 20: 0.0

federated global round: 32, step: 6
select part of clients to conduct local training
[3, 8, 27, 7]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=22.085622799396514
Loss made of: CE 0.6790531873703003, LKD 6.065186500549316, LDE 0.0, LReg 0.0, POD 13.708820343017578 EntMin 0.0
Epoch 1, Class Loss=0.7011866569519043, Reg Loss=5.997512340545654
Clinet index 3, End of Epoch 1/6, Average Loss=6.698698997497559, Class Loss=0.7011866569519043, Reg Loss=5.997512340545654
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=19.565947270393373
Loss made of: CE 0.5402223467826843, LKD 5.620892524719238, LDE 0.0, LReg 0.0, POD 13.855939865112305 EntMin 0.0
Epoch 2, Class Loss=0.5101916193962097, Reg Loss=5.914508819580078
Clinet index 3, End of Epoch 2/6, Average Loss=6.4247002601623535, Class Loss=0.5101916193962097, Reg Loss=5.914508819580078
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=19.151856741309167
Loss made of: CE 0.45988816022872925, LKD 5.763060569763184, LDE 0.0, LReg 0.0, POD 14.775030136108398 EntMin 0.0
Epoch 3, Class Loss=0.4589635133743286, Reg Loss=5.927890777587891
Clinet index 3, End of Epoch 3/6, Average Loss=6.38685417175293, Class Loss=0.4589635133743286, Reg Loss=5.927890777587891
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=18.533640223741532
Loss made of: CE 0.40796929597854614, LKD 5.24920654296875, LDE 0.0, LReg 0.0, POD 11.855488777160645 EntMin 0.0
Epoch 4, Class Loss=0.432601660490036, Reg Loss=5.889985084533691
Clinet index 3, End of Epoch 4/6, Average Loss=6.322586536407471, Class Loss=0.432601660490036, Reg Loss=5.889985084533691
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=18.327787429094315
Loss made of: CE 0.35077565908432007, LKD 5.638070583343506, LDE 0.0, LReg 0.0, POD 14.836841583251953 EntMin 0.0
Epoch 5, Class Loss=0.42052900791168213, Reg Loss=5.877998352050781
Clinet index 3, End of Epoch 5/6, Average Loss=6.298527240753174, Class Loss=0.42052900791168213, Reg Loss=5.877998352050781
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=17.808881086111068
Loss made of: CE 0.337179034948349, LKD 5.801064491271973, LDE 0.0, LReg 0.0, POD 10.7200927734375 EntMin 0.0
Epoch 6, Class Loss=0.40131494402885437, Reg Loss=5.874533653259277
Clinet index 3, End of Epoch 6/6, Average Loss=6.275848388671875, Class Loss=0.40131494402885437, Reg Loss=5.874533653259277
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=21.460909631848335
Loss made of: CE 0.3623250722885132, LKD 7.549821853637695, LDE 0.0, LReg 0.0, POD 11.540512084960938 EntMin 0.0
Epoch 1, Class Loss=0.563393235206604, Reg Loss=7.581194877624512
Clinet index 8, End of Epoch 1/6, Average Loss=8.144588470458984, Class Loss=0.563393235206604, Reg Loss=7.581194877624512
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=19.86435536146164
Loss made of: CE 0.49315139651298523, LKD 7.363713264465332, LDE 0.0, LReg 0.0, POD 10.50136947631836 EntMin 0.0
Epoch 2, Class Loss=0.4973559081554413, Reg Loss=7.574952602386475
Clinet index 8, End of Epoch 2/6, Average Loss=8.072308540344238, Class Loss=0.4973559081554413, Reg Loss=7.574952602386475
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=19.410587733983995
Loss made of: CE 0.4705497920513153, LKD 7.334545135498047, LDE 0.0, LReg 0.0, POD 10.118058204650879 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.44010573625564575, Reg Loss=7.58859395980835
Clinet index 8, End of Epoch 3/6, Average Loss=8.02869987487793, Class Loss=0.44010573625564575, Reg Loss=7.58859395980835
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=18.907150548696517
Loss made of: CE 0.5479792356491089, LKD 7.340202331542969, LDE 0.0, LReg 0.0, POD 9.802406311035156 EntMin 0.0
Epoch 4, Class Loss=0.4169735014438629, Reg Loss=7.569172382354736
Clinet index 8, End of Epoch 4/6, Average Loss=7.986145973205566, Class Loss=0.4169735014438629, Reg Loss=7.569172382354736
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=18.598926723003387
Loss made of: CE 0.3198777735233307, LKD 7.858170986175537, LDE 0.0, LReg 0.0, POD 11.61744499206543 EntMin 0.0
Epoch 5, Class Loss=0.40607383847236633, Reg Loss=7.519789218902588
Clinet index 8, End of Epoch 5/6, Average Loss=7.925863265991211, Class Loss=0.40607383847236633, Reg Loss=7.519789218902588
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=17.970763686299325
Loss made of: CE 0.4646633267402649, LKD 7.265799522399902, LDE 0.0, LReg 0.0, POD 11.02180004119873 EntMin 0.0
Epoch 6, Class Loss=0.3752521574497223, Reg Loss=7.525951862335205
Clinet index 8, End of Epoch 6/6, Average Loss=7.9012041091918945, Class Loss=0.3752521574497223, Reg Loss=7.525951862335205
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=21.80665802061558
Loss made of: CE 0.5570549964904785, LKD 7.601332664489746, LDE 0.0, LReg 0.0, POD 10.824033737182617 EntMin 0.0
Epoch 1, Class Loss=0.5980859398841858, Reg Loss=7.64561128616333
Clinet index 27, End of Epoch 1/6, Average Loss=8.243697166442871, Class Loss=0.5980859398841858, Reg Loss=7.64561128616333
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=19.804139563441275
Loss made of: CE 0.7163154482841492, LKD 7.107623100280762, LDE 0.0, LReg 0.0, POD 13.701580047607422 EntMin 0.0
Epoch 2, Class Loss=0.5224002003669739, Reg Loss=7.568809986114502
Clinet index 27, End of Epoch 2/6, Average Loss=8.09121036529541, Class Loss=0.5224002003669739, Reg Loss=7.568809986114502
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=19.529666724801064
Loss made of: CE 0.4292937219142914, LKD 7.586307048797607, LDE 0.0, LReg 0.0, POD 11.862871170043945 EntMin 0.0
Epoch 3, Class Loss=0.4543134868144989, Reg Loss=7.5754475593566895
Clinet index 27, End of Epoch 3/6, Average Loss=8.02976131439209, Class Loss=0.4543134868144989, Reg Loss=7.5754475593566895
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=19.053366059064864
Loss made of: CE 0.38363832235336304, LKD 7.877772808074951, LDE 0.0, LReg 0.0, POD 9.896383285522461 EntMin 0.0
Epoch 4, Class Loss=0.42841362953186035, Reg Loss=7.582607746124268
Clinet index 27, End of Epoch 4/6, Average Loss=8.011021614074707, Class Loss=0.42841362953186035, Reg Loss=7.582607746124268
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=18.756305888295174
Loss made of: CE 0.43977636098861694, LKD 7.691033840179443, LDE 0.0, LReg 0.0, POD 10.050411224365234 EntMin 0.0
Epoch 5, Class Loss=0.3968949019908905, Reg Loss=7.59122371673584
Clinet index 27, End of Epoch 5/6, Average Loss=7.988118648529053, Class Loss=0.3968949019908905, Reg Loss=7.59122371673584
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=18.47124924361706
Loss made of: CE 0.39946073293685913, LKD 7.729590892791748, LDE 0.0, LReg 0.0, POD 9.97674560546875 EntMin 0.0
Epoch 6, Class Loss=0.39934393763542175, Reg Loss=7.580841541290283
Clinet index 27, End of Epoch 6/6, Average Loss=7.980185508728027, Class Loss=0.39934393763542175, Reg Loss=7.580841541290283
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=22.507848912477492
Loss made of: CE 0.6762144565582275, LKD 7.644002437591553, LDE 0.0, LReg 0.0, POD 13.540797233581543 EntMin 0.0
Epoch 1, Class Loss=0.5989956259727478, Reg Loss=7.720149993896484
Clinet index 7, End of Epoch 1/6, Average Loss=8.319145202636719, Class Loss=0.5989956259727478, Reg Loss=7.720149993896484
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=20.098215025663375
Loss made of: CE 0.5919729471206665, LKD 7.438033580780029, LDE 0.0, LReg 0.0, POD 10.333457946777344 EntMin 0.0
Epoch 2, Class Loss=0.5262929797172546, Reg Loss=7.744490146636963
Clinet index 7, End of Epoch 2/6, Average Loss=8.270783424377441, Class Loss=0.5262929797172546, Reg Loss=7.744490146636963
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=19.34084802865982
Loss made of: CE 0.40032824873924255, LKD 8.056490898132324, LDE 0.0, LReg 0.0, POD 9.585521697998047 EntMin 0.0
Epoch 3, Class Loss=0.46986985206604004, Reg Loss=7.682046413421631
Clinet index 7, End of Epoch 3/6, Average Loss=8.15191650390625, Class Loss=0.46986985206604004, Reg Loss=7.682046413421631
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=19.282732805609704
Loss made of: CE 0.3507601022720337, LKD 7.2584943771362305, LDE 0.0, LReg 0.0, POD 10.549046516418457 EntMin 0.0
Epoch 4, Class Loss=0.4312871992588043, Reg Loss=7.6174821853637695
Clinet index 7, End of Epoch 4/6, Average Loss=8.048768997192383, Class Loss=0.4312871992588043, Reg Loss=7.6174821853637695
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=18.759922900795935
Loss made of: CE 0.4565856158733368, LKD 7.555198669433594, LDE 0.0, LReg 0.0, POD 10.458015441894531 EntMin 0.0
Epoch 5, Class Loss=0.41320663690567017, Reg Loss=7.613043308258057
Clinet index 7, End of Epoch 5/6, Average Loss=8.026249885559082, Class Loss=0.41320663690567017, Reg Loss=7.613043308258057
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=18.196129420399664
Loss made of: CE 0.3576647937297821, LKD 7.926548957824707, LDE 0.0, LReg 0.0, POD 9.7415771484375 EntMin 0.0
Epoch 6, Class Loss=0.39487338066101074, Reg Loss=7.586324691772461
Clinet index 7, End of Epoch 6/6, Average Loss=7.981198310852051, Class Loss=0.39487338066101074, Reg Loss=7.586324691772461
federated aggregation...
Validation, Class Loss=0.9827905297279358, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.766584
Mean Acc: 0.450986
FreqW Acc: 0.684093
Mean IoU: 0.337430
Class IoU:
	class 0: 0.81135285
	class 1: 0.5973277
	class 2: 0.32132214
	class 3: 0.41167086
	class 4: 0.59659016
	class 5: 0.65227073
	class 6: 0.71570045
	class 7: 0.79775065
	class 8: 0.7256982
	class 9: 0.051946186
	class 10: 0.0
	class 11: 0.0033551503
	class 12: 0.36008185
	class 13: 0.044638038
	class 14: 0.0029421875
	class 15: 0.66778785
	class 16: 0.0317383
	class 17: 0.0
	class 18: 0.089206405
	class 19: 0.12957989
	class 20: 0.07506268
Class Acc:
	class 0: 0.8691592
	class 1: 0.6043004
	class 2: 0.89362437
	class 3: 0.41498452
	class 4: 0.65352196
	class 5: 0.7048599
	class 6: 0.7637196
	class 7: 0.9363906
	class 8: 0.8131314
	class 9: 0.4095992
	class 10: 0.0
	class 11: 0.0033555382
	class 12: 0.7927334
	class 13: 0.048161604
	class 14: 0.0029471645
	class 15: 0.9059622
	class 16: 0.03208408
	class 17: 0.0
	class 18: 0.40632752
	class 19: 0.13287775
	class 20: 0.08296137

federated global round: 33, step: 6
select part of clients to conduct local training
[6, 20, 13, 10]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=19.004636019468307
Loss made of: CE 0.40179139375686646, LKD 7.584883689880371, LDE 0.0, LReg 0.0, POD 9.870868682861328 EntMin 0.0
Epoch 1, Class Loss=0.44232791662216187, Reg Loss=7.612508773803711
Clinet index 6, End of Epoch 1/6, Average Loss=8.05483627319336, Class Loss=0.44232791662216187, Reg Loss=7.612508773803711
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=18.67707179784775
Loss made of: CE 0.31487464904785156, LKD 7.510533332824707, LDE 0.0, LReg 0.0, POD 9.669234275817871 EntMin 0.0
Epoch 2, Class Loss=0.4108358323574066, Reg Loss=7.550230979919434
Clinet index 6, End of Epoch 2/6, Average Loss=7.961066722869873, Class Loss=0.4108358323574066, Reg Loss=7.550230979919434
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=18.75927926301956
Loss made of: CE 0.39919814467430115, LKD 7.5392656326293945, LDE 0.0, LReg 0.0, POD 10.161559104919434 EntMin 0.0
Epoch 3, Class Loss=0.40268102288246155, Reg Loss=7.631161212921143
Clinet index 6, End of Epoch 3/6, Average Loss=8.033842086791992, Class Loss=0.40268102288246155, Reg Loss=7.631161212921143
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=18.768948027491568
Loss made of: CE 0.402527779340744, LKD 7.580613136291504, LDE 0.0, LReg 0.0, POD 10.78282356262207 EntMin 0.0
Epoch 4, Class Loss=0.38256049156188965, Reg Loss=7.576785087585449
Clinet index 6, End of Epoch 4/6, Average Loss=7.959345817565918, Class Loss=0.38256049156188965, Reg Loss=7.576785087585449
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=18.19699536561966
Loss made of: CE 0.31754666566848755, LKD 7.397162914276123, LDE 0.0, LReg 0.0, POD 12.349691390991211 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Class Loss=0.3702617287635803, Reg Loss=7.594484329223633
Clinet index 6, End of Epoch 5/6, Average Loss=7.964745998382568, Class Loss=0.3702617287635803, Reg Loss=7.594484329223633
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=17.506150701642035
Loss made of: CE 0.3296883702278137, LKD 7.548144817352295, LDE 0.0, LReg 0.0, POD 8.461797714233398 EntMin 0.0
Epoch 6, Class Loss=0.36813852190971375, Reg Loss=7.581971645355225
Clinet index 6, End of Epoch 6/6, Average Loss=7.950109958648682, Class Loss=0.36813852190971375, Reg Loss=7.581971645355225
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=18.46283002793789
Loss made of: CE 0.3670665919780731, LKD 8.094168663024902, LDE 0.0, LReg 0.0, POD 10.225687026977539 EntMin 0.0
Epoch 1, Class Loss=0.4113004505634308, Reg Loss=7.6595354080200195
Clinet index 20, End of Epoch 1/6, Average Loss=8.070836067199707, Class Loss=0.4113004505634308, Reg Loss=7.6595354080200195
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=18.745583564043045
Loss made of: CE 0.4269142746925354, LKD 7.88120174407959, LDE 0.0, LReg 0.0, POD 10.068124771118164 EntMin 0.0
Epoch 2, Class Loss=0.3928813934326172, Reg Loss=7.614239692687988
Clinet index 20, End of Epoch 2/6, Average Loss=8.007121086120605, Class Loss=0.3928813934326172, Reg Loss=7.614239692687988
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=18.208384558558464
Loss made of: CE 0.3204406797885895, LKD 7.511495113372803, LDE 0.0, LReg 0.0, POD 8.961359977722168 EntMin 0.0
Epoch 3, Class Loss=0.37850630283355713, Reg Loss=7.581748008728027
Clinet index 20, End of Epoch 3/6, Average Loss=7.960254192352295, Class Loss=0.37850630283355713, Reg Loss=7.581748008728027
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=18.25067943930626
Loss made of: CE 0.3751116693019867, LKD 7.651589870452881, LDE 0.0, LReg 0.0, POD 10.662089347839355 EntMin 0.0
Epoch 4, Class Loss=0.36051321029663086, Reg Loss=7.585199356079102
Clinet index 20, End of Epoch 4/6, Average Loss=7.945712566375732, Class Loss=0.36051321029663086, Reg Loss=7.585199356079102
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 10/13, Loss=18.18691474199295
Loss made of: CE 0.3450859487056732, LKD 6.704869747161865, LDE 0.0, LReg 0.0, POD 9.651195526123047 EntMin 0.0
Epoch 5, Class Loss=0.3598588705062866, Reg Loss=7.663366317749023
Clinet index 20, End of Epoch 5/6, Average Loss=8.023224830627441, Class Loss=0.3598588705062866, Reg Loss=7.663366317749023
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=17.76742462515831
Loss made of: CE 0.2904019057750702, LKD 7.716479301452637, LDE 0.0, LReg 0.0, POD 8.854548454284668 EntMin 0.0
Epoch 6, Class Loss=0.33844512701034546, Reg Loss=7.550667762756348
Clinet index 20, End of Epoch 6/6, Average Loss=7.889112949371338, Class Loss=0.33844512701034546, Reg Loss=7.550667762756348
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=25.095997399091722
Loss made of: CE 0.7861018180847168, LKD 6.2528076171875, LDE 0.0, LReg 0.0, POD 14.319870948791504 EntMin 0.0
Epoch 1, Class Loss=0.8565729856491089, Reg Loss=6.015158653259277
Clinet index 13, End of Epoch 1/6, Average Loss=6.871731758117676, Class Loss=0.8565729856491089, Reg Loss=6.015158653259277
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=20.439249956607817
Loss made of: CE 0.6159623861312866, LKD 6.040736198425293, LDE 0.0, LReg 0.0, POD 15.070680618286133 EntMin 0.0
Epoch 2, Class Loss=0.5858554244041443, Reg Loss=5.729252815246582
Clinet index 13, End of Epoch 2/6, Average Loss=6.315108299255371, Class Loss=0.5858554244041443, Reg Loss=5.729252815246582
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=18.95026373565197
Loss made of: CE 0.4394840598106384, LKD 6.092519283294678, LDE 0.0, LReg 0.0, POD 12.051668167114258 EntMin 0.0
Epoch 3, Class Loss=0.45817095041275024, Reg Loss=5.772457122802734
Clinet index 13, End of Epoch 3/6, Average Loss=6.23062801361084, Class Loss=0.45817095041275024, Reg Loss=5.772457122802734
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=19.082880303263664
Loss made of: CE 0.4590383470058441, LKD 6.149678707122803, LDE 0.0, LReg 0.0, POD 13.110383033752441 EntMin 0.0
Epoch 4, Class Loss=0.4193034768104553, Reg Loss=5.758910179138184
Clinet index 13, End of Epoch 4/6, Average Loss=6.178213596343994, Class Loss=0.4193034768104553, Reg Loss=5.758910179138184
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=18.156220027804373
Loss made of: CE 0.38685259222984314, LKD 5.429959774017334, LDE 0.0, LReg 0.0, POD 11.969615936279297 EntMin 0.0
Epoch 5, Class Loss=0.4105182886123657, Reg Loss=5.744873046875
Clinet index 13, End of Epoch 5/6, Average Loss=6.155391216278076, Class Loss=0.4105182886123657, Reg Loss=5.744873046875
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=18.0674941688776
Loss made of: CE 0.41772907972335815, LKD 5.2563347816467285, LDE 0.0, LReg 0.0, POD 13.6284761428833 EntMin 0.0
Epoch 6, Class Loss=0.3932366371154785, Reg Loss=5.7344183921813965
Clinet index 13, End of Epoch 6/6, Average Loss=6.127655029296875, Class Loss=0.3932366371154785, Reg Loss=5.7344183921813965
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=25.766800117492675
Loss made of: CE 0.9862792491912842, LKD 6.576685905456543, LDE 0.0, LReg 0.0, POD 15.133362770080566 EntMin 0.0
Epoch 1, Class Loss=0.8722027540206909, Reg Loss=6.093961238861084
Clinet index 10, End of Epoch 1/6, Average Loss=6.9661641120910645, Class Loss=0.8722027540206909, Reg Loss=6.093961238861084
Pseudo labeling is: None
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/12, Loss=20.56242263317108
Loss made of: CE 0.821616530418396, LKD 6.105395317077637, LDE 0.0, LReg 0.0, POD 14.079577445983887 EntMin 0.0
Epoch 2, Class Loss=0.6389633417129517, Reg Loss=5.824761867523193
Clinet index 10, End of Epoch 2/6, Average Loss=6.4637250900268555, Class Loss=0.6389633417129517, Reg Loss=5.824761867523193
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=19.484782564640046
Loss made of: CE 0.5812311172485352, LKD 5.944713592529297, LDE 0.0, LReg 0.0, POD 13.047870635986328 EntMin 0.0
Epoch 3, Class Loss=0.49794667959213257, Reg Loss=5.822432994842529
Clinet index 10, End of Epoch 3/6, Average Loss=6.320379734039307, Class Loss=0.49794667959213257, Reg Loss=5.822432994842529
Pseudo labeling is: None
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/12, Loss=18.89637935757637
Loss made of: CE 0.37809500098228455, LKD 5.46149206161499, LDE 0.0, LReg 0.0, POD 11.596569061279297 EntMin 0.0
Epoch 4, Class Loss=0.44586676359176636, Reg Loss=5.810320854187012
Clinet index 10, End of Epoch 4/6, Average Loss=6.256187438964844, Class Loss=0.44586676359176636, Reg Loss=5.810320854187012
Pseudo labeling is: None
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/12, Loss=18.133627101778984
Loss made of: CE 0.4051521420478821, LKD 5.591445446014404, LDE 0.0, LReg 0.0, POD 11.843770980834961 EntMin 0.0
Epoch 5, Class Loss=0.4107007384300232, Reg Loss=5.782197952270508
Clinet index 10, End of Epoch 5/6, Average Loss=6.192898750305176, Class Loss=0.4107007384300232, Reg Loss=5.782197952270508
Pseudo labeling is: None
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/12, Loss=18.28734886050224
Loss made of: CE 0.37761521339416504, LKD 5.749880790710449, LDE 0.0, LReg 0.0, POD 12.946813583374023 EntMin 0.0
Epoch 6, Class Loss=0.4178234040737152, Reg Loss=5.84535551071167
Clinet index 10, End of Epoch 6/6, Average Loss=6.263178825378418, Class Loss=0.4178234040737152, Reg Loss=5.84535551071167
federated aggregation...
Validation, Class Loss=0.934873104095459, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.779907
Mean Acc: 0.455239
FreqW Acc: 0.699418
Mean IoU: 0.345918
Class IoU:
	class 0: 0.8256906
	class 1: 0.56410044
	class 2: 0.3180258
	class 3: 0.371354
	class 4: 0.57344604
	class 5: 0.65016466
	class 6: 0.71716624
	class 7: 0.81292456
	class 8: 0.71511483
	class 9: 0.04923666
	class 10: 0.0
	class 11: 0.0028601026
	class 12: 0.36794844
	class 13: 0.056458637
	class 14: 0.002179057
	class 15: 0.68823576
	class 16: 0.024851548
	class 17: 0.0
	class 18: 0.09196054
	class 19: 0.393217
	class 20: 0.03933338
Class Acc:
	class 0: 0.88478446
	class 1: 0.569366
	class 2: 0.8994714
	class 3: 0.37344635
	class 4: 0.62352777
	class 5: 0.69949865
	class 6: 0.7430669
	class 7: 0.9303112
	class 8: 0.80753773
	class 9: 0.3614016
	class 10: 0.0
	class 11: 0.002860432
	class 12: 0.76730496
	class 13: 0.06096956
	class 14: 0.002181105
	class 15: 0.8875407
	class 16: 0.02513713
	class 17: 0.0
	class 18: 0.39224494
	class 19: 0.4898514
	class 20: 0.039524205

federated global round: 34, step: 6
select part of clients to conduct local training
[19, 18, 27, 14]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=21.036226019263268
Loss made of: CE 0.42929792404174805, LKD 7.744279861450195, LDE 0.0, LReg 0.0, POD 10.052043914794922 EntMin 0.0
Epoch 1, Class Loss=0.4881550073623657, Reg Loss=7.60142183303833
Clinet index 19, End of Epoch 1/6, Average Loss=8.089576721191406, Class Loss=0.4881550073623657, Reg Loss=7.60142183303833
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=19.413407006859778
Loss made of: CE 0.5184274911880493, LKD 7.823232173919678, LDE 0.0, LReg 0.0, POD 11.617189407348633 EntMin 0.0
Epoch 2, Class Loss=0.41535115242004395, Reg Loss=7.564548015594482
Clinet index 19, End of Epoch 2/6, Average Loss=7.9798994064331055, Class Loss=0.41535115242004395, Reg Loss=7.564548015594482
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=18.406402492523192
Loss made of: CE 0.388716459274292, LKD 7.504149913787842, LDE 0.0, LReg 0.0, POD 11.019702911376953 EntMin 0.0
Epoch 3, Class Loss=0.40152794122695923, Reg Loss=7.49905252456665
Clinet index 19, End of Epoch 3/6, Average Loss=7.900580406188965, Class Loss=0.40152794122695923, Reg Loss=7.49905252456665
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=17.977876046299933
Loss made of: CE 0.4011446237564087, LKD 8.048369407653809, LDE 0.0, LReg 0.0, POD 10.505108833312988 EntMin 0.0
Epoch 4, Class Loss=0.3872438967227936, Reg Loss=7.588987350463867
Clinet index 19, End of Epoch 4/6, Average Loss=7.976231098175049, Class Loss=0.3872438967227936, Reg Loss=7.588987350463867
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=18.061263686418535
Loss made of: CE 0.33848270773887634, LKD 7.761277675628662, LDE 0.0, LReg 0.0, POD 10.054048538208008 EntMin 0.0
Epoch 5, Class Loss=0.38414859771728516, Reg Loss=7.525058746337891
Clinet index 19, End of Epoch 5/6, Average Loss=7.909207344055176, Class Loss=0.38414859771728516, Reg Loss=7.525058746337891
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=17.71524280011654
Loss made of: CE 0.27479952573776245, LKD 7.741849422454834, LDE 0.0, LReg 0.0, POD 8.833334922790527 EntMin 0.0
Epoch 6, Class Loss=0.3725990056991577, Reg Loss=7.510460376739502
Clinet index 19, End of Epoch 6/6, Average Loss=7.883059501647949, Class Loss=0.3725990056991577, Reg Loss=7.510460376739502
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=20.63105519413948
Loss made of: CE 0.4847491681575775, LKD 6.992959499359131, LDE 0.0, LReg 0.0, POD 12.709213256835938 EntMin 0.0
Epoch 1, Class Loss=0.43793755769729614, Reg Loss=7.7187395095825195
Clinet index 18, End of Epoch 1/6, Average Loss=8.15667724609375, Class Loss=0.43793755769729614, Reg Loss=7.7187395095825195
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=18.674278324842454
Loss made of: CE 0.38712796568870544, LKD 7.4905195236206055, LDE 0.0, LReg 0.0, POD 9.553757667541504 EntMin 0.0
Epoch 2, Class Loss=0.399262011051178, Reg Loss=7.668702125549316
Clinet index 18, End of Epoch 2/6, Average Loss=8.067964553833008, Class Loss=0.399262011051178, Reg Loss=7.668702125549316
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=18.50205943286419
Loss made of: CE 0.3409413695335388, LKD 7.8007001876831055, LDE 0.0, LReg 0.0, POD 9.206526756286621 EntMin 0.0
Epoch 3, Class Loss=0.3793756663799286, Reg Loss=7.669327735900879
Clinet index 18, End of Epoch 3/6, Average Loss=8.04870319366455, Class Loss=0.3793756663799286, Reg Loss=7.669327735900879
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=18.109537279605867
Loss made of: CE 0.39395999908447266, LKD 7.5946502685546875, LDE 0.0, LReg 0.0, POD 8.970222473144531 EntMin 0.0
Epoch 4, Class Loss=0.3733217716217041, Reg Loss=7.719316482543945
Clinet index 18, End of Epoch 4/6, Average Loss=8.09263801574707, Class Loss=0.3733217716217041, Reg Loss=7.719316482543945
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=17.829482251405715
Loss made of: CE 0.42187005281448364, LKD 7.732137203216553, LDE 0.0, LReg 0.0, POD 11.431844711303711 EntMin 0.0
Epoch 5, Class Loss=0.35595670342445374, Reg Loss=7.623134613037109
Clinet index 18, End of Epoch 5/6, Average Loss=7.979091167449951, Class Loss=0.35595670342445374, Reg Loss=7.623134613037109
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=17.553159779310228
Loss made of: CE 0.30843162536621094, LKD 7.550045013427734, LDE 0.0, LReg 0.0, POD 8.484964370727539 EntMin 0.0
Epoch 6, Class Loss=0.36044442653656006, Reg Loss=7.665593147277832
Clinet index 18, End of Epoch 6/6, Average Loss=8.026037216186523, Class Loss=0.36044442653656006, Reg Loss=7.665593147277832
Current Client Index:  27
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=20.79890237748623
Loss made of: CE 0.40356382727622986, LKD 7.550929069519043, LDE 0.0, LReg 0.0, POD 10.140636444091797 EntMin 0.0
Epoch 1, Class Loss=0.47800329327583313, Reg Loss=7.668473243713379
Clinet index 27, End of Epoch 1/6, Average Loss=8.146476745605469, Class Loss=0.47800329327583313, Reg Loss=7.668473243713379
Pseudo labeling is: None
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/13, Loss=18.528304833173753
Loss made of: CE 0.6128431558609009, LKD 6.910056114196777, LDE 0.0, LReg 0.0, POD 11.750520706176758 EntMin 0.0
Epoch 2, Class Loss=0.43789783120155334, Reg Loss=7.596402168273926
Clinet index 27, End of Epoch 2/6, Average Loss=8.034299850463867, Class Loss=0.43789783120155334, Reg Loss=7.596402168273926
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/13, Loss=18.004056990146637
Loss made of: CE 0.3855607509613037, LKD 7.506085395812988, LDE 0.0, LReg 0.0, POD 10.034748077392578 EntMin 0.0
Epoch 3, Class Loss=0.412859708070755, Reg Loss=7.633427619934082
Clinet index 27, End of Epoch 3/6, Average Loss=8.046287536621094, Class Loss=0.412859708070755, Reg Loss=7.633427619934082
Pseudo labeling is: None
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/13, Loss=17.504453107714653
Loss made of: CE 0.37387022376060486, LKD 7.987666130065918, LDE 0.0, LReg 0.0, POD 8.443008422851562 EntMin 0.0
Epoch 4, Class Loss=0.3991944491863251, Reg Loss=7.586296558380127
Clinet index 27, End of Epoch 4/6, Average Loss=7.985490798950195, Class Loss=0.3991944491863251, Reg Loss=7.586296558380127
Pseudo labeling is: None
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/13, Loss=17.399127420783042
Loss made of: CE 0.40153002738952637, LKD 7.581053733825684, LDE 0.0, LReg 0.0, POD 8.363248825073242 EntMin 0.0
Epoch 5, Class Loss=0.3870398998260498, Reg Loss=7.610517501831055
Clinet index 27, End of Epoch 5/6, Average Loss=7.997557640075684, Class Loss=0.3870398998260498, Reg Loss=7.610517501831055
Pseudo labeling is: None
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/13, Loss=17.09443852007389
Loss made of: CE 0.3751083314418793, LKD 7.802464962005615, LDE 0.0, LReg 0.0, POD 8.421041488647461 EntMin 0.0
Epoch 6, Class Loss=0.385196328163147, Reg Loss=7.574967861175537
Clinet index 27, End of Epoch 6/6, Average Loss=7.9601640701293945, Class Loss=0.385196328163147, Reg Loss=7.574967861175537
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=20.399424338340758
Loss made of: CE 0.44717758893966675, LKD 7.795695781707764, LDE 0.0, LReg 0.0, POD 10.668127059936523 EntMin 0.0
Epoch 1, Class Loss=0.43863070011138916, Reg Loss=7.632009983062744
Clinet index 14, End of Epoch 1/6, Average Loss=8.070640563964844, Class Loss=0.43863070011138916, Reg Loss=7.632009983062744
Pseudo labeling is: None
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/13, Loss=18.569994202256204
Loss made of: CE 0.31898751854896545, LKD 7.7168474197387695, LDE 0.0, LReg 0.0, POD 10.941591262817383 EntMin 0.0
Epoch 2, Class Loss=0.40756192803382874, Reg Loss=7.571943283081055
Clinet index 14, End of Epoch 2/6, Average Loss=7.9795050621032715, Class Loss=0.40756192803382874, Reg Loss=7.571943283081055
Pseudo labeling is: None
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/13, Loss=18.02720640897751
Loss made of: CE 0.3287460505962372, LKD 8.157419204711914, LDE 0.0, LReg 0.0, POD 9.613426208496094 EntMin 0.0
Epoch 3, Class Loss=0.3869878947734833, Reg Loss=7.563810348510742
Clinet index 14, End of Epoch 3/6, Average Loss=7.950798034667969, Class Loss=0.3869878947734833, Reg Loss=7.563810348510742
Pseudo labeling is: None
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/13, Loss=17.464218735694885
Loss made of: CE 0.2852194011211395, LKD 7.228074550628662, LDE 0.0, LReg 0.0, POD 9.698349952697754 EntMin 0.0
Epoch 4, Class Loss=0.37460482120513916, Reg Loss=7.506669998168945
Clinet index 14, End of Epoch 4/6, Average Loss=7.881274700164795, Class Loss=0.37460482120513916, Reg Loss=7.506669998168945
Pseudo labeling is: None
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/13, Loss=17.371926781535148
Loss made of: CE 0.322181761264801, LKD 7.354558944702148, LDE 0.0, LReg 0.0, POD 8.74087142944336 EntMin 0.0
Epoch 5, Class Loss=0.3626282215118408, Reg Loss=7.537775993347168
Clinet index 14, End of Epoch 5/6, Average Loss=7.90040397644043, Class Loss=0.3626282215118408, Reg Loss=7.537775993347168
Pseudo labeling is: None
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/13, Loss=17.389660063385964
Loss made of: CE 0.4975876808166504, LKD 7.209786891937256, LDE 0.0, LReg 0.0, POD 10.907707214355469 EntMin 0.0
Epoch 6, Class Loss=0.3670535683631897, Reg Loss=7.553971767425537
Clinet index 14, End of Epoch 6/6, Average Loss=7.921025276184082, Class Loss=0.3670535683631897, Reg Loss=7.553971767425537
federated aggregation...
Validation, Class Loss=1.0169669389724731, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.759294
Mean Acc: 0.472115
FreqW Acc: 0.681927
Mean IoU: 0.336748
Class IoU:
	class 0: 0.81106275
	class 1: 0.5921624
	class 2: 0.31937376
	class 3: 0.430495
	class 4: 0.60260695
	class 5: 0.64329565
	class 6: 0.63513607
	class 7: 0.81514204
	class 8: 0.74210703
	class 9: 0.055572376
	class 10: 0.0
	class 11: 0.0023147282
	class 12: 0.35067835
	class 13: 0.03958865
	class 14: 0.0023853788
	class 15: 0.6621698
	class 16: 0.026274456
	class 17: 0.0
	class 18: 0.08990887
	class 19: 0.05843094
	class 20: 0.19300097
Class Acc:
	class 0: 0.8544236
	class 1: 0.59876543
	class 2: 0.8909739
	class 3: 0.43568742
	class 4: 0.656167
	class 5: 0.6990644
	class 6: 0.6587814
	class 7: 0.9332103
	class 8: 0.86819524
	class 9: 0.4377279
	class 10: 0.0
	class 11: 0.0023148046
	class 12: 0.8067368
	class 13: 0.04229848
	class 14: 0.0023882007
	class 15: 0.9073801
	class 16: 0.026473282
	class 17: 0.0
	class 18: 0.40212542
	class 19: 0.059374873
	class 20: 0.63232386

voc_8-2_RCIL On GPUs 2
Run in 134274s
