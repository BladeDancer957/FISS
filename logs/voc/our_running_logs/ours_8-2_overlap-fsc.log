nohup: ignoring input
35
kvoc_8-2_OURS-FSC On GPUs 0\Writing in results/seed_2023-ov/2023-03-20_voc_8-2_OURS-FSC.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[4, 3, 1, 2]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[0, 4, 7, 2]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  4
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[1, 9, 3, 8]
Current Client Index:  1
Current Client Index:  9
Current Client Index:  3
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[5, 9, 0, 4]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Current Client Index:  0
Current Client Index:  4
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.11884414404630661, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.959771
Mean Acc: 0.898867
FreqW Acc: 0.927997
Mean IoU: 0.803600
Class IoU:
	class 0: 0.9505860659032771
	class 1: 0.9002121543757099
	class 2: 0.3911996045853578
	class 3: 0.7886755539422873
	class 4: 0.7227948650191602
	class 5: 0.777020491034379
	class 6: 0.9452273228283382
	class 7: 0.8680230685934628
	class 8: 0.8886579855388657
Class Acc:
	class 0: 0.9742031520387592
	class 1: 0.9505188705401268
	class 2: 0.8333169681087177
	class 3: 0.7954920085182839
	class 4: 0.8649138231019398
	class 5: 0.839701567241524
	class 6: 0.9766569231687022
	class 7: 0.9440467027150332
	class 8: 0.910953836504121

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[5, 11, 7, 1]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/27, Loss=10.209232354164124
Loss made of: CE 0.6810283660888672, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.773332595825195 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.725074890255929
Loss made of: CE 0.3011434078216553, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.704000473022461 EntMin 0.0
Epoch 1, Class Loss=0.6237346529960632, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.6237346529960632, Class Loss=0.6237346529960632, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.518242993950844
Loss made of: CE 0.4168548583984375, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1420488357543945 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.033149847388268
Loss made of: CE 0.35636991262435913, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.379183769226074 EntMin 0.0
Epoch 2, Class Loss=0.4287678301334381, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.4287678301334381, Class Loss=0.4287678301334381, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.803538742661476
Loss made of: CE 0.4568840265274048, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.080414772033691 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.4358504712581635
Loss made of: CE 0.34861910343170166, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9786529541015625 EntMin 0.0
Epoch 3, Class Loss=0.40439125895500183, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.40439125895500183, Class Loss=0.40439125895500183, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.42432316839695
Loss made of: CE 0.35396885871887207, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.162269592285156 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.079293513298035
Loss made of: CE 0.3965865969657898, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.584533214569092 EntMin 0.0
Epoch 4, Class Loss=0.39802905917167664, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.39802905917167664, Class Loss=0.39802905917167664, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.1291259080171585
Loss made of: CE 0.32642310857772827, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.855292320251465 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.975541645288468
Loss made of: CE 0.39005908370018005, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4282989501953125 EntMin 0.0
Epoch 5, Class Loss=0.3780849874019623, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.3780849874019623, Class Loss=0.3780849874019623, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=6.1413704544305805
Loss made of: CE 0.39588433504104614, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.548821449279785 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.881476032733917
Loss made of: CE 0.37678810954093933, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.324987411499023 EntMin 0.0
Epoch 6, Class Loss=0.3628561496734619, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.3628561496734619, Class Loss=0.3628561496734619, Reg Loss=0.0
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1307322978973389, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=1.1307322978973389, Class Loss=1.1307322978973389, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.8344869613647461, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.8344869613647461, Class Loss=0.8344869613647461, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6748695373535156, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.6748695373535156, Class Loss=0.6748695373535156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.5899732708930969, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5899732708930969, Class Loss=0.5899732708930969, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.5192806720733643, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5192806720733643, Class Loss=0.5192806720733643, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.47402477264404297, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.47402477264404297, Class Loss=0.47402477264404297, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.154440993070603
Loss made of: CE 0.7238550186157227, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.078470230102539 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.507765057682992
Loss made of: CE 0.522221565246582, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.681706428527832 EntMin 0.0
Epoch 1, Class Loss=0.6176780462265015, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.6176780462265015, Class Loss=0.6176780462265015, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.5779657661914825
Loss made of: CE 0.5728013515472412, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.300084590911865 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.1192690372467045
Loss made of: CE 0.4663161635398865, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0518646240234375 EntMin 0.0
Epoch 2, Class Loss=0.4251180589199066, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.4251180589199066, Class Loss=0.4251180589199066, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.621736207604409
Loss made of: CE 0.4530206620693207, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.165144920349121 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.486707270145416
Loss made of: CE 0.3954261243343353, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.865787506103516 EntMin 0.0
Epoch 3, Class Loss=0.41421523690223694, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.41421523690223694, Class Loss=0.41421523690223694, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.221788981556893
Loss made of: CE 0.3788033425807953, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.943681716918945 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.289652732014656
Loss made of: CE 0.514478325843811, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.814925670623779 EntMin 0.0
Epoch 4, Class Loss=0.4029955267906189, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4029955267906189, Class Loss=0.4029955267906189, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=6.267635291814804
Loss made of: CE 0.4164535701274872, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.409074306488037 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.834567975997925
Loss made of: CE 0.28569191694259644, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.3945722579956055 EntMin 0.0
Epoch 5, Class Loss=0.3851872980594635, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.3851872980594635, Class Loss=0.3851872980594635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=5.921749433875084
Loss made of: CE 0.4089824855327606, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.485330581665039 EntMin 0.0
Epoch 6, Batch 20/27, Loss=6.095475563406945
Loss made of: CE 0.3005727231502533, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.748449325561523 EntMin 0.0
Epoch 6, Class Loss=0.3758378028869629, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.3758378028869629, Class Loss=0.3758378028869629, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=10.020435398817062
Loss made of: CE 0.6448721289634705, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.214950561523438 EntMin 0.0
Epoch 1, Batch 20/27, Loss=8.74130976498127
Loss made of: CE 0.4337114691734314, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.149184226989746 EntMin 0.0
Epoch 1, Class Loss=0.6125096082687378, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.6125096082687378, Class Loss=0.6125096082687378, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/27, Loss=7.369603818655014
Loss made of: CE 0.3600326180458069, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.690756797790527 EntMin 0.0
Epoch 2, Batch 20/27, Loss=7.220941519737243
Loss made of: CE 0.46521735191345215, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.8003973960876465 EntMin 0.0
Epoch 2, Class Loss=0.4175782799720764, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.4175782799720764, Class Loss=0.4175782799720764, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/27, Loss=6.517110142111778
Loss made of: CE 0.4066619873046875, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3996710777282715 EntMin 0.0
Epoch 3, Batch 20/27, Loss=6.452507352828979
Loss made of: CE 0.43356189131736755, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.908466339111328 EntMin 0.0
Epoch 3, Class Loss=0.409101277589798, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.409101277589798, Class Loss=0.409101277589798, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/27, Loss=6.172087335586548
Loss made of: CE 0.4208734929561615, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.8089518547058105 EntMin 0.0
Epoch 4, Batch 20/27, Loss=6.266967681050301
Loss made of: CE 0.3809846341609955, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.831010818481445 EntMin 0.0
Epoch 4, Class Loss=0.39472314715385437, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.39472314715385437, Class Loss=0.39472314715385437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/27, Loss=5.924745896458626
Loss made of: CE 0.39194273948669434, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.518008232116699 EntMin 0.0
Epoch 5, Batch 20/27, Loss=6.047847512364387
Loss made of: CE 0.30970847606658936, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.783046245574951 EntMin 0.0
Epoch 5, Class Loss=0.38778531551361084, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.38778531551361084, Class Loss=0.38778531551361084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/27, Loss=5.801474186778068
Loss made of: CE 0.3558533191680908, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.30336856842041 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.808804470300674
Loss made of: CE 0.4001547694206238, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.332713603973389 EntMin 0.0
Epoch 6, Class Loss=0.3672039210796356, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.3672039210796356, Class Loss=0.3672039210796356, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.32391658425331116, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923459
Mean Acc: 0.739394
FreqW Acc: 0.861069
Mean IoU: 0.638464
Class IoU:
	class 0: 0.9102623
	class 1: 0.8824835
	class 2: 0.33180767
	class 3: 0.76304847
	class 4: 0.7003308
	class 5: 0.7821411
	class 6: 0.92270553
	class 7: 0.8636246
	class 8: 0.86669964
	class 9: 0.0
	class 10: 0.0
Class Acc:
	class 0: 0.9711112
	class 1: 0.9498146
	class 2: 0.8736973
	class 3: 0.7736145
	class 4: 0.8495593
	class 5: 0.89162827
	class 6: 0.9573279
	class 7: 0.92185795
	class 8: 0.94472176
	class 9: 0.0
	class 10: 0.0

federated global round: 6, step: 1
select part of clients to conduct local training
[1, 6, 7, 3]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=6.0647622555494305
Loss made of: CE 0.3316860795021057, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.556683540344238 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.953947830200195
Loss made of: CE 0.40871462225914, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.555970668792725 EntMin 0.0
Epoch 1, Class Loss=0.3947291374206543, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.3947291374206543, Class Loss=0.3947291374206543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=5.666170135140419
Loss made of: CE 0.37786877155303955, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4867072105407715 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.907562816143036
Loss made of: CE 0.3823854327201843, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.521842956542969 EntMin 0.0
Epoch 2, Class Loss=0.38250404596328735, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.38250404596328735, Class Loss=0.38250404596328735, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=5.574254512786865
Loss made of: CE 0.36054927110671997, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.400984764099121 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.62837478518486
Loss made of: CE 0.37465736269950867, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.16187047958374 EntMin 0.0
Epoch 3, Class Loss=0.36492443084716797, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.36492443084716797, Class Loss=0.36492443084716797, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=5.554058587551117
Loss made of: CE 0.375438928604126, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.127196311950684 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.685369610786438
Loss made of: CE 0.33856484293937683, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.450235366821289 EntMin 0.0
Epoch 4, Class Loss=0.3463651239871979, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.3463651239871979, Class Loss=0.3463651239871979, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.391479948163033
Loss made of: CE 0.33704936504364014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.927518367767334 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.544543665647507
Loss made of: CE 0.29024478793144226, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.360081672668457 EntMin 0.0
Epoch 5, Class Loss=0.3334429860115051, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.3334429860115051, Class Loss=0.3334429860115051, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=5.345858171582222
Loss made of: CE 0.3403362035751343, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.974884986877441 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.343219178915024
Loss made of: CE 0.33132320642471313, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.002218246459961 EntMin 0.0
Epoch 6, Class Loss=0.32302239537239075, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.32302239537239075, Class Loss=0.32302239537239075, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.038590100407601
Loss made of: CE 0.3265839219093323, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.369283199310303 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.777735984325409
Loss made of: CE 0.26666259765625, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.576340675354004 EntMin 0.0
Epoch 1, Class Loss=0.35882121324539185, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.35882121324539185, Class Loss=0.35882121324539185, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/27, Loss=5.745433989167213
Loss made of: CE 0.3376663327217102, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.610124111175537 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.848328629136086
Loss made of: CE 0.3624435365200043, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6019392013549805 EntMin 0.0
Epoch 2, Class Loss=0.35635465383529663, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.35635465383529663, Class Loss=0.35635465383529663, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/27, Loss=5.610659828782081
Loss made of: CE 0.37252044677734375, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.244982719421387 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.633858308196068
Loss made of: CE 0.35506361722946167, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.046834945678711 EntMin 0.0
Epoch 3, Class Loss=0.338531494140625, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.338531494140625, Class Loss=0.338531494140625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/27, Loss=5.505668598413467
Loss made of: CE 0.3560875356197357, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.048797607421875 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.530410093069077
Loss made of: CE 0.2904476225376129, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.10120964050293 EntMin 0.0
Epoch 4, Class Loss=0.33038341999053955, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.33038341999053955, Class Loss=0.33038341999053955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/27, Loss=5.555208396911621
Loss made of: CE 0.3249409794807434, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.774362564086914 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.455562464892864
Loss made of: CE 0.2375020980834961, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.120702743530273 EntMin 0.0
Epoch 5, Class Loss=0.3214324712753296, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.3214324712753296, Class Loss=0.3214324712753296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/27, Loss=5.449738922715187
Loss made of: CE 0.2804149389266968, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.130785942077637 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.304795902967453
Loss made of: CE 0.2651479244232178, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5403852462768555 EntMin 0.0
Epoch 6, Class Loss=0.31206658482551575, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.31206658482551575, Class Loss=0.31206658482551575, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=6.157653400301934
Loss made of: CE 0.4209138751029968, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.886233806610107 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.912824124097824
Loss made of: CE 0.4531485438346863, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.452483654022217 EntMin 0.0
Epoch 1, Class Loss=0.39656582474708557, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.39656582474708557, Class Loss=0.39656582474708557, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/27, Loss=5.997053158283234
Loss made of: CE 0.5525826811790466, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.944261074066162 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.822447881102562
Loss made of: CE 0.41662949323654175, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.00691032409668 EntMin 0.0
Epoch 2, Class Loss=0.3899233341217041, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.3899233341217041, Class Loss=0.3899233341217041, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/27, Loss=5.678622207045555
Loss made of: CE 0.40975284576416016, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.397820472717285 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.7177940845489506
Loss made of: CE 0.3754575252532959, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.188488006591797 EntMin 0.0
Epoch 3, Class Loss=0.3723567724227905, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.3723567724227905, Class Loss=0.3723567724227905, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/27, Loss=5.540020668506623
Loss made of: CE 0.3206683099269867, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.112124919891357 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.641305083036423
Loss made of: CE 0.44305214285850525, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.201781749725342 EntMin 0.0
Epoch 4, Class Loss=0.3528522253036499, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.3528522253036499, Class Loss=0.3528522253036499, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.72296938598156
Loss made of: CE 0.3803258538246155, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.916858673095703 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.290441972017288
Loss made of: CE 0.26101750135421753, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.078079700469971 EntMin 0.0
Epoch 5, Class Loss=0.3406524658203125, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.3406524658203125, Class Loss=0.3406524658203125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/27, Loss=5.507702419161797
Loss made of: CE 0.3756811022758484, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.128121376037598 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.7385903120040895
Loss made of: CE 0.2728300094604492, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.442904949188232 EntMin 0.0
Epoch 6, Class Loss=0.33097928762435913, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.33097928762435913, Class Loss=0.33097928762435913, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.0698003768920898, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=1.0698003768920898, Class Loss=1.0698003768920898, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.9922314882278442, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.9922314882278442, Class Loss=0.9922314882278442, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.8272088766098022, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.8272088766098022, Class Loss=0.8272088766098022, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.7123968601226807, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.7123968601226807, Class Loss=0.7123968601226807, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.6234790086746216, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.6234790086746216, Class Loss=0.6234790086746216, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.5088104605674744, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.5088104605674744, Class Loss=0.5088104605674744, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.32001179456710815, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923308
Mean Acc: 0.752684
FreqW Acc: 0.861869
Mean IoU: 0.644538
Class IoU:
	class 0: 0.9100603
	class 1: 0.87631094
	class 2: 0.32749555
	class 3: 0.7909235
	class 4: 0.7036542
	class 5: 0.7815844
	class 6: 0.9195146
	class 7: 0.8659131
	class 8: 0.86375004
	class 9: 0.05070778
	class 10: 0.0
Class Acc:
	class 0: 0.9676874
	class 1: 0.9546565
	class 2: 0.8947274
	class 3: 0.8035755
	class 4: 0.8576509
	class 5: 0.90871716
	class 6: 0.94977856
	class 7: 0.93223584
	class 8: 0.9576656
	class 9: 0.052831996
	class 10: 0.0

federated global round: 7, step: 1
select part of clients to conduct local training
[13, 8, 0, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.0376006364822388, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=1.0376006364822388, Class Loss=1.0376006364822388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.9692429304122925, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.9692429304122925, Class Loss=0.9692429304122925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.8027770519256592, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.8027770519256592, Class Loss=0.8027770519256592, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.6808632612228394, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.6808632612228394, Class Loss=0.6808632612228394, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5536460876464844, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5536460876464844, Class Loss=0.5536460876464844, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.49854135513305664, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.49854135513305664, Class Loss=0.49854135513305664, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.1161160469055176, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.1161160469055176, Class Loss=1.1161160469055176, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.971633791923523, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.971633791923523, Class Loss=0.971633791923523, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.8554889559745789, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.8554889559745789, Class Loss=0.8554889559745789, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.7666191458702087, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.7666191458702087, Class Loss=0.7666191458702087, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5906524658203125, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.5906524658203125, Class Loss=0.5906524658203125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.4887709617614746, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.4887709617614746, Class Loss=0.4887709617614746, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=1.0758262872695923, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=1.0758262872695923, Class Loss=1.0758262872695923, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=1.033504605293274, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=1.033504605293274, Class Loss=1.033504605293274, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.8609894514083862, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.8609894514083862, Class Loss=0.8609894514083862, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.6808081865310669, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.6808081865310669, Class Loss=0.6808081865310669, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.5423219203948975, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.5423219203948975, Class Loss=0.5423219203948975, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.463828444480896, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.463828444480896, Class Loss=0.463828444480896, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/27, Loss=5.789129775762558
Loss made of: CE 0.2944289743900299, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.481683731079102 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.6193772733211516
Loss made of: CE 0.27173295617103577, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.009901523590088 EntMin 0.0
Epoch 1, Class Loss=0.3386323153972626, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.3386323153972626, Class Loss=0.3386323153972626, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/27, Loss=5.440233251452446
Loss made of: CE 0.3083087205886841, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1631059646606445 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.303203892707825
Loss made of: CE 0.2713468074798584, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9211931228637695 EntMin 0.0
Epoch 2, Class Loss=0.3180513381958008, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.3180513381958008, Class Loss=0.3180513381958008, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/27, Loss=5.463838392496109
Loss made of: CE 0.3273603618144989, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.152855396270752 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.257055407762527
Loss made of: CE 0.25645631551742554, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.120793342590332 EntMin 0.0
Epoch 3, Class Loss=0.30372363328933716, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.30372363328933716, Class Loss=0.30372363328933716, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/27, Loss=5.4380502358078955
Loss made of: CE 0.27835604548454285, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.375611782073975 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.333103296160698
Loss made of: CE 0.2969807982444763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.864718437194824 EntMin 0.0
Epoch 4, Class Loss=0.29851436614990234, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.29851436614990234, Class Loss=0.29851436614990234, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/27, Loss=5.4007913261652
Loss made of: CE 0.2704673707485199, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.222049713134766 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.234239156544208
Loss made of: CE 0.27804479002952576, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.770586013793945 EntMin 0.0
Epoch 5, Class Loss=0.2930322289466858, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.2930322289466858, Class Loss=0.2930322289466858, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/27, Loss=5.334301754832268
Loss made of: CE 0.32874155044555664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8667778968811035 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.226440262794495
Loss made of: CE 0.29204222559928894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.771387100219727 EntMin 0.0
Epoch 6, Class Loss=0.28557702898979187, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.28557702898979187, Class Loss=0.28557702898979187, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2900148630142212, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.920646
Mean Acc: 0.734098
FreqW Acc: 0.857496
Mean IoU: 0.631149
Class IoU:
	class 0: 0.90775734
	class 1: 0.87272954
	class 2: 0.32349297
	class 3: 0.74832463
	class 4: 0.6656048
	class 5: 0.7649619
	class 6: 0.898887
	class 7: 0.86453575
	class 8: 0.86430645
	class 9: 0.03203256
	class 10: 2.1595426e-06
Class Acc:
	class 0: 0.9711006
	class 1: 0.9276857
	class 2: 0.91919786
	class 3: 0.75952363
	class 4: 0.8373359
	class 5: 0.8673336
	class 6: 0.9183579
	class 7: 0.91436106
	class 8: 0.9267702
	class 9: 0.033415627
	class 10: 2.1595426e-06

federated global round: 8, step: 1
select part of clients to conduct local training
[12, 9, 4, 13]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.5876475214958194
Loss made of: CE 0.4320346713066101, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5236921310424805 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.533587040007115
Loss made of: CE 0.2972927689552307, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.020427703857422 EntMin 0.0
Epoch 1, Class Loss=0.32790490984916687, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.32790490984916687, Class Loss=0.32790490984916687, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.346682900190354
Loss made of: CE 0.3090236783027649, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1252665519714355 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.510403856635094
Loss made of: CE 0.349923312664032, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.345969200134277 EntMin 0.0
Epoch 2, Class Loss=0.2947137653827667, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.2947137653827667, Class Loss=0.2947137653827667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=5.425334697961807
Loss made of: CE 0.2754639983177185, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8670854568481445 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.447502553462982
Loss made of: CE 0.2672380805015564, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.747297286987305 EntMin 0.0
Epoch 3, Class Loss=0.2825143039226532, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.2825143039226532, Class Loss=0.2825143039226532, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.3206928968429565
Loss made of: CE 0.26036423444747925, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.971711158752441 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.104043366014958
Loss made of: CE 0.2683142423629761, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.853332996368408 EntMin 0.0
Epoch 4, Class Loss=0.27358970046043396, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.27358970046043396, Class Loss=0.27358970046043396, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.323654839396477
Loss made of: CE 0.2617846429347992, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.234131813049316 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.3287293568253515
Loss made of: CE 0.2674950361251831, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.263617992401123 EntMin 0.0
Epoch 5, Class Loss=0.27178889513015747, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.27178889513015747, Class Loss=0.27178889513015747, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.081095904111862
Loss made of: CE 0.32955461740493774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6103835105896 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.077174328267574
Loss made of: CE 0.2892553210258484, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.007620334625244 EntMin 0.0
Epoch 6, Class Loss=0.27419525384902954, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.27419525384902954, Class Loss=0.27419525384902954, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Class Loss=0.5423536896705627, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5423536896705627, Class Loss=0.5423536896705627, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.4850131869316101, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.4850131869316101, Class Loss=0.4850131869316101, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.4034148156642914, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.4034148156642914, Class Loss=0.4034148156642914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.3657209575176239, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.3657209575176239, Class Loss=0.3657209575176239, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.3458940386772156, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.3458940386772156, Class Loss=0.3458940386772156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Class Loss=0.31007859110832214, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.31007859110832214, Class Loss=0.31007859110832214, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/27, Loss=6.545423585176468
Loss made of: CE 0.3576037883758545, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.6922736167907715 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.693219238519669
Loss made of: CE 0.3049282431602478, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.329535961151123 EntMin 0.0
Epoch 1, Class Loss=0.33454492688179016, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.33454492688179016, Class Loss=0.33454492688179016, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/27, Loss=5.491537994146347
Loss made of: CE 0.27159321308135986, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.718657493591309 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.529696995019913
Loss made of: CE 0.32997578382492065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.764339447021484 EntMin 0.0
Epoch 2, Class Loss=0.3026764392852783, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.3026764392852783, Class Loss=0.3026764392852783, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/27, Loss=5.178271496295929
Loss made of: CE 0.31968235969543457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.829217910766602 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.454047498106957
Loss made of: CE 0.31787389516830444, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8057861328125 EntMin 0.0
Epoch 3, Class Loss=0.28958460688591003, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.28958460688591003, Class Loss=0.28958460688591003, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/27, Loss=5.294641962647438
Loss made of: CE 0.329974502325058, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.292476177215576 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.4349725738167765
Loss made of: CE 0.2537956237792969, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.101988792419434 EntMin 0.0
Epoch 4, Class Loss=0.28247129917144775, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.28247129917144775, Class Loss=0.28247129917144775, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/27, Loss=5.286093260347843
Loss made of: CE 0.271812379360199, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.267020225524902 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.260766117274761
Loss made of: CE 0.27023565769195557, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.912469863891602 EntMin 0.0
Epoch 5, Class Loss=0.28460171818733215, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.28460171818733215, Class Loss=0.28460171818733215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/27, Loss=5.147702531516552
Loss made of: CE 0.35429370403289795, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9204912185668945 EntMin 0.0
Epoch 6, Batch 20/27, Loss=5.119786570966244
Loss made of: CE 0.2855569124221802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.473369598388672 EntMin 0.0
Epoch 6, Class Loss=0.2817894220352173, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.2817894220352173, Class Loss=0.2817894220352173, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Epoch 1, Class Loss=0.5506042242050171, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5506042242050171, Class Loss=0.5506042242050171, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Epoch 2, Class Loss=0.5164452791213989, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.5164452791213989, Class Loss=0.5164452791213989, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.4553876519203186, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.4553876519203186, Class Loss=0.4553876519203186, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.42379260063171387, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.42379260063171387, Class Loss=0.42379260063171387, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.3787306249141693, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.3787306249141693, Class Loss=0.3787306249141693, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.3758007287979126, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.3758007287979126, Class Loss=0.3758007287979126, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2633095979690552, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.923031
Mean Acc: 0.747111
FreqW Acc: 0.862926
Mean IoU: 0.644515
Class IoU:
	class 0: 0.91162956
	class 1: 0.8782468
	class 2: 0.3283607
	class 3: 0.77883637
	class 4: 0.68429756
	class 5: 0.7831256
	class 6: 0.89970124
	class 7: 0.8698783
	class 8: 0.8698905
	class 9: 0.085695885
	class 10: 0.0
Class Acc:
	class 0: 0.97003794
	class 1: 0.9348584
	class 2: 0.9083029
	class 3: 0.7914775
	class 4: 0.8346515
	class 5: 0.8913609
	class 6: 0.9164502
	class 7: 0.91516745
	class 8: 0.9548444
	class 9: 0.10107397
	class 10: 0.0

federated global round: 9, step: 1
select part of clients to conduct local training
[4, 6, 13, 12]
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=5.770728114247322
Loss made of: CE 0.33678650856018066, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.112249374389648 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.391681504249573
Loss made of: CE 0.3527297377586365, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.046507358551025 EntMin 0.0
Epoch 1, Class Loss=0.3514377772808075, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.3514377772808075, Class Loss=0.3514377772808075, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.197783029079437
Loss made of: CE 0.2942967414855957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.704843521118164 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.297349220514297
Loss made of: CE 0.33700308203697205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.468899726867676 EntMin 0.0
Epoch 2, Class Loss=0.3292815387248993, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.3292815387248993, Class Loss=0.3292815387248993, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=4.925101631879807
Loss made of: CE 0.34484195709228516, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5383100509643555 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.266937729716301
Loss made of: CE 0.3573201596736908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664743423461914 EntMin 0.0
Epoch 3, Class Loss=0.31314998865127563, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.31314998865127563, Class Loss=0.31314998865127563, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.037542098760605
Loss made of: CE 0.4499748647212982, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.108301162719727 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.157228887081146
Loss made of: CE 0.31083816289901733, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8218183517456055 EntMin 0.0
Epoch 4, Class Loss=0.3188353180885315, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.3188353180885315, Class Loss=0.3188353180885315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.03809557557106
Loss made of: CE 0.28024184703826904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.762630462646484 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.069474071264267
Loss made of: CE 0.28666743636131287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7680864334106445 EntMin 0.0
Epoch 5, Class Loss=0.30537816882133484, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.30537816882133484, Class Loss=0.30537816882133484, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=5.089653170108795
Loss made of: CE 0.37072914838790894, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.796675205230713 EntMin 0.0
Epoch 6, Batch 20/27, Loss=4.965665732324124
Loss made of: CE 0.32621580362319946, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.356165885925293 EntMin 0.0
Epoch 6, Class Loss=0.3061293959617615, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.3061293959617615, Class Loss=0.3061293959617615, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/27, Loss=5.726441487669945
Loss made of: CE 0.3271099030971527, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.850106239318848 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.322066760063171
Loss made of: CE 0.2957441210746765, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.04408073425293 EntMin 0.0
Epoch 1, Class Loss=0.36320436000823975, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.36320436000823975, Class Loss=0.36320436000823975, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/27, Loss=5.157891744375229
Loss made of: CE 0.3320934772491455, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.111973285675049 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.266875711083412
Loss made of: CE 0.37858033180236816, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.062203884124756 EntMin 0.0
Epoch 2, Class Loss=0.3299552798271179, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3299552798271179, Class Loss=0.3299552798271179, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/27, Loss=5.100602394342422
Loss made of: CE 0.35423967242240906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.794839859008789 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.144321730732917
Loss made of: CE 0.3415209650993347, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.523123741149902 EntMin 0.0
Epoch 3, Class Loss=0.3165596127510071, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.3165596127510071, Class Loss=0.3165596127510071, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/27, Loss=4.977111461758613
Loss made of: CE 0.3102162480354309, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421562194824219 EntMin 0.0
Epoch 4, Batch 20/27, Loss=5.090254646539688
Loss made of: CE 0.27466028928756714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.693014144897461 EntMin 0.0
Epoch 4, Class Loss=0.3094409108161926, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3094409108161926, Class Loss=0.3094409108161926, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/27, Loss=5.0474916189908985
Loss made of: CE 0.285539448261261, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3071489334106445 EntMin 0.0
Epoch 5, Batch 20/27, Loss=4.95955012589693
Loss made of: CE 0.23073193430900574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.845312118530273 EntMin 0.0
Epoch 5, Class Loss=0.30663666129112244, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.30663666129112244, Class Loss=0.30663666129112244, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/27, Loss=4.906384570896625
Loss made of: CE 0.2413354218006134, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.590957164764404 EntMin 0.0
Epoch 6, Batch 20/27, Loss=4.814358459413052
Loss made of: CE 0.26584818959236145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.310758590698242 EntMin 0.0
Epoch 6, Class Loss=0.2988143265247345, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.2988143265247345, Class Loss=0.2988143265247345, Reg Loss=0.0
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000372
Epoch 1, Class Loss=0.5976885557174683, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.5976885557174683, Class Loss=0.5976885557174683, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000316
Epoch 2, Class Loss=0.5538073778152466, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.5538073778152466, Class Loss=0.5538073778152466, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000258
Epoch 3, Class Loss=0.5170033574104309, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.5170033574104309, Class Loss=0.5170033574104309, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000199
Epoch 4, Class Loss=0.5134584903717041, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.5134584903717041, Class Loss=0.5134584903717041, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000138
Epoch 5, Class Loss=0.4939380884170532, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.4939380884170532, Class Loss=0.4939380884170532, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000074
Epoch 6, Class Loss=0.49895644187927246, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.49895644187927246, Class Loss=0.49895644187927246, Reg Loss=0.0
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/27, Loss=5.852620688080788
Loss made of: CE 0.4248690605163574, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.913200855255127 EntMin 0.0
Epoch 1, Batch 20/27, Loss=5.298268291354179
Loss made of: CE 0.33546632528305054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.815207481384277 EntMin 0.0
Epoch 1, Class Loss=0.3437412679195404, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.3437412679195404, Class Loss=0.3437412679195404, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/27, Loss=5.16871662735939
Loss made of: CE 0.3425397276878357, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758143424987793 EntMin 0.0
Epoch 2, Batch 20/27, Loss=5.377756559848786
Loss made of: CE 0.4078627824783325, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.17454195022583 EntMin 0.0
Epoch 2, Class Loss=0.3188396394252777, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.3188396394252777, Class Loss=0.3188396394252777, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/27, Loss=5.185774084925652
Loss made of: CE 0.26126575469970703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.724869728088379 EntMin 0.0
Epoch 3, Batch 20/27, Loss=5.1629619836807255
Loss made of: CE 0.2891516387462616, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.472079277038574 EntMin 0.0
Epoch 3, Class Loss=0.3067947030067444, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.3067947030067444, Class Loss=0.3067947030067444, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/27, Loss=5.064423447847366
Loss made of: CE 0.26917847990989685, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.81856107711792 EntMin 0.0
Epoch 4, Batch 20/27, Loss=4.880754140019417
Loss made of: CE 0.2993693947792053, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.79433012008667 EntMin 0.0
Epoch 4, Class Loss=0.29917195439338684, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.29917195439338684, Class Loss=0.29917195439338684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/27, Loss=5.1347535967826845
Loss made of: CE 0.27587518095970154, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.006655216217041 EntMin 0.0
Epoch 5, Batch 20/27, Loss=5.0859114021062855
Loss made of: CE 0.28011640906333923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.921421051025391 EntMin 0.0
Epoch 5, Class Loss=0.2960933744907379, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.2960933744907379, Class Loss=0.2960933744907379, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/27, Loss=4.985707759857178
Loss made of: CE 0.3422713279724121, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.603268146514893 EntMin 0.0
Epoch 6, Batch 20/27, Loss=4.875383207201958
Loss made of: CE 0.3243899941444397, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.710880279541016 EntMin 0.0
Epoch 6, Class Loss=0.2950591742992401, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.2950591742992401, Class Loss=0.2950591742992401, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.2709082365036011, Reg Loss=0.0 (without scaling)

Total samples: 869.000000
Overall Acc: 0.922551
Mean Acc: 0.773134
FreqW Acc: 0.865796
Mean IoU: 0.658131
Class IoU:
	class 0: 0.9120953
	class 1: 0.88216347
	class 2: 0.32767385
	class 3: 0.8198969
	class 4: 0.69979894
	class 5: 0.78285533
	class 6: 0.91011685
	class 7: 0.86960816
	class 8: 0.870046
	class 9: 0.16518246
	class 10: 0.0
Class Acc:
	class 0: 0.9628294
	class 1: 0.9488265
	class 2: 0.9145556
	class 3: 0.8357338
	class 4: 0.8606426
	class 5: 0.9056157
	class 6: 0.92773116
	class 7: 0.9282469
	class 8: 0.9668484
	class 9: 0.25344017
	class 10: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[10, 14, 16, 7]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=11.495892409048974
Loss made of: CE 0.048609428107738495, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.354491233825684 EntMin 0.0
Epoch 1, Batch 20/29, Loss=9.879697273857891
Loss made of: CE 0.047646429389715195, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.24549674987793 EntMin 0.0
Epoch 1, Class Loss=0.05165522173047066, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.05165522173047066, Class Loss=0.05165522173047066, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=8.925966459512711
Loss made of: CE 0.09004094451665878, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.01309871673584 EntMin 0.0
Epoch 2, Batch 20/29, Loss=8.54037984982133
Loss made of: CE 0.16822287440299988, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.960478782653809 EntMin 0.0
Epoch 2, Class Loss=0.1286904364824295, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.1286904364824295, Class Loss=0.1286904364824295, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.435696324706077
Loss made of: CE 0.1637447327375412, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.518594741821289 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.771436087787151
Loss made of: CE 0.19789475202560425, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.026640892028809 EntMin 0.0
Epoch 3, Class Loss=0.23596958816051483, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.23596958816051483, Class Loss=0.23596958816051483, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=7.978807181119919
Loss made of: CE 0.38450559973716736, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.04407024383545 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.862660737335682
Loss made of: CE 0.3351582884788513, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.451102256774902 EntMin 0.0
Epoch 4, Class Loss=0.3087939918041229, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.3087939918041229, Class Loss=0.3087939918041229, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.5551479533314705
Loss made of: CE 0.30397266149520874, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.6563005447387695 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.722820322215557
Loss made of: CE 0.34013205766677856, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.258453369140625 EntMin 0.0
Epoch 5, Class Loss=0.29588499665260315, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.29588499665260315, Class Loss=0.29588499665260315, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.260129714012146
Loss made of: CE 0.2637847065925598, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.345296859741211 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.2938584834337234
Loss made of: CE 0.2280009388923645, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.522462368011475 EntMin 0.0
Epoch 6, Class Loss=0.27494487166404724, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.27494487166404724, Class Loss=0.27494487166404724, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.813740364462137
Loss made of: CE 0.1284511536359787, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.263876914978027 EntMin 0.0
Epoch 1, Class Loss=0.14395974576473236, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.14395974576473236, Class Loss=0.14395974576473236, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.282745368778706
Loss made of: CE 0.22954688966274261, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.84722900390625 EntMin 0.0
Epoch 2, Class Loss=0.2934448719024658, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.2934448719024658, Class Loss=0.2934448719024658, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=5.937937599420548
Loss made of: CE 0.3607334494590759, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.062526226043701 EntMin 0.0
Epoch 3, Class Loss=0.4057892858982086, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.4057892858982086, Class Loss=0.4057892858982086, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.680490034818649
Loss made of: CE 0.5114511251449585, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.533048152923584 EntMin 0.0
Epoch 4, Class Loss=0.4610441327095032, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.4610441327095032, Class Loss=0.4610441327095032, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.592829570174217
Loss made of: CE 0.4656795859336853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.807729721069336 EntMin 0.0
Epoch 5, Class Loss=0.47886553406715393, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.47886553406715393, Class Loss=0.47886553406715393, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.444863176345825
Loss made of: CE 0.5376114845275879, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.87868595123291 EntMin 0.0
Epoch 6, Class Loss=0.4930459260940552, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.4930459260940552, Class Loss=0.4930459260940552, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=11.802123336493969
Loss made of: CE 0.0389288105070591, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.03705883026123 EntMin 0.0
Epoch 1, Batch 20/29, Loss=10.037192940711975
Loss made of: CE 0.050998982042074203, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.850099563598633 EntMin 0.0
Epoch 1, Class Loss=0.04650099202990532, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.04650099202990532, Class Loss=0.04650099202990532, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/29, Loss=8.81698247641325
Loss made of: CE 0.06018824875354767, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.626043319702148 EntMin 0.0
Epoch 2, Batch 20/29, Loss=8.83723755851388
Loss made of: CE 0.19382552802562714, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.744770050048828 EntMin 0.0
Epoch 2, Class Loss=0.13801580667495728, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.13801580667495728, Class Loss=0.13801580667495728, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/29, Loss=8.581219114363194
Loss made of: CE 0.2188461422920227, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.952945709228516 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.84312968403101
Loss made of: CE 0.3205852210521698, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.265121459960938 EntMin 0.0
Epoch 3, Class Loss=0.24590179324150085, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.24590179324150085, Class Loss=0.24590179324150085, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/29, Loss=8.027970802783965
Loss made of: CE 0.39479899406433105, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.134751796722412 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.991010782122612
Loss made of: CE 0.2639755606651306, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.857333660125732 EntMin 0.0
Epoch 4, Class Loss=0.29333171248435974, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.29333171248435974, Class Loss=0.29333171248435974, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/29, Loss=7.4754318878054615
Loss made of: CE 0.2565801739692688, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.361081600189209 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.796949760615826
Loss made of: CE 0.3801261782646179, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.154664516448975 EntMin 0.0
Epoch 5, Class Loss=0.29462888836860657, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.29462888836860657, Class Loss=0.29462888836860657, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/29, Loss=7.405634704232216
Loss made of: CE 0.37945279479026794, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.426712989807129 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.380556367337704
Loss made of: CE 0.2470162808895111, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.671462535858154 EntMin 0.0
Epoch 6, Class Loss=0.2778412699699402, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.2778412699699402, Class Loss=0.2778412699699402, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.664922552555799
Loss made of: CE 0.14459705352783203, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.9133124351501465 EntMin 0.0
Epoch 1, Class Loss=0.13153314590454102, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.13153314590454102, Class Loss=0.13153314590454102, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=6.21585705280304
Loss made of: CE 0.27294814586639404, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.341890335083008 EntMin 0.0
Epoch 2, Class Loss=0.2819089889526367, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.2819089889526367, Class Loss=0.2819089889526367, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=5.842035952210426
Loss made of: CE 0.2517170310020447, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.720948696136475 EntMin 0.0
Epoch 3, Class Loss=0.38774505257606506, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.38774505257606506, Class Loss=0.38774505257606506, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=5.825396132469177
Loss made of: CE 0.34383460879325867, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.060658931732178 EntMin 0.0
Epoch 4, Class Loss=0.4424278736114502, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.4424278736114502, Class Loss=0.4424278736114502, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=5.719199332594871
Loss made of: CE 0.5681882500648499, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.738070964813232 EntMin 0.0
Epoch 5, Class Loss=0.4836925268173218, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4836925268173218, Class Loss=0.4836925268173218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=5.624693793058396
Loss made of: CE 0.510102391242981, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.661910057067871 EntMin 0.0
Epoch 6, Class Loss=0.4958312213420868, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.4958312213420868, Class Loss=0.4958312213420868, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5246036052703857, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.858442
Mean Acc: 0.659588
FreqW Acc: 0.771585
Mean IoU: 0.517150
Class IoU:
	class 0: 0.8584264
	class 1: 0.8419416
	class 2: 0.26203674
	class 3: 0.78667784
	class 4: 0.67575467
	class 5: 0.73788446
	class 6: 0.8801213
	class 7: 0.82679397
	class 8: 0.73471004
	class 9: 0.11617483
	class 10: 1.1779324e-06
	class 11: 0.0
	class 12: 0.002424636
Class Acc:
	class 0: 0.9389794
	class 1: 0.92079186
	class 2: 0.9335739
	class 3: 0.8110449
	class 4: 0.8212365
	class 5: 0.93306506
	class 6: 0.90057606
	class 7: 0.9311295
	class 8: 0.9739045
	class 9: 0.40792155
	class 10: 1.1779324e-06
	class 11: 0.0
	class 12: 0.0024246916

federated global round: 11, step: 2
select part of clients to conduct local training
[10, 13, 6, 1]
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/29, Loss=9.409604901075364
Loss made of: CE 0.6963831186294556, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.838033199310303 EntMin 0.0
Epoch 1, Batch 20/29, Loss=8.387935400009155
Loss made of: CE 0.6255591511726379, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.466151714324951 EntMin 0.0
Epoch 1, Class Loss=0.7254567742347717, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.7254567742347717, Class Loss=0.7254567742347717, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/29, Loss=8.095438966155053
Loss made of: CE 0.46281665563583374, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.605762481689453 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.845631042122841
Loss made of: CE 0.41365277767181396, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0870561599731445 EntMin 0.0
Epoch 2, Class Loss=0.4404737651348114, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.4404737651348114, Class Loss=0.4404737651348114, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/29, Loss=7.627363634109497
Loss made of: CE 0.35835880041122437, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.494326591491699 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.22172545492649
Loss made of: CE 0.3082795739173889, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.514064311981201 EntMin 0.0
Epoch 3, Class Loss=0.358660489320755, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.358660489320755, Class Loss=0.358660489320755, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/29, Loss=7.348864951729775
Loss made of: CE 0.3500519394874573, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.3022918701171875 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.303329184651375
Loss made of: CE 0.30476662516593933, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.888466835021973 EntMin 0.0
Epoch 4, Class Loss=0.32325097918510437, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.32325097918510437, Class Loss=0.32325097918510437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=7.263589707016945
Loss made of: CE 0.2711902856826782, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4113054275512695 EntMin 0.0
Epoch 5, Batch 20/29, Loss=7.171860983967781
Loss made of: CE 0.31198328733444214, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.700018882751465 EntMin 0.0
Epoch 5, Class Loss=0.30233103036880493, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.30233103036880493, Class Loss=0.30233103036880493, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/29, Loss=6.961036486923694
Loss made of: CE 0.2379922866821289, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.821796417236328 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.734458777308464
Loss made of: CE 0.28942763805389404, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.178328990936279 EntMin 0.0
Epoch 6, Class Loss=0.27426743507385254, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.27426743507385254, Class Loss=0.27426743507385254, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.882006277097389
Loss made of: CE 0.020628672093153, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.453754425048828 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.999999057874083
Loss made of: CE 0.016917210072278976, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.933666229248047 EntMin 0.0
Epoch 1, Class Loss=0.02748037874698639, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.02748037874698639, Class Loss=0.02748037874698639, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/29, Loss=7.9476403698325155
Loss made of: CE 0.06529081612825394, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.776177406311035 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.313227469846606
Loss made of: CE 0.026069816201925278, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.298582077026367 EntMin 0.0
Epoch 2, Class Loss=0.07769608497619629, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.07769608497619629, Class Loss=0.07769608497619629, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/29, Loss=7.235944700241089
Loss made of: CE 0.15107455849647522, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.259019374847412 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.7576465584337715
Loss made of: CE 0.07907944172620773, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.10475492477417 EntMin 0.0
Epoch 3, Class Loss=0.13419178128242493, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.13419178128242493, Class Loss=0.13419178128242493, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/29, Loss=7.353703589737416
Loss made of: CE 0.17309576272964478, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.284390926361084 EntMin 0.0
Epoch 4, Batch 20/29, Loss=7.4021968185901645
Loss made of: CE 0.17830520868301392, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.049724578857422 EntMin 0.0
Epoch 4, Class Loss=0.19023765623569489, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.19023765623569489, Class Loss=0.19023765623569489, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/29, Loss=7.033543735742569
Loss made of: CE 0.13959690928459167, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.110578536987305 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 20/29, Loss=7.200001955032349
Loss made of: CE 0.21646277606487274, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.299947738647461 EntMin 0.0
Epoch 5, Class Loss=0.19981622695922852, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.19981622695922852, Class Loss=0.19981622695922852, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/29, Loss=6.924194370210171
Loss made of: CE 0.2087341845035553, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.700462341308594 EntMin 0.0
Epoch 6, Batch 20/29, Loss=7.09944921284914
Loss made of: CE 0.18582765758037567, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.106656074523926 EntMin 0.0
Epoch 6, Class Loss=0.21196353435516357, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.21196353435516357, Class Loss=0.21196353435516357, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.444084653258324
Loss made of: CE 0.15581101179122925, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.609687328338623 EntMin 0.0
Epoch 1, Class Loss=0.1329258233308792, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.1329258233308792, Class Loss=0.1329258233308792, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=5.797271178662777
Loss made of: CE 0.24462738633155823, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.388656139373779 EntMin 0.0
Epoch 2, Class Loss=0.2598625719547272, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.2598625719547272, Class Loss=0.2598625719547272, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=5.556401893496513
Loss made of: CE 0.27007216215133667, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.637978553771973 EntMin 0.0
Epoch 3, Class Loss=0.35742345452308655, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.35742345452308655, Class Loss=0.35742345452308655, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.486907219886779
Loss made of: CE 0.5111403465270996, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.191547870635986 EntMin 0.0
Epoch 4, Class Loss=0.40750524401664734, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.40750524401664734, Class Loss=0.40750524401664734, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.376369145512581
Loss made of: CE 0.42926138639450073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.702553749084473 EntMin 0.0
Epoch 5, Class Loss=0.4198772609233856, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.4198772609233856, Class Loss=0.4198772609233856, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.2102304100990295
Loss made of: CE 0.39574119448661804, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.154339790344238 EntMin 0.0
Epoch 6, Class Loss=0.41914790868759155, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.41914790868759155, Class Loss=0.41914790868759155, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.400065092742443
Loss made of: CE 0.15622630715370178, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.555208206176758 EntMin 0.0
Epoch 1, Class Loss=0.14501433074474335, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.14501433074474335, Class Loss=0.14501433074474335, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=5.873536908626557
Loss made of: CE 0.26132985949516296, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.243220329284668 EntMin 0.0
Epoch 2, Class Loss=0.27021923661231995, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.27021923661231995, Class Loss=0.27021923661231995, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/13, Loss=5.380206120014191
Loss made of: CE 0.3385407328605652, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.561377048492432 EntMin 0.0
Epoch 3, Class Loss=0.3519594669342041, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3519594669342041, Class Loss=0.3519594669342041, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=5.4547054201364515
Loss made of: CE 0.4039883613586426, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746148109436035 EntMin 0.0
Epoch 4, Class Loss=0.40813663601875305, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.40813663601875305, Class Loss=0.40813663601875305, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.346186861395836
Loss made of: CE 0.39353781938552856, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.650681018829346 EntMin 0.0
Epoch 5, Class Loss=0.4184802770614624, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4184802770614624, Class Loss=0.4184802770614624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=5.112550586462021
Loss made of: CE 0.42517513036727905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.981454849243164 EntMin 0.0
Epoch 6, Class Loss=0.42233145236968994, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.42233145236968994, Class Loss=0.42233145236968994, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.43867507576942444, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.879533
Mean Acc: 0.678390
FreqW Acc: 0.800491
Mean IoU: 0.545503
Class IoU:
	class 0: 0.8802573
	class 1: 0.84727246
	class 2: 0.27260965
	class 3: 0.79648405
	class 4: 0.67939055
	class 5: 0.72385323
	class 6: 0.8048174
	class 7: 0.8416282
	class 8: 0.7506839
	class 9: 0.13330601
	class 10: 0.0
	class 11: 0.0
	class 12: 0.3612409
Class Acc:
	class 0: 0.9511291
	class 1: 0.9102976
	class 2: 0.9334786
	class 3: 0.822401
	class 4: 0.81499696
	class 5: 0.93924916
	class 6: 0.81447387
	class 7: 0.92451763
	class 8: 0.9690412
	class 9: 0.30114564
	class 10: 0.0
	class 11: 0.0
	class 12: 0.43833596

federated global round: 12, step: 2
select part of clients to conduct local training
[11, 0, 8, 14]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.34997288286686
Loss made of: CE 0.12101618945598602, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.868928909301758 EntMin 0.0
Epoch 1, Class Loss=0.12579406797885895, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.12579406797885895, Class Loss=0.12579406797885895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=5.797478729486466
Loss made of: CE 0.3148140013217926, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.930396556854248 EntMin 0.0
Epoch 2, Class Loss=0.25844112038612366, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.25844112038612366, Class Loss=0.25844112038612366, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.356518262624741
Loss made of: CE 0.28101062774658203, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4728193283081055 EntMin 0.0
Epoch 3, Class Loss=0.31696948409080505, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.31696948409080505, Class Loss=0.31696948409080505, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.352575945854187
Loss made of: CE 0.38422060012817383, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.028658390045166 EntMin 0.0
Epoch 4, Class Loss=0.334734171628952, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.334734171628952, Class Loss=0.334734171628952, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.49929358959198
Loss made of: CE 0.38032349944114685, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.765539169311523 EntMin 0.0
Epoch 5, Class Loss=0.36770331859588623, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.36770331859588623, Class Loss=0.36770331859588623, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.1067776650190355
Loss made of: CE 0.33100584149360657, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.194215297698975 EntMin 0.0
Epoch 6, Class Loss=0.3760133683681488, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.3760133683681488, Class Loss=0.3760133683681488, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.918485357309692
Loss made of: CE 0.012830846011638641, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.520817756652832 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.1998618936399
Loss made of: CE 0.03489579260349274, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.216121673583984 EntMin 0.0
Epoch 1, Class Loss=0.01563914120197296, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.01563914120197296, Class Loss=0.01563914120197296, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=6.981586214154959
Loss made of: CE 0.03362816199660301, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.738362789154053 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.170243580080569
Loss made of: CE 0.039218585938215256, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.148270606994629 EntMin 0.0
Epoch 2, Class Loss=0.049047112464904785, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.049047112464904785, Class Loss=0.049047112464904785, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=7.006747986376285
Loss made of: CE 0.11211800575256348, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.288363456726074 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.656368873268366
Loss made of: CE 0.07858120650053024, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.413743495941162 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Class Loss=0.08695851266384125, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.08695851266384125, Class Loss=0.08695851266384125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=6.751353164017201
Loss made of: CE 0.11277761310338974, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.1406683921813965 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.687437677383423
Loss made of: CE 0.10832482576370239, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.392986297607422 EntMin 0.0
Epoch 4, Class Loss=0.12464423477649689, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.12464423477649689, Class Loss=0.12464423477649689, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=6.549572762846947
Loss made of: CE 0.19850867986679077, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.227515697479248 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.677291162312031
Loss made of: CE 0.1756572723388672, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.034653663635254 EntMin 0.0
Epoch 5, Class Loss=0.15989498794078827, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.15989498794078827, Class Loss=0.15989498794078827, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=6.607262620329857
Loss made of: CE 0.15077655017375946, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.121729850769043 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.629311247169971
Loss made of: CE 0.15920694172382355, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.412753582000732 EntMin 0.0
Epoch 6, Class Loss=0.1769912987947464, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.1769912987947464, Class Loss=0.1769912987947464, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=8.136622219812125
Loss made of: CE 0.034528203308582306, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.085487365722656 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.278342551505193
Loss made of: CE 0.009292739443480968, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.734858512878418 EntMin 0.0
Epoch 1, Class Loss=0.017161227762699127, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.017161227762699127, Class Loss=0.017161227762699127, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/29, Loss=6.80132917650044
Loss made of: CE 0.03086664527654648, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.367862224578857 EntMin 0.0
Epoch 2, Batch 20/29, Loss=7.165584224462509
Loss made of: CE 0.04089248925447464, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.119678020477295 EntMin 0.0
Epoch 2, Class Loss=0.04836021363735199, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.04836021363735199, Class Loss=0.04836021363735199, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/29, Loss=7.181484328210354
Loss made of: CE 0.05206030607223511, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4014482498168945 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.097615407779813
Loss made of: CE 0.12402866780757904, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.456244468688965 EntMin 0.0
Epoch 3, Class Loss=0.09342232346534729, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.09342232346534729, Class Loss=0.09342232346534729, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/29, Loss=6.5704465508461
Loss made of: CE 0.12615013122558594, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.477546691894531 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.989457831531763
Loss made of: CE 0.15664805471897125, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.212700843811035 EntMin 0.0
Epoch 4, Class Loss=0.13135100901126862, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.13135100901126862, Class Loss=0.13135100901126862, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/29, Loss=6.768834160268307
Loss made of: CE 0.13487674295902252, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.385622978210449 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.51446118503809
Loss made of: CE 0.14183369278907776, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.799716472625732 EntMin 0.0
Epoch 5, Class Loss=0.15579849481582642, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.15579849481582642, Class Loss=0.15579849481582642, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/29, Loss=6.774696487188339
Loss made of: CE 0.18463444709777832, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.190250396728516 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.719760033488273
Loss made of: CE 0.1531258225440979, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.610869407653809 EntMin 0.0
Epoch 6, Class Loss=0.17647172510623932, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.17647172510623932, Class Loss=0.17647172510623932, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.876994115114212
Loss made of: CE 0.7244771718978882, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.449703216552734 EntMin 0.0
Epoch 1, Class Loss=0.6739576458930969, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.6739576458930969, Class Loss=0.6739576458930969, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/13, Loss=6.08070522248745
Loss made of: CE 0.5615419745445251, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.00833797454834 EntMin 0.0
Epoch 2, Class Loss=0.586215615272522, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.586215615272522, Class Loss=0.586215615272522, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/13, Loss=5.693097305297852
Loss made of: CE 0.4487122893333435, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.840660095214844 EntMin 0.0
Epoch 3, Class Loss=0.48164549469947815, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.48164549469947815, Class Loss=0.48164549469947815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/13, Loss=5.336576217412949
Loss made of: CE 0.41867494583129883, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.310671806335449 EntMin 0.0
Epoch 4, Class Loss=0.416786253452301, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.416786253452301, Class Loss=0.416786253452301, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/13, Loss=5.237275964021682
Loss made of: CE 0.4378132224082947, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.644974708557129 EntMin 0.0
Epoch 5, Class Loss=0.38451334834098816, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.38451334834098816, Class Loss=0.38451334834098816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/13, Loss=5.118909540772438
Loss made of: CE 0.37589430809020996, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.452060699462891 EntMin 0.0
Epoch 6, Class Loss=0.35909581184387207, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.35909581184387207, Class Loss=0.35909581184387207, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.43217891454696655, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.882393
Mean Acc: 0.703317
FreqW Acc: 0.809716
Mean IoU: 0.556170
Class IoU:
	class 0: 0.8871956
	class 1: 0.8620871
	class 2: 0.27350777
	class 3: 0.79302555
	class 4: 0.68493026
	class 5: 0.7179182
	class 6: 0.8102185
	class 7: 0.84372586
	class 8: 0.74611175
	class 9: 0.15579385
	class 10: 0.0
	class 11: 0.0
	class 12: 0.45570067
Class Acc:
	class 0: 0.9422285
	class 1: 0.92862135
	class 2: 0.9355793
	class 3: 0.8268826
	class 4: 0.82247657
	class 5: 0.9376315
	class 6: 0.8194152
	class 7: 0.92490876
	class 8: 0.9729062
	class 9: 0.3828563
	class 10: 0.0
	class 11: 0.0
	class 12: 0.6496186

federated global round: 13, step: 2
select part of clients to conduct local training
[13, 5, 15, 7]
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/29, Loss=8.243350329995156
Loss made of: CE 0.4319199025630951, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.788713455200195 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.540421968698501
Loss made of: CE 0.3845028579235077, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.382628917694092 EntMin 0.0
Epoch 1, Class Loss=0.494850754737854, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.494850754737854, Class Loss=0.494850754737854, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/29, Loss=7.4228535205125805
Loss made of: CE 0.368837833404541, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.758205413818359 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.848900416493416
Loss made of: CE 0.3257635831832886, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4504899978637695 EntMin 0.0
Epoch 2, Class Loss=0.3442092537879944, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.3442092537879944, Class Loss=0.3442092537879944, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/29, Loss=6.755388477444649
Loss made of: CE 0.2781093716621399, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.560878753662109 EntMin 0.0
Epoch 3, Batch 20/29, Loss=7.221684378385544
Loss made of: CE 0.2927744388580322, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.213512420654297 EntMin 0.0
Epoch 3, Class Loss=0.3029960095882416, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.3029960095882416, Class Loss=0.3029960095882416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/29, Loss=6.845926414430141
Loss made of: CE 0.2688940465450287, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.691009044647217 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.953257510066033
Loss made of: CE 0.2772032916545868, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.346962928771973 EntMin 0.0
Epoch 4, Class Loss=0.28620046377182007, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.28620046377182007, Class Loss=0.28620046377182007, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/29, Loss=6.673785334825515
Loss made of: CE 0.26498711109161377, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3891520500183105 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.6280701324343685
Loss made of: CE 0.295585036277771, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.709017753601074 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.27076661586761475, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.27076661586761475, Class Loss=0.27076661586761475, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/29, Loss=6.539873848855495
Loss made of: CE 0.2320079505443573, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.563961029052734 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.743868072330952
Loss made of: CE 0.29776710271835327, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4087982177734375 EntMin 0.0
Epoch 6, Class Loss=0.25845620036125183, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.25845620036125183, Class Loss=0.25845620036125183, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.086841190257109
Loss made of: CE 0.023272786289453506, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.038722038269043 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.173211901425384
Loss made of: CE 0.010176319628953934, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.075377464294434 EntMin 0.0
Epoch 1, Class Loss=0.013269711285829544, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.013269711285829544, Class Loss=0.013269711285829544, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/29, Loss=7.2350298492237926
Loss made of: CE 0.03992244601249695, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.963895320892334 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/29, Loss=6.811961383372545
Loss made of: CE 0.05305624008178711, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.879064559936523 EntMin 0.0
Epoch 2, Class Loss=0.04061876982450485, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.04061876982450485, Class Loss=0.04061876982450485, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/29, Loss=6.867103318497539
Loss made of: CE 0.059513505548238754, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.80564022064209 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.499626692011953
Loss made of: CE 0.1010824590921402, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.36274528503418 EntMin 0.0
Epoch 3, Class Loss=0.08075059950351715, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.08075059950351715, Class Loss=0.08075059950351715, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/29, Loss=6.515218596160412
Loss made of: CE 0.12389211356639862, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.584567070007324 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.564177771657706
Loss made of: CE 0.1686696708202362, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.080124855041504 EntMin 0.0
Epoch 4, Class Loss=0.11552577465772629, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.11552577465772629, Class Loss=0.11552577465772629, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/29, Loss=6.377932260930538
Loss made of: CE 0.11804979294538498, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.002475738525391 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.524790936708451
Loss made of: CE 0.127355694770813, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.934208869934082 EntMin 0.0
Epoch 5, Class Loss=0.13947762548923492, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.13947762548923492, Class Loss=0.13947762548923492, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/29, Loss=6.459616473317146
Loss made of: CE 0.13540826737880707, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.837221145629883 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.424327762424946
Loss made of: CE 0.16255417466163635, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.009682655334473 EntMin 0.0
Epoch 6, Class Loss=0.16348206996917725, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.16348206996917725, Class Loss=0.16348206996917725, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.262238242477179
Loss made of: CE 0.10885708034038544, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.809384346008301 EntMin 0.0
Epoch 1, Class Loss=0.1019388809800148, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.1019388809800148, Class Loss=0.1019388809800148, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.680425296723842
Loss made of: CE 0.200157031416893, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.644110679626465 EntMin 0.0
Epoch 2, Class Loss=0.18941518664360046, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.18941518664360046, Class Loss=0.18941518664360046, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.44106425344944
Loss made of: CE 0.18121199309825897, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.49993896484375 EntMin 0.0
Epoch 3, Class Loss=0.2578371465206146, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.2578371465206146, Class Loss=0.2578371465206146, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.1108601808547975
Loss made of: CE 0.297837495803833, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.084859848022461 EntMin 0.0
Epoch 4, Class Loss=0.2824893593788147, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.2824893593788147, Class Loss=0.2824893593788147, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.163637456297875
Loss made of: CE 0.28670889139175415, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9434943199157715 EntMin 0.0
Epoch 5, Class Loss=0.30109062790870667, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.30109062790870667, Class Loss=0.30109062790870667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=4.900668802857399
Loss made of: CE 0.29896092414855957, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4580864906311035 EntMin 0.0
Epoch 6, Class Loss=0.32306233048439026, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.32306233048439026, Class Loss=0.32306233048439026, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/13, Loss=6.888946014642715
Loss made of: CE 0.5474376678466797, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3702287673950195 EntMin 0.0
Epoch 1, Class Loss=0.5813345909118652, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.5813345909118652, Class Loss=0.5813345909118652, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/13, Loss=5.977392241358757
Loss made of: CE 0.4456271529197693, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.119450569152832 EntMin 0.0
Epoch 2, Class Loss=0.485321581363678, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.485321581363678, Class Loss=0.485321581363678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.50687826871872
Loss made of: CE 0.34677833318710327, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.635498523712158 EntMin 0.0
Epoch 3, Class Loss=0.41171181201934814, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.41171181201934814, Class Loss=0.41171181201934814, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/13, Loss=5.520536640286446
Loss made of: CE 0.3493689000606537, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.844887733459473 EntMin 0.0
Epoch 4, Class Loss=0.366227924823761, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.366227924823761, Class Loss=0.366227924823761, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/13, Loss=5.397502738237381
Loss made of: CE 0.39576101303100586, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.886636734008789 EntMin 0.0
Epoch 5, Class Loss=0.3448176383972168, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.3448176383972168, Class Loss=0.3448176383972168, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/13, Loss=5.261522227525711
Loss made of: CE 0.34562623500823975, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.574557781219482 EntMin 0.0
Epoch 6, Class Loss=0.3189227283000946, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.3189227283000946, Class Loss=0.3189227283000946, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4036884307861328, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.888476
Mean Acc: 0.694878
FreqW Acc: 0.815295
Mean IoU: 0.558220
Class IoU:
	class 0: 0.8931388
	class 1: 0.8533548
	class 2: 0.2907704
	class 3: 0.76553255
	class 4: 0.6736576
	class 5: 0.7242014
	class 6: 0.77193844
	class 7: 0.8456188
	class 8: 0.7785574
	class 9: 0.16915691
	class 10: 0.0
	class 11: 0.0035805968
	class 12: 0.48735482
Class Acc:
	class 0: 0.95026684
	class 1: 0.89678454
	class 2: 0.92987204
	class 3: 0.7897561
	class 4: 0.77678496
	class 5: 0.9411393
	class 6: 0.77823746
	class 7: 0.9274987
	class 8: 0.96307325
	class 9: 0.32680947
	class 10: 0.0
	class 11: 0.0035813558
	class 12: 0.7496097

federated global round: 14, step: 2
select part of clients to conduct local training
[17, 3, 12, 16]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/29, Loss=7.196000731736421
Loss made of: CE 0.010602275840938091, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.976461410522461 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.291553355194628
Loss made of: CE 0.01196188386529684, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.989894390106201 EntMin 0.0
Epoch 1, Class Loss=0.011433747597038746, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.011433747597038746, Class Loss=0.011433747597038746, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=6.826026366837323
Loss made of: CE 0.025108790025115013, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.844606399536133 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.540911707095802
Loss made of: CE 0.019676033407449722, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.501155376434326 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Class Loss=0.033214911818504333, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.033214911818504333, Class Loss=0.033214911818504333, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=6.235342640429735
Loss made of: CE 0.06646190583705902, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.725266933441162 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.638108070567251
Loss made of: CE 0.05487486347556114, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.055501937866211 EntMin 0.0
Epoch 3, Class Loss=0.063746877014637, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.063746877014637, Class Loss=0.063746877014637, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=6.321062955260277
Loss made of: CE 0.08848544210195541, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.318383693695068 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.391350875794887
Loss made of: CE 0.10043176263570786, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.3495635986328125 EntMin 0.0
Epoch 4, Class Loss=0.09867461025714874, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.09867461025714874, Class Loss=0.09867461025714874, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.449788817018271
Loss made of: CE 0.12366937100887299, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.646214962005615 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.2529435835778715
Loss made of: CE 0.13978692889213562, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.757894515991211 EntMin 0.0
Epoch 5, Class Loss=0.14568310976028442, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.14568310976028442, Class Loss=0.14568310976028442, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.440528513491154
Loss made of: CE 0.19405078887939453, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.159204483032227 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.229019369184971
Loss made of: CE 0.15962961316108704, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.375085353851318 EntMin 0.0
Epoch 6, Class Loss=0.18242864310741425, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.18242864310741425, Class Loss=0.18242864310741425, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/29, Loss=7.486504263011739
Loss made of: CE 0.015430731698870659, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.5777907371521 EntMin 0.0
Epoch 1, Batch 20/29, Loss=6.744120969669893
Loss made of: CE 0.005851530935615301, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.464370250701904 EntMin 0.0
Epoch 1, Class Loss=0.012433873489499092, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.012433873489499092, Class Loss=0.012433873489499092, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/29, Loss=6.603032836318016
Loss made of: CE 0.025980478152632713, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.299487590789795 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.603933796100319
Loss made of: CE 0.08323335647583008, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.854062080383301 EntMin 0.0
Epoch 2, Class Loss=0.03450486809015274, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.03450486809015274, Class Loss=0.03450486809015274, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/29, Loss=6.52725323215127
Loss made of: CE 0.06992623209953308, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.909883499145508 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.647949808835984
Loss made of: CE 0.09206770360469818, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.752349376678467 EntMin 0.0
Epoch 3, Class Loss=0.06757517904043198, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.06757517904043198, Class Loss=0.06757517904043198, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/29, Loss=6.7194541864097115
Loss made of: CE 0.11893332004547119, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.1841254234313965 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.268054916709661
Loss made of: CE 0.1270921230316162, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.940276145935059 EntMin 0.0
Epoch 4, Class Loss=0.10813245922327042, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.10813245922327042, Class Loss=0.10813245922327042, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/29, Loss=6.34149494022131
Loss made of: CE 0.15836605429649353, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.422294616699219 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.419604789465666
Loss made of: CE 0.1498413383960724, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.7584686279296875 EntMin 0.0
Epoch 5, Class Loss=0.13938479125499725, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.13938479125499725, Class Loss=0.13938479125499725, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/29, Loss=6.320983074605465
Loss made of: CE 0.16650432348251343, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.911530494689941 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.070019413530827
Loss made of: CE 0.22437211871147156, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.505955696105957 EntMin 0.0
Epoch 6, Class Loss=0.17801029980182648, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.17801029980182648, Class Loss=0.17801029980182648, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/13, Loss=6.177095141261816
Loss made of: CE 0.0636591762304306, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.154975414276123 EntMin 0.0
Epoch 1, Class Loss=0.10416845232248306, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.10416845232248306, Class Loss=0.10416845232248306, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.555062249302864
Loss made of: CE 0.17335638403892517, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.057435989379883 EntMin 0.0
Epoch 2, Class Loss=0.18449987471103668, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.18449987471103668, Class Loss=0.18449987471103668, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/13, Loss=5.3961733192205426
Loss made of: CE 0.22065794467926025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9238600730896 EntMin 0.0
Epoch 3, Class Loss=0.24408911168575287, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.24408911168575287, Class Loss=0.24408911168575287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.1898696050047874
Loss made of: CE 0.22929805517196655, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.998251914978027 EntMin 0.0
Epoch 4, Class Loss=0.2779202163219452, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.2779202163219452, Class Loss=0.2779202163219452, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.1578055083751675
Loss made of: CE 0.4158932566642761, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.87802791595459 EntMin 0.0
Epoch 5, Class Loss=0.3164403736591339, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.3164403736591339, Class Loss=0.3164403736591339, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.1947883695364
Loss made of: CE 0.3411773443222046, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.706456184387207 EntMin 0.0
Epoch 6, Class Loss=0.34287187457084656, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.34287187457084656, Class Loss=0.34287187457084656, Reg Loss=0.0
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/29, Loss=7.837697926163673
Loss made of: CE 0.4332472085952759, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.306092262268066 EntMin 0.0
Epoch 1, Batch 20/29, Loss=7.147866371273994
Loss made of: CE 0.36935147643089294, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.383310317993164 EntMin 0.0
Epoch 1, Class Loss=0.4151010513305664, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.4151010513305664, Class Loss=0.4151010513305664, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/29, Loss=6.772399201989174
Loss made of: CE 0.29356175661087036, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.30046272277832 EntMin 0.0
Epoch 2, Batch 20/29, Loss=6.974268832802773
Loss made of: CE 0.27854302525520325, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.01546573638916 EntMin 0.0
Epoch 2, Class Loss=0.3116595149040222, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.3116595149040222, Class Loss=0.3116595149040222, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/29, Loss=6.805803726613521
Loss made of: CE 0.3273119330406189, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.897233486175537 EntMin 0.0
Epoch 3, Batch 20/29, Loss=6.553446164727211
Loss made of: CE 0.2410432994365692, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.8232879638671875 EntMin 0.0
Epoch 3, Class Loss=0.28300929069519043, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.28300929069519043, Class Loss=0.28300929069519043, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/29, Loss=6.571272304654121
Loss made of: CE 0.26129305362701416, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.915500640869141 EntMin 0.0
Epoch 4, Batch 20/29, Loss=6.590818756818772
Loss made of: CE 0.24771562218666077, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.494698524475098 EntMin 0.0
Epoch 4, Class Loss=0.27006033062934875, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.27006033062934875, Class Loss=0.27006033062934875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/29, Loss=6.177551655471325
Loss made of: CE 0.2711288630962372, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.9962663650512695 EntMin 0.0
Epoch 5, Batch 20/29, Loss=6.598832273483277
Loss made of: CE 0.20839759707450867, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.541423797607422 EntMin 0.0
Epoch 5, Class Loss=0.2640140950679779, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.2640140950679779, Class Loss=0.2640140950679779, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 10/29, Loss=6.262074345350266
Loss made of: CE 0.23612412810325623, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.512145519256592 EntMin 0.0
Epoch 6, Batch 20/29, Loss=6.418483839929104
Loss made of: CE 0.23424284160137177, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.453916549682617 EntMin 0.0
Epoch 6, Class Loss=0.26253294944763184, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.26253294944763184, Class Loss=0.26253294944763184, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.4029865860939026, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.887938
Mean Acc: 0.686820
FreqW Acc: 0.816936
Mean IoU: 0.550018
Class IoU:
	class 0: 0.89696574
	class 1: 0.8545256
	class 2: 0.30073586
	class 3: 0.5888805
	class 4: 0.6675362
	class 5: 0.74986607
	class 6: 0.7645313
	class 7: 0.84910136
	class 8: 0.81913644
	class 9: 0.18317309
	class 10: 0.0
	class 11: 2.396813e-05
	class 12: 0.47576174
Class Acc:
	class 0: 0.94871366
	class 1: 0.89397407
	class 2: 0.9223371
	class 3: 0.59771276
	class 4: 0.77960145
	class 5: 0.92747396
	class 6: 0.77042234
	class 7: 0.9301164
	class 8: 0.9349722
	class 9: 0.334939
	class 10: 0.0
	class 11: 2.396813e-05
	class 12: 0.8883741

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[13, 9, 6, 5]
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=8.947579038143157
Loss made of: CE 0.16912829875946045, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.944821357727051 EntMin 0.0
Epoch 1, Class Loss=0.19444437325000763, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.19444437325000763, Class Loss=0.19444437325000763, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=7.78967769742012
Loss made of: CE 0.35737285017967224, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.012638092041016 EntMin 0.0
Epoch 2, Class Loss=0.36799493432044983, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.36799493432044983, Class Loss=0.36799493432044983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.4055554360151294
Loss made of: CE 0.4748588800430298, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.058465003967285 EntMin 0.0
Epoch 3, Class Loss=0.48333972692489624, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.48333972692489624, Class Loss=0.48333972692489624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/11, Loss=7.038345330953598
Loss made of: CE 0.3944419324398041, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.534024715423584 EntMin 0.0
Epoch 4, Class Loss=0.4967598617076874, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.4967598617076874, Class Loss=0.4967598617076874, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Batch 10/11, Loss=6.858766031265259
Loss made of: CE 0.5335942506790161, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.453911781311035 EntMin 0.0
Epoch 5, Class Loss=0.5207010507583618, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.5207010507583618, Class Loss=0.5207010507583618, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.683355236053467
Loss made of: CE 0.439943790435791, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.10274076461792 EntMin 0.0
Epoch 6, Class Loss=0.48892903327941895, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.48892903327941895, Class Loss=0.48892903327941895, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=11.468179999664425
Loss made of: CE 0.05694691836833954, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.109212875366211 EntMin 0.0
Epoch 1, Class Loss=0.07067428529262543, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.07067428529262543, Class Loss=0.07067428529262543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=9.710590274631977
Loss made of: CE 0.3109416961669922, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.79659366607666 EntMin 0.0
Epoch 2, Class Loss=0.19018691778182983, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.19018691778182983, Class Loss=0.19018691778182983, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=8.907508678734303
Loss made of: CE 0.31641826033592224, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.385666847229004 EntMin 0.0
Epoch 3, Class Loss=0.34346434473991394, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.34346434473991394, Class Loss=0.34346434473991394, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.459819290041924
Loss made of: CE 0.33908116817474365, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.065366268157959 EntMin 0.0
Epoch 4, Class Loss=0.4407908320426941, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.4407908320426941, Class Loss=0.4407908320426941, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.482509699463844
Loss made of: CE 0.5382811427116394, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.252513885498047 EntMin 0.0
Epoch 5, Class Loss=0.5193061232566833, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5193061232566833, Class Loss=0.5193061232566833, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=8.124919280409813
Loss made of: CE 0.465625524520874, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.454249382019043 EntMin 0.0
Epoch 6, Class Loss=0.4880307912826538, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.4880307912826538, Class Loss=0.4880307912826538, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=9.079151370376348
Loss made of: CE 0.1642187237739563, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.047773361206055 EntMin 0.0
Epoch 1, Class Loss=0.20086009800434113, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.20086009800434113, Class Loss=0.20086009800434113, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/11, Loss=7.990640991926194
Loss made of: CE 0.28798115253448486, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.375298976898193 EntMin 0.0
Epoch 2, Class Loss=0.3832937777042389, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.3832937777042389, Class Loss=0.3832937777042389, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/11, Loss=7.439850828051567
Loss made of: CE 0.36501526832580566, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.053301811218262 EntMin 0.0
Epoch 3, Class Loss=0.4777419865131378, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4777419865131378, Class Loss=0.4777419865131378, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/11, Loss=7.138230416178703
Loss made of: CE 0.4745790958404541, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.252885341644287 EntMin 0.0
Epoch 4, Class Loss=0.5190569758415222, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.5190569758415222, Class Loss=0.5190569758415222, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/11, Loss=6.911228412389756
Loss made of: CE 0.49922436475753784, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.6760149002075195 EntMin 0.0
Epoch 5, Class Loss=0.5161722302436829, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.5161722302436829, Class Loss=0.5161722302436829, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/11, Loss=6.863550782203674
Loss made of: CE 0.45313066244125366, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.711560249328613 EntMin 0.0
Epoch 6, Class Loss=0.49390265345573425, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.49390265345573425, Class Loss=0.49390265345573425, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=11.322193842753768
Loss made of: CE 0.05874243378639221, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.764598846435547 EntMin 0.0
Epoch 1, Class Loss=0.06313979625701904, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.06313979625701904, Class Loss=0.06313979625701904, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=9.566254403442144
Loss made of: CE 0.21797554194927216, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.319713592529297 EntMin 0.0
Epoch 2, Class Loss=0.19283895194530487, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.19283895194530487, Class Loss=0.19283895194530487, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=8.845752209424973
Loss made of: CE 0.32308274507522583, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.41960334777832 EntMin 0.0
Epoch 3, Class Loss=0.3323321044445038, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.3323321044445038, Class Loss=0.3323321044445038, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=8.588206934928895
Loss made of: CE 0.45493239164352417, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.783656120300293 EntMin 0.0
Epoch 4, Class Loss=0.4465782344341278, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.4465782344341278, Class Loss=0.4465782344341278, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=8.403399720788002
Loss made of: CE 0.49460721015930176, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.903611660003662 EntMin 0.0
Epoch 5, Class Loss=0.4932658076286316, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.4932658076286316, Class Loss=0.4932658076286316, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=8.008184814453125
Loss made of: CE 0.4653533399105072, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.446042537689209 EntMin 0.0
Epoch 6, Class Loss=0.4935513138771057, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.4935513138771057, Class Loss=0.4935513138771057, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6254423260688782, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.849401
Mean Acc: 0.488146
FreqW Acc: 0.750858
Mean IoU: 0.394499
Class IoU:
	class 0: 0.869373
	class 1: 0.6693547
	class 2: 0.27803668
	class 3: 0.47885314
	class 4: 0.5491459
	class 5: 0.6938429
	class 6: 0.35096964
	class 7: 0.8022805
	class 8: 0.7657808
	class 9: 0.10116145
	class 10: 0.0
	class 11: 0.0008024485
	class 12: 0.35787883
	class 13: 0.0
	class 14: 0.0
Class Acc:
	class 0: 0.9705905
	class 1: 0.6892943
	class 2: 0.76562876
	class 3: 0.488112
	class 4: 0.62313795
	class 5: 0.7492274
	class 6: 0.35178453
	class 7: 0.9032636
	class 8: 0.85721654
	class 9: 0.21125244
	class 10: 0.0
	class 11: 0.0008024624
	class 12: 0.71187884
	class 13: 0.0
	class 14: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[18, 5, 20, 0]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.511720626801253
Loss made of: CE 0.026324227452278137, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.731602668762207 EntMin 0.0
Epoch 1, Class Loss=0.034875303506851196, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.034875303506851196, Class Loss=0.034875303506851196, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=8.004437850415707
Loss made of: CE 0.15116217732429504, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.966930389404297 EntMin 0.0
Epoch 2, Class Loss=0.1098114401102066, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.1098114401102066, Class Loss=0.1098114401102066, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.677790570259094
Loss made of: CE 0.24200305342674255, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.391040325164795 EntMin 0.0
Epoch 3, Class Loss=0.2264760434627533, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.2264760434627533, Class Loss=0.2264760434627533, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.573526591062546
Loss made of: CE 0.3021582067012787, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.705909729003906 EntMin 0.0
Epoch 4, Class Loss=0.33545732498168945, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.33545732498168945, Class Loss=0.33545732498168945, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.4430757611989975
Loss made of: CE 0.34890222549438477, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.116670608520508 EntMin 0.0
Epoch 5, Class Loss=0.35914015769958496, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.35914015769958496, Class Loss=0.35914015769958496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.318504655361176
Loss made of: CE 0.40488165616989136, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.716151714324951 EntMin 0.0
Epoch 6, Class Loss=0.3817596137523651, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.3817596137523651, Class Loss=0.3817596137523651, Reg Loss=0.0
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/12, Loss=9.441228008270263
Loss made of: CE 1.0188568830490112, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.868297576904297 EntMin 0.0
Epoch 1, Class Loss=1.0640367269515991, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=1.0640367269515991, Class Loss=1.0640367269515991, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=8.621290093660354
Loss made of: CE 0.7685114741325378, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.191221237182617 EntMin 0.0
Epoch 2, Class Loss=0.7997388243675232, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.7997388243675232, Class Loss=0.7997388243675232, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=8.245421051979065
Loss made of: CE 0.6003204584121704, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.892864227294922 EntMin 0.0
Epoch 3, Class Loss=0.6035661101341248, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.6035661101341248, Class Loss=0.6035661101341248, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/12, Loss=7.898031949996948
Loss made of: CE 0.4796062409877777, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.28343391418457 EntMin 0.0
Epoch 4, Class Loss=0.489746630191803, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.489746630191803, Class Loss=0.489746630191803, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.753657728433609
Loss made of: CE 0.40740567445755005, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.584790229797363 EntMin 0.0
Epoch 5, Class Loss=0.42981797456741333, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.42981797456741333, Class Loss=0.42981797456741333, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=7.5117973446846005
Loss made of: CE 0.451863169670105, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.948354721069336 EntMin 0.0
Epoch 6, Class Loss=0.40077656507492065, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.40077656507492065, Class Loss=0.40077656507492065, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.401070112362504
Loss made of: CE 0.03267735242843628, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.659335136413574 EntMin 0.0
Epoch 1, Class Loss=0.03644700348377228, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.03644700348377228, Class Loss=0.03644700348377228, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=7.897344909608364
Loss made of: CE 0.09167172014713287, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.435894012451172 EntMin 0.0
Epoch 2, Class Loss=0.10187456011772156, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.10187456011772156, Class Loss=0.10187456011772156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=7.847191287577152
Loss made of: CE 0.17985253036022186, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.941823482513428 EntMin 0.0
Epoch 3, Class Loss=0.2142588347196579, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.2142588347196579, Class Loss=0.2142588347196579, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=7.643672332167625
Loss made of: CE 0.3024512529373169, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.274024963378906 EntMin 0.0
Epoch 4, Class Loss=0.32589191198349, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.32589191198349, Class Loss=0.32589191198349, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=7.546826484799385
Loss made of: CE 0.2814704179763794, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.302506446838379 EntMin 0.0
Epoch 5, Class Loss=0.39412304759025574, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.39412304759025574, Class Loss=0.39412304759025574, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.272668242454529
Loss made of: CE 0.3412199914455414, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.076052665710449 EntMin 0.0
Epoch 6, Class Loss=0.3770121932029724, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.3770121932029724, Class Loss=0.3770121932029724, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=7.341559311747551
Loss made of: CE 0.1163562759757042, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.981807708740234 EntMin 0.0
Epoch 1, Class Loss=0.10911861807107925, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.10911861807107925, Class Loss=0.10911861807107925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/11, Loss=6.7749706819653515
Loss made of: CE 0.2042466402053833, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.938485622406006 EntMin 0.0
Epoch 2, Class Loss=0.22715096175670624, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.22715096175670624, Class Loss=0.22715096175670624, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/11, Loss=6.58238667845726
Loss made of: CE 0.3618695139884949, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.485167503356934 EntMin 0.0
Epoch 3, Class Loss=0.33552736043930054, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.33552736043930054, Class Loss=0.33552736043930054, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/11, Loss=6.590222847461701
Loss made of: CE 0.33976906538009644, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.831893444061279 EntMin 0.0
Epoch 4, Class Loss=0.3805810809135437, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3805810809135437, Class Loss=0.3805810809135437, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/11, Loss=6.424668830633164
Loss made of: CE 0.3876180648803711, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.473616600036621 EntMin 0.0
Epoch 5, Class Loss=0.3953809440135956, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.3953809440135956, Class Loss=0.3953809440135956, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/11, Loss=6.275823378562928
Loss made of: CE 0.4016141891479492, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.760720252990723 EntMin 0.0
Epoch 6, Class Loss=0.38075941801071167, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.38075941801071167, Class Loss=0.38075941801071167, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5326003432273865, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.862622
Mean Acc: 0.519717
FreqW Acc: 0.763432
Mean IoU: 0.441961
Class IoU:
	class 0: 0.8685883
	class 1: 0.701052
	class 2: 0.3153579
	class 3: 0.47534764
	class 4: 0.52661276
	class 5: 0.6622588
	class 6: 0.53151864
	class 7: 0.8172499
	class 8: 0.75165546
	class 9: 0.09745274
	class 10: 0.0
	class 11: 0.00022464248
	class 12: 0.34949437
	class 13: 0.0
	class 14: 0.53260064
Class Acc:
	class 0: 0.97835565
	class 1: 0.71734005
	class 2: 0.76126456
	class 3: 0.47936755
	class 4: 0.5702473
	class 5: 0.7023006
	class 6: 0.53366494
	class 7: 0.9114746
	class 8: 0.8112611
	class 9: 0.14147219
	class 10: 0.0
	class 11: 0.00022464248
	class 12: 0.56399864
	class 13: 0.0
	class 14: 0.624777

federated global round: 17, step: 3
select part of clients to conduct local training
[1, 16, 6, 21]
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.02440948858857
Loss made of: CE 0.10300024598836899, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.337440490722656 EntMin 0.0
Epoch 1, Class Loss=0.10442432016134262, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.10442432016134262, Class Loss=0.10442432016134262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=6.737656277418137
Loss made of: CE 0.19156192243099213, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.557115077972412 EntMin 0.0
Epoch 2, Class Loss=0.2320493906736374, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.2320493906736374, Class Loss=0.2320493906736374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.5134278446435925
Loss made of: CE 0.31558942794799805, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.500539302825928 EntMin 0.0
Epoch 3, Class Loss=0.30883458256721497, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.30883458256721497, Class Loss=0.30883458256721497, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.369606927037239
Loss made of: CE 0.30525434017181396, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.667240142822266 EntMin 0.0
Epoch 4, Class Loss=0.3860641419887543, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.3860641419887543, Class Loss=0.3860641419887543, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.156394809484482
Loss made of: CE 0.39661359786987305, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4102983474731445 EntMin 0.0
Epoch 5, Class Loss=0.37573593854904175, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.37573593854904175, Class Loss=0.37573593854904175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=6.071419954299927
Loss made of: CE 0.3571586012840271, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.077917098999023 EntMin 0.0
Epoch 6, Class Loss=0.36489138007164, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.36489138007164, Class Loss=0.36489138007164, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=7.869734610617161
Loss made of: CE 0.09972162544727325, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.549411773681641 EntMin 0.0
Epoch 1, Class Loss=0.10449899733066559, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.10449899733066559, Class Loss=0.10449899733066559, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/11, Loss=6.733775691688061
Loss made of: CE 0.22960907220840454, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.880929470062256 EntMin 0.0
Epoch 2, Class Loss=0.23599159717559814, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.23599159717559814, Class Loss=0.23599159717559814, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/11, Loss=6.554058414697647
Loss made of: CE 0.253970742225647, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.78453254699707 EntMin 0.0
Epoch 3, Class Loss=0.32133224606513977, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.32133224606513977, Class Loss=0.32133224606513977, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/11, Loss=6.475046023726463
Loss made of: CE 0.36127573251724243, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.522729396820068 EntMin 0.0
Epoch 4, Class Loss=0.3732953667640686, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.3732953667640686, Class Loss=0.3732953667640686, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/11, Loss=6.267314693331718
Loss made of: CE 0.30033135414123535, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.655190944671631 EntMin 0.0
Epoch 5, Class Loss=0.3666703701019287, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.3666703701019287, Class Loss=0.3666703701019287, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/11, Loss=6.008066183328628
Loss made of: CE 0.3785717189311981, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.264080047607422 EntMin 0.0
Epoch 6, Class Loss=0.355552077293396, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.355552077293396, Class Loss=0.355552077293396, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=8.767971402406692
Loss made of: CE 0.733421266078949, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.012598991394043 EntMin 0.0
Epoch 1, Class Loss=0.733230710029602, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.733230710029602, Class Loss=0.733230710029602, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/11, Loss=7.205224657058716
Loss made of: CE 0.6032229661941528, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.747180938720703 EntMin 0.0
Epoch 2, Class Loss=0.5939803123474121, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.5939803123474121, Class Loss=0.5939803123474121, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/11, Loss=6.760963833332061
Loss made of: CE 0.4531802535057068, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.486997604370117 EntMin 0.0
Epoch 3, Class Loss=0.4622299075126648, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.4622299075126648, Class Loss=0.4622299075126648, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/11, Loss=6.4436792731285095
Loss made of: CE 0.3965885639190674, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.874740123748779 EntMin 0.0
Epoch 4, Class Loss=0.38383594155311584, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.38383594155311584, Class Loss=0.38383594155311584, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/11, Loss=6.285540041327477
Loss made of: CE 0.31221771240234375, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.197981834411621 EntMin 0.0
Epoch 5, Class Loss=0.324945330619812, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.324945330619812, Class Loss=0.324945330619812, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/11, Loss=6.124077421426773
Loss made of: CE 0.25317662954330444, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.343200206756592 EntMin 0.0
Epoch 6, Class Loss=0.29010066390037537, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.29010066390037537, Class Loss=0.29010066390037537, Reg Loss=0.0
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=7.246151367481798
Loss made of: CE 0.020147766917943954, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.415124893188477 EntMin 0.0
Epoch 1, Class Loss=0.015269077382981777, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.015269077382981777, Class Loss=0.015269077382981777, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=7.0344582188874485
Loss made of: CE 0.07024557888507843, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.351315498352051 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.05457587167620659, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.05457587167620659, Class Loss=0.05457587167620659, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=7.11146702170372
Loss made of: CE 0.1135181114077568, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.7831244468688965 EntMin 0.0
Epoch 3, Class Loss=0.12000332772731781, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.12000332772731781, Class Loss=0.12000332772731781, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=7.081876148283482
Loss made of: CE 0.2113167941570282, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.218761444091797 EntMin 0.0
Epoch 4, Class Loss=0.18423005938529968, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.18423005938529968, Class Loss=0.18423005938529968, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=6.798026065528393
Loss made of: CE 0.23635980486869812, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.745673179626465 EntMin 0.0
Epoch 5, Class Loss=0.23358413577079773, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.23358413577079773, Class Loss=0.23358413577079773, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=7.027257910370826
Loss made of: CE 0.25584346055984497, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.432122230529785 EntMin 0.0
Epoch 6, Class Loss=0.26099368929862976, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.26099368929862976, Class Loss=0.26099368929862976, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5193955898284912, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.855303
Mean Acc: 0.515895
FreqW Acc: 0.764117
Mean IoU: 0.423343
Class IoU:
	class 0: 0.8762837
	class 1: 0.6953317
	class 2: 0.31468347
	class 3: 0.5146646
	class 4: 0.5124561
	class 5: 0.6796248
	class 6: 0.5240172
	class 7: 0.8037353
	class 8: 0.71503675
	class 9: 0.11902904
	class 10: 0.0
	class 11: 0.0010494178
	class 12: 0.38114476
	class 13: 0.15320897
	class 14: 0.059874047
Class Acc:
	class 0: 0.9718129
	class 1: 0.7081081
	class 2: 0.8394392
	class 3: 0.5189085
	class 4: 0.5581535
	class 5: 0.72080773
	class 6: 0.5261802
	class 7: 0.92898583
	class 8: 0.7556978
	class 9: 0.2545836
	class 10: 0.0
	class 11: 0.0010494281
	class 12: 0.6146263
	class 13: 0.27994797
	class 14: 0.06013068

federated global round: 18, step: 3
select part of clients to conduct local training
[8, 2, 17, 14]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=5.852772789075971
Loss made of: CE 0.06144016981124878, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.510981559753418 EntMin 0.0
Epoch 1, Class Loss=0.054178427904844284, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.054178427904844284, Class Loss=0.054178427904844284, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/11, Loss=5.624436964094639
Loss made of: CE 0.09809361398220062, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.102543830871582 EntMin 0.0
Epoch 2, Class Loss=0.11788986623287201, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.11788986623287201, Class Loss=0.11788986623287201, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.703804837167263
Loss made of: CE 0.1542922854423523, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.516785621643066 EntMin 0.0
Epoch 3, Class Loss=0.17574843764305115, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.17574843764305115, Class Loss=0.17574843764305115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.534300073981285
Loss made of: CE 0.1893734633922577, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.153285980224609 EntMin 0.0
Epoch 4, Class Loss=0.2141307145357132, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.2141307145357132, Class Loss=0.2141307145357132, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.563991187512874
Loss made of: CE 0.19815593957901, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.688714981079102 EntMin 0.0
Epoch 5, Class Loss=0.24190257489681244, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.24190257489681244, Class Loss=0.24190257489681244, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.452546565234661
Loss made of: CE 0.23369424045085907, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.674632549285889 EntMin 0.0
Epoch 6, Class Loss=0.25292110443115234, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.25292110443115234, Class Loss=0.25292110443115234, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=6.138241826370359
Loss made of: CE 0.03777055814862251, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.611627101898193 EntMin 0.0
Epoch 1, Class Loss=0.05439376085996628, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.05439376085996628, Class Loss=0.05439376085996628, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=5.834812885522842
Loss made of: CE 0.13181351125240326, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.622803688049316 EntMin 0.0
Epoch 2, Class Loss=0.11945509910583496, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.11945509910583496, Class Loss=0.11945509910583496, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.8039216443896295
Loss made of: CE 0.16755422949790955, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.032517433166504 EntMin 0.0
Epoch 3, Class Loss=0.17103131115436554, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.17103131115436554, Class Loss=0.17103131115436554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.80077860802412
Loss made of: CE 0.21569910645484924, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.479135036468506 EntMin 0.0
Epoch 4, Class Loss=0.22149275243282318, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.22149275243282318, Class Loss=0.22149275243282318, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.698057147860527
Loss made of: CE 0.2435566633939743, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.548800945281982 EntMin 0.0
Epoch 5, Class Loss=0.24557583034038544, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.24557583034038544, Class Loss=0.24557583034038544, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.604130056500435
Loss made of: CE 0.26277679204940796, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.916303634643555 EntMin 0.0
Epoch 6, Class Loss=0.2602323293685913, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.2602323293685913, Class Loss=0.2602323293685913, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.492467440385372
Loss made of: CE 0.013320647180080414, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.481067180633545 EntMin 0.0
Epoch 1, Class Loss=0.02335735224187374, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.02335735224187374, Class Loss=0.02335735224187374, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=7.449037907272578
Loss made of: CE 0.07478754967451096, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.422406196594238 EntMin 0.0
Epoch 2, Class Loss=0.0760728195309639, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.0760728195309639, Class Loss=0.0760728195309639, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=7.2549127146601675
Loss made of: CE 0.15923842787742615, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.6305036544799805 EntMin 0.0
Epoch 3, Class Loss=0.14269810914993286, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.14269810914993286, Class Loss=0.14269810914993286, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=7.256992602348328
Loss made of: CE 0.27169403433799744, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.725054740905762 EntMin 0.0
Epoch 4, Class Loss=0.2290707528591156, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.2290707528591156, Class Loss=0.2290707528591156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=6.888151380419731
Loss made of: CE 0.22381186485290527, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.409690856933594 EntMin 0.0
Epoch 5, Class Loss=0.2509509325027466, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.2509509325027466, Class Loss=0.2509509325027466, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=7.047451029717922
Loss made of: CE 0.23649023473262787, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.579035758972168 EntMin 0.0
Epoch 6, Class Loss=0.28930792212486267, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.28930792212486267, Class Loss=0.28930792212486267, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/11, Loss=6.121066784486175
Loss made of: CE 0.04227367043495178, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.176098823547363 EntMin 0.0
Epoch 1, Class Loss=0.04868590459227562, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.04868590459227562, Class Loss=0.04868590459227562, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/11, Loss=5.9431636415421965
Loss made of: CE 0.12484654039144516, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.882274627685547 EntMin 0.0
Epoch 2, Class Loss=0.11497587710618973, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.11497587710618973, Class Loss=0.11497587710618973, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/11, Loss=5.911386689543724
Loss made of: CE 0.16158868372440338, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.711845397949219 EntMin 0.0
Epoch 3, Class Loss=0.16664588451385498, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.16664588451385498, Class Loss=0.16664588451385498, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/11, Loss=5.8426836162805555
Loss made of: CE 0.19118821620941162, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.221835136413574 EntMin 0.0
Epoch 4, Class Loss=0.21781852841377258, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.21781852841377258, Class Loss=0.21781852841377258, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/11, Loss=5.687228527665138
Loss made of: CE 0.2165321558713913, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0412139892578125 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 5, Class Loss=0.24391667544841766, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.24391667544841766, Class Loss=0.24391667544841766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/11, Loss=5.774766851961613
Loss made of: CE 0.24652592837810516, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.179875373840332 EntMin 0.0
Epoch 6, Class Loss=0.27161669731140137, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.27161669731140137, Class Loss=0.27161669731140137, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5253040790557861, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.854137
Mean Acc: 0.540111
FreqW Acc: 0.771437
Mean IoU: 0.431793
Class IoU:
	class 0: 0.88232917
	class 1: 0.7260667
	class 2: 0.31616333
	class 3: 0.4873849
	class 4: 0.5270123
	class 5: 0.70021474
	class 6: 0.5834384
	class 7: 0.79380494
	class 8: 0.70388585
	class 9: 0.13916488
	class 10: 0.0
	class 11: 0.0004088681
	class 12: 0.42764717
	class 13: 0.18938094
	class 14: 0.0
Class Acc:
	class 0: 0.96234375
	class 1: 0.74026394
	class 2: 0.8466366
	class 3: 0.4913486
	class 4: 0.5766503
	class 5: 0.74551785
	class 6: 0.5862517
	class 7: 0.937353
	class 8: 0.7399875
	class 9: 0.27053663
	class 10: 0.0
	class 11: 0.0004088681
	class 12: 0.59341484
	class 13: 0.61094475
	class 14: 0.0

federated global round: 19, step: 3
select part of clients to conduct local training
[1, 9, 8, 0]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/11, Loss=6.002622073888778
Loss made of: CE 0.3217312693595886, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.472461700439453 EntMin 0.0
Epoch 1, Class Loss=0.3429396450519562, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.3429396450519562, Class Loss=0.3429396450519562, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/11, Loss=5.692075324058533
Loss made of: CE 0.31328269839286804, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.552838325500488 EntMin 0.0
Epoch 2, Class Loss=0.31645113229751587, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.31645113229751587, Class Loss=0.31645113229751587, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/11, Loss=5.633617696166039
Loss made of: CE 0.286350816488266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.754579067230225 EntMin 0.0
Epoch 3, Class Loss=0.28949299454689026, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.28949299454689026, Class Loss=0.28949299454689026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/11, Loss=5.5951467752456665
Loss made of: CE 0.2505589425563812, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.936846733093262 EntMin 0.0
Epoch 4, Class Loss=0.2704770267009735, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.2704770267009735, Class Loss=0.2704770267009735, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/11, Loss=5.624504141509533
Loss made of: CE 0.27373966574668884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.960574626922607 EntMin 0.0
Epoch 5, Class Loss=0.26352158188819885, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.26352158188819885, Class Loss=0.26352158188819885, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/11, Loss=5.452735550701618
Loss made of: CE 0.24501991271972656, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.625125408172607 EntMin 0.0
Epoch 6, Class Loss=0.25563693046569824, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.25563693046569824, Class Loss=0.25563693046569824, Reg Loss=0.0
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=9.255844980478287
Loss made of: CE 0.9267973899841309, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.662927627563477 EntMin 0.0
Epoch 1, Class Loss=0.930891752243042, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.930891752243042, Class Loss=0.930891752243042, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000694
Epoch 2, Batch 10/12, Loss=7.940058320760727
Loss made of: CE 0.5637401342391968, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.988751411437988 EntMin 0.0
Epoch 2, Class Loss=0.652925968170166, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.652925968170166, Class Loss=0.652925968170166, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/12, Loss=7.424941837787628
Loss made of: CE 0.49489647150039673, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.070478439331055 EntMin 0.0
Epoch 3, Class Loss=0.47864577174186707, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.47864577174186707, Class Loss=0.47864577174186707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/12, Loss=7.091497537493706
Loss made of: CE 0.36641570925712585, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.828534126281738 EntMin 0.0
Epoch 4, Class Loss=0.41477009654045105, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.41477009654045105, Class Loss=0.41477009654045105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/12, Loss=7.351921758055687
Loss made of: CE 0.40220436453819275, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.799901485443115 EntMin 0.0
Epoch 5, Class Loss=0.3966516852378845, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.3966516852378845, Class Loss=0.3966516852378845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000163
Epoch 6, Batch 10/12, Loss=7.186430218815803
Loss made of: CE 0.33510857820510864, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.853941917419434 EntMin 0.0
Epoch 6, Class Loss=0.3894527852535248, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.3894527852535248, Class Loss=0.3894527852535248, Reg Loss=0.0
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/11, Loss=5.777733892202377
Loss made of: CE 0.35212695598602295, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.005247116088867 EntMin 0.0
Epoch 1, Class Loss=0.32667288184165955, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.32667288184165955, Class Loss=0.32667288184165955, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/11, Loss=5.429372400045395
Loss made of: CE 0.3080713748931885, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.827028274536133 EntMin 0.0
Epoch 2, Class Loss=0.31040531396865845, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.31040531396865845, Class Loss=0.31040531396865845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000372
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/11, Loss=5.433526659011841
Loss made of: CE 0.26720234751701355, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.230567932128906 EntMin 0.0
Epoch 3, Class Loss=0.2941039800643921, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.2941039800643921, Class Loss=0.2941039800643921, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/11, Loss=5.358330288529396
Loss made of: CE 0.2498798668384552, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.075481414794922 EntMin 0.0
Epoch 4, Class Loss=0.2732125520706177, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.2732125520706177, Class Loss=0.2732125520706177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/11, Loss=5.402038438618183
Loss made of: CE 0.23311683535575867, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.5733537673950195 EntMin 0.0
Epoch 5, Class Loss=0.2609420418739319, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.2609420418739319, Class Loss=0.2609420418739319, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/11, Loss=5.371929214894772
Loss made of: CE 0.24998651444911957, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.387404441833496 EntMin 0.0
Epoch 6, Class Loss=0.2589780390262604, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.2589780390262604, Class Loss=0.2589780390262604, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/11, Loss=5.83898748755455
Loss made of: CE 0.35744234919548035, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.506192684173584 EntMin 0.0
Epoch 1, Class Loss=0.34528982639312744, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.34528982639312744, Class Loss=0.34528982639312744, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/11, Loss=5.7080671191215515
Loss made of: CE 0.3027687072753906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.773456573486328 EntMin 0.0
Epoch 2, Class Loss=0.30680298805236816, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.30680298805236816, Class Loss=0.30680298805236816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/11, Loss=5.689377452433109
Loss made of: CE 0.2695162296295166, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.656640529632568 EntMin 0.0
Epoch 3, Class Loss=0.2851487696170807, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.2851487696170807, Class Loss=0.2851487696170807, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/11, Loss=5.666750660538673
Loss made of: CE 0.26899081468582153, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.070807456970215 EntMin 0.0
Epoch 4, Class Loss=0.2711937427520752, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.2711937427520752, Class Loss=0.2711937427520752, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/11, Loss=5.5402446910738945
Loss made of: CE 0.26188695430755615, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.725167274475098 EntMin 0.0
Epoch 5, Class Loss=0.25766006112098694, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.25766006112098694, Class Loss=0.25766006112098694, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/11, Loss=5.573085863888264
Loss made of: CE 0.2413819283246994, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.027895927429199 EntMin 0.0
Epoch 6, Class Loss=0.25055772066116333, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.25055772066116333, Class Loss=0.25055772066116333, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5092389583587646, Reg Loss=0.0 (without scaling)

Total samples: 1140.000000
Overall Acc: 0.847414
Mean Acc: 0.509533
FreqW Acc: 0.760218
Mean IoU: 0.407752
Class IoU:
	class 0: 0.8780871
	class 1: 0.6747008
	class 2: 0.32892537
	class 3: 0.42840195
	class 4: 0.48273832
	class 5: 0.66066355
	class 6: 0.5096003
	class 7: 0.81744426
	class 8: 0.63251626
	class 9: 0.13789777
	class 10: 0.0
	class 11: 2.960769e-05
	class 12: 0.37243935
	class 13: 0.1910816
	class 14: 0.0017548305
Class Acc:
	class 0: 0.96763504
	class 1: 0.6831602
	class 2: 0.8494813
	class 3: 0.43061736
	class 4: 0.5176955
	class 5: 0.6908217
	class 6: 0.5111802
	class 7: 0.92982537
	class 8: 0.65489304
	class 9: 0.2338245
	class 10: 0.0
	class 11: 2.960769e-05
	class 12: 0.47520307
	class 13: 0.6968742
	class 14: 0.001755228

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[11, 2, 15, 6]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=8.676678264141083
Loss made of: CE 0.11995643377304077, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.227422714233398 EntMin 0.0
Epoch 1, Class Loss=0.15099838376045227, Reg Loss=0.0
Clinet index 11, End of Epoch 1/6, Average Loss=0.15099838376045227, Class Loss=0.15099838376045227, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.237151652574539
Loss made of: CE 0.3543904721736908, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.213347434997559 EntMin 0.0
Epoch 2, Class Loss=0.32038766145706177, Reg Loss=0.0
Clinet index 11, End of Epoch 2/6, Average Loss=0.32038766145706177, Class Loss=0.32038766145706177, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.55044557750225
Loss made of: CE 0.4360675513744354, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.82123327255249 EntMin 0.0
Epoch 3, Class Loss=0.43364089727401733, Reg Loss=0.0
Clinet index 11, End of Epoch 3/6, Average Loss=0.43364089727401733, Class Loss=0.43364089727401733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.223536252975464
Loss made of: CE 0.5046732425689697, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.690834045410156 EntMin 0.0
Epoch 4, Class Loss=0.5201531648635864, Reg Loss=0.0
Clinet index 11, End of Epoch 4/6, Average Loss=0.5201531648635864, Class Loss=0.5201531648635864, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.107548525929451
Loss made of: CE 0.5356223583221436, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.579404830932617 EntMin 0.0
Epoch 5, Class Loss=0.5373633503913879, Reg Loss=0.0
Clinet index 11, End of Epoch 5/6, Average Loss=0.5373633503913879, Class Loss=0.5373633503913879, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.078306710720062
Loss made of: CE 0.582152247428894, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.663186073303223 EntMin 0.0
Epoch 6, Class Loss=0.5860750675201416, Reg Loss=0.0
Clinet index 11, End of Epoch 6/6, Average Loss=0.5860750675201416, Class Loss=0.5860750675201416, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=8.634415561705827
Loss made of: CE 0.18589231371879578, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.469012260437012 EntMin 0.0
Epoch 1, Class Loss=0.13277769088745117, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.13277769088745117, Class Loss=0.13277769088745117, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.072289009392262
Loss made of: CE 0.3230656385421753, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.190708160400391 EntMin 0.0
Epoch 2, Class Loss=0.30287671089172363, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.30287671089172363, Class Loss=0.30287671089172363, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.529446429014206
Loss made of: CE 0.39191490411758423, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.253217697143555 EntMin 0.0
Epoch 3, Class Loss=0.4386768937110901, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.4386768937110901, Class Loss=0.4386768937110901, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.363498714566231
Loss made of: CE 0.3807729184627533, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.705076217651367 EntMin 0.0
Epoch 4, Class Loss=0.4972606897354126, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.4972606897354126, Class Loss=0.4972606897354126, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.050003239512444
Loss made of: CE 0.5881322622299194, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.55519962310791 EntMin 0.0
Epoch 5, Class Loss=0.5602510571479797, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5602510571479797, Class Loss=0.5602510571479797, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.145505276322365
Loss made of: CE 0.6768025159835815, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.349061012268066 EntMin 0.0
Epoch 6, Class Loss=0.5966553688049316, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.5966553688049316, Class Loss=0.5966553688049316, Reg Loss=0.0
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=8.535213869810104
Loss made of: CE 0.09395410865545273, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.21492862701416 EntMin 0.0
Epoch 1, Class Loss=0.11763779819011688, Reg Loss=0.0
Clinet index 15, End of Epoch 1/6, Average Loss=0.11763779819011688, Class Loss=0.11763779819011688, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=7.03287196457386
Loss made of: CE 0.2810605764389038, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.180601119995117 EntMin 0.0
Epoch 2, Class Loss=0.2858073115348816, Reg Loss=0.0
Clinet index 15, End of Epoch 2/6, Average Loss=0.2858073115348816, Class Loss=0.2858073115348816, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=6.527003400027752
Loss made of: CE 0.45163679122924805, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.835989952087402 EntMin 0.0
Epoch 3, Class Loss=0.41709399223327637, Reg Loss=0.0
Clinet index 15, End of Epoch 3/6, Average Loss=0.41709399223327637, Class Loss=0.41709399223327637, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=6.43890588581562
Loss made of: CE 0.4049672484397888, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.908534049987793 EntMin 0.0
Epoch 4, Class Loss=0.4958454966545105, Reg Loss=0.0
Clinet index 15, End of Epoch 4/6, Average Loss=0.4958454966545105, Class Loss=0.4958454966545105, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=6.190652719140052
Loss made of: CE 0.6598212718963623, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.882628440856934 EntMin 0.0
Epoch 5, Class Loss=0.5602554082870483, Reg Loss=0.0
Clinet index 15, End of Epoch 5/6, Average Loss=0.5602554082870483, Class Loss=0.5602554082870483, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=6.103741696476936
Loss made of: CE 0.5695434212684631, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.890263557434082 EntMin 0.0
Epoch 6, Class Loss=0.582625150680542, Reg Loss=0.0
Clinet index 15, End of Epoch 6/6, Average Loss=0.582625150680542, Class Loss=0.582625150680542, Reg Loss=0.0
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=8.023507986217737
Loss made of: CE 0.2009199559688568, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.927281379699707 EntMin 0.0
Epoch 1, Batch 20/97, Loss=7.077316395193338
Loss made of: CE 0.13294309377670288, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.788640022277832 EntMin 0.0
Epoch 1, Batch 30/97, Loss=6.1899841502308846
Loss made of: CE 0.15315212309360504, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.980486869812012 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.9611683875322345
Loss made of: CE 0.1324380487203598, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.54182767868042 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.9499329924583435
Loss made of: CE 0.09380151331424713, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.946164131164551 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.839248410239816
Loss made of: CE 0.07357759773731232, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.433098793029785 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.73712231144309
Loss made of: CE 0.10575705766677856, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.346463203430176 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.387300813943147
Loss made of: CE 0.1408807337284088, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.663680553436279 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.376977349817753
Loss made of: CE 0.0927278995513916, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.258892059326172 EntMin 0.0
Epoch 1, Class Loss=0.11352039128541946, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.11352039128541946, Class Loss=0.11352039128541946, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/97, Loss=5.601293754577637
Loss made of: CE 0.17783348262310028, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.76611328125 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.539609961211681
Loss made of: CE 0.18885068595409393, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.99949836730957 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.251990294456482
Loss made of: CE 0.17513996362686157, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.015473365783691 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.370616289228201
Loss made of: CE 0.16385075449943542, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.610673427581787 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.256531240046025
Loss made of: CE 0.16192132234573364, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.778904914855957 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.308549289405346
Loss made of: CE 0.1507953554391861, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.215259552001953 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.264306651055813
Loss made of: CE 0.1588757336139679, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.229020118713379 EntMin 0.0
Epoch 2, Batch 80/97, Loss=5.366358079761267
Loss made of: CE 0.18605855107307434, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4277753829956055 EntMin 0.0
Epoch 2, Batch 90/97, Loss=5.360944505780935
Loss made of: CE 0.112089142203331, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.292194366455078 EntMin 0.0
Epoch 2, Class Loss=0.17056092619895935, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.17056092619895935, Class Loss=0.17056092619895935, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/97, Loss=5.199208554625511
Loss made of: CE 0.15210923552513123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.752104759216309 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.213001330196858
Loss made of: CE 0.24524235725402832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.606836318969727 EntMin 0.0
Epoch 3, Batch 30/97, Loss=5.5539546951651575
Loss made of: CE 0.2762688994407654, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.489345550537109 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.34248084872961
Loss made of: CE 0.21220427751541138, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.43781852722168 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.379328040778637
Loss made of: CE 0.19160905480384827, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.801620006561279 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.325248293578625
Loss made of: CE 0.13987064361572266, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.677743911743164 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.038597437739372
Loss made of: CE 0.15627571940422058, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7552170753479 EntMin 0.0
Epoch 3, Batch 80/97, Loss=5.307079811394215
Loss made of: CE 0.15060244500637054, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.904296875 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.262188532948494
Loss made of: CE 0.1872166246175766, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.905552864074707 EntMin 0.0
Epoch 3, Class Loss=0.19201862812042236, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.19201862812042236, Class Loss=0.19201862812042236, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/97, Loss=5.162136191129685
Loss made of: CE 0.20979902148246765, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664087295532227 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.165845195949077
Loss made of: CE 0.20718374848365784, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.530389308929443 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.958097632229328
Loss made of: CE 0.15987643599510193, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.452710151672363 EntMin 0.0
Epoch 4, Batch 40/97, Loss=5.056628483533859
Loss made of: CE 0.265590637922287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8998494148254395 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.013720357418061
Loss made of: CE 0.1556212604045868, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.932153701782227 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.885653698444367
Loss made of: CE 0.20515048503875732, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8150787353515625 EntMin 0.0
Epoch 4, Batch 70/97, Loss=5.125041709840298
Loss made of: CE 0.17091034352779388, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.453558921813965 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.175841037929058
Loss made of: CE 0.2251662015914917, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.738344669342041 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.983948734402657
Loss made of: CE 0.17284318804740906, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.163173675537109 EntMin 0.0
Epoch 4, Class Loss=0.2053474336862564, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.2053474336862564, Class Loss=0.2053474336862564, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/97, Loss=5.010200138390064
Loss made of: CE 0.274669885635376, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.150893211364746 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.9144523069262505
Loss made of: CE 0.21593134105205536, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.59382438659668 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.9349035307765
Loss made of: CE 0.21957401931285858, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.177153587341309 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.009860894083976
Loss made of: CE 0.2874890863895416, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.227781295776367 EntMin 0.0
Epoch 5, Batch 50/97, Loss=5.059958991408348
Loss made of: CE 0.16887561976909637, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.926380634307861 EntMin 0.0
Epoch 5, Batch 60/97, Loss=5.041040863096714
Loss made of: CE 0.23595397174358368, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.298339366912842 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.731086806952954
Loss made of: CE 0.1834665834903717, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2342634201049805 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.925108680129052
Loss made of: CE 0.198897123336792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.065152645111084 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.909442107379436
Loss made of: CE 0.17589280009269714, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.23345422744751 EntMin 0.0
Epoch 5, Class Loss=0.2179652899503708, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.2179652899503708, Class Loss=0.2179652899503708, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/97, Loss=4.9922886043787
Loss made of: CE 0.255216121673584, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.182616233825684 EntMin 0.0
Epoch 6, Batch 20/97, Loss=5.173930439352989
Loss made of: CE 0.20710372924804688, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3412604331970215 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.886109548807144
Loss made of: CE 0.17866317927837372, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.790457248687744 EntMin 0.0
Epoch 6, Batch 40/97, Loss=5.042640148103237
Loss made of: CE 0.2557793855667114, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.933035373687744 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.857457028329373
Loss made of: CE 0.20592710375785828, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.378644943237305 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.90796467512846
Loss made of: CE 0.2130141258239746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8309326171875 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.88688093572855
Loss made of: CE 0.26748526096343994, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2168474197387695 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.837121666967869
Loss made of: CE 0.20051679015159607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.948552131652832 EntMin 0.0
Epoch 6, Batch 90/97, Loss=5.059407265484333
Loss made of: CE 0.22504256665706635, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.890371799468994 EntMin 0.0
Epoch 6, Class Loss=0.22985050082206726, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.22985050082206726, Class Loss=0.22985050082206726, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.71590256690979, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.780767
Mean Acc: 0.450655
FreqW Acc: 0.649056
Mean IoU: 0.340918
Class IoU:
	class 0: 0.8012048
	class 1: 0.6647114
	class 2: 0.30604988
	class 3: 0.3180309
	class 4: 0.43434763
	class 5: 0.6459268
	class 6: 0.36952102
	class 7: 0.7820245
	class 8: 0.6619486
	class 9: 0.17179997
	class 10: 0.0
	class 11: 5.7100548e-05
	class 12: 0.4662107
	class 13: 0.17376515
	class 14: 0.0
	class 15: 0.0
	class 16: 0.0
Class Acc:
	class 0: 0.95803833
	class 1: 0.6794796
	class 2: 0.88398445
	class 3: 0.3199359
	class 4: 0.4636333
	class 5: 0.69757754
	class 6: 0.3711232
	class 7: 0.92002296
	class 8: 0.6954562
	class 9: 0.35806748
	class 10: 0.0
	class 11: 5.7100548e-05
	class 12: 0.6130733
	class 13: 0.70068794
	class 14: 0.0
	class 15: 0.0
	class 16: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[20, 2, 24, 4]
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=6.579127360135317
Loss made of: CE 0.06212146207690239, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.930845260620117 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.305641879141331
Loss made of: CE 0.06322117894887924, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.964045524597168 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.4580249331891535
Loss made of: CE 0.07456472516059875, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.207884311676025 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.51577376164496
Loss made of: CE 0.08469781279563904, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.54970645904541 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.197107758373022
Loss made of: CE 0.05416973680257797, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4373369216918945 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.203508414328098
Loss made of: CE 0.0474318191409111, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.930049896240234 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.3397344838827845
Loss made of: CE 0.07210034877061844, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.675502300262451 EntMin 0.0
Epoch 1, Batch 80/97, Loss=5.150698936730623
Loss made of: CE 0.07540689408779144, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7141804695129395 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 90/97, Loss=5.45678575374186
Loss made of: CE 0.051458939909935, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.127574920654297 EntMin 0.0
Epoch 1, Class Loss=0.07433664798736572, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.07433664798736572, Class Loss=0.07433664798736572, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/97, Loss=5.081838211417198
Loss made of: CE 0.1312810778617859, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.960627555847168 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.093172036111355
Loss made of: CE 0.06529618799686432, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.066991329193115 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.032260574400425
Loss made of: CE 0.06826964020729065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.907412052154541 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.118055494129658
Loss made of: CE 0.07276028394699097, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.60282039642334 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.034795246273279
Loss made of: CE 0.08747369050979614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.536397457122803 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.998242915794253
Loss made of: CE 0.12662887573242188, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.880186080932617 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.376629163324833
Loss made of: CE 0.16956806182861328, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.290846824645996 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.90545142069459
Loss made of: CE 0.09053059667348862, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9315409660339355 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.937544865906238
Loss made of: CE 0.06902860105037689, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.273373126983643 EntMin 0.0
Epoch 2, Class Loss=0.12340647727251053, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.12340647727251053, Class Loss=0.12340647727251053, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/97, Loss=5.045372273027897
Loss made of: CE 0.13814899325370789, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.856020450592041 EntMin 0.0
Epoch 3, Batch 20/97, Loss=5.246944659948349
Loss made of: CE 0.15127979218959808, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.21045446395874 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.95736575871706
Loss made of: CE 0.13056206703186035, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.717990875244141 EntMin 0.0
Epoch 3, Batch 40/97, Loss=5.048251509666443
Loss made of: CE 0.2477237582206726, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.806809902191162 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.952322828769684
Loss made of: CE 0.15172265470027924, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5197882652282715 EntMin 0.0
Epoch 3, Batch 60/97, Loss=5.2819269366562365
Loss made of: CE 0.12174765765666962, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.616967678070068 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.822608685493469
Loss made of: CE 0.13367268443107605, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.768037796020508 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.9508781336247925
Loss made of: CE 0.11739468574523926, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.322813510894775 EntMin 0.0
Epoch 3, Batch 90/97, Loss=5.14700133651495
Loss made of: CE 0.09277331829071045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.436850547790527 EntMin 0.0
Epoch 3, Class Loss=0.1476907730102539, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.1476907730102539, Class Loss=0.1476907730102539, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/97, Loss=5.248572367429733
Loss made of: CE 0.211960107088089, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.784808158874512 EntMin 0.0
Epoch 4, Batch 20/97, Loss=5.040018604695797
Loss made of: CE 0.21829967200756073, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.702740669250488 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.941334673762322
Loss made of: CE 0.1830088347196579, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.326418876647949 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.79848527610302
Loss made of: CE 0.15263615548610687, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.592433929443359 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.955432034283876
Loss made of: CE 0.18900083005428314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.559361934661865 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.758449599146843
Loss made of: CE 0.18042795360088348, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.460005760192871 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.813655982911587
Loss made of: CE 0.2269822210073471, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.931982040405273 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.239614422619343
Loss made of: CE 0.17381569743156433, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.753911018371582 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.949601337313652
Loss made of: CE 0.16005367040634155, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.834150314331055 EntMin 0.0
Epoch 4, Class Loss=0.17026090621948242, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.17026090621948242, Class Loss=0.17026090621948242, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/97, Loss=4.950340342521668
Loss made of: CE 0.14488697052001953, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.547731399536133 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.899048653244972
Loss made of: CE 0.156508207321167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.875373363494873 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.962088780105114
Loss made of: CE 0.21264103055000305, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.041236400604248 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.870012779533863
Loss made of: CE 0.16556160151958466, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.526975154876709 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.920551566779613
Loss made of: CE 0.1594635248184204, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.111635684967041 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.790896382927895
Loss made of: CE 0.1838979423046112, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.627561569213867 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.7990797236561775
Loss made of: CE 0.1855110377073288, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.226513385772705 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.698373372852802
Loss made of: CE 0.19443748891353607, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.477812767028809 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.689056956768036
Loss made of: CE 0.16959846019744873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.253262996673584 EntMin 0.0
Epoch 5, Class Loss=0.1908411979675293, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.1908411979675293, Class Loss=0.1908411979675293, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/97, Loss=4.630669109523296
Loss made of: CE 0.23288694024085999, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.24753475189209 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.974038083851338
Loss made of: CE 0.24863915145397186, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.607194900512695 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.657408928871154
Loss made of: CE 0.24192452430725098, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.479949951171875 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.962138679623604
Loss made of: CE 0.2114969938993454, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.74384069442749 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.910966473817825
Loss made of: CE 0.20511530339717865, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363846778869629 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.738575717806816
Loss made of: CE 0.20587341487407684, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.453649520874023 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.709053052961826
Loss made of: CE 0.27774369716644287, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5276994705200195 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.844150476157665
Loss made of: CE 0.21565930545330048, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.602138519287109 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.634263455867767
Loss made of: CE 0.22220754623413086, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.225467681884766 EntMin 0.0
Epoch 6, Class Loss=0.21514232456684113, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.21514232456684113, Class Loss=0.21514232456684113, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.933453199267388
Loss made of: CE 0.7769041061401367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.747223377227783 EntMin 0.0
Epoch 1, Class Loss=0.6791259050369263, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.6791259050369263, Class Loss=0.6791259050369263, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=5.781080734729767
Loss made of: CE 0.5443433523178101, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.629870414733887 EntMin 0.0
Epoch 2, Class Loss=0.6219382286071777, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.6219382286071777, Class Loss=0.6219382286071777, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=5.624556320905685
Loss made of: CE 0.5236378312110901, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.230360984802246 EntMin 0.0
Epoch 3, Class Loss=0.5820790529251099, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.5820790529251099, Class Loss=0.5820790529251099, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 10/12, Loss=5.63155779838562
Loss made of: CE 0.5096174478530884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.828728675842285 EntMin 0.0
Epoch 4, Class Loss=0.5629750490188599, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.5629750490188599, Class Loss=0.5629750490188599, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.38215711414814
Loss made of: CE 0.5821338891983032, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.105121612548828 EntMin 0.0
Epoch 5, Class Loss=0.5127239227294922, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5127239227294922, Class Loss=0.5127239227294922, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=5.407082188129425
Loss made of: CE 0.5947883129119873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.730686187744141 EntMin 0.0
Epoch 6, Class Loss=0.49192842841148376, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.49192842841148376, Class Loss=0.49192842841148376, Reg Loss=0.0
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.268698847293853
Loss made of: CE 0.05349336192011833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.745189666748047 EntMin 0.0
Epoch 1, Class Loss=0.0875169187784195, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.0875169187784195, Class Loss=0.0875169187784195, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.290163023024798
Loss made of: CE 0.2440645694732666, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.629133224487305 EntMin 0.0
Epoch 2, Class Loss=0.18147391080856323, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.18147391080856323, Class Loss=0.18147391080856323, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.329899325966835
Loss made of: CE 0.23374450206756592, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.41934871673584 EntMin 0.0
Epoch 3, Class Loss=0.2690168619155884, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.2690168619155884, Class Loss=0.2690168619155884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=5.442136135697365
Loss made of: CE 0.4031692445278168, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.388391017913818 EntMin 0.0
Epoch 4, Class Loss=0.3616069257259369, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.3616069257259369, Class Loss=0.3616069257259369, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.491503641009331
Loss made of: CE 0.3359854221343994, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.017772674560547 EntMin 0.0
Epoch 5, Class Loss=0.4359245300292969, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.4359245300292969, Class Loss=0.4359245300292969, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.363245123624802
Loss made of: CE 0.46989211440086365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.845757961273193 EntMin 0.0
Epoch 6, Class Loss=0.48257026076316833, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.48257026076316833, Class Loss=0.48257026076316833, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.356197981908918
Loss made of: CE 0.08501731604337692, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.243438720703125 EntMin 0.0
Epoch 1, Class Loss=0.10904611647129059, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.10904611647129059, Class Loss=0.10904611647129059, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=5.512522402405739
Loss made of: CE 0.24933753907680511, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.220349311828613 EntMin 0.0
Epoch 2, Class Loss=0.22251704335212708, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.22251704335212708, Class Loss=0.22251704335212708, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=5.602447533607483
Loss made of: CE 0.3909875750541687, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.21275520324707 EntMin 0.0
Epoch 3, Class Loss=0.30493441224098206, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.30493441224098206, Class Loss=0.30493441224098206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=5.429465988278389
Loss made of: CE 0.5711947679519653, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.105226516723633 EntMin 0.0
Epoch 4, Class Loss=0.3813367784023285, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.3813367784023285, Class Loss=0.3813367784023285, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.472815236449241
Loss made of: CE 0.34691476821899414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.937080383300781 EntMin 0.0
Epoch 5, Class Loss=0.45030760765075684, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.45030760765075684, Class Loss=0.45030760765075684, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=5.395767191052437
Loss made of: CE 0.5152571201324463, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.119409561157227 EntMin 0.0
Epoch 6, Class Loss=0.4978821277618408, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.4978821277618408, Class Loss=0.4978821277618408, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.6553619503974915, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.792010
Mean Acc: 0.471750
FreqW Acc: 0.668163
Mean IoU: 0.359434
Class IoU:
	class 0: 0.8126084
	class 1: 0.714863
	class 2: 0.30955303
	class 3: 0.38351214
	class 4: 0.44300768
	class 5: 0.6850103
	class 6: 0.37421706
	class 7: 0.77987367
	class 8: 0.67172295
	class 9: 0.16462912
	class 10: 0.0
	class 11: 7.09636e-05
	class 12: 0.46838546
	class 13: 0.17707498
	class 14: 0.0
	class 15: 0.12585664
	class 16: 0.0
Class Acc:
	class 0: 0.9572853
	class 1: 0.7347555
	class 2: 0.9015965
	class 3: 0.38673875
	class 4: 0.47479647
	class 5: 0.7565042
	class 6: 0.37534973
	class 7: 0.92704535
	class 8: 0.7020692
	class 9: 0.34784243
	class 10: 0.0
	class 11: 7.096447e-05
	class 12: 0.61837727
	class 13: 0.71053183
	class 14: 0.0
	class 15: 0.12678103
	class 16: 0.0

federated global round: 22, step: 4
select part of clients to conduct local training
[21, 0, 25, 23]
Current Client Index:  21
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=5.770250482298434
Loss made of: CE 0.11727523058652878, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.284053325653076 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.354356181621552
Loss made of: CE 0.049038082361221313, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.234337329864502 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.127782592177391
Loss made of: CE 0.060832809656858444, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.092626571655273 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.236758524924516
Loss made of: CE 0.06583031266927719, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4915289878845215 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.043824072182178
Loss made of: CE 0.04257529601454735, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.553894519805908 EntMin 0.0
Epoch 1, Batch 60/97, Loss=5.0313984982669355
Loss made of: CE 0.03149636834859848, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.606414318084717 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.944488622434437
Loss made of: CE 0.03989627584815025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.62166690826416 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.8102853711694475
Loss made of: CE 0.02141418121755123, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.761465072631836 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.866322081722319
Loss made of: CE 0.022183164954185486, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.36396598815918 EntMin 0.0
Epoch 1, Class Loss=0.05609038099646568, Reg Loss=0.0
Clinet index 21, End of Epoch 1/6, Average Loss=0.05609038099646568, Class Loss=0.05609038099646568, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.868890774250031
Loss made of: CE 0.1171526312828064, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746326923370361 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.9078557588160034
Loss made of: CE 0.05681738257408142, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53186559677124 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.004683703929186
Loss made of: CE 0.09398695826530457, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.644041061401367 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.931042249873281
Loss made of: CE 0.08916549384593964, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.240705490112305 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.855192144960165
Loss made of: CE 0.0804862231016159, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.450348377227783 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.879432169348002
Loss made of: CE 0.07477810233831406, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.299993991851807 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.806122035905719
Loss made of: CE 0.07074247300624847, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5695109367370605 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 80/97, Loss=4.978345314785838
Loss made of: CE 0.10008382052183151, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.612446308135986 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.705890187621117
Loss made of: CE 0.0698002278804779, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.110511779785156 EntMin 0.0
Epoch 2, Class Loss=0.09269680827856064, Reg Loss=0.0
Clinet index 21, End of Epoch 2/6, Average Loss=0.09269680827856064, Class Loss=0.09269680827856064, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.790265486389399
Loss made of: CE 0.1601046919822693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.720399856567383 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.913334143906832
Loss made of: CE 0.13558757305145264, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.664731979370117 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.789926047623157
Loss made of: CE 0.1273772269487381, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.311139106750488 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.728428383916617
Loss made of: CE 0.07974173128604889, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.635960578918457 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.732580032944679
Loss made of: CE 0.13475263118743896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.343410491943359 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.898527250438929
Loss made of: CE 0.1144307479262352, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.57016658782959 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.8497942693531515
Loss made of: CE 0.08400051295757294, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.714102745056152 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.866509225219488
Loss made of: CE 0.12483226507902145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.616437911987305 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.715773480385542
Loss made of: CE 0.10459701716899872, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.185820579528809 EntMin 0.0
Epoch 3, Class Loss=0.1198282390832901, Reg Loss=0.0
Clinet index 21, End of Epoch 3/6, Average Loss=0.1198282390832901, Class Loss=0.1198282390832901, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.802493399381637
Loss made of: CE 0.16275011003017426, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.460690021514893 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.747019839286804
Loss made of: CE 0.15069617331027985, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.181740760803223 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.891203109174967
Loss made of: CE 0.18584680557250977, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.853492736816406 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.766216863691807
Loss made of: CE 0.21946661174297333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.812764644622803 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.8576821781694886
Loss made of: CE 0.1376640796661377, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.854029655456543 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.876603365689516
Loss made of: CE 0.17868384718894958, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.103765487670898 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.788283068686724
Loss made of: CE 0.12349645793437958, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.446826934814453 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.711893011629582
Loss made of: CE 0.12761665880680084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.224628448486328 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.855505935102701
Loss made of: CE 0.15966451168060303, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.498782157897949 EntMin 0.0
Epoch 4, Class Loss=0.15031655132770538, Reg Loss=0.0
Clinet index 21, End of Epoch 4/6, Average Loss=0.15031655132770538, Class Loss=0.15031655132770538, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.764898949861527
Loss made of: CE 0.16102799773216248, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.858909606933594 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.6652072876691815
Loss made of: CE 0.17143049836158752, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.887075901031494 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.686049503087998
Loss made of: CE 0.21667832136154175, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.527612686157227 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.843253752589225
Loss made of: CE 0.1871524155139923, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.755529403686523 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.539606267213822
Loss made of: CE 0.16571027040481567, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.438516616821289 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.847036081552505
Loss made of: CE 0.18585073947906494, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.524212837219238 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.735654336214066
Loss made of: CE 0.17477747797966003, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.484501838684082 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.773062415421009
Loss made of: CE 0.1781289279460907, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.048454284667969 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.813389155268669
Loss made of: CE 0.19554583728313446, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.734830379486084 EntMin 0.0
Epoch 5, Class Loss=0.17836293578147888, Reg Loss=0.0
Clinet index 21, End of Epoch 5/6, Average Loss=0.17836293578147888, Class Loss=0.17836293578147888, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.569239193201065
Loss made of: CE 0.24675437808036804, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6922407150268555 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.645265701413154
Loss made of: CE 0.21626238524913788, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.734220027923584 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.833274728059768
Loss made of: CE 0.19156800210475922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.087106227874756 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.76364191621542
Loss made of: CE 0.22524647414684296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.422051429748535 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.580242238938808
Loss made of: CE 0.1937239021062851, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.208464622497559 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.674766202270985
Loss made of: CE 0.1494632065296173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.927856922149658 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.57704436480999
Loss made of: CE 0.18422552943229675, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9444432258605957 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.493051968514919
Loss made of: CE 0.17231373488903046, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.206211566925049 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.432627575099469
Loss made of: CE 0.21056292951107025, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9195618629455566 EntMin 0.0
Epoch 6, Class Loss=0.2031409740447998, Reg Loss=0.0
Clinet index 21, End of Epoch 6/6, Average Loss=0.2031409740447998, Class Loss=0.2031409740447998, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.5733217351138595
Loss made of: CE 0.04565490782260895, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.366518974304199 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.0833229593932625
Loss made of: CE 0.07810669392347336, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.505897045135498 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.033414982631802
Loss made of: CE 0.05400983616709709, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.093156814575195 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.238870894536376
Loss made of: CE 0.06513217091560364, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.254186153411865 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.011336981505155
Loss made of: CE 0.09278605878353119, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.784562110900879 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.81181093417108
Loss made of: CE 0.026421017944812775, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1928300857543945 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.997717447020113
Loss made of: CE 0.09117777645587921, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2833709716796875 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.894994475319981
Loss made of: CE 0.1068250834941864, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6196818351745605 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.1369285389781
Loss made of: CE 0.07993733882904053, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.146177291870117 EntMin 0.0
Epoch 1, Class Loss=0.05744515731930733, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.05744515731930733, Class Loss=0.05744515731930733, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=5.033567609637975
Loss made of: CE 0.08615562319755554, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6914801597595215 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.946788834780454
Loss made of: CE 0.0979556068778038, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.555441856384277 EntMin 0.0
Epoch 2, Batch 30/97, Loss=5.040846419334412
Loss made of: CE 0.06877818703651428, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.161293029785156 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.739437001943588
Loss made of: CE 0.08178502321243286, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.586379051208496 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.926702247560025
Loss made of: CE 0.11383508145809174, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.474565505981445 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.905718017369509
Loss made of: CE 0.06482058763504028, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.019396781921387 EntMin 0.0
Epoch 2, Batch 70/97, Loss=5.02719466984272
Loss made of: CE 0.09170785546302795, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.997482776641846 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.8963001880794765
Loss made of: CE 0.11684708297252655, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.089141845703125 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.86793416403234
Loss made of: CE 0.09836047887802124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8420515060424805 EntMin 0.0
Epoch 2, Class Loss=0.09496548771858215, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.09496548771858215, Class Loss=0.09496548771858215, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.739146745949983
Loss made of: CE 0.14048175513744354, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.301643371582031 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.695242986828089
Loss made of: CE 0.11516925692558289, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.427966117858887 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.945941451191902
Loss made of: CE 0.1207297071814537, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.028279781341553 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.997488431632519
Loss made of: CE 0.14157381653785706, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.910670280456543 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.037421878427267
Loss made of: CE 0.09685453772544861, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.319663047790527 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.944140863418579
Loss made of: CE 0.12148892879486084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.796027183532715 EntMin 0.0
Epoch 3, Batch 70/97, Loss=5.010769332945347
Loss made of: CE 0.09281201660633087, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.200801849365234 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.922853220254183
Loss made of: CE 0.11515617370605469, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251673698425293 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.925013574212789
Loss made of: CE 0.11624805629253387, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4766035079956055 EntMin 0.0
Epoch 3, Class Loss=0.12219612300395966, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.12219612300395966, Class Loss=0.12219612300395966, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.702553334832191
Loss made of: CE 0.1611073762178421, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.336918830871582 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.793015015125275
Loss made of: CE 0.16315965354442596, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.935074806213379 EntMin 0.0
Epoch 4, Batch 30/97, Loss=5.137184718251229
Loss made of: CE 0.14702150225639343, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.219568252563477 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.81217357814312
Loss made of: CE 0.11513467878103256, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.04144287109375 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.7273686826229095
Loss made of: CE 0.13214175403118134, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.981864929199219 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.6227768227458
Loss made of: CE 0.12700313329696655, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.488672733306885 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.839273124188185
Loss made of: CE 0.1314712017774582, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.511075496673584 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.628998349606991
Loss made of: CE 0.13964731991291046, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.026902675628662 EntMin 0.0
Epoch 4, Batch 90/97, Loss=5.037124209851027
Loss made of: CE 0.13132941722869873, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.473600387573242 EntMin 0.0
Epoch 4, Class Loss=0.14955763518810272, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.14955763518810272, Class Loss=0.14955763518810272, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.6103983402252195
Loss made of: CE 0.17244702577590942, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.147216796875 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.862428447604179
Loss made of: CE 0.20083367824554443, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.544382095336914 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.785740554332733
Loss made of: CE 0.18357758224010468, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.334163188934326 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.87846849411726
Loss made of: CE 0.1682809442281723, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.876287937164307 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.736103500425815
Loss made of: CE 0.18292395770549774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.327780723571777 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.6974801704287525
Loss made of: CE 0.18704575300216675, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.023972511291504 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.825197459757328
Loss made of: CE 0.20289409160614014, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.17161226272583 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.483284613490104
Loss made of: CE 0.2060876190662384, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.318228244781494 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.533823414891958
Loss made of: CE 0.1567981094121933, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.19586181640625 EntMin 0.0
Epoch 5, Class Loss=0.17673969268798828, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.17673969268798828, Class Loss=0.17673969268798828, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.59479840695858
Loss made of: CE 0.18947282433509827, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.223582744598389 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.7211587354540825
Loss made of: CE 0.1980656087398529, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.873812198638916 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.918084083497524
Loss made of: CE 0.21213695406913757, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.12808895111084 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.898923794925213
Loss made of: CE 0.1941690295934677, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.639710426330566 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.674392703175545
Loss made of: CE 0.22582411766052246, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.119150638580322 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.908740919828415
Loss made of: CE 0.22244183719158173, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.095029830932617 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.664341303706169
Loss made of: CE 0.19310548901557922, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4223175048828125 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.698249706625939
Loss made of: CE 0.19152510166168213, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4038567543029785 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.81719536036253
Loss made of: CE 0.1836281716823578, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.278317451477051 EntMin 0.0
Epoch 6, Class Loss=0.20817716419696808, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.20817716419696808, Class Loss=0.20817716419696808, Reg Loss=0.0
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.516415922343731
Loss made of: CE 0.08599092811346054, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.353259563446045 EntMin 0.0
Epoch 1, Batch 20/97, Loss=5.331932808458805
Loss made of: CE 0.06423868238925934, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.150932312011719 EntMin 0.0
Epoch 1, Batch 30/97, Loss=5.134863824304193
Loss made of: CE 0.04080508276820183, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.268610954284668 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.029172963276506
Loss made of: CE 0.029566124081611633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.80914306640625 EntMin 0.0
Epoch 1, Batch 50/97, Loss=5.0248480360955
Loss made of: CE 0.04756314679980278, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.858178615570068 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.9721118967980145
Loss made of: CE 0.05442879721522331, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.762960433959961 EntMin 0.0
Epoch 1, Batch 70/97, Loss=5.1365881014615296
Loss made of: CE 0.03640734776854515, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.344844818115234 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.992229286208749
Loss made of: CE 0.03976878523826599, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0460405349731445 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.154203417524696
Loss made of: CE 0.035069193691015244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.966412544250488 EntMin 0.0
Epoch 1, Class Loss=0.05662904307246208, Reg Loss=0.0
Clinet index 25, End of Epoch 1/6, Average Loss=0.05662904307246208, Class Loss=0.05662904307246208, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.712131493911147
Loss made of: CE 0.08595218509435654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.798691749572754 EntMin 0.0
Epoch 2, Batch 20/97, Loss=5.06721376106143
Loss made of: CE 0.057342663407325745, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.435124397277832 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.927852556854487
Loss made of: CE 0.08835252374410629, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304633617401123 EntMin 0.0
Epoch 2, Batch 40/97, Loss=5.086638902127743
Loss made of: CE 0.09492307156324387, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.232219696044922 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.957773752138019
Loss made of: CE 0.05845176801085472, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.556560516357422 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.160949602723122
Loss made of: CE 0.09870823472738266, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.968860149383545 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.872484330832958
Loss made of: CE 0.06455904245376587, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8813276290893555 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.7731348145753145
Loss made of: CE 0.08313606679439545, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.393598556518555 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.820012924075127
Loss made of: CE 0.06275369226932526, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.601413249969482 EntMin 0.0
Epoch 2, Class Loss=0.09495139122009277, Reg Loss=0.0
Clinet index 25, End of Epoch 2/6, Average Loss=0.09495139122009277, Class Loss=0.09495139122009277, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.741608452796936
Loss made of: CE 0.16231170296669006, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.555144309997559 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.850663962215185
Loss made of: CE 0.0936461091041565, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.50644588470459 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.755842164158821
Loss made of: CE 0.1067383885383606, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.326552391052246 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.978147754073143
Loss made of: CE 0.10644835233688354, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.379107475280762 EntMin 0.0
Epoch 3, Batch 50/97, Loss=5.095600904896855
Loss made of: CE 0.05216459557414055, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.597428321838379 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.94802392423153
Loss made of: CE 0.1112792044878006, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7163777351379395 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.811578545719385
Loss made of: CE 0.15523414313793182, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.625025272369385 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.785944370180369
Loss made of: CE 0.13246527314186096, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1282806396484375 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.809331528097391
Loss made of: CE 0.11341345310211182, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.707330226898193 EntMin 0.0
Epoch 3, Class Loss=0.12146934866905212, Reg Loss=0.0
Clinet index 25, End of Epoch 3/6, Average Loss=0.12146934866905212, Class Loss=0.12146934866905212, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.823968107998371
Loss made of: CE 0.1654476821422577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2472333908081055 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.808353141695261
Loss made of: CE 0.15924692153930664, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.45025634765625 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.628199505060911
Loss made of: CE 0.15963397920131683, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4491424560546875 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.781848818063736
Loss made of: CE 0.21375040709972382, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7131876945495605 EntMin 0.0
Epoch 4, Batch 50/97, Loss=5.007203767448663
Loss made of: CE 0.2111322581768036, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.607066631317139 EntMin 0.0
Epoch 4, Batch 60/97, Loss=5.200670413672924
Loss made of: CE 0.1848105490207672, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.595422744750977 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.741531777381897
Loss made of: CE 0.15695920586585999, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.21613883972168 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.680286698043346
Loss made of: CE 0.14528033137321472, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.114053726196289 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.71300237774849
Loss made of: CE 0.16981226205825806, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.502825736999512 EntMin 0.0
Epoch 4, Class Loss=0.15162713825702667, Reg Loss=0.0
Clinet index 25, End of Epoch 4/6, Average Loss=0.15162713825702667, Class Loss=0.15162713825702667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.550503748655319
Loss made of: CE 0.18074482679367065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.411954402923584 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.429084868729115
Loss made of: CE 0.17326608300209045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2670416831970215 EntMin 0.0
Epoch 5, Batch 30/97, Loss=5.032162818312645
Loss made of: CE 0.19944541156291962, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.780048370361328 EntMin 0.0
Epoch 5, Batch 40/97, Loss=5.0632721647620205
Loss made of: CE 0.1519017368555069, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.905635833740234 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.814163267612457
Loss made of: CE 0.19969505071640015, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684875011444092 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.865944418311119
Loss made of: CE 0.19351474940776825, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.37725830078125 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.792792315781116
Loss made of: CE 0.157196044921875, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.281307220458984 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.591477055847645
Loss made of: CE 0.1590251624584198, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.472238540649414 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.758646476268768
Loss made of: CE 0.16859397292137146, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.675992965698242 EntMin 0.0
Epoch 5, Class Loss=0.17960074543952942, Reg Loss=0.0
Clinet index 25, End of Epoch 5/6, Average Loss=0.17960074543952942, Class Loss=0.17960074543952942, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.5577953711152075
Loss made of: CE 0.21156296133995056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.414790153503418 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.691104750335216
Loss made of: CE 0.23895364999771118, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.128918170928955 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.702053682506085
Loss made of: CE 0.1939765214920044, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.964567184448242 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.551025523245334
Loss made of: CE 0.20749682188034058, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.347489356994629 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.630910123884678
Loss made of: CE 0.21795150637626648, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611626625061035 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.795225174725056
Loss made of: CE 0.1770741045475006, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8535566329956055 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.682884320616722
Loss made of: CE 0.22564797103405, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.338345527648926 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.680828869342804
Loss made of: CE 0.17333535850048065, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1920318603515625 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.4020825430750845
Loss made of: CE 0.16468173265457153, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0643768310546875 EntMin 0.0
Epoch 6, Class Loss=0.20597226917743683, Reg Loss=0.0
Clinet index 25, End of Epoch 6/6, Average Loss=0.20597226917743683, Class Loss=0.20597226917743683, Reg Loss=0.0
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=5.9993165921419855
Loss made of: CE 0.041779737919569016, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.454323768615723 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.929909306392074
Loss made of: CE 0.046376317739486694, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5146636962890625 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.8371129721403126
Loss made of: CE 0.0700635090470314, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.745450019836426 EntMin 0.0
Epoch 1, Batch 40/97, Loss=5.116416477784514
Loss made of: CE 0.05691219121217728, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.74552059173584 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.987024866603315
Loss made of: CE 0.06161242723464966, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.101139545440674 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.756774824112654
Loss made of: CE 0.06082084774971008, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.064365386962891 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.867890364490449
Loss made of: CE 0.04345327615737915, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.066904067993164 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.949730352684855
Loss made of: CE 0.05861738324165344, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2728376388549805 EntMin 0.0
Epoch 1, Batch 90/97, Loss=5.10081860665232
Loss made of: CE 0.04643727466464043, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.279482841491699 EntMin 0.0
Epoch 1, Class Loss=0.05638449266552925, Reg Loss=0.0
Clinet index 23, End of Epoch 1/6, Average Loss=0.05638449266552925, Class Loss=0.05638449266552925, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/97, Loss=4.885081515461207
Loss made of: CE 0.047368258237838745, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2075395584106445 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.919971083477139
Loss made of: CE 0.10393916815519333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.945969581604004 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.79155480787158
Loss made of: CE 0.11310368776321411, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.577628135681152 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.891076792776585
Loss made of: CE 0.05577215552330017, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.193314552307129 EntMin 0.0
Epoch 2, Batch 50/97, Loss=5.006735444068909
Loss made of: CE 0.07305529713630676, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.557791709899902 EntMin 0.0
Epoch 2, Batch 60/97, Loss=5.154855994135142
Loss made of: CE 0.0935460552573204, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.847379207611084 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.870849715173245
Loss made of: CE 0.06980642676353455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.829829692840576 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.922935633733869
Loss made of: CE 0.10565836727619171, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.928852081298828 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.985047941654921
Loss made of: CE 0.10473942756652832, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.3240461349487305 EntMin 0.0
Epoch 2, Class Loss=0.0918496698141098, Reg Loss=0.0
Clinet index 23, End of Epoch 2/6, Average Loss=0.0918496698141098, Class Loss=0.0918496698141098, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/97, Loss=4.931047874689102
Loss made of: CE 0.10697895288467407, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.018557548522949 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.944136472046376
Loss made of: CE 0.09350179135799408, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.866217613220215 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.945302174985409
Loss made of: CE 0.15889815986156464, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.381473541259766 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.858695437014103
Loss made of: CE 0.17578116059303284, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758853435516357 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.939077463746071
Loss made of: CE 0.08660616725683212, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7680511474609375 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.744264281541109
Loss made of: CE 0.08988383412361145, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.435728073120117 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.687212189286948
Loss made of: CE 0.0738452896475792, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.550533294677734 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.625171180069446
Loss made of: CE 0.14141324162483215, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.967909812927246 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.945594656467438
Loss made of: CE 0.1509997844696045, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.684634685516357 EntMin 0.0
Epoch 3, Class Loss=0.12094243615865707, Reg Loss=0.0
Clinet index 23, End of Epoch 3/6, Average Loss=0.12094243615865707, Class Loss=0.12094243615865707, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/97, Loss=4.745735193789005
Loss made of: CE 0.16383925080299377, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.659088611602783 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.839015001803636
Loss made of: CE 0.15899121761322021, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.315271377563477 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.698042614758014
Loss made of: CE 0.16354325413703918, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.721601486206055 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.915217622369528
Loss made of: CE 0.1880580633878708, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.217663764953613 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.901731541752815
Loss made of: CE 0.16152837872505188, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.823507785797119 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.661309611052275
Loss made of: CE 0.15363754332065582, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623141765594482 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.874925012141466
Loss made of: CE 0.1720145344734192, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.369133949279785 EntMin 0.0
Epoch 4, Batch 80/97, Loss=5.051659645140171
Loss made of: CE 0.10282957553863525, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.305649280548096 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.638563799113035
Loss made of: CE 0.14299112558364868, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.556656360626221 EntMin 0.0
Epoch 4, Class Loss=0.15165531635284424, Reg Loss=0.0
Clinet index 23, End of Epoch 4/6, Average Loss=0.15165531635284424, Class Loss=0.15165531635284424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/97, Loss=4.854881925880909
Loss made of: CE 0.14980703592300415, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.49348258972168 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.80390677601099
Loss made of: CE 0.2579321265220642, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.641606330871582 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.851199869811535
Loss made of: CE 0.17915329337120056, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.52091121673584 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.7977231472730635
Loss made of: CE 0.16210034489631653, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.907917022705078 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.5654402613639835
Loss made of: CE 0.15575814247131348, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.521177768707275 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.49442950040102
Loss made of: CE 0.13842549920082092, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.905245304107666 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.615153689682484
Loss made of: CE 0.177552729845047, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2354536056518555 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.583100931346417
Loss made of: CE 0.16001789271831512, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.270943641662598 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.796986274421215
Loss made of: CE 0.169758602976799, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.244106292724609 EntMin 0.0
Epoch 5, Class Loss=0.18002860248088837, Reg Loss=0.0
Clinet index 23, End of Epoch 5/6, Average Loss=0.18002860248088837, Class Loss=0.18002860248088837, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/97, Loss=4.636554764211178
Loss made of: CE 0.2049097865819931, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.17612361907959 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.688294397294522
Loss made of: CE 0.1919439435005188, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.270627975463867 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.667664310336113
Loss made of: CE 0.2335171401500702, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.46206521987915 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.502172999083996
Loss made of: CE 0.17133674025535583, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.050686836242676 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.730595651268959
Loss made of: CE 0.17762205004692078, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.251031875610352 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.717370606958866
Loss made of: CE 0.2016502022743225, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611979007720947 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.775830486416817
Loss made of: CE 0.2222827970981598, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.08573579788208 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.69261749535799
Loss made of: CE 0.21044331789016724, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.014917850494385 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.648455509543419
Loss made of: CE 0.2296115607023239, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.291425704956055 EntMin 0.0
Epoch 6, Class Loss=0.207195445895195, Reg Loss=0.0
Clinet index 23, End of Epoch 6/6, Average Loss=0.207195445895195, Class Loss=0.207195445895195, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5785337686538696, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.826896
Mean Acc: 0.500896
FreqW Acc: 0.726842
Mean IoU: 0.392264
Class IoU:
	class 0: 0.8553493
	class 1: 0.7480301
	class 2: 0.33415103
	class 3: 0.32189205
	class 4: 0.5067822
	class 5: 0.6017533
	class 6: 0.5975228
	class 7: 0.7934367
	class 8: 0.6295048
	class 9: 0.13790694
	class 10: 0.0
	class 11: 0.0
	class 12: 0.40812477
	class 13: 0.23312183
	class 14: 0.0
	class 15: 0.5009053
	class 16: 0.0
Class Acc:
	class 0: 0.9343424
	class 1: 0.76488894
	class 2: 0.86091375
	class 3: 0.32319406
	class 4: 0.55674314
	class 5: 0.6263637
	class 6: 0.6006244
	class 7: 0.9421707
	class 8: 0.64672863
	class 9: 0.23188937
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5202661
	class 13: 0.606605
	class 14: 0.0
	class 15: 0.9005049
	class 16: 0.0

federated global round: 23, step: 4
select part of clients to conduct local training
[17, 3, 5, 22]
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.239243436604738
Loss made of: CE 0.010127687826752663, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.3303351402282715 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/97, Loss=4.262656558118761
Loss made of: CE 0.02343979850411415, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7086730003356934 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.4425315331667665
Loss made of: CE 0.014122819527983665, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.213115215301514 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.465525970887393
Loss made of: CE 0.019126813858747482, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.840277671813965 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.471273917611688
Loss made of: CE 0.017063120380043983, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8779845237731934 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.410183904040605
Loss made of: CE 0.024491852149367332, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.252233505249023 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.255944763403386
Loss made of: CE 0.015133528038859367, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.24226713180542 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.553576441295445
Loss made of: CE 0.02093757502734661, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.449886322021484 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.537336379475891
Loss made of: CE 0.02030470035970211, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.414905071258545 EntMin 0.0
Epoch 1, Class Loss=0.019804731011390686, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.019804731011390686, Class Loss=0.019804731011390686, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.4463632963597775
Loss made of: CE 0.04489685222506523, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.090185165405273 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.284049559570849
Loss made of: CE 0.04785759374499321, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.351753234863281 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.734566402807832
Loss made of: CE 0.024728253483772278, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.200382709503174 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.527336736954749
Loss made of: CE 0.03389155492186546, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.521328926086426 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.4225703872740265
Loss made of: CE 0.04960956424474716, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594447135925293 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.553564275056123
Loss made of: CE 0.044973861426115036, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.637033939361572 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.680872901156545
Loss made of: CE 0.03365568071603775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.568493366241455 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.541458191722631
Loss made of: CE 0.027980580925941467, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.207547187805176 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.319460865482688
Loss made of: CE 0.04080186039209366, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.058574676513672 EntMin 0.0
Epoch 2, Class Loss=0.045161742717027664, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.045161742717027664, Class Loss=0.045161742717027664, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.5304196290671825
Loss made of: CE 0.06877540796995163, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.53414249420166 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.58090463578701
Loss made of: CE 0.064756840467453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.969615936279297 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.291189615055918
Loss made of: CE 0.07803324609994888, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.277148723602295 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.599774920195341
Loss made of: CE 0.07691845297813416, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.377080917358398 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.286248860508204
Loss made of: CE 0.10049669444561005, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.18942928314209 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.645274967700243
Loss made of: CE 0.11524492502212524, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.688936710357666 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.544104778021574
Loss made of: CE 0.07623906433582306, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5214033126831055 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.413392452523112
Loss made of: CE 0.04788228124380112, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0389838218688965 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.48590727224946
Loss made of: CE 0.08703263849020004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.058950424194336 EntMin 0.0
Epoch 3, Class Loss=0.07478264719247818, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.07478264719247818, Class Loss=0.07478264719247818, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.395868971198797
Loss made of: CE 0.10781057924032211, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.228372573852539 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.76255736798048
Loss made of: CE 0.17766956984996796, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.839158058166504 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.50971607118845
Loss made of: CE 0.08783646672964096, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9356954097747803 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.40227777287364
Loss made of: CE 0.09259951114654541, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.034282684326172 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.563111052662134
Loss made of: CE 0.1527286022901535, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.293004035949707 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.5622767701745035
Loss made of: CE 0.10497250407934189, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.226909637451172 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.393242490291596
Loss made of: CE 0.08739351481199265, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.217782020568848 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.4571141138672825
Loss made of: CE 0.09549855440855026, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.227170467376709 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.185967376083136
Loss made of: CE 0.07029054313898087, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.079586982727051 EntMin 0.0
Epoch 4, Class Loss=0.11211538314819336, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.11211538314819336, Class Loss=0.11211538314819336, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.318440449982882
Loss made of: CE 0.18090300261974335, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679960250854492 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.565650800615549
Loss made of: CE 0.16313336789608002, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.679100036621094 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.438506977260113
Loss made of: CE 0.11740761250257492, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.245851516723633 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.548033667355776
Loss made of: CE 0.12474732846021652, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.290916442871094 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.653704592585564
Loss made of: CE 0.10610806196928024, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.922101974487305 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.456767188757658
Loss made of: CE 0.10967715829610825, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.326999187469482 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.5022341512143615
Loss made of: CE 0.1586495339870453, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.505034446716309 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.45849979519844
Loss made of: CE 0.14376986026763916, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.388462066650391 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.522718574106693
Loss made of: CE 0.15756520628929138, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304708957672119 EntMin 0.0
Epoch 5, Class Loss=0.15043571591377258, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.15043571591377258, Class Loss=0.15043571591377258, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.591605631262064
Loss made of: CE 0.16318175196647644, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.623950958251953 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.359886023402214
Loss made of: CE 0.193298801779747, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9097023010253906 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.417793767154217
Loss made of: CE 0.18689237534999847, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.37980842590332 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.344567823410034
Loss made of: CE 0.1771494746208191, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9244961738586426 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.53080812394619
Loss made of: CE 0.1720207929611206, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.394813060760498 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.468426965177059
Loss made of: CE 0.16334417462348938, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0945515632629395 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.518902091681957
Loss made of: CE 0.19655287265777588, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.780135154724121 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.491321946680546
Loss made of: CE 0.17371425032615662, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.0171308517456055 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.689129042625427
Loss made of: CE 0.19351324439048767, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4590606689453125 EntMin 0.0
Epoch 6, Class Loss=0.18332934379577637, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.18332934379577637, Class Loss=0.18332934379577637, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.36376416683197
Loss made of: CE 0.0748312920331955, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.522447109222412 EntMin 0.0
Epoch 1, Class Loss=0.15905478596687317, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.15905478596687317, Class Loss=0.15905478596687317, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=5.7143340989947315
Loss made of: CE 0.24291539192199707, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.364318370819092 EntMin 0.0
Epoch 2, Class Loss=0.28795957565307617, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.28795957565307617, Class Loss=0.28795957565307617, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.470932952314615
Loss made of: CE 0.09383472055196762, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.646442890167236 EntMin 0.0
Epoch 3, Class Loss=0.4225790500640869, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.4225790500640869, Class Loss=0.4225790500640869, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.555687713623047
Loss made of: CE 0.20949682593345642, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.11220645904541 EntMin 0.0
Epoch 4, Class Loss=0.48365551233291626, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.48365551233291626, Class Loss=0.48365551233291626, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.3518758684396746
Loss made of: CE 0.5616854429244995, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.660990238189697 EntMin 0.0
Epoch 5, Class Loss=0.5528860092163086, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.5528860092163086, Class Loss=0.5528860092163086, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.483016526699066
Loss made of: CE 0.35636839270591736, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.746563911437988 EntMin 0.0
Epoch 6, Class Loss=0.5651862025260925, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.5651862025260925, Class Loss=0.5651862025260925, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/97, Loss=4.121788854058832
Loss made of: CE 0.015403199009597301, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9236295223236084 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.348193309176713
Loss made of: CE 0.028558101505041122, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.400479793548584 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 30/97, Loss=4.320909505710006
Loss made of: CE 0.015112420544028282, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.464696407318115 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.38585194312036
Loss made of: CE 0.007757901679724455, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.207672119140625 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.465058671031147
Loss made of: CE 0.007456495426595211, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.469918251037598 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.380545296147465
Loss made of: CE 0.0156878475099802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.026871681213379 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.51110816616565
Loss made of: CE 0.012716191820800304, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.993091583251953 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.664120972529053
Loss made of: CE 0.01560071762651205, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.293949127197266 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.509777979552746
Loss made of: CE 0.013710973784327507, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.670931339263916 EntMin 0.0
Epoch 1, Class Loss=0.01907351240515709, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.01907351240515709, Class Loss=0.01907351240515709, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/97, Loss=4.456184961274266
Loss made of: CE 0.04038127884268761, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6208271980285645 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.5163473945111035
Loss made of: CE 0.04616738483309746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.001533508300781 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.388283840939403
Loss made of: CE 0.05092325434088707, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.757013320922852 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.341965516284108
Loss made of: CE 0.05458412319421768, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.257927417755127 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.50597650166601
Loss made of: CE 0.05061057209968567, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.895834922790527 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.535843847319484
Loss made of: CE 0.05454457923769951, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.195461750030518 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.450796932913363
Loss made of: CE 0.048511065542697906, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.818835258483887 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.539120819047094
Loss made of: CE 0.047750454396009445, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.42037296295166 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.661570032313466
Loss made of: CE 0.039294131100177765, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.009850978851318 EntMin 0.0
Epoch 2, Class Loss=0.04367709159851074, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.04367709159851074, Class Loss=0.04367709159851074, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/97, Loss=4.600162073969841
Loss made of: CE 0.07525844871997833, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.292611598968506 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.573066782206297
Loss made of: CE 0.06154325604438782, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.647520542144775 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.437710597366094
Loss made of: CE 0.09290360659360886, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.026845932006836 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.629387908056378
Loss made of: CE 0.06025480479001999, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.106637001037598 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.550169746950269
Loss made of: CE 0.089464470744133, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.361927032470703 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.496589514240623
Loss made of: CE 0.09955429285764694, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.34958028793335 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.419594793766737
Loss made of: CE 0.06819973886013031, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.049355506896973 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.4180627729743716
Loss made of: CE 0.07298732548952103, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.141833305358887 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.406717982143164
Loss made of: CE 0.08322019129991531, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.525709629058838 EntMin 0.0
Epoch 3, Class Loss=0.07539945840835571, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.07539945840835571, Class Loss=0.07539945840835571, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/97, Loss=4.5665850766003135
Loss made of: CE 0.13308803737163544, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.508450508117676 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.37753911241889
Loss made of: CE 0.09277453273534775, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.176109790802002 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.4390635214745995
Loss made of: CE 0.12377133965492249, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.242149353027344 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.348004911094904
Loss made of: CE 0.15725715458393097, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.014804363250732 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.267436668276787
Loss made of: CE 0.1285925954580307, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.135786533355713 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.438056642562151
Loss made of: CE 0.11273028701543808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.571334362030029 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.443697332590818
Loss made of: CE 0.10835038125514984, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.526541709899902 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.205705581605434
Loss made of: CE 0.08696135133504868, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7465662956237793 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.360210663825273
Loss made of: CE 0.11750670522451401, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.95784330368042 EntMin 0.0
Epoch 4, Class Loss=0.11153878271579742, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.11153878271579742, Class Loss=0.11153878271579742, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/97, Loss=4.517115844786167
Loss made of: CE 0.1542869657278061, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.360421180725098 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.51644217222929
Loss made of: CE 0.13726356625556946, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.157095909118652 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.422594822943211
Loss made of: CE 0.1736495941877365, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.36336088180542 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.203088136017323
Loss made of: CE 0.13601844012737274, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.330052375793457 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.3732207924127575
Loss made of: CE 0.16857045888900757, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9369559288024902 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.63630221709609
Loss made of: CE 0.16118912398815155, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.372897148132324 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.493588298559189
Loss made of: CE 0.12351267039775848, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.761163711547852 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.569142556935549
Loss made of: CE 0.1257242113351822, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9416160583496094 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.475751677155495
Loss made of: CE 0.1487814038991928, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.188385009765625 EntMin 0.0
Epoch 5, Class Loss=0.14788436889648438, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.14788436889648438, Class Loss=0.14788436889648438, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/97, Loss=4.4372635468840596
Loss made of: CE 0.18112997710704803, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.143954277038574 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.271006558835507
Loss made of: CE 0.16299401223659515, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.765066146850586 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.256980195641518
Loss made of: CE 0.2297576367855072, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.990828275680542 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.6711934298276905
Loss made of: CE 0.2401072233915329, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.995489120483398 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.300800684094429
Loss made of: CE 0.17090201377868652, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.613387584686279 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.386514323949814
Loss made of: CE 0.18137255311012268, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.753395080566406 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.2273664191365246
Loss made of: CE 0.15835627913475037, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.261629104614258 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.320512339472771
Loss made of: CE 0.17971639335155487, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304074287414551 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.4039187416434284
Loss made of: CE 0.1773155778646469, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.994933605194092 EntMin 0.0
Epoch 6, Class Loss=0.1845671534538269, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.1845671534538269, Class Loss=0.1845671534538269, Reg Loss=0.0
Current Client Index:  22
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=6.3648890618234875
Loss made of: CE 0.14340481162071228, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.843703746795654 EntMin 0.0
Epoch 1, Class Loss=0.17027631402015686, Reg Loss=0.0
Clinet index 22, End of Epoch 1/6, Average Loss=0.17027631402015686, Class Loss=0.17027631402015686, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=5.612840735912323
Loss made of: CE 0.281665563583374, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.887376308441162 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=0.2981143891811371, Reg Loss=0.0
Clinet index 22, End of Epoch 2/6, Average Loss=0.2981143891811371, Class Loss=0.2981143891811371, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=5.39301487505436
Loss made of: CE 0.7079919576644897, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.108720302581787 EntMin 0.0
Epoch 3, Class Loss=0.4244142472743988, Reg Loss=0.0
Clinet index 22, End of Epoch 3/6, Average Loss=0.4244142472743988, Class Loss=0.4244142472743988, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=5.575953160226345
Loss made of: CE 0.4966369569301605, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.05810546875 EntMin 0.0
Epoch 4, Class Loss=0.5207712650299072, Reg Loss=0.0
Clinet index 22, End of Epoch 4/6, Average Loss=0.5207712650299072, Class Loss=0.5207712650299072, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=5.353854703903198
Loss made of: CE 0.36406683921813965, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.360785961151123 EntMin 0.0
Epoch 5, Class Loss=0.5354284644126892, Reg Loss=0.0
Clinet index 22, End of Epoch 5/6, Average Loss=0.5354284644126892, Class Loss=0.5354284644126892, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=5.2618207961320875
Loss made of: CE 0.451532781124115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.558213233947754 EntMin 0.0
Epoch 6, Class Loss=0.5738456845283508, Reg Loss=0.0
Clinet index 22, End of Epoch 6/6, Average Loss=0.5738456845283508, Class Loss=0.5738456845283508, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.5930460691452026, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.824819
Mean Acc: 0.520018
FreqW Acc: 0.728383
Mean IoU: 0.401034
Class IoU:
	class 0: 0.8532284
	class 1: 0.7680155
	class 2: 0.3251507
	class 3: 0.36698422
	class 4: 0.4780304
	class 5: 0.6361804
	class 6: 0.61674947
	class 7: 0.76981604
	class 8: 0.6666688
	class 9: 0.15636714
	class 10: 0.0
	class 11: 0.0
	class 12: 0.45383635
	class 13: 0.23376362
	class 14: 0.0
	class 15: 0.49278614
	class 16: 0.0
Class Acc:
	class 0: 0.92037296
	class 1: 0.7900685
	class 2: 0.8728644
	class 3: 0.36932337
	class 4: 0.5199482
	class 5: 0.6688881
	class 6: 0.62022936
	class 7: 0.9448772
	class 8: 0.6899716
	class 9: 0.2679769
	class 10: 0.0
	class 11: 0.0
	class 12: 0.5786033
	class 13: 0.67276835
	class 14: 0.0
	class 15: 0.9244128
	class 16: 0.0

federated global round: 24, step: 4
select part of clients to conduct local training
[16, 9, 12, 2]
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/97, Loss=4.576289926096797
Loss made of: CE 0.02211378514766693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.918639183044434 EntMin 0.0
Epoch 1, Batch 20/97, Loss=4.26578711681068
Loss made of: CE 0.01851935312151909, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.302199363708496 EntMin 0.0
Epoch 1, Batch 30/97, Loss=4.260215451102704
Loss made of: CE 0.015531502664089203, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.95566725730896 EntMin 0.0
Epoch 1, Batch 40/97, Loss=4.294085643440485
Loss made of: CE 0.020341629162430763, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.451503276824951 EntMin 0.0
Epoch 1, Batch 50/97, Loss=4.243437027558684
Loss made of: CE 0.01686432957649231, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.722248077392578 EntMin 0.0
Epoch 1, Batch 60/97, Loss=4.621337779145688
Loss made of: CE 0.023743486031889915, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.576003074645996 EntMin 0.0
Epoch 1, Batch 70/97, Loss=4.4321846220642325
Loss made of: CE 0.014518948271870613, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.674241542816162 EntMin 0.0
Epoch 1, Batch 80/97, Loss=4.416755073796958
Loss made of: CE 0.04431989789009094, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9274282455444336 EntMin 0.0
Epoch 1, Batch 90/97, Loss=4.304249728005379
Loss made of: CE 0.02541535720229149, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.578592300415039 EntMin 0.0
Epoch 1, Class Loss=0.021984485909342766, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.021984485909342766, Class Loss=0.021984485909342766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/97, Loss=4.491425948776305
Loss made of: CE 0.03274877741932869, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.54564094543457 EntMin 0.0
Epoch 2, Batch 20/97, Loss=4.362070265226066
Loss made of: CE 0.03377789258956909, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.265702247619629 EntMin 0.0
Epoch 2, Batch 30/97, Loss=4.494706160575151
Loss made of: CE 0.04908464103937149, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.804189682006836 EntMin 0.0
Epoch 2, Batch 40/97, Loss=4.567564674094319
Loss made of: CE 0.052705612033605576, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594597816467285 EntMin 0.0
Epoch 2, Batch 50/97, Loss=4.293425129167735
Loss made of: CE 0.033727504312992096, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.423834800720215 EntMin 0.0
Epoch 2, Batch 60/97, Loss=4.307226225547493
Loss made of: CE 0.05040457472205162, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.540278434753418 EntMin 0.0
Epoch 2, Batch 70/97, Loss=4.308959766104818
Loss made of: CE 0.05504169687628746, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.552089691162109 EntMin 0.0
Epoch 2, Batch 80/97, Loss=4.380220290273428
Loss made of: CE 0.024547602981328964, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.12643575668335 EntMin 0.0
Epoch 2, Batch 90/97, Loss=4.431178654171527
Loss made of: CE 0.04440748319029808, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.8478240966796875 EntMin 0.0
Epoch 2, Class Loss=0.044763170182704926, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.044763170182704926, Class Loss=0.044763170182704926, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/97, Loss=4.083774996548891
Loss made of: CE 0.07650585472583771, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8489012718200684 EntMin 0.0
Epoch 3, Batch 20/97, Loss=4.2821485627442595
Loss made of: CE 0.05147711560130119, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.31091833114624 EntMin 0.0
Epoch 3, Batch 30/97, Loss=4.6596883933991196
Loss made of: CE 0.08103899657726288, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.926958084106445 EntMin 0.0
Epoch 3, Batch 40/97, Loss=4.6306333966553215
Loss made of: CE 0.07062168419361115, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.159684181213379 EntMin 0.0
Epoch 3, Batch 50/97, Loss=4.308503003045916
Loss made of: CE 0.061334144324064255, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.545234203338623 EntMin 0.0
Epoch 3, Batch 60/97, Loss=4.322455586865544
Loss made of: CE 0.07003853470087051, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.129306316375732 EntMin 0.0
Epoch 3, Batch 70/97, Loss=4.512112210318446
Loss made of: CE 0.08349301666021347, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1652092933654785 EntMin 0.0
Epoch 3, Batch 80/97, Loss=4.469888151437044
Loss made of: CE 0.06053650379180908, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.585834503173828 EntMin 0.0
Epoch 3, Batch 90/97, Loss=4.124219853430986
Loss made of: CE 0.06276965141296387, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9544410705566406 EntMin 0.0
Epoch 3, Class Loss=0.075030118227005, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.075030118227005, Class Loss=0.075030118227005, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/97, Loss=4.274744939059019
Loss made of: CE 0.13654404878616333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.607734680175781 EntMin 0.0
Epoch 4, Batch 20/97, Loss=4.191312724351883
Loss made of: CE 0.07323142886161804, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9900078773498535 EntMin 0.0
Epoch 4, Batch 30/97, Loss=4.288903175294399
Loss made of: CE 0.09687501192092896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.637920379638672 EntMin 0.0
Epoch 4, Batch 40/97, Loss=4.2813521027565
Loss made of: CE 0.12890313565731049, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.631052017211914 EntMin 0.0
Epoch 4, Batch 50/97, Loss=4.309352757036686
Loss made of: CE 0.08088281750679016, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9120137691497803 EntMin 0.0
Epoch 4, Batch 60/97, Loss=4.267722057551145
Loss made of: CE 0.08998602628707886, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.051074981689453 EntMin 0.0
Epoch 4, Batch 70/97, Loss=4.321409797668457
Loss made of: CE 0.09178216755390167, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2811198234558105 EntMin 0.0
Epoch 4, Batch 80/97, Loss=4.226760322600603
Loss made of: CE 0.11765878647565842, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.289674758911133 EntMin 0.0
Epoch 4, Batch 90/97, Loss=4.1142191253602505
Loss made of: CE 0.08067455142736435, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.247619152069092 EntMin 0.0
Epoch 4, Class Loss=0.1082107201218605, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.1082107201218605, Class Loss=0.1082107201218605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/97, Loss=4.218114705383778
Loss made of: CE 0.15667328238487244, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4617414474487305 EntMin 0.0
Epoch 5, Batch 20/97, Loss=4.317037180066109
Loss made of: CE 0.16602034866809845, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.719510555267334 EntMin 0.0
Epoch 5, Batch 30/97, Loss=4.219309820979833
Loss made of: CE 0.16880707442760468, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.275961875915527 EntMin 0.0
Epoch 5, Batch 40/97, Loss=4.309305796772241
Loss made of: CE 0.11733987182378769, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.719954252243042 EntMin 0.0
Epoch 5, Batch 50/97, Loss=4.263259039819241
Loss made of: CE 0.16556470096111298, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.082315921783447 EntMin 0.0
Epoch 5, Batch 60/97, Loss=4.373596193641424
Loss made of: CE 0.17057275772094727, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.232110977172852 EntMin 0.0
Epoch 5, Batch 70/97, Loss=4.273764833807945
Loss made of: CE 0.1248219907283783, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.923213481903076 EntMin 0.0
Epoch 5, Batch 80/97, Loss=4.1896955944597725
Loss made of: CE 0.12414994090795517, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.127446174621582 EntMin 0.0
Epoch 5, Batch 90/97, Loss=4.382655653357506
Loss made of: CE 0.18220019340515137, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.642745494842529 EntMin 0.0
Epoch 5, Class Loss=0.15137355029582977, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.15137355029582977, Class Loss=0.15137355029582977, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/97, Loss=4.285222360491753
Loss made of: CE 0.17225031554698944, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.6879539489746094 EntMin 0.0
Epoch 6, Batch 20/97, Loss=4.263220633566379
Loss made of: CE 0.28575998544692993, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.445700645446777 EntMin 0.0
Epoch 6, Batch 30/97, Loss=4.0919397383928295
Loss made of: CE 0.15572388470172882, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.491218090057373 EntMin 0.0
Epoch 6, Batch 40/97, Loss=4.083117647469043
Loss made of: CE 0.20608393847942352, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.269813060760498 EntMin 0.0
Epoch 6, Batch 50/97, Loss=4.039753676950932
Loss made of: CE 0.13896745443344116, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.4127213954925537 EntMin 0.0
Epoch 6, Batch 60/97, Loss=4.148876330256462
Loss made of: CE 0.16976842284202576, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8959054946899414 EntMin 0.0
Epoch 6, Batch 70/97, Loss=4.142748041450977
Loss made of: CE 0.1874021738767624, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.018350601196289 EntMin 0.0
Epoch 6, Batch 80/97, Loss=4.160564859211445
Loss made of: CE 0.1625395268201828, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.799194097518921 EntMin 0.0
Epoch 6, Batch 90/97, Loss=4.441472892463207
Loss made of: CE 0.20116274058818817, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9953255653381348 EntMin 0.0
Epoch 6, Class Loss=0.19190429151058197, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.19190429151058197, Class Loss=0.19190429151058197, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.363085089623928
Loss made of: CE 0.15724395215511322, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.424872398376465 EntMin 0.0
Epoch 1, Class Loss=0.12205567955970764, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.12205567955970764, Class Loss=0.12205567955970764, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.152131313085556
Loss made of: CE 0.32505109906196594, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.062540054321289 EntMin 0.0
Epoch 2, Class Loss=0.23576995730400085, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.23576995730400085, Class Loss=0.23576995730400085, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.043179546296597
Loss made of: CE 0.219197079539299, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.767754554748535 EntMin 0.0
Epoch 3, Class Loss=0.3559838533401489, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.3559838533401489, Class Loss=0.3559838533401489, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=5.262706127762795
Loss made of: CE 0.34545785188674927, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.614334583282471 EntMin 0.0
Epoch 4, Class Loss=0.43434178829193115, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.43434178829193115, Class Loss=0.43434178829193115, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=5.115864995121956
Loss made of: CE 0.35671326518058777, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.568629264831543 EntMin 0.0
Epoch 5, Class Loss=0.4481944441795349, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.4481944441795349, Class Loss=0.4481944441795349, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=5.105676349997521
Loss made of: CE 0.3399899899959564, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.906956672668457 EntMin 0.0
Epoch 6, Class Loss=0.5551995635032654, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.5551995635032654, Class Loss=0.5551995635032654, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.154649003967643
Loss made of: CE 0.0727912113070488, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.094663143157959 EntMin 0.0
Epoch 1, Class Loss=0.11544014513492584, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.11544014513492584, Class Loss=0.11544014513492584, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=5.04959479123354
Loss made of: CE 0.250357449054718, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.856627464294434 EntMin 0.0
Epoch 2, Class Loss=0.22255563735961914, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.22255563735961914, Class Loss=0.22255563735961914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=5.02159934937954
Loss made of: CE 0.20926448702812195, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.589578151702881 EntMin 0.0
Epoch 3, Class Loss=0.3279714286327362, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.3279714286327362, Class Loss=0.3279714286327362, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.969006375968457
Loss made of: CE 0.5044368505477905, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.4539384841918945 EntMin 0.0
Epoch 4, Class Loss=0.4155197739601135, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.4155197739601135, Class Loss=0.4155197739601135, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=5.052071294188499
Loss made of: CE 0.4293154180049896, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.344168663024902 EntMin 0.0
Epoch 5, Class Loss=0.4649401605129242, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.4649401605129242, Class Loss=0.4649401605129242, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.931110510230065
Loss made of: CE 0.46842002868652344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.676804542541504 EntMin 0.0
Epoch 6, Class Loss=0.5276608467102051, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.5276608467102051, Class Loss=0.5276608467102051, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=5.901054346561432
Loss made of: CE 0.9156773090362549, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.611965656280518 EntMin 0.0
Epoch 1, Class Loss=0.7411848306655884, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.7411848306655884, Class Loss=0.7411848306655884, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/12, Loss=5.462747320532799
Loss made of: CE 0.48573845624923706, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.43623685836792 EntMin 0.0
Epoch 2, Class Loss=0.6612470746040344, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.6612470746040344, Class Loss=0.6612470746040344, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/12, Loss=5.330795073509217
Loss made of: CE 0.6275234222412109, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.904403209686279 EntMin 0.0
Epoch 3, Class Loss=0.6326749324798584, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.6326749324798584, Class Loss=0.6326749324798584, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/12, Loss=5.328945237398147
Loss made of: CE 0.6594256162643433, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7246270179748535 EntMin 0.0
Epoch 4, Class Loss=0.586985170841217, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.586985170841217, Class Loss=0.586985170841217, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/12, Loss=5.129762786626816
Loss made of: CE 0.6822994947433472, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.939942359924316 EntMin 0.0
Epoch 5, Class Loss=0.5828577876091003, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.5828577876091003, Class Loss=0.5828577876091003, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/12, Loss=5.2123701810836796
Loss made of: CE 0.5783183574676514, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.480731010437012 EntMin 0.0
Epoch 6, Class Loss=0.5404891967773438, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.5404891967773438, Class Loss=0.5404891967773438, Reg Loss=0.0
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.6063838601112366, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.822711
Mean Acc: 0.528690
FreqW Acc: 0.728354
Mean IoU: 0.405742
Class IoU:
	class 0: 0.8505479
	class 1: 0.77017856
	class 2: 0.31667027
	class 3: 0.39419803
	class 4: 0.46673682
	class 5: 0.6304251
	class 6: 0.64619297
	class 7: 0.76773185
	class 8: 0.70611477
	class 9: 0.16739368
	class 10: 0.0
	class 11: 0.0
	class 12: 0.47299647
	class 13: 0.22710578
	class 14: 0.0
	class 15: 0.48131555
	class 16: 0.0
Class Acc:
	class 0: 0.9117002
	class 1: 0.79439765
	class 2: 0.87498236
	class 3: 0.39795965
	class 4: 0.5068748
	class 5: 0.6661875
	class 6: 0.6506722
	class 7: 0.9430523
	class 8: 0.7389319
	class 9: 0.2826091
	class 10: 0.0
	class 11: 0.0
	class 12: 0.6003189
	class 13: 0.6896146
	class 14: 0.0
	class 15: 0.930428
	class 16: 0.0

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 25, step: 5
select part of clients to conduct local training
[24, 7, 4, 12]
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.899079608172178
Loss made of: CE 0.10432126373052597, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.583849906921387 EntMin 0.0
Epoch 1, Class Loss=0.2179664969444275, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.2179664969444275, Class Loss=0.2179664969444275, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=5.840652868151665
Loss made of: CE 0.3177036941051483, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.680147171020508 EntMin 0.0
Epoch 2, Class Loss=0.3585411012172699, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.3585411012172699, Class Loss=0.3585411012172699, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.718353855609894
Loss made of: CE 0.45804092288017273, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.465902328491211 EntMin 0.0
Epoch 3, Class Loss=0.4383057951927185, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.4383057951927185, Class Loss=0.4383057951927185, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.590181675553322
Loss made of: CE 0.5570498704910278, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.826904773712158 EntMin 0.0
Epoch 4, Class Loss=0.4984860122203827, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.4984860122203827, Class Loss=0.4984860122203827, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.368136873841285
Loss made of: CE 0.5053262710571289, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.423404216766357 EntMin 0.0
Epoch 5, Class Loss=0.5307925939559937, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.5307925939559937, Class Loss=0.5307925939559937, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.345055174827576
Loss made of: CE 0.5946121215820312, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.761362075805664 EntMin 0.0
Epoch 6, Class Loss=0.6595373153686523, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.6595373153686523, Class Loss=0.6595373153686523, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4136744439601898, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.4136744439601898, Class Loss=0.4136744439601898, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Class Loss=0.5593653917312622, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.5593653917312622, Class Loss=0.5593653917312622, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Class Loss=0.6034507751464844, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.6034507751464844, Class Loss=0.6034507751464844, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Class Loss=0.6461620330810547, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.6461620330810547, Class Loss=0.6461620330810547, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Class Loss=0.6252933740615845, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.6252933740615845, Class Loss=0.6252933740615845, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Class Loss=0.6401649713516235, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.6401649713516235, Class Loss=0.6401649713516235, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.603671646118164
Loss made of: CE 0.16722068190574646, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.820706367492676 EntMin 0.0
Epoch 1, Class Loss=0.21519002318382263, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.21519002318382263, Class Loss=0.21519002318382263, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=5.836069318652153
Loss made of: CE 0.2845858931541443, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.763652324676514 EntMin 0.0
Epoch 2, Class Loss=0.37449777126312256, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.37449777126312256, Class Loss=0.37449777126312256, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.586651915311814
Loss made of: CE 0.5160465836524963, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.826343059539795 EntMin 0.0
Epoch 3, Class Loss=0.469374418258667, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.469374418258667, Class Loss=0.469374418258667, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.553221321105957
Loss made of: CE 0.5242215394973755, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.386996269226074 EntMin 0.0
Epoch 4, Class Loss=0.5199066400527954, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.5199066400527954, Class Loss=0.5199066400527954, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.290412855148316
Loss made of: CE 0.6220608353614807, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.428723335266113 EntMin 0.0
Epoch 5, Class Loss=0.5386252999305725, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5386252999305725, Class Loss=0.5386252999305725, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.299699318408966
Loss made of: CE 0.6053546667098999, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.688332557678223 EntMin 0.0
Epoch 6, Class Loss=0.6536260843276978, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6536260843276978, Class Loss=0.6536260843276978, Reg Loss=0.0
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.843637338280677
Loss made of: CE 0.25321972370147705, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.0486907958984375 EntMin 0.0
Epoch 1, Class Loss=0.2173711061477661, Reg Loss=0.0
Clinet index 12, End of Epoch 1/6, Average Loss=0.2173711061477661, Class Loss=0.2173711061477661, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=5.966704922914505
Loss made of: CE 0.40528246760368347, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.607100963592529 EntMin 0.0
Epoch 2, Class Loss=0.37358593940734863, Reg Loss=0.0
Clinet index 12, End of Epoch 2/6, Average Loss=0.37358593940734863, Class Loss=0.37358593940734863, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=5.617552992701531
Loss made of: CE 0.538932204246521, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.152695178985596 EntMin 0.0
Epoch 3, Class Loss=0.4602004289627075, Reg Loss=0.0
Clinet index 12, End of Epoch 3/6, Average Loss=0.4602004289627075, Class Loss=0.4602004289627075, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=5.5006718903779985
Loss made of: CE 0.46900445222854614, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.515921592712402 EntMin 0.0
Epoch 4, Class Loss=0.5136687755584717, Reg Loss=0.0
Clinet index 12, End of Epoch 4/6, Average Loss=0.5136687755584717, Class Loss=0.5136687755584717, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=5.2022427946329115
Loss made of: CE 0.4934978485107422, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5226922035217285 EntMin 0.0
Epoch 5, Class Loss=0.5543876886367798, Reg Loss=0.0
Clinet index 12, End of Epoch 5/6, Average Loss=0.5543876886367798, Class Loss=0.5543876886367798, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=5.373651558160782
Loss made of: CE 0.59455406665802, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.467624664306641 EntMin 0.0
Epoch 6, Class Loss=0.6637731194496155, Reg Loss=0.0
Clinet index 12, End of Epoch 6/6, Average Loss=0.6637731194496155, Class Loss=0.6637731194496155, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8722445368766785, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.790895
Mean Acc: 0.416265
FreqW Acc: 0.674271
Mean IoU: 0.305672
Class IoU:
	class 0: 0.81941754
	class 1: 0.5542046
	class 2: 0.2996806
	class 3: 0.2877861
	class 4: 0.37065995
	class 5: 0.5120732
	class 6: 0.37276646
	class 7: 0.72976094
	class 8: 0.6299501
	class 9: 0.14499724
	class 10: 0.0
	class 11: 7.04945e-07
	class 12: 0.39812294
	class 13: 0.20930757
	class 14: 0.0
	class 15: 0.4790403
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0
Class Acc:
	class 0: 0.91606045
	class 1: 0.56848747
	class 2: 0.84601736
	class 3: 0.28990445
	class 4: 0.40411752
	class 5: 0.523113
	class 6: 0.3749472
	class 7: 0.9264049
	class 8: 0.65585417
	class 9: 0.24997622
	class 10: 0.0
	class 11: 7.04945e-07
	class 12: 0.5641372
	class 13: 0.6487884
	class 14: 0.0
	class 15: 0.94122654
	class 16: 0.0
	class 17: 0.0
	class 18: 0.0

federated global round: 26, step: 5
select part of clients to conduct local training
[6, 28, 20, 4]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.942973179370165
Loss made of: CE 0.15123291313648224, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.757723808288574 EntMin 0.0
Epoch 1, Class Loss=0.13072147965431213, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.13072147965431213, Class Loss=0.13072147965431213, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/12, Loss=4.906970754265785
Loss made of: CE 0.2706947326660156, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.170505523681641 EntMin 0.0
Epoch 2, Class Loss=0.25346052646636963, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.25346052646636963, Class Loss=0.25346052646636963, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=4.904556202888489
Loss made of: CE 0.33797287940979004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.5685882568359375 EntMin 0.0
Epoch 3, Class Loss=0.33893832564353943, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.33893832564353943, Class Loss=0.33893832564353943, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=4.725856930017471
Loss made of: CE 0.4104570746421814, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.557557106018066 EntMin 0.0
Epoch 4, Class Loss=0.3982507586479187, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3982507586479187, Class Loss=0.3982507586479187, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=5.035465088486672
Loss made of: CE 0.5168846845626831, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.910378932952881 EntMin 0.0
Epoch 5, Class Loss=0.4418049454689026, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.4418049454689026, Class Loss=0.4418049454689026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=4.862000584602356
Loss made of: CE 0.5139693021774292, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.91339111328125 EntMin 0.0
Epoch 6, Class Loss=0.5933253765106201, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5933253765106201, Class Loss=0.5933253765106201, Reg Loss=0.0
Current Client Index:  28
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.27254095673561096, Reg Loss=0.0
Clinet index 28, End of Epoch 1/6, Average Loss=0.27254095673561096, Class Loss=0.27254095673561096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.42683207988739014, Reg Loss=0.0
Clinet index 28, End of Epoch 2/6, Average Loss=0.42683207988739014, Class Loss=0.42683207988739014, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.564198911190033, Reg Loss=0.0
Clinet index 28, End of Epoch 3/6, Average Loss=0.564198911190033, Class Loss=0.564198911190033, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Class Loss=0.6494459509849548, Reg Loss=0.0
Clinet index 28, End of Epoch 4/6, Average Loss=0.6494459509849548, Class Loss=0.6494459509849548, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.6831314563751221, Reg Loss=0.0
Clinet index 28, End of Epoch 5/6, Average Loss=0.6831314563751221, Class Loss=0.6831314563751221, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.6793140172958374, Reg Loss=0.0
Clinet index 28, End of Epoch 6/6, Average Loss=0.6793140172958374, Class Loss=0.6793140172958374, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.2764676511287689, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.2764676511287689, Class Loss=0.2764676511287689, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Class Loss=0.4075249135494232, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.4075249135494232, Class Loss=0.4075249135494232, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Class Loss=0.5401641130447388, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.5401641130447388, Class Loss=0.5401641130447388, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Class Loss=0.6096651554107666, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.6096651554107666, Class Loss=0.6096651554107666, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Class Loss=0.7055484056472778, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.7055484056472778, Class Loss=0.7055484056472778, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Class Loss=0.6943950057029724, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.6943950057029724, Class Loss=0.6943950057029724, Reg Loss=0.0
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.487222290039062
Loss made of: CE 0.7094113230705261, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.205883979797363 EntMin 0.0
Epoch 1, Class Loss=0.7676575183868408, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.7676575183868408, Class Loss=0.7676575183868408, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/12, Loss=5.247025507688522
Loss made of: CE 0.6751049757003784, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.806703567504883 EntMin 0.0
Epoch 2, Class Loss=0.7040181159973145, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.7040181159973145, Class Loss=0.7040181159973145, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/12, Loss=5.1346135377883915
Loss made of: CE 0.5756598711013794, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.196993350982666 EntMin 0.0
Epoch 3, Class Loss=0.6547192335128784, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.6547192335128784, Class Loss=0.6547192335128784, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/12, Loss=5.196245497465133
Loss made of: CE 0.6425856351852417, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.126458644866943 EntMin 0.0
Epoch 4, Class Loss=0.6061167120933533, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.6061167120933533, Class Loss=0.6061167120933533, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.994531387090683
Loss made of: CE 0.6115940809249878, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.95017671585083 EntMin 0.0
Epoch 5, Class Loss=0.5849546194076538, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.5849546194076538, Class Loss=0.5849546194076538, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/12, Loss=4.732452687621117
Loss made of: CE 0.49959537386894226, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.655621528625488 EntMin 0.0
Epoch 6, Class Loss=0.5392895340919495, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.5392895340919495, Class Loss=0.5392895340919495, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8104061484336853, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.787943
Mean Acc: 0.389331
FreqW Acc: 0.664927
Mean IoU: 0.284716
Class IoU:
	class 0: 0.8131979
	class 1: 0.4786779
	class 2: 0.3055944
	class 3: 0.21376169
	class 4: 0.31448346
	class 5: 0.4298339
	class 6: 0.2586339
	class 7: 0.7498084
	class 8: 0.5921613
	class 9: 0.1376491
	class 10: 0.0
	class 11: 4.6526373e-05
	class 12: 0.39225814
	class 13: 0.21731967
	class 14: 0.0
	class 15: 0.49721244
	class 16: 0.0
	class 17: 0.0
	class 18: 0.008960491
Class Acc:
	class 0: 0.923764
	class 1: 0.48538703
	class 2: 0.84062684
	class 3: 0.21456072
	class 4: 0.33321902
	class 5: 0.43583965
	class 6: 0.2594074
	class 7: 0.92037016
	class 8: 0.60836804
	class 9: 0.25829732
	class 10: 0.0
	class 11: 4.6526373e-05
	class 12: 0.52805644
	class 13: 0.6393868
	class 14: 0.0
	class 15: 0.94097525
	class 16: 0.0
	class 17: 0.0
	class 18: 0.008987732

federated global round: 27, step: 5
select part of clients to conduct local training
[24, 8, 26, 0]
Current Client Index:  24
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=6.146238148212433
Loss made of: CE 0.7197476625442505, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.442635536193848 EntMin 0.0
Epoch 1, Class Loss=0.7126728892326355, Reg Loss=0.0
Clinet index 24, End of Epoch 1/6, Average Loss=0.7126728892326355, Class Loss=0.7126728892326355, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/12, Loss=5.323118275403976
Loss made of: CE 0.6383048295974731, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.786604881286621 EntMin 0.0
Epoch 2, Class Loss=0.64520263671875, Reg Loss=0.0
Clinet index 24, End of Epoch 2/6, Average Loss=0.64520263671875, Class Loss=0.64520263671875, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/12, Loss=5.190564489364624
Loss made of: CE 0.5686342716217041, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.304067134857178 EntMin 0.0
Epoch 3, Class Loss=0.5931519269943237, Reg Loss=0.0
Clinet index 24, End of Epoch 3/6, Average Loss=0.5931519269943237, Class Loss=0.5931519269943237, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/12, Loss=5.153138202428818
Loss made of: CE 0.5407969951629639, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.363727569580078 EntMin 0.0
Epoch 4, Class Loss=0.5741426348686218, Reg Loss=0.0
Clinet index 24, End of Epoch 4/6, Average Loss=0.5741426348686218, Class Loss=0.5741426348686218, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/12, Loss=4.88772577047348
Loss made of: CE 0.5725064277648926, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.198284149169922 EntMin 0.0
Epoch 5, Class Loss=0.5429431796073914, Reg Loss=0.0
Clinet index 24, End of Epoch 5/6, Average Loss=0.5429431796073914, Class Loss=0.5429431796073914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/12, Loss=4.802852323651313
Loss made of: CE 0.4567291736602783, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.218807697296143 EntMin 0.0
Epoch 6, Class Loss=0.5234355330467224, Reg Loss=0.0
Clinet index 24, End of Epoch 6/6, Average Loss=0.5234355330467224, Class Loss=0.5234355330467224, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.20619140565395355, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.20619140565395355, Class Loss=0.20619140565395355, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Class Loss=0.3732556104660034, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.3732556104660034, Class Loss=0.3732556104660034, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Class Loss=0.4904535710811615, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.4904535710811615, Class Loss=0.4904535710811615, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Class Loss=0.5624037981033325, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.5624037981033325, Class Loss=0.5624037981033325, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Class Loss=0.626178503036499, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.626178503036499, Class Loss=0.626178503036499, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Class Loss=0.6316823363304138, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6316823363304138, Class Loss=0.6316823363304138, Reg Loss=0.0
Current Client Index:  26
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.456482916325331
Loss made of: CE 0.13263294100761414, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7526679039001465 EntMin 0.0
Epoch 1, Class Loss=0.12360309809446335, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.12360309809446335, Class Loss=0.12360309809446335, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=4.898237167298793
Loss made of: CE 0.28574809432029724, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.263716220855713 EntMin 0.0
Epoch 2, Class Loss=0.23446762561798096, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.23446762561798096, Class Loss=0.23446762561798096, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=4.901347634196282
Loss made of: CE 0.3345014452934265, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.758732318878174 EntMin 0.0
Epoch 3, Class Loss=0.30422690510749817, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.30422690510749817, Class Loss=0.30422690510749817, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=4.902410677075386
Loss made of: CE 0.32406145334243774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.576279640197754 EntMin 0.0
Epoch 4, Class Loss=0.3639030158519745, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.3639030158519745, Class Loss=0.3639030158519745, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.597259882092476
Loss made of: CE 0.40780267119407654, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.908535003662109 EntMin 0.0
Epoch 5, Class Loss=0.3896753191947937, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.3896753191947937, Class Loss=0.3896753191947937, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.844649595022202
Loss made of: CE 0.5218026638031006, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.242717266082764 EntMin 0.0
Epoch 6, Class Loss=0.5285063982009888, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.5285063982009888, Class Loss=0.5285063982009888, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.131205126643181
Loss made of: CE 0.0920286625623703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.436672210693359 EntMin 0.0
Epoch 1, Class Loss=0.12066647410392761, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.12066647410392761, Class Loss=0.12066647410392761, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=4.699921664595604
Loss made of: CE 0.22710174322128296, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.217319965362549 EntMin 0.0
Epoch 2, Class Loss=0.2224157154560089, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.2224157154560089, Class Loss=0.2224157154560089, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=4.778335718810558
Loss made of: CE 0.38538098335266113, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.349311351776123 EntMin 0.0
Epoch 3, Class Loss=0.2931404709815979, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.2931404709815979, Class Loss=0.2931404709815979, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=4.692874738574028
Loss made of: CE 0.3121825158596039, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.537168979644775 EntMin 0.0
Epoch 4, Class Loss=0.3382403552532196, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.3382403552532196, Class Loss=0.3382403552532196, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=4.700778415799141
Loss made of: CE 0.40217888355255127, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.489335060119629 EntMin 0.0
Epoch 5, Class Loss=0.3885548710823059, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.3885548710823059, Class Loss=0.3885548710823059, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=4.751624357700348
Loss made of: CE 0.5488908290863037, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8359639644622803 EntMin 0.0
Epoch 6, Class Loss=0.5350604057312012, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.5350604057312012, Class Loss=0.5350604057312012, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8103405237197876, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.791765
Mean Acc: 0.439845
FreqW Acc: 0.687656
Mean IoU: 0.322397
Class IoU:
	class 0: 0.8258128
	class 1: 0.57110757
	class 2: 0.29891133
	class 3: 0.2767144
	class 4: 0.36398056
	class 5: 0.5507645
	class 6: 0.32263044
	class 7: 0.7594352
	class 8: 0.7082623
	class 9: 0.13613595
	class 10: 0.0
	class 11: 0.0
	class 12: 0.43214554
	class 13: 0.21811801
	class 14: 0.0
	class 15: 0.5379217
	class 16: 0.0
	class 17: 0.0
	class 18: 0.12360001
Class Acc:
	class 0: 0.905407
	class 1: 0.5825812
	class 2: 0.86198455
	class 3: 0.2786206
	class 4: 0.39138383
	class 5: 0.5670763
	class 6: 0.3237466
	class 7: 0.91541606
	class 8: 0.7501974
	class 9: 0.21747737
	class 10: 0.0
	class 11: 0.0
	class 12: 0.6268747
	class 13: 0.617257
	class 14: 0.0
	class 15: 0.92905504
	class 16: 0.0
	class 17: 0.0
	class 18: 0.38997203

federated global round: 28, step: 5
select part of clients to conduct local training
[8, 9, 16, 3]
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.0899895429611206, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=1.0899895429611206, Class Loss=1.0899895429611206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000642
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Class Loss=1.013582706451416, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=1.013582706451416, Class Loss=1.013582706451416, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000589
Epoch 3, Class Loss=0.8942179679870605, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.8942179679870605, Class Loss=0.8942179679870605, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.7770582437515259, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.7770582437515259, Class Loss=0.7770582437515259, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000482
Epoch 5, Class Loss=0.689618706703186, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.689618706703186, Class Loss=0.689618706703186, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000427
Epoch 6, Class Loss=0.6431125998497009, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.6431125998497009, Class Loss=0.6431125998497009, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.2586684226989746, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.2586684226989746, Class Loss=0.2586684226989746, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Class Loss=0.3982190787792206, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.3982190787792206, Class Loss=0.3982190787792206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Class Loss=0.5310139656066895, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.5310139656066895, Class Loss=0.5310139656066895, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Class Loss=0.6297409534454346, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.6297409534454346, Class Loss=0.6297409534454346, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Class Loss=0.6414105296134949, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.6414105296134949, Class Loss=0.6414105296134949, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.6938563585281372, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.6938563585281372, Class Loss=0.6938563585281372, Reg Loss=0.0
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.536010000482202
Loss made of: CE 0.10815014690160751, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.594795227050781 EntMin 0.0
Epoch 1, Class Loss=0.0841483622789383, Reg Loss=0.0
Clinet index 16, End of Epoch 1/6, Average Loss=0.0841483622789383, Class Loss=0.0841483622789383, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=4.31286481320858
Loss made of: CE 0.21027302742004395, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.610443115234375 EntMin 0.0
Epoch 2, Class Loss=0.1713014543056488, Reg Loss=0.0
Clinet index 16, End of Epoch 2/6, Average Loss=0.1713014543056488, Class Loss=0.1713014543056488, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.317262952029705
Loss made of: CE 0.200527161359787, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8434553146362305 EntMin 0.0
Epoch 3, Class Loss=0.2320501208305359, Reg Loss=0.0
Clinet index 16, End of Epoch 3/6, Average Loss=0.2320501208305359, Class Loss=0.2320501208305359, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.292408955097199
Loss made of: CE 0.25358903408050537, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9766900539398193 EntMin 0.0
Epoch 4, Class Loss=0.2899787724018097, Reg Loss=0.0
Clinet index 16, End of Epoch 4/6, Average Loss=0.2899787724018097, Class Loss=0.2899787724018097, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.365951746702194
Loss made of: CE 0.32625871896743774, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7521045207977295 EntMin 0.0
Epoch 5, Class Loss=0.34100693464279175, Reg Loss=0.0
Clinet index 16, End of Epoch 5/6, Average Loss=0.34100693464279175, Class Loss=0.34100693464279175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.555040428042412
Loss made of: CE 0.537975549697876, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.127771377563477 EntMin 0.0
Epoch 6, Class Loss=0.5100544691085815, Reg Loss=0.0
Clinet index 16, End of Epoch 6/6, Average Loss=0.5100544691085815, Class Loss=0.5100544691085815, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.477232635766268
Loss made of: CE 0.07530276477336884, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.228797435760498 EntMin 0.0
Epoch 1, Class Loss=0.0868118554353714, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.0868118554353714, Class Loss=0.0868118554353714, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=4.559404557943344
Loss made of: CE 0.1843961924314499, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7125349044799805 EntMin 0.0
Epoch 2, Class Loss=0.1837066411972046, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.1837066411972046, Class Loss=0.1837066411972046, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=4.595639225840569
Loss made of: CE 0.3006233870983124, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.788595676422119 EntMin 0.0
Epoch 3, Class Loss=0.2521255314350128, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.2521255314350128, Class Loss=0.2521255314350128, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=4.5536945104599
Loss made of: CE 0.29021602869033813, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9628238677978516 EntMin 0.0
Epoch 4, Class Loss=0.2987060248851776, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.2987060248851776, Class Loss=0.2987060248851776, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=4.596344590187073
Loss made of: CE 0.34899482131004333, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.421920299530029 EntMin 0.0
Epoch 5, Class Loss=0.3488271236419678, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.3488271236419678, Class Loss=0.3488271236419678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=4.677622738480568
Loss made of: CE 0.46579277515411377, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8301334381103516 EntMin 0.0
Epoch 6, Class Loss=0.4886707067489624, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.4886707067489624, Class Loss=0.4886707067489624, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7682614922523499, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.796023
Mean Acc: 0.422617
FreqW Acc: 0.683902
Mean IoU: 0.311778
Class IoU:
	class 0: 0.825404
	class 1: 0.5464318
	class 2: 0.29591346
	class 3: 0.25817373
	class 4: 0.3252713
	class 5: 0.5035167
	class 6: 0.30235845
	class 7: 0.754881
	class 8: 0.67938864
	class 9: 0.13387723
	class 10: 0.0
	class 11: 9.399267e-07
	class 12: 0.40733078
	class 13: 0.2205049
	class 14: 0.0
	class 15: 0.53216356
	class 16: 0.0
	class 17: 0.0
	class 18: 0.13857274
Class Acc:
	class 0: 0.92072225
	class 1: 0.5552692
	class 2: 0.8618842
	class 3: 0.25988793
	class 4: 0.34282503
	class 5: 0.5149162
	class 6: 0.3032759
	class 7: 0.9223244
	class 8: 0.71014094
	class 9: 0.25854582
	class 10: 0.0
	class 11: 9.399267e-07
	class 12: 0.5495308
	class 13: 0.6312366
	class 14: 0.0
	class 15: 0.929504
	class 16: 0.0
	class 17: 0.0
	class 18: 0.26965943

federated global round: 29, step: 5
select part of clients to conduct local training
[29, 27, 26, 18]
Current Client Index:  29
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=4.932132123038173
Loss made of: CE 0.16011765599250793, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.114672660827637 EntMin 0.0
Epoch 1, Class Loss=0.09836921095848083, Reg Loss=0.0
Clinet index 29, End of Epoch 1/6, Average Loss=0.09836921095848083, Class Loss=0.09836921095848083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=4.7620040565729145
Loss made of: CE 0.16099387407302856, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.8763608932495117 EntMin 0.0
Epoch 2, Class Loss=0.19658659398555756, Reg Loss=0.0
Clinet index 29, End of Epoch 2/6, Average Loss=0.19658659398555756, Class Loss=0.19658659398555756, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.417946727573872
Loss made of: CE 0.2172667235136032, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.148312091827393 EntMin 0.0
Epoch 3, Class Loss=0.25352612137794495, Reg Loss=0.0
Clinet index 29, End of Epoch 3/6, Average Loss=0.25352612137794495, Class Loss=0.25352612137794495, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.6662495449185375
Loss made of: CE 0.2753126323223114, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.1450090408325195 EntMin 0.0
Epoch 4, Class Loss=0.30748969316482544, Reg Loss=0.0
Clinet index 29, End of Epoch 4/6, Average Loss=0.30748969316482544, Class Loss=0.30748969316482544, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.5478567510843275
Loss made of: CE 0.3662113547325134, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.007204532623291 EntMin 0.0
Epoch 5, Class Loss=0.34999752044677734, Reg Loss=0.0
Clinet index 29, End of Epoch 5/6, Average Loss=0.34999752044677734, Class Loss=0.34999752044677734, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.591852113604546
Loss made of: CE 0.4855177402496338, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.7534141540527344 EntMin 0.0
Epoch 6, Class Loss=0.5085843205451965, Reg Loss=0.0
Clinet index 29, End of Epoch 6/6, Average Loss=0.5085843205451965, Class Loss=0.5085843205451965, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.18878541886806488, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.18878541886806488, Class Loss=0.18878541886806488, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Class Loss=0.3696443438529968, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.3696443438529968, Class Loss=0.3696443438529968, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Class Loss=0.42842164635658264, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.42842164635658264, Class Loss=0.42842164635658264, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Class Loss=0.5592827796936035, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.5592827796936035, Class Loss=0.5592827796936035, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Class Loss=0.6539937257766724, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.6539937257766724, Class Loss=0.6539937257766724, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Class Loss=0.7095769047737122, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.7095769047737122, Class Loss=0.7095769047737122, Reg Loss=0.0
Current Client Index:  26
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.488586699962616
Loss made of: CE 0.5329939723014832, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.371616363525391 EntMin 0.0
Epoch 1, Class Loss=0.6024898886680603, Reg Loss=0.0
Clinet index 26, End of Epoch 1/6, Average Loss=0.6024898886680603, Class Loss=0.6024898886680603, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/12, Loss=4.86947460770607
Loss made of: CE 0.5854976177215576, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1296234130859375 EntMin 0.0
Epoch 2, Class Loss=0.5720962285995483, Reg Loss=0.0
Clinet index 26, End of Epoch 2/6, Average Loss=0.5720962285995483, Class Loss=0.5720962285995483, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/12, Loss=4.832727602124214
Loss made of: CE 0.5182945728302002, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.751662731170654 EntMin 0.0
Epoch 3, Class Loss=0.5388020873069763, Reg Loss=0.0
Clinet index 26, End of Epoch 3/6, Average Loss=0.5388020873069763, Class Loss=0.5388020873069763, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/12, Loss=4.675109088420868
Loss made of: CE 0.48610520362854004, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.065097808837891 EntMin 0.0
Epoch 4, Class Loss=0.5231989622116089, Reg Loss=0.0
Clinet index 26, End of Epoch 4/6, Average Loss=0.5231989622116089, Class Loss=0.5231989622116089, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/12, Loss=4.417897287011146
Loss made of: CE 0.4823995530605316, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.268206596374512 EntMin 0.0
Epoch 5, Class Loss=0.514844536781311, Reg Loss=0.0
Clinet index 26, End of Epoch 5/6, Average Loss=0.514844536781311, Class Loss=0.514844536781311, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/12, Loss=4.533852288126946
Loss made of: CE 0.4949800968170166, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.309109687805176 EntMin 0.0
Epoch 6, Class Loss=0.49986177682876587, Reg Loss=0.0
Clinet index 26, End of Epoch 6/6, Average Loss=0.49986177682876587, Class Loss=0.49986177682876587, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=5.0217563807964325
Loss made of: CE 0.09755361080169678, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.741996765136719 EntMin 0.0
Epoch 1, Class Loss=0.09182238578796387, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.09182238578796387, Class Loss=0.09182238578796387, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/12, Loss=4.606441511213779
Loss made of: CE 0.14721213281154633, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.462224006652832 EntMin 0.0
Epoch 2, Class Loss=0.18725872039794922, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.18725872039794922, Class Loss=0.18725872039794922, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=4.516652286052704
Loss made of: CE 0.22214452922344208, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.319941520690918 EntMin 0.0
Epoch 3, Class Loss=0.24884632229804993, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.24884632229804993, Class Loss=0.24884632229804993, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/12, Loss=4.512878987193107
Loss made of: CE 0.2810654640197754, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.091584205627441 EntMin 0.0
Epoch 4, Class Loss=0.2908346354961395, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.2908346354961395, Class Loss=0.2908346354961395, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/12, Loss=4.507725787162781
Loss made of: CE 0.34930872917175293, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.9694037437438965 EntMin 0.0
Epoch 5, Class Loss=0.3554280996322632, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.3554280996322632, Class Loss=0.3554280996322632, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/12, Loss=4.581134536862374
Loss made of: CE 0.4781653583049774, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.2054948806762695 EntMin 0.0
Epoch 6, Class Loss=0.5166395306587219, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.5166395306587219, Class Loss=0.5166395306587219, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.7924076318740845, Reg Loss=0.0 (without scaling)

Total samples: 1353.000000
Overall Acc: 0.786758
Mean Acc: 0.443666
FreqW Acc: 0.689701
Mean IoU: 0.324757
Class IoU:
	class 0: 0.82545143
	class 1: 0.60387695
	class 2: 0.30204052
	class 3: 0.27797028
	class 4: 0.35975245
	class 5: 0.53321505
	class 6: 0.33292884
	class 7: 0.74812615
	class 8: 0.72072893
	class 9: 0.12674552
	class 10: 0.0
	class 11: 0.0
	class 12: 0.4535811
	class 13: 0.21715978
	class 14: 0.0
	class 15: 0.56041455
	class 16: 0.0
	class 17: 0.0
	class 18: 0.1083914
Class Acc:
	class 0: 0.89609694
	class 1: 0.61666465
	class 2: 0.8581608
	class 3: 0.27991804
	class 4: 0.38432413
	class 5: 0.5473897
	class 6: 0.33414754
	class 7: 0.91538155
	class 8: 0.7672516
	class 9: 0.19405988
	class 10: 0.0
	class 11: 0.0
	class 12: 0.6273188
	class 13: 0.60380465
	class 14: 0.0
	class 15: 0.9212042
	class 16: 0.0
	class 17: 0.0
	class 18: 0.48393315

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 30, step: 6
select part of clients to conduct local training
[10, 30, 4, 33]
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=13.14528601244092
Loss made of: CE 0.10103317350149155, LKD 0.0, LDE 0.0, LReg 0.0, POD 12.381092071533203 EntMin 0.0
Epoch 1, Class Loss=0.08611275255680084, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.08611275255680084, Class Loss=0.08611275255680084, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=11.573457673192024
Loss made of: CE 0.25276607275009155, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.591867446899414 EntMin 0.0
Epoch 2, Class Loss=0.32794636487960815, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.32794636487960815, Class Loss=0.32794636487960815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/12, Loss=11.223438355326653
Loss made of: CE 0.6879391670227051, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.72307014465332 EntMin 0.0
Epoch 3, Class Loss=0.6188578605651855, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.6188578605651855, Class Loss=0.6188578605651855, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.865656405687332
Loss made of: CE 0.7573326826095581, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.120923042297363 EntMin 0.0
Epoch 4, Class Loss=0.7663721442222595, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.7663721442222595, Class Loss=0.7663721442222595, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.52253121137619
Loss made of: CE 0.6334897875785828, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.365137100219727 EntMin 0.0
Epoch 5, Class Loss=0.7003614902496338, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.7003614902496338, Class Loss=0.7003614902496338, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.884622228145599
Loss made of: CE 0.5968170166015625, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.339458465576172 EntMin 0.0
Epoch 6, Class Loss=0.5961014628410339, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.5961014628410339, Class Loss=0.5961014628410339, Reg Loss=0.0
Current Client Index:  30
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=8.973680652305484
Loss made of: CE 0.07748743891716003, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.113134384155273 EntMin 0.0
Epoch 1, Class Loss=0.060850195586681366, Reg Loss=0.0
Clinet index 30, End of Epoch 1/6, Average Loss=0.060850195586681366, Class Loss=0.060850195586681366, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/13, Loss=7.888487474620343
Loss made of: CE 0.2336435765028, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.199340343475342 EntMin 0.0
Epoch 2, Class Loss=0.20576955378055573, Reg Loss=0.0
Clinet index 30, End of Epoch 2/6, Average Loss=0.20576955378055573, Class Loss=0.20576955378055573, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/13, Loss=6.92572916150093
Loss made of: CE 0.35727232694625854, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.930670738220215 EntMin 0.0
Epoch 3, Class Loss=0.36051055788993835, Reg Loss=0.0
Clinet index 30, End of Epoch 3/6, Average Loss=0.36051055788993835, Class Loss=0.36051055788993835, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/13, Loss=6.716895762085914
Loss made of: CE 0.5437723398208618, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.380433559417725 EntMin 0.0
Epoch 4, Class Loss=0.452460914850235, Reg Loss=0.0
Clinet index 30, End of Epoch 4/6, Average Loss=0.452460914850235, Class Loss=0.452460914850235, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/13, Loss=6.605588924884796
Loss made of: CE 0.632238507270813, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.708733558654785 EntMin 0.0
Epoch 5, Class Loss=0.5181995630264282, Reg Loss=0.0
Clinet index 30, End of Epoch 5/6, Average Loss=0.5181995630264282, Class Loss=0.5181995630264282, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/13, Loss=6.461520594358444
Loss made of: CE 0.7077395915985107, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.976344585418701 EntMin 0.0
Epoch 6, Class Loss=0.6329895853996277, Reg Loss=0.0
Clinet index 30, End of Epoch 6/6, Average Loss=0.6329895853996277, Class Loss=0.6329895853996277, Reg Loss=0.0
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=12.795664471387862
Loss made of: CE 0.05934654176235199, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.27773380279541 EntMin 0.0
Epoch 1, Class Loss=0.07178201526403427, Reg Loss=0.0
Clinet index 4, End of Epoch 1/6, Average Loss=0.07178201526403427, Class Loss=0.07178201526403427, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=11.02875355631113
Loss made of: CE 0.18213020265102386, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.678789138793945 EntMin 0.0
Epoch 2, Class Loss=0.30821630358695984, Reg Loss=0.0
Clinet index 4, End of Epoch 2/6, Average Loss=0.30821630358695984, Class Loss=0.30821630358695984, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/12, Loss=11.070943993330001
Loss made of: CE 0.6908363103866577, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.406042098999023 EntMin 0.0
Epoch 3, Class Loss=0.6518838405609131, Reg Loss=0.0
Clinet index 4, End of Epoch 3/6, Average Loss=0.6518838405609131, Class Loss=0.6518838405609131, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.51294783949852
Loss made of: CE 0.8185572624206543, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.154867172241211 EntMin 0.0
Epoch 4, Class Loss=0.7794469594955444, Reg Loss=0.0
Clinet index 4, End of Epoch 4/6, Average Loss=0.7794469594955444, Class Loss=0.7794469594955444, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.203490144014358
Loss made of: CE 0.6055150032043457, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.77357292175293 EntMin 0.0
Epoch 5, Class Loss=0.7009330987930298, Reg Loss=0.0
Clinet index 4, End of Epoch 5/6, Average Loss=0.7009330987930298, Class Loss=0.7009330987930298, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.387849235534668
Loss made of: CE 0.5741972923278809, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.699037551879883 EntMin 0.0
Epoch 6, Class Loss=0.6118766665458679, Reg Loss=0.0
Clinet index 4, End of Epoch 6/6, Average Loss=0.6118766665458679, Class Loss=0.6118766665458679, Reg Loss=0.0
Current Client Index:  33
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=12.988393788784743
Loss made of: CE 0.029129713773727417, LKD 0.0, LDE 0.0, LReg 0.0, POD 11.542667388916016 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.06882980465888977, Reg Loss=0.0
Clinet index 33, End of Epoch 1/6, Average Loss=0.06882980465888977, Class Loss=0.06882980465888977, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/12, Loss=11.7282449349761
Loss made of: CE 0.3114660680294037, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.739289283752441 EntMin 0.0
Epoch 2, Class Loss=0.31762760877609253, Reg Loss=0.0
Clinet index 33, End of Epoch 2/6, Average Loss=0.31762760877609253, Class Loss=0.31762760877609253, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 3, Batch 10/12, Loss=11.50777051448822
Loss made of: CE 0.6579558253288269, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.823671340942383 EntMin 0.0
Epoch 3, Class Loss=0.6557416319847107, Reg Loss=0.0
Clinet index 33, End of Epoch 3/6, Average Loss=0.6557416319847107, Class Loss=0.6557416319847107, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/12, Loss=10.996277385950089
Loss made of: CE 0.830024242401123, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.510502815246582 EntMin 0.0
Epoch 4, Class Loss=0.7879523038864136, Reg Loss=0.0
Clinet index 33, End of Epoch 4/6, Average Loss=0.7879523038864136, Class Loss=0.7879523038864136, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/12, Loss=10.688730347156525
Loss made of: CE 0.6171683073043823, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.447329521179199 EntMin 0.0
Epoch 5, Class Loss=0.7051255702972412, Reg Loss=0.0
Clinet index 33, End of Epoch 5/6, Average Loss=0.7051255702972412, Class Loss=0.7051255702972412, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/12, Loss=9.92614397406578
Loss made of: CE 0.5637514591217041, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.444393157958984 EntMin 0.0
Epoch 6, Class Loss=0.5964329242706299, Reg Loss=0.0
Clinet index 33, End of Epoch 6/6, Average Loss=0.5964329242706299, Class Loss=0.5964329242706299, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.9716817736625671, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.758857
Mean Acc: 0.324292
FreqW Acc: 0.621632
Mean IoU: 0.232381
Class IoU:
	class 0: 0.7807486
	class 1: 0.42230752
	class 2: 0.32026538
	class 3: 0.053648487
	class 4: 0.16486219
	class 5: 0.32509997
	class 6: 0.22657295
	class 7: 0.74538684
	class 8: 0.37373084
	class 9: 0.1146916
	class 10: 0.0
	class 11: 4.2296692e-06
	class 12: 0.33535054
	class 13: 0.20628773
	class 14: 0.0
	class 15: 0.46147782
	class 16: 0.0
	class 17: 0.0
	class 18: 0.064500555
	class 19: 0.28507155
	class 20: 0.0
Class Acc:
	class 0: 0.91545767
	class 1: 0.42761996
	class 2: 0.78659153
	class 3: 0.053670898
	class 4: 0.1696385
	class 5: 0.32668346
	class 6: 0.22725452
	class 7: 0.89799184
	class 8: 0.37698814
	class 9: 0.19522226
	class 10: 0.0
	class 11: 4.22967e-06
	class 12: 0.43296948
	class 13: 0.6015444
	class 14: 0.0
	class 15: 0.94299114
	class 16: 0.0
	class 17: 0.0
	class 18: 0.07752324
	class 19: 0.37798455
	class 20: 0.0

federated global round: 31, step: 6
select part of clients to conduct local training
[9, 17, 14, 1]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=8.296495773736387
Loss made of: CE 0.0962478369474411, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.2054572105407715 EntMin 0.0
Epoch 1, Class Loss=0.05285881832242012, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.05285881832242012, Class Loss=0.05285881832242012, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=7.131988805532456
Loss made of: CE 0.08234228193759918, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.997361660003662 EntMin 0.0
Epoch 2, Class Loss=0.19386158883571625, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.19386158883571625, Class Loss=0.19386158883571625, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=6.445572605729103
Loss made of: CE 0.37377259135246277, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.410026550292969 EntMin 0.0
Epoch 3, Class Loss=0.31219807267189026, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.31219807267189026, Class Loss=0.31219807267189026, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.202611115574837
Loss made of: CE 0.3915519416332245, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.909314155578613 EntMin 0.0
Epoch 4, Class Loss=0.4387089014053345, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.4387089014053345, Class Loss=0.4387089014053345, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=6.018314445018769
Loss made of: CE 0.4767061471939087, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.403329849243164 EntMin 0.0
Epoch 5, Class Loss=0.5009894967079163, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.5009894967079163, Class Loss=0.5009894967079163, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=6.026096555590629
Loss made of: CE 0.44882258772850037, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.2645392417907715 EntMin 0.0
Epoch 6, Class Loss=0.549132227897644, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.549132227897644, Class Loss=0.549132227897644, Reg Loss=0.0
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/12, Loss=9.34897732399404
Loss made of: CE 0.0159638449549675, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.727945327758789 EntMin 0.0
Epoch 1, Class Loss=0.036961719393730164, Reg Loss=0.0
Clinet index 17, End of Epoch 1/6, Average Loss=0.036961719393730164, Class Loss=0.036961719393730164, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/12, Loss=9.48803424462676
Loss made of: CE 0.10937526077032089, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.480276107788086 EntMin 0.0
Epoch 2, Class Loss=0.14527422189712524, Reg Loss=0.0
Clinet index 17, End of Epoch 2/6, Average Loss=0.14527422189712524, Class Loss=0.14527422189712524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/12, Loss=8.924204298853875
Loss made of: CE 0.2785826027393341, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.512578964233398 EntMin 0.0
Epoch 3, Class Loss=0.29137611389160156, Reg Loss=0.0
Clinet index 17, End of Epoch 3/6, Average Loss=0.29137611389160156, Class Loss=0.29137611389160156, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.769615158438683
Loss made of: CE 0.3827265501022339, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.29635763168335 EntMin 0.0
Epoch 4, Class Loss=0.3888999819755554, Reg Loss=0.0
Clinet index 17, End of Epoch 4/6, Average Loss=0.3888999819755554, Class Loss=0.3888999819755554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.282930505275726
Loss made of: CE 0.45078837871551514, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.012004375457764 EntMin 0.0
Epoch 5, Class Loss=0.4258061349391937, Reg Loss=0.0
Clinet index 17, End of Epoch 5/6, Average Loss=0.4258061349391937, Class Loss=0.4258061349391937, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=7.939357760548591
Loss made of: CE 0.36531195044517517, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.416539669036865 EntMin 0.0
Epoch 6, Class Loss=0.4009108543395996, Reg Loss=0.0
Clinet index 17, End of Epoch 6/6, Average Loss=0.4009108543395996, Class Loss=0.4009108543395996, Reg Loss=0.0
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=8.434458689019085
Loss made of: CE 0.08363064378499985, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.11194133758545 EntMin 0.0
Epoch 1, Class Loss=0.05931004136800766, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.05931004136800766, Class Loss=0.05931004136800766, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/13, Loss=7.022753065824508
Loss made of: CE 0.1483037918806076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.826186656951904 EntMin 0.0
Epoch 2, Class Loss=0.22016079723834991, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.22016079723834991, Class Loss=0.22016079723834991, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/13, Loss=6.5373167306184765
Loss made of: CE 0.19033083319664001, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.992725849151611 EntMin 0.0
Epoch 3, Class Loss=0.3338634669780731, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.3338634669780731, Class Loss=0.3338634669780731, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/13, Loss=6.2752309292554855
Loss made of: CE 0.27149105072021484, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.221972942352295 EntMin 0.0
Epoch 4, Class Loss=0.4179617166519165, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.4179617166519165, Class Loss=0.4179617166519165, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/13, Loss=5.930875608325005
Loss made of: CE 0.4221828877925873, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.0302252769470215 EntMin 0.0
Epoch 5, Class Loss=0.4759852886199951, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.4759852886199951, Class Loss=0.4759852886199951, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/13, Loss=6.02408296763897
Loss made of: CE 0.7005228996276855, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.922125816345215 EntMin 0.0
Epoch 6, Class Loss=0.523152232170105, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.523152232170105, Class Loss=0.523152232170105, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/12, Loss=9.448724222555757
Loss made of: CE 0.033689819276332855, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.19952392578125 EntMin 0.0
Epoch 1, Class Loss=0.028868742287158966, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.028868742287158966, Class Loss=0.028868742287158966, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000962
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/12, Loss=8.917955081164838
Loss made of: CE 0.19585971534252167, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.273052215576172 EntMin 0.0
Epoch 2, Class Loss=0.13878881931304932, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.13878881931304932, Class Loss=0.13878881931304932, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/12, Loss=8.680494177341462
Loss made of: CE 0.25774210691452026, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.019943237304688 EntMin 0.0
Epoch 3, Class Loss=0.3085289001464844, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.3085289001464844, Class Loss=0.3085289001464844, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/12, Loss=8.750133126974106
Loss made of: CE 0.39019501209259033, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.426347732543945 EntMin 0.0
Epoch 4, Class Loss=0.40371954441070557, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.40371954441070557, Class Loss=0.40371954441070557, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/12, Loss=8.525287848711013
Loss made of: CE 0.42106562852859497, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.512170791625977 EntMin 0.0
Epoch 5, Class Loss=0.4274408519268036, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.4274408519268036, Class Loss=0.4274408519268036, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/12, Loss=8.11212098300457
Loss made of: CE 0.4939323365688324, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.824573040008545 EntMin 0.0
Epoch 6, Class Loss=0.422671377658844, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.422671377658844, Class Loss=0.422671377658844, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8750665187835693, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.778853
Mean Acc: 0.370230
FreqW Acc: 0.654348
Mean IoU: 0.270365
Class IoU:
	class 0: 0.8060592
	class 1: 0.46544078
	class 2: 0.3166132
	class 3: 0.099224515
	class 4: 0.28811646
	class 5: 0.4292496
	class 6: 0.37451
	class 7: 0.7500522
	class 8: 0.4712773
	class 9: 0.12563019
	class 10: 0.0
	class 11: 8.482815e-05
	class 12: 0.36798283
	class 13: 0.19677615
	class 14: 0.0
	class 15: 0.5280012
	class 16: 0.0
	class 17: 0.0
	class 18: 0.13461469
	class 19: 0.324035
	class 20: 0.0
Class Acc:
	class 0: 0.9232545
	class 1: 0.47145194
	class 2: 0.78121823
	class 3: 0.099328205
	class 4: 0.30536962
	class 5: 0.43524724
	class 6: 0.37703717
	class 7: 0.90777546
	class 8: 0.47889844
	class 9: 0.2299314
	class 10: 0.0
	class 11: 8.482838e-05
	class 12: 0.47724092
	class 13: 0.6428829
	class 14: 0.0
	class 15: 0.92766345
	class 16: 0.0
	class 17: 0.0
	class 18: 0.19887005
	class 19: 0.518578
	class 20: 0.0

federated global round: 32, step: 6
select part of clients to conduct local training
[3, 8, 27, 7]
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=8.708552949782462
Loss made of: CE 0.010984616354107857, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.097616195678711 EntMin 0.0
Epoch 1, Class Loss=0.03158070892095566, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.03158070892095566, Class Loss=0.03158070892095566, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/12, Loss=8.073319794237614
Loss made of: CE 0.1640443503856659, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.731892585754395 EntMin 0.0
Epoch 2, Class Loss=0.12511974573135376, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.12511974573135376, Class Loss=0.12511974573135376, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/12, Loss=7.879684974253178
Loss made of: CE 0.3200722932815552, LKD 0.0, LDE 0.0, LReg 0.0, POD 10.097286224365234 EntMin 0.0
Epoch 3, Class Loss=0.25417661666870117, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.25417661666870117, Class Loss=0.25417661666870117, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/12, Loss=7.674457311630249
Loss made of: CE 0.33493030071258545, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.827043533325195 EntMin 0.0
Epoch 4, Class Loss=0.3336953818798065, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.3336953818798065, Class Loss=0.3336953818798065, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/12, Loss=7.445833936333656
Loss made of: CE 0.2879943251609802, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.710915565490723 EntMin 0.0
Epoch 5, Class Loss=0.38451239466667175, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.38451239466667175, Class Loss=0.38451239466667175, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/12, Loss=7.24663625061512
Loss made of: CE 0.4052622318267822, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.381468772888184 EntMin 0.0
Epoch 6, Class Loss=0.3898397386074066, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.3898397386074066, Class Loss=0.3898397386074066, Reg Loss=0.0
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.543405007198453
Loss made of: CE 0.0664796382188797, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.4782867431640625 EntMin 0.0
Epoch 1, Class Loss=0.05759477987885475, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.05759477987885475, Class Loss=0.05759477987885475, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=5.918366152048111
Loss made of: CE 0.19198831915855408, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.088409423828125 EntMin 0.0
Epoch 2, Class Loss=0.20085889101028442, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.20085889101028442, Class Loss=0.20085889101028442, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.863039702177048
Loss made of: CE 0.27320921421051025, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.112133026123047 EntMin 0.0
Epoch 3, Class Loss=0.2893748879432678, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.2893748879432678, Class Loss=0.2893748879432678, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.584120865166187
Loss made of: CE 0.4789145886898041, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.756654262542725 EntMin 0.0
Epoch 4, Class Loss=0.370053768157959, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.370053768157959, Class Loss=0.370053768157959, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.536017662286758
Loss made of: CE 0.34526753425598145, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.855504035949707 EntMin 0.0
Epoch 5, Class Loss=0.4513762295246124, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.4513762295246124, Class Loss=0.4513762295246124, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.3680334240198135
Loss made of: CE 0.5512685179710388, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.060153961181641 EntMin 0.0
Epoch 6, Class Loss=0.48277947306632996, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.48277947306632996, Class Loss=0.48277947306632996, Reg Loss=0.0
Current Client Index:  27
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.556471177935601
Loss made of: CE 0.10719054937362671, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.357233047485352 EntMin 0.0
Epoch 1, Class Loss=0.06327187269926071, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.06327187269926071, Class Loss=0.06327187269926071, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=5.758217082172632
Loss made of: CE 0.2832367420196533, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.4803290367126465 EntMin 0.0
Epoch 2, Class Loss=0.20153605937957764, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.20153605937957764, Class Loss=0.20153605937957764, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.771172407269478
Loss made of: CE 0.3250749707221985, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.51458740234375 EntMin 0.0
Epoch 3, Class Loss=0.2894200086593628, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.2894200086593628, Class Loss=0.2894200086593628, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=5.5221026092767715
Loss made of: CE 0.28830528259277344, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.359500408172607 EntMin 0.0
Epoch 4, Class Loss=0.38271644711494446, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.38271644711494446, Class Loss=0.38271644711494446, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.433117836713791
Loss made of: CE 0.4520528018474579, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.6892290115356445 EntMin 0.0
Epoch 5, Class Loss=0.45114192366600037, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.45114192366600037, Class Loss=0.45114192366600037, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.5632323801517485
Loss made of: CE 0.5505639910697937, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.639383316040039 EntMin 0.0
Epoch 6, Class Loss=0.5561122894287109, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.5561122894287109, Class Loss=0.5561122894287109, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.872532127611339
Loss made of: CE 0.0321708619594574, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.129349708557129 EntMin 0.0
Epoch 1, Class Loss=0.05786027014255524, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.05786027014255524, Class Loss=0.05786027014255524, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/13, Loss=6.202235177159309
Loss made of: CE 0.1590675413608551, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.338595867156982 EntMin 0.0
Epoch 2, Class Loss=0.18740931153297424, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.18740931153297424, Class Loss=0.18740931153297424, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/13, Loss=5.70799720287323
Loss made of: CE 0.2504398226737976, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7083539962768555 EntMin 0.0
Epoch 3, Class Loss=0.2899218499660492, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.2899218499660492, Class Loss=0.2899218499660492, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/13, Loss=6.023071381449699
Loss made of: CE 0.40254300832748413, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.023929119110107 EntMin 0.0
Epoch 4, Class Loss=0.39053794741630554, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.39053794741630554, Class Loss=0.39053794741630554, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/13, Loss=5.61089214682579
Loss made of: CE 0.4305313527584076, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.92331075668335 EntMin 0.0
Epoch 5, Class Loss=0.4392855167388916, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.4392855167388916, Class Loss=0.4392855167388916, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/13, Loss=5.6684364676475525
Loss made of: CE 0.5039969682693481, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.7503461837768555 EntMin 0.0
Epoch 6, Class Loss=0.5670269131660461, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.5670269131660461, Class Loss=0.5670269131660461, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8718488216400146, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.779010
Mean Acc: 0.401461
FreqW Acc: 0.663135
Mean IoU: 0.295826
Class IoU:
	class 0: 0.80755347
	class 1: 0.5839767
	class 2: 0.3052646
	class 3: 0.15130107
	class 4: 0.37321353
	class 5: 0.51445305
	class 6: 0.52408546
	class 7: 0.75313747
	class 8: 0.58196664
	class 9: 0.13115744
	class 10: 0.0
	class 11: 0.0001372262
	class 12: 0.39722183
	class 13: 0.19464295
	class 14: 0.0
	class 15: 0.51674426
	class 16: 0.0
	class 17: 0.0
	class 18: 0.15629192
	class 19: 0.2185349
	class 20: 0.0026666045
Class Acc:
	class 0: 0.90846246
	class 1: 0.5985332
	class 2: 0.7803911
	class 3: 0.15169136
	class 4: 0.40296108
	class 5: 0.52865005
	class 6: 0.53256446
	class 7: 0.902961
	class 8: 0.6014546
	class 9: 0.2353945
	class 10: 0.0
	class 11: 0.0001372293
	class 12: 0.5544331
	class 13: 0.6740821
	class 14: 0.0
	class 15: 0.93727344
	class 16: 0.0
	class 17: 0.0
	class 18: 0.3775824
	class 19: 0.2414448
	class 20: 0.0026666815

federated global round: 33, step: 6
select part of clients to conduct local training
[6, 20, 13, 10]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/13, Loss=5.176526091992855
Loss made of: CE 0.016937032341957092, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.707025527954102 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=0.03437536954879761, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.03437536954879761, Class Loss=0.03437536954879761, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.332694759964943
Loss made of: CE 0.10632295906543732, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.197208881378174 EntMin 0.0
Epoch 2, Class Loss=0.12366947531700134, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.12366947531700134, Class Loss=0.12366947531700134, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.2138775661587715
Loss made of: CE 0.2469906508922577, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.674581527709961 EntMin 0.0
Epoch 3, Class Loss=0.20629800856113434, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.20629800856113434, Class Loss=0.20629800856113434, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.3949531957507135
Loss made of: CE 0.31992992758750916, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.140127182006836 EntMin 0.0
Epoch 4, Class Loss=0.3096480071544647, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.3096480071544647, Class Loss=0.3096480071544647, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.333466970920563
Loss made of: CE 0.34627699851989746, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.565183639526367 EntMin 0.0
Epoch 5, Class Loss=0.3741108477115631, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.3741108477115631, Class Loss=0.3741108477115631, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.0283614754676815
Loss made of: CE 0.47645077109336853, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.223198890686035 EntMin 0.0
Epoch 6, Class Loss=0.5099315047264099, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.5099315047264099, Class Loss=0.5099315047264099, Reg Loss=0.0
Current Client Index:  20
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=5.08312692027539
Loss made of: CE 0.02604038640856743, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.870326042175293 EntMin 0.0
Epoch 1, Class Loss=0.03790835663676262, Reg Loss=0.0
Clinet index 20, End of Epoch 1/6, Average Loss=0.03790835663676262, Class Loss=0.03790835663676262, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/13, Loss=5.240864485129714
Loss made of: CE 0.09153684228658676, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.512684345245361 EntMin 0.0
Epoch 2, Class Loss=0.12236840277910233, Reg Loss=0.0
Clinet index 20, End of Epoch 2/6, Average Loss=0.12236840277910233, Class Loss=0.12236840277910233, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/13, Loss=5.29303065687418
Loss made of: CE 0.20884248614311218, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.189189910888672 EntMin 0.0
Epoch 3, Class Loss=0.21996404230594635, Reg Loss=0.0
Clinet index 20, End of Epoch 3/6, Average Loss=0.21996404230594635, Class Loss=0.21996404230594635, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/13, Loss=5.118456597626209
Loss made of: CE 0.26316937804222107, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.138335704803467 EntMin 0.0
Epoch 4, Class Loss=0.29645952582359314, Reg Loss=0.0
Clinet index 20, End of Epoch 4/6, Average Loss=0.29645952582359314, Class Loss=0.29645952582359314, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/13, Loss=5.229571625590324
Loss made of: CE 0.4369351863861084, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.445637226104736 EntMin 0.0
Epoch 5, Class Loss=0.3664916753768921, Reg Loss=0.0
Clinet index 20, End of Epoch 5/6, Average Loss=0.3664916753768921, Class Loss=0.3664916753768921, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/13, Loss=5.2061281442642215
Loss made of: CE 0.4808620810508728, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.613918304443359 EntMin 0.0
Epoch 6, Class Loss=0.4805932343006134, Reg Loss=0.0
Clinet index 20, End of Epoch 6/6, Average Loss=0.4805932343006134, Class Loss=0.4805932343006134, Reg Loss=0.0
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=9.892113677598536
Loss made of: CE 0.02771984413266182, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.33326530456543 EntMin 0.0
Epoch 1, Class Loss=0.0423923060297966, Reg Loss=0.0
Clinet index 13, End of Epoch 1/6, Average Loss=0.0423923060297966, Class Loss=0.0423923060297966, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/12, Loss=8.008637508749961
Loss made of: CE 0.19453909993171692, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.670760154724121 EntMin 0.0
Epoch 2, Class Loss=0.17978255450725555, Reg Loss=0.0
Clinet index 13, End of Epoch 2/6, Average Loss=0.17978255450725555, Class Loss=0.17978255450725555, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/12, Loss=8.078657880425453
Loss made of: CE 0.2641547918319702, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.755171775817871 EntMin 0.0
Epoch 3, Class Loss=0.3189248740673065, Reg Loss=0.0
Clinet index 13, End of Epoch 3/6, Average Loss=0.3189248740673065, Class Loss=0.3189248740673065, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/12, Loss=7.883013048768044
Loss made of: CE 0.35597771406173706, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.375375747680664 EntMin 0.0
Epoch 4, Class Loss=0.3961387574672699, Reg Loss=0.0
Clinet index 13, End of Epoch 4/6, Average Loss=0.3961387574672699, Class Loss=0.3961387574672699, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/12, Loss=7.494641950726509
Loss made of: CE 0.37579721212387085, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.506862163543701 EntMin 0.0
Epoch 5, Class Loss=0.39164823293685913, Reg Loss=0.0
Clinet index 13, End of Epoch 5/6, Average Loss=0.39164823293685913, Class Loss=0.39164823293685913, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/12, Loss=7.455864715576172
Loss made of: CE 0.39283961057662964, LKD 0.0, LDE 0.0, LReg 0.0, POD 8.17121696472168 EntMin 0.0
Epoch 6, Class Loss=0.3939245641231537, Reg Loss=0.0
Clinet index 13, End of Epoch 6/6, Average Loss=0.3939245641231537, Class Loss=0.3939245641231537, Reg Loss=0.0
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/12, Loss=11.216660571098327
Loss made of: CE 0.8628351092338562, LKD 0.0, LDE 0.0, LReg 0.0, POD 9.300756454467773 EntMin 0.0
Epoch 1, Class Loss=0.8885184526443481, Reg Loss=0.0
Clinet index 10, End of Epoch 1/6, Average Loss=0.8885184526443481, Class Loss=0.8885184526443481, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/12, Loss=8.800242441892625
Loss made of: CE 0.6576575040817261, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.573393821716309 EntMin 0.0
Epoch 2, Class Loss=0.6114643216133118, Reg Loss=0.0
Clinet index 10, End of Epoch 2/6, Average Loss=0.6114643216133118, Class Loss=0.6114643216133118, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/12, Loss=8.322979810833932
Loss made of: CE 0.43942898511886597, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.803321838378906 EntMin 0.0
Epoch 3, Class Loss=0.44268620014190674, Reg Loss=0.0
Clinet index 10, End of Epoch 3/6, Average Loss=0.44268620014190674, Class Loss=0.44268620014190674, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/12, Loss=7.9392934262752535
Loss made of: CE 0.34760910272598267, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.827943325042725 EntMin 0.0
Epoch 4, Class Loss=0.3690822124481201, Reg Loss=0.0
Clinet index 10, End of Epoch 4/6, Average Loss=0.3690822124481201, Class Loss=0.3690822124481201, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/12, Loss=7.591017484664917
Loss made of: CE 0.32302165031433105, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.795072555541992 EntMin 0.0
Epoch 5, Class Loss=0.3337477445602417, Reg Loss=0.0
Clinet index 10, End of Epoch 5/6, Average Loss=0.3337477445602417, Class Loss=0.3337477445602417, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/12, Loss=7.487357375025749
Loss made of: CE 0.3164311349391937, LKD 0.0, LDE 0.0, LReg 0.0, POD 7.949180603027344 EntMin 0.0
Epoch 6, Class Loss=0.314934641122818, Reg Loss=0.0
Clinet index 10, End of Epoch 6/6, Average Loss=0.314934641122818, Class Loss=0.314934641122818, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8218052387237549, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.785296
Mean Acc: 0.398938
FreqW Acc: 0.671493
Mean IoU: 0.287727
Class IoU:
	class 0: 0.8224804
	class 1: 0.48042452
	class 2: 0.31324294
	class 3: 0.15449032
	class 4: 0.2937609
	class 5: 0.4946923
	class 6: 0.39885622
	class 7: 0.76432014
	class 8: 0.5834288
	class 9: 0.13446477
	class 10: 0.0
	class 11: 9.540256e-05
	class 12: 0.38701287
	class 13: 0.20208536
	class 14: 0.0
	class 15: 0.5190191
	class 16: 0.0
	class 17: 0.0
	class 18: 0.16842024
	class 19: 0.32547036
	class 20: 9.750207e-06
Class Acc:
	class 0: 0.91635066
	class 1: 0.48703867
	class 2: 0.7958394
	class 3: 0.15466419
	class 4: 0.31339422
	class 5: 0.5055871
	class 6: 0.40154952
	class 7: 0.9044676
	class 8: 0.5985582
	class 9: 0.2388681
	class 10: 0.0
	class 11: 9.540256e-05
	class 12: 0.52164817
	class 13: 0.65500116
	class 14: 0.0
	class 15: 0.93245065
	class 16: 0.0
	class 17: 0.0
	class 18: 0.28094244
	class 19: 0.67123014
	class 20: 9.750207e-06

federated global round: 34, step: 6
select part of clients to conduct local training
[19, 18, 27, 14]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.157425074465573
Loss made of: CE 0.013040471822023392, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.528871059417725 EntMin 0.0
Epoch 1, Class Loss=0.04347167909145355, Reg Loss=0.0
Clinet index 19, End of Epoch 1/6, Average Loss=0.04347167909145355, Class Loss=0.04347167909145355, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.629874377697706
Loss made of: CE 0.17796406149864197, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.790452480316162 EntMin 0.0
Epoch 2, Class Loss=0.15081572532653809, Reg Loss=0.0
Clinet index 19, End of Epoch 2/6, Average Loss=0.15081572532653809, Class Loss=0.15081572532653809, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.293324621021748
Loss made of: CE 0.24730324745178223, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.046744346618652 EntMin 0.0
Epoch 3, Class Loss=0.23811385035514832, Reg Loss=0.0
Clinet index 19, End of Epoch 3/6, Average Loss=0.23811385035514832, Class Loss=0.23811385035514832, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=5.058914540708065
Loss made of: CE 0.2947501540184021, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.875286102294922 EntMin 0.0
Epoch 4, Class Loss=0.3063527047634125, Reg Loss=0.0
Clinet index 19, End of Epoch 4/6, Average Loss=0.3063527047634125, Class Loss=0.3063527047634125, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=5.410705849528313
Loss made of: CE 0.2803637385368347, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.857386589050293 EntMin 0.0
Epoch 5, Class Loss=0.3927896022796631, Reg Loss=0.0
Clinet index 19, End of Epoch 5/6, Average Loss=0.3927896022796631, Class Loss=0.3927896022796631, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.340928438305855
Loss made of: CE 0.4324142336845398, LKD 0.0, LDE 0.0, LReg 0.0, POD 3.9556503295898438 EntMin 0.0
Epoch 6, Class Loss=0.5228845477104187, Reg Loss=0.0
Clinet index 19, End of Epoch 6/6, Average Loss=0.5228845477104187, Class Loss=0.5228845477104187, Reg Loss=0.0
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=5.857090658508241
Loss made of: CE 0.04604653641581535, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.616933345794678 EntMin 0.0
Epoch 1, Class Loss=0.039388369768857956, Reg Loss=0.0
Clinet index 18, End of Epoch 1/6, Average Loss=0.039388369768857956, Class Loss=0.039388369768857956, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/13, Loss=5.087549913674593
Loss made of: CE 0.10844911634922028, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.642264366149902 EntMin 0.0
Epoch 2, Class Loss=0.12886229157447815, Reg Loss=0.0
Clinet index 18, End of Epoch 2/6, Average Loss=0.12886229157447815, Class Loss=0.12886229157447815, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/13, Loss=5.1095664098858835
Loss made of: CE 0.20857247710227966, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.112683296203613 EntMin 0.0
Epoch 3, Class Loss=0.20188754796981812, Reg Loss=0.0
Clinet index 18, End of Epoch 3/6, Average Loss=0.20188754796981812, Class Loss=0.20188754796981812, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/13, Loss=4.941572819650173
Loss made of: CE 0.23453234136104584, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.05267333984375 EntMin 0.0
Epoch 4, Class Loss=0.29142704606056213, Reg Loss=0.0
Clinet index 18, End of Epoch 4/6, Average Loss=0.29142704606056213, Class Loss=0.29142704606056213, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/13, Loss=4.973331797122955
Loss made of: CE 0.427224725484848, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.38575553894043 EntMin 0.0
Epoch 5, Class Loss=0.3694791793823242, Reg Loss=0.0
Clinet index 18, End of Epoch 5/6, Average Loss=0.3694791793823242, Class Loss=0.3694791793823242, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/13, Loss=5.07825588285923
Loss made of: CE 0.4534789025783539, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.240337371826172 EntMin 0.0
Epoch 6, Class Loss=0.5186691284179688, Reg Loss=0.0
Clinet index 18, End of Epoch 6/6, Average Loss=0.5186691284179688, Class Loss=0.5186691284179688, Reg Loss=0.0
Current Client Index:  27
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/13, Loss=6.554391020536423
Loss made of: CE 0.6512637138366699, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.495465278625488 EntMin 0.0
Epoch 1, Class Loss=0.6876073479652405, Reg Loss=0.0
Clinet index 27, End of Epoch 1/6, Average Loss=0.6876073479652405, Class Loss=0.6876073479652405, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/13, Loss=5.527587482333184
Loss made of: CE 0.8165013194084167, LKD 0.0, LDE 0.0, LReg 0.0, POD 6.0421881675720215 EntMin 0.0
Epoch 2, Class Loss=0.6289551854133606, Reg Loss=0.0
Clinet index 27, End of Epoch 2/6, Average Loss=0.6289551854133606, Class Loss=0.6289551854133606, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/13, Loss=5.399903181195259
Loss made of: CE 0.5464829206466675, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.765961170196533 EntMin 0.0
Epoch 3, Class Loss=0.5597319602966309, Reg Loss=0.0
Clinet index 27, End of Epoch 3/6, Average Loss=0.5597319602966309, Class Loss=0.5597319602966309, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/13, Loss=5.192711552977562
Loss made of: CE 0.4827151894569397, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.048694133758545 EntMin 0.0
Epoch 4, Class Loss=0.5289011001586914, Reg Loss=0.0
Clinet index 27, End of Epoch 4/6, Average Loss=0.5289011001586914, Class Loss=0.5289011001586914, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/13, Loss=5.187340340018272
Loss made of: CE 0.5378782749176025, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.316776275634766 EntMin 0.0
Epoch 5, Class Loss=0.5250711441040039, Reg Loss=0.0
Clinet index 27, End of Epoch 5/6, Average Loss=0.5250711441040039, Class Loss=0.5250711441040039, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/13, Loss=5.031932139396668
Loss made of: CE 0.49126967787742615, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.186759948730469 EntMin 0.0
Epoch 6, Class Loss=0.500027596950531, Reg Loss=0.0
Clinet index 27, End of Epoch 6/6, Average Loss=0.500027596950531, Class Loss=0.500027596950531, Reg Loss=0.0
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: entropy
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 1, Batch 10/13, Loss=6.661857730150222
Loss made of: CE 0.7549176216125488, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.1891021728515625 EntMin 0.0
Epoch 1, Class Loss=0.6616324782371521, Reg Loss=0.0
Clinet index 14, End of Epoch 1/6, Average Loss=0.6616324782371521, Class Loss=0.6616324782371521, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 2, lr = 0.000655
Epoch 2, Batch 10/13, Loss=5.629487788677215
Loss made of: CE 0.48526477813720703, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.910006999969482 EntMin 0.0
Epoch 2, Class Loss=0.6071131825447083, Reg Loss=0.0
Clinet index 14, End of Epoch 2/6, Average Loss=0.6071131825447083, Class Loss=0.6071131825447083, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/13, Loss=5.390536707639694
Loss made of: CE 0.45804131031036377, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.243350505828857 EntMin 0.0
Epoch 3, Class Loss=0.5496058464050293, Reg Loss=0.0
Clinet index 14, End of Epoch 3/6, Average Loss=0.5496058464050293, Class Loss=0.5496058464050293, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 4, lr = 0.000414
Epoch 4, Batch 10/13, Loss=5.188713809847831
Loss made of: CE 0.4398898482322693, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.716322898864746 EntMin 0.0
Epoch 4, Class Loss=0.5166252851486206, Reg Loss=0.0
Clinet index 14, End of Epoch 4/6, Average Loss=0.5166252851486206, Class Loss=0.5166252851486206, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 5, lr = 0.000287
Epoch 5, Batch 10/13, Loss=5.153370428085327
Loss made of: CE 0.44737571477890015, LKD 0.0, LDE 0.0, LReg 0.0, POD 4.255129814147949 EntMin 0.0
Epoch 5, Class Loss=0.5077903866767883, Reg Loss=0.0
Clinet index 14, End of Epoch 5/6, Average Loss=0.5077903866767883, Class Loss=0.5077903866767883, Reg Loss=0.0
Pseudo labeling is: entropy
Epoch 6, lr = 0.000154
Epoch 6, Batch 10/13, Loss=5.17035396695137
Loss made of: CE 0.6449588537216187, LKD 0.0, LDE 0.0, LReg 0.0, POD 5.167333602905273 EntMin 0.0
Epoch 6, Class Loss=0.5025930404663086, Reg Loss=0.0
Clinet index 14, End of Epoch 6/6, Average Loss=0.5025930404663086, Class Loss=0.5025930404663086, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.8362725377082825, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.783378
Mean Acc: 0.406313
FreqW Acc: 0.678838
Mean IoU: 0.292054
Class IoU:
	class 0: 0.8315895
	class 1: 0.5717592
	class 2: 0.3125374
	class 3: 0.1866705
	class 4: 0.37051177
	class 5: 0.50570947
	class 6: 0.32922897
	class 7: 0.76718247
	class 8: 0.66871136
	class 9: 0.11789487
	class 10: 0.0
	class 11: 6.532481e-05
	class 12: 0.40610814
	class 13: 0.20417932
	class 14: 0.0
	class 15: 0.5678618
	class 16: 0.0
	class 17: 0.0
	class 18: 0.15727317
	class 19: 0.030059446
	class 20: 0.10579053
Class Acc:
	class 0: 0.9193609
	class 1: 0.58181083
	class 2: 0.7825176
	class 3: 0.18747358
	class 4: 0.39415348
	class 5: 0.52140933
	class 6: 0.33277813
	class 7: 0.8962757
	class 8: 0.7012921
	class 9: 0.2004944
	class 10: 0.0
	class 11: 6.532491e-05
	class 12: 0.5629592
	class 13: 0.6700012
	class 14: 0.0
	class 15: 0.9178532
	class 16: 0.0
	class 17: 0.0
	class 18: 0.41001725
	class 19: 0.030322727
	class 20: 0.42379692

voc_8-2_OURS-FSC On GPUs 0
Run in 79703s
