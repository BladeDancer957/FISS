nohup: ignoring input
25
kvoc_4-4_RCIL On GPUs 2\Writing in results/seed_2023-ov/2023-03-17_voc_4-4_RCIL.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: None
Epoch 1, lr = 0.010000
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch 1, Batch 10/28, Loss=0.9080259770154953
Loss made of: CE 0.5398248434066772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.4092328459024429
Loss made of: CE 0.34500497579574585, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5586775541305542, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.5586775541305542, Class Loss=0.5586775541305542, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/28, Loss=0.2291960060596466
Loss made of: CE 0.22771607339382172, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.20334092527627945
Loss made of: CE 0.1577518731355667, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.22050423920154572, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.22050423920154572, Class Loss=0.22050423920154572, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/28, Loss=0.1772696405649185
Loss made of: CE 0.20945405960083008, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.1791258007287979
Loss made of: CE 0.22090886533260345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.17554694414138794, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.17554694414138794, Class Loss=0.17554694414138794, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/28, Loss=0.15729102566838266
Loss made of: CE 0.1466713547706604, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.144112216681242
Loss made of: CE 0.11106246709823608, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.153766930103302, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.153766930103302, Class Loss=0.153766930103302, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/28, Loss=0.145108812302351
Loss made of: CE 0.11830984055995941, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.1399560049176216
Loss made of: CE 0.10761384665966034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.13638633489608765, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.13638633489608765, Class Loss=0.13638633489608765, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/28, Loss=0.12074879482388497
Loss made of: CE 0.11125974357128143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.1289937138557434
Loss made of: CE 0.12476079910993576, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.12397490441799164, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.12397490441799164, Class Loss=0.12397490441799164, Reg Loss=0.0
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/32, Loss=0.8938353359699249
Loss made of: CE 0.7732664942741394, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/32, Loss=0.37814000248908997
Loss made of: CE 0.32099008560180664, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/32, Loss=0.27053330689668653
Loss made of: CE 0.21097610890865326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.49463003873825073, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.49463003873825073, Class Loss=0.49463003873825073, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/32, Loss=0.21055740863084793
Loss made of: CE 0.3432803750038147, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/32, Loss=0.17041558623313904
Loss made of: CE 0.15773579478263855, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/32, Loss=0.17797211408615113
Loss made of: CE 0.15334224700927734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.18517811596393585, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.18517811596393585, Class Loss=0.18517811596393585, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/32, Loss=0.14488981291651726
Loss made of: CE 0.1264631748199463, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/32, Loss=0.1578090339899063
Loss made of: CE 0.16889658570289612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/32, Loss=0.14224405884742736
Loss made of: CE 0.14984062314033508, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.14674729108810425, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.14674729108810425, Class Loss=0.14674729108810425, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/32, Loss=0.13453130125999452
Loss made of: CE 0.11338623613119125, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/32, Loss=0.1331867679953575
Loss made of: CE 0.11434226483106613, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/32, Loss=0.12809110283851624
Loss made of: CE 0.0947117805480957, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1312955915927887, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.1312955915927887, Class Loss=0.1312955915927887, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/32, Loss=0.11636323258280754
Loss made of: CE 0.1064574345946312, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/32, Loss=0.12684095948934554
Loss made of: CE 0.12254773080348969, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/32, Loss=0.10105448737740516
Loss made of: CE 0.09512125700712204, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.11894115060567856, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.11894115060567856, Class Loss=0.11894115060567856, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/32, Loss=0.1146170124411583
Loss made of: CE 0.09403534233570099, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/32, Loss=0.10824071019887924
Loss made of: CE 0.09801481664180756, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/32, Loss=0.1058416485786438
Loss made of: CE 0.12742877006530762, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10922465473413467, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.10922465473413467, Class Loss=0.10922465473413467, Reg Loss=0.0
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/29, Loss=0.8634769201278687
Loss made of: CE 0.6617618799209595, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.44846259653568266
Loss made of: CE 0.3608621060848236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5441516041755676, Reg Loss=0.0
Clinet index 9, End of Epoch 1/6, Average Loss=0.5441516041755676, Class Loss=0.5441516041755676, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/29, Loss=0.23265746831893921
Loss made of: CE 0.21450960636138916, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.19475641399621962
Loss made of: CE 0.1775566041469574, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.20861577987670898, Reg Loss=0.0
Clinet index 9, End of Epoch 2/6, Average Loss=0.20861577987670898, Class Loss=0.20861577987670898, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/29, Loss=0.1830538496375084
Loss made of: CE 0.15933403372764587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.16219767779111863
Loss made of: CE 0.16947206854820251, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.16975556313991547, Reg Loss=0.0
Clinet index 9, End of Epoch 3/6, Average Loss=0.16975556313991547, Class Loss=0.16975556313991547, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/29, Loss=0.14685581848025323
Loss made of: CE 0.12302291393280029, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.13968821242451668
Loss made of: CE 0.13344280421733856, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.14823757112026215, Reg Loss=0.0
Clinet index 9, End of Epoch 4/6, Average Loss=0.14823757112026215, Class Loss=0.14823757112026215, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/29, Loss=0.13261817693710326
Loss made of: CE 0.13720542192459106, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.13382712379097939
Loss made of: CE 0.11983304470777512, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.13459911942481995, Reg Loss=0.0
Clinet index 9, End of Epoch 5/6, Average Loss=0.13459911942481995, Class Loss=0.13459911942481995, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/29, Loss=0.1341112583875656
Loss made of: CE 0.15402868390083313, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.12564024627208709
Loss made of: CE 0.1339339017868042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.12997649610042572, Reg Loss=0.0
Clinet index 9, End of Epoch 6/6, Average Loss=0.12997649610042572, Class Loss=0.12997649610042572, Reg Loss=0.0
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/29, Loss=0.8505906403064728
Loss made of: CE 0.5538663864135742, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.42012170851230624
Loss made of: CE 0.2828211188316345, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5301659107208252, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.5301659107208252, Class Loss=0.5301659107208252, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009699
Epoch 2, Batch 10/29, Loss=0.22516971677541733
Loss made of: CE 0.21339432895183563, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.19069663137197496
Loss made of: CE 0.30734342336654663, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.20419935882091522, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.20419935882091522, Class Loss=0.20419935882091522, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009398
Epoch 3, Batch 10/29, Loss=0.1695456475019455
Loss made of: CE 0.13728223741054535, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.16264269798994063
Loss made of: CE 0.15000292658805847, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.1639590561389923, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.1639590561389923, Class Loss=0.1639590561389923, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.009095
Epoch 4, Batch 10/29, Loss=0.1574242815375328
Loss made of: CE 0.13144823908805847, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.14078491777181626
Loss made of: CE 0.10930360853672028, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.14456413686275482, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.14456413686275482, Class Loss=0.14456413686275482, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008792
Epoch 5, Batch 10/29, Loss=0.1344515025615692
Loss made of: CE 0.16322240233421326, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.14058732464909554
Loss made of: CE 0.1326630562543869, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.1354001760482788, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.1354001760482788, Class Loss=0.1354001760482788, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008487
Epoch 6, Batch 10/29, Loss=0.12190487012267112
Loss made of: CE 0.10782977938652039, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.12054496333003044
Loss made of: CE 0.11784432828426361, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.1231989711523056, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.1231989711523056, Class Loss=0.1231989711523056, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.29911696910858154, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.902427
Mean Acc: 0.381166
FreqW Acc: 0.815287
Mean IoU: 0.341439
Class IoU:
	class 0: 0.8991727
	class 1: 0.0
	class 2: 0.0016535445
	class 3: 0.80636984
	class 4: 0.0
Class Acc:
	class 0: 0.99435914
	class 1: 0.0
	class 2: 0.0016585019
	class 3: 0.90981114
	class 4: 0.0

federated global round: 1, step: 0
select part of clients to conduct local training
[6, 0, 7, 2]
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/28, Loss=0.34930925667285917
Loss made of: CE 0.23971375823020935, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.18142893314361572
Loss made of: CE 0.18723313510417938, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2368346005678177, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.2368346005678177, Class Loss=0.2368346005678177, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/28, Loss=0.14448177218437194
Loss made of: CE 0.12326055765151978, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.13296881467103958
Loss made of: CE 0.11259065568447113, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1415001004934311, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.1415001004934311, Class Loss=0.1415001004934311, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/28, Loss=0.12570019215345382
Loss made of: CE 0.16553755104541779, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.12468033656477928
Loss made of: CE 0.14327530562877655, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.12572257220745087, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.12572257220745087, Class Loss=0.12572257220745087, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/28, Loss=0.11528438478708267
Loss made of: CE 0.11136358976364136, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.10629136934876442
Loss made of: CE 0.08037184178829193, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.11455116420984268, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.11455116420984268, Class Loss=0.11455116420984268, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/28, Loss=0.10723520517349243
Loss made of: CE 0.0993180200457573, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.11117621883749962
Loss made of: CE 0.08114107698202133, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10626067221164703, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.10626067221164703, Class Loss=0.10626067221164703, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/28, Loss=0.09939625337719918
Loss made of: CE 0.09333056211471558, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.10716277062892914
Loss made of: CE 0.09468267858028412, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10121890902519226, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.10121890902519226, Class Loss=0.10121890902519226, Reg Loss=0.0
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/26, Loss=0.7104561299085617
Loss made of: CE 0.31384485960006714, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=0.31563169956207277
Loss made of: CE 0.2973702847957611, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4487839341163635, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.4487839341163635, Class Loss=0.4487839341163635, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009624
Epoch 2, Batch 10/26, Loss=0.19868485480546952
Loss made of: CE 0.14696630835533142, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=0.18449847623705865
Loss made of: CE 0.21860574185848236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.19741308689117432, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.19741308689117432, Class Loss=0.19741308689117432, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.009247
Epoch 3, Batch 10/26, Loss=0.16042719259858132
Loss made of: CE 0.12182464450597763, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=0.16280959844589232
Loss made of: CE 0.259890079498291, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.15544523298740387, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.15544523298740387, Class Loss=0.15544523298740387, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008868
Epoch 4, Batch 10/26, Loss=0.1297337755560875
Loss made of: CE 0.14411628246307373, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=0.14352521821856498
Loss made of: CE 0.11297434568405151, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.13729074597358704, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.13729074597358704, Class Loss=0.13729074597358704, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.008487
Epoch 5, Batch 10/26, Loss=0.11812328323721885
Loss made of: CE 0.10277911275625229, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=0.12323243245482444
Loss made of: CE 0.1257340908050537, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.12091304361820221, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.12091304361820221, Class Loss=0.12091304361820221, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.008104
Epoch 6, Batch 10/26, Loss=0.11341581642627716
Loss made of: CE 0.12745371460914612, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=0.11022216826677322
Loss made of: CE 0.12580232322216034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.11072970926761627, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.11072970926761627, Class Loss=0.11072970926761627, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/32, Loss=0.32117521315813063
Loss made of: CE 0.26747453212738037, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/32, Loss=0.1525626227259636
Loss made of: CE 0.1588655412197113, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/32, Loss=0.14213274642825127
Loss made of: CE 0.12853586673736572, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.19966305792331696, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.19966305792331696, Class Loss=0.19966305792331696, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/32, Loss=0.1263232260942459
Loss made of: CE 0.16996429860591888, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/32, Loss=0.11259766817092895
Loss made of: CE 0.11188840866088867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/32, Loss=0.12454000413417816
Loss made of: CE 0.11764794588088989, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.12004198879003525, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.12004198879003525, Class Loss=0.12004198879003525, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/32, Loss=0.10325763449072838
Loss made of: CE 0.09135901927947998, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/32, Loss=0.10957176014780998
Loss made of: CE 0.11732715368270874, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/32, Loss=0.10041406154632568
Loss made of: CE 0.10161329805850983, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10329846292734146, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.10329846292734146, Class Loss=0.10329846292734146, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/32, Loss=0.10210906565189362
Loss made of: CE 0.07586163282394409, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/32, Loss=0.0931052066385746
Loss made of: CE 0.08894799649715424, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/32, Loss=0.09416221603751182
Loss made of: CE 0.08974969387054443, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09648030251264572, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.09648030251264572, Class Loss=0.09648030251264572, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/32, Loss=0.08889206796884537
Loss made of: CE 0.08730645477771759, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/32, Loss=0.09716388210654259
Loss made of: CE 0.08909376710653305, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/32, Loss=0.08248930424451828
Loss made of: CE 0.07798021286725998, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0933692455291748, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.0933692455291748, Class Loss=0.0933692455291748, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/32, Loss=0.09321497231721879
Loss made of: CE 0.08031383156776428, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/32, Loss=0.08569893017411231
Loss made of: CE 0.096577487885952, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/32, Loss=0.08567492365837097
Loss made of: CE 0.09359055012464523, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08792518824338913, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.08792518824338913, Class Loss=0.08792518824338913, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.008181
Epoch 1, Batch 10/29, Loss=0.17820197194814683
Loss made of: CE 0.12246423959732056, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.15648216828703881
Loss made of: CE 0.14030171930789948, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.15887966752052307, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.15887966752052307, Class Loss=0.15887966752052307, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007873
Epoch 2, Batch 10/29, Loss=0.1266084998846054
Loss made of: CE 0.11863168329000473, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.12713424414396285
Loss made of: CE 0.1827375292778015, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.1254282146692276, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.1254282146692276, Class Loss=0.1254282146692276, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.007564
Epoch 3, Batch 10/29, Loss=0.11431247964501381
Loss made of: CE 0.09547692537307739, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.11568964198231697
Loss made of: CE 0.12585248053073883, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.11649983376264572, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.11649983376264572, Class Loss=0.11649983376264572, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.007254
Epoch 4, Batch 10/29, Loss=0.12012395858764649
Loss made of: CE 0.09891295433044434, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.11040999442338943
Loss made of: CE 0.08761058747768402, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.1114593967795372, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.1114593967795372, Class Loss=0.1114593967795372, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.006943
Epoch 5, Batch 10/29, Loss=0.10539629757404327
Loss made of: CE 0.14925062656402588, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.114100381731987
Loss made of: CE 0.12671247124671936, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.10882552713155746, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.10882552713155746, Class Loss=0.10882552713155746, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.006629
Epoch 6, Batch 10/29, Loss=0.09914662763476371
Loss made of: CE 0.08865930140018463, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.09614379405975342
Loss made of: CE 0.10046528279781342, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.10294675827026367, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.10294675827026367, Class Loss=0.10294675827026367, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.15639270842075348, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.937682
Mean Acc: 0.580247
FreqW Acc: 0.881276
Mean IoU: 0.555980
Class IoU:
	class 0: 0.9331544
	class 1: 0.5498227
	class 2: 0.01842299
	class 3: 0.747732
	class 4: 0.5307684
Class Acc:
	class 0: 0.9960583
	class 1: 0.5571566
	class 2: 0.019102875
	class 3: 0.75995326
	class 4: 0.56896496

federated global round: 2, step: 0
select part of clients to conduct local training
[8, 5, 3, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/28, Loss=0.15617796108126641
Loss made of: CE 0.1589430421590805, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.13407385870814323
Loss made of: CE 0.15815216302871704, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1403542309999466, Reg Loss=0.0
Clinet index 8, End of Epoch 1/6, Average Loss=0.1403542309999466, Class Loss=0.1403542309999466, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/28, Loss=0.0982910968363285
Loss made of: CE 0.06895829737186432, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.09965852573513985
Loss made of: CE 0.10861067473888397, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10372395068407059, Reg Loss=0.0
Clinet index 8, End of Epoch 2/6, Average Loss=0.10372395068407059, Class Loss=0.10372395068407059, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/28, Loss=0.09235642701387406
Loss made of: CE 0.0914536565542221, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.09839066192507744
Loss made of: CE 0.09228723496198654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09639546275138855, Reg Loss=0.0
Clinet index 8, End of Epoch 3/6, Average Loss=0.09639546275138855, Class Loss=0.09639546275138855, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/28, Loss=0.08480344489216804
Loss made of: CE 0.08170221745967865, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.0892008200287819
Loss made of: CE 0.10115515440702438, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08816152065992355, Reg Loss=0.0
Clinet index 8, End of Epoch 4/6, Average Loss=0.08816152065992355, Class Loss=0.08816152065992355, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/28, Loss=0.08380374535918236
Loss made of: CE 0.0796193927526474, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.07766041830182076
Loss made of: CE 0.084974005818367, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08543188869953156, Reg Loss=0.0
Clinet index 8, End of Epoch 5/6, Average Loss=0.08543188869953156, Class Loss=0.08543188869953156, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/28, Loss=0.07564067244529724
Loss made of: CE 0.08051423728466034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.07782377749681473
Loss made of: CE 0.0894940122961998, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08004359900951385, Reg Loss=0.0
Clinet index 8, End of Epoch 6/6, Average Loss=0.08004359900951385, Class Loss=0.08004359900951385, Reg Loss=0.0
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/26, Loss=0.24001310020685196
Loss made of: CE 0.15530124306678772, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=0.1912197783589363
Loss made of: CE 0.15129797160625458, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.20476123690605164, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.20476123690605164, Class Loss=0.20476123690605164, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/26, Loss=0.14123932719230653
Loss made of: CE 0.14065903425216675, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=0.13104404956102372
Loss made of: CE 0.11389754712581635, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.13713152706623077, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.13713152706623077, Class Loss=0.13713152706623077, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/26, Loss=0.11996151208877563
Loss made of: CE 0.09247712790966034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=0.1203035593032837
Loss made of: CE 0.11817403882741928, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.12037517875432968, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.12037517875432968, Class Loss=0.12037517875432968, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/26, Loss=0.10757011771202088
Loss made of: CE 0.11155790090560913, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=0.11516490429639817
Loss made of: CE 0.15460963547229767, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10638874769210815, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.10638874769210815, Class Loss=0.10638874769210815, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/26, Loss=0.0968027763068676
Loss made of: CE 0.09388536214828491, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=0.08905768543481826
Loss made of: CE 0.10692054033279419, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09281279891729355, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.09281279891729355, Class Loss=0.09281279891729355, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/26, Loss=0.08919095918536186
Loss made of: CE 0.07655060291290283, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=0.09342558905482293
Loss made of: CE 0.09482350945472717, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.09134749323129654, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.09134749323129654, Class Loss=0.09134749323129654, Reg Loss=0.0
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/28, Loss=0.14783514514565468
Loss made of: CE 0.13667979836463928, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.1338764764368534
Loss made of: CE 0.1464887261390686, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1414276361465454, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.1414276361465454, Class Loss=0.1414276361465454, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.009499
Epoch 2, Batch 10/28, Loss=0.1346906341612339
Loss made of: CE 0.13350644707679749, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.10705694481730461
Loss made of: CE 0.10388913750648499, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.11801570653915405, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.11801570653915405, Class Loss=0.11801570653915405, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.008994
Epoch 3, Batch 10/28, Loss=0.10747885555028916
Loss made of: CE 0.07455677539110184, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.1040190577507019
Loss made of: CE 0.09659119695425034, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10644499957561493, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.10644499957561493, Class Loss=0.10644499957561493, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.008487
Epoch 4, Batch 10/28, Loss=0.09549491703510285
Loss made of: CE 0.09652438014745712, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.09041504934430122
Loss made of: CE 0.08020345866680145, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09233853220939636, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.09233853220939636, Class Loss=0.09233853220939636, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.007976
Epoch 5, Batch 10/28, Loss=0.08489017188549042
Loss made of: CE 0.07527168095111847, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.07642831616103649
Loss made of: CE 0.06750808656215668, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08432784676551819, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.08432784676551819, Class Loss=0.08432784676551819, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.007461
Epoch 6, Batch 10/28, Loss=0.07864458262920379
Loss made of: CE 0.06944163143634796, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.08641422018408776
Loss made of: CE 0.08221060037612915, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08264520764350891, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.08264520764350891, Class Loss=0.08264520764350891, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/32, Loss=0.15507499650120735
Loss made of: CE 0.14372913539409637, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/32, Loss=0.10658111721277237
Loss made of: CE 0.10887221992015839, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/32, Loss=0.10610573440790176
Loss made of: CE 0.08686721324920654, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.12049298733472824, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.12049298733472824, Class Loss=0.12049298733472824, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.005998
Epoch 2, Batch 10/32, Loss=0.09547429829835892
Loss made of: CE 0.11998294293880463, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/32, Loss=0.08899756968021393
Loss made of: CE 0.0867428109049797, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/32, Loss=0.09558098390698433
Loss made of: CE 0.08597340434789658, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.09276481717824936, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.09276481717824936, Class Loss=0.09276481717824936, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.005679
Epoch 3, Batch 10/32, Loss=0.08232663720846176
Loss made of: CE 0.07398919761180878, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/32, Loss=0.09089360237121583
Loss made of: CE 0.09007378667593002, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/32, Loss=0.07933603972196579
Loss made of: CE 0.07454746961593628, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08328671008348465, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.08328671008348465, Class Loss=0.08328671008348465, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/32, Loss=0.08411169275641442
Loss made of: CE 0.07084588706493378, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/32, Loss=0.077053252607584
Loss made of: CE 0.07174162566661835, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/32, Loss=0.07868150882422924
Loss made of: CE 0.06892938911914825, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07998385280370712, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.07998385280370712, Class Loss=0.07998385280370712, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.005036
Epoch 5, Batch 10/32, Loss=0.0775404866784811
Loss made of: CE 0.07211783528327942, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/32, Loss=0.08299269527196884
Loss made of: CE 0.07855431735515594, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/32, Loss=0.06916847340762615
Loss made of: CE 0.07168227434158325, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07976606488227844, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.07976606488227844, Class Loss=0.07976606488227844, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.004711
Epoch 6, Batch 10/32, Loss=0.08000572770833969
Loss made of: CE 0.07462750375270844, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/32, Loss=0.07390314117074012
Loss made of: CE 0.06786737591028214, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/32, Loss=0.07269961759448051
Loss made of: CE 0.0865815281867981, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07573168724775314, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.07573168724775314, Class Loss=0.07573168724775314, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.13164052367210388, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.951787
Mean Acc: 0.700961
FreqW Acc: 0.910936
Mean IoU: 0.631722
Class IoU:
	class 0: 0.9493148
	class 1: 0.7574109
	class 2: 0.00016769902
	class 3: 0.86104727
	class 4: 0.59067154
Class Acc:
	class 0: 0.98550534
	class 1: 0.7745872
	class 2: 0.00016771369
	class 3: 0.88723004
	class 4: 0.85731584

federated global round: 3, step: 0
select part of clients to conduct local training
[5, 2, 0, 3]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006943
Epoch 1, Batch 10/26, Loss=0.15791308805346488
Loss made of: CE 0.109766386449337, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=0.13330996856093408
Loss made of: CE 0.12101812660694122, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.13729117810726166, Reg Loss=0.0
Clinet index 5, End of Epoch 1/6, Average Loss=0.13729117810726166, Class Loss=0.13729117810726166, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.006420
Epoch 2, Batch 10/26, Loss=0.10678325518965721
Loss made of: CE 0.10564570873975754, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=0.10270448997616768
Loss made of: CE 0.08887684345245361, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10588686913251877, Reg Loss=0.0
Clinet index 5, End of Epoch 2/6, Average Loss=0.10588686913251877, Class Loss=0.10588686913251877, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.005892
Epoch 3, Batch 10/26, Loss=0.09500741958618164
Loss made of: CE 0.08023123443126678, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=0.09823111668229104
Loss made of: CE 0.09636428207159042, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09843744337558746, Reg Loss=0.0
Clinet index 5, End of Epoch 3/6, Average Loss=0.09843744337558746, Class Loss=0.09843744337558746, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/26, Loss=0.09443533718585968
Loss made of: CE 0.09213610738515854, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=0.0930493026971817
Loss made of: CE 0.11008977144956589, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09079994261264801, Reg Loss=0.0
Clinet index 5, End of Epoch 4/6, Average Loss=0.09079994261264801, Class Loss=0.09079994261264801, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.004820
Epoch 5, Batch 10/26, Loss=0.08554969355463982
Loss made of: CE 0.0748475193977356, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=0.0820657379925251
Loss made of: CE 0.10721151530742645, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.084891177713871, Reg Loss=0.0
Clinet index 5, End of Epoch 5/6, Average Loss=0.084891177713871, Class Loss=0.084891177713871, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.004274
Epoch 6, Batch 10/26, Loss=0.0807794138789177
Loss made of: CE 0.06067459285259247, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=0.08837020732462406
Loss made of: CE 0.09143660962581635, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0838397741317749, Reg Loss=0.0
Clinet index 5, End of Epoch 6/6, Average Loss=0.0838397741317749, Class Loss=0.0838397741317749, Reg Loss=0.0
Current Client Index:  2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/29, Loss=0.2573246344923973
Loss made of: CE 0.1615302413702011, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.1409642904996872
Loss made of: CE 0.12456496804952621, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.17606620490550995, Reg Loss=0.0
Clinet index 2, End of Epoch 1/6, Average Loss=0.17606620490550995, Class Loss=0.17606620490550995, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.005839
Epoch 2, Batch 10/29, Loss=0.10415216237306595
Loss made of: CE 0.1109301969408989, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.10206942558288574
Loss made of: CE 0.14480993151664734, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10403843224048615, Reg Loss=0.0
Clinet index 2, End of Epoch 2/6, Average Loss=0.10403843224048615, Class Loss=0.10403843224048615, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.005359
Epoch 3, Batch 10/29, Loss=0.09460093379020691
Loss made of: CE 0.07547629624605179, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.09707914963364601
Loss made of: CE 0.11216515302658081, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09634600579738617, Reg Loss=0.0
Clinet index 2, End of Epoch 3/6, Average Loss=0.09634600579738617, Class Loss=0.09634600579738617, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.004874
Epoch 4, Batch 10/29, Loss=0.09714721143245697
Loss made of: CE 0.0846826583147049, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.09470579028129578
Loss made of: CE 0.07184751331806183, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09418951719999313, Reg Loss=0.0
Clinet index 2, End of Epoch 4/6, Average Loss=0.09418951719999313, Class Loss=0.09418951719999313, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.004384
Epoch 5, Batch 10/29, Loss=0.0906288594007492
Loss made of: CE 0.1330532729625702, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.09183702208101749
Loss made of: CE 0.09869305789470673, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.09027618914842606, Reg Loss=0.0
Clinet index 2, End of Epoch 5/6, Average Loss=0.09027618914842606, Class Loss=0.09027618914842606, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.003887
Epoch 6, Batch 10/29, Loss=0.082949423417449
Loss made of: CE 0.07241540402173996, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.08043749853968621
Loss made of: CE 0.08337124437093735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08809631317853928, Reg Loss=0.0
Clinet index 2, End of Epoch 6/6, Average Loss=0.08809631317853928, Class Loss=0.08809631317853928, Reg Loss=0.0
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.007719
Epoch 1, Batch 10/26, Loss=0.2027619533240795
Loss made of: CE 0.10781392455101013, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=0.14584085196256638
Loss made of: CE 0.1361638456583023, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.16022025048732758, Reg Loss=0.0
Clinet index 0, End of Epoch 1/6, Average Loss=0.16022025048732758, Class Loss=0.16022025048732758, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.007137
Epoch 2, Batch 10/26, Loss=0.11917604506015778
Loss made of: CE 0.08162690699100494, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=0.11333994492888451
Loss made of: CE 0.14431385695934296, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.12044309824705124, Reg Loss=0.0
Clinet index 0, End of Epoch 2/6, Average Loss=0.12044309824705124, Class Loss=0.12044309824705124, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.006551
Epoch 3, Batch 10/26, Loss=0.11166647896170616
Loss made of: CE 0.11258497089147568, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=0.11161940321326255
Loss made of: CE 0.1281883716583252, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.10694173723459244, Reg Loss=0.0
Clinet index 0, End of Epoch 3/6, Average Loss=0.10694173723459244, Class Loss=0.10694173723459244, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005958
Epoch 4, Batch 10/26, Loss=0.09194683879613877
Loss made of: CE 0.11437954753637314, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=0.1081117145717144
Loss made of: CE 0.09698688983917236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.10033107548952103, Reg Loss=0.0
Clinet index 0, End of Epoch 4/6, Average Loss=0.10033107548952103, Class Loss=0.10033107548952103, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.005359
Epoch 5, Batch 10/26, Loss=0.08459705710411072
Loss made of: CE 0.08294134587049484, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=0.09073178842663765
Loss made of: CE 0.11083777248859406, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08944875001907349, Reg Loss=0.0
Clinet index 0, End of Epoch 5/6, Average Loss=0.08944875001907349, Class Loss=0.08944875001907349, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.004752
Epoch 6, Batch 10/26, Loss=0.08849335238337516
Loss made of: CE 0.10163118690252304, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=0.08608749955892563
Loss made of: CE 0.09488417953252792, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08496657013893127, Reg Loss=0.0
Clinet index 0, End of Epoch 6/6, Average Loss=0.08496657013893127, Class Loss=0.08496657013893127, Reg Loss=0.0
Current Client Index:  3
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006943
Epoch 1, Batch 10/28, Loss=0.08961414322257041
Loss made of: CE 0.09823998063802719, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.09763747900724411
Loss made of: CE 0.1227196678519249, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.09618870913982391, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.09618870913982391, Class Loss=0.09618870913982391, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.006420
Epoch 2, Batch 10/28, Loss=0.09298932105302811
Loss made of: CE 0.0952877402305603, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.07860952876508236
Loss made of: CE 0.08259642869234085, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08509175479412079, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.08509175479412079, Class Loss=0.08509175479412079, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.005892
Epoch 3, Batch 10/28, Loss=0.08188938423991203
Loss made of: CE 0.059816114604473114, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.08012381121516228
Loss made of: CE 0.07270261645317078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.08201997727155685, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.08201997727155685, Class Loss=0.08201997727155685, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/28, Loss=0.08421044424176216
Loss made of: CE 0.09101906418800354, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.08023643307387829
Loss made of: CE 0.0793495625257492, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.08083465695381165, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.08083465695381165, Class Loss=0.08083465695381165, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.004820
Epoch 5, Batch 10/28, Loss=0.06901782043278218
Loss made of: CE 0.05601472780108452, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.0674121130257845
Loss made of: CE 0.060985639691352844, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.0728818029165268, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.0728818029165268, Class Loss=0.0728818029165268, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.004274
Epoch 6, Batch 10/28, Loss=0.06748125851154327
Loss made of: CE 0.053647905588150024, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.07513347119092942
Loss made of: CE 0.06745928525924683, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.07268917560577393, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.07268917560577393, Class Loss=0.07268917560577393, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.11463841050863266, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.952607
Mean Acc: 0.739187
FreqW Acc: 0.915174
Mean IoU: 0.669163
Class IoU:
	class 0: 0.9492095
	class 1: 0.80901223
	class 2: 0.11261512
	class 3: 0.836157
	class 4: 0.63882285
Class Acc:
	class 0: 0.98157924
	class 1: 0.84147507
	class 2: 0.1444624
	class 3: 0.8519093
	class 4: 0.8765095

federated global round: 4, step: 0
select part of clients to conduct local training
[3, 6, 1, 7]
Current Client Index:  3
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.003720
Epoch 1, Batch 10/28, Loss=0.10065444186329842
Loss made of: CE 0.08818547427654266, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.08952332362532615
Loss made of: CE 0.10740586370229721, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.0947592705488205, Reg Loss=0.0
Clinet index 3, End of Epoch 1/6, Average Loss=0.0947592705488205, Class Loss=0.0947592705488205, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.003157
Epoch 2, Batch 10/28, Loss=0.08973047882318497
Loss made of: CE 0.0955602377653122, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.07469050027430058
Loss made of: CE 0.07678358256816864, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.08079110831022263, Reg Loss=0.0
Clinet index 3, End of Epoch 2/6, Average Loss=0.08079110831022263, Class Loss=0.08079110831022263, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.002583
Epoch 3, Batch 10/28, Loss=0.0792989905923605
Loss made of: CE 0.05534365773200989, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.07358319200575351
Loss made of: CE 0.06079263612627983, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07904980331659317, Reg Loss=0.0
Clinet index 3, End of Epoch 3/6, Average Loss=0.07904980331659317, Class Loss=0.07904980331659317, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.001994
Epoch 4, Batch 10/28, Loss=0.07913467884063721
Loss made of: CE 0.07991859316825867, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.07712276093661785
Loss made of: CE 0.08575998991727829, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.07706168293952942, Reg Loss=0.0
Clinet index 3, End of Epoch 4/6, Average Loss=0.07706168293952942, Class Loss=0.07706168293952942, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.001384
Epoch 5, Batch 10/28, Loss=0.06931757666170597
Loss made of: CE 0.06276892870664597, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.06781600564718246
Loss made of: CE 0.07533684372901917, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07123145461082458, Reg Loss=0.0
Clinet index 3, End of Epoch 5/6, Average Loss=0.07123145461082458, Class Loss=0.07123145461082458, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.000742
Epoch 6, Batch 10/28, Loss=0.06690820306539536
Loss made of: CE 0.05596461147069931, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.07620964907109737
Loss made of: CE 0.0680321678519249, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0731513723731041, Reg Loss=0.0
Clinet index 3, End of Epoch 6/6, Average Loss=0.0731513723731041, Class Loss=0.0731513723731041, Reg Loss=0.0
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.006314
Epoch 1, Batch 10/28, Loss=0.11824140623211861
Loss made of: CE 0.06864635646343231, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/28, Loss=0.08892490863800048
Loss made of: CE 0.10511195659637451, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.09871023148298264, Reg Loss=0.0
Clinet index 6, End of Epoch 1/6, Average Loss=0.09871023148298264, Class Loss=0.09871023148298264, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.005359
Epoch 2, Batch 10/28, Loss=0.07731044739484787
Loss made of: CE 0.07026877999305725, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/28, Loss=0.07362128607928753
Loss made of: CE 0.06851524114608765, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.07750780880451202, Reg Loss=0.0
Clinet index 6, End of Epoch 2/6, Average Loss=0.07750780880451202, Class Loss=0.07750780880451202, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.004384
Epoch 3, Batch 10/28, Loss=0.0719822458922863
Loss made of: CE 0.09667406976222992, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/28, Loss=0.07724847197532654
Loss made of: CE 0.07076117396354675, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.07525447010993958, Reg Loss=0.0
Clinet index 6, End of Epoch 3/6, Average Loss=0.07525447010993958, Class Loss=0.07525447010993958, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.003384
Epoch 4, Batch 10/28, Loss=0.06873405687510967
Loss made of: CE 0.0634998232126236, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/28, Loss=0.07178100273013115
Loss made of: CE 0.0494927242398262, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.0725875273346901, Reg Loss=0.0
Clinet index 6, End of Epoch 4/6, Average Loss=0.0725875273346901, Class Loss=0.0725875273346901, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.002349
Epoch 5, Batch 10/28, Loss=0.07590232342481613
Loss made of: CE 0.07231946289539337, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/28, Loss=0.07485331036150455
Loss made of: CE 0.0554838702082634, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.07293453067541122, Reg Loss=0.0
Clinet index 6, End of Epoch 5/6, Average Loss=0.07293453067541122, Class Loss=0.07293453067541122, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.001259
Epoch 6, Batch 10/28, Loss=0.06799484938383102
Loss made of: CE 0.06092575192451477, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/28, Loss=0.07667895630002022
Loss made of: CE 0.07098430395126343, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.0720280110836029, Reg Loss=0.0
Clinet index 6, End of Epoch 6/6, Average Loss=0.0720280110836029, Class Loss=0.0720280110836029, Reg Loss=0.0
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.010000
Epoch 1, Batch 10/29, Loss=0.190401653945446
Loss made of: CE 0.15701481699943542, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/29, Loss=0.134439105540514
Loss made of: CE 0.12918248772621155, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.15177713334560394, Reg Loss=0.0
Clinet index 1, End of Epoch 1/6, Average Loss=0.15177713334560394, Class Loss=0.15177713334560394, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.008487
Epoch 2, Batch 10/29, Loss=0.10556002110242843
Loss made of: CE 0.09970395267009735, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/29, Loss=0.09437417909502983
Loss made of: CE 0.0791531503200531, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.10079606622457504, Reg Loss=0.0
Clinet index 1, End of Epoch 2/6, Average Loss=0.10079606622457504, Class Loss=0.10079606622457504, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.006943
Epoch 3, Batch 10/29, Loss=0.08993399366736413
Loss made of: CE 0.06823039054870605, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/29, Loss=0.1052738144993782
Loss made of: CE 0.10228493064641953, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.09572853893041611, Reg Loss=0.0
Clinet index 1, End of Epoch 3/6, Average Loss=0.09572853893041611, Class Loss=0.09572853893041611, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.005359
Epoch 4, Batch 10/29, Loss=0.08706524632871152
Loss made of: CE 0.07174788415431976, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/29, Loss=0.09673428721725941
Loss made of: CE 0.1155291199684143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.09127409011125565, Reg Loss=0.0
Clinet index 1, End of Epoch 4/6, Average Loss=0.09127409011125565, Class Loss=0.09127409011125565, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.003720
Epoch 5, Batch 10/29, Loss=0.08877317160367966
Loss made of: CE 0.08704777806997299, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/29, Loss=0.08901695087552071
Loss made of: CE 0.08390112966299057, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.08734271675348282, Reg Loss=0.0
Clinet index 1, End of Epoch 5/6, Average Loss=0.08734271675348282, Class Loss=0.08734271675348282, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.001994
Epoch 6, Batch 10/29, Loss=0.08537488430738449
Loss made of: CE 0.082265704870224, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/29, Loss=0.07718932218849658
Loss made of: CE 0.07677827775478363, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.08273263275623322, Reg Loss=0.0
Clinet index 1, End of Epoch 6/6, Average Loss=0.08273263275623322, Class Loss=0.08273263275623322, Reg Loss=0.0
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.004384
Epoch 1, Batch 10/32, Loss=0.1418236769735813
Loss made of: CE 0.12101916968822479, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/32, Loss=0.08472743183374405
Loss made of: CE 0.0858868882060051, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/32, Loss=0.07924261689186096
Loss made of: CE 0.0739341527223587, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.1004331186413765, Reg Loss=0.0
Clinet index 7, End of Epoch 1/6, Average Loss=0.1004331186413765, Class Loss=0.1004331186413765, Reg Loss=0.0
Pseudo labeling is: None
Epoch 2, lr = 0.003720
Epoch 2, Batch 10/32, Loss=0.07308690510690212
Loss made of: CE 0.08439531177282333, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/32, Loss=0.06651615612208843
Loss made of: CE 0.07567508518695831, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/32, Loss=0.07497044056653976
Loss made of: CE 0.070334792137146, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.07154130935668945, Reg Loss=0.0
Clinet index 7, End of Epoch 2/6, Average Loss=0.07154130935668945, Class Loss=0.07154130935668945, Reg Loss=0.0
Pseudo labeling is: None
Epoch 3, lr = 0.003043
Epoch 3, Batch 10/32, Loss=0.0661389872431755
Loss made of: CE 0.0641687735915184, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/32, Loss=0.07542724572122098
Loss made of: CE 0.07710108160972595, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/32, Loss=0.06437197551131249
Loss made of: CE 0.05907518416643143, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.0682058110833168, Reg Loss=0.0
Clinet index 7, End of Epoch 3/6, Average Loss=0.0682058110833168, Class Loss=0.0682058110833168, Reg Loss=0.0
Pseudo labeling is: None
Epoch 4, lr = 0.002349
Epoch 4, Batch 10/32, Loss=0.07137035876512528
Loss made of: CE 0.06434657424688339, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/32, Loss=0.06410638391971588
Loss made of: CE 0.061871081590652466, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/32, Loss=0.06525452919304371
Loss made of: CE 0.05625952407717705, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.06708235293626785, Reg Loss=0.0
Clinet index 7, End of Epoch 4/6, Average Loss=0.06708235293626785, Class Loss=0.06708235293626785, Reg Loss=0.0
Pseudo labeling is: None
Epoch 5, lr = 0.001631
Epoch 5, Batch 10/32, Loss=0.06550952717661858
Loss made of: CE 0.05845928192138672, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/32, Loss=0.0712903007864952
Loss made of: CE 0.06697741895914078, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/32, Loss=0.059243596345186236
Loss made of: CE 0.05651623010635376, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.06784061342477798, Reg Loss=0.0
Clinet index 7, End of Epoch 5/6, Average Loss=0.06784061342477798, Class Loss=0.06784061342477798, Reg Loss=0.0
Pseudo labeling is: None
Epoch 6, lr = 0.000874
Epoch 6, Batch 10/32, Loss=0.07116828337311745
Loss made of: CE 0.05849199742078781, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/32, Loss=0.06446617469191551
Loss made of: CE 0.06996965408325195, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/32, Loss=0.0633820965886116
Loss made of: CE 0.09119418263435364, LKD 0.0, LDE 0.0, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.06650052964687347, Reg Loss=0.0
Clinet index 7, End of Epoch 6/6, Average Loss=0.06650052964687347, Class Loss=0.06650052964687347, Reg Loss=0.0
federated aggregation...
Validation, Class Loss=0.10286262631416321, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.955481
Mean Acc: 0.773728
FreqW Acc: 0.921195
Mean IoU: 0.704874
Class IoU:
	class 0: 0.95112103
	class 1: 0.7782308
	class 2: 0.1949954
	class 3: 0.88024694
	class 4: 0.7197753
Class Acc:
	class 0: 0.9800695
	class 1: 0.7904032
	class 2: 0.29571363
	class 3: 0.95226395
	class 4: 0.85018796

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[0, 12, 6, 10]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/52, Loss=25.427976667881012
Loss made of: CE 1.3691037893295288, LKD 5.207973003387451, LDE 0.0, LReg 0.0, POD 17.742626190185547 EntMin 0.0
Epoch 1, Batch 20/52, Loss=20.87477340698242
Loss made of: CE 1.0278058052062988, LKD 4.827800750732422, LDE 0.0, LReg 0.0, POD 15.369462966918945 EntMin 0.0
Epoch 1, Batch 30/52, Loss=19.419282937049864
Loss made of: CE 0.8373963832855225, LKD 4.077199935913086, LDE 0.0, LReg 0.0, POD 14.186625480651855 EntMin 0.0
Epoch 1, Batch 40/52, Loss=18.033337032794954
Loss made of: CE 0.5882782340049744, LKD 3.3434741497039795, LDE 0.0, LReg 0.0, POD 12.429267883300781 EntMin 0.0
Epoch 1, Batch 50/52, Loss=18.550742512941362
Loss made of: CE 0.6069029569625854, LKD 2.8303611278533936, LDE 0.0, LReg 0.0, POD 12.410795211791992 EntMin 0.0
Epoch 1, Class Loss=1.0209802389144897, Reg Loss=4.538144588470459
Clinet index 0, End of Epoch 1/6, Average Loss=5.559124946594238, Class Loss=1.0209802389144897, Reg Loss=4.538144588470459
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/52, Loss=17.414238578081132
Loss made of: CE 0.7192481756210327, LKD 4.474462509155273, LDE 0.0, LReg 0.0, POD 12.693880081176758 EntMin 0.0
Epoch 2, Batch 20/52, Loss=16.99878532886505
Loss made of: CE 0.6168214678764343, LKD 3.6211745738983154, LDE 0.0, LReg 0.0, POD 12.00596809387207 EntMin 0.0
Epoch 2, Batch 30/52, Loss=17.18825003504753
Loss made of: CE 0.5593254566192627, LKD 4.274827480316162, LDE 0.0, LReg 0.0, POD 11.46295166015625 EntMin 0.0
Epoch 2, Batch 40/52, Loss=16.461073863506318
Loss made of: CE 0.5422630310058594, LKD 5.499910354614258, LDE 0.0, LReg 0.0, POD 10.775413513183594 EntMin 0.0
Epoch 2, Batch 50/52, Loss=16.010308343172074
Loss made of: CE 0.5796591639518738, LKD 4.5623931884765625, LDE 0.0, LReg 0.0, POD 13.115406036376953 EntMin 0.0
Epoch 2, Class Loss=0.5963221192359924, Reg Loss=4.001519203186035
Clinet index 0, End of Epoch 2/6, Average Loss=4.597841262817383, Class Loss=0.5963221192359924, Reg Loss=4.001519203186035
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/52, Loss=15.818527579307556
Loss made of: CE 0.4178985059261322, LKD 3.6016268730163574, LDE 0.0, LReg 0.0, POD 11.357145309448242 EntMin 0.0
Epoch 3, Batch 20/52, Loss=16.18489480316639
Loss made of: CE 0.4230537414550781, LKD 3.7967233657836914, LDE 0.0, LReg 0.0, POD 10.89640998840332 EntMin 0.0
Epoch 3, Batch 30/52, Loss=15.65374051630497
Loss made of: CE 0.5938578248023987, LKD 4.665504455566406, LDE 0.0, LReg 0.0, POD 9.696507453918457 EntMin 0.0
Epoch 3, Batch 40/52, Loss=15.293689754605293
Loss made of: CE 0.424005389213562, LKD 2.9728801250457764, LDE 0.0, LReg 0.0, POD 10.618143081665039 EntMin 0.0
Epoch 3, Batch 50/52, Loss=14.932051223516464
Loss made of: CE 0.40646791458129883, LKD 4.03280782699585, LDE 0.0, LReg 0.0, POD 10.723530769348145 EntMin 0.0
Epoch 3, Class Loss=0.49987930059432983, Reg Loss=3.8675055503845215
Clinet index 0, End of Epoch 3/6, Average Loss=4.367384910583496, Class Loss=0.49987930059432983, Reg Loss=3.8675055503845215
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/52, Loss=15.475017672777176
Loss made of: CE 0.6539888381958008, LKD 3.389918804168701, LDE 0.0, LReg 0.0, POD 10.9225492477417 EntMin 0.0
Epoch 4, Batch 20/52, Loss=14.53575920164585
Loss made of: CE 0.43913859128952026, LKD 3.7399768829345703, LDE 0.0, LReg 0.0, POD 10.079139709472656 EntMin 0.0
Epoch 4, Batch 30/52, Loss=14.971301844716072
Loss made of: CE 0.3527591824531555, LKD 3.169527053833008, LDE 0.0, LReg 0.0, POD 10.102127075195312 EntMin 0.0
Epoch 4, Batch 40/52, Loss=15.099323317408562
Loss made of: CE 0.3672507405281067, LKD 3.2987060546875, LDE 0.0, LReg 0.0, POD 9.502557754516602 EntMin 0.0
Epoch 4, Batch 50/52, Loss=13.680653980374336
Loss made of: CE 0.3776324689388275, LKD 3.825587749481201, LDE 0.0, LReg 0.0, POD 9.32575798034668 EntMin 0.0
Epoch 4, Class Loss=0.4406448304653168, Reg Loss=3.779559850692749
Clinet index 0, End of Epoch 4/6, Average Loss=4.220204830169678, Class Loss=0.4406448304653168, Reg Loss=3.779559850692749
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/52, Loss=14.31503758430481
Loss made of: CE 0.44155892729759216, LKD 3.9173266887664795, LDE 0.0, LReg 0.0, POD 9.545919418334961 EntMin 0.0
Epoch 5, Batch 20/52, Loss=14.227889621257782
Loss made of: CE 0.35985445976257324, LKD 3.1972904205322266, LDE 0.0, LReg 0.0, POD 9.724846839904785 EntMin 0.0
Epoch 5, Batch 30/52, Loss=13.721479526162147
Loss made of: CE 0.4220399856567383, LKD 3.2713146209716797, LDE 0.0, LReg 0.0, POD 10.371103286743164 EntMin 0.0
Epoch 5, Batch 40/52, Loss=13.863324084877968
Loss made of: CE 0.4013097882270813, LKD 3.5250604152679443, LDE 0.0, LReg 0.0, POD 10.766786575317383 EntMin 0.0
Epoch 5, Batch 50/52, Loss=14.173875573277474
Loss made of: CE 0.33255812525749207, LKD 3.645820140838623, LDE 0.0, LReg 0.0, POD 9.446837425231934 EntMin 0.0
Epoch 5, Class Loss=0.40715137124061584, Reg Loss=3.6881275177001953
Clinet index 0, End of Epoch 5/6, Average Loss=4.095278739929199, Class Loss=0.40715137124061584, Reg Loss=3.6881275177001953
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/52, Loss=13.210934865474702
Loss made of: CE 0.32627856731414795, LKD 3.6863925457000732, LDE 0.0, LReg 0.0, POD 8.930498123168945 EntMin 0.0
Epoch 6, Batch 20/52, Loss=14.409621143341065
Loss made of: CE 0.3560361862182617, LKD 3.712826728820801, LDE 0.0, LReg 0.0, POD 8.677742004394531 EntMin 0.0
Epoch 6, Batch 30/52, Loss=14.085651192069054
Loss made of: CE 0.41623473167419434, LKD 3.1829886436462402, LDE 0.0, LReg 0.0, POD 9.313982963562012 EntMin 0.0
Epoch 6, Batch 40/52, Loss=13.536083453893662
Loss made of: CE 0.3649912476539612, LKD 2.801959753036499, LDE 0.0, LReg 0.0, POD 9.606036186218262 EntMin 0.0
Epoch 6, Batch 50/52, Loss=13.663305807113648
Loss made of: CE 0.36918091773986816, LKD 2.4324951171875, LDE 0.0, LReg 0.0, POD 10.159623146057129 EntMin 0.0
Epoch 6, Class Loss=0.3894060254096985, Reg Loss=3.7019569873809814
Clinet index 0, End of Epoch 6/6, Average Loss=4.091362953186035, Class Loss=0.3894060254096985, Reg Loss=3.7019569873809814
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/40, Loss=28.083522689342498
Loss made of: CE 1.2445282936096191, LKD 5.160261154174805, LDE 0.0, LReg 0.0, POD 19.505529403686523 EntMin 0.0
Epoch 1, Batch 20/40, Loss=25.05362382531166
Loss made of: CE 0.8440313339233398, LKD 4.07387113571167, LDE 0.0, LReg 0.0, POD 17.496078491210938 EntMin 0.0
Epoch 1, Batch 30/40, Loss=22.33681122660637
Loss made of: CE 0.7342535853385925, LKD 4.435559272766113, LDE 0.0, LReg 0.0, POD 16.29618263244629 EntMin 0.0
Epoch 1, Batch 40/40, Loss=21.099335610866547
Loss made of: CE 0.6957192420959473, LKD 5.637127876281738, LDE 0.0, LReg 0.0, POD 16.798538208007812 EntMin 0.0
Epoch 1, Class Loss=1.0283000469207764, Reg Loss=5.185520172119141
Clinet index 12, End of Epoch 1/6, Average Loss=6.213820457458496, Class Loss=1.0283000469207764, Reg Loss=5.185520172119141
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/40, Loss=20.695635044574736
Loss made of: CE 0.43157219886779785, LKD 4.677777290344238, LDE 0.0, LReg 0.0, POD 14.269980430603027 EntMin 0.0
Epoch 2, Batch 20/40, Loss=19.557073614001276
Loss made of: CE 0.49858444929122925, LKD 4.714803695678711, LDE 0.0, LReg 0.0, POD 14.305777549743652 EntMin 0.0
Epoch 2, Batch 30/40, Loss=18.898804700374605
Loss made of: CE 0.5084806680679321, LKD 3.6232776641845703, LDE 0.0, LReg 0.0, POD 13.0867280960083 EntMin 0.0
Epoch 2, Batch 40/40, Loss=18.946297588944436
Loss made of: CE 0.5227298140525818, LKD 4.329926490783691, LDE 0.0, LReg 0.0, POD 15.80223560333252 EntMin 0.0
Epoch 2, Class Loss=0.5383371114730835, Reg Loss=4.247726917266846
Clinet index 12, End of Epoch 2/6, Average Loss=4.786064147949219, Class Loss=0.5383371114730835, Reg Loss=4.247726917266846
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/40, Loss=18.37922585308552
Loss made of: CE 0.3825322985649109, LKD 4.339472770690918, LDE 0.0, LReg 0.0, POD 12.945357322692871 EntMin 0.0
Epoch 3, Batch 20/40, Loss=17.44494571685791
Loss made of: CE 0.5800899863243103, LKD 3.6223063468933105, LDE 0.0, LReg 0.0, POD 13.276042938232422 EntMin 0.0
Epoch 3, Batch 30/40, Loss=17.447432464361192
Loss made of: CE 0.3886665105819702, LKD 4.4544572830200195, LDE 0.0, LReg 0.0, POD 13.386201858520508 EntMin 0.0
Epoch 3, Batch 40/40, Loss=17.7902881115675
Loss made of: CE 0.3602301776409149, LKD 4.5148396492004395, LDE 0.0, LReg 0.0, POD 12.600590705871582 EntMin 0.0
Epoch 3, Class Loss=0.4616493880748749, Reg Loss=4.020125865936279
Clinet index 12, End of Epoch 3/6, Average Loss=4.481775283813477, Class Loss=0.4616493880748749, Reg Loss=4.020125865936279
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/40, Loss=17.309548386931418
Loss made of: CE 0.456623375415802, LKD 3.6732568740844727, LDE 0.0, LReg 0.0, POD 10.84217643737793 EntMin 0.0
Epoch 4, Batch 20/40, Loss=16.932393330335618
Loss made of: CE 0.2696009874343872, LKD 4.27498197555542, LDE 0.0, LReg 0.0, POD 11.967710494995117 EntMin 0.0
Epoch 4, Batch 30/40, Loss=16.680423200130463
Loss made of: CE 0.3743976950645447, LKD 3.218843460083008, LDE 0.0, LReg 0.0, POD 11.530868530273438 EntMin 0.0
Epoch 4, Batch 40/40, Loss=16.940515804290772
Loss made of: CE 0.41762933135032654, LKD 4.013210296630859, LDE 0.0, LReg 0.0, POD 13.106645584106445 EntMin 0.0
Epoch 4, Class Loss=0.4234558045864105, Reg Loss=4.038363933563232
Clinet index 12, End of Epoch 4/6, Average Loss=4.461819648742676, Class Loss=0.4234558045864105, Reg Loss=4.038363933563232
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/40, Loss=16.731409430503845
Loss made of: CE 0.3554593026638031, LKD 3.2862603664398193, LDE 0.0, LReg 0.0, POD 11.702314376831055 EntMin 0.0
Epoch 5, Batch 20/40, Loss=15.72862575352192
Loss made of: CE 0.5032632350921631, LKD 3.9243247509002686, LDE 0.0, LReg 0.0, POD 13.086090087890625 EntMin 0.0
Epoch 5, Batch 30/40, Loss=16.162999770045282
Loss made of: CE 0.32500720024108887, LKD 3.956555128097534, LDE 0.0, LReg 0.0, POD 11.551443099975586 EntMin 0.0
Epoch 5, Batch 40/40, Loss=16.157494646310806
Loss made of: CE 0.379211962223053, LKD 3.4393043518066406, LDE 0.0, LReg 0.0, POD 10.513031005859375 EntMin 0.0
Epoch 5, Class Loss=0.3806280791759491, Reg Loss=3.8220088481903076
Clinet index 12, End of Epoch 5/6, Average Loss=4.20263671875, Class Loss=0.3806280791759491, Reg Loss=3.8220088481903076
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/40, Loss=15.872541111707687
Loss made of: CE 0.32447314262390137, LKD 5.173409938812256, LDE 0.0, LReg 0.0, POD 11.271604537963867 EntMin 0.0
Epoch 6, Batch 20/40, Loss=15.731813523173333
Loss made of: CE 0.35187792778015137, LKD 3.041558265686035, LDE 0.0, LReg 0.0, POD 12.942489624023438 EntMin 0.0
Epoch 6, Batch 30/40, Loss=16.15702107846737
Loss made of: CE 0.34935706853866577, LKD 3.8603248596191406, LDE 0.0, LReg 0.0, POD 12.082069396972656 EntMin 0.0
Epoch 6, Batch 40/40, Loss=15.039460134506225
Loss made of: CE 0.2935682237148285, LKD 3.021634578704834, LDE 0.0, LReg 0.0, POD 11.105147361755371 EntMin 0.0
Epoch 6, Class Loss=0.3659464716911316, Reg Loss=3.842190980911255
Clinet index 12, End of Epoch 6/6, Average Loss=4.208137512207031, Class Loss=0.3659464716911316, Reg Loss=3.842190980911255
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/34, Loss=29.442118322849275
Loss made of: CE 1.4038057327270508, LKD 3.9211349487304688, LDE 0.0, LReg 0.0, POD 20.152320861816406 EntMin 0.0
Epoch 1, Batch 20/34, Loss=24.3052787899971
Loss made of: CE 0.9927167892456055, LKD 4.426942825317383, LDE 0.0, LReg 0.0, POD 16.256221771240234 EntMin 0.0
Epoch 1, Batch 30/34, Loss=24.013205510377883
Loss made of: CE 0.918289303779602, LKD 4.625495910644531, LDE 0.0, LReg 0.0, POD 16.97117042541504 EntMin 0.0
Epoch 1, Class Loss=1.2276448011398315, Reg Loss=5.173358917236328
Clinet index 6, End of Epoch 1/6, Average Loss=6.401003837585449, Class Loss=1.2276448011398315, Reg Loss=5.173358917236328
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/34, Loss=20.973672193288802
Loss made of: CE 0.6508584022521973, LKD 4.096433639526367, LDE 0.0, LReg 0.0, POD 16.41968536376953 EntMin 0.0
Epoch 2, Batch 20/34, Loss=20.285024613142014
Loss made of: CE 0.6860976219177246, LKD 4.647460460662842, LDE 0.0, LReg 0.0, POD 14.398860931396484 EntMin 0.0
Epoch 2, Batch 30/34, Loss=19.925106447935104
Loss made of: CE 0.6048437356948853, LKD 5.026556015014648, LDE 0.0, LReg 0.0, POD 14.564125061035156 EntMin 0.0
Epoch 2, Class Loss=0.6617484092712402, Reg Loss=4.232089996337891
Clinet index 6, End of Epoch 2/6, Average Loss=4.893838405609131, Class Loss=0.6617484092712402, Reg Loss=4.232089996337891
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/34, Loss=18.156108182668685
Loss made of: CE 0.5526838302612305, LKD 4.500980377197266, LDE 0.0, LReg 0.0, POD 13.711483001708984 EntMin 0.0
Epoch 3, Batch 20/34, Loss=18.1686650454998
Loss made of: CE 0.49187684059143066, LKD 3.199277400970459, LDE 0.0, LReg 0.0, POD 12.838556289672852 EntMin 0.0
Epoch 3, Batch 30/34, Loss=18.746777501702308
Loss made of: CE 0.4921812415122986, LKD 4.943303108215332, LDE 0.0, LReg 0.0, POD 13.619668960571289 EntMin 0.0
Epoch 3, Class Loss=0.5201197266578674, Reg Loss=4.090333938598633
Clinet index 6, End of Epoch 3/6, Average Loss=4.6104536056518555, Class Loss=0.5201197266578674, Reg Loss=4.090333938598633
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/34, Loss=17.659665817022322
Loss made of: CE 0.39526963233947754, LKD 3.7999134063720703, LDE 0.0, LReg 0.0, POD 12.576437950134277 EntMin 0.0
Epoch 4, Batch 20/34, Loss=17.074694040417672
Loss made of: CE 0.4239545464515686, LKD 3.16601824760437, LDE 0.0, LReg 0.0, POD 12.285650253295898 EntMin 0.0
Epoch 4, Batch 30/34, Loss=17.439264786243438
Loss made of: CE 0.5436427593231201, LKD 4.0350751876831055, LDE 0.0, LReg 0.0, POD 14.581924438476562 EntMin 0.0
Epoch 4, Class Loss=0.4553593695163727, Reg Loss=3.9312210083007812
Clinet index 6, End of Epoch 4/6, Average Loss=4.386580467224121, Class Loss=0.4553593695163727, Reg Loss=3.9312210083007812
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/34, Loss=16.955111214518546
Loss made of: CE 0.4166450500488281, LKD 4.093076705932617, LDE 0.0, LReg 0.0, POD 11.790626525878906 EntMin 0.0
Epoch 5, Batch 20/34, Loss=17.030088180303572
Loss made of: CE 0.5062072277069092, LKD 4.840433120727539, LDE 0.0, LReg 0.0, POD 15.954008102416992 EntMin 0.0
Epoch 5, Batch 30/34, Loss=15.869069901108741
Loss made of: CE 0.4131205976009369, LKD 4.2117767333984375, LDE 0.0, LReg 0.0, POD 11.537834167480469 EntMin 0.0
Epoch 5, Class Loss=0.4307144582271576, Reg Loss=3.965775966644287
Clinet index 6, End of Epoch 5/6, Average Loss=4.396490573883057, Class Loss=0.4307144582271576, Reg Loss=3.965775966644287
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/34, Loss=16.43735177218914
Loss made of: CE 0.42216742038726807, LKD 3.03009295463562, LDE 0.0, LReg 0.0, POD 12.13473129272461 EntMin 0.0
Epoch 6, Batch 20/34, Loss=16.068512332439422
Loss made of: CE 0.44123610854148865, LKD 4.069484233856201, LDE 0.0, LReg 0.0, POD 14.44605541229248 EntMin 0.0
Epoch 6, Batch 30/34, Loss=16.263276180624963
Loss made of: CE 0.3893803060054779, LKD 3.6510539054870605, LDE 0.0, LReg 0.0, POD 10.477832794189453 EntMin 0.0
Epoch 6, Class Loss=0.4048791825771332, Reg Loss=3.9157261848449707
Clinet index 6, End of Epoch 6/6, Average Loss=4.320605278015137, Class Loss=0.4048791825771332, Reg Loss=3.9157261848449707
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/33, Loss=25.229300141334534
Loss made of: CE 1.2897932529449463, LKD 2.372464418411255, LDE 0.0, LReg 0.0, POD 16.938684463500977 EntMin 0.0
Epoch 1, Batch 20/33, Loss=20.440955424308775
Loss made of: CE 1.0084060430526733, LKD 3.4077844619750977, LDE 0.0, LReg 0.0, POD 16.473922729492188 EntMin 0.0
Epoch 1, Batch 30/33, Loss=18.197648948431016
Loss made of: CE 0.752982497215271, LKD 2.7810535430908203, LDE 0.0, LReg 0.0, POD 13.489837646484375 EntMin 0.0
Epoch 1, Class Loss=1.1329967975616455, Reg Loss=4.001706123352051
Clinet index 10, End of Epoch 1/6, Average Loss=5.134702682495117, Class Loss=1.1329967975616455, Reg Loss=4.001706123352051
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/33, Loss=16.714522117376326
Loss made of: CE 0.7341440916061401, LKD 4.392496585845947, LDE 0.0, LReg 0.0, POD 11.948493957519531 EntMin 0.0
Epoch 2, Batch 20/33, Loss=15.806252145767212
Loss made of: CE 0.85551518201828, LKD 3.891773223876953, LDE 0.0, LReg 0.0, POD 12.52403450012207 EntMin 0.0
Epoch 2, Batch 30/33, Loss=15.297833234071732
Loss made of: CE 0.5497559905052185, LKD 2.3900465965270996, LDE 0.0, LReg 0.0, POD 11.086204528808594 EntMin 0.0
Epoch 2, Class Loss=0.6912782192230225, Reg Loss=3.2505075931549072
Clinet index 10, End of Epoch 2/6, Average Loss=3.9417858123779297, Class Loss=0.6912782192230225, Reg Loss=3.2505075931549072
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/33, Loss=14.497656688094139
Loss made of: CE 0.5540463328361511, LKD 2.408616781234741, LDE 0.0, LReg 0.0, POD 10.329032897949219 EntMin 0.0
Epoch 3, Batch 20/33, Loss=14.258751040697097
Loss made of: CE 0.47110098600387573, LKD 2.9719159603118896, LDE 0.0, LReg 0.0, POD 9.955960273742676 EntMin 0.0
Epoch 3, Batch 30/33, Loss=14.123719480633735
Loss made of: CE 0.44922399520874023, LKD 1.9971555471420288, LDE 0.0, LReg 0.0, POD 9.913336753845215 EntMin 0.0
Epoch 3, Class Loss=0.5367714762687683, Reg Loss=3.109666109085083
Clinet index 10, End of Epoch 3/6, Average Loss=3.646437644958496, Class Loss=0.5367714762687683, Reg Loss=3.109666109085083
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/33, Loss=13.179127815365792
Loss made of: CE 0.3573873043060303, LKD 2.3172202110290527, LDE 0.0, LReg 0.0, POD 9.281481742858887 EntMin 0.0
Epoch 4, Batch 20/33, Loss=13.460530698299408
Loss made of: CE 0.40049439668655396, LKD 3.02494478225708, LDE 0.0, LReg 0.0, POD 9.519866943359375 EntMin 0.0
Epoch 4, Batch 30/33, Loss=13.480519130825996
Loss made of: CE 0.35609322786331177, LKD 3.0807032585144043, LDE 0.0, LReg 0.0, POD 9.08258056640625 EntMin 0.0
Epoch 4, Class Loss=0.441890686750412, Reg Loss=3.00762939453125
Clinet index 10, End of Epoch 4/6, Average Loss=3.4495201110839844, Class Loss=0.441890686750412, Reg Loss=3.00762939453125
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/33, Loss=13.245287191867828
Loss made of: CE 0.506527304649353, LKD 4.429058074951172, LDE 0.0, LReg 0.0, POD 9.454985618591309 EntMin 0.0
Epoch 5, Batch 20/33, Loss=12.846301901340485
Loss made of: CE 0.3404158055782318, LKD 3.8041625022888184, LDE 0.0, LReg 0.0, POD 8.742229461669922 EntMin 0.0
Epoch 5, Batch 30/33, Loss=12.688386291265488
Loss made of: CE 0.41719958186149597, LKD 2.7785110473632812, LDE 0.0, LReg 0.0, POD 10.793985366821289 EntMin 0.0
Epoch 5, Class Loss=0.3862544894218445, Reg Loss=3.013183116912842
Clinet index 10, End of Epoch 5/6, Average Loss=3.399437665939331, Class Loss=0.3862544894218445, Reg Loss=3.013183116912842
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/33, Loss=12.8937245875597
Loss made of: CE 0.4966481626033783, LKD 3.957620143890381, LDE 0.0, LReg 0.0, POD 9.190238952636719 EntMin 0.0
Epoch 6, Batch 20/33, Loss=12.284926360845565
Loss made of: CE 0.2984803318977356, LKD 2.5649232864379883, LDE 0.0, LReg 0.0, POD 9.266253471374512 EntMin 0.0
Epoch 6, Batch 30/33, Loss=12.380870494246484
Loss made of: CE 0.3465847671031952, LKD 2.9779529571533203, LDE 0.0, LReg 0.0, POD 9.468975067138672 EntMin 0.0
Epoch 6, Class Loss=0.35785606503486633, Reg Loss=3.008594274520874
Clinet index 10, End of Epoch 6/6, Average Loss=3.366450309753418, Class Loss=0.35785606503486633, Reg Loss=3.008594274520874
federated aggregation...
Validation, Class Loss=0.42527061700820923, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.831105
Mean Acc: 0.481892
FreqW Acc: 0.703899
Mean IoU: 0.344415
Class IoU:
	class 0: 0.84260076
	class 1: 0.7535445
	class 2: 0.2585357
	class 3: 0.47974432
	class 4: 0.5415533
	class 5: 0.0
	class 6: 0.012194583
	class 7: 0.0
	class 8: 0.21156146
Class Acc:
	class 0: 0.98257905
	class 1: 0.84294313
	class 2: 0.51094544
	class 3: 0.93223023
	class 4: 0.8433813
	class 5: 0.0
	class 6: 0.0121945925
	class 7: 0.0
	class 8: 0.21275766

federated global round: 6, step: 1
select part of clients to conduct local training
[11, 5, 8, 12]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/43, Loss=15.183944007754326
Loss made of: CE 0.4560891091823578, LKD 2.755384922027588, LDE 0.0, LReg 0.0, POD 10.82868766784668 EntMin 0.0
Epoch 1, Batch 20/43, Loss=13.659879410266877
Loss made of: CE 0.7094294428825378, LKD 3.2814090251922607, LDE 0.0, LReg 0.0, POD 9.699897766113281 EntMin 0.0
Epoch 1, Batch 30/43, Loss=13.272566765546799
Loss made of: CE 0.48446330428123474, LKD 2.8270106315612793, LDE 0.0, LReg 0.0, POD 10.843376159667969 EntMin 0.0
Epoch 1, Batch 40/43, Loss=12.938501644134522
Loss made of: CE 0.5269023180007935, LKD 3.1686758995056152, LDE 0.0, LReg 0.0, POD 8.981494903564453 EntMin 0.0
Epoch 1, Class Loss=0.5553476214408875, Reg Loss=3.0748465061187744
Clinet index 11, End of Epoch 1/6, Average Loss=3.6301941871643066, Class Loss=0.5553476214408875, Reg Loss=3.0748465061187744
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/43, Loss=12.220687627792358
Loss made of: CE 0.3746781349182129, LKD 1.9641278982162476, LDE 0.0, LReg 0.0, POD 9.010013580322266 EntMin 0.0
Epoch 2, Batch 20/43, Loss=12.73680904507637
Loss made of: CE 0.32323190569877625, LKD 2.001807928085327, LDE 0.0, LReg 0.0, POD 8.995512008666992 EntMin 0.0
Epoch 2, Batch 30/43, Loss=12.900855997204781
Loss made of: CE 0.5720541477203369, LKD 3.259949207305908, LDE 0.0, LReg 0.0, POD 10.367435455322266 EntMin 0.0
Epoch 2, Batch 40/43, Loss=12.819698828458787
Loss made of: CE 0.47096967697143555, LKD 2.330932140350342, LDE 0.0, LReg 0.0, POD 9.201361656188965 EntMin 0.0
Epoch 2, Class Loss=0.41924193501472473, Reg Loss=3.0557034015655518
Clinet index 11, End of Epoch 2/6, Average Loss=3.474945306777954, Class Loss=0.41924193501472473, Reg Loss=3.0557034015655518
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/43, Loss=12.178379848599434
Loss made of: CE 0.44608545303344727, LKD 2.3385119438171387, LDE 0.0, LReg 0.0, POD 8.459576606750488 EntMin 0.0
Epoch 3, Batch 20/43, Loss=12.24531497657299
Loss made of: CE 0.3043598234653473, LKD 2.0759572982788086, LDE 0.0, LReg 0.0, POD 9.0310697555542 EntMin 0.0
Epoch 3, Batch 30/43, Loss=12.060539688169957
Loss made of: CE 0.46205076575279236, LKD 2.21146297454834, LDE 0.0, LReg 0.0, POD 8.65118408203125 EntMin 0.0
Epoch 3, Batch 40/43, Loss=12.42224053144455
Loss made of: CE 0.21096906065940857, LKD 2.221562385559082, LDE 0.0, LReg 0.0, POD 8.603609085083008 EntMin 0.0
Epoch 3, Class Loss=0.3765941560268402, Reg Loss=2.9272775650024414
Clinet index 11, End of Epoch 3/6, Average Loss=3.3038716316223145, Class Loss=0.3765941560268402, Reg Loss=2.9272775650024414
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/43, Loss=12.086454954743385
Loss made of: CE 0.426553338766098, LKD 2.693016529083252, LDE 0.0, LReg 0.0, POD 8.987492561340332 EntMin 0.0
Epoch 4, Batch 20/43, Loss=11.562353831529617
Loss made of: CE 0.35543936491012573, LKD 3.1486146450042725, LDE 0.0, LReg 0.0, POD 8.375343322753906 EntMin 0.0
Epoch 4, Batch 30/43, Loss=11.875188674032689
Loss made of: CE 0.40554279088974, LKD 2.9138247966766357, LDE 0.0, LReg 0.0, POD 8.187996864318848 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 40/43, Loss=12.07796019911766
Loss made of: CE 0.24006518721580505, LKD 2.331552028656006, LDE 0.0, LReg 0.0, POD 8.240791320800781 EntMin 0.0
Epoch 4, Class Loss=0.33984455466270447, Reg Loss=2.919292449951172
Clinet index 11, End of Epoch 4/6, Average Loss=3.259136915206909, Class Loss=0.33984455466270447, Reg Loss=2.919292449951172
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/43, Loss=12.013491773605347
Loss made of: CE 0.2889688014984131, LKD 3.6070973873138428, LDE 0.0, LReg 0.0, POD 9.406623840332031 EntMin 0.0
Epoch 5, Batch 20/43, Loss=11.851244950294495
Loss made of: CE 0.34884393215179443, LKD 2.341128349304199, LDE 0.0, LReg 0.0, POD 8.914054870605469 EntMin 0.0
Epoch 5, Batch 30/43, Loss=11.818574529886245
Loss made of: CE 0.2790169417858124, LKD 1.9374639987945557, LDE 0.0, LReg 0.0, POD 8.43372631072998 EntMin 0.0
Epoch 5, Batch 40/43, Loss=11.89333185106516
Loss made of: CE 0.3011825680732727, LKD 2.7196035385131836, LDE 0.0, LReg 0.0, POD 8.107100486755371 EntMin 0.0
Epoch 5, Class Loss=0.3206634819507599, Reg Loss=2.9661316871643066
Clinet index 11, End of Epoch 5/6, Average Loss=3.286795139312744, Class Loss=0.3206634819507599, Reg Loss=2.9661316871643066
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/43, Loss=11.31365384608507
Loss made of: CE 0.2779107093811035, LKD 2.534511089324951, LDE 0.0, LReg 0.0, POD 7.789696216583252 EntMin 0.0
Epoch 6, Batch 20/43, Loss=11.614167760312558
Loss made of: CE 0.3239455819129944, LKD 2.7762210369110107, LDE 0.0, LReg 0.0, POD 8.021581649780273 EntMin 0.0
Epoch 6, Batch 30/43, Loss=11.690898439288139
Loss made of: CE 0.3058852255344391, LKD 3.5997018814086914, LDE 0.0, LReg 0.0, POD 7.747585296630859 EntMin 0.0
Epoch 6, Batch 40/43, Loss=11.416463586688042
Loss made of: CE 0.3229503035545349, LKD 3.318056106567383, LDE 0.0, LReg 0.0, POD 8.044837951660156 EntMin 0.0
Epoch 6, Class Loss=0.30075863003730774, Reg Loss=2.938382863998413
Clinet index 11, End of Epoch 6/6, Average Loss=3.2391414642333984, Class Loss=0.30075863003730774, Reg Loss=2.938382863998413
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/34, Loss=17.291737389564513
Loss made of: CE 0.7895616292953491, LKD 3.3964004516601562, LDE 0.0, LReg 0.0, POD 12.577392578125 EntMin 0.0
Epoch 1, Batch 20/34, Loss=16.81247839331627
Loss made of: CE 0.44822975993156433, LKD 4.088319778442383, LDE 0.0, LReg 0.0, POD 12.127484321594238 EntMin 0.0
Epoch 1, Batch 30/34, Loss=16.93627796173096
Loss made of: CE 0.45538362860679626, LKD 5.160757064819336, LDE 0.0, LReg 0.0, POD 15.12321662902832 EntMin 0.0
Epoch 1, Class Loss=0.5581521391868591, Reg Loss=4.033893585205078
Clinet index 5, End of Epoch 1/6, Average Loss=4.592045783996582, Class Loss=0.5581521391868591, Reg Loss=4.033893585205078
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/34, Loss=16.76557510793209
Loss made of: CE 0.4182755947113037, LKD 3.8727893829345703, LDE 0.0, LReg 0.0, POD 11.28352165222168 EntMin 0.0
Epoch 2, Batch 20/34, Loss=15.261256736516952
Loss made of: CE 0.40630123019218445, LKD 3.9561946392059326, LDE 0.0, LReg 0.0, POD 11.695079803466797 EntMin 0.0
Epoch 2, Batch 30/34, Loss=16.619023287296294
Loss made of: CE 0.4730914533138275, LKD 4.401285648345947, LDE 0.0, LReg 0.0, POD 13.958815574645996 EntMin 0.0
Epoch 2, Class Loss=0.41150546073913574, Reg Loss=4.015663146972656
Clinet index 5, End of Epoch 2/6, Average Loss=4.427168846130371, Class Loss=0.41150546073913574, Reg Loss=4.015663146972656
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/34, Loss=15.163184997439384
Loss made of: CE 0.3393023610115051, LKD 4.4346795082092285, LDE 0.0, LReg 0.0, POD 11.175769805908203 EntMin 0.0
Epoch 3, Batch 20/34, Loss=15.31455500125885
Loss made of: CE 0.3732193410396576, LKD 3.7534165382385254, LDE 0.0, LReg 0.0, POD 12.758912086486816 EntMin 0.0
Epoch 3, Batch 30/34, Loss=16.03883602321148
Loss made of: CE 0.42897582054138184, LKD 4.072028160095215, LDE 0.0, LReg 0.0, POD 12.746415138244629 EntMin 0.0
Epoch 3, Class Loss=0.38352710008621216, Reg Loss=3.9927351474761963
Clinet index 5, End of Epoch 3/6, Average Loss=4.376262187957764, Class Loss=0.38352710008621216, Reg Loss=3.9927351474761963
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/34, Loss=15.316233670711517
Loss made of: CE 0.3375318944454193, LKD 3.841677665710449, LDE 0.0, LReg 0.0, POD 10.388505935668945 EntMin 0.0
Epoch 4, Batch 20/34, Loss=15.69585091471672
Loss made of: CE 0.4481137990951538, LKD 4.168505668640137, LDE 0.0, LReg 0.0, POD 11.878395080566406 EntMin 0.0
Epoch 4, Batch 30/34, Loss=15.202392715215684
Loss made of: CE 0.3851012587547302, LKD 4.247003555297852, LDE 0.0, LReg 0.0, POD 10.051433563232422 EntMin 0.0
Epoch 4, Class Loss=0.3701256513595581, Reg Loss=3.959665536880493
Clinet index 5, End of Epoch 4/6, Average Loss=4.329791069030762, Class Loss=0.3701256513595581, Reg Loss=3.959665536880493
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/34, Loss=14.922533693909646
Loss made of: CE 0.3514963984489441, LKD 5.130752086639404, LDE 0.0, LReg 0.0, POD 10.77088737487793 EntMin 0.0
Epoch 5, Batch 20/34, Loss=15.095550921559333
Loss made of: CE 0.36977633833885193, LKD 4.110618591308594, LDE 0.0, LReg 0.0, POD 10.784074783325195 EntMin 0.0
Epoch 5, Batch 30/34, Loss=15.634265559911729
Loss made of: CE 0.3777289390563965, LKD 4.555952072143555, LDE 0.0, LReg 0.0, POD 11.404532432556152 EntMin 0.0
Epoch 5, Class Loss=0.3567705750465393, Reg Loss=4.023160457611084
Clinet index 5, End of Epoch 5/6, Average Loss=4.3799309730529785, Class Loss=0.3567705750465393, Reg Loss=4.023160457611084
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/34, Loss=15.402203664183617
Loss made of: CE 0.3823000192642212, LKD 4.165587425231934, LDE 0.0, LReg 0.0, POD 10.812712669372559 EntMin 0.0
Epoch 6, Batch 20/34, Loss=14.598499616980552
Loss made of: CE 0.2895807921886444, LKD 4.092521667480469, LDE 0.0, LReg 0.0, POD 10.833337783813477 EntMin 0.0
Epoch 6, Batch 30/34, Loss=14.811575731635093
Loss made of: CE 0.46815305948257446, LKD 4.684868812561035, LDE 0.0, LReg 0.0, POD 12.40226936340332 EntMin 0.0
Epoch 6, Class Loss=0.3479982912540436, Reg Loss=3.9130802154541016
Clinet index 5, End of Epoch 6/6, Average Loss=4.261078357696533, Class Loss=0.3479982912540436, Reg Loss=3.9130802154541016
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/40, Loss=16.856526133418082
Loss made of: CE 0.630022406578064, LKD 4.063418865203857, LDE 0.0, LReg 0.0, POD 13.66983413696289 EntMin 0.0
Epoch 1, Batch 20/40, Loss=16.928531232476235
Loss made of: CE 0.3827688694000244, LKD 3.854691982269287, LDE 0.0, LReg 0.0, POD 12.499650955200195 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 30/40, Loss=16.767540901899338
Loss made of: CE 0.46894675493240356, LKD 3.760291576385498, LDE 0.0, LReg 0.0, POD 11.12668514251709 EntMin 0.0
Epoch 1, Batch 40/40, Loss=16.466868844628333
Loss made of: CE 0.381621778011322, LKD 3.7537808418273926, LDE 0.0, LReg 0.0, POD 11.085084915161133 EntMin 0.0
Epoch 1, Class Loss=0.48862606287002563, Reg Loss=3.878718614578247
Clinet index 8, End of Epoch 1/6, Average Loss=4.367344856262207, Class Loss=0.48862606287002563, Reg Loss=3.878718614578247
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/40, Loss=15.874879622459412
Loss made of: CE 0.45678067207336426, LKD 3.9742021560668945, LDE 0.0, LReg 0.0, POD 11.320049285888672 EntMin 0.0
Epoch 2, Batch 20/40, Loss=15.254292544722556
Loss made of: CE 0.2693486213684082, LKD 4.700713157653809, LDE 0.0, LReg 0.0, POD 10.398126602172852 EntMin 0.0
Epoch 2, Batch 30/40, Loss=16.56095187664032
Loss made of: CE 0.4227030277252197, LKD 4.282783031463623, LDE 0.0, LReg 0.0, POD 11.564105987548828 EntMin 0.0
Epoch 2, Batch 40/40, Loss=15.785601684451104
Loss made of: CE 0.37632402777671814, LKD 3.176849126815796, LDE 0.0, LReg 0.0, POD 11.329191207885742 EntMin 0.0
Epoch 2, Class Loss=0.4066717326641083, Reg Loss=3.843852996826172
Clinet index 8, End of Epoch 2/6, Average Loss=4.250524520874023, Class Loss=0.4066717326641083, Reg Loss=3.843852996826172
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/40, Loss=15.127631878852844
Loss made of: CE 0.27378153800964355, LKD 2.6105246543884277, LDE 0.0, LReg 0.0, POD 10.469573974609375 EntMin 0.0
Epoch 3, Batch 20/40, Loss=15.058798459172248
Loss made of: CE 0.3929447531700134, LKD 4.281261444091797, LDE 0.0, LReg 0.0, POD 11.26861572265625 EntMin 0.0
Epoch 3, Batch 30/40, Loss=15.182792726159096
Loss made of: CE 0.42914626002311707, LKD 5.207630157470703, LDE 0.0, LReg 0.0, POD 10.46022891998291 EntMin 0.0
Epoch 3, Batch 40/40, Loss=15.28666294515133
Loss made of: CE 0.36455225944519043, LKD 3.3312129974365234, LDE 0.0, LReg 0.0, POD 11.271946907043457 EntMin 0.0
Epoch 3, Class Loss=0.37796685099601746, Reg Loss=3.7353813648223877
Clinet index 8, End of Epoch 3/6, Average Loss=4.113348007202148, Class Loss=0.37796685099601746, Reg Loss=3.7353813648223877
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/40, Loss=15.128977748751641
Loss made of: CE 0.5036424398422241, LKD 3.973745346069336, LDE 0.0, LReg 0.0, POD 11.937091827392578 EntMin 0.0
Epoch 4, Batch 20/40, Loss=15.059884351491927
Loss made of: CE 0.3197835087776184, LKD 4.092729091644287, LDE 0.0, LReg 0.0, POD 11.084970474243164 EntMin 0.0
Epoch 4, Batch 30/40, Loss=14.806925162672997
Loss made of: CE 0.2859548330307007, LKD 2.7072341442108154, LDE 0.0, LReg 0.0, POD 10.707412719726562 EntMin 0.0
Epoch 4, Batch 40/40, Loss=15.185191377997398
Loss made of: CE 0.35379546880722046, LKD 3.8032615184783936, LDE 0.0, LReg 0.0, POD 10.203889846801758 EntMin 0.0
Epoch 4, Class Loss=0.3511999547481537, Reg Loss=3.7915120124816895
Clinet index 8, End of Epoch 4/6, Average Loss=4.142712116241455, Class Loss=0.3511999547481537, Reg Loss=3.7915120124816895
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/40, Loss=14.38028243482113
Loss made of: CE 0.3239218592643738, LKD 3.18174147605896, LDE 0.0, LReg 0.0, POD 10.507716178894043 EntMin 0.0
Epoch 5, Batch 20/40, Loss=15.263997066020966
Loss made of: CE 0.3587496876716614, LKD 2.9709901809692383, LDE 0.0, LReg 0.0, POD 10.691349029541016 EntMin 0.0
Epoch 5, Batch 30/40, Loss=14.772596168518067
Loss made of: CE 0.4172484874725342, LKD 3.3789196014404297, LDE 0.0, LReg 0.0, POD 10.481311798095703 EntMin 0.0
Epoch 5, Batch 40/40, Loss=14.579570084810257
Loss made of: CE 0.3009665608406067, LKD 4.192449569702148, LDE 0.0, LReg 0.0, POD 11.260046005249023 EntMin 0.0
Epoch 5, Class Loss=0.3360956311225891, Reg Loss=3.7514188289642334
Clinet index 8, End of Epoch 5/6, Average Loss=4.087514400482178, Class Loss=0.3360956311225891, Reg Loss=3.7514188289642334
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/40, Loss=14.517273569107056
Loss made of: CE 0.27496233582496643, LKD 3.464731216430664, LDE 0.0, LReg 0.0, POD 10.787543296813965 EntMin 0.0
Epoch 6, Batch 20/40, Loss=14.469161811470986
Loss made of: CE 0.32528066635131836, LKD 3.473912239074707, LDE 0.0, LReg 0.0, POD 10.326822280883789 EntMin 0.0
Epoch 6, Batch 30/40, Loss=14.670200359821319
Loss made of: CE 0.3537660837173462, LKD 4.619750499725342, LDE 0.0, LReg 0.0, POD 10.642971992492676 EntMin 0.0
Epoch 6, Batch 40/40, Loss=14.373137840628624
Loss made of: CE 0.2660518288612366, LKD 3.8377845287323, LDE 0.0, LReg 0.0, POD 10.18081283569336 EntMin 0.0
Epoch 6, Class Loss=0.31963133811950684, Reg Loss=3.6825473308563232
Clinet index 8, End of Epoch 6/6, Average Loss=4.00217866897583, Class Loss=0.31963133811950684, Reg Loss=3.6825473308563232
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/40, Loss=17.1749384701252
Loss made of: CE 0.5363253355026245, LKD 4.0645599365234375, LDE 0.0, LReg 0.0, POD 12.924211502075195 EntMin 0.0
Epoch 1, Batch 20/40, Loss=17.066448852419853
Loss made of: CE 0.3840770423412323, LKD 3.3644118309020996, LDE 0.0, LReg 0.0, POD 11.52161693572998 EntMin 0.0
Epoch 1, Batch 30/40, Loss=16.290176478028297
Loss made of: CE 0.5718252658843994, LKD 3.8363044261932373, LDE 0.0, LReg 0.0, POD 12.170829772949219 EntMin 0.0
Epoch 1, Batch 40/40, Loss=16.304946205019952
Loss made of: CE 0.47367966175079346, LKD 5.220444679260254, LDE 0.0, LReg 0.0, POD 12.351043701171875 EntMin 0.0
Epoch 1, Class Loss=0.5134429335594177, Reg Loss=3.953470230102539
Clinet index 12, End of Epoch 1/6, Average Loss=4.466913223266602, Class Loss=0.5134429335594177, Reg Loss=3.953470230102539
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/40, Loss=16.50159809291363
Loss made of: CE 0.3102167844772339, LKD 4.616542339324951, LDE 0.0, LReg 0.0, POD 10.451255798339844 EntMin 0.0
Epoch 2, Batch 20/40, Loss=15.683033591508865
Loss made of: CE 0.40056535601615906, LKD 4.04591178894043, LDE 0.0, LReg 0.0, POD 10.777105331420898 EntMin 0.0
Epoch 2, Batch 30/40, Loss=15.37224243581295
Loss made of: CE 0.43125879764556885, LKD 3.507004499435425, LDE 0.0, LReg 0.0, POD 10.623427391052246 EntMin 0.0
Epoch 2, Batch 40/40, Loss=16.137183326482774
Loss made of: CE 0.44901785254478455, LKD 4.527132511138916, LDE 0.0, LReg 0.0, POD 13.661334037780762 EntMin 0.0
Epoch 2, Class Loss=0.42561981081962585, Reg Loss=3.9065778255462646
Clinet index 12, End of Epoch 2/6, Average Loss=4.332197666168213, Class Loss=0.42561981081962585, Reg Loss=3.9065778255462646
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/40, Loss=15.76749962568283
Loss made of: CE 0.3227296769618988, LKD 4.012330532073975, LDE 0.0, LReg 0.0, POD 10.641483306884766 EntMin 0.0
Epoch 3, Batch 20/40, Loss=15.082167130708694
Loss made of: CE 0.5739511847496033, LKD 3.6534793376922607, LDE 0.0, LReg 0.0, POD 11.785638809204102 EntMin 0.0
Epoch 3, Batch 30/40, Loss=15.041455967724323
Loss made of: CE 0.34919941425323486, LKD 3.9625535011291504, LDE 0.0, LReg 0.0, POD 11.346275329589844 EntMin 0.0
Epoch 3, Batch 40/40, Loss=15.769100251793862
Loss made of: CE 0.34467074275016785, LKD 4.265329360961914, LDE 0.0, LReg 0.0, POD 10.545610427856445 EntMin 0.0
Epoch 3, Class Loss=0.40234002470970154, Reg Loss=3.859199285507202
Clinet index 12, End of Epoch 3/6, Average Loss=4.261539459228516, Class Loss=0.40234002470970154, Reg Loss=3.859199285507202
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/40, Loss=15.07545526921749
Loss made of: CE 0.37452322244644165, LKD 3.29938006401062, LDE 0.0, LReg 0.0, POD 9.421895980834961 EntMin 0.0
Epoch 4, Batch 20/40, Loss=14.970119674503803
Loss made of: CE 0.24881498515605927, LKD 4.009801387786865, LDE 0.0, LReg 0.0, POD 9.782459259033203 EntMin 0.0
Epoch 4, Batch 30/40, Loss=15.587010934948921
Loss made of: CE 0.35273176431655884, LKD 3.158407688140869, LDE 0.0, LReg 0.0, POD 10.466829299926758 EntMin 0.0
Epoch 4, Batch 40/40, Loss=15.462234899401665
Loss made of: CE 0.4077204167842865, LKD 4.094310283660889, LDE 0.0, LReg 0.0, POD 12.609957695007324 EntMin 0.0
Epoch 4, Class Loss=0.387463241815567, Reg Loss=3.9365546703338623
Clinet index 12, End of Epoch 4/6, Average Loss=4.3240180015563965, Class Loss=0.387463241815567, Reg Loss=3.9365546703338623
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/40, Loss=15.316205400228501
Loss made of: CE 0.320135235786438, LKD 4.1957597732543945, LDE 0.0, LReg 0.0, POD 10.848674774169922 EntMin 0.0
Epoch 5, Batch 20/40, Loss=14.490268179774285
Loss made of: CE 0.4594826102256775, LKD 3.624272108078003, LDE 0.0, LReg 0.0, POD 11.997064590454102 EntMin 0.0
Epoch 5, Batch 30/40, Loss=14.752985695004464
Loss made of: CE 0.2849227488040924, LKD 4.209239482879639, LDE 0.0, LReg 0.0, POD 10.15972900390625 EntMin 0.0
Epoch 5, Batch 40/40, Loss=14.962629920244217
Loss made of: CE 0.3485226035118103, LKD 3.3890910148620605, LDE 0.0, LReg 0.0, POD 9.988306045532227 EntMin 0.0
Epoch 5, Class Loss=0.3476676642894745, Reg Loss=3.795776128768921
Clinet index 12, End of Epoch 5/6, Average Loss=4.143443584442139, Class Loss=0.3476676642894745, Reg Loss=3.795776128768921
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/40, Loss=14.9832229077816
Loss made of: CE 0.2983158528804779, LKD 4.047582626342773, LDE 0.0, LReg 0.0, POD 10.010068893432617 EntMin 0.0
Epoch 6, Batch 20/40, Loss=14.748648262023925
Loss made of: CE 0.34721696376800537, LKD 3.6181139945983887, LDE 0.0, LReg 0.0, POD 11.323631286621094 EntMin 0.0
Epoch 6, Batch 30/40, Loss=15.046235412359238
Loss made of: CE 0.35044002532958984, LKD 3.4390177726745605, LDE 0.0, LReg 0.0, POD 10.92487907409668 EntMin 0.0
Epoch 6, Batch 40/40, Loss=14.275919339060783
Loss made of: CE 0.2855207622051239, LKD 3.062246084213257, LDE 0.0, LReg 0.0, POD 9.731024742126465 EntMin 0.0
Epoch 6, Class Loss=0.34557071328163147, Reg Loss=3.9013288021087646
Clinet index 12, End of Epoch 6/6, Average Loss=4.246899604797363, Class Loss=0.34557071328163147, Reg Loss=3.9013288021087646
federated aggregation...
Validation, Class Loss=0.3416667580604553, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.865181
Mean Acc: 0.560575
FreqW Acc: 0.761702
Mean IoU: 0.431200
Class IoU:
	class 0: 0.8699781
	class 1: 0.76381755
	class 2: 0.2431514
	class 3: 0.5809423
	class 4: 0.52147186
	class 5: 0.014851719
	class 6: 0.2891724
	class 7: 0.0048553008
	class 8: 0.5925561
Class Acc:
	class 0: 0.97834957
	class 1: 0.84309995
	class 2: 0.47365355
	class 3: 0.9418742
	class 4: 0.85460347
	class 5: 0.014857226
	class 6: 0.2926955
	class 7: 0.004855325
	class 8: 0.6411887

federated global round: 7, step: 1
select part of clients to conduct local training
[0, 6, 13, 5]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/52, Loss=13.961592358350753
Loss made of: CE 0.7832746505737305, LKD 4.767620086669922, LDE 0.0, LReg 0.0, POD 9.33062744140625 EntMin 0.0
Epoch 1, Batch 20/52, Loss=13.669637614488602
Loss made of: CE 0.4917452931404114, LKD 4.078100681304932, LDE 0.0, LReg 0.0, POD 8.807982444763184 EntMin 0.0
Epoch 1, Batch 30/52, Loss=13.21725782752037
Loss made of: CE 0.44944557547569275, LKD 3.5175461769104004, LDE 0.0, LReg 0.0, POD 8.930007934570312 EntMin 0.0
Epoch 1, Batch 40/52, Loss=13.251175740361214
Loss made of: CE 0.32815688848495483, LKD 3.2541863918304443, LDE 0.0, LReg 0.0, POD 9.549154281616211 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 50/52, Loss=13.614961916208268
Loss made of: CE 0.31843119859695435, LKD 2.4151792526245117, LDE 0.0, LReg 0.0, POD 8.713754653930664 EntMin 0.0
Epoch 1, Class Loss=0.4714242219924927, Reg Loss=3.741705894470215
Clinet index 0, End of Epoch 1/6, Average Loss=4.213129997253418, Class Loss=0.4714242219924927, Reg Loss=3.741705894470215
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/52, Loss=13.584131425619125
Loss made of: CE 0.49573588371276855, LKD 4.018366813659668, LDE 0.0, LReg 0.0, POD 9.026135444641113 EntMin 0.0
Epoch 2, Batch 20/52, Loss=13.199878096580505
Loss made of: CE 0.3785078525543213, LKD 3.369783401489258, LDE 0.0, LReg 0.0, POD 8.619173049926758 EntMin 0.0
Epoch 2, Batch 30/52, Loss=13.72552519440651
Loss made of: CE 0.4147617220878601, LKD 3.9448468685150146, LDE 0.0, LReg 0.0, POD 9.754329681396484 EntMin 0.0
Epoch 2, Batch 40/52, Loss=13.38734765946865
Loss made of: CE 0.3687742352485657, LKD 4.1617560386657715, LDE 0.0, LReg 0.0, POD 8.948152542114258 EntMin 0.0
Epoch 2, Batch 50/52, Loss=13.251697152853012
Loss made of: CE 0.3993406891822815, LKD 4.493594169616699, LDE 0.0, LReg 0.0, POD 10.486053466796875 EntMin 0.0
Epoch 2, Class Loss=0.3837524652481079, Reg Loss=3.7075295448303223
Clinet index 0, End of Epoch 2/6, Average Loss=4.091281890869141, Class Loss=0.3837524652481079, Reg Loss=3.7075295448303223
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/52, Loss=13.337637180089951
Loss made of: CE 0.30434584617614746, LKD 3.5030055046081543, LDE 0.0, LReg 0.0, POD 9.08734130859375 EntMin 0.0
Epoch 3, Batch 20/52, Loss=13.97508346438408
Loss made of: CE 0.3264460265636444, LKD 3.84169864654541, LDE 0.0, LReg 0.0, POD 9.867204666137695 EntMin 0.0
Epoch 3, Batch 30/52, Loss=13.663665020465851
Loss made of: CE 0.48189061880111694, LKD 4.416661262512207, LDE 0.0, LReg 0.0, POD 8.486513137817383 EntMin 0.0
Epoch 3, Batch 40/52, Loss=13.214582285284996
Loss made of: CE 0.31243896484375, LKD 2.747279644012451, LDE 0.0, LReg 0.0, POD 9.445199966430664 EntMin 0.0
Epoch 3, Batch 50/52, Loss=13.022648358345032
Loss made of: CE 0.3310326337814331, LKD 3.528289794921875, LDE 0.0, LReg 0.0, POD 8.588494300842285 EntMin 0.0
Epoch 3, Class Loss=0.3651783764362335, Reg Loss=3.7083635330200195
Clinet index 0, End of Epoch 3/6, Average Loss=4.07354211807251, Class Loss=0.3651783764362335, Reg Loss=3.7083635330200195
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/52, Loss=13.634335044026376
Loss made of: CE 0.4452039897441864, LKD 3.957562208175659, LDE 0.0, LReg 0.0, POD 9.711788177490234 EntMin 0.0
Epoch 4, Batch 20/52, Loss=12.887380000948905
Loss made of: CE 0.30849727988243103, LKD 3.999391555786133, LDE 0.0, LReg 0.0, POD 8.34748363494873 EntMin 0.0
Epoch 4, Batch 30/52, Loss=12.891337466239928
Loss made of: CE 0.2835654616355896, LKD 2.740396499633789, LDE 0.0, LReg 0.0, POD 8.533432960510254 EntMin 0.0
Epoch 4, Batch 40/52, Loss=13.309770834445953
Loss made of: CE 0.3174477517604828, LKD 2.6543211936950684, LDE 0.0, LReg 0.0, POD 8.342462539672852 EntMin 0.0
Epoch 4, Batch 50/52, Loss=12.427822139859199
Loss made of: CE 0.3054659366607666, LKD 4.103752613067627, LDE 0.0, LReg 0.0, POD 8.828234672546387 EntMin 0.0
Epoch 4, Class Loss=0.346984326839447, Reg Loss=3.6115000247955322
Clinet index 0, End of Epoch 4/6, Average Loss=3.958484411239624, Class Loss=0.346984326839447, Reg Loss=3.6115000247955322
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/52, Loss=12.960143351554871
Loss made of: CE 0.33415937423706055, LKD 3.695589780807495, LDE 0.0, LReg 0.0, POD 8.415879249572754 EntMin 0.0
Epoch 5, Batch 20/52, Loss=12.748528915643693
Loss made of: CE 0.3031325340270996, LKD 3.4189038276672363, LDE 0.0, LReg 0.0, POD 8.722265243530273 EntMin 0.0
Epoch 5, Batch 30/52, Loss=12.642451107501984
Loss made of: CE 0.328277587890625, LKD 3.124422788619995, LDE 0.0, LReg 0.0, POD 9.351057052612305 EntMin 0.0
Epoch 5, Batch 40/52, Loss=12.605594256520272
Loss made of: CE 0.3286028504371643, LKD 3.3987412452697754, LDE 0.0, LReg 0.0, POD 9.443211555480957 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 50/52, Loss=12.812716785073281
Loss made of: CE 0.34596675634384155, LKD 3.4027180671691895, LDE 0.0, LReg 0.0, POD 8.181814193725586 EntMin 0.0
Epoch 5, Class Loss=0.3385940492153168, Reg Loss=3.6293416023254395
Clinet index 0, End of Epoch 5/6, Average Loss=3.967935562133789, Class Loss=0.3385940492153168, Reg Loss=3.6293416023254395
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/52, Loss=12.048241075873374
Loss made of: CE 0.2736632823944092, LKD 3.2166552543640137, LDE 0.0, LReg 0.0, POD 8.09957504272461 EntMin 0.0
Epoch 6, Batch 20/52, Loss=13.311084401607513
Loss made of: CE 0.31504911184310913, LKD 3.4196510314941406, LDE 0.0, LReg 0.0, POD 8.108978271484375 EntMin 0.0
Epoch 6, Batch 30/52, Loss=13.096876019239426
Loss made of: CE 0.3561994433403015, LKD 3.028726100921631, LDE 0.0, LReg 0.0, POD 8.627443313598633 EntMin 0.0
Epoch 6, Batch 40/52, Loss=12.43652065396309
Loss made of: CE 0.30963701009750366, LKD 2.7039356231689453, LDE 0.0, LReg 0.0, POD 8.093635559082031 EntMin 0.0
Epoch 6, Batch 50/52, Loss=12.587670803070068
Loss made of: CE 0.26564285159111023, LKD 2.31965708732605, LDE 0.0, LReg 0.0, POD 8.863088607788086 EntMin 0.0
Epoch 6, Class Loss=0.3308469355106354, Reg Loss=3.681427240371704
Clinet index 0, End of Epoch 6/6, Average Loss=4.012274265289307, Class Loss=0.3308469355106354, Reg Loss=3.681427240371704
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/34, Loss=15.595630538463592
Loss made of: CE 0.6193417310714722, LKD 3.4507274627685547, LDE 0.0, LReg 0.0, POD 10.495697975158691 EntMin 0.0
Epoch 1, Batch 20/34, Loss=14.28849067389965
Loss made of: CE 0.3448229432106018, LKD 3.894486427307129, LDE 0.0, LReg 0.0, POD 9.187180519104004 EntMin 0.0
Epoch 1, Batch 30/34, Loss=15.877892798185348
Loss made of: CE 0.42881298065185547, LKD 4.034728050231934, LDE 0.0, LReg 0.0, POD 10.349868774414062 EntMin 0.0
Epoch 1, Class Loss=0.4824642837047577, Reg Loss=3.813318967819214
Clinet index 6, End of Epoch 1/6, Average Loss=4.295783042907715, Class Loss=0.4824642837047577, Reg Loss=3.813318967819214
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/34, Loss=14.525601252913475
Loss made of: CE 0.318010538816452, LKD 3.494741439819336, LDE 0.0, LReg 0.0, POD 11.204643249511719 EntMin 0.0
Epoch 2, Batch 20/34, Loss=14.585235124826431
Loss made of: CE 0.4085104465484619, LKD 4.345089912414551, LDE 0.0, LReg 0.0, POD 9.783819198608398 EntMin 0.0
Epoch 2, Batch 30/34, Loss=14.801320660114289
Loss made of: CE 0.3903449475765228, LKD 4.509006500244141, LDE 0.0, LReg 0.0, POD 10.887821197509766 EntMin 0.0
Epoch 2, Class Loss=0.3695071041584015, Reg Loss=3.8532938957214355
Clinet index 6, End of Epoch 2/6, Average Loss=4.222801208496094, Class Loss=0.3695071041584015, Reg Loss=3.8532938957214355
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/34, Loss=13.941582679748535
Loss made of: CE 0.3469413220882416, LKD 4.536502838134766, LDE 0.0, LReg 0.0, POD 10.510555267333984 EntMin 0.0
Epoch 3, Batch 20/34, Loss=14.453295895457268
Loss made of: CE 0.3912203311920166, LKD 3.0741653442382812, LDE 0.0, LReg 0.0, POD 9.576421737670898 EntMin 0.0
Epoch 3, Batch 30/34, Loss=14.7184161901474
Loss made of: CE 0.3394657373428345, LKD 4.282552242279053, LDE 0.0, LReg 0.0, POD 10.38638687133789 EntMin 0.0
Epoch 3, Class Loss=0.3586256504058838, Reg Loss=3.8281092643737793
Clinet index 6, End of Epoch 3/6, Average Loss=4.186735153198242, Class Loss=0.3586256504058838, Reg Loss=3.8281092643737793
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/34, Loss=14.422339308261872
Loss made of: CE 0.3111628293991089, LKD 3.140653133392334, LDE 0.0, LReg 0.0, POD 10.037233352661133 EntMin 0.0
Epoch 4, Batch 20/34, Loss=13.942695274949074
Loss made of: CE 0.34825024008750916, LKD 3.008984327316284, LDE 0.0, LReg 0.0, POD 9.855993270874023 EntMin 0.0
Epoch 4, Batch 30/34, Loss=14.504579904675484
Loss made of: CE 0.45709070563316345, LKD 3.9440207481384277, LDE 0.0, LReg 0.0, POD 11.003437042236328 EntMin 0.0
Epoch 4, Class Loss=0.34803661704063416, Reg Loss=3.74660325050354
Clinet index 6, End of Epoch 4/6, Average Loss=4.094639778137207, Class Loss=0.34803661704063416, Reg Loss=3.74660325050354
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/34, Loss=14.227576717734337
Loss made of: CE 0.3342946767807007, LKD 3.7548282146453857, LDE 0.0, LReg 0.0, POD 9.264074325561523 EntMin 0.0
Epoch 5, Batch 20/34, Loss=14.482960918545723
Loss made of: CE 0.41433224081993103, LKD 5.24576997756958, LDE 0.0, LReg 0.0, POD 12.836527824401855 EntMin 0.0
Epoch 5, Batch 30/34, Loss=13.52798318862915
Loss made of: CE 0.3447283208370209, LKD 4.440825462341309, LDE 0.0, LReg 0.0, POD 9.106024742126465 EntMin 0.0
Epoch 5, Class Loss=0.34477150440216064, Reg Loss=3.873152017593384
Clinet index 6, End of Epoch 5/6, Average Loss=4.217923641204834, Class Loss=0.34477150440216064, Reg Loss=3.873152017593384
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/34, Loss=14.23966570198536
Loss made of: CE 0.36771315336227417, LKD 2.8784425258636475, LDE 0.0, LReg 0.0, POD 9.897071838378906 EntMin 0.0
Epoch 6, Batch 20/34, Loss=13.708019977807998
Loss made of: CE 0.3924754858016968, LKD 4.565717697143555, LDE 0.0, LReg 0.0, POD 12.60066032409668 EntMin 0.0
Epoch 6, Batch 30/34, Loss=13.971928852796555
Loss made of: CE 0.3144797086715698, LKD 3.909632682800293, LDE 0.0, LReg 0.0, POD 8.745027542114258 EntMin 0.0
Epoch 6, Class Loss=0.33541762828826904, Reg Loss=3.7860183715820312
Clinet index 6, End of Epoch 6/6, Average Loss=4.12143611907959, Class Loss=0.33541762828826904, Reg Loss=3.7860183715820312
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/33, Loss=18.150255304574966
Loss made of: CE 0.712158203125, LKD 3.4084267616271973, LDE 0.0, LReg 0.0, POD 12.107643127441406 EntMin 0.0
Epoch 1, Batch 20/33, Loss=15.837570160627365
Loss made of: CE 0.7573652267456055, LKD 2.6831748485565186, LDE 0.0, LReg 0.0, POD 12.124629974365234 EntMin 0.0
Epoch 1, Batch 30/33, Loss=14.349855995178222
Loss made of: CE 0.4386015832424164, LKD 3.128312587738037, LDE 0.0, LReg 0.0, POD 9.928951263427734 EntMin 0.0
Epoch 1, Class Loss=0.6940339207649231, Reg Loss=3.3064639568328857
Clinet index 13, End of Epoch 1/6, Average Loss=4.000497817993164, Class Loss=0.6940339207649231, Reg Loss=3.3064639568328857
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/33, Loss=13.45116634964943
Loss made of: CE 0.3924446105957031, LKD 3.2364776134490967, LDE 0.0, LReg 0.0, POD 9.685186386108398 EntMin 0.0
Epoch 2, Batch 20/33, Loss=13.304085341095924
Loss made of: CE 0.30400609970092773, LKD 2.907343626022339, LDE 0.0, LReg 0.0, POD 9.258620262145996 EntMin 0.0
Epoch 2, Batch 30/33, Loss=12.386794079840183
Loss made of: CE 0.5156338214874268, LKD 3.414958953857422, LDE 0.0, LReg 0.0, POD 10.29037094116211 EntMin 0.0
Epoch 2, Class Loss=0.3860839009284973, Reg Loss=3.0305376052856445
Clinet index 13, End of Epoch 2/6, Average Loss=3.416621446609497, Class Loss=0.3860839009284973, Reg Loss=3.0305376052856445
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/33, Loss=12.529487672448159
Loss made of: CE 0.3728625178337097, LKD 3.1805121898651123, LDE 0.0, LReg 0.0, POD 10.496957778930664 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 20/33, Loss=12.60080124437809
Loss made of: CE 0.2878522276878357, LKD 3.426020622253418, LDE 0.0, LReg 0.0, POD 9.380699157714844 EntMin 0.0
Epoch 3, Batch 30/33, Loss=12.071602642536163
Loss made of: CE 0.2865627408027649, LKD 2.4807071685791016, LDE 0.0, LReg 0.0, POD 8.886680603027344 EntMin 0.0
Epoch 3, Class Loss=0.33921653032302856, Reg Loss=3.007575035095215
Clinet index 13, End of Epoch 3/6, Average Loss=3.3467915058135986, Class Loss=0.33921653032302856, Reg Loss=3.007575035095215
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/33, Loss=11.998501721024514
Loss made of: CE 0.29794713854789734, LKD 1.7248132228851318, LDE 0.0, LReg 0.0, POD 8.103724479675293 EntMin 0.0
Epoch 4, Batch 20/33, Loss=11.799699050188064
Loss made of: CE 0.3844085931777954, LKD 3.254333972930908, LDE 0.0, LReg 0.0, POD 9.258186340332031 EntMin 0.0
Epoch 4, Batch 30/33, Loss=12.026995596289634
Loss made of: CE 0.29491981863975525, LKD 2.9378485679626465, LDE 0.0, LReg 0.0, POD 8.279343605041504 EntMin 0.0
Epoch 4, Class Loss=0.3083530366420746, Reg Loss=2.9352493286132812
Clinet index 13, End of Epoch 4/6, Average Loss=3.2436022758483887, Class Loss=0.3083530366420746, Reg Loss=2.9352493286132812
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/33, Loss=11.918851184844971
Loss made of: CE 0.2958390712738037, LKD 3.8267266750335693, LDE 0.0, LReg 0.0, POD 8.310571670532227 EntMin 0.0
Epoch 5, Batch 20/33, Loss=11.786478555202484
Loss made of: CE 0.27386757731437683, LKD 2.487391948699951, LDE 0.0, LReg 0.0, POD 7.8920793533325195 EntMin 0.0
Epoch 5, Batch 30/33, Loss=11.511665713787078
Loss made of: CE 0.27436038851737976, LKD 2.334301710128784, LDE 0.0, LReg 0.0, POD 8.206273078918457 EntMin 0.0
Epoch 5, Class Loss=0.29543599486351013, Reg Loss=2.9685964584350586
Clinet index 13, End of Epoch 5/6, Average Loss=3.2640323638916016, Class Loss=0.29543599486351013, Reg Loss=2.9685964584350586
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/33, Loss=11.772167386114598
Loss made of: CE 0.3567311763763428, LKD 3.825343132019043, LDE 0.0, LReg 0.0, POD 8.511565208435059 EntMin 0.0
Epoch 6, Batch 20/33, Loss=11.573731780052185
Loss made of: CE 0.24978803098201752, LKD 3.051154613494873, LDE 0.0, LReg 0.0, POD 7.866055011749268 EntMin 0.0
Epoch 6, Batch 30/33, Loss=11.508349134027958
Loss made of: CE 0.27670052647590637, LKD 2.097525119781494, LDE 0.0, LReg 0.0, POD 8.60500431060791 EntMin 0.0
Epoch 6, Class Loss=0.2889724671840668, Reg Loss=2.9957528114318848
Clinet index 13, End of Epoch 6/6, Average Loss=3.2847251892089844, Class Loss=0.2889724671840668, Reg Loss=2.9957528114318848
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/34, Loss=15.794423857331276
Loss made of: CE 0.680342435836792, LKD 2.99399471282959, LDE 0.0, LReg 0.0, POD 11.100152969360352 EntMin 0.0
Epoch 1, Batch 20/34, Loss=15.204608660936355
Loss made of: CE 0.42826640605926514, LKD 3.717191696166992, LDE 0.0, LReg 0.0, POD 10.81182861328125 EntMin 0.0
Epoch 1, Batch 30/34, Loss=14.889923283457756
Loss made of: CE 0.4058119058609009, LKD 4.548744201660156, LDE 0.0, LReg 0.0, POD 13.465288162231445 EntMin 0.0
Epoch 1, Class Loss=0.4803590178489685, Reg Loss=3.952157974243164
Clinet index 5, End of Epoch 1/6, Average Loss=4.432517051696777, Class Loss=0.4803590178489685, Reg Loss=3.952157974243164
Pseudo labeling is: None
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/34, Loss=15.1821148365736
Loss made of: CE 0.34040194749832153, LKD 3.7957024574279785, LDE 0.0, LReg 0.0, POD 9.718056678771973 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/34, Loss=14.017341908812522
Loss made of: CE 0.3656975030899048, LKD 4.0979108810424805, LDE 0.0, LReg 0.0, POD 10.463179588317871 EntMin 0.0
Epoch 2, Batch 30/34, Loss=15.176047432422639
Loss made of: CE 0.4434816241264343, LKD 4.006740570068359, LDE 0.0, LReg 0.0, POD 11.940594673156738 EntMin 0.0
Epoch 2, Class Loss=0.382079154253006, Reg Loss=3.8568618297576904
Clinet index 5, End of Epoch 2/6, Average Loss=4.238941192626953, Class Loss=0.382079154253006, Reg Loss=3.8568618297576904
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/34, Loss=14.133168345689773
Loss made of: CE 0.3439455032348633, LKD 4.255593776702881, LDE 0.0, LReg 0.0, POD 10.163431167602539 EntMin 0.0
Epoch 3, Batch 20/34, Loss=14.164969724416732
Loss made of: CE 0.3347724676132202, LKD 3.8256025314331055, LDE 0.0, LReg 0.0, POD 11.508871078491211 EntMin 0.0
Epoch 3, Batch 30/34, Loss=14.82078657746315
Loss made of: CE 0.36679863929748535, LKD 3.914919376373291, LDE 0.0, LReg 0.0, POD 11.977432250976562 EntMin 0.0
Epoch 3, Class Loss=0.3554544448852539, Reg Loss=3.8778202533721924
Clinet index 5, End of Epoch 3/6, Average Loss=4.233274459838867, Class Loss=0.3554544448852539, Reg Loss=3.8778202533721924
Pseudo labeling is: None
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/34, Loss=14.242993456125259
Loss made of: CE 0.3322104811668396, LKD 3.664885997772217, LDE 0.0, LReg 0.0, POD 9.75615119934082 EntMin 0.0
Epoch 4, Batch 20/34, Loss=14.828381451964379
Loss made of: CE 0.4185030460357666, LKD 3.8837363719940186, LDE 0.0, LReg 0.0, POD 11.280895233154297 EntMin 0.0
Epoch 4, Batch 30/34, Loss=14.051545336842537
Loss made of: CE 0.36397790908813477, LKD 4.035236358642578, LDE 0.0, LReg 0.0, POD 8.765087127685547 EntMin 0.0
Epoch 4, Class Loss=0.3565543293952942, Reg Loss=3.8389830589294434
Clinet index 5, End of Epoch 4/6, Average Loss=4.195537567138672, Class Loss=0.3565543293952942, Reg Loss=3.8389830589294434
Pseudo labeling is: None
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/34, Loss=14.1420867562294
Loss made of: CE 0.3693816661834717, LKD 4.524562835693359, LDE 0.0, LReg 0.0, POD 9.42745590209961 EntMin 0.0
Epoch 5, Batch 20/34, Loss=14.057394489645958
Loss made of: CE 0.37005382776260376, LKD 3.5172383785247803, LDE 0.0, LReg 0.0, POD 9.604969024658203 EntMin 0.0
Epoch 5, Batch 30/34, Loss=14.574794110655784
Loss made of: CE 0.39709457755088806, LKD 4.177804470062256, LDE 0.0, LReg 0.0, POD 10.36599349975586 EntMin 0.0
Epoch 5, Class Loss=0.34740084409713745, Reg Loss=3.9098241329193115
Clinet index 5, End of Epoch 5/6, Average Loss=4.257225036621094, Class Loss=0.34740084409713745, Reg Loss=3.9098241329193115
Pseudo labeling is: None
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/34, Loss=14.97755922973156
Loss made of: CE 0.3779604434967041, LKD 4.248076438903809, LDE 0.0, LReg 0.0, POD 9.538199424743652 EntMin 0.0
Epoch 6, Batch 20/34, Loss=13.909037813544273
Loss made of: CE 0.28665417432785034, LKD 4.294130802154541, LDE 0.0, LReg 0.0, POD 9.947307586669922 EntMin 0.0
Epoch 6, Batch 30/34, Loss=14.229880806803703
Loss made of: CE 0.46480807662010193, LKD 4.926196098327637, LDE 0.0, LReg 0.0, POD 11.840084075927734 EntMin 0.0
Epoch 6, Class Loss=0.34379738569259644, Reg Loss=3.9394760131835938
Clinet index 5, End of Epoch 6/6, Average Loss=4.283273220062256, Class Loss=0.34379738569259644, Reg Loss=3.9394760131835938
federated aggregation...
Validation, Class Loss=0.2881263792514801, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.906783
Mean Acc: 0.676217
FreqW Acc: 0.835143
Mean IoU: 0.553983
Class IoU:
	class 0: 0.91202956
	class 1: 0.79478556
	class 2: 0.23775199
	class 3: 0.6166401
	class 4: 0.61264914
	class 5: 0.0
	class 6: 0.77905256
	class 7: 0.3931929
	class 8: 0.63974756
Class Acc:
	class 0: 0.9746288
	class 1: 0.8498632
	class 2: 0.4643569
	class 3: 0.9355995
	class 4: 0.8568424
	class 5: 0.0
	class 6: 0.8936849
	class 7: 0.39883643
	class 8: 0.71213865

federated global round: 8, step: 1
select part of clients to conduct local training
[4, 11, 9, 6]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=15.218112468719482
Loss made of: CE 0.5029728412628174, LKD 2.9237632751464844, LDE 0.0, LReg 0.0, POD 10.899190902709961 EntMin 0.0
Epoch 1, Batch 20/25, Loss=13.869798424839974
Loss made of: CE 0.3454587161540985, LKD 2.783547878265381, LDE 0.0, LReg 0.0, POD 10.088153839111328 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.4470199942588806, Reg Loss=2.8461904525756836
Clinet index 4, End of Epoch 1/6, Average Loss=3.293210506439209, Class Loss=0.4470199942588806, Reg Loss=2.8461904525756836
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/25, Loss=13.122912506759167
Loss made of: CE 0.3049149513244629, LKD 2.685952663421631, LDE 0.0, LReg 0.0, POD 9.12912368774414 EntMin 0.0
Epoch 2, Batch 20/25, Loss=12.753799465298652
Loss made of: CE 0.4282265305519104, LKD 3.8578574657440186, LDE 0.0, LReg 0.0, POD 10.09006118774414 EntMin 0.0
Epoch 2, Class Loss=0.35535040497779846, Reg Loss=2.7733278274536133
Clinet index 4, End of Epoch 2/6, Average Loss=3.128678321838379, Class Loss=0.35535040497779846, Reg Loss=2.7733278274536133
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/25, Loss=12.505767291784286
Loss made of: CE 0.27801474928855896, LKD 2.5385031700134277, LDE 0.0, LReg 0.0, POD 9.473024368286133 EntMin 0.0
Epoch 3, Batch 20/25, Loss=12.39033776819706
Loss made of: CE 0.33958110213279724, LKD 2.594025135040283, LDE 0.0, LReg 0.0, POD 9.328054428100586 EntMin 0.0
Epoch 3, Class Loss=0.32790660858154297, Reg Loss=2.715502977371216
Clinet index 4, End of Epoch 3/6, Average Loss=3.043409585952759, Class Loss=0.32790660858154297, Reg Loss=2.715502977371216
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/25, Loss=12.35928509682417
Loss made of: CE 0.2804923355579376, LKD 2.617856740951538, LDE 0.0, LReg 0.0, POD 9.743600845336914 EntMin 0.0
Epoch 4, Batch 20/25, Loss=11.833160573244095
Loss made of: CE 0.2619601786136627, LKD 3.294703245162964, LDE 0.0, LReg 0.0, POD 9.546070098876953 EntMin 0.0
Epoch 4, Class Loss=0.28294479846954346, Reg Loss=2.603302478790283
Clinet index 4, End of Epoch 4/6, Average Loss=2.886247158050537, Class Loss=0.28294479846954346, Reg Loss=2.603302478790283
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/25, Loss=11.88804716616869
Loss made of: CE 0.2090619057416916, LKD 2.795492172241211, LDE 0.0, LReg 0.0, POD 8.196735382080078 EntMin 0.0
Epoch 5, Batch 20/25, Loss=12.497118383646011
Loss made of: CE 0.44593846797943115, LKD 3.0748488903045654, LDE 0.0, LReg 0.0, POD 11.418519973754883 EntMin 0.0
Epoch 5, Class Loss=0.28106260299682617, Reg Loss=2.6710667610168457
Clinet index 4, End of Epoch 5/6, Average Loss=2.952129364013672, Class Loss=0.28106260299682617, Reg Loss=2.6710667610168457
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/25, Loss=11.52863376736641
Loss made of: CE 0.2468380182981491, LKD 3.666249990463257, LDE 0.0, LReg 0.0, POD 9.005260467529297 EntMin 0.0
Epoch 6, Batch 20/25, Loss=11.660195909440517
Loss made of: CE 0.3301298916339874, LKD 2.4124841690063477, LDE 0.0, LReg 0.0, POD 9.571767807006836 EntMin 0.0
Epoch 6, Class Loss=0.24981269240379333, Reg Loss=2.66713285446167
Clinet index 4, End of Epoch 6/6, Average Loss=2.916945457458496, Class Loss=0.24981269240379333, Reg Loss=2.66713285446167
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/43, Loss=14.105972629785537
Loss made of: CE 0.3203180432319641, LKD 2.585543155670166, LDE 0.0, LReg 0.0, POD 10.098811149597168 EntMin 0.0
Epoch 1, Batch 20/43, Loss=13.051093012094498
Loss made of: CE 0.5290895700454712, LKD 3.1906981468200684, LDE 0.0, LReg 0.0, POD 9.130563735961914 EntMin 0.0
Epoch 1, Batch 30/43, Loss=12.501282888650895
Loss made of: CE 0.32840394973754883, LKD 2.37943172454834, LDE 0.0, LReg 0.0, POD 10.066915512084961 EntMin 0.0
Epoch 1, Batch 40/43, Loss=12.16101590692997
Loss made of: CE 0.4636662006378174, LKD 3.05899977684021, LDE 0.0, LReg 0.0, POD 8.466784477233887 EntMin 0.0
Epoch 1, Class Loss=0.4334915578365326, Reg Loss=2.95687198638916
Clinet index 11, End of Epoch 1/6, Average Loss=3.3903634548187256, Class Loss=0.4334915578365326, Reg Loss=2.95687198638916
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/43, Loss=11.163017147779465
Loss made of: CE 0.2827971875667572, LKD 1.9795745611190796, LDE 0.0, LReg 0.0, POD 7.935967445373535 EntMin 0.0
Epoch 2, Batch 20/43, Loss=11.731257836520673
Loss made of: CE 0.29986798763275146, LKD 2.0985116958618164, LDE 0.0, LReg 0.0, POD 7.975366592407227 EntMin 0.0
Epoch 2, Batch 30/43, Loss=11.879018411040306
Loss made of: CE 0.4076096713542938, LKD 2.6121578216552734, LDE 0.0, LReg 0.0, POD 9.238649368286133 EntMin 0.0
Epoch 2, Batch 40/43, Loss=12.026408842206001
Loss made of: CE 0.3502311706542969, LKD 2.3789803981781006, LDE 0.0, LReg 0.0, POD 8.81118392944336 EntMin 0.0
Epoch 2, Class Loss=0.33916792273521423, Reg Loss=2.855526924133301
Clinet index 11, End of Epoch 2/6, Average Loss=3.194694757461548, Class Loss=0.33916792273521423, Reg Loss=2.855526924133301
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/43, Loss=11.361488401889801
Loss made of: CE 0.371462345123291, LKD 2.7311110496520996, LDE 0.0, LReg 0.0, POD 7.684704303741455 EntMin 0.0
Epoch 3, Batch 20/43, Loss=11.492676028609276
Loss made of: CE 0.2808408737182617, LKD 2.2261998653411865, LDE 0.0, LReg 0.0, POD 8.323131561279297 EntMin 0.0
Epoch 3, Batch 30/43, Loss=11.181195253133774
Loss made of: CE 0.35678502917289734, LKD 2.471151113510132, LDE 0.0, LReg 0.0, POD 7.84239387512207 EntMin 0.0
Epoch 3, Batch 40/43, Loss=11.46363161802292
Loss made of: CE 0.17464059591293335, LKD 2.137432098388672, LDE 0.0, LReg 0.0, POD 8.125015258789062 EntMin 0.0
Epoch 3, Class Loss=0.3140968978404999, Reg Loss=2.8114829063415527
Clinet index 11, End of Epoch 3/6, Average Loss=3.125579833984375, Class Loss=0.3140968978404999, Reg Loss=2.8114829063415527
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/43, Loss=11.286703231930733
Loss made of: CE 0.3902571201324463, LKD 3.1525771617889404, LDE 0.0, LReg 0.0, POD 8.502141952514648 EntMin 0.0
Epoch 4, Batch 20/43, Loss=10.84999285042286
Loss made of: CE 0.28737109899520874, LKD 3.0136032104492188, LDE 0.0, LReg 0.0, POD 7.692176818847656 EntMin 0.0
Epoch 4, Batch 30/43, Loss=11.097927580773831
Loss made of: CE 0.3415023684501648, LKD 2.895296096801758, LDE 0.0, LReg 0.0, POD 7.902132987976074 EntMin 0.0
Epoch 4, Batch 40/43, Loss=11.482582233846188
Loss made of: CE 0.21619436144828796, LKD 2.3716211318969727, LDE 0.0, LReg 0.0, POD 7.908000946044922 EntMin 0.0
Epoch 4, Class Loss=0.29513150453567505, Reg Loss=2.8467211723327637
Clinet index 11, End of Epoch 4/6, Average Loss=3.141852617263794, Class Loss=0.29513150453567505, Reg Loss=2.8467211723327637
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/43, Loss=11.405646885931493
Loss made of: CE 0.2696525454521179, LKD 3.3677337169647217, LDE 0.0, LReg 0.0, POD 9.14360237121582 EntMin 0.0
Epoch 5, Batch 20/43, Loss=11.155647775530815
Loss made of: CE 0.29732340574264526, LKD 2.1771745681762695, LDE 0.0, LReg 0.0, POD 8.449592590332031 EntMin 0.0
Epoch 5, Batch 30/43, Loss=11.261808241903783
Loss made of: CE 0.2599782943725586, LKD 1.91983163356781, LDE 0.0, LReg 0.0, POD 7.939038276672363 EntMin 0.0
Epoch 5, Batch 40/43, Loss=11.227614425122738
Loss made of: CE 0.2707008123397827, LKD 3.1115920543670654, LDE 0.0, LReg 0.0, POD 7.753609657287598 EntMin 0.0
Epoch 5, Class Loss=0.2913724482059479, Reg Loss=2.8737573623657227
Clinet index 11, End of Epoch 5/6, Average Loss=3.1651298999786377, Class Loss=0.2913724482059479, Reg Loss=2.8737573623657227
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/43, Loss=10.666125613451005
Loss made of: CE 0.25871405005455017, LKD 2.2647626399993896, LDE 0.0, LReg 0.0, POD 7.662228107452393 EntMin 0.0
Epoch 6, Batch 20/43, Loss=10.926008412241936
Loss made of: CE 0.2454901933670044, LKD 2.3272485733032227, LDE 0.0, LReg 0.0, POD 7.354898929595947 EntMin 0.0
Epoch 6, Batch 30/43, Loss=11.443622529506683
Loss made of: CE 0.28769275546073914, LKD 4.281203269958496, LDE 0.0, LReg 0.0, POD 7.413571834564209 EntMin 0.0
Epoch 6, Batch 40/43, Loss=11.035181021690368
Loss made of: CE 0.32733413577079773, LKD 3.0908782482147217, LDE 0.0, LReg 0.0, POD 7.592017650604248 EntMin 0.0
Epoch 6, Class Loss=0.27281826734542847, Reg Loss=2.895599126815796
Clinet index 11, End of Epoch 6/6, Average Loss=3.168417453765869, Class Loss=0.27281826734542847, Reg Loss=2.895599126815796
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/52, Loss=12.942503839731216
Loss made of: CE 0.4167442321777344, LKD 5.563011169433594, LDE 0.0, LReg 0.0, POD 8.95541763305664 EntMin 0.0
Epoch 1, Batch 20/52, Loss=13.56828387081623
Loss made of: CE 0.32004261016845703, LKD 3.7993178367614746, LDE 0.0, LReg 0.0, POD 8.973005294799805 EntMin 0.0
Epoch 1, Batch 30/52, Loss=13.590112075209618
Loss made of: CE 0.3075562119483948, LKD 3.4735543727874756, LDE 0.0, LReg 0.0, POD 9.040632247924805 EntMin 0.0
Epoch 1, Batch 40/52, Loss=12.819168701767921
Loss made of: CE 0.39953717589378357, LKD 4.064451217651367, LDE 0.0, LReg 0.0, POD 8.41513729095459 EntMin 0.0
Epoch 1, Batch 50/52, Loss=13.525633904337884
Loss made of: CE 0.31620532274246216, LKD 2.8510050773620605, LDE 0.0, LReg 0.0, POD 10.224193572998047 EntMin 0.0
Epoch 1, Class Loss=0.3778172433376312, Reg Loss=3.8269715309143066
Clinet index 9, End of Epoch 1/6, Average Loss=4.204788684844971, Class Loss=0.3778172433376312, Reg Loss=3.8269715309143066
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/52, Loss=13.659050521254539
Loss made of: CE 0.3833730220794678, LKD 3.805133819580078, LDE 0.0, LReg 0.0, POD 9.235626220703125 EntMin 0.0
Epoch 2, Batch 20/52, Loss=13.72736821770668
Loss made of: CE 0.42062443494796753, LKD 4.292294979095459, LDE 0.0, LReg 0.0, POD 9.321760177612305 EntMin 0.0
Epoch 2, Batch 30/52, Loss=14.387446245551109
Loss made of: CE 0.35833415389060974, LKD 4.465710163116455, LDE 0.0, LReg 0.0, POD 9.182514190673828 EntMin 0.0
Epoch 2, Batch 40/52, Loss=13.414945536851883
Loss made of: CE 0.28645429015159607, LKD 3.1383674144744873, LDE 0.0, LReg 0.0, POD 8.79825496673584 EntMin 0.0
Epoch 2, Batch 50/52, Loss=13.124548241496086
Loss made of: CE 0.27121245861053467, LKD 2.77471923828125, LDE 0.0, LReg 0.0, POD 10.068511962890625 EntMin 0.0
Epoch 2, Class Loss=0.34776797890663147, Reg Loss=3.876086473464966
Clinet index 9, End of Epoch 2/6, Average Loss=4.2238545417785645, Class Loss=0.34776797890663147, Reg Loss=3.876086473464966
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/52, Loss=13.285739386081696
Loss made of: CE 0.3484143912792206, LKD 4.739121913909912, LDE 0.0, LReg 0.0, POD 9.278559684753418 EntMin 0.0
Epoch 3, Batch 20/52, Loss=13.087440851330758
Loss made of: CE 0.39105188846588135, LKD 3.2082204818725586, LDE 0.0, LReg 0.0, POD 11.805185317993164 EntMin 0.0
Epoch 3, Batch 30/52, Loss=13.50581017434597
Loss made of: CE 0.3100481331348419, LKD 3.1552200317382812, LDE 0.0, LReg 0.0, POD 8.69821548461914 EntMin 0.0
Epoch 3, Batch 40/52, Loss=13.709027269482613
Loss made of: CE 0.3315533399581909, LKD 3.8332552909851074, LDE 0.0, LReg 0.0, POD 9.517305374145508 EntMin 0.0
Epoch 3, Batch 50/52, Loss=13.34093582034111
Loss made of: CE 0.3299204111099243, LKD 3.9985625743865967, LDE 0.0, LReg 0.0, POD 9.464924812316895 EntMin 0.0
Epoch 3, Class Loss=0.3347654938697815, Reg Loss=3.843642473220825
Clinet index 9, End of Epoch 3/6, Average Loss=4.178408145904541, Class Loss=0.3347654938697815, Reg Loss=3.843642473220825
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/52, Loss=13.490780806541443
Loss made of: CE 0.30735325813293457, LKD 4.383306503295898, LDE 0.0, LReg 0.0, POD 9.287988662719727 EntMin 0.0
Epoch 4, Batch 20/52, Loss=13.08821680545807
Loss made of: CE 0.31162068247795105, LKD 3.274707555770874, LDE 0.0, LReg 0.0, POD 8.745378494262695 EntMin 0.0
Epoch 4, Batch 30/52, Loss=13.103685742616653
Loss made of: CE 0.3360075354576111, LKD 4.015927791595459, LDE 0.0, LReg 0.0, POD 8.701425552368164 EntMin 0.0
Epoch 4, Batch 40/52, Loss=13.233283895254136
Loss made of: CE 0.2506582736968994, LKD 3.8745946884155273, LDE 0.0, LReg 0.0, POD 8.659883499145508 EntMin 0.0
Epoch 4, Batch 50/52, Loss=12.722526130080222
Loss made of: CE 0.31951022148132324, LKD 4.341951370239258, LDE 0.0, LReg 0.0, POD 9.121818542480469 EntMin 0.0
Epoch 4, Class Loss=0.32252237200737, Reg Loss=3.770470142364502
Clinet index 9, End of Epoch 4/6, Average Loss=4.092992305755615, Class Loss=0.32252237200737, Reg Loss=3.770470142364502
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/52, Loss=12.762744805216789
Loss made of: CE 0.2458488941192627, LKD 2.797025203704834, LDE 0.0, LReg 0.0, POD 8.592527389526367 EntMin 0.0
Epoch 5, Batch 20/52, Loss=13.259896001219749
Loss made of: CE 0.2804785966873169, LKD 3.514087200164795, LDE 0.0, LReg 0.0, POD 9.253734588623047 EntMin 0.0
Epoch 5, Batch 30/52, Loss=12.468102081120014
Loss made of: CE 0.38200458884239197, LKD 4.477022171020508, LDE 0.0, LReg 0.0, POD 9.364301681518555 EntMin 0.0
Epoch 5, Batch 40/52, Loss=13.711903282999993
Loss made of: CE 0.35413575172424316, LKD 4.171492576599121, LDE 0.0, LReg 0.0, POD 9.2128267288208 EntMin 0.0
Epoch 5, Batch 50/52, Loss=13.553647515177726
Loss made of: CE 0.309314101934433, LKD 3.5849008560180664, LDE 0.0, LReg 0.0, POD 10.409879684448242 EntMin 0.0
Epoch 5, Class Loss=0.3231945037841797, Reg Loss=3.7856688499450684
Clinet index 9, End of Epoch 5/6, Average Loss=4.108863353729248, Class Loss=0.3231945037841797, Reg Loss=3.7856688499450684
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/52, Loss=12.839986743032933
Loss made of: CE 0.22491119801998138, LKD 3.7945621013641357, LDE 0.0, LReg 0.0, POD 8.6987943649292 EntMin 0.0
Epoch 6, Batch 20/52, Loss=13.283425509929657
Loss made of: CE 0.3154982030391693, LKD 3.4524455070495605, LDE 0.0, LReg 0.0, POD 9.938946723937988 EntMin 0.0
Epoch 6, Batch 30/52, Loss=12.691812640428543
Loss made of: CE 0.2812613248825073, LKD 3.9441964626312256, LDE 0.0, LReg 0.0, POD 8.742560386657715 EntMin 0.0
Epoch 6, Batch 40/52, Loss=13.268052846193314
Loss made of: CE 0.32232925295829773, LKD 4.497939586639404, LDE 0.0, LReg 0.0, POD 10.624086380004883 EntMin 0.0
Epoch 6, Batch 50/52, Loss=12.466586282849311
Loss made of: CE 0.29546481370925903, LKD 3.6744556427001953, LDE 0.0, LReg 0.0, POD 9.5269775390625 EntMin 0.0
Epoch 6, Class Loss=0.30883559584617615, Reg Loss=3.7720346450805664
Clinet index 9, End of Epoch 6/6, Average Loss=4.080870151519775, Class Loss=0.30883559584617615, Reg Loss=3.7720346450805664
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/34, Loss=15.346088570356368
Loss made of: CE 0.44723081588745117, LKD 3.4092154502868652, LDE 0.0, LReg 0.0, POD 8.985895156860352 EntMin 0.0
Epoch 1, Batch 20/34, Loss=13.600115370750427
Loss made of: CE 0.3049260377883911, LKD 4.340026378631592, LDE 0.0, LReg 0.0, POD 9.073518753051758 EntMin 0.0
Epoch 1, Batch 30/34, Loss=15.052388209104539
Loss made of: CE 0.4046013355255127, LKD 3.9699504375457764, LDE 0.0, LReg 0.0, POD 9.590023040771484 EntMin 0.0
Epoch 1, Class Loss=0.40043097734451294, Reg Loss=3.863037586212158
Clinet index 6, End of Epoch 1/6, Average Loss=4.2634687423706055, Class Loss=0.40043097734451294, Reg Loss=3.863037586212158
Pseudo labeling is: None
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/34, Loss=13.741391041874886
Loss made of: CE 0.3218362033367157, LKD 3.8106985092163086, LDE 0.0, LReg 0.0, POD 10.20621109008789 EntMin 0.0
Epoch 2, Batch 20/34, Loss=13.500424119830132
Loss made of: CE 0.33919042348861694, LKD 4.45479679107666, LDE 0.0, LReg 0.0, POD 8.654430389404297 EntMin 0.0
Epoch 2, Batch 30/34, Loss=14.154927667975425
Loss made of: CE 0.34926995635032654, LKD 4.427590370178223, LDE 0.0, LReg 0.0, POD 10.580780029296875 EntMin 0.0
Epoch 2, Class Loss=0.34576866030693054, Reg Loss=3.835137367248535
Clinet index 6, End of Epoch 2/6, Average Loss=4.180905818939209, Class Loss=0.34576866030693054, Reg Loss=3.835137367248535
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/34, Loss=13.161927664279938
Loss made of: CE 0.3146304488182068, LKD 4.312629699707031, LDE 0.0, LReg 0.0, POD 9.460973739624023 EntMin 0.0
Epoch 3, Batch 20/34, Loss=13.519341638684272
Loss made of: CE 0.3248717784881592, LKD 2.9150047302246094, LDE 0.0, LReg 0.0, POD 8.771767616271973 EntMin 0.0
Epoch 3, Batch 30/34, Loss=14.079006767272949
Loss made of: CE 0.32688701152801514, LKD 4.197652816772461, LDE 0.0, LReg 0.0, POD 9.640119552612305 EntMin 0.0
Epoch 3, Class Loss=0.34172704815864563, Reg Loss=3.7650482654571533
Clinet index 6, End of Epoch 3/6, Average Loss=4.106775283813477, Class Loss=0.34172704815864563, Reg Loss=3.7650482654571533
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/34, Loss=13.821968287229538
Loss made of: CE 0.28521397709846497, LKD 2.9653353691101074, LDE 0.0, LReg 0.0, POD 9.200798034667969 EntMin 0.0
Epoch 4, Batch 20/34, Loss=13.202740183472633
Loss made of: CE 0.3127066195011139, LKD 3.4676780700683594, LDE 0.0, LReg 0.0, POD 9.456278800964355 EntMin 0.0
Epoch 4, Batch 30/34, Loss=13.796854880452155
Loss made of: CE 0.45599883794784546, LKD 3.774873733520508, LDE 0.0, LReg 0.0, POD 11.002279281616211 EntMin 0.0
Epoch 4, Class Loss=0.33894893527030945, Reg Loss=3.763216257095337
Clinet index 6, End of Epoch 4/6, Average Loss=4.102165222167969, Class Loss=0.33894893527030945, Reg Loss=3.763216257095337
Pseudo labeling is: None
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/34, Loss=13.287914565205574
Loss made of: CE 0.34008312225341797, LKD 3.6359519958496094, LDE 0.0, LReg 0.0, POD 8.377161026000977 EntMin 0.0
Epoch 5, Batch 20/34, Loss=13.9371991366148
Loss made of: CE 0.39703458547592163, LKD 4.218522548675537, LDE 0.0, LReg 0.0, POD 12.641124725341797 EntMin 0.0
Epoch 5, Batch 30/34, Loss=12.583630532026291
Loss made of: CE 0.3358965814113617, LKD 4.240220546722412, LDE 0.0, LReg 0.0, POD 8.914985656738281 EntMin 0.0
Epoch 5, Class Loss=0.3340626060962677, Reg Loss=3.6889779567718506
Clinet index 6, End of Epoch 5/6, Average Loss=4.023040771484375, Class Loss=0.3340626060962677, Reg Loss=3.6889779567718506
Pseudo labeling is: None
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/34, Loss=13.645966130495072
Loss made of: CE 0.3519580364227295, LKD 3.3577849864959717, LDE 0.0, LReg 0.0, POD 9.810735702514648 EntMin 0.0
Epoch 6, Batch 20/34, Loss=13.387474474310874
Loss made of: CE 0.38524681329727173, LKD 4.010404586791992, LDE 0.0, LReg 0.0, POD 12.125977516174316 EntMin 0.0
Epoch 6, Batch 30/34, Loss=13.390839993953705
Loss made of: CE 0.3002299964427948, LKD 3.6188416481018066, LDE 0.0, LReg 0.0, POD 8.07862663269043 EntMin 0.0
Epoch 6, Class Loss=0.33446231484413147, Reg Loss=3.773721218109131
Clinet index 6, End of Epoch 6/6, Average Loss=4.10818338394165, Class Loss=0.33446231484413147, Reg Loss=3.773721218109131
federated aggregation...
Validation, Class Loss=0.26453882455825806, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.914125
Mean Acc: 0.707971
FreqW Acc: 0.849947
Mean IoU: 0.584347
Class IoU:
	class 0: 0.9193577
	class 1: 0.8044071
	class 2: 0.2415938
	class 3: 0.5746655
	class 4: 0.5954043
	class 5: 0.06263549
	class 6: 0.8329369
	class 7: 0.6177891
	class 8: 0.6103316
Class Acc:
	class 0: 0.9746642
	class 1: 0.85934234
	class 2: 0.4722129
	class 3: 0.9486482
	class 4: 0.8695811
	class 5: 0.06290891
	class 6: 0.8789443
	class 7: 0.6435767
	class 8: 0.6618584

federated global round: 9, step: 1
select part of clients to conduct local training
[5, 0, 9, 2]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/34, Loss=15.435420197248458
Loss made of: CE 0.5043569803237915, LKD 3.217555522918701, LDE 0.0, LReg 0.0, POD 9.870530128479004 EntMin 0.0
Epoch 1, Batch 20/34, Loss=14.913659298419953
Loss made of: CE 0.43353602290153503, LKD 3.9489905834198, LDE 0.0, LReg 0.0, POD 10.39907455444336 EntMin 0.0
Epoch 1, Batch 30/34, Loss=14.92044036090374
Loss made of: CE 0.4727556109428406, LKD 4.391060829162598, LDE 0.0, LReg 0.0, POD 12.894123077392578 EntMin 0.0
Epoch 1, Class Loss=0.45274633169174194, Reg Loss=3.9565443992614746
Clinet index 5, End of Epoch 1/6, Average Loss=4.409290790557861, Class Loss=0.45274633169174194, Reg Loss=3.9565443992614746
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/34, Loss=14.646260046958924
Loss made of: CE 0.3551211953163147, LKD 3.2914493083953857, LDE 0.0, LReg 0.0, POD 9.200674057006836 EntMin 0.0
Epoch 2, Batch 20/34, Loss=13.186702159047126
Loss made of: CE 0.34555789828300476, LKD 3.780027389526367, LDE 0.0, LReg 0.0, POD 9.944401741027832 EntMin 0.0
Epoch 2, Batch 30/34, Loss=14.599034464359283
Loss made of: CE 0.4368438720703125, LKD 4.717642784118652, LDE 0.0, LReg 0.0, POD 12.156761169433594 EntMin 0.0
Epoch 2, Class Loss=0.3612358272075653, Reg Loss=3.862405776977539
Clinet index 5, End of Epoch 2/6, Average Loss=4.223641395568848, Class Loss=0.3612358272075653, Reg Loss=3.862405776977539
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/34, Loss=13.223258048295975
Loss made of: CE 0.3119443356990814, LKD 4.515342712402344, LDE 0.0, LReg 0.0, POD 8.828611373901367 EntMin 0.0
Epoch 3, Batch 20/34, Loss=13.117518872022629
Loss made of: CE 0.3490402400493622, LKD 3.551614284515381, LDE 0.0, LReg 0.0, POD 11.327369689941406 EntMin 0.0
Epoch 3, Batch 30/34, Loss=14.332263016700745
Loss made of: CE 0.4177781045436859, LKD 4.028552055358887, LDE 0.0, LReg 0.0, POD 12.407968521118164 EntMin 0.0
Epoch 3, Class Loss=0.33828088641166687, Reg Loss=3.8083786964416504
Clinet index 5, End of Epoch 3/6, Average Loss=4.1466593742370605, Class Loss=0.33828088641166687, Reg Loss=3.8083786964416504
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/34, Loss=13.302154925465583
Loss made of: CE 0.3285094201564789, LKD 3.500844955444336, LDE 0.0, LReg 0.0, POD 9.119707107543945 EntMin 0.0
Epoch 4, Batch 20/34, Loss=13.66711746752262
Loss made of: CE 0.3958589732646942, LKD 3.6248221397399902, LDE 0.0, LReg 0.0, POD 9.719972610473633 EntMin 0.0
Epoch 4, Batch 30/34, Loss=13.484278956055642
Loss made of: CE 0.3786632716655731, LKD 3.972588539123535, LDE 0.0, LReg 0.0, POD 7.9927263259887695 EntMin 0.0
Epoch 4, Class Loss=0.33488768339157104, Reg Loss=3.7507193088531494
Clinet index 5, End of Epoch 4/6, Average Loss=4.085607051849365, Class Loss=0.33488768339157104, Reg Loss=3.7507193088531494
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/34, Loss=13.359599646925925
Loss made of: CE 0.35653218626976013, LKD 4.820698261260986, LDE 0.0, LReg 0.0, POD 9.108149528503418 EntMin 0.0
Epoch 5, Batch 20/34, Loss=13.392697849869728
Loss made of: CE 0.3330056369304657, LKD 3.7090163230895996, LDE 0.0, LReg 0.0, POD 9.27943229675293 EntMin 0.0
Epoch 5, Batch 30/34, Loss=13.903043034672738
Loss made of: CE 0.3661143481731415, LKD 4.474510192871094, LDE 0.0, LReg 0.0, POD 10.186057090759277 EntMin 0.0
Epoch 5, Class Loss=0.330573171377182, Reg Loss=3.852724075317383
Clinet index 5, End of Epoch 5/6, Average Loss=4.183297157287598, Class Loss=0.330573171377182, Reg Loss=3.852724075317383
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/34, Loss=14.3599166482687
Loss made of: CE 0.3669602870941162, LKD 4.460663318634033, LDE 0.0, LReg 0.0, POD 9.07330322265625 EntMin 0.0
Epoch 6, Batch 20/34, Loss=13.20994633436203
Loss made of: CE 0.2987649440765381, LKD 4.184486389160156, LDE 0.0, LReg 0.0, POD 9.454712867736816 EntMin 0.0
Epoch 6, Batch 30/34, Loss=13.351780194044114
Loss made of: CE 0.482927143573761, LKD 4.557023048400879, LDE 0.0, LReg 0.0, POD 11.295385360717773 EntMin 0.0
Epoch 6, Class Loss=0.3364231586456299, Reg Loss=3.870032787322998
Clinet index 5, End of Epoch 6/6, Average Loss=4.206456184387207, Class Loss=0.3364231586456299, Reg Loss=3.870032787322998
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/52, Loss=12.97705456018448
Loss made of: CE 0.5338782072067261, LKD 4.339299201965332, LDE 0.0, LReg 0.0, POD 8.051252365112305 EntMin 0.0
Epoch 1, Batch 20/52, Loss=12.45015452504158
Loss made of: CE 0.3731977343559265, LKD 4.302585601806641, LDE 0.0, LReg 0.0, POD 7.944694519042969 EntMin 0.0
Epoch 1, Batch 30/52, Loss=12.31518497467041
Loss made of: CE 0.3482198119163513, LKD 3.6575517654418945, LDE 0.0, LReg 0.0, POD 8.03170108795166 EntMin 0.0
Epoch 1, Batch 40/52, Loss=12.151155051589011
Loss made of: CE 0.25800764560699463, LKD 2.949974775314331, LDE 0.0, LReg 0.0, POD 9.13194751739502 EntMin 0.0
Epoch 1, Batch 50/52, Loss=12.689862996339798
Loss made of: CE 0.2680007517337799, LKD 2.5409886837005615, LDE 0.0, LReg 0.0, POD 7.990987777709961 EntMin 0.0
Epoch 1, Class Loss=0.37105441093444824, Reg Loss=3.6310110092163086
Clinet index 0, End of Epoch 1/6, Average Loss=4.002065658569336, Class Loss=0.37105441093444824, Reg Loss=3.6310110092163086
Pseudo labeling is: None
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/52, Loss=12.697713440656662
Loss made of: CE 0.4736725091934204, LKD 3.9284560680389404, LDE 0.0, LReg 0.0, POD 8.18542194366455 EntMin 0.0
Epoch 2, Batch 20/52, Loss=12.433946272730827
Loss made of: CE 0.3217284679412842, LKD 3.45599365234375, LDE 0.0, LReg 0.0, POD 7.70963191986084 EntMin 0.0
Epoch 2, Batch 30/52, Loss=12.786407145857812
Loss made of: CE 0.35104161500930786, LKD 3.41874098777771, LDE 0.0, LReg 0.0, POD 8.791722297668457 EntMin 0.0
Epoch 2, Batch 40/52, Loss=12.327368620038033
Loss made of: CE 0.36397475004196167, LKD 4.228418350219727, LDE 0.0, LReg 0.0, POD 7.834814071655273 EntMin 0.0
Epoch 2, Batch 50/52, Loss=12.290573757886886
Loss made of: CE 0.3816709518432617, LKD 3.8118045330047607, LDE 0.0, LReg 0.0, POD 9.94368839263916 EntMin 0.0
Epoch 2, Class Loss=0.3352709710597992, Reg Loss=3.6308512687683105
Clinet index 0, End of Epoch 2/6, Average Loss=3.9661221504211426, Class Loss=0.3352709710597992, Reg Loss=3.6308512687683105
Pseudo labeling is: None
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/52, Loss=12.322355234622956
Loss made of: CE 0.29056262969970703, LKD 3.348813533782959, LDE 0.0, LReg 0.0, POD 8.53941535949707 EntMin 0.0
Epoch 3, Batch 20/52, Loss=13.015782734751701
Loss made of: CE 0.2959129810333252, LKD 3.7044754028320312, LDE 0.0, LReg 0.0, POD 8.603219985961914 EntMin 0.0
Epoch 3, Batch 30/52, Loss=12.53452771306038
Loss made of: CE 0.4817051291465759, LKD 4.709259033203125, LDE 0.0, LReg 0.0, POD 7.931586742401123 EntMin 0.0
Epoch 3, Batch 40/52, Loss=12.181743587553502
Loss made of: CE 0.27268123626708984, LKD 2.5132875442504883, LDE 0.0, LReg 0.0, POD 8.679374694824219 EntMin 0.0
Epoch 3, Batch 50/52, Loss=11.889191767573356
Loss made of: CE 0.3137759566307068, LKD 3.7697200775146484, LDE 0.0, LReg 0.0, POD 7.806009292602539 EntMin 0.0
Epoch 3, Class Loss=0.32730504870414734, Reg Loss=3.5743563175201416
Clinet index 0, End of Epoch 3/6, Average Loss=3.9016613960266113, Class Loss=0.32730504870414734, Reg Loss=3.5743563175201416
Pseudo labeling is: None
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/52, Loss=12.958034259080886
Loss made of: CE 0.37010860443115234, LKD 3.5298023223876953, LDE 0.0, LReg 0.0, POD 8.272822380065918 EntMin 0.0
Epoch 4, Batch 20/52, Loss=11.875337249040603
Loss made of: CE 0.2937659025192261, LKD 3.5588836669921875, LDE 0.0, LReg 0.0, POD 7.7491455078125 EntMin 0.0
Epoch 4, Batch 30/52, Loss=12.142621344327926
Loss made of: CE 0.25305891036987305, LKD 2.981813430786133, LDE 0.0, LReg 0.0, POD 7.654822826385498 EntMin 0.0
Epoch 4, Batch 40/52, Loss=12.334293860197068
Loss made of: CE 0.31720995903015137, LKD 3.2255821228027344, LDE 0.0, LReg 0.0, POD 7.280488967895508 EntMin 0.0
Epoch 4, Batch 50/52, Loss=11.489929124712944
Loss made of: CE 0.2911783456802368, LKD 3.4774856567382812, LDE 0.0, LReg 0.0, POD 7.321188449859619 EntMin 0.0
Epoch 4, Class Loss=0.31754040718078613, Reg Loss=3.5887610912323
Clinet index 0, End of Epoch 4/6, Average Loss=3.906301498413086, Class Loss=0.31754040718078613, Reg Loss=3.5887610912323
Pseudo labeling is: None
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/52, Loss=11.92799015045166
Loss made of: CE 0.31307774782180786, LKD 3.664381742477417, LDE 0.0, LReg 0.0, POD 7.757212162017822 EntMin 0.0
Epoch 5, Batch 20/52, Loss=11.941795510053634
Loss made of: CE 0.27192163467407227, LKD 3.0677363872528076, LDE 0.0, LReg 0.0, POD 7.893621921539307 EntMin 0.0
Epoch 5, Batch 30/52, Loss=11.677738064527512
Loss made of: CE 0.3219359517097473, LKD 3.436474561691284, LDE 0.0, LReg 0.0, POD 8.125295639038086 EntMin 0.0
Epoch 5, Batch 40/52, Loss=11.876560464501381
Loss made of: CE 0.3252882957458496, LKD 3.3788676261901855, LDE 0.0, LReg 0.0, POD 8.691006660461426 EntMin 0.0
Epoch 5, Batch 50/52, Loss=11.89672256708145
Loss made of: CE 0.2929299473762512, LKD 3.658090353012085, LDE 0.0, LReg 0.0, POD 7.601226806640625 EntMin 0.0
Epoch 5, Class Loss=0.31519246101379395, Reg Loss=3.546285390853882
Clinet index 0, End of Epoch 5/6, Average Loss=3.861477851867676, Class Loss=0.31519246101379395, Reg Loss=3.546285390853882
Pseudo labeling is: None
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/52, Loss=11.269174574315548
Loss made of: CE 0.2948741316795349, LKD 3.676238536834717, LDE 0.0, LReg 0.0, POD 7.257333755493164 EntMin 0.0
Epoch 6, Batch 20/52, Loss=12.184012538194656
Loss made of: CE 0.27830833196640015, LKD 3.3968334197998047, LDE 0.0, LReg 0.0, POD 7.065690040588379 EntMin 0.0
Epoch 6, Batch 30/52, Loss=12.110763105750085
Loss made of: CE 0.32662463188171387, LKD 2.9591012001037598, LDE 0.0, LReg 0.0, POD 8.018754959106445 EntMin 0.0
Epoch 6, Batch 40/52, Loss=11.516897612810135
Loss made of: CE 0.29664888978004456, LKD 2.6282095909118652, LDE 0.0, LReg 0.0, POD 7.51875638961792 EntMin 0.0
Epoch 6, Batch 50/52, Loss=11.887877559661865
Loss made of: CE 0.24153617024421692, LKD 2.2558157444000244, LDE 0.0, LReg 0.0, POD 8.34537410736084 EntMin 0.0
Epoch 6, Class Loss=0.31960317492485046, Reg Loss=3.5881757736206055
Clinet index 0, End of Epoch 6/6, Average Loss=3.9077789783477783, Class Loss=0.31960317492485046, Reg Loss=3.5881757736206055
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/52, Loss=12.695523816347123
Loss made of: CE 0.4366261959075928, LKD 5.586341857910156, LDE 0.0, LReg 0.0, POD 8.383699417114258 EntMin 0.0
Epoch 1, Batch 20/52, Loss=13.349156698584556
Loss made of: CE 0.34586891531944275, LKD 3.982454299926758, LDE 0.0, LReg 0.0, POD 8.995706558227539 EntMin 0.0
Epoch 1, Batch 30/52, Loss=13.050403872132302
Loss made of: CE 0.30840057134628296, LKD 3.736424207687378, LDE 0.0, LReg 0.0, POD 8.995941162109375 EntMin 0.0
Epoch 1, Batch 40/52, Loss=12.056203630566596
Loss made of: CE 0.4079805314540863, LKD 4.024327754974365, LDE 0.0, LReg 0.0, POD 8.028115272521973 EntMin 0.0
Epoch 1, Batch 50/52, Loss=12.908665379881858
Loss made of: CE 0.32333433628082275, LKD 2.9020304679870605, LDE 0.0, LReg 0.0, POD 9.945504188537598 EntMin 0.0
Epoch 1, Class Loss=0.3775820732116699, Reg Loss=3.7619776725769043
Clinet index 9, End of Epoch 1/6, Average Loss=4.139559745788574, Class Loss=0.3775820732116699, Reg Loss=3.7619776725769043
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/52, Loss=12.46404258608818
Loss made of: CE 0.3632206916809082, LKD 4.018491744995117, LDE 0.0, LReg 0.0, POD 8.327751159667969 EntMin 0.0
Epoch 2, Batch 20/52, Loss=13.150102439522744
Loss made of: CE 0.42368754744529724, LKD 4.369215965270996, LDE 0.0, LReg 0.0, POD 8.488654136657715 EntMin 0.0
Epoch 2, Batch 30/52, Loss=13.502267888188362
Loss made of: CE 0.3533414304256439, LKD 4.547713756561279, LDE 0.0, LReg 0.0, POD 8.155815124511719 EntMin 0.0
Epoch 2, Batch 40/52, Loss=12.243102154135704
Loss made of: CE 0.3022676706314087, LKD 2.941852569580078, LDE 0.0, LReg 0.0, POD 7.674037456512451 EntMin 0.0
Epoch 2, Batch 50/52, Loss=12.335171800851821
Loss made of: CE 0.2876116633415222, LKD 3.022289276123047, LDE 0.0, LReg 0.0, POD 9.236839294433594 EntMin 0.0
Epoch 2, Class Loss=0.34706777334213257, Reg Loss=3.768918037414551
Clinet index 9, End of Epoch 2/6, Average Loss=4.115985870361328, Class Loss=0.34706777334213257, Reg Loss=3.768918037414551
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/52, Loss=12.518173488974572
Loss made of: CE 0.3256876766681671, LKD 4.2318315505981445, LDE 0.0, LReg 0.0, POD 8.935096740722656 EntMin 0.0
Epoch 3, Batch 20/52, Loss=12.337794703245162
Loss made of: CE 0.3622171878814697, LKD 3.5134668350219727, LDE 0.0, LReg 0.0, POD 10.937747955322266 EntMin 0.0
Epoch 3, Batch 30/52, Loss=12.858688712120056
Loss made of: CE 0.31919264793395996, LKD 3.356863021850586, LDE 0.0, LReg 0.0, POD 7.791494846343994 EntMin 0.0
Epoch 3, Batch 40/52, Loss=12.961433264613152
Loss made of: CE 0.3700849711894989, LKD 3.7979540824890137, LDE 0.0, LReg 0.0, POD 9.192365646362305 EntMin 0.0
Epoch 3, Batch 50/52, Loss=12.50529690682888
Loss made of: CE 0.3035782277584076, LKD 3.7706618309020996, LDE 0.0, LReg 0.0, POD 8.62545394897461 EntMin 0.0
Epoch 3, Class Loss=0.3330187499523163, Reg Loss=3.7606003284454346
Clinet index 9, End of Epoch 3/6, Average Loss=4.093618869781494, Class Loss=0.3330187499523163, Reg Loss=3.7606003284454346
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/52, Loss=12.59860489666462
Loss made of: CE 0.3183269500732422, LKD 4.335458755493164, LDE 0.0, LReg 0.0, POD 8.627108573913574 EntMin 0.0
Epoch 4, Batch 20/52, Loss=12.128092023730279
Loss made of: CE 0.2866572439670563, LKD 3.145911931991577, LDE 0.0, LReg 0.0, POD 7.975204944610596 EntMin 0.0
Epoch 4, Batch 30/52, Loss=12.291178616881371
Loss made of: CE 0.3352195918560028, LKD 3.9830844402313232, LDE 0.0, LReg 0.0, POD 8.137178421020508 EntMin 0.0
Epoch 4, Batch 40/52, Loss=12.576952096819877
Loss made of: CE 0.2683113217353821, LKD 4.045947551727295, LDE 0.0, LReg 0.0, POD 8.367049217224121 EntMin 0.0
Epoch 4, Batch 50/52, Loss=12.053424870967865
Loss made of: CE 0.3153768479824066, LKD 4.205875396728516, LDE 0.0, LReg 0.0, POD 8.839496612548828 EntMin 0.0
Epoch 4, Class Loss=0.3252270221710205, Reg Loss=3.626678705215454
Clinet index 9, End of Epoch 4/6, Average Loss=3.9519057273864746, Class Loss=0.3252270221710205, Reg Loss=3.626678705215454
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/52, Loss=11.96249428987503
Loss made of: CE 0.2451498806476593, LKD 2.900743007659912, LDE 0.0, LReg 0.0, POD 7.679033279418945 EntMin 0.0
Epoch 5, Batch 20/52, Loss=12.570996329188347
Loss made of: CE 0.3152201771736145, LKD 3.6196513175964355, LDE 0.0, LReg 0.0, POD 8.648815155029297 EntMin 0.0
Epoch 5, Batch 30/52, Loss=11.699031794071198
Loss made of: CE 0.3966712951660156, LKD 4.229808330535889, LDE 0.0, LReg 0.0, POD 7.8014068603515625 EntMin 0.0
Epoch 5, Batch 40/52, Loss=12.675933992862701
Loss made of: CE 0.3154847025871277, LKD 3.868863105773926, LDE 0.0, LReg 0.0, POD 8.538789749145508 EntMin 0.0
Epoch 5, Batch 50/52, Loss=12.369349923729896
Loss made of: CE 0.34870433807373047, LKD 3.637863874435425, LDE 0.0, LReg 0.0, POD 9.186027526855469 EntMin 0.0
Epoch 5, Class Loss=0.32550764083862305, Reg Loss=3.6610114574432373
Clinet index 9, End of Epoch 5/6, Average Loss=3.9865190982818604, Class Loss=0.32550764083862305, Reg Loss=3.6610114574432373
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/52, Loss=12.070170336961747
Loss made of: CE 0.23381492495536804, LKD 3.452069044113159, LDE 0.0, LReg 0.0, POD 8.32960033416748 EntMin 0.0
Epoch 6, Batch 20/52, Loss=12.426887169480324
Loss made of: CE 0.32983073592185974, LKD 3.656827449798584, LDE 0.0, LReg 0.0, POD 8.845853805541992 EntMin 0.0
Epoch 6, Batch 30/52, Loss=12.281590186059475
Loss made of: CE 0.29293209314346313, LKD 4.175266265869141, LDE 0.0, LReg 0.0, POD 8.19478988647461 EntMin 0.0
Epoch 6, Batch 40/52, Loss=12.60843760073185
Loss made of: CE 0.3343539834022522, LKD 3.8850791454315186, LDE 0.0, LReg 0.0, POD 11.024402618408203 EntMin 0.0
Epoch 6, Batch 50/52, Loss=11.843210831284523
Loss made of: CE 0.3319331109523773, LKD 3.4578351974487305, LDE 0.0, LReg 0.0, POD 8.5601806640625 EntMin 0.0
Epoch 6, Class Loss=0.3238639831542969, Reg Loss=3.715762138366699
Clinet index 9, End of Epoch 6/6, Average Loss=4.039626121520996, Class Loss=0.3238639831542969, Reg Loss=3.715762138366699
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/25, Loss=13.570622646808625
Loss made of: CE 0.3877132833003998, LKD 3.589019298553467, LDE 0.0, LReg 0.0, POD 10.176170349121094 EntMin 0.0
Epoch 1, Batch 20/25, Loss=12.648248308897019
Loss made of: CE 0.4585512578487396, LKD 3.0542569160461426, LDE 0.0, LReg 0.0, POD 9.371007919311523 EntMin 0.0
Epoch 1, Class Loss=0.3672296702861786, Reg Loss=2.8814330101013184
Clinet index 2, End of Epoch 1/6, Average Loss=3.2486627101898193, Class Loss=0.3672296702861786, Reg Loss=2.8814330101013184
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/25, Loss=11.972776652872563
Loss made of: CE 0.3422909379005432, LKD 3.100368022918701, LDE 0.0, LReg 0.0, POD 8.497488021850586 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 20/25, Loss=12.583454521000386
Loss made of: CE 0.19178926944732666, LKD 2.4998040199279785, LDE 0.0, LReg 0.0, POD 8.906230926513672 EntMin 0.0
Epoch 2, Class Loss=0.30235424637794495, Reg Loss=2.730689525604248
Clinet index 2, End of Epoch 2/6, Average Loss=3.03304386138916, Class Loss=0.30235424637794495, Reg Loss=2.730689525604248
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/25, Loss=11.554554723203182
Loss made of: CE 0.1830810308456421, LKD 2.382659435272217, LDE 0.0, LReg 0.0, POD 8.762275695800781 EntMin 0.0
Epoch 3, Batch 20/25, Loss=12.10046965032816
Loss made of: CE 0.26590776443481445, LKD 2.678614616394043, LDE 0.0, LReg 0.0, POD 8.500123023986816 EntMin 0.0
Epoch 3, Class Loss=0.2740626335144043, Reg Loss=2.6693360805511475
Clinet index 2, End of Epoch 3/6, Average Loss=2.9433987140655518, Class Loss=0.2740626335144043, Reg Loss=2.6693360805511475
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/25, Loss=11.630790193378925
Loss made of: CE 0.2277098000049591, LKD 2.6201348304748535, LDE 0.0, LReg 0.0, POD 7.964192867279053 EntMin 0.0
Epoch 4, Batch 20/25, Loss=11.491225539147853
Loss made of: CE 0.20840129256248474, LKD 2.4634995460510254, LDE 0.0, LReg 0.0, POD 7.305741310119629 EntMin 0.0
Epoch 4, Class Loss=0.2617509067058563, Reg Loss=2.744659900665283
Clinet index 2, End of Epoch 4/6, Average Loss=3.006410837173462, Class Loss=0.2617509067058563, Reg Loss=2.744659900665283
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/25, Loss=11.459955547749995
Loss made of: CE 0.17106448113918304, LKD 2.318913221359253, LDE 0.0, LReg 0.0, POD 8.036084175109863 EntMin 0.0
Epoch 5, Batch 20/25, Loss=11.241488654911517
Loss made of: CE 0.25024574995040894, LKD 2.636777877807617, LDE 0.0, LReg 0.0, POD 7.5664238929748535 EntMin 0.0
Epoch 5, Class Loss=0.2500065267086029, Reg Loss=2.6830620765686035
Clinet index 2, End of Epoch 5/6, Average Loss=2.9330685138702393, Class Loss=0.2500065267086029, Reg Loss=2.6830620765686035
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/25, Loss=11.196934984624386
Loss made of: CE 0.283038467168808, LKD 2.197295665740967, LDE 0.0, LReg 0.0, POD 8.297048568725586 EntMin 0.0
Epoch 6, Batch 20/25, Loss=11.292871299386025
Loss made of: CE 0.2681717276573181, LKD 2.4716134071350098, LDE 0.0, LReg 0.0, POD 7.8270158767700195 EntMin 0.0
Epoch 6, Class Loss=0.2530803084373474, Reg Loss=2.702687978744507
Clinet index 2, End of Epoch 6/6, Average Loss=2.955768346786499, Class Loss=0.2530803084373474, Reg Loss=2.702687978744507
federated aggregation...
Validation, Class Loss=0.25418463349342346, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.919283
Mean Acc: 0.723876
FreqW Acc: 0.859332
Mean IoU: 0.602592
Class IoU:
	class 0: 0.9243321
	class 1: 0.80777
	class 2: 0.23649372
	class 3: 0.5769106
	class 4: 0.6254522
	class 5: 0.08762914
	class 6: 0.8471486
	class 7: 0.67541105
	class 8: 0.64217806
Class Acc:
	class 0: 0.97220916
	class 1: 0.8602977
	class 2: 0.44680718
	class 3: 0.95052534
	class 4: 0.85985297
	class 5: 0.08824898
	class 6: 0.8923321
	class 7: 0.72440654
	class 8: 0.7202038

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[0, 15, 1, 4]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=21.49593926668167
Loss made of: CE 1.1975173950195312, LKD 3.807445526123047, LDE 0.0, LReg 0.0, POD 14.149198532104492 EntMin 0.0
Epoch 1, Class Loss=1.3112492561340332, Reg Loss=4.567963600158691
Clinet index 0, End of Epoch 1/6, Average Loss=5.879212856292725, Class Loss=1.3112492561340332, Reg Loss=4.567963600158691
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=16.740375918149947
Loss made of: CE 0.9018961191177368, LKD 3.37943172454834, LDE 0.0, LReg 0.0, POD 11.833945274353027 EntMin 0.0
Epoch 2, Class Loss=0.8957585096359253, Reg Loss=3.839592456817627
Clinet index 0, End of Epoch 2/6, Average Loss=4.735351085662842, Class Loss=0.8957585096359253, Reg Loss=3.839592456817627
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=16.02251535654068
Loss made of: CE 0.9432834386825562, LKD 4.4951910972595215, LDE 0.0, LReg 0.0, POD 10.779670715332031 EntMin 0.0
Epoch 3, Class Loss=0.812703013420105, Reg Loss=3.8070318698883057
Clinet index 0, End of Epoch 3/6, Average Loss=4.619734764099121, Class Loss=0.812703013420105, Reg Loss=3.8070318698883057
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=14.74663171172142
Loss made of: CE 0.7939841747283936, LKD 3.3075034618377686, LDE 0.0, LReg 0.0, POD 10.950372695922852 EntMin 0.0
Epoch 4, Class Loss=0.7496244311332703, Reg Loss=3.7138137817382812
Clinet index 0, End of Epoch 4/6, Average Loss=4.463438034057617, Class Loss=0.7496244311332703, Reg Loss=3.7138137817382812
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=14.734495007991791
Loss made of: CE 0.727127730846405, LKD 3.749721050262451, LDE 0.0, LReg 0.0, POD 10.450737953186035 EntMin 0.0
Epoch 5, Class Loss=0.6900651454925537, Reg Loss=3.736304521560669
Clinet index 0, End of Epoch 5/6, Average Loss=4.426369667053223, Class Loss=0.6900651454925537, Reg Loss=3.736304521560669
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=14.17808747291565
Loss made of: CE 0.5979802012443542, LKD 3.3955841064453125, LDE 0.0, LReg 0.0, POD 9.399009704589844 EntMin 0.0
Epoch 6, Class Loss=0.6540735960006714, Reg Loss=3.7831838130950928
Clinet index 0, End of Epoch 6/6, Average Loss=4.437257289886475, Class Loss=0.6540735960006714, Reg Loss=3.7831838130950928
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=21.304383909702302
Loss made of: CE 1.147264838218689, LKD 3.434713363647461, LDE 0.0, LReg 0.0, POD 13.73208236694336 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=1.3232605457305908, Reg Loss=4.594491958618164
Clinet index 15, End of Epoch 1/6, Average Loss=5.917752265930176, Class Loss=1.3232605457305908, Reg Loss=4.594491958618164
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=17.352418148517607
Loss made of: CE 0.6970846056938171, LKD 3.480494260787964, LDE 0.0, LReg 0.0, POD 12.389619827270508 EntMin 0.0
Epoch 2, Class Loss=0.9389681816101074, Reg Loss=3.8027942180633545
Clinet index 15, End of Epoch 2/6, Average Loss=4.741762161254883, Class Loss=0.9389681816101074, Reg Loss=3.8027942180633545
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=15.71709077358246
Loss made of: CE 0.9228519201278687, LKD 3.2121493816375732, LDE 0.0, LReg 0.0, POD 10.164739608764648 EntMin 0.0
Epoch 3, Class Loss=0.8294591903686523, Reg Loss=3.675795793533325
Clinet index 15, End of Epoch 3/6, Average Loss=4.505254745483398, Class Loss=0.8294591903686523, Reg Loss=3.675795793533325
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=15.229428946971893
Loss made of: CE 0.7820762395858765, LKD 3.854991912841797, LDE 0.0, LReg 0.0, POD 10.906012535095215 EntMin 0.0
Epoch 4, Class Loss=0.7574453949928284, Reg Loss=3.7224626541137695
Clinet index 15, End of Epoch 4/6, Average Loss=4.479907989501953, Class Loss=0.7574453949928284, Reg Loss=3.7224626541137695
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=14.477344185113907
Loss made of: CE 0.6490079760551453, LKD 3.6834733486175537, LDE 0.0, LReg 0.0, POD 10.341419219970703 EntMin 0.0
Epoch 5, Class Loss=0.7125570178031921, Reg Loss=3.6217710971832275
Clinet index 15, End of Epoch 5/6, Average Loss=4.3343281745910645, Class Loss=0.7125570178031921, Reg Loss=3.6217710971832275
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=14.039624601602554
Loss made of: CE 0.610912024974823, LKD 3.245637893676758, LDE 0.0, LReg 0.0, POD 9.138798713684082 EntMin 0.0
Epoch 6, Class Loss=0.6608866453170776, Reg Loss=3.6901888847351074
Clinet index 15, End of Epoch 6/6, Average Loss=4.351075649261475, Class Loss=0.6608866453170776, Reg Loss=3.6901888847351074
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=26.53603584766388
Loss made of: CE 1.5388352870941162, LKD 6.9242987632751465, LDE 0.0, LReg 0.0, POD 17.648094177246094 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/35, Loss=22.804965257644653
Loss made of: CE 1.1101315021514893, LKD 5.428678035736084, LDE 0.0, LReg 0.0, POD 14.999369621276855 EntMin 0.0
Epoch 1, Batch 30/35, Loss=21.562176370620726
Loss made of: CE 0.999840259552002, LKD 6.720911979675293, LDE 0.0, LReg 0.0, POD 15.019448280334473 EntMin 0.0
Epoch 1, Class Loss=1.315384864807129, Reg Loss=5.908679962158203
Clinet index 1, End of Epoch 1/6, Average Loss=7.224064826965332, Class Loss=1.315384864807129, Reg Loss=5.908679962158203
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/35, Loss=20.18539434671402
Loss made of: CE 0.7793155908584595, LKD 5.630335807800293, LDE 0.0, LReg 0.0, POD 13.35195541381836 EntMin 0.0
Epoch 2, Batch 20/35, Loss=20.401292467117308
Loss made of: CE 0.9088687896728516, LKD 5.477568626403809, LDE 0.0, LReg 0.0, POD 14.44690227508545 EntMin 0.0
Epoch 2, Batch 30/35, Loss=19.425986570119857
Loss made of: CE 0.6617061495780945, LKD 5.9692301750183105, LDE 0.0, LReg 0.0, POD 12.498855590820312 EntMin 0.0
Epoch 2, Class Loss=0.7935413718223572, Reg Loss=5.7241411209106445
Clinet index 1, End of Epoch 2/6, Average Loss=6.5176825523376465, Class Loss=0.7935413718223572, Reg Loss=5.7241411209106445
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/35, Loss=19.11444243788719
Loss made of: CE 0.7423527240753174, LKD 6.730016708374023, LDE 0.0, LReg 0.0, POD 13.412582397460938 EntMin 0.0
Epoch 3, Batch 20/35, Loss=19.706214433908464
Loss made of: CE 0.7120949625968933, LKD 6.126364707946777, LDE 0.0, LReg 0.0, POD 13.65573501586914 EntMin 0.0
Epoch 3, Batch 30/35, Loss=18.750009816884994
Loss made of: CE 0.6937910914421082, LKD 5.895954132080078, LDE 0.0, LReg 0.0, POD 13.174087524414062 EntMin 0.0
Epoch 3, Class Loss=0.6903504729270935, Reg Loss=5.630119323730469
Clinet index 1, End of Epoch 3/6, Average Loss=6.320469856262207, Class Loss=0.6903504729270935, Reg Loss=5.630119323730469
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/35, Loss=18.88948181271553
Loss made of: CE 0.6860792636871338, LKD 5.4129438400268555, LDE 0.0, LReg 0.0, POD 13.315537452697754 EntMin 0.0
Epoch 4, Batch 20/35, Loss=18.180833435058595
Loss made of: CE 0.5867549180984497, LKD 4.50338077545166, LDE 0.0, LReg 0.0, POD 13.163848876953125 EntMin 0.0
Epoch 4, Batch 30/35, Loss=18.43782040476799
Loss made of: CE 0.700019359588623, LKD 5.008002281188965, LDE 0.0, LReg 0.0, POD 12.307255744934082 EntMin 0.0
Epoch 4, Class Loss=0.6456549763679504, Reg Loss=5.59691047668457
Clinet index 1, End of Epoch 4/6, Average Loss=6.242565631866455, Class Loss=0.6456549763679504, Reg Loss=5.59691047668457
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/35, Loss=18.4265267431736
Loss made of: CE 0.5389639735221863, LKD 5.401453018188477, LDE 0.0, LReg 0.0, POD 12.31852912902832 EntMin 0.0
Epoch 5, Batch 20/35, Loss=18.410060280561446
Loss made of: CE 0.6768569350242615, LKD 6.4618988037109375, LDE 0.0, LReg 0.0, POD 12.000600814819336 EntMin 0.0
Epoch 5, Batch 30/35, Loss=17.01049100756645
Loss made of: CE 0.545799970626831, LKD 5.046963691711426, LDE 0.0, LReg 0.0, POD 11.213069915771484 EntMin 0.0
Epoch 5, Class Loss=0.6068279147148132, Reg Loss=5.570406913757324
Clinet index 1, End of Epoch 5/6, Average Loss=6.177234649658203, Class Loss=0.6068279147148132, Reg Loss=5.570406913757324
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/35, Loss=17.56926432847977
Loss made of: CE 0.4989092946052551, LKD 5.048354625701904, LDE 0.0, LReg 0.0, POD 11.652796745300293 EntMin 0.0
Epoch 6, Batch 20/35, Loss=18.272681519389153
Loss made of: CE 0.5914920568466187, LKD 6.2412848472595215, LDE 0.0, LReg 0.0, POD 11.306427001953125 EntMin 0.0
Epoch 6, Batch 30/35, Loss=18.45352789759636
Loss made of: CE 0.6886376738548279, LKD 5.398699760437012, LDE 0.0, LReg 0.0, POD 11.44297981262207 EntMin 0.0
Epoch 6, Class Loss=0.6011444926261902, Reg Loss=5.631658554077148
Clinet index 1, End of Epoch 6/6, Average Loss=6.232802867889404, Class Loss=0.6011444926261902, Reg Loss=5.631658554077148
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=21.382588660717012
Loss made of: CE 1.3132275342941284, LKD 4.018440246582031, LDE 0.0, LReg 0.0, POD 13.582830429077148 EntMin 0.0
Epoch 1, Class Loss=1.3090907335281372, Reg Loss=4.50928258895874
Clinet index 4, End of Epoch 1/6, Average Loss=5.818373203277588, Class Loss=1.3090907335281372, Reg Loss=4.50928258895874
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=16.951867473125457
Loss made of: CE 0.858576238155365, LKD 4.252407073974609, LDE 0.0, LReg 0.0, POD 11.018987655639648 EntMin 0.0
Epoch 2, Class Loss=0.9007514119148254, Reg Loss=3.7847795486450195
Clinet index 4, End of Epoch 2/6, Average Loss=4.685531139373779, Class Loss=0.9007514119148254, Reg Loss=3.7847795486450195
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=15.502561450004578
Loss made of: CE 0.7324289083480835, LKD 3.8177733421325684, LDE 0.0, LReg 0.0, POD 10.125879287719727 EntMin 0.0
Epoch 3, Class Loss=0.8291806578636169, Reg Loss=3.6595680713653564
Clinet index 4, End of Epoch 3/6, Average Loss=4.488748550415039, Class Loss=0.8291806578636169, Reg Loss=3.6595680713653564
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=14.761948412656784
Loss made of: CE 0.8024184107780457, LKD 4.06998348236084, LDE 0.0, LReg 0.0, POD 9.786554336547852 EntMin 0.0
Epoch 4, Class Loss=0.7512689828872681, Reg Loss=3.6207332611083984
Clinet index 4, End of Epoch 4/6, Average Loss=4.372002124786377, Class Loss=0.7512689828872681, Reg Loss=3.6207332611083984
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=14.592662799358369
Loss made of: CE 0.7578262686729431, LKD 4.117100715637207, LDE 0.0, LReg 0.0, POD 10.125839233398438 EntMin 0.0
Epoch 5, Class Loss=0.6931586861610413, Reg Loss=3.6656711101531982
Clinet index 4, End of Epoch 5/6, Average Loss=4.358829975128174, Class Loss=0.6931586861610413, Reg Loss=3.6656711101531982
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=14.049209666252136
Loss made of: CE 0.5414694547653198, LKD 3.0607070922851562, LDE 0.0, LReg 0.0, POD 9.130860328674316 EntMin 0.0
Epoch 6, Class Loss=0.635686993598938, Reg Loss=3.68257212638855
Clinet index 4, End of Epoch 6/6, Average Loss=4.318259239196777, Class Loss=0.635686993598938, Reg Loss=3.68257212638855
federated aggregation...
Validation, Class Loss=0.5519996285438538, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.837800
Mean Acc: 0.448342
FreqW Acc: 0.727457
Mean IoU: 0.334167
Class IoU:
	class 0: 0.86972266
	class 1: 0.72763073
	class 2: 0.2299398
	class 3: 0.39922392
	class 4: 0.5895769
	class 5: 0.13297807
	class 6: 0.45878127
	class 7: 0.43536982
	class 8: 0.40168437
	class 9: 0.0
	class 10: 0.09925985
	class 11: 0.0
	class 12: 0.0
Class Acc:
	class 0: 0.98054105
	class 1: 0.7468643
	class 2: 0.4221081
	class 3: 0.9444018
	class 4: 0.8445976
	class 5: 0.13498259
	class 6: 0.46378177
	class 7: 0.4536879
	class 8: 0.73806393
	class 9: 0.0
	class 10: 0.099421814
	class 11: 0.0
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[1, 13, 11, 12]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=22.490796744823456
Loss made of: CE 1.0952825546264648, LKD 6.386219024658203, LDE 0.0, LReg 0.0, POD 14.486068725585938 EntMin 0.0
Epoch 1, Batch 20/35, Loss=20.079446870088578
Loss made of: CE 0.9390244483947754, LKD 5.6168646812438965, LDE 0.0, LReg 0.0, POD 13.022857666015625 EntMin 0.0
Epoch 1, Batch 30/35, Loss=19.62436250448227
Loss made of: CE 0.9974716901779175, LKD 6.587653160095215, LDE 0.0, LReg 0.0, POD 13.628307342529297 EntMin 0.0
Epoch 1, Class Loss=1.0148574113845825, Reg Loss=5.742600917816162
Clinet index 1, End of Epoch 1/6, Average Loss=6.757458209991455, Class Loss=1.0148574113845825, Reg Loss=5.742600917816162
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/35, Loss=18.63747955560684
Loss made of: CE 0.6741786003112793, LKD 5.520987510681152, LDE 0.0, LReg 0.0, POD 12.132562637329102 EntMin 0.0
Epoch 2, Batch 20/35, Loss=18.879602468013765
Loss made of: CE 0.7552341818809509, LKD 5.222187519073486, LDE 0.0, LReg 0.0, POD 12.852901458740234 EntMin 0.0
Epoch 2, Batch 30/35, Loss=18.46975570321083
Loss made of: CE 0.5584571957588196, LKD 5.7939887046813965, LDE 0.0, LReg 0.0, POD 12.138590812683105 EntMin 0.0
Epoch 2, Class Loss=0.6913582682609558, Reg Loss=5.662581443786621
Clinet index 1, End of Epoch 2/6, Average Loss=6.353939533233643, Class Loss=0.6913582682609558, Reg Loss=5.662581443786621
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/35, Loss=17.988657778501512
Loss made of: CE 0.6306584477424622, LKD 6.3338141441345215, LDE 0.0, LReg 0.0, POD 11.479984283447266 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 20/35, Loss=18.16571690440178
Loss made of: CE 0.6340250372886658, LKD 5.925656795501709, LDE 0.0, LReg 0.0, POD 11.719606399536133 EntMin 0.0
Epoch 3, Batch 30/35, Loss=17.997909593582154
Loss made of: CE 0.6866075992584229, LKD 6.004546165466309, LDE 0.0, LReg 0.0, POD 13.380952835083008 EntMin 0.0
Epoch 3, Class Loss=0.6261398792266846, Reg Loss=5.605445384979248
Clinet index 1, End of Epoch 3/6, Average Loss=6.231585502624512, Class Loss=0.6261398792266846, Reg Loss=5.605445384979248
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/35, Loss=18.30575325489044
Loss made of: CE 0.6347789168357849, LKD 5.183799743652344, LDE 0.0, LReg 0.0, POD 12.947787284851074 EntMin 0.0
Epoch 4, Batch 20/35, Loss=17.229271018505095
Loss made of: CE 0.5491276979446411, LKD 4.554540157318115, LDE 0.0, LReg 0.0, POD 12.189868927001953 EntMin 0.0
Epoch 4, Batch 30/35, Loss=17.759270656108857
Loss made of: CE 0.622529149055481, LKD 5.1724958419799805, LDE 0.0, LReg 0.0, POD 12.401735305786133 EntMin 0.0
Epoch 4, Class Loss=0.6131065487861633, Reg Loss=5.611015796661377
Clinet index 1, End of Epoch 4/6, Average Loss=6.224122524261475, Class Loss=0.6131065487861633, Reg Loss=5.611015796661377
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/35, Loss=17.52532599568367
Loss made of: CE 0.543822169303894, LKD 5.473506927490234, LDE 0.0, LReg 0.0, POD 11.184150695800781 EntMin 0.0
Epoch 5, Batch 20/35, Loss=17.75310198664665
Loss made of: CE 0.6184675693511963, LKD 6.266139030456543, LDE 0.0, LReg 0.0, POD 11.600202560424805 EntMin 0.0
Epoch 5, Batch 30/35, Loss=16.766340017318726
Loss made of: CE 0.5324044227600098, LKD 5.1629791259765625, LDE 0.0, LReg 0.0, POD 10.875094413757324 EntMin 0.0
Epoch 5, Class Loss=0.5890390872955322, Reg Loss=5.604833126068115
Clinet index 1, End of Epoch 5/6, Average Loss=6.193872451782227, Class Loss=0.5890390872955322, Reg Loss=5.604833126068115
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/35, Loss=16.819089937210084
Loss made of: CE 0.4935489594936371, LKD 5.169591426849365, LDE 0.0, LReg 0.0, POD 10.713130950927734 EntMin 0.0
Epoch 6, Batch 20/35, Loss=17.686686956882475
Loss made of: CE 0.5324350595474243, LKD 6.003407955169678, LDE 0.0, LReg 0.0, POD 11.118346214294434 EntMin 0.0
Epoch 6, Batch 30/35, Loss=17.78244975209236
Loss made of: CE 0.6746411323547363, LKD 5.631975173950195, LDE 0.0, LReg 0.0, POD 10.73533821105957 EntMin 0.0
Epoch 6, Class Loss=0.5902071595191956, Reg Loss=5.6577534675598145
Clinet index 1, End of Epoch 6/6, Average Loss=6.247960567474365, Class Loss=0.5902071595191956, Reg Loss=5.6577534675598145
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=14.063136106729507
Loss made of: CE 0.6029684543609619, LKD 3.205476760864258, LDE 0.0, LReg 0.0, POD 9.315897941589355 EntMin 0.0
Epoch 1, Class Loss=0.7061344981193542, Reg Loss=3.695643424987793
Clinet index 13, End of Epoch 1/6, Average Loss=4.401777744293213, Class Loss=0.7061344981193542, Reg Loss=3.695643424987793
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/19, Loss=14.350547677278518
Loss made of: CE 0.606316089630127, LKD 3.7711782455444336, LDE 0.0, LReg 0.0, POD 10.832389831542969 EntMin 0.0
Epoch 2, Class Loss=0.6459583640098572, Reg Loss=3.6939399242401123
Clinet index 13, End of Epoch 2/6, Average Loss=4.339898109436035, Class Loss=0.6459583640098572, Reg Loss=3.6939399242401123
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/19, Loss=14.031243199110031
Loss made of: CE 0.5330652594566345, LKD 3.4688496589660645, LDE 0.0, LReg 0.0, POD 9.542518615722656 EntMin 0.0
Epoch 3, Class Loss=0.6050530672073364, Reg Loss=3.7755167484283447
Clinet index 13, End of Epoch 3/6, Average Loss=4.380569934844971, Class Loss=0.6050530672073364, Reg Loss=3.7755167484283447
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/19, Loss=13.740525794029235
Loss made of: CE 0.640134334564209, LKD 3.2233569622039795, LDE 0.0, LReg 0.0, POD 10.21008586883545 EntMin 0.0
Epoch 4, Class Loss=0.562612771987915, Reg Loss=3.6672563552856445
Clinet index 13, End of Epoch 4/6, Average Loss=4.2298688888549805, Class Loss=0.562612771987915, Reg Loss=3.6672563552856445
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/19, Loss=13.953078517317772
Loss made of: CE 0.5174365043640137, LKD 3.808436155319214, LDE 0.0, LReg 0.0, POD 8.564078330993652 EntMin 0.0
Epoch 5, Class Loss=0.5493069291114807, Reg Loss=3.672595500946045
Clinet index 13, End of Epoch 5/6, Average Loss=4.221902370452881, Class Loss=0.5493069291114807, Reg Loss=3.672595500946045
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/19, Loss=13.219118675589561
Loss made of: CE 0.5235517024993896, LKD 3.7238645553588867, LDE 0.0, LReg 0.0, POD 9.263130187988281 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Class Loss=0.5368242859840393, Reg Loss=3.6519274711608887
Clinet index 13, End of Epoch 6/6, Average Loss=4.188751697540283, Class Loss=0.5368242859840393, Reg Loss=3.6519274711608887
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/33, Loss=14.60071673989296
Loss made of: CE 0.7419430613517761, LKD 3.0510010719299316, LDE 0.0, LReg 0.0, POD 10.385356903076172 EntMin 0.0
Epoch 1, Batch 20/33, Loss=14.245045101642608
Loss made of: CE 0.7181390523910522, LKD 4.608795166015625, LDE 0.0, LReg 0.0, POD 9.398752212524414 EntMin 0.0
Epoch 1, Batch 30/33, Loss=14.624448627233505
Loss made of: CE 0.5804139971733093, LKD 3.990023374557495, LDE 0.0, LReg 0.0, POD 9.194759368896484 EntMin 0.0
Epoch 1, Class Loss=0.6889793276786804, Reg Loss=3.7853217124938965
Clinet index 11, End of Epoch 1/6, Average Loss=4.474300861358643, Class Loss=0.6889793276786804, Reg Loss=3.7853217124938965
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=14.076490134000778
Loss made of: CE 0.7723905444145203, LKD 3.399714946746826, LDE 0.0, LReg 0.0, POD 10.77827262878418 EntMin 0.0
Epoch 2, Batch 20/33, Loss=13.871973645687103
Loss made of: CE 0.6015068292617798, LKD 3.8004531860351562, LDE 0.0, LReg 0.0, POD 9.14505386352539 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 30/33, Loss=14.0343585729599
Loss made of: CE 0.5754537582397461, LKD 3.7959258556365967, LDE 0.0, LReg 0.0, POD 9.908218383789062 EntMin 0.0
Epoch 2, Class Loss=0.6126552820205688, Reg Loss=3.71640682220459
Clinet index 11, End of Epoch 2/6, Average Loss=4.329061985015869, Class Loss=0.6126552820205688, Reg Loss=3.71640682220459
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=13.782076966762542
Loss made of: CE 0.540978729724884, LKD 4.307831764221191, LDE 0.0, LReg 0.0, POD 8.90511703491211 EntMin 0.0
Epoch 3, Batch 20/33, Loss=13.90085466504097
Loss made of: CE 0.5884515047073364, LKD 3.2208728790283203, LDE 0.0, LReg 0.0, POD 10.575002670288086 EntMin 0.0
Epoch 3, Batch 30/33, Loss=13.589791762828828
Loss made of: CE 0.6696900129318237, LKD 3.1144814491271973, LDE 0.0, LReg 0.0, POD 9.803564071655273 EntMin 0.0
Epoch 3, Class Loss=0.5728292465209961, Reg Loss=3.6840665340423584
Clinet index 11, End of Epoch 3/6, Average Loss=4.256896018981934, Class Loss=0.5728292465209961, Reg Loss=3.6840665340423584
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=13.565492814779281
Loss made of: CE 0.534315824508667, LKD 4.1181864738464355, LDE 0.0, LReg 0.0, POD 8.635519981384277 EntMin 0.0
Epoch 4, Batch 20/33, Loss=13.66560282111168
Loss made of: CE 0.4752344787120819, LKD 3.908792018890381, LDE 0.0, LReg 0.0, POD 9.004470825195312 EntMin 0.0
Epoch 4, Batch 30/33, Loss=13.638142120838165
Loss made of: CE 0.6510312557220459, LKD 4.0060272216796875, LDE 0.0, LReg 0.0, POD 8.254400253295898 EntMin 0.0
Epoch 4, Class Loss=0.5436095595359802, Reg Loss=3.672503709793091
Clinet index 11, End of Epoch 4/6, Average Loss=4.216113090515137, Class Loss=0.5436095595359802, Reg Loss=3.672503709793091
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=13.301574435830116
Loss made of: CE 0.48644840717315674, LKD 3.6048645973205566, LDE 0.0, LReg 0.0, POD 8.692893981933594 EntMin 0.0
Epoch 5, Batch 20/33, Loss=13.59016231894493
Loss made of: CE 0.48384666442871094, LKD 3.663670301437378, LDE 0.0, LReg 0.0, POD 8.908275604248047 EntMin 0.0
Epoch 5, Batch 30/33, Loss=13.264528101682663
Loss made of: CE 0.6181809306144714, LKD 3.8879096508026123, LDE 0.0, LReg 0.0, POD 8.868849754333496 EntMin 0.0
Epoch 5, Class Loss=0.5315495729446411, Reg Loss=3.6902968883514404
Clinet index 11, End of Epoch 5/6, Average Loss=4.221846580505371, Class Loss=0.5315495729446411, Reg Loss=3.6902968883514404
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=12.943235251307488
Loss made of: CE 0.4405669867992401, LKD 3.329888343811035, LDE 0.0, LReg 0.0, POD 8.671178817749023 EntMin 0.0
Epoch 6, Batch 20/33, Loss=12.907768073678017
Loss made of: CE 0.4965066909790039, LKD 3.4999594688415527, LDE 0.0, LReg 0.0, POD 7.756341457366943 EntMin 0.0
Epoch 6, Batch 30/33, Loss=12.901913866400719
Loss made of: CE 0.4421676993370056, LKD 3.034931182861328, LDE 0.0, LReg 0.0, POD 9.506695747375488 EntMin 0.0
Epoch 6, Class Loss=0.5157656669616699, Reg Loss=3.616440534591675
Clinet index 11, End of Epoch 6/6, Average Loss=4.132205963134766, Class Loss=0.5157656669616699, Reg Loss=3.616440534591675
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/33, Loss=14.618734470009803
Loss made of: CE 0.7515664100646973, LKD 3.602015256881714, LDE 0.0, LReg 0.0, POD 11.059823989868164 EntMin 0.0
Epoch 1, Batch 20/33, Loss=14.244226813316345
Loss made of: CE 0.8052409887313843, LKD 3.9636874198913574, LDE 0.0, LReg 0.0, POD 10.331045150756836 EntMin 0.0
Epoch 1, Batch 30/33, Loss=13.877564227581024
Loss made of: CE 0.5001051425933838, LKD 3.5680599212646484, LDE 0.0, LReg 0.0, POD 10.129697799682617 EntMin 0.0
Epoch 1, Class Loss=0.687372088432312, Reg Loss=3.702312707901001
Clinet index 12, End of Epoch 1/6, Average Loss=4.389684677124023, Class Loss=0.687372088432312, Reg Loss=3.702312707901001
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=14.054902455210685
Loss made of: CE 0.4849793612957001, LKD 3.590817928314209, LDE 0.0, LReg 0.0, POD 9.976332664489746 EntMin 0.0
Epoch 2, Batch 20/33, Loss=14.018199861049652
Loss made of: CE 0.8314952254295349, LKD 4.1623969078063965, LDE 0.0, LReg 0.0, POD 10.154051780700684 EntMin 0.0
Epoch 2, Batch 30/33, Loss=13.54272589981556
Loss made of: CE 0.5726041793823242, LKD 4.267402648925781, LDE 0.0, LReg 0.0, POD 8.511017799377441 EntMin 0.0
Epoch 2, Class Loss=0.617567777633667, Reg Loss=3.6910438537597656
Clinet index 12, End of Epoch 2/6, Average Loss=4.308611869812012, Class Loss=0.617567777633667, Reg Loss=3.6910438537597656
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=13.637036058306695
Loss made of: CE 0.6415906548500061, LKD 3.336031913757324, LDE 0.0, LReg 0.0, POD 8.852071762084961 EntMin 0.0
Epoch 3, Batch 20/33, Loss=13.707199123501777
Loss made of: CE 0.5661015510559082, LKD 4.066532135009766, LDE 0.0, LReg 0.0, POD 9.283794403076172 EntMin 0.0
Epoch 3, Batch 30/33, Loss=13.327053666114807
Loss made of: CE 0.622262716293335, LKD 3.918598175048828, LDE 0.0, LReg 0.0, POD 9.109801292419434 EntMin 0.0
Epoch 3, Class Loss=0.5689126253128052, Reg Loss=3.6380224227905273
Clinet index 12, End of Epoch 3/6, Average Loss=4.206934928894043, Class Loss=0.5689126253128052, Reg Loss=3.6380224227905273
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=13.808648321032525
Loss made of: CE 0.4817984998226166, LKD 3.1344962120056152, LDE 0.0, LReg 0.0, POD 9.08077621459961 EntMin 0.0
Epoch 4, Batch 20/33, Loss=13.441037356853485
Loss made of: CE 0.5223701000213623, LKD 4.191841125488281, LDE 0.0, LReg 0.0, POD 8.863592147827148 EntMin 0.0
Epoch 4, Batch 30/33, Loss=12.848756504058837
Loss made of: CE 0.5640379190444946, LKD 3.0344114303588867, LDE 0.0, LReg 0.0, POD 9.806640625 EntMin 0.0
Epoch 4, Class Loss=0.5425142645835876, Reg Loss=3.6890504360198975
Clinet index 12, End of Epoch 4/6, Average Loss=4.231564521789551, Class Loss=0.5425142645835876, Reg Loss=3.6890504360198975
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=13.123745262622833
Loss made of: CE 0.5107629299163818, LKD 4.244232654571533, LDE 0.0, LReg 0.0, POD 7.906121253967285 EntMin 0.0
Epoch 5, Batch 20/33, Loss=12.948673328757286
Loss made of: CE 0.6488445997238159, LKD 4.032812118530273, LDE 0.0, LReg 0.0, POD 8.672886848449707 EntMin 0.0
Epoch 5, Batch 30/33, Loss=13.029491525888442
Loss made of: CE 0.610986590385437, LKD 4.360037803649902, LDE 0.0, LReg 0.0, POD 9.398274421691895 EntMin 0.0
Epoch 5, Class Loss=0.5237282514572144, Reg Loss=3.665168523788452
Clinet index 12, End of Epoch 5/6, Average Loss=4.188896656036377, Class Loss=0.5237282514572144, Reg Loss=3.665168523788452
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=13.394641980528831
Loss made of: CE 0.6846939325332642, LKD 4.673027992248535, LDE 0.0, LReg 0.0, POD 10.043169975280762 EntMin 0.0
Epoch 6, Batch 20/33, Loss=13.100001394748688
Loss made of: CE 0.4784952998161316, LKD 4.224160194396973, LDE 0.0, LReg 0.0, POD 8.891668319702148 EntMin 0.0
Epoch 6, Batch 30/33, Loss=13.15896729528904
Loss made of: CE 0.5132133960723877, LKD 4.41079568862915, LDE 0.0, LReg 0.0, POD 8.317410469055176 EntMin 0.0
Epoch 6, Class Loss=0.5181559920310974, Reg Loss=3.6746840476989746
Clinet index 12, End of Epoch 6/6, Average Loss=4.192840099334717, Class Loss=0.5181559920310974, Reg Loss=3.6746840476989746
federated aggregation...
Validation, Class Loss=0.4908168613910675, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.853123
Mean Acc: 0.512257
FreqW Acc: 0.754822
Mean IoU: 0.388615
Class IoU:
	class 0: 0.88461465
	class 1: 0.72834444
	class 2: 0.26950654
	class 3: 0.4296198
	class 4: 0.5663205
	class 5: 0.13309243
	class 6: 0.618186
	class 7: 0.5562115
	class 8: 0.41062504
	class 9: 0.0
	class 10: 0.4409702
	class 11: 0.014509936
	class 12: 0.0
Class Acc:
	class 0: 0.97599053
	class 1: 0.7418867
	class 2: 0.5169065
	class 3: 0.93933123
	class 4: 0.8683025
	class 5: 0.1346428
	class 6: 0.63018554
	class 7: 0.6063186
	class 8: 0.7811609
	class 9: 0.0
	class 10: 0.45007423
	class 11: 0.014540666
	class 12: 0.0

federated global round: 12, step: 2
select part of clients to conduct local training
[0, 16, 7, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=13.470505386590958
Loss made of: CE 0.6109553575515747, LKD 3.32954478263855, LDE 0.0, LReg 0.0, POD 9.325248718261719 EntMin 0.0
Epoch 1, Class Loss=0.6227870583534241, Reg Loss=3.6491236686706543
Clinet index 0, End of Epoch 1/6, Average Loss=4.271910667419434, Class Loss=0.6227870583534241, Reg Loss=3.6491236686706543
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=13.198221808671951
Loss made of: CE 0.5959553718566895, LKD 3.1888835430145264, LDE 0.0, LReg 0.0, POD 9.086225509643555 EntMin 0.0
Epoch 2, Class Loss=0.5572414994239807, Reg Loss=3.6931376457214355
Clinet index 0, End of Epoch 2/6, Average Loss=4.2503790855407715, Class Loss=0.5572414994239807, Reg Loss=3.6931376457214355
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=13.612983918190002
Loss made of: CE 0.6337258815765381, LKD 3.935232162475586, LDE 0.0, LReg 0.0, POD 8.576681137084961 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.5400599241256714, Reg Loss=3.7289538383483887
Clinet index 0, End of Epoch 3/6, Average Loss=4.26901388168335, Class Loss=0.5400599241256714, Reg Loss=3.7289538383483887
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=12.87587475478649
Loss made of: CE 0.5728790760040283, LKD 3.3728346824645996, LDE 0.0, LReg 0.0, POD 9.447830200195312 EntMin 0.0
Epoch 4, Class Loss=0.5141815543174744, Reg Loss=3.687532901763916
Clinet index 0, End of Epoch 4/6, Average Loss=4.201714515686035, Class Loss=0.5141815543174744, Reg Loss=3.687532901763916
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=13.136637589335441
Loss made of: CE 0.516303300857544, LKD 3.519568920135498, LDE 0.0, LReg 0.0, POD 9.081842422485352 EntMin 0.0
Epoch 5, Class Loss=0.5048102140426636, Reg Loss=3.6358189582824707
Clinet index 0, End of Epoch 5/6, Average Loss=4.140629291534424, Class Loss=0.5048102140426636, Reg Loss=3.6358189582824707
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=12.838457158207893
Loss made of: CE 0.44640588760375977, LKD 2.833515167236328, LDE 0.0, LReg 0.0, POD 8.15189266204834 EntMin 0.0
Epoch 6, Class Loss=0.49704864621162415, Reg Loss=3.736523389816284
Clinet index 0, End of Epoch 6/6, Average Loss=4.233572006225586, Class Loss=0.49704864621162415, Reg Loss=3.736523389816284
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/42, Loss=17.728420162200926
Loss made of: CE 0.9810437560081482, LKD 5.821136474609375, LDE 0.0, LReg 0.0, POD 12.46817398071289 EntMin 0.0
Epoch 1, Batch 20/42, Loss=17.303876054286956
Loss made of: CE 0.6667923331260681, LKD 4.517972946166992, LDE 0.0, LReg 0.0, POD 10.528217315673828 EntMin 0.0
Epoch 1, Batch 30/42, Loss=17.425176352262497
Loss made of: CE 0.81268709897995, LKD 5.231164455413818, LDE 0.0, LReg 0.0, POD 11.14992904663086 EntMin 0.0
Epoch 1, Batch 40/42, Loss=16.443076997995377
Loss made of: CE 0.5988632440567017, LKD 5.513252258300781, LDE 0.0, LReg 0.0, POD 9.936805725097656 EntMin 0.0
Epoch 1, Class Loss=0.8508762121200562, Reg Loss=5.101053714752197
Clinet index 16, End of Epoch 1/6, Average Loss=5.951930046081543, Class Loss=0.8508762121200562, Reg Loss=5.101053714752197
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/42, Loss=17.3700867831707
Loss made of: CE 0.5860037803649902, LKD 5.316924571990967, LDE 0.0, LReg 0.0, POD 11.276323318481445 EntMin 0.0
Epoch 2, Batch 20/42, Loss=16.934218233823778
Loss made of: CE 0.6057698130607605, LKD 5.428780555725098, LDE 0.0, LReg 0.0, POD 12.049083709716797 EntMin 0.0
Epoch 2, Batch 30/42, Loss=16.13100648224354
Loss made of: CE 0.48435744643211365, LKD 5.241271495819092, LDE 0.0, LReg 0.0, POD 11.514324188232422 EntMin 0.0
Epoch 2, Batch 40/42, Loss=16.37734996676445
Loss made of: CE 0.5725307464599609, LKD 4.440564155578613, LDE 0.0, LReg 0.0, POD 10.18385124206543 EntMin 0.0
Epoch 2, Class Loss=0.6166048645973206, Reg Loss=5.097550868988037
Clinet index 16, End of Epoch 2/6, Average Loss=5.714155673980713, Class Loss=0.6166048645973206, Reg Loss=5.097550868988037
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/42, Loss=16.459418910741807
Loss made of: CE 0.6510196924209595, LKD 4.994694709777832, LDE 0.0, LReg 0.0, POD 10.176419258117676 EntMin 0.0
Epoch 3, Batch 20/42, Loss=17.20318940281868
Loss made of: CE 0.526136577129364, LKD 6.899331569671631, LDE 0.0, LReg 0.0, POD 12.061470031738281 EntMin 0.0
Epoch 3, Batch 30/42, Loss=16.094498121738432
Loss made of: CE 0.5543429851531982, LKD 5.362773895263672, LDE 0.0, LReg 0.0, POD 10.705452919006348 EntMin 0.0
Epoch 3, Batch 40/42, Loss=16.294712913036346
Loss made of: CE 0.5612952709197998, LKD 5.655673503875732, LDE 0.0, LReg 0.0, POD 11.6199312210083 EntMin 0.0
Epoch 3, Class Loss=0.5961652398109436, Reg Loss=5.090915679931641
Clinet index 16, End of Epoch 3/6, Average Loss=5.6870808601379395, Class Loss=0.5961652398109436, Reg Loss=5.090915679931641
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/42, Loss=16.49027047753334
Loss made of: CE 0.5185620188713074, LKD 4.348139762878418, LDE 0.0, LReg 0.0, POD 10.37346076965332 EntMin 0.0
Epoch 4, Batch 20/42, Loss=16.438865488767625
Loss made of: CE 0.5500950217247009, LKD 5.257692813873291, LDE 0.0, LReg 0.0, POD 10.550824165344238 EntMin 0.0
Epoch 4, Batch 30/42, Loss=16.10583481490612
Loss made of: CE 0.5486139059066772, LKD 4.622801780700684, LDE 0.0, LReg 0.0, POD 9.371567726135254 EntMin 0.0
Epoch 4, Batch 40/42, Loss=16.259768906235696
Loss made of: CE 0.5450019836425781, LKD 4.644939422607422, LDE 0.0, LReg 0.0, POD 10.325678825378418 EntMin 0.0
Epoch 4, Class Loss=0.576836347579956, Reg Loss=5.082146644592285
Clinet index 16, End of Epoch 4/6, Average Loss=5.65898323059082, Class Loss=0.576836347579956, Reg Loss=5.082146644592285
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/42, Loss=16.084333455562593
Loss made of: CE 0.5294743776321411, LKD 5.280786514282227, LDE 0.0, LReg 0.0, POD 11.212757110595703 EntMin 0.0
Epoch 5, Batch 20/42, Loss=15.891810089349747
Loss made of: CE 0.4667370021343231, LKD 4.524649620056152, LDE 0.0, LReg 0.0, POD 9.935121536254883 EntMin 0.0
Epoch 5, Batch 30/42, Loss=16.112071999907492
Loss made of: CE 0.61275315284729, LKD 4.813802719116211, LDE 0.0, LReg 0.0, POD 10.413816452026367 EntMin 0.0
Epoch 5, Batch 40/42, Loss=16.082135006785393
Loss made of: CE 0.6170308589935303, LKD 6.462270259857178, LDE 0.0, LReg 0.0, POD 10.528732299804688 EntMin 0.0
Epoch 5, Class Loss=0.5658254623413086, Reg Loss=5.0662641525268555
Clinet index 16, End of Epoch 5/6, Average Loss=5.632089614868164, Class Loss=0.5658254623413086, Reg Loss=5.0662641525268555
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/42, Loss=16.063371908664703
Loss made of: CE 0.4938224256038666, LKD 4.5884504318237305, LDE 0.0, LReg 0.0, POD 9.227214813232422 EntMin 0.0
Epoch 6, Batch 20/42, Loss=15.704505541920662
Loss made of: CE 0.5349732041358948, LKD 4.536923408508301, LDE 0.0, LReg 0.0, POD 9.913068771362305 EntMin 0.0
Epoch 6, Batch 30/42, Loss=16.18622152209282
Loss made of: CE 0.6476330757141113, LKD 4.9080939292907715, LDE 0.0, LReg 0.0, POD 9.826070785522461 EntMin 0.0
Epoch 6, Batch 40/42, Loss=16.48734555840492
Loss made of: CE 0.6234695911407471, LKD 4.8849077224731445, LDE 0.0, LReg 0.0, POD 10.74022102355957 EntMin 0.0
Epoch 6, Class Loss=0.5616744160652161, Reg Loss=5.097152233123779
Clinet index 16, End of Epoch 6/6, Average Loss=5.65882682800293, Class Loss=0.5616744160652161, Reg Loss=5.097152233123779
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/31, Loss=15.08835574388504
Loss made of: CE 0.7177795171737671, LKD 3.248859405517578, LDE 0.0, LReg 0.0, POD 11.539802551269531 EntMin 0.0
Epoch 1, Batch 20/31, Loss=14.12455061674118
Loss made of: CE 0.6524120569229126, LKD 4.332631587982178, LDE 0.0, LReg 0.0, POD 10.556053161621094 EntMin 0.0
Epoch 1, Batch 30/31, Loss=14.055886131525039
Loss made of: CE 0.5710046291351318, LKD 2.861212730407715, LDE 0.0, LReg 0.0, POD 9.477222442626953 EntMin 0.0
Epoch 1, Class Loss=0.6300985813140869, Reg Loss=3.5887956619262695
Clinet index 7, End of Epoch 1/6, Average Loss=4.218894004821777, Class Loss=0.6300985813140869, Reg Loss=3.5887956619262695
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/31, Loss=13.567617425322533
Loss made of: CE 0.5369539260864258, LKD 3.0276503562927246, LDE 0.0, LReg 0.0, POD 8.914910316467285 EntMin 0.0
Epoch 2, Batch 20/31, Loss=12.900466540455819
Loss made of: CE 0.5967294573783875, LKD 3.6382617950439453, LDE 0.0, LReg 0.0, POD 8.920095443725586 EntMin 0.0
Epoch 2, Batch 30/31, Loss=13.815548419952393
Loss made of: CE 0.4771200120449066, LKD 3.1191487312316895, LDE 0.0, LReg 0.0, POD 8.728782653808594 EntMin 0.0
Epoch 2, Class Loss=0.5489819645881653, Reg Loss=3.449357748031616
Clinet index 7, End of Epoch 2/6, Average Loss=3.9983396530151367, Class Loss=0.5489819645881653, Reg Loss=3.449357748031616
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/31, Loss=13.162731462717057
Loss made of: CE 0.6101180911064148, LKD 4.0348944664001465, LDE 0.0, LReg 0.0, POD 8.96933650970459 EntMin 0.0
Epoch 3, Batch 20/31, Loss=13.031698945164681
Loss made of: CE 0.5614656209945679, LKD 3.6836838722229004, LDE 0.0, LReg 0.0, POD 8.70238971710205 EntMin 0.0
Epoch 3, Batch 30/31, Loss=12.86651791036129
Loss made of: CE 0.38590192794799805, LKD 3.9249653816223145, LDE 0.0, LReg 0.0, POD 8.152599334716797 EntMin 0.0
Epoch 3, Class Loss=0.5192688703536987, Reg Loss=3.509329319000244
Clinet index 7, End of Epoch 3/6, Average Loss=4.028598308563232, Class Loss=0.5192688703536987, Reg Loss=3.509329319000244
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/31, Loss=13.404849261045456
Loss made of: CE 0.6345679759979248, LKD 3.65109920501709, LDE 0.0, LReg 0.0, POD 10.078157424926758 EntMin 0.0
Epoch 4, Batch 20/31, Loss=12.722097119688987
Loss made of: CE 0.6168891191482544, LKD 3.442824363708496, LDE 0.0, LReg 0.0, POD 8.764002799987793 EntMin 0.0
Epoch 4, Batch 30/31, Loss=12.74082179069519
Loss made of: CE 0.6605727076530457, LKD 3.254453182220459, LDE 0.0, LReg 0.0, POD 10.667604446411133 EntMin 0.0
Epoch 4, Class Loss=0.5137624144554138, Reg Loss=3.5040814876556396
Clinet index 7, End of Epoch 4/6, Average Loss=4.017843723297119, Class Loss=0.5137624144554138, Reg Loss=3.5040814876556396
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/31, Loss=12.853337001800536
Loss made of: CE 0.45821046829223633, LKD 3.2774906158447266, LDE 0.0, LReg 0.0, POD 8.51646614074707 EntMin 0.0
Epoch 5, Batch 20/31, Loss=12.346375727653504
Loss made of: CE 0.5012273192405701, LKD 3.4996089935302734, LDE 0.0, LReg 0.0, POD 8.305904388427734 EntMin 0.0
Epoch 5, Batch 30/31, Loss=13.291365045309067
Loss made of: CE 0.46503275632858276, LKD 4.1219096183776855, LDE 0.0, LReg 0.0, POD 7.969603538513184 EntMin 0.0
Epoch 5, Class Loss=0.4931836724281311, Reg Loss=3.487464189529419
Clinet index 7, End of Epoch 5/6, Average Loss=3.9806478023529053, Class Loss=0.4931836724281311, Reg Loss=3.487464189529419
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/31, Loss=12.463586550951003
Loss made of: CE 0.6210634708404541, LKD 3.799494504928589, LDE 0.0, LReg 0.0, POD 8.605009078979492 EntMin 0.0
Epoch 6, Batch 20/31, Loss=12.860679399967193
Loss made of: CE 0.4926113486289978, LKD 3.774318218231201, LDE 0.0, LReg 0.0, POD 9.593132019042969 EntMin 0.0
Epoch 6, Batch 30/31, Loss=12.47976839542389
Loss made of: CE 0.5999995470046997, LKD 3.726996898651123, LDE 0.0, LReg 0.0, POD 8.36562728881836 EntMin 0.0
Epoch 6, Class Loss=0.4883405864238739, Reg Loss=3.4959728717803955
Clinet index 7, End of Epoch 6/6, Average Loss=3.984313488006592, Class Loss=0.4883405864238739, Reg Loss=3.4959728717803955
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=13.444280314445496
Loss made of: CE 0.6355181336402893, LKD 3.7290868759155273, LDE 0.0, LReg 0.0, POD 9.034905433654785 EntMin 0.0
Epoch 1, Class Loss=0.6362341046333313, Reg Loss=3.596217155456543
Clinet index 4, End of Epoch 1/6, Average Loss=4.232451438903809, Class Loss=0.6362341046333313, Reg Loss=3.596217155456543
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=13.568585821986199
Loss made of: CE 0.5301183462142944, LKD 3.554757595062256, LDE 0.0, LReg 0.0, POD 9.092002868652344 EntMin 0.0
Epoch 2, Class Loss=0.5656787753105164, Reg Loss=3.653078556060791
Clinet index 4, End of Epoch 2/6, Average Loss=4.218757152557373, Class Loss=0.5656787753105164, Reg Loss=3.653078556060791
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=13.003186470270157
Loss made of: CE 0.4512270390987396, LKD 3.8891773223876953, LDE 0.0, LReg 0.0, POD 7.959184169769287 EntMin 0.0
Epoch 3, Class Loss=0.5284913182258606, Reg Loss=3.5363409519195557
Clinet index 4, End of Epoch 3/6, Average Loss=4.0648322105407715, Class Loss=0.5284913182258606, Reg Loss=3.5363409519195557
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=12.739140877127648
Loss made of: CE 0.5212734937667847, LKD 3.594011068344116, LDE 0.0, LReg 0.0, POD 8.375663757324219 EntMin 0.0
Epoch 4, Class Loss=0.5091466307640076, Reg Loss=3.5634329319000244
Clinet index 4, End of Epoch 4/6, Average Loss=4.072579383850098, Class Loss=0.5091466307640076, Reg Loss=3.5634329319000244
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=12.944013679027558
Loss made of: CE 0.5066765546798706, LKD 3.899106502532959, LDE 0.0, LReg 0.0, POD 8.814874649047852 EntMin 0.0
Epoch 5, Class Loss=0.4968320429325104, Reg Loss=3.588524580001831
Clinet index 4, End of Epoch 5/6, Average Loss=4.085356712341309, Class Loss=0.4968320429325104, Reg Loss=3.588524580001831
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=12.689485701918603
Loss made of: CE 0.41016197204589844, LKD 3.1147494316101074, LDE 0.0, LReg 0.0, POD 7.826353549957275 EntMin 0.0
Epoch 6, Class Loss=0.48889079689979553, Reg Loss=3.634378671646118
Clinet index 4, End of Epoch 6/6, Average Loss=4.123269557952881, Class Loss=0.48889079689979553, Reg Loss=3.634378671646118
federated aggregation...
Validation, Class Loss=0.4866291880607605, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.850239
Mean Acc: 0.526213
FreqW Acc: 0.758024
Mean IoU: 0.386097
Class IoU:
	class 0: 0.8910876
	class 1: 0.7256013
	class 2: 0.28293082
	class 3: 0.42641342
	class 4: 0.5626431
	class 5: 0.097812
	class 6: 0.638796
	class 7: 0.5826779
	class 8: 0.38821515
	class 9: 0.00043910363
	class 10: 0.14178875
	class 11: 0.2808546
	class 12: 0.0
Class Acc:
	class 0: 0.96888644
	class 1: 0.7383757
	class 2: 0.5642254
	class 3: 0.93539256
	class 4: 0.87830126
	class 5: 0.098167785
	class 6: 0.65198815
	class 7: 0.6444367
	class 8: 0.7888805
	class 9: 0.00043910404
	class 10: 0.14208142
	class 11: 0.4295923
	class 12: 0.0

federated global round: 13, step: 2
select part of clients to conduct local training
[14, 13, 0, 1]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=12.910796421766282
Loss made of: CE 0.5330665707588196, LKD 3.839010715484619, LDE 0.0, LReg 0.0, POD 8.404052734375 EntMin 0.0
Epoch 1, Class Loss=0.5439977049827576, Reg Loss=3.6288182735443115
Clinet index 14, End of Epoch 1/6, Average Loss=4.172815799713135, Class Loss=0.5439977049827576, Reg Loss=3.6288182735443115
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=13.202550840377807
Loss made of: CE 0.6116594076156616, LKD 4.734139919281006, LDE 0.0, LReg 0.0, POD 9.247373580932617 EntMin 0.0
Epoch 2, Class Loss=0.4946993291378021, Reg Loss=3.707639694213867
Clinet index 14, End of Epoch 2/6, Average Loss=4.202339172363281, Class Loss=0.4946993291378021, Reg Loss=3.707639694213867
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=13.195925134420396
Loss made of: CE 0.3630216717720032, LKD 3.185412883758545, LDE 0.0, LReg 0.0, POD 8.86191463470459 EntMin 0.0
Epoch 3, Class Loss=0.49073275923728943, Reg Loss=3.619229316711426
Clinet index 14, End of Epoch 3/6, Average Loss=4.109961986541748, Class Loss=0.49073275923728943, Reg Loss=3.619229316711426
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 10/19, Loss=12.86290667951107
Loss made of: CE 0.4981352686882019, LKD 4.025384902954102, LDE 0.0, LReg 0.0, POD 8.98286247253418 EntMin 0.0
Epoch 4, Class Loss=0.4820321202278137, Reg Loss=3.727003335952759
Clinet index 14, End of Epoch 4/6, Average Loss=4.209035396575928, Class Loss=0.4820321202278137, Reg Loss=3.727003335952759
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=13.085233119130134
Loss made of: CE 0.4255097806453705, LKD 3.365243673324585, LDE 0.0, LReg 0.0, POD 8.099791526794434 EntMin 0.0
Epoch 5, Class Loss=0.47332432866096497, Reg Loss=3.617061138153076
Clinet index 14, End of Epoch 5/6, Average Loss=4.090385437011719, Class Loss=0.47332432866096497, Reg Loss=3.617061138153076
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=12.857823863625526
Loss made of: CE 0.3759312629699707, LKD 3.623033285140991, LDE 0.0, LReg 0.0, POD 8.12470817565918 EntMin 0.0
Epoch 6, Class Loss=0.4622717499732971, Reg Loss=3.599578857421875
Clinet index 14, End of Epoch 6/6, Average Loss=4.061850547790527, Class Loss=0.4622717499732971, Reg Loss=3.599578857421875
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=12.991362681984901
Loss made of: CE 0.49377188086509705, LKD 3.3400180339813232, LDE 0.0, LReg 0.0, POD 7.9636101722717285 EntMin 0.0
Epoch 1, Class Loss=0.5501716732978821, Reg Loss=3.662466287612915
Clinet index 13, End of Epoch 1/6, Average Loss=4.212637901306152, Class Loss=0.5501716732978821, Reg Loss=3.662466287612915
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/19, Loss=13.094299560785293
Loss made of: CE 0.4942809045314789, LKD 3.699998617172241, LDE 0.0, LReg 0.0, POD 9.256532669067383 EntMin 0.0
Epoch 2, Class Loss=0.5069082975387573, Reg Loss=3.6683523654937744
Clinet index 13, End of Epoch 2/6, Average Loss=4.175260543823242, Class Loss=0.5069082975387573, Reg Loss=3.6683523654937744
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/19, Loss=12.899463224411011
Loss made of: CE 0.41840776801109314, LKD 3.2276084423065186, LDE 0.0, LReg 0.0, POD 8.372940063476562 EntMin 0.0
Epoch 3, Class Loss=0.4960286021232605, Reg Loss=3.655707597732544
Clinet index 13, End of Epoch 3/6, Average Loss=4.151736259460449, Class Loss=0.4960286021232605, Reg Loss=3.655707597732544
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/19, Loss=12.53393729031086
Loss made of: CE 0.5316007137298584, LKD 3.369785785675049, LDE 0.0, LReg 0.0, POD 9.033560752868652 EntMin 0.0
Epoch 4, Class Loss=0.475523442029953, Reg Loss=3.6483664512634277
Clinet index 13, End of Epoch 4/6, Average Loss=4.123889923095703, Class Loss=0.475523442029953, Reg Loss=3.6483664512634277
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/19, Loss=12.948809629678726
Loss made of: CE 0.465451717376709, LKD 3.425795316696167, LDE 0.0, LReg 0.0, POD 7.577189922332764 EntMin 0.0
Epoch 5, Class Loss=0.47574174404144287, Reg Loss=3.614640951156616
Clinet index 13, End of Epoch 5/6, Average Loss=4.0903825759887695, Class Loss=0.47574174404144287, Reg Loss=3.614640951156616
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/19, Loss=12.420248445868491
Loss made of: CE 0.47080835700035095, LKD 3.697072982788086, LDE 0.0, LReg 0.0, POD 8.4606351852417 EntMin 0.0
Epoch 6, Class Loss=0.4721125066280365, Reg Loss=3.5927376747131348
Clinet index 13, End of Epoch 6/6, Average Loss=4.064850330352783, Class Loss=0.4721125066280365, Reg Loss=3.5927376747131348
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=12.882999294996262
Loss made of: CE 0.48403072357177734, LKD 3.5307958126068115, LDE 0.0, LReg 0.0, POD 8.260494232177734 EntMin 0.0
Epoch 1, Class Loss=0.5470127463340759, Reg Loss=3.660388708114624
Clinet index 0, End of Epoch 1/6, Average Loss=4.207401275634766, Class Loss=0.5470127463340759, Reg Loss=3.660388708114624
Pseudo labeling is: None
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/19, Loss=12.63658747971058
Loss made of: CE 0.579673171043396, LKD 2.9978694915771484, LDE 0.0, LReg 0.0, POD 8.542524337768555 EntMin 0.0
Epoch 2, Class Loss=0.5073873996734619, Reg Loss=3.6647555828094482
Clinet index 0, End of Epoch 2/6, Average Loss=4.17214298248291, Class Loss=0.5073873996734619, Reg Loss=3.6647555828094482
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/19, Loss=13.149725380539895
Loss made of: CE 0.6307353377342224, LKD 4.301591396331787, LDE 0.0, LReg 0.0, POD 8.23204517364502 EntMin 0.0
Epoch 3, Class Loss=0.5004960298538208, Reg Loss=3.734208822250366
Clinet index 0, End of Epoch 3/6, Average Loss=4.234704971313477, Class Loss=0.5004960298538208, Reg Loss=3.734208822250366
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/19, Loss=12.346086904406548
Loss made of: CE 0.5460437536239624, LKD 3.222886800765991, LDE 0.0, LReg 0.0, POD 8.838248252868652 EntMin 0.0
Epoch 4, Class Loss=0.488438218832016, Reg Loss=3.6304049491882324
Clinet index 0, End of Epoch 4/6, Average Loss=4.118843078613281, Class Loss=0.488438218832016, Reg Loss=3.6304049491882324
Pseudo labeling is: None
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/19, Loss=12.770262858271598
Loss made of: CE 0.5137891173362732, LKD 3.7848193645477295, LDE 0.0, LReg 0.0, POD 8.91220760345459 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Class Loss=0.4818982183933258, Reg Loss=3.6340155601501465
Clinet index 0, End of Epoch 5/6, Average Loss=4.1159138679504395, Class Loss=0.4818982183933258, Reg Loss=3.6340155601501465
Pseudo labeling is: None
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/19, Loss=12.466197600960731
Loss made of: CE 0.4728180766105652, LKD 3.157947301864624, LDE 0.0, LReg 0.0, POD 7.488696098327637 EntMin 0.0
Epoch 6, Class Loss=0.4848964214324951, Reg Loss=3.7002673149108887
Clinet index 0, End of Epoch 6/6, Average Loss=4.185163497924805, Class Loss=0.4848964214324951, Reg Loss=3.7002673149108887
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=23.05540828704834
Loss made of: CE 1.0418076515197754, LKD 7.226006984710693, LDE 0.0, LReg 0.0, POD 15.571050643920898 EntMin 0.0
Epoch 1, Batch 20/35, Loss=19.88524808883667
Loss made of: CE 0.8142412900924683, LKD 5.381962776184082, LDE 0.0, LReg 0.0, POD 12.041427612304688 EntMin 0.0
Epoch 1, Batch 30/35, Loss=19.312759971618654
Loss made of: CE 0.9162158966064453, LKD 6.852414608001709, LDE 0.0, LReg 0.0, POD 13.595478057861328 EntMin 0.0
Epoch 1, Class Loss=0.926802396774292, Reg Loss=5.842535972595215
Clinet index 1, End of Epoch 1/6, Average Loss=6.769338607788086, Class Loss=0.926802396774292, Reg Loss=5.842535972595215
Pseudo labeling is: None
Epoch 2, lr = 0.000584
Epoch 2, Batch 10/35, Loss=18.861225217580795
Loss made of: CE 0.6482099890708923, LKD 5.721235275268555, LDE 0.0, LReg 0.0, POD 11.798847198486328 EntMin 0.0
Epoch 2, Batch 20/35, Loss=18.619379663467406
Loss made of: CE 0.7122749090194702, LKD 5.3283233642578125, LDE 0.0, LReg 0.0, POD 12.77180290222168 EntMin 0.0
Epoch 2, Batch 30/35, Loss=17.883550745248794
Loss made of: CE 0.5793384313583374, LKD 5.78207540512085, LDE 0.0, LReg 0.0, POD 11.469829559326172 EntMin 0.0
Epoch 2, Class Loss=0.6745831370353699, Reg Loss=5.724971294403076
Clinet index 1, End of Epoch 2/6, Average Loss=6.399554252624512, Class Loss=0.6745831370353699, Reg Loss=5.724971294403076
Pseudo labeling is: None
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/35, Loss=17.351771116256714
Loss made of: CE 0.6277453899383545, LKD 6.040820121765137, LDE 0.0, LReg 0.0, POD 11.018226623535156 EntMin 0.0
Epoch 3, Batch 20/35, Loss=17.913561922311782
Loss made of: CE 0.6753072738647461, LKD 6.180397987365723, LDE 0.0, LReg 0.0, POD 11.800025939941406 EntMin 0.0
Epoch 3, Batch 30/35, Loss=17.71268364191055
Loss made of: CE 0.6880058646202087, LKD 6.0773444175720215, LDE 0.0, LReg 0.0, POD 13.185917854309082 EntMin 0.0
Epoch 3, Class Loss=0.6136996746063232, Reg Loss=5.598474979400635
Clinet index 1, End of Epoch 3/6, Average Loss=6.212174415588379, Class Loss=0.6136996746063232, Reg Loss=5.598474979400635
Pseudo labeling is: None
Epoch 4, lr = 0.000487
Epoch 4, Batch 10/35, Loss=17.422803527116777
Loss made of: CE 0.5846925973892212, LKD 5.136675834655762, LDE 0.0, LReg 0.0, POD 11.230066299438477 EntMin 0.0
Epoch 4, Batch 20/35, Loss=16.609551155567168
Loss made of: CE 0.5193825960159302, LKD 4.641049861907959, LDE 0.0, LReg 0.0, POD 11.562023162841797 EntMin 0.0
Epoch 4, Batch 30/35, Loss=16.849131482839585
Loss made of: CE 0.5698606967926025, LKD 4.694620132446289, LDE 0.0, LReg 0.0, POD 10.767548561096191 EntMin 0.0
Epoch 4, Class Loss=0.5863696932792664, Reg Loss=5.5726704597473145
Clinet index 1, End of Epoch 4/6, Average Loss=6.1590399742126465, Class Loss=0.5863696932792664, Reg Loss=5.5726704597473145
Pseudo labeling is: None
Epoch 5, lr = 0.000438
Epoch 5, Batch 10/35, Loss=17.097493284940718
Loss made of: CE 0.5678911209106445, LKD 5.227048873901367, LDE 0.0, LReg 0.0, POD 11.373708724975586 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 20/35, Loss=17.12730987071991
Loss made of: CE 0.5987752676010132, LKD 6.341892242431641, LDE 0.0, LReg 0.0, POD 11.319686889648438 EntMin 0.0
Epoch 5, Batch 30/35, Loss=15.984437349438668
Loss made of: CE 0.49117717146873474, LKD 4.903230667114258, LDE 0.0, LReg 0.0, POD 9.919151306152344 EntMin 0.0
Epoch 5, Class Loss=0.5764312744140625, Reg Loss=5.502505302429199
Clinet index 1, End of Epoch 5/6, Average Loss=6.078936576843262, Class Loss=0.5764312744140625, Reg Loss=5.502505302429199
Pseudo labeling is: None
Epoch 6, lr = 0.000389
Epoch 6, Batch 10/35, Loss=16.403076499700546
Loss made of: CE 0.4944998621940613, LKD 5.175432205200195, LDE 0.0, LReg 0.0, POD 10.217771530151367 EntMin 0.0
Epoch 6, Batch 20/35, Loss=17.17852932214737
Loss made of: CE 0.5339850187301636, LKD 6.068423271179199, LDE 0.0, LReg 0.0, POD 10.263467788696289 EntMin 0.0
Epoch 6, Batch 30/35, Loss=17.50523705482483
Loss made of: CE 0.6099689602851868, LKD 5.619271755218506, LDE 0.0, LReg 0.0, POD 10.851940155029297 EntMin 0.0
Epoch 6, Class Loss=0.582859992980957, Reg Loss=5.6296586990356445
Clinet index 1, End of Epoch 6/6, Average Loss=6.212518692016602, Class Loss=0.582859992980957, Reg Loss=5.6296586990356445
federated aggregation...
Validation, Class Loss=0.46285608410835266, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.856827
Mean Acc: 0.545266
FreqW Acc: 0.767129
Mean IoU: 0.410738
Class IoU:
	class 0: 0.89374
	class 1: 0.70901936
	class 2: 0.27606994
	class 3: 0.42904162
	class 4: 0.5728602
	class 5: 0.0865371
	class 6: 0.6531576
	class 7: 0.58002806
	class 8: 0.40072274
	class 9: 0.00016844108
	class 10: 0.46355128
	class 11: 0.27469718
	class 12: 0.0
Class Acc:
	class 0: 0.9699669
	class 1: 0.7180801
	class 2: 0.53422683
	class 3: 0.9409724
	class 4: 0.86907125
	class 5: 0.08682838
	class 6: 0.6673499
	class 7: 0.6481604
	class 8: 0.773533
	class 9: 0.00016844108
	class 10: 0.47595242
	class 11: 0.40414497
	class 12: 0.0

federated global round: 14, step: 2
select part of clients to conduct local training
[16, 9, 4, 0]
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/42, Loss=16.86781103014946
Loss made of: CE 0.8273071050643921, LKD 5.684165954589844, LDE 0.0, LReg 0.0, POD 11.527948379516602 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/42, Loss=16.45714968442917
Loss made of: CE 0.5997024774551392, LKD 4.478735446929932, LDE 0.0, LReg 0.0, POD 10.040261268615723 EntMin 0.0
Epoch 1, Batch 30/42, Loss=16.75505598783493
Loss made of: CE 0.7246193885803223, LKD 5.21329402923584, LDE 0.0, LReg 0.0, POD 10.215045928955078 EntMin 0.0
Epoch 1, Batch 40/42, Loss=15.728072005510331
Loss made of: CE 0.5779179334640503, LKD 5.546443939208984, LDE 0.0, LReg 0.0, POD 9.369547843933105 EntMin 0.0
Epoch 1, Class Loss=0.7459338903427124, Reg Loss=5.0510735511779785
Clinet index 16, End of Epoch 1/6, Average Loss=5.7970075607299805, Class Loss=0.7459338903427124, Reg Loss=5.0510735511779785
Pseudo labeling is: None
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/42, Loss=16.365720051527024
Loss made of: CE 0.5666500329971313, LKD 5.472865581512451, LDE 0.0, LReg 0.0, POD 10.019401550292969 EntMin 0.0
Epoch 2, Batch 20/42, Loss=15.969491469860078
Loss made of: CE 0.6396418809890747, LKD 5.431163787841797, LDE 0.0, LReg 0.0, POD 11.304922103881836 EntMin 0.0
Epoch 2, Batch 30/42, Loss=15.179266852140426
Loss made of: CE 0.5277428030967712, LKD 5.246138095855713, LDE 0.0, LReg 0.0, POD 10.923477172851562 EntMin 0.0
Epoch 2, Batch 40/42, Loss=15.379286098480225
Loss made of: CE 0.5306995511054993, LKD 4.230708599090576, LDE 0.0, LReg 0.0, POD 8.834342956542969 EntMin 0.0
Epoch 2, Class Loss=0.5929003953933716, Reg Loss=5.0703654289245605
Clinet index 16, End of Epoch 2/6, Average Loss=5.663265705108643, Class Loss=0.5929003953933716, Reg Loss=5.0703654289245605
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/42, Loss=15.489703768491745
Loss made of: CE 0.5943558812141418, LKD 4.911304473876953, LDE 0.0, LReg 0.0, POD 9.235994338989258 EntMin 0.0
Epoch 3, Batch 20/42, Loss=16.43707343041897
Loss made of: CE 0.5723484754562378, LKD 6.8969340324401855, LDE 0.0, LReg 0.0, POD 11.286747932434082 EntMin 0.0
Epoch 3, Batch 30/42, Loss=15.10886862874031
Loss made of: CE 0.5518050193786621, LKD 5.315274238586426, LDE 0.0, LReg 0.0, POD 9.806962966918945 EntMin 0.0
Epoch 3, Batch 40/42, Loss=15.535130405426026
Loss made of: CE 0.5513181090354919, LKD 5.4211554527282715, LDE 0.0, LReg 0.0, POD 10.789819717407227 EntMin 0.0
Epoch 3, Class Loss=0.5714395046234131, Reg Loss=5.040012836456299
Clinet index 16, End of Epoch 3/6, Average Loss=5.611452102661133, Class Loss=0.5714395046234131, Reg Loss=5.040012836456299
Pseudo labeling is: None
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/42, Loss=15.637233507633209
Loss made of: CE 0.5053150653839111, LKD 4.31846809387207, LDE 0.0, LReg 0.0, POD 9.458196640014648 EntMin 0.0
Epoch 4, Batch 20/42, Loss=15.160309094190598
Loss made of: CE 0.5408124923706055, LKD 5.052830696105957, LDE 0.0, LReg 0.0, POD 9.03303050994873 EntMin 0.0
Epoch 4, Batch 30/42, Loss=15.069565552473069
Loss made of: CE 0.5424331426620483, LKD 4.697038173675537, LDE 0.0, LReg 0.0, POD 8.497979164123535 EntMin 0.0
Epoch 4, Batch 40/42, Loss=15.29889212846756
Loss made of: CE 0.5202645063400269, LKD 4.446438312530518, LDE 0.0, LReg 0.0, POD 9.189713478088379 EntMin 0.0
Epoch 4, Class Loss=0.5697968006134033, Reg Loss=5.004461288452148
Clinet index 16, End of Epoch 4/6, Average Loss=5.574257850646973, Class Loss=0.5697968006134033, Reg Loss=5.004461288452148
Pseudo labeling is: None
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/42, Loss=15.151912447810172
Loss made of: CE 0.5213141441345215, LKD 5.026109218597412, LDE 0.0, LReg 0.0, POD 9.789037704467773 EntMin 0.0
Epoch 5, Batch 20/42, Loss=15.299263113737107
Loss made of: CE 0.4818364977836609, LKD 4.511253356933594, LDE 0.0, LReg 0.0, POD 9.30086898803711 EntMin 0.0
Epoch 5, Batch 30/42, Loss=15.037128487229348
Loss made of: CE 0.6136742830276489, LKD 4.801762104034424, LDE 0.0, LReg 0.0, POD 9.392613410949707 EntMin 0.0
Epoch 5, Batch 40/42, Loss=15.189929530024529
Loss made of: CE 0.5993868112564087, LKD 6.09052848815918, LDE 0.0, LReg 0.0, POD 9.38599967956543 EntMin 0.0
Epoch 5, Class Loss=0.5604667067527771, Reg Loss=4.999794006347656
Clinet index 16, End of Epoch 5/6, Average Loss=5.560260772705078, Class Loss=0.5604667067527771, Reg Loss=4.999794006347656
Pseudo labeling is: None
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/42, Loss=15.014669644832612
Loss made of: CE 0.4969027042388916, LKD 4.425094127655029, LDE 0.0, LReg 0.0, POD 8.465730667114258 EntMin 0.0
Epoch 6, Batch 20/42, Loss=14.511188507080078
Loss made of: CE 0.5553313493728638, LKD 4.484429836273193, LDE 0.0, LReg 0.0, POD 8.516375541687012 EntMin 0.0
Epoch 6, Batch 30/42, Loss=14.991907632350921
Loss made of: CE 0.6048038005828857, LKD 4.696537971496582, LDE 0.0, LReg 0.0, POD 8.597888946533203 EntMin 0.0
Epoch 6, Batch 40/42, Loss=15.530703088641166
Loss made of: CE 0.611952006816864, LKD 4.672318935394287, LDE 0.0, LReg 0.0, POD 9.855474472045898 EntMin 0.0
Epoch 6, Class Loss=0.5507121086120605, Reg Loss=4.983624458312988
Clinet index 16, End of Epoch 6/6, Average Loss=5.534336566925049, Class Loss=0.5507121086120605, Reg Loss=4.983624458312988
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=20.893871551752092
Loss made of: CE 0.9746711254119873, LKD 6.158238410949707, LDE 0.0, LReg 0.0, POD 14.733245849609375 EntMin 0.0
Epoch 1, Batch 20/35, Loss=19.734317123889923
Loss made of: CE 0.7718724608421326, LKD 5.288163661956787, LDE 0.0, LReg 0.0, POD 12.547933578491211 EntMin 0.0
Epoch 1, Batch 30/35, Loss=18.130744379758834
Loss made of: CE 0.6621798872947693, LKD 5.925612926483154, LDE 0.0, LReg 0.0, POD 12.964519500732422 EntMin 0.0
Epoch 1, Class Loss=0.7953498959541321, Reg Loss=5.731377124786377
Clinet index 9, End of Epoch 1/6, Average Loss=6.526727199554443, Class Loss=0.7953498959541321, Reg Loss=5.731377124786377
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/35, Loss=17.741808524727823
Loss made of: CE 0.6038885116577148, LKD 4.732425212860107, LDE 0.0, LReg 0.0, POD 12.664701461791992 EntMin 0.0
Epoch 2, Batch 20/35, Loss=18.558481535315515
Loss made of: CE 0.4908030927181244, LKD 5.539478302001953, LDE 0.0, LReg 0.0, POD 11.872448921203613 EntMin 0.0
Epoch 2, Batch 30/35, Loss=18.16852625608444
Loss made of: CE 0.5038580894470215, LKD 5.4073076248168945, LDE 0.0, LReg 0.0, POD 10.38965129852295 EntMin 0.0
Epoch 2, Class Loss=0.5941785573959351, Reg Loss=5.631686210632324
Clinet index 9, End of Epoch 2/6, Average Loss=6.225864887237549, Class Loss=0.5941785573959351, Reg Loss=5.631686210632324
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/35, Loss=17.829005813598634
Loss made of: CE 0.5952874422073364, LKD 6.279199600219727, LDE 0.0, LReg 0.0, POD 11.350798606872559 EntMin 0.0
Epoch 3, Batch 20/35, Loss=18.397804993391038
Loss made of: CE 0.6067426204681396, LKD 5.780385494232178, LDE 0.0, LReg 0.0, POD 11.11890697479248 EntMin 0.0
Epoch 3, Batch 30/35, Loss=17.195234885811807
Loss made of: CE 0.5278489589691162, LKD 5.178516387939453, LDE 0.0, LReg 0.0, POD 10.10894775390625 EntMin 0.0
Epoch 3, Class Loss=0.5882095098495483, Reg Loss=5.7056097984313965
Clinet index 9, End of Epoch 3/6, Average Loss=6.293819427490234, Class Loss=0.5882095098495483, Reg Loss=5.7056097984313965
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/35, Loss=17.826183727383615
Loss made of: CE 0.5556984543800354, LKD 4.75119161605835, LDE 0.0, LReg 0.0, POD 11.759115219116211 EntMin 0.0
Epoch 4, Batch 20/35, Loss=17.26219836473465
Loss made of: CE 0.4832140803337097, LKD 5.2019171714782715, LDE 0.0, LReg 0.0, POD 9.748495101928711 EntMin 0.0
Epoch 4, Batch 30/35, Loss=17.194752791523932
Loss made of: CE 0.6191043257713318, LKD 5.364774227142334, LDE 0.0, LReg 0.0, POD 11.759937286376953 EntMin 0.0
Epoch 4, Class Loss=0.563541829586029, Reg Loss=5.627497673034668
Clinet index 9, End of Epoch 4/6, Average Loss=6.191039562225342, Class Loss=0.563541829586029, Reg Loss=5.627497673034668
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/35, Loss=16.544877684116365
Loss made of: CE 0.4308745265007019, LKD 4.533170700073242, LDE 0.0, LReg 0.0, POD 11.005475997924805 EntMin 0.0
Epoch 5, Batch 20/35, Loss=17.379028674960136
Loss made of: CE 0.499898761510849, LKD 5.694507598876953, LDE 0.0, LReg 0.0, POD 9.487582206726074 EntMin 0.0
Epoch 5, Batch 30/35, Loss=16.509194415807723
Loss made of: CE 0.63655686378479, LKD 6.876691818237305, LDE 0.0, LReg 0.0, POD 12.13719367980957 EntMin 0.0
Epoch 5, Class Loss=0.5570659041404724, Reg Loss=5.595341205596924
Clinet index 9, End of Epoch 5/6, Average Loss=6.152407169342041, Class Loss=0.5570659041404724, Reg Loss=5.595341205596924
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/35, Loss=16.943712359666826
Loss made of: CE 0.6215783953666687, LKD 5.98201847076416, LDE 0.0, LReg 0.0, POD 11.33803653717041 EntMin 0.0
Epoch 6, Batch 20/35, Loss=15.921069931983947
Loss made of: CE 0.545359194278717, LKD 5.624584197998047, LDE 0.0, LReg 0.0, POD 10.154712677001953 EntMin 0.0
Epoch 6, Batch 30/35, Loss=16.69117662012577
Loss made of: CE 0.5109054446220398, LKD 5.1354827880859375, LDE 0.0, LReg 0.0, POD 10.760984420776367 EntMin 0.0
Epoch 6, Class Loss=0.5478958487510681, Reg Loss=5.552708625793457
Clinet index 9, End of Epoch 6/6, Average Loss=6.10060453414917, Class Loss=0.5478958487510681, Reg Loss=5.552708625793457
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=12.555384302139283
Loss made of: CE 0.46654218435287476, LKD 3.6601481437683105, LDE 0.0, LReg 0.0, POD 8.093520164489746 EntMin 0.0
Epoch 1, Class Loss=0.4904232621192932, Reg Loss=3.535356283187866
Clinet index 4, End of Epoch 1/6, Average Loss=4.025779724121094, Class Loss=0.4904232621192932, Reg Loss=3.535356283187866
Pseudo labeling is: None
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/19, Loss=12.704793038964272
Loss made of: CE 0.4991537034511566, LKD 3.740175247192383, LDE 0.0, LReg 0.0, POD 8.440528869628906 EntMin 0.0
Epoch 2, Class Loss=0.4837896525859833, Reg Loss=3.60758900642395
Clinet index 4, End of Epoch 2/6, Average Loss=4.091378688812256, Class Loss=0.4837896525859833, Reg Loss=3.60758900642395
Pseudo labeling is: None
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/19, Loss=12.276222303509712
Loss made of: CE 0.401546448469162, LKD 3.7293219566345215, LDE 0.0, LReg 0.0, POD 7.186123847961426 EntMin 0.0
Epoch 3, Class Loss=0.4745679199695587, Reg Loss=3.5685389041900635
Clinet index 4, End of Epoch 3/6, Average Loss=4.043107032775879, Class Loss=0.4745679199695587, Reg Loss=3.5685389041900635
Pseudo labeling is: None
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/19, Loss=12.011089557409287
Loss made of: CE 0.48772555589675903, LKD 3.7217540740966797, LDE 0.0, LReg 0.0, POD 7.8065924644470215 EntMin 0.0
Epoch 4, Class Loss=0.46514758467674255, Reg Loss=3.6061244010925293
Clinet index 4, End of Epoch 4/6, Average Loss=4.071271896362305, Class Loss=0.46514758467674255, Reg Loss=3.6061244010925293
Pseudo labeling is: None
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/19, Loss=12.38651438355446
Loss made of: CE 0.4994008541107178, LKD 3.9586093425750732, LDE 0.0, LReg 0.0, POD 7.9735636711120605 EntMin 0.0
Epoch 5, Class Loss=0.4625660181045532, Reg Loss=3.649247884750366
Clinet index 4, End of Epoch 5/6, Average Loss=4.111814022064209, Class Loss=0.4625660181045532, Reg Loss=3.649247884750366
Pseudo labeling is: None
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/19, Loss=12.068112733960152
Loss made of: CE 0.3761415481567383, LKD 3.0867886543273926, LDE 0.0, LReg 0.0, POD 7.437788963317871 EntMin 0.0
Epoch 6, Class Loss=0.4568563401699066, Reg Loss=3.6326775550842285
Clinet index 4, End of Epoch 6/6, Average Loss=4.089533805847168, Class Loss=0.4568563401699066, Reg Loss=3.6326775550842285
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000304
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=12.773026925325393
Loss made of: CE 0.45653754472732544, LKD 3.5627455711364746, LDE 0.0, LReg 0.0, POD 9.109806060791016 EntMin 0.0
Epoch 1, Class Loss=0.4990108013153076, Reg Loss=3.66257643699646
Clinet index 0, End of Epoch 1/6, Average Loss=4.161587238311768, Class Loss=0.4990108013153076, Reg Loss=3.66257643699646
Pseudo labeling is: None
Epoch 2, lr = 0.000258
Epoch 2, Batch 10/19, Loss=12.252287194132805
Loss made of: CE 0.5585212707519531, LKD 3.2070727348327637, LDE 0.0, LReg 0.0, POD 8.086835861206055 EntMin 0.0
Epoch 2, Class Loss=0.47698909044265747, Reg Loss=3.6048741340637207
Clinet index 0, End of Epoch 2/6, Average Loss=4.0818634033203125, Class Loss=0.47698909044265747, Reg Loss=3.6048741340637207
Pseudo labeling is: None
Epoch 3, lr = 0.000211
Epoch 3, Batch 10/19, Loss=12.629937607049943
Loss made of: CE 0.567205548286438, LKD 4.033638000488281, LDE 0.0, LReg 0.0, POD 7.808082103729248 EntMin 0.0
Epoch 3, Class Loss=0.4842384159564972, Reg Loss=3.705009937286377
Clinet index 0, End of Epoch 3/6, Average Loss=4.189248561859131, Class Loss=0.4842384159564972, Reg Loss=3.705009937286377
Pseudo labeling is: None
Epoch 4, lr = 0.000163
Epoch 4, Batch 10/19, Loss=12.269256964325905
Loss made of: CE 0.5908963084220886, LKD 3.4410386085510254, LDE 0.0, LReg 0.0, POD 9.064506530761719 EntMin 0.0
Epoch 4, Class Loss=0.48913735151290894, Reg Loss=3.685351848602295
Clinet index 0, End of Epoch 4/6, Average Loss=4.1744890213012695, Class Loss=0.48913735151290894, Reg Loss=3.685351848602295
Pseudo labeling is: None
Epoch 5, lr = 0.000113
Epoch 5, Batch 10/19, Loss=12.397542122006417
Loss made of: CE 0.4942072033882141, LKD 3.744053602218628, LDE 0.0, LReg 0.0, POD 8.586530685424805 EntMin 0.0
Epoch 5, Class Loss=0.4750820994377136, Reg Loss=3.6529815196990967
Clinet index 0, End of Epoch 5/6, Average Loss=4.128063678741455, Class Loss=0.4750820994377136, Reg Loss=3.6529815196990967
Pseudo labeling is: None
Epoch 6, lr = 0.000061
Epoch 6, Batch 10/19, Loss=12.101747131347656
Loss made of: CE 0.46819448471069336, LKD 3.097572088241577, LDE 0.0, LReg 0.0, POD 7.389560699462891 EntMin 0.0
Epoch 6, Class Loss=0.4796888530254364, Reg Loss=3.6735057830810547
Clinet index 0, End of Epoch 6/6, Average Loss=4.153194427490234, Class Loss=0.4796888530254364, Reg Loss=3.6735057830810547
federated aggregation...
Validation, Class Loss=0.4390307366847992, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.855333
Mean Acc: 0.533170
FreqW Acc: 0.765188
Mean IoU: 0.401228
Class IoU:
	class 0: 0.8941872
	class 1: 0.69252807
	class 2: 0.27145934
	class 3: 0.43561774
	class 4: 0.57647103
	class 5: 0.0852414
	class 6: 0.6517337
	class 7: 0.58508676
	class 8: 0.39000493
	class 9: 0.00014176052
	class 10: 0.34022763
	class 11: 0.2714552
	class 12: 0.021811932
Class Acc:
	class 0: 0.97049725
	class 1: 0.699773
	class 2: 0.5163639
	class 3: 0.9347707
	class 4: 0.8645336
	class 5: 0.085505575
	class 6: 0.6638567
	class 7: 0.6616293
	class 8: 0.7946663
	class 9: 0.00014176052
	class 10: 0.34322122
	class 11: 0.37441394
	class 12: 0.021836165

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[11, 6, 10, 7]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=21.490271782875062
Loss made of: CE 1.137567400932312, LKD 5.286466598510742, LDE 0.0, LReg 0.0, POD 14.49081802368164 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/102, Loss=19.868027293682097
Loss made of: CE 0.8938755989074707, LKD 5.783507347106934, LDE 0.0, LReg 0.0, POD 12.593799591064453 EntMin 0.0
Epoch 1, Batch 30/102, Loss=18.582968086004257
Loss made of: CE 0.9118779897689819, LKD 6.612305641174316, LDE 0.0, LReg 0.0, POD 12.821981430053711 EntMin 0.0
Epoch 1, Batch 40/102, Loss=18.390564781427383
Loss made of: CE 0.6143735647201538, LKD 5.77881383895874, LDE 0.0, LReg 0.0, POD 10.370210647583008 EntMin 0.0
Epoch 1, Batch 50/102, Loss=17.54754998087883
Loss made of: CE 0.563507616519928, LKD 5.173501968383789, LDE 0.0, LReg 0.0, POD 9.893681526184082 EntMin 0.0
Epoch 1, Batch 60/102, Loss=17.802600252628327
Loss made of: CE 0.7239623069763184, LKD 5.635066032409668, LDE 0.0, LReg 0.0, POD 9.604137420654297 EntMin 0.0
Epoch 1, Batch 70/102, Loss=16.979123216867446
Loss made of: CE 0.6296942234039307, LKD 5.618892192840576, LDE 0.0, LReg 0.0, POD 10.072389602661133 EntMin 0.0
Epoch 1, Batch 80/102, Loss=17.053900051116944
Loss made of: CE 0.6846374869346619, LKD 5.861130714416504, LDE 0.0, LReg 0.0, POD 10.545320510864258 EntMin 0.0
Epoch 1, Batch 90/102, Loss=16.13100711107254
Loss made of: CE 0.576470136642456, LKD 6.732353210449219, LDE 0.0, LReg 0.0, POD 9.591760635375977 EntMin 0.0
Epoch 1, Batch 100/102, Loss=16.449695241451263
Loss made of: CE 0.7020713686943054, LKD 6.820738792419434, LDE 0.0, LReg 0.0, POD 10.39651870727539 EntMin 0.0
Epoch 1, Class Loss=0.8023779988288879, Reg Loss=6.030125141143799
Clinet index 11, End of Epoch 1/6, Average Loss=6.832503318786621, Class Loss=0.8023779988288879, Reg Loss=6.030125141143799
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=16.37126863002777
Loss made of: CE 0.5790413618087769, LKD 5.555724143981934, LDE 0.0, LReg 0.0, POD 9.17398452758789 EntMin 0.0
Epoch 2, Batch 20/102, Loss=15.985705634951591
Loss made of: CE 0.47207581996917725, LKD 5.683387756347656, LDE 0.0, LReg 0.0, POD 9.944960594177246 EntMin 0.0
Epoch 2, Batch 30/102, Loss=16.277145314216614
Loss made of: CE 0.6673638820648193, LKD 6.127988338470459, LDE 0.0, LReg 0.0, POD 9.589241981506348 EntMin 0.0
Epoch 2, Batch 40/102, Loss=16.24172584414482
Loss made of: CE 0.6115847826004028, LKD 5.867857933044434, LDE 0.0, LReg 0.0, POD 9.19120979309082 EntMin 0.0
Epoch 2, Batch 50/102, Loss=16.373815995454787
Loss made of: CE 0.5235437750816345, LKD 6.436274528503418, LDE 0.0, LReg 0.0, POD 10.409581184387207 EntMin 0.0
Epoch 2, Batch 60/102, Loss=16.608111622929574
Loss made of: CE 0.4839152693748474, LKD 6.516125679016113, LDE 0.0, LReg 0.0, POD 9.611882209777832 EntMin 0.0
Epoch 2, Batch 70/102, Loss=15.725364020466804
Loss made of: CE 0.46259042620658875, LKD 6.8337860107421875, LDE 0.0, LReg 0.0, POD 9.104375839233398 EntMin 0.0
Epoch 2, Batch 80/102, Loss=16.46132953464985
Loss made of: CE 0.3942505717277527, LKD 6.928744316101074, LDE 0.0, LReg 0.0, POD 10.784138679504395 EntMin 0.0
Epoch 2, Batch 90/102, Loss=16.31513022184372
Loss made of: CE 0.7033753395080566, LKD 6.9404425621032715, LDE 0.0, LReg 0.0, POD 10.978006362915039 EntMin 0.0
Epoch 2, Batch 100/102, Loss=16.079711538553237
Loss made of: CE 0.45253467559814453, LKD 5.958453178405762, LDE 0.0, LReg 0.0, POD 10.295852661132812 EntMin 0.0
Epoch 2, Class Loss=0.5282872319221497, Reg Loss=5.901051998138428
Clinet index 11, End of Epoch 2/6, Average Loss=6.429339408874512, Class Loss=0.5282872319221497, Reg Loss=5.901051998138428
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=16.266168490052223
Loss made of: CE 0.5904291868209839, LKD 6.414061546325684, LDE 0.0, LReg 0.0, POD 9.012057304382324 EntMin 0.0
Epoch 3, Batch 20/102, Loss=15.89489334821701
Loss made of: CE 0.5953022241592407, LKD 6.007143974304199, LDE 0.0, LReg 0.0, POD 8.730420112609863 EntMin 0.0
Epoch 3, Batch 30/102, Loss=15.659940257668495
Loss made of: CE 0.371508926153183, LKD 6.0939130783081055, LDE 0.0, LReg 0.0, POD 9.982589721679688 EntMin 0.0
Epoch 3, Batch 40/102, Loss=16.166204020380974
Loss made of: CE 0.42303913831710815, LKD 6.545402526855469, LDE 0.0, LReg 0.0, POD 10.089954376220703 EntMin 0.0
Epoch 3, Batch 50/102, Loss=16.530859154462814
Loss made of: CE 0.47221705317497253, LKD 6.228121757507324, LDE 0.0, LReg 0.0, POD 9.41757869720459 EntMin 0.0
Epoch 3, Batch 60/102, Loss=15.950904828310012
Loss made of: CE 0.36211031675338745, LKD 5.439321517944336, LDE 0.0, LReg 0.0, POD 9.496599197387695 EntMin 0.0
Epoch 3, Batch 70/102, Loss=15.928863954544067
Loss made of: CE 0.38408026099205017, LKD 5.5172438621521, LDE 0.0, LReg 0.0, POD 9.688711166381836 EntMin 0.0
Epoch 3, Batch 80/102, Loss=16.189688727259636
Loss made of: CE 0.37677595019340515, LKD 5.785140037536621, LDE 0.0, LReg 0.0, POD 10.331867218017578 EntMin 0.0
Epoch 3, Batch 90/102, Loss=16.171841365098953
Loss made of: CE 0.49309319257736206, LKD 5.372605323791504, LDE 0.0, LReg 0.0, POD 9.618327140808105 EntMin 0.0
Epoch 3, Batch 100/102, Loss=16.05616277754307
Loss made of: CE 0.3830041289329529, LKD 5.757638454437256, LDE 0.0, LReg 0.0, POD 8.395347595214844 EntMin 0.0
Epoch 3, Class Loss=0.45598292350769043, Reg Loss=5.913051605224609
Clinet index 11, End of Epoch 3/6, Average Loss=6.369034767150879, Class Loss=0.45598292350769043, Reg Loss=5.913051605224609
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=15.779478913545608
Loss made of: CE 0.4619271755218506, LKD 5.398963451385498, LDE 0.0, LReg 0.0, POD 8.905234336853027 EntMin 0.0
Epoch 4, Batch 20/102, Loss=15.824143487215043
Loss made of: CE 0.35572999715805054, LKD 5.1301164627075195, LDE 0.0, LReg 0.0, POD 10.519414901733398 EntMin 0.0
Epoch 4, Batch 30/102, Loss=15.89590364396572
Loss made of: CE 0.6067139506340027, LKD 5.96726131439209, LDE 0.0, LReg 0.0, POD 10.046972274780273 EntMin 0.0
Epoch 4, Batch 40/102, Loss=15.747638326883315
Loss made of: CE 0.4049954414367676, LKD 5.770023345947266, LDE 0.0, LReg 0.0, POD 8.649497985839844 EntMin 0.0
Epoch 4, Batch 50/102, Loss=15.726532968878747
Loss made of: CE 0.48171210289001465, LKD 5.980594635009766, LDE 0.0, LReg 0.0, POD 9.320984840393066 EntMin 0.0
Epoch 4, Batch 60/102, Loss=15.93917045891285
Loss made of: CE 0.43680471181869507, LKD 5.2734808921813965, LDE 0.0, LReg 0.0, POD 9.915061950683594 EntMin 0.0
Epoch 4, Batch 70/102, Loss=15.764987727999687
Loss made of: CE 0.5173222422599792, LKD 6.538606643676758, LDE 0.0, LReg 0.0, POD 9.230720520019531 EntMin 0.0
Epoch 4, Batch 80/102, Loss=15.911976033449173
Loss made of: CE 0.4713065028190613, LKD 5.948220729827881, LDE 0.0, LReg 0.0, POD 11.55813980102539 EntMin 0.0
Epoch 4, Batch 90/102, Loss=15.904438787698746
Loss made of: CE 0.37252408266067505, LKD 6.4411115646362305, LDE 0.0, LReg 0.0, POD 8.488834381103516 EntMin 0.0
Epoch 4, Batch 100/102, Loss=15.546706333756447
Loss made of: CE 0.5087873339653015, LKD 6.0378007888793945, LDE 0.0, LReg 0.0, POD 10.16919994354248 EntMin 0.0
Epoch 4, Class Loss=0.4275708794593811, Reg Loss=5.890417098999023
Clinet index 11, End of Epoch 4/6, Average Loss=6.31798791885376, Class Loss=0.4275708794593811, Reg Loss=5.890417098999023
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=15.901441609859466
Loss made of: CE 0.428445428609848, LKD 6.582111358642578, LDE 0.0, LReg 0.0, POD 8.719782829284668 EntMin 0.0
Epoch 5, Batch 20/102, Loss=15.913495245575906
Loss made of: CE 0.311689168214798, LKD 6.122081279754639, LDE 0.0, LReg 0.0, POD 9.674139022827148 EntMin 0.0
Epoch 5, Batch 30/102, Loss=15.756144419312477
Loss made of: CE 0.44916218519210815, LKD 5.24206018447876, LDE 0.0, LReg 0.0, POD 8.823305130004883 EntMin 0.0
Epoch 5, Batch 40/102, Loss=15.424146911501884
Loss made of: CE 0.4986783266067505, LKD 5.9521403312683105, LDE 0.0, LReg 0.0, POD 8.798066139221191 EntMin 0.0
Epoch 5, Batch 50/102, Loss=15.617698156833649
Loss made of: CE 0.3552802801132202, LKD 5.631606101989746, LDE 0.0, LReg 0.0, POD 10.463142395019531 EntMin 0.0
Epoch 5, Batch 60/102, Loss=15.446956893801689
Loss made of: CE 0.4114867150783539, LKD 7.164613246917725, LDE 0.0, LReg 0.0, POD 9.332581520080566 EntMin 0.0
Epoch 5, Batch 70/102, Loss=14.913195589184761
Loss made of: CE 0.42305228114128113, LKD 6.347958564758301, LDE 0.0, LReg 0.0, POD 8.81037425994873 EntMin 0.0
Epoch 5, Batch 80/102, Loss=15.287009844183922
Loss made of: CE 0.39292043447494507, LKD 5.742232799530029, LDE 0.0, LReg 0.0, POD 9.901737213134766 EntMin 0.0
Epoch 5, Batch 90/102, Loss=15.323143416643143
Loss made of: CE 0.3156557083129883, LKD 6.16608190536499, LDE 0.0, LReg 0.0, POD 8.37205982208252 EntMin 0.0
Epoch 5, Batch 100/102, Loss=15.530492261052132
Loss made of: CE 0.39496004581451416, LKD 6.382827281951904, LDE 0.0, LReg 0.0, POD 9.81326675415039 EntMin 0.0
Epoch 5, Class Loss=0.40294569730758667, Reg Loss=5.888775825500488
Clinet index 11, End of Epoch 5/6, Average Loss=6.291721343994141, Class Loss=0.40294569730758667, Reg Loss=5.888775825500488
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=15.921732273697852
Loss made of: CE 0.34473752975463867, LKD 5.556471824645996, LDE 0.0, LReg 0.0, POD 8.638236999511719 EntMin 0.0
Epoch 6, Batch 20/102, Loss=15.270219323039054
Loss made of: CE 0.4231722354888916, LKD 6.616576671600342, LDE 0.0, LReg 0.0, POD 9.143573760986328 EntMin 0.0
Epoch 6, Batch 30/102, Loss=15.040363290905953
Loss made of: CE 0.30401068925857544, LKD 5.024756908416748, LDE 0.0, LReg 0.0, POD 9.35708999633789 EntMin 0.0
Epoch 6, Batch 40/102, Loss=15.897607213258743
Loss made of: CE 0.5648865699768066, LKD 6.062203884124756, LDE 0.0, LReg 0.0, POD 10.11508560180664 EntMin 0.0
Epoch 6, Batch 50/102, Loss=15.336775171756745
Loss made of: CE 0.4338368773460388, LKD 5.887984752655029, LDE 0.0, LReg 0.0, POD 8.351330757141113 EntMin 0.0
Epoch 6, Batch 60/102, Loss=15.620773547887802
Loss made of: CE 0.4256609082221985, LKD 7.190379619598389, LDE 0.0, LReg 0.0, POD 9.864696502685547 EntMin 0.0
Epoch 6, Batch 70/102, Loss=15.215396058559417
Loss made of: CE 0.3595026433467865, LKD 5.409271240234375, LDE 0.0, LReg 0.0, POD 8.805761337280273 EntMin 0.0
Epoch 6, Batch 80/102, Loss=15.125901746749879
Loss made of: CE 0.3994565010070801, LKD 6.794381618499756, LDE 0.0, LReg 0.0, POD 9.148788452148438 EntMin 0.0
Epoch 6, Batch 90/102, Loss=15.437745574116708
Loss made of: CE 0.3263411521911621, LKD 5.007586479187012, LDE 0.0, LReg 0.0, POD 8.285367965698242 EntMin 0.0
Epoch 6, Batch 100/102, Loss=15.378747767210006
Loss made of: CE 0.4340241849422455, LKD 6.503154754638672, LDE 0.0, LReg 0.0, POD 9.297195434570312 EntMin 0.0
Epoch 6, Class Loss=0.3899648189544678, Reg Loss=5.896054744720459
Clinet index 11, End of Epoch 6/6, Average Loss=6.286019325256348, Class Loss=0.3899648189544678, Reg Loss=5.896054744720459
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/23, Loss=23.82335157394409
Loss made of: CE 1.3755779266357422, LKD 7.274655342102051, LDE 0.0, LReg 0.0, POD 15.57899284362793 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/23, Loss=20.241201508045197
Loss made of: CE 1.1419297456741333, LKD 6.788283348083496, LDE 0.0, LReg 0.0, POD 11.425145149230957 EntMin 0.0
Epoch 1, Class Loss=1.3456512689590454, Reg Loss=7.112414360046387
Clinet index 6, End of Epoch 1/6, Average Loss=8.4580659866333, Class Loss=1.3456512689590454, Reg Loss=7.112414360046387
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=19.11377494931221
Loss made of: CE 1.1025524139404297, LKD 7.021998405456543, LDE 0.0, LReg 0.0, POD 11.194357872009277 EntMin 0.0
Epoch 2, Batch 20/23, Loss=18.318044650554658
Loss made of: CE 0.7027603387832642, LKD 6.095240592956543, LDE 0.0, LReg 0.0, POD 10.485578536987305 EntMin 0.0
Epoch 2, Class Loss=0.9811814427375793, Reg Loss=6.882978916168213
Clinet index 6, End of Epoch 2/6, Average Loss=7.864160537719727, Class Loss=0.9811814427375793, Reg Loss=6.882978916168213
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=17.494039899110795
Loss made of: CE 0.8444604873657227, LKD 6.547198295593262, LDE 0.0, LReg 0.0, POD 10.285221099853516 EntMin 0.0
Epoch 3, Batch 20/23, Loss=17.71459874510765
Loss made of: CE 0.9053429365158081, LKD 6.560024261474609, LDE 0.0, LReg 0.0, POD 8.729151725769043 EntMin 0.0
Epoch 3, Class Loss=0.8759673833847046, Reg Loss=6.773860454559326
Clinet index 6, End of Epoch 3/6, Average Loss=7.64982795715332, Class Loss=0.8759673833847046, Reg Loss=6.773860454559326
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=17.184334403276445
Loss made of: CE 0.9289097189903259, LKD 7.655651092529297, LDE 0.0, LReg 0.0, POD 10.728326797485352 EntMin 0.0
Epoch 4, Batch 20/23, Loss=16.76980009675026
Loss made of: CE 0.6218360662460327, LKD 5.929107666015625, LDE 0.0, LReg 0.0, POD 9.89427375793457 EntMin 0.0
Epoch 4, Class Loss=0.8160488605499268, Reg Loss=6.816588878631592
Clinet index 6, End of Epoch 4/6, Average Loss=7.632637977600098, Class Loss=0.8160488605499268, Reg Loss=6.816588878631592
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=16.371696490049363
Loss made of: CE 0.7426720857620239, LKD 6.267708778381348, LDE 0.0, LReg 0.0, POD 9.831361770629883 EntMin 0.0
Epoch 5, Batch 20/23, Loss=16.386670553684233
Loss made of: CE 0.6384860873222351, LKD 5.968695640563965, LDE 0.0, LReg 0.0, POD 9.789222717285156 EntMin 0.0
Epoch 5, Class Loss=0.757836639881134, Reg Loss=6.775088787078857
Clinet index 6, End of Epoch 5/6, Average Loss=7.532925605773926, Class Loss=0.757836639881134, Reg Loss=6.775088787078857
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=16.55051833987236
Loss made of: CE 0.8168058395385742, LKD 8.08595085144043, LDE 0.0, LReg 0.0, POD 9.725210189819336 EntMin 0.0
Epoch 6, Batch 20/23, Loss=16.38871157169342
Loss made of: CE 0.6995905637741089, LKD 6.2035627365112305, LDE 0.0, LReg 0.0, POD 7.827685356140137 EntMin 0.0
Epoch 6, Class Loss=0.74057936668396, Reg Loss=6.852141857147217
Clinet index 6, End of Epoch 6/6, Average Loss=7.592720985412598, Class Loss=0.74057936668396, Reg Loss=6.852141857147217
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/23, Loss=23.72173627614975
Loss made of: CE 1.3853096961975098, LKD 6.96613073348999, LDE 0.0, LReg 0.0, POD 13.327439308166504 EntMin 0.0
Epoch 1, Batch 20/23, Loss=20.13466544151306
Loss made of: CE 1.0871191024780273, LKD 6.395014762878418, LDE 0.0, LReg 0.0, POD 11.684194564819336 EntMin 0.0
Epoch 1, Class Loss=1.3279786109924316, Reg Loss=7.063612461090088
Clinet index 10, End of Epoch 1/6, Average Loss=8.39159107208252, Class Loss=1.3279786109924316, Reg Loss=7.063612461090088
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/23, Loss=19.085760694742202
Loss made of: CE 1.1020139455795288, LKD 8.209744453430176, LDE 0.0, LReg 0.0, POD 12.809877395629883 EntMin 0.0
Epoch 2, Batch 20/23, Loss=17.7672267138958
Loss made of: CE 0.8047577738761902, LKD 6.839044570922852, LDE 0.0, LReg 0.0, POD 10.645944595336914 EntMin 0.0
Epoch 2, Class Loss=0.9833364486694336, Reg Loss=6.801022052764893
Clinet index 10, End of Epoch 2/6, Average Loss=7.784358501434326, Class Loss=0.9833364486694336, Reg Loss=6.801022052764893
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=17.089031553268434
Loss made of: CE 0.8243823051452637, LKD 7.321545600891113, LDE 0.0, LReg 0.0, POD 10.693975448608398 EntMin 0.0
Epoch 3, Batch 20/23, Loss=17.585366690158843
Loss made of: CE 0.8527891039848328, LKD 6.235358238220215, LDE 0.0, LReg 0.0, POD 9.749183654785156 EntMin 0.0
Epoch 3, Class Loss=0.8823634386062622, Reg Loss=6.738088130950928
Clinet index 10, End of Epoch 3/6, Average Loss=7.6204514503479, Class Loss=0.8823634386062622, Reg Loss=6.738088130950928
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=16.722459518909453
Loss made of: CE 0.7242083549499512, LKD 6.569876670837402, LDE 0.0, LReg 0.0, POD 9.674809455871582 EntMin 0.0
Epoch 4, Batch 20/23, Loss=16.866235375404358
Loss made of: CE 0.7299973368644714, LKD 6.49704122543335, LDE 0.0, LReg 0.0, POD 8.551619529724121 EntMin 0.0
Epoch 4, Class Loss=0.8038733005523682, Reg Loss=6.715879917144775
Clinet index 10, End of Epoch 4/6, Average Loss=7.519753456115723, Class Loss=0.8038733005523682, Reg Loss=6.715879917144775
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=17.139418566226958
Loss made of: CE 0.764166533946991, LKD 7.421416282653809, LDE 0.0, LReg 0.0, POD 8.933008193969727 EntMin 0.0
Epoch 5, Batch 20/23, Loss=16.43137609362602
Loss made of: CE 0.9247193336486816, LKD 8.230537414550781, LDE 0.0, LReg 0.0, POD 9.870587348937988 EntMin 0.0
Epoch 5, Class Loss=0.7618651390075684, Reg Loss=6.766145706176758
Clinet index 10, End of Epoch 5/6, Average Loss=7.528010845184326, Class Loss=0.7618651390075684, Reg Loss=6.766145706176758
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=16.47381574511528
Loss made of: CE 0.5477390289306641, LKD 5.945184707641602, LDE 0.0, LReg 0.0, POD 8.973775863647461 EntMin 0.0
Epoch 6, Batch 20/23, Loss=16.16695582270622
Loss made of: CE 0.6793742179870605, LKD 6.227349758148193, LDE 0.0, LReg 0.0, POD 8.932038307189941 EntMin 0.0
Epoch 6, Class Loss=0.7253797650337219, Reg Loss=6.7377166748046875
Clinet index 10, End of Epoch 6/6, Average Loss=7.463096618652344, Class Loss=0.7253797650337219, Reg Loss=6.7377166748046875
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=21.532532680034638
Loss made of: CE 1.2183488607406616, LKD 6.626622676849365, LDE 0.0, LReg 0.0, POD 11.834031105041504 EntMin 0.0
Epoch 1, Batch 20/102, Loss=18.7971340239048
Loss made of: CE 0.901888370513916, LKD 5.778175354003906, LDE 0.0, LReg 0.0, POD 12.491117477416992 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 30/102, Loss=18.38345851302147
Loss made of: CE 0.8162750005722046, LKD 5.675136089324951, LDE 0.0, LReg 0.0, POD 10.632786750793457 EntMin 0.0
Epoch 1, Batch 40/102, Loss=18.291273242235185
Loss made of: CE 0.6168074607849121, LKD 6.6501264572143555, LDE 0.0, LReg 0.0, POD 10.62533187866211 EntMin 0.0
Epoch 1, Batch 50/102, Loss=17.921764093637467
Loss made of: CE 0.7368917465209961, LKD 6.147274017333984, LDE 0.0, LReg 0.0, POD 11.7020263671875 EntMin 0.0
Epoch 1, Batch 60/102, Loss=17.35558284521103
Loss made of: CE 0.7200934290885925, LKD 5.58409309387207, LDE 0.0, LReg 0.0, POD 10.148588180541992 EntMin 0.0
Epoch 1, Batch 70/102, Loss=17.17241963148117
Loss made of: CE 0.7210347056388855, LKD 6.005126953125, LDE 0.0, LReg 0.0, POD 9.948661804199219 EntMin 0.0
Epoch 1, Batch 80/102, Loss=17.239005011320113
Loss made of: CE 0.7775459289550781, LKD 6.978557586669922, LDE 0.0, LReg 0.0, POD 9.577192306518555 EntMin 0.0
Epoch 1, Batch 90/102, Loss=16.313173669576646
Loss made of: CE 0.5600307583808899, LKD 5.496294975280762, LDE 0.0, LReg 0.0, POD 8.872293472290039 EntMin 0.0
Epoch 1, Batch 100/102, Loss=16.788017445802687
Loss made of: CE 0.6371945142745972, LKD 5.591641902923584, LDE 0.0, LReg 0.0, POD 9.568593978881836 EntMin 0.0
Epoch 1, Class Loss=0.8021755218505859, Reg Loss=6.053073883056641
Clinet index 7, End of Epoch 1/6, Average Loss=6.855249404907227, Class Loss=0.8021755218505859, Reg Loss=6.053073883056641
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=16.829674151539802
Loss made of: CE 0.6215423941612244, LKD 6.246372222900391, LDE 0.0, LReg 0.0, POD 8.941265106201172 EntMin 0.0
Epoch 2, Batch 20/102, Loss=16.29988161325455
Loss made of: CE 0.5382961630821228, LKD 5.9485273361206055, LDE 0.0, LReg 0.0, POD 9.951959609985352 EntMin 0.0
Epoch 2, Batch 30/102, Loss=16.483857142925263
Loss made of: CE 0.5596386194229126, LKD 6.105086803436279, LDE 0.0, LReg 0.0, POD 10.418172836303711 EntMin 0.0
Epoch 2, Batch 40/102, Loss=16.570746314525604
Loss made of: CE 0.4640600085258484, LKD 5.585089683532715, LDE 0.0, LReg 0.0, POD 10.793595314025879 EntMin 0.0
Epoch 2, Batch 50/102, Loss=16.502651578187944
Loss made of: CE 0.40781745314598083, LKD 5.115869522094727, LDE 0.0, LReg 0.0, POD 9.441145896911621 EntMin 0.0
Epoch 2, Batch 60/102, Loss=16.416020238399504
Loss made of: CE 0.5511074066162109, LKD 5.5233154296875, LDE 0.0, LReg 0.0, POD 8.999252319335938 EntMin 0.0
Epoch 2, Batch 70/102, Loss=16.806234100461005
Loss made of: CE 0.4985068738460541, LKD 5.94639778137207, LDE 0.0, LReg 0.0, POD 11.319684982299805 EntMin 0.0
Epoch 2, Batch 80/102, Loss=15.96396054327488
Loss made of: CE 0.43624386191368103, LKD 5.57672643661499, LDE 0.0, LReg 0.0, POD 8.898787498474121 EntMin 0.0
Epoch 2, Batch 90/102, Loss=16.42898777127266
Loss made of: CE 0.4164636731147766, LKD 5.3475871086120605, LDE 0.0, LReg 0.0, POD 9.644740104675293 EntMin 0.0
Epoch 2, Batch 100/102, Loss=16.10439985692501
Loss made of: CE 0.37409669160842896, LKD 5.514642238616943, LDE 0.0, LReg 0.0, POD 10.306970596313477 EntMin 0.0
Epoch 2, Class Loss=0.5274976491928101, Reg Loss=5.941647052764893
Clinet index 7, End of Epoch 2/6, Average Loss=6.469144821166992, Class Loss=0.5274976491928101, Reg Loss=5.941647052764893
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=16.825558882951736
Loss made of: CE 0.5563831925392151, LKD 5.663360595703125, LDE 0.0, LReg 0.0, POD 9.098400115966797 EntMin 0.0
Epoch 3, Batch 20/102, Loss=15.845338305830955
Loss made of: CE 0.5410765409469604, LKD 6.41818904876709, LDE 0.0, LReg 0.0, POD 8.49326229095459 EntMin 0.0
Epoch 3, Batch 30/102, Loss=15.851052388548851
Loss made of: CE 0.45336711406707764, LKD 6.144162178039551, LDE 0.0, LReg 0.0, POD 9.55811595916748 EntMin 0.0
Epoch 3, Batch 40/102, Loss=15.935233440995216
Loss made of: CE 0.38167810440063477, LKD 4.8345232009887695, LDE 0.0, LReg 0.0, POD 9.556407928466797 EntMin 0.0
Epoch 3, Batch 50/102, Loss=16.2796782463789
Loss made of: CE 0.4094749391078949, LKD 5.60366153717041, LDE 0.0, LReg 0.0, POD 10.79371452331543 EntMin 0.0
Epoch 3, Batch 60/102, Loss=15.72276342511177
Loss made of: CE 0.35585543513298035, LKD 6.2888946533203125, LDE 0.0, LReg 0.0, POD 8.965644836425781 EntMin 0.0
Epoch 3, Batch 70/102, Loss=16.068802234530448
Loss made of: CE 0.3456048369407654, LKD 6.789004325866699, LDE 0.0, LReg 0.0, POD 9.31200885772705 EntMin 0.0
Epoch 3, Batch 80/102, Loss=15.573396545648574
Loss made of: CE 0.380166620016098, LKD 6.52921199798584, LDE 0.0, LReg 0.0, POD 8.993936538696289 EntMin 0.0
Epoch 3, Batch 90/102, Loss=16.57658116519451
Loss made of: CE 0.4291273057460785, LKD 6.362839698791504, LDE 0.0, LReg 0.0, POD 8.774255752563477 EntMin 0.0
Epoch 3, Batch 100/102, Loss=15.657281917333602
Loss made of: CE 0.3882256746292114, LKD 5.410851955413818, LDE 0.0, LReg 0.0, POD 9.42103385925293 EntMin 0.0
Epoch 3, Class Loss=0.459295392036438, Reg Loss=5.927602291107178
Clinet index 7, End of Epoch 3/6, Average Loss=6.386897563934326, Class Loss=0.459295392036438, Reg Loss=5.927602291107178
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=15.800379076600075
Loss made of: CE 0.4020960032939911, LKD 5.775191307067871, LDE 0.0, LReg 0.0, POD 9.323161125183105 EntMin 0.0
Epoch 4, Batch 20/102, Loss=15.755714043974876
Loss made of: CE 0.536711573600769, LKD 6.686415672302246, LDE 0.0, LReg 0.0, POD 9.890862464904785 EntMin 0.0
Epoch 4, Batch 30/102, Loss=16.32344987988472
Loss made of: CE 0.32484784722328186, LKD 5.760059356689453, LDE 0.0, LReg 0.0, POD 8.865945816040039 EntMin 0.0
Epoch 4, Batch 40/102, Loss=15.53873044848442
Loss made of: CE 0.4254406690597534, LKD 5.705599784851074, LDE 0.0, LReg 0.0, POD 9.19338607788086 EntMin 0.0
Epoch 4, Batch 50/102, Loss=15.588522788882255
Loss made of: CE 0.49473899602890015, LKD 6.362029075622559, LDE 0.0, LReg 0.0, POD 8.527754783630371 EntMin 0.0
Epoch 4, Batch 60/102, Loss=15.887166109681129
Loss made of: CE 0.5170621871948242, LKD 5.855729579925537, LDE 0.0, LReg 0.0, POD 9.725776672363281 EntMin 0.0
Epoch 4, Batch 70/102, Loss=15.480847188830376
Loss made of: CE 0.4062485098838806, LKD 6.068395137786865, LDE 0.0, LReg 0.0, POD 9.107423782348633 EntMin 0.0
Epoch 4, Batch 80/102, Loss=15.639647537469864
Loss made of: CE 0.4515308737754822, LKD 6.380434989929199, LDE 0.0, LReg 0.0, POD 9.081029891967773 EntMin 0.0
Epoch 4, Batch 90/102, Loss=15.772033512592316
Loss made of: CE 0.4496932923793793, LKD 5.168105125427246, LDE 0.0, LReg 0.0, POD 10.061718940734863 EntMin 0.0
Epoch 4, Batch 100/102, Loss=16.126845094561578
Loss made of: CE 0.32083725929260254, LKD 5.526946544647217, LDE 0.0, LReg 0.0, POD 9.290287017822266 EntMin 0.0
Epoch 4, Class Loss=0.422075480222702, Reg Loss=5.917489528656006
Clinet index 7, End of Epoch 4/6, Average Loss=6.339564800262451, Class Loss=0.422075480222702, Reg Loss=5.917489528656006
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=15.649278736114502
Loss made of: CE 0.375227153301239, LKD 6.312534332275391, LDE 0.0, LReg 0.0, POD 9.567856788635254 EntMin 0.0
Epoch 5, Batch 20/102, Loss=15.608187037706376
Loss made of: CE 0.3526577949523926, LKD 5.635095119476318, LDE 0.0, LReg 0.0, POD 9.509871482849121 EntMin 0.0
Epoch 5, Batch 30/102, Loss=15.750942143797875
Loss made of: CE 0.5006440877914429, LKD 6.036689758300781, LDE 0.0, LReg 0.0, POD 8.934893608093262 EntMin 0.0
Epoch 5, Batch 40/102, Loss=15.657820600271226
Loss made of: CE 0.4104585647583008, LKD 6.401229381561279, LDE 0.0, LReg 0.0, POD 9.192851066589355 EntMin 0.0
Epoch 5, Batch 50/102, Loss=15.295058357715607
Loss made of: CE 0.49628329277038574, LKD 6.095497131347656, LDE 0.0, LReg 0.0, POD 9.72550106048584 EntMin 0.0
Epoch 5, Batch 60/102, Loss=15.403229960799218
Loss made of: CE 0.3887465000152588, LKD 6.463241100311279, LDE 0.0, LReg 0.0, POD 8.347922325134277 EntMin 0.0
Epoch 5, Batch 70/102, Loss=15.666599422693253
Loss made of: CE 0.3761387765407562, LKD 5.159877300262451, LDE 0.0, LReg 0.0, POD 9.687828063964844 EntMin 0.0
Epoch 5, Batch 80/102, Loss=15.874981689453126
Loss made of: CE 0.3424752652645111, LKD 5.6068115234375, LDE 0.0, LReg 0.0, POD 9.692428588867188 EntMin 0.0
Epoch 5, Batch 90/102, Loss=15.817606940865517
Loss made of: CE 0.577377200126648, LKD 7.087008476257324, LDE 0.0, LReg 0.0, POD 8.966497421264648 EntMin 0.0
Epoch 5, Batch 100/102, Loss=15.322815147042274
Loss made of: CE 0.37660497426986694, LKD 5.555473327636719, LDE 0.0, LReg 0.0, POD 9.966085433959961 EntMin 0.0
Epoch 5, Class Loss=0.4060887098312378, Reg Loss=5.923851490020752
Clinet index 7, End of Epoch 5/6, Average Loss=6.329940319061279, Class Loss=0.4060887098312378, Reg Loss=5.923851490020752
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=15.902938887476921
Loss made of: CE 0.45591044425964355, LKD 6.332426071166992, LDE 0.0, LReg 0.0, POD 7.800753593444824 EntMin 0.0
Epoch 6, Batch 20/102, Loss=16.215865349769594
Loss made of: CE 0.36696910858154297, LKD 6.7589006423950195, LDE 0.0, LReg 0.0, POD 8.659772872924805 EntMin 0.0
Epoch 6, Batch 30/102, Loss=15.411896422505379
Loss made of: CE 0.43585366010665894, LKD 6.321038722991943, LDE 0.0, LReg 0.0, POD 8.46628475189209 EntMin 0.0
Epoch 6, Batch 40/102, Loss=16.135766518115997
Loss made of: CE 0.39213311672210693, LKD 6.093977928161621, LDE 0.0, LReg 0.0, POD 8.356266021728516 EntMin 0.0
Epoch 6, Batch 50/102, Loss=15.236672306060791
Loss made of: CE 0.5005308389663696, LKD 5.328007698059082, LDE 0.0, LReg 0.0, POD 8.62669563293457 EntMin 0.0
Epoch 6, Batch 60/102, Loss=15.046503368020058
Loss made of: CE 0.31490790843963623, LKD 5.697177410125732, LDE 0.0, LReg 0.0, POD 8.824511528015137 EntMin 0.0
Epoch 6, Batch 70/102, Loss=15.14510155916214
Loss made of: CE 0.3624808192253113, LKD 6.0405988693237305, LDE 0.0, LReg 0.0, POD 9.352167129516602 EntMin 0.0
Epoch 6, Batch 80/102, Loss=15.309150421619416
Loss made of: CE 0.3301863968372345, LKD 6.194844722747803, LDE 0.0, LReg 0.0, POD 9.271236419677734 EntMin 0.0
Epoch 6, Batch 90/102, Loss=15.25338555574417
Loss made of: CE 0.3623945116996765, LKD 6.148946285247803, LDE 0.0, LReg 0.0, POD 8.889809608459473 EntMin 0.0
Epoch 6, Batch 100/102, Loss=15.28773399591446
Loss made of: CE 0.4856587052345276, LKD 6.100339889526367, LDE 0.0, LReg 0.0, POD 8.753772735595703 EntMin 0.0
Epoch 6, Class Loss=0.393096923828125, Reg Loss=5.961185932159424
Clinet index 7, End of Epoch 6/6, Average Loss=6.354282855987549, Class Loss=0.393096923828125, Reg Loss=5.961185932159424
federated aggregation...
Validation, Class Loss=0.64035564661026, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.802524
Mean Acc: 0.463472
FreqW Acc: 0.679088
Mean IoU: 0.322677
Class IoU:
	class 0: 0.8382322
	class 1: 0.66144514
	class 2: 0.244185
	class 3: 0.45203292
	class 4: 0.5390256
	class 5: 0.07158683
	class 6: 0.6628498
	class 7: 0.4958753
	class 8: 0.3953036
	class 9: 0.0028267035
	class 10: 0.3146867
	class 11: 0.25715688
	class 12: 0.02196087
	class 13: 0.0
	class 14: 0.2378228
	class 15: 0.29052135
	class 16: 0.0
Class Acc:
	class 0: 0.9638733
	class 1: 0.67130107
	class 2: 0.61352533
	class 3: 0.9041071
	class 4: 0.86290413
	class 5: 0.072076045
	class 6: 0.69865394
	class 7: 0.7294446
	class 8: 0.83119226
	class 9: 0.0028316746
	class 10: 0.4277895
	class 11: 0.4858023
	class 12: 0.022004979
	class 13: 0.0
	class 14: 0.29702652
	class 15: 0.2964959
	class 16: 0.0

federated global round: 16, step: 3
select part of clients to conduct local training
[7, 11, 16, 18]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=15.885379794239999
Loss made of: CE 0.6074971556663513, LKD 6.482388973236084, LDE 0.0, LReg 0.0, POD 7.969949722290039 EntMin 0.0
Epoch 1, Batch 20/102, Loss=14.933534276485442
Loss made of: CE 0.5004308223724365, LKD 6.040648460388184, LDE 0.0, LReg 0.0, POD 8.81171989440918 EntMin 0.0
Epoch 1, Batch 30/102, Loss=15.293128433823586
Loss made of: CE 0.4376310706138611, LKD 5.62787389755249, LDE 0.0, LReg 0.0, POD 9.268383026123047 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 40/102, Loss=15.839046198129655
Loss made of: CE 0.3708765506744385, LKD 6.655814170837402, LDE 0.0, LReg 0.0, POD 9.10518741607666 EntMin 0.0
Epoch 1, Batch 50/102, Loss=15.931699758768081
Loss made of: CE 0.510368824005127, LKD 6.191798686981201, LDE 0.0, LReg 0.0, POD 10.305184364318848 EntMin 0.0
Epoch 1, Batch 60/102, Loss=15.410248497128487
Loss made of: CE 0.43019795417785645, LKD 5.43625020980835, LDE 0.0, LReg 0.0, POD 8.345094680786133 EntMin 0.0
Epoch 1, Batch 70/102, Loss=15.471747958660126
Loss made of: CE 0.49637889862060547, LKD 5.749476432800293, LDE 0.0, LReg 0.0, POD 8.8818941116333 EntMin 0.0
Epoch 1, Batch 80/102, Loss=15.717943516373634
Loss made of: CE 0.5839663743972778, LKD 7.0394606590271, LDE 0.0, LReg 0.0, POD 8.420003890991211 EntMin 0.0
Epoch 1, Batch 90/102, Loss=14.722640955448151
Loss made of: CE 0.4130793511867523, LKD 5.456721782684326, LDE 0.0, LReg 0.0, POD 7.723904132843018 EntMin 0.0
Epoch 1, Batch 100/102, Loss=15.650999507308006
Loss made of: CE 0.4783427119255066, LKD 5.5836501121521, LDE 0.0, LReg 0.0, POD 8.672107696533203 EntMin 0.0
Epoch 1, Class Loss=0.4727493226528168, Reg Loss=5.930017471313477
Clinet index 7, End of Epoch 1/6, Average Loss=6.402766704559326, Class Loss=0.4727493226528168, Reg Loss=5.930017471313477
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=15.549432510137558
Loss made of: CE 0.5034000873565674, LKD 5.998406887054443, LDE 0.0, LReg 0.0, POD 8.34308910369873 EntMin 0.0
Epoch 2, Batch 20/102, Loss=15.307275116443634
Loss made of: CE 0.41264861822128296, LKD 6.143796920776367, LDE 0.0, LReg 0.0, POD 9.652485847473145 EntMin 0.0
Epoch 2, Batch 30/102, Loss=15.682355210185051
Loss made of: CE 0.41273677349090576, LKD 6.040199279785156, LDE 0.0, LReg 0.0, POD 10.721007347106934 EntMin 0.0
Epoch 2, Batch 40/102, Loss=15.613049519062042
Loss made of: CE 0.40103673934936523, LKD 5.802445888519287, LDE 0.0, LReg 0.0, POD 10.093769073486328 EntMin 0.0
Epoch 2, Batch 50/102, Loss=15.431628981232643
Loss made of: CE 0.3254956007003784, LKD 5.04451322555542, LDE 0.0, LReg 0.0, POD 8.343550682067871 EntMin 0.0
Epoch 2, Batch 60/102, Loss=15.483426836133003
Loss made of: CE 0.413383424282074, LKD 5.321438312530518, LDE 0.0, LReg 0.0, POD 7.693465709686279 EntMin 0.0
Epoch 2, Batch 70/102, Loss=15.69893271625042
Loss made of: CE 0.4116894602775574, LKD 5.367443084716797, LDE 0.0, LReg 0.0, POD 10.162832260131836 EntMin 0.0
Epoch 2, Batch 80/102, Loss=14.890378087759018
Loss made of: CE 0.36591362953186035, LKD 5.510615348815918, LDE 0.0, LReg 0.0, POD 8.50759220123291 EntMin 0.0
Epoch 2, Batch 90/102, Loss=15.461776575446128
Loss made of: CE 0.33780425786972046, LKD 5.415133476257324, LDE 0.0, LReg 0.0, POD 8.84526252746582 EntMin 0.0
Epoch 2, Batch 100/102, Loss=15.163679483532906
Loss made of: CE 0.31996607780456543, LKD 5.5125732421875, LDE 0.0, LReg 0.0, POD 9.572738647460938 EntMin 0.0
Epoch 2, Class Loss=0.4194593131542206, Reg Loss=5.926748275756836
Clinet index 7, End of Epoch 2/6, Average Loss=6.346207618713379, Class Loss=0.4194593131542206, Reg Loss=5.926748275756836
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=15.833923155069352
Loss made of: CE 0.469723105430603, LKD 5.891773223876953, LDE 0.0, LReg 0.0, POD 8.843931198120117 EntMin 0.0
Epoch 3, Batch 20/102, Loss=15.13056439757347
Loss made of: CE 0.4139058589935303, LKD 6.333489894866943, LDE 0.0, LReg 0.0, POD 7.873546600341797 EntMin 0.0
Epoch 3, Batch 30/102, Loss=15.07570738196373
Loss made of: CE 0.38831889629364014, LKD 6.234683036804199, LDE 0.0, LReg 0.0, POD 8.818934440612793 EntMin 0.0
Epoch 3, Batch 40/102, Loss=15.261242008209228
Loss made of: CE 0.31922364234924316, LKD 5.051392078399658, LDE 0.0, LReg 0.0, POD 8.583135604858398 EntMin 0.0
Epoch 3, Batch 50/102, Loss=15.334209397435188
Loss made of: CE 0.360233873128891, LKD 5.769321441650391, LDE 0.0, LReg 0.0, POD 9.981988906860352 EntMin 0.0
Epoch 3, Batch 60/102, Loss=14.819841423630715
Loss made of: CE 0.29616862535476685, LKD 6.261089324951172, LDE 0.0, LReg 0.0, POD 8.282451629638672 EntMin 0.0
Epoch 3, Batch 70/102, Loss=15.297608861327172
Loss made of: CE 0.32877275347709656, LKD 6.802406311035156, LDE 0.0, LReg 0.0, POD 8.398941040039062 EntMin 0.0
Epoch 3, Batch 80/102, Loss=14.909888422489166
Loss made of: CE 0.34422600269317627, LKD 6.66165828704834, LDE 0.0, LReg 0.0, POD 8.60300064086914 EntMin 0.0
Epoch 3, Batch 90/102, Loss=15.407439675927161
Loss made of: CE 0.4095548987388611, LKD 6.498505592346191, LDE 0.0, LReg 0.0, POD 8.523941040039062 EntMin 0.0
Epoch 3, Batch 100/102, Loss=14.790792086720467
Loss made of: CE 0.3275795876979828, LKD 5.433032989501953, LDE 0.0, LReg 0.0, POD 8.200150489807129 EntMin 0.0
Epoch 3, Class Loss=0.4001254737377167, Reg Loss=5.92968225479126
Clinet index 7, End of Epoch 3/6, Average Loss=6.329807758331299, Class Loss=0.4001254737377167, Reg Loss=5.92968225479126
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=14.930322563648224
Loss made of: CE 0.3646637797355652, LKD 5.869937896728516, LDE 0.0, LReg 0.0, POD 8.609586715698242 EntMin 0.0
Epoch 4, Batch 20/102, Loss=15.254021430015564
Loss made of: CE 0.48669323325157166, LKD 6.773702144622803, LDE 0.0, LReg 0.0, POD 8.882328987121582 EntMin 0.0
Epoch 4, Batch 30/102, Loss=15.550752764940261
Loss made of: CE 0.29183724522590637, LKD 5.531259059906006, LDE 0.0, LReg 0.0, POD 8.095670700073242 EntMin 0.0
Epoch 4, Batch 40/102, Loss=14.939079388976097
Loss made of: CE 0.37539786100387573, LKD 5.529630661010742, LDE 0.0, LReg 0.0, POD 9.064287185668945 EntMin 0.0
Epoch 4, Batch 50/102, Loss=14.827641454339027
Loss made of: CE 0.44026267528533936, LKD 5.979001045227051, LDE 0.0, LReg 0.0, POD 8.290525436401367 EntMin 0.0
Epoch 4, Batch 60/102, Loss=15.212243521213532
Loss made of: CE 0.4400638937950134, LKD 5.996622085571289, LDE 0.0, LReg 0.0, POD 8.763297080993652 EntMin 0.0
Epoch 4, Batch 70/102, Loss=14.889806070923806
Loss made of: CE 0.36952275037765503, LKD 5.650969505310059, LDE 0.0, LReg 0.0, POD 8.261606216430664 EntMin 0.0
Epoch 4, Batch 80/102, Loss=15.276334500312805
Loss made of: CE 0.3993821144104004, LKD 6.376926422119141, LDE 0.0, LReg 0.0, POD 8.02780532836914 EntMin 0.0
Epoch 4, Batch 90/102, Loss=15.081706160306931
Loss made of: CE 0.37290510535240173, LKD 5.346734046936035, LDE 0.0, LReg 0.0, POD 9.239441871643066 EntMin 0.0
Epoch 4, Batch 100/102, Loss=15.398351216316223
Loss made of: CE 0.2896270155906677, LKD 5.491258144378662, LDE 0.0, LReg 0.0, POD 8.683955192565918 EntMin 0.0
Epoch 4, Class Loss=0.38839828968048096, Reg Loss=5.895196914672852
Clinet index 7, End of Epoch 4/6, Average Loss=6.283595085144043, Class Loss=0.38839828968048096, Reg Loss=5.895196914672852
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=15.054435515403748
Loss made of: CE 0.3899948000907898, LKD 6.528613567352295, LDE 0.0, LReg 0.0, POD 9.089591979980469 EntMin 0.0
Epoch 5, Batch 20/102, Loss=14.972955492138862
Loss made of: CE 0.3124576210975647, LKD 5.663142681121826, LDE 0.0, LReg 0.0, POD 8.477264404296875 EntMin 0.0
Epoch 5, Batch 30/102, Loss=15.050776255130767
Loss made of: CE 0.45885008573532104, LKD 6.093735694885254, LDE 0.0, LReg 0.0, POD 8.960973739624023 EntMin 0.0
Epoch 5, Batch 40/102, Loss=15.254036372900009
Loss made of: CE 0.3964858055114746, LKD 6.18044376373291, LDE 0.0, LReg 0.0, POD 8.400812149047852 EntMin 0.0
Epoch 5, Batch 50/102, Loss=14.64389672279358
Loss made of: CE 0.47594255208969116, LKD 5.9956374168396, LDE 0.0, LReg 0.0, POD 8.71705436706543 EntMin 0.0
Epoch 5, Batch 60/102, Loss=14.884310480952262
Loss made of: CE 0.37740474939346313, LKD 6.720269203186035, LDE 0.0, LReg 0.0, POD 7.965907096862793 EntMin 0.0
Epoch 5, Batch 70/102, Loss=15.261116287112236
Loss made of: CE 0.37361884117126465, LKD 5.3045220375061035, LDE 0.0, LReg 0.0, POD 9.599044799804688 EntMin 0.0
Epoch 5, Batch 80/102, Loss=15.14609861075878
Loss made of: CE 0.3140314519405365, LKD 5.771634578704834, LDE 0.0, LReg 0.0, POD 9.091381072998047 EntMin 0.0
Epoch 5, Batch 90/102, Loss=15.283600747585297
Loss made of: CE 0.559053361415863, LKD 6.805461406707764, LDE 0.0, LReg 0.0, POD 8.373477935791016 EntMin 0.0
Epoch 5, Batch 100/102, Loss=14.607062366604804
Loss made of: CE 0.3297894299030304, LKD 5.710229396820068, LDE 0.0, LReg 0.0, POD 9.114007949829102 EntMin 0.0
Epoch 5, Class Loss=0.38070201873779297, Reg Loss=5.911865234375
Clinet index 7, End of Epoch 5/6, Average Loss=6.292567253112793, Class Loss=0.38070201873779297, Reg Loss=5.911865234375
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=15.39588475227356
Loss made of: CE 0.42490968108177185, LKD 6.084859371185303, LDE 0.0, LReg 0.0, POD 7.6191792488098145 EntMin 0.0
Epoch 6, Batch 20/102, Loss=15.34657938182354
Loss made of: CE 0.35487645864486694, LKD 6.826735973358154, LDE 0.0, LReg 0.0, POD 7.5676469802856445 EntMin 0.0
Epoch 6, Batch 30/102, Loss=14.735849145054818
Loss made of: CE 0.3933514952659607, LKD 6.322025299072266, LDE 0.0, LReg 0.0, POD 7.779107570648193 EntMin 0.0
Epoch 6, Batch 40/102, Loss=15.367029863595963
Loss made of: CE 0.36813148856163025, LKD 5.671067714691162, LDE 0.0, LReg 0.0, POD 7.722161769866943 EntMin 0.0
Epoch 6, Batch 50/102, Loss=14.680986449122429
Loss made of: CE 0.4689767062664032, LKD 5.58688497543335, LDE 0.0, LReg 0.0, POD 8.341286659240723 EntMin 0.0
Epoch 6, Batch 60/102, Loss=14.618270388245582
Loss made of: CE 0.29381856322288513, LKD 5.693118572235107, LDE 0.0, LReg 0.0, POD 8.335583686828613 EntMin 0.0
Epoch 6, Batch 70/102, Loss=14.38723241686821
Loss made of: CE 0.36922797560691833, LKD 6.273833274841309, LDE 0.0, LReg 0.0, POD 8.285152435302734 EntMin 0.0
Epoch 6, Batch 80/102, Loss=14.907367867231368
Loss made of: CE 0.323208212852478, LKD 6.2901611328125, LDE 0.0, LReg 0.0, POD 9.02060317993164 EntMin 0.0
Epoch 6, Batch 90/102, Loss=14.770962822437287
Loss made of: CE 0.3120717704296112, LKD 6.056587219238281, LDE 0.0, LReg 0.0, POD 8.855148315429688 EntMin 0.0
Epoch 6, Batch 100/102, Loss=14.770210328698159
Loss made of: CE 0.4615461230278015, LKD 6.26678466796875, LDE 0.0, LReg 0.0, POD 8.46619701385498 EntMin 0.0
Epoch 6, Class Loss=0.3721575438976288, Reg Loss=5.909905910491943
Clinet index 7, End of Epoch 6/6, Average Loss=6.2820634841918945, Class Loss=0.3721575438976288, Reg Loss=5.909905910491943
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=15.413920125365257
Loss made of: CE 0.5243019461631775, LKD 5.314063549041748, LDE 0.0, LReg 0.0, POD 11.09609603881836 EntMin 0.0
Epoch 1, Batch 20/102, Loss=16.10080571770668
Loss made of: CE 0.45618531107902527, LKD 5.313815593719482, LDE 0.0, LReg 0.0, POD 9.871352195739746 EntMin 0.0
Epoch 1, Batch 30/102, Loss=15.640221685171127
Loss made of: CE 0.5087642669677734, LKD 6.8057966232299805, LDE 0.0, LReg 0.0, POD 9.900154113769531 EntMin 0.0
Epoch 1, Batch 40/102, Loss=16.034414595365526
Loss made of: CE 0.38963496685028076, LKD 5.63931941986084, LDE 0.0, LReg 0.0, POD 9.250431060791016 EntMin 0.0
Epoch 1, Batch 50/102, Loss=15.635146591067315
Loss made of: CE 0.34955617785453796, LKD 4.870736598968506, LDE 0.0, LReg 0.0, POD 8.830314636230469 EntMin 0.0
Epoch 1, Batch 60/102, Loss=15.771376726031303
Loss made of: CE 0.4385894238948822, LKD 5.374974727630615, LDE 0.0, LReg 0.0, POD 8.222671508789062 EntMin 0.0
Epoch 1, Batch 70/102, Loss=15.294153413176536
Loss made of: CE 0.42183995246887207, LKD 5.525164604187012, LDE 0.0, LReg 0.0, POD 8.75436019897461 EntMin 0.0
Epoch 1, Batch 80/102, Loss=15.409584307670594
Loss made of: CE 0.5107983946800232, LKD 5.81430721282959, LDE 0.0, LReg 0.0, POD 9.035148620605469 EntMin 0.0
Epoch 1, Batch 90/102, Loss=14.769704735279083
Loss made of: CE 0.41603732109069824, LKD 6.555802822113037, LDE 0.0, LReg 0.0, POD 8.36088752746582 EntMin 0.0
Epoch 1, Batch 100/102, Loss=15.086337786912917
Loss made of: CE 0.46805691719055176, LKD 6.931333065032959, LDE 0.0, LReg 0.0, POD 9.42561149597168 EntMin 0.0
Epoch 1, Class Loss=0.47252389788627625, Reg Loss=5.893324375152588
Clinet index 11, End of Epoch 1/6, Average Loss=6.365848064422607, Class Loss=0.47252389788627625, Reg Loss=5.893324375152588
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=15.33511574268341
Loss made of: CE 0.4319751262664795, LKD 5.359016418457031, LDE 0.0, LReg 0.0, POD 8.668591499328613 EntMin 0.0
Epoch 2, Batch 20/102, Loss=14.868222546577453
Loss made of: CE 0.37049850821495056, LKD 5.539920806884766, LDE 0.0, LReg 0.0, POD 9.071990966796875 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 30/102, Loss=15.410754474997521
Loss made of: CE 0.5235021114349365, LKD 6.395610332489014, LDE 0.0, LReg 0.0, POD 8.42411994934082 EntMin 0.0
Epoch 2, Batch 40/102, Loss=15.16778014600277
Loss made of: CE 0.5623707175254822, LKD 6.284919738769531, LDE 0.0, LReg 0.0, POD 8.431744575500488 EntMin 0.0
Epoch 2, Batch 50/102, Loss=15.197363927960396
Loss made of: CE 0.4131639003753662, LKD 6.269329071044922, LDE 0.0, LReg 0.0, POD 8.822275161743164 EntMin 0.0
Epoch 2, Batch 60/102, Loss=15.448518547415734
Loss made of: CE 0.38613438606262207, LKD 6.560178279876709, LDE 0.0, LReg 0.0, POD 8.42718505859375 EntMin 0.0
Epoch 2, Batch 70/102, Loss=14.616612529754638
Loss made of: CE 0.3829426169395447, LKD 6.710407257080078, LDE 0.0, LReg 0.0, POD 8.097798347473145 EntMin 0.0
Epoch 2, Batch 80/102, Loss=15.570376485586166
Loss made of: CE 0.33463606238365173, LKD 7.158595085144043, LDE 0.0, LReg 0.0, POD 10.62085247039795 EntMin 0.0
Epoch 2, Batch 90/102, Loss=15.035105687379836
Loss made of: CE 0.5420085191726685, LKD 6.379356384277344, LDE 0.0, LReg 0.0, POD 10.06636905670166 EntMin 0.0
Epoch 2, Batch 100/102, Loss=15.276315754652023
Loss made of: CE 0.3822000026702881, LKD 6.040266990661621, LDE 0.0, LReg 0.0, POD 9.523346900939941 EntMin 0.0
Epoch 2, Class Loss=0.41966429352760315, Reg Loss=5.901431560516357
Clinet index 11, End of Epoch 2/6, Average Loss=6.321095943450928, Class Loss=0.41966429352760315, Reg Loss=5.901431560516357
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=15.222761201858521
Loss made of: CE 0.47508054971694946, LKD 6.502553462982178, LDE 0.0, LReg 0.0, POD 8.194994926452637 EntMin 0.0
Epoch 3, Batch 20/102, Loss=14.89754419028759
Loss made of: CE 0.517951250076294, LKD 6.345514297485352, LDE 0.0, LReg 0.0, POD 7.900007724761963 EntMin 0.0
Epoch 3, Batch 30/102, Loss=14.806838530302048
Loss made of: CE 0.35466092824935913, LKD 6.017337322235107, LDE 0.0, LReg 0.0, POD 8.992607116699219 EntMin 0.0
Epoch 3, Batch 40/102, Loss=15.12132993042469
Loss made of: CE 0.3203194737434387, LKD 6.5754594802856445, LDE 0.0, LReg 0.0, POD 9.27836799621582 EntMin 0.0
Epoch 3, Batch 50/102, Loss=15.28474086523056
Loss made of: CE 0.48902881145477295, LKD 6.764631271362305, LDE 0.0, LReg 0.0, POD 8.3493070602417 EntMin 0.0
Epoch 3, Batch 60/102, Loss=15.22551670372486
Loss made of: CE 0.33148881793022156, LKD 5.460902214050293, LDE 0.0, LReg 0.0, POD 8.384286880493164 EntMin 0.0
Epoch 3, Batch 70/102, Loss=14.964434018731117
Loss made of: CE 0.33021873235702515, LKD 5.2728118896484375, LDE 0.0, LReg 0.0, POD 9.229264259338379 EntMin 0.0
Epoch 3, Batch 80/102, Loss=15.284048196673393
Loss made of: CE 0.33953937888145447, LKD 5.746481895446777, LDE 0.0, LReg 0.0, POD 9.598644256591797 EntMin 0.0
Epoch 3, Batch 90/102, Loss=15.358853963017463
Loss made of: CE 0.4322184920310974, LKD 5.295487403869629, LDE 0.0, LReg 0.0, POD 8.524406433105469 EntMin 0.0
Epoch 3, Batch 100/102, Loss=15.291175821423531
Loss made of: CE 0.3598030209541321, LKD 5.692675590515137, LDE 0.0, LReg 0.0, POD 7.8703718185424805 EntMin 0.0
Epoch 3, Class Loss=0.39926114678382874, Reg Loss=5.884475231170654
Clinet index 11, End of Epoch 3/6, Average Loss=6.283736228942871, Class Loss=0.39926114678382874, Reg Loss=5.884475231170654
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=15.01818426847458
Loss made of: CE 0.4144167900085449, LKD 5.104400634765625, LDE 0.0, LReg 0.0, POD 8.126286506652832 EntMin 0.0
Epoch 4, Batch 20/102, Loss=15.105875718593598
Loss made of: CE 0.314561665058136, LKD 5.049561500549316, LDE 0.0, LReg 0.0, POD 9.119902610778809 EntMin 0.0
Epoch 4, Batch 30/102, Loss=15.136304572224617
Loss made of: CE 0.5370644927024841, LKD 6.062051296234131, LDE 0.0, LReg 0.0, POD 8.717032432556152 EntMin 0.0
Epoch 4, Batch 40/102, Loss=14.942968520522118
Loss made of: CE 0.36300987005233765, LKD 5.530595302581787, LDE 0.0, LReg 0.0, POD 8.20455551147461 EntMin 0.0
Epoch 4, Batch 50/102, Loss=15.08933919966221
Loss made of: CE 0.49112242460250854, LKD 6.082797050476074, LDE 0.0, LReg 0.0, POD 8.762092590332031 EntMin 0.0
Epoch 4, Batch 60/102, Loss=15.300560167431831
Loss made of: CE 0.39972853660583496, LKD 5.279275417327881, LDE 0.0, LReg 0.0, POD 8.686033248901367 EntMin 0.0
Epoch 4, Batch 70/102, Loss=15.244969210028648
Loss made of: CE 0.4855845868587494, LKD 6.657613754272461, LDE 0.0, LReg 0.0, POD 8.599883079528809 EntMin 0.0
Epoch 4, Batch 80/102, Loss=15.209653744101525
Loss made of: CE 0.45066237449645996, LKD 6.060549736022949, LDE 0.0, LReg 0.0, POD 11.169414520263672 EntMin 0.0
Epoch 4, Batch 90/102, Loss=15.366585364937782
Loss made of: CE 0.34473881125450134, LKD 6.172204494476318, LDE 0.0, LReg 0.0, POD 8.37600326538086 EntMin 0.0
Epoch 4, Batch 100/102, Loss=15.047248220443725
Loss made of: CE 0.48643073439598083, LKD 6.2969207763671875, LDE 0.0, LReg 0.0, POD 9.912405967712402 EntMin 0.0
Epoch 4, Class Loss=0.39022397994995117, Reg Loss=5.871057987213135
Clinet index 11, End of Epoch 4/6, Average Loss=6.261281967163086, Class Loss=0.39022397994995117, Reg Loss=5.871057987213135
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=15.183030998706817
Loss made of: CE 0.35992759466171265, LKD 6.316222667694092, LDE 0.0, LReg 0.0, POD 8.321060180664062 EntMin 0.0
Epoch 5, Batch 20/102, Loss=15.258956852555276
Loss made of: CE 0.2703147530555725, LKD 6.186631679534912, LDE 0.0, LReg 0.0, POD 9.465965270996094 EntMin 0.0
Epoch 5, Batch 30/102, Loss=15.300849270820617
Loss made of: CE 0.4191838204860687, LKD 5.195502758026123, LDE 0.0, LReg 0.0, POD 8.885442733764648 EntMin 0.0
Epoch 5, Batch 40/102, Loss=14.757470479607582
Loss made of: CE 0.4491386413574219, LKD 5.804368495941162, LDE 0.0, LReg 0.0, POD 8.325380325317383 EntMin 0.0
Epoch 5, Batch 50/102, Loss=15.014362752437592
Loss made of: CE 0.336261123418808, LKD 5.555474758148193, LDE 0.0, LReg 0.0, POD 9.736818313598633 EntMin 0.0
Epoch 5, Batch 60/102, Loss=14.90576778948307
Loss made of: CE 0.39356979727745056, LKD 7.069728851318359, LDE 0.0, LReg 0.0, POD 8.96325397491455 EntMin 0.0
Epoch 5, Batch 70/102, Loss=14.177020305395127
Loss made of: CE 0.36443209648132324, LKD 5.9090728759765625, LDE 0.0, LReg 0.0, POD 7.985907554626465 EntMin 0.0
Epoch 5, Batch 80/102, Loss=14.596892580389977
Loss made of: CE 0.34925326704978943, LKD 5.57674503326416, LDE 0.0, LReg 0.0, POD 9.234997749328613 EntMin 0.0
Epoch 5, Batch 90/102, Loss=14.903324455022812
Loss made of: CE 0.29253455996513367, LKD 6.4874372482299805, LDE 0.0, LReg 0.0, POD 8.330495834350586 EntMin 0.0
Epoch 5, Batch 100/102, Loss=14.863106995821
Loss made of: CE 0.3639938235282898, LKD 5.9749908447265625, LDE 0.0, LReg 0.0, POD 8.647857666015625 EntMin 0.0
Epoch 5, Class Loss=0.3740791082382202, Reg Loss=5.853206634521484
Clinet index 11, End of Epoch 5/6, Average Loss=6.227285861968994, Class Loss=0.3740791082382202, Reg Loss=5.853206634521484
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=15.095585477352142
Loss made of: CE 0.3344135284423828, LKD 5.475537300109863, LDE 0.0, LReg 0.0, POD 8.007447242736816 EntMin 0.0
Epoch 6, Batch 20/102, Loss=14.695993429422378
Loss made of: CE 0.42514240741729736, LKD 6.798978805541992, LDE 0.0, LReg 0.0, POD 8.578658103942871 EntMin 0.0
Epoch 6, Batch 30/102, Loss=14.414539122581482
Loss made of: CE 0.2955623269081116, LKD 5.059243679046631, LDE 0.0, LReg 0.0, POD 8.912662506103516 EntMin 0.0
Epoch 6, Batch 40/102, Loss=15.239999854564667
Loss made of: CE 0.49324315786361694, LKD 5.849410057067871, LDE 0.0, LReg 0.0, POD 9.146203994750977 EntMin 0.0
Epoch 6, Batch 50/102, Loss=14.724729657173157
Loss made of: CE 0.41653093695640564, LKD 6.011877059936523, LDE 0.0, LReg 0.0, POD 8.077939987182617 EntMin 0.0
Epoch 6, Batch 60/102, Loss=15.03741580247879
Loss made of: CE 0.3881497085094452, LKD 6.874423027038574, LDE 0.0, LReg 0.0, POD 8.691823959350586 EntMin 0.0
Epoch 6, Batch 70/102, Loss=14.418297863006591
Loss made of: CE 0.33556851744651794, LKD 5.394330024719238, LDE 0.0, LReg 0.0, POD 8.1314697265625 EntMin 0.0
Epoch 6, Batch 80/102, Loss=14.431296643614768
Loss made of: CE 0.38158082962036133, LKD 7.508481979370117, LDE 0.0, LReg 0.0, POD 9.175262451171875 EntMin 0.0
Epoch 6, Batch 90/102, Loss=14.845382386446
Loss made of: CE 0.34492945671081543, LKD 5.467087745666504, LDE 0.0, LReg 0.0, POD 7.761002540588379 EntMin 0.0
Epoch 6, Batch 100/102, Loss=14.812155157327652
Loss made of: CE 0.40920811891555786, LKD 6.353219509124756, LDE 0.0, LReg 0.0, POD 8.506088256835938 EntMin 0.0
Epoch 6, Class Loss=0.3683110177516937, Reg Loss=5.877172946929932
Clinet index 11, End of Epoch 6/6, Average Loss=6.245483875274658, Class Loss=0.3683110177516937, Reg Loss=5.877172946929932
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/105, Loss=16.073485165834427
Loss made of: CE 0.7889999151229858, LKD 5.582676887512207, LDE 0.0, LReg 0.0, POD 11.598030090332031 EntMin 0.0
Epoch 1, Batch 20/105, Loss=15.40752594769001
Loss made of: CE 0.5720481872558594, LKD 5.730362892150879, LDE 0.0, LReg 0.0, POD 9.019706726074219 EntMin 0.0
Epoch 1, Batch 30/105, Loss=14.863599601387978
Loss made of: CE 0.5117562413215637, LKD 6.040493965148926, LDE 0.0, LReg 0.0, POD 9.563604354858398 EntMin 0.0
Epoch 1, Batch 40/105, Loss=15.568222323060036
Loss made of: CE 0.35321539640426636, LKD 5.243717193603516, LDE 0.0, LReg 0.0, POD 8.169855117797852 EntMin 0.0
Epoch 1, Batch 50/105, Loss=15.319199618697166
Loss made of: CE 0.4796038568019867, LKD 5.077337265014648, LDE 0.0, LReg 0.0, POD 8.165054321289062 EntMin 0.0
Epoch 1, Batch 60/105, Loss=15.140353617072105
Loss made of: CE 0.5702551603317261, LKD 7.427377223968506, LDE 0.0, LReg 0.0, POD 10.200798034667969 EntMin 0.0
Epoch 1, Batch 70/105, Loss=15.654040467739105
Loss made of: CE 0.46669864654541016, LKD 5.267099380493164, LDE 0.0, LReg 0.0, POD 10.316686630249023 EntMin 0.0
Epoch 1, Batch 80/105, Loss=15.704830688238143
Loss made of: CE 0.3519863486289978, LKD 5.052221775054932, LDE 0.0, LReg 0.0, POD 10.781103134155273 EntMin 0.0
Epoch 1, Batch 90/105, Loss=15.696197089552879
Loss made of: CE 0.4480806291103363, LKD 5.483023643493652, LDE 0.0, LReg 0.0, POD 8.330440521240234 EntMin 0.0
Epoch 1, Batch 100/105, Loss=15.126137122511864
Loss made of: CE 0.4056086540222168, LKD 5.600471496582031, LDE 0.0, LReg 0.0, POD 8.884496688842773 EntMin 0.0
Epoch 1, Class Loss=0.4773871600627899, Reg Loss=5.6961822509765625
Clinet index 16, End of Epoch 1/6, Average Loss=6.173569202423096, Class Loss=0.4773871600627899, Reg Loss=5.6961822509765625
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/105, Loss=15.089251706004143
Loss made of: CE 0.4703645706176758, LKD 5.29563570022583, LDE 0.0, LReg 0.0, POD 8.655619621276855 EntMin 0.0
Epoch 2, Batch 20/105, Loss=15.107897567749024
Loss made of: CE 0.4822870194911957, LKD 6.073054313659668, LDE 0.0, LReg 0.0, POD 9.68807601928711 EntMin 0.0
Epoch 2, Batch 30/105, Loss=14.980689409375191
Loss made of: CE 0.41063210368156433, LKD 6.401803016662598, LDE 0.0, LReg 0.0, POD 10.535611152648926 EntMin 0.0
Epoch 2, Batch 40/105, Loss=15.63686411678791
Loss made of: CE 0.31821173429489136, LKD 5.115738391876221, LDE 0.0, LReg 0.0, POD 9.035160064697266 EntMin 0.0
Epoch 2, Batch 50/105, Loss=15.035920131206513
Loss made of: CE 0.3602607250213623, LKD 6.028110504150391, LDE 0.0, LReg 0.0, POD 8.607635498046875 EntMin 0.0
Epoch 2, Batch 60/105, Loss=15.439047160744668
Loss made of: CE 0.33075273036956787, LKD 5.189741611480713, LDE 0.0, LReg 0.0, POD 9.012102127075195 EntMin 0.0
Epoch 2, Batch 70/105, Loss=15.758215042948724
Loss made of: CE 0.43932539224624634, LKD 6.3681488037109375, LDE 0.0, LReg 0.0, POD 9.422420501708984 EntMin 0.0
Epoch 2, Batch 80/105, Loss=15.602744862437248
Loss made of: CE 0.40748876333236694, LKD 5.15647029876709, LDE 0.0, LReg 0.0, POD 9.797769546508789 EntMin 0.0
Epoch 2, Batch 90/105, Loss=15.818581247329712
Loss made of: CE 0.35583066940307617, LKD 6.1333160400390625, LDE 0.0, LReg 0.0, POD 10.283687591552734 EntMin 0.0
Epoch 2, Batch 100/105, Loss=15.826647055149078
Loss made of: CE 0.3952504098415375, LKD 5.621871471405029, LDE 0.0, LReg 0.0, POD 9.824052810668945 EntMin 0.0
Epoch 2, Class Loss=0.4230343699455261, Reg Loss=5.68690824508667
Clinet index 16, End of Epoch 2/6, Average Loss=6.109942436218262, Class Loss=0.4230343699455261, Reg Loss=5.68690824508667
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/105, Loss=15.266993993520737
Loss made of: CE 0.3705079257488251, LKD 7.16796875, LDE 0.0, LReg 0.0, POD 9.085535049438477 EntMin 0.0
Epoch 3, Batch 20/105, Loss=15.362381511926651
Loss made of: CE 0.41228610277175903, LKD 5.891709804534912, LDE 0.0, LReg 0.0, POD 9.028995513916016 EntMin 0.0
Epoch 3, Batch 30/105, Loss=15.184865236282349
Loss made of: CE 0.3121264576911926, LKD 5.615561485290527, LDE 0.0, LReg 0.0, POD 9.366374969482422 EntMin 0.0
Epoch 3, Batch 40/105, Loss=15.584189543128014
Loss made of: CE 0.4608157277107239, LKD 5.683204174041748, LDE 0.0, LReg 0.0, POD 9.14993953704834 EntMin 0.0
Epoch 3, Batch 50/105, Loss=15.164300951361657
Loss made of: CE 0.33498138189315796, LKD 6.414725303649902, LDE 0.0, LReg 0.0, POD 9.571832656860352 EntMin 0.0
Epoch 3, Batch 60/105, Loss=15.6568568110466
Loss made of: CE 0.509847104549408, LKD 6.589122772216797, LDE 0.0, LReg 0.0, POD 9.135318756103516 EntMin 0.0
Epoch 3, Batch 70/105, Loss=14.97786104977131
Loss made of: CE 0.33929163217544556, LKD 4.812665939331055, LDE 0.0, LReg 0.0, POD 8.809249877929688 EntMin 0.0
Epoch 3, Batch 80/105, Loss=15.079809084534645
Loss made of: CE 0.4510605037212372, LKD 5.504852294921875, LDE 0.0, LReg 0.0, POD 8.923080444335938 EntMin 0.0
Epoch 3, Batch 90/105, Loss=15.388793617486954
Loss made of: CE 0.39568859338760376, LKD 6.278968334197998, LDE 0.0, LReg 0.0, POD 8.94521713256836 EntMin 0.0
Epoch 3, Batch 100/105, Loss=15.291677090525628
Loss made of: CE 0.43314582109451294, LKD 5.6251654624938965, LDE 0.0, LReg 0.0, POD 9.037378311157227 EntMin 0.0
Epoch 3, Class Loss=0.40328919887542725, Reg Loss=5.724645614624023
Clinet index 16, End of Epoch 3/6, Average Loss=6.12793493270874, Class Loss=0.40328919887542725, Reg Loss=5.724645614624023
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/105, Loss=14.817412871122361
Loss made of: CE 0.39311251044273376, LKD 4.812877178192139, LDE 0.0, LReg 0.0, POD 8.162668228149414 EntMin 0.0
Epoch 4, Batch 20/105, Loss=15.309725445508956
Loss made of: CE 0.3369210958480835, LKD 6.1461710929870605, LDE 0.0, LReg 0.0, POD 8.350337982177734 EntMin 0.0
Epoch 4, Batch 30/105, Loss=14.899265918135644
Loss made of: CE 0.3452135920524597, LKD 5.709266185760498, LDE 0.0, LReg 0.0, POD 8.63887882232666 EntMin 0.0
Epoch 4, Batch 40/105, Loss=15.21328586935997
Loss made of: CE 0.314919650554657, LKD 5.291225433349609, LDE 0.0, LReg 0.0, POD 8.214799880981445 EntMin 0.0
Epoch 4, Batch 50/105, Loss=15.13998538851738
Loss made of: CE 0.3883204460144043, LKD 5.50199031829834, LDE 0.0, LReg 0.0, POD 8.63276481628418 EntMin 0.0
Epoch 4, Batch 60/105, Loss=15.296521925926209
Loss made of: CE 0.4522733688354492, LKD 6.077546119689941, LDE 0.0, LReg 0.0, POD 8.57007122039795 EntMin 0.0
Epoch 4, Batch 70/105, Loss=15.354608982801437
Loss made of: CE 0.4117369055747986, LKD 5.268777847290039, LDE 0.0, LReg 0.0, POD 8.255644798278809 EntMin 0.0
Epoch 4, Batch 80/105, Loss=14.745417860150337
Loss made of: CE 0.40250492095947266, LKD 4.729551315307617, LDE 0.0, LReg 0.0, POD 8.676552772521973 EntMin 0.0
Epoch 4, Batch 90/105, Loss=14.81601454615593
Loss made of: CE 0.3745537996292114, LKD 5.034597873687744, LDE 0.0, LReg 0.0, POD 9.374629020690918 EntMin 0.0
Epoch 4, Batch 100/105, Loss=14.94985291659832
Loss made of: CE 0.3156370520591736, LKD 5.746755123138428, LDE 0.0, LReg 0.0, POD 8.073339462280273 EntMin 0.0
Epoch 4, Class Loss=0.3877548277378082, Reg Loss=5.66919469833374
Clinet index 16, End of Epoch 4/6, Average Loss=6.056949615478516, Class Loss=0.3877548277378082, Reg Loss=5.66919469833374
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/105, Loss=15.578526866436004
Loss made of: CE 0.3982836604118347, LKD 5.702118873596191, LDE 0.0, LReg 0.0, POD 9.9881591796875 EntMin 0.0
Epoch 5, Batch 20/105, Loss=15.1680783867836
Loss made of: CE 0.3994831442832947, LKD 5.640800476074219, LDE 0.0, LReg 0.0, POD 8.53674602508545 EntMin 0.0
Epoch 5, Batch 30/105, Loss=14.832434833049774
Loss made of: CE 0.3527548015117645, LKD 5.239378929138184, LDE 0.0, LReg 0.0, POD 7.973095417022705 EntMin 0.0
Epoch 5, Batch 40/105, Loss=15.530683168768883
Loss made of: CE 0.3706425130367279, LKD 6.124847412109375, LDE 0.0, LReg 0.0, POD 10.178670883178711 EntMin 0.0
Epoch 5, Batch 50/105, Loss=15.03608467578888
Loss made of: CE 0.5605748891830444, LKD 5.745268821716309, LDE 0.0, LReg 0.0, POD 11.710317611694336 EntMin 0.0
Epoch 5, Batch 60/105, Loss=14.835738337039947
Loss made of: CE 0.4066976010799408, LKD 5.395244598388672, LDE 0.0, LReg 0.0, POD 8.90416431427002 EntMin 0.0
Epoch 5, Batch 70/105, Loss=14.85062266588211
Loss made of: CE 0.30730047821998596, LKD 6.066245079040527, LDE 0.0, LReg 0.0, POD 8.799448013305664 EntMin 0.0
Epoch 5, Batch 80/105, Loss=14.934687042236328
Loss made of: CE 0.4441837668418884, LKD 6.192723274230957, LDE 0.0, LReg 0.0, POD 7.740190505981445 EntMin 0.0
Epoch 5, Batch 90/105, Loss=15.553713962435722
Loss made of: CE 0.38290804624557495, LKD 5.983180522918701, LDE 0.0, LReg 0.0, POD 8.494012832641602 EntMin 0.0
Epoch 5, Batch 100/105, Loss=15.271800696849823
Loss made of: CE 0.3635478913784027, LKD 4.999318599700928, LDE 0.0, LReg 0.0, POD 10.028024673461914 EntMin 0.0
Epoch 5, Class Loss=0.37994855642318726, Reg Loss=5.704172134399414
Clinet index 16, End of Epoch 5/6, Average Loss=6.084120750427246, Class Loss=0.37994855642318726, Reg Loss=5.704172134399414
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/105, Loss=14.694094935059548
Loss made of: CE 0.37011727690696716, LKD 5.658872127532959, LDE 0.0, LReg 0.0, POD 8.285396575927734 EntMin 0.0
Epoch 6, Batch 20/105, Loss=15.664328548312188
Loss made of: CE 0.6418696641921997, LKD 7.200860023498535, LDE 0.0, LReg 0.0, POD 11.45042610168457 EntMin 0.0
Epoch 6, Batch 30/105, Loss=14.900954699516296
Loss made of: CE 0.40503090620040894, LKD 5.745333671569824, LDE 0.0, LReg 0.0, POD 8.042928695678711 EntMin 0.0
Epoch 6, Batch 40/105, Loss=14.594472390413284
Loss made of: CE 0.3343486189842224, LKD 5.5667948722839355, LDE 0.0, LReg 0.0, POD 7.928841590881348 EntMin 0.0
Epoch 6, Batch 50/105, Loss=14.516992369294167
Loss made of: CE 0.2945502698421478, LKD 5.938741207122803, LDE 0.0, LReg 0.0, POD 7.941370010375977 EntMin 0.0
Epoch 6, Batch 60/105, Loss=15.06427529156208
Loss made of: CE 0.3003513514995575, LKD 6.530045986175537, LDE 0.0, LReg 0.0, POD 9.194615364074707 EntMin 0.0
Epoch 6, Batch 70/105, Loss=14.714231243729591
Loss made of: CE 0.33629533648490906, LKD 7.238988876342773, LDE 0.0, LReg 0.0, POD 9.392294883728027 EntMin 0.0
Epoch 6, Batch 80/105, Loss=15.192251254618167
Loss made of: CE 0.31243762373924255, LKD 4.96358585357666, LDE 0.0, LReg 0.0, POD 9.86433219909668 EntMin 0.0
Epoch 6, Batch 90/105, Loss=14.36204060614109
Loss made of: CE 0.3284089267253876, LKD 5.577317714691162, LDE 0.0, LReg 0.0, POD 7.828134059906006 EntMin 0.0
Epoch 6, Batch 100/105, Loss=14.661860194802284
Loss made of: CE 0.3398094177246094, LKD 6.757972717285156, LDE 0.0, LReg 0.0, POD 8.952951431274414 EntMin 0.0
Epoch 6, Class Loss=0.3701877295970917, Reg Loss=5.684859275817871
Clinet index 16, End of Epoch 6/6, Average Loss=6.055047035217285, Class Loss=0.3701877295970917, Reg Loss=5.684859275817871
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=15.646824592351914
Loss made of: CE 0.6610064506530762, LKD 6.039000511169434, LDE 0.0, LReg 0.0, POD 8.162849426269531 EntMin 0.0
Epoch 1, Batch 20/102, Loss=16.0820449590683
Loss made of: CE 0.5212626457214355, LKD 6.156132698059082, LDE 0.0, LReg 0.0, POD 10.259309768676758 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 30/102, Loss=15.680507230758668
Loss made of: CE 0.5826026201248169, LKD 6.666027069091797, LDE 0.0, LReg 0.0, POD 9.087335586547852 EntMin 0.0
Epoch 1, Batch 40/102, Loss=15.677045905590058
Loss made of: CE 0.38327813148498535, LKD 5.843764781951904, LDE 0.0, LReg 0.0, POD 9.298933029174805 EntMin 0.0
Epoch 1, Batch 50/102, Loss=15.339234206080437
Loss made of: CE 0.41593408584594727, LKD 6.309088706970215, LDE 0.0, LReg 0.0, POD 8.077156066894531 EntMin 0.0
Epoch 1, Batch 60/102, Loss=15.9303187251091
Loss made of: CE 0.5504993200302124, LKD 6.128523826599121, LDE 0.0, LReg 0.0, POD 8.567193984985352 EntMin 0.0
Epoch 1, Batch 70/102, Loss=15.363013875484466
Loss made of: CE 0.4920315742492676, LKD 5.552958965301514, LDE 0.0, LReg 0.0, POD 8.899328231811523 EntMin 0.0
Epoch 1, Batch 80/102, Loss=15.591918727755546
Loss made of: CE 0.44148653745651245, LKD 5.139220714569092, LDE 0.0, LReg 0.0, POD 8.8774995803833 EntMin 0.0
Epoch 1, Batch 90/102, Loss=15.89461909532547
Loss made of: CE 0.4528864324092865, LKD 6.458022117614746, LDE 0.0, LReg 0.0, POD 8.359823226928711 EntMin 0.0
Epoch 1, Batch 100/102, Loss=15.649502781033515
Loss made of: CE 0.46164196729660034, LKD 7.159564018249512, LDE 0.0, LReg 0.0, POD 9.27287483215332 EntMin 0.0
Epoch 1, Class Loss=0.47026774287223816, Reg Loss=5.892828941345215
Clinet index 18, End of Epoch 1/6, Average Loss=6.363096714019775, Class Loss=0.47026774287223816, Reg Loss=5.892828941345215
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/102, Loss=15.130829790234566
Loss made of: CE 0.38751253485679626, LKD 6.274500846862793, LDE 0.0, LReg 0.0, POD 8.659345626831055 EntMin 0.0
Epoch 2, Batch 20/102, Loss=15.844490092992782
Loss made of: CE 0.5006094574928284, LKD 6.971837520599365, LDE 0.0, LReg 0.0, POD 9.54171371459961 EntMin 0.0
Epoch 2, Batch 30/102, Loss=15.469548881053925
Loss made of: CE 0.472817987203598, LKD 5.738311767578125, LDE 0.0, LReg 0.0, POD 8.69450855255127 EntMin 0.0
Epoch 2, Batch 40/102, Loss=15.430925819277764
Loss made of: CE 0.3411509394645691, LKD 4.961379051208496, LDE 0.0, LReg 0.0, POD 9.596635818481445 EntMin 0.0
Epoch 2, Batch 50/102, Loss=15.474439877271653
Loss made of: CE 0.4162050187587738, LKD 6.618276119232178, LDE 0.0, LReg 0.0, POD 8.526285171508789 EntMin 0.0
Epoch 2, Batch 60/102, Loss=16.016230618953706
Loss made of: CE 0.40637993812561035, LKD 7.737043380737305, LDE 0.0, LReg 0.0, POD 10.431584358215332 EntMin 0.0
Epoch 2, Batch 70/102, Loss=15.329915699362754
Loss made of: CE 0.3069605231285095, LKD 5.145130157470703, LDE 0.0, LReg 0.0, POD 8.459869384765625 EntMin 0.0
Epoch 2, Batch 80/102, Loss=15.557939341664314
Loss made of: CE 0.3577992618083954, LKD 5.776956081390381, LDE 0.0, LReg 0.0, POD 9.765484809875488 EntMin 0.0
Epoch 2, Batch 90/102, Loss=14.858928707242011
Loss made of: CE 0.3642767667770386, LKD 5.516829490661621, LDE 0.0, LReg 0.0, POD 9.262167930603027 EntMin 0.0
Epoch 2, Batch 100/102, Loss=15.490619042515755
Loss made of: CE 0.36555370688438416, LKD 5.767179489135742, LDE 0.0, LReg 0.0, POD 9.017240524291992 EntMin 0.0
Epoch 2, Class Loss=0.41275960206985474, Reg Loss=5.860678672790527
Clinet index 18, End of Epoch 2/6, Average Loss=6.273438453674316, Class Loss=0.41275960206985474, Reg Loss=5.860678672790527
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/102, Loss=15.296536937355995
Loss made of: CE 0.3833731412887573, LKD 6.582522392272949, LDE 0.0, LReg 0.0, POD 9.010018348693848 EntMin 0.0
Epoch 3, Batch 20/102, Loss=15.617878937721253
Loss made of: CE 0.42029163241386414, LKD 5.3816938400268555, LDE 0.0, LReg 0.0, POD 9.204498291015625 EntMin 0.0
Epoch 3, Batch 30/102, Loss=15.527307280898095
Loss made of: CE 0.33140820264816284, LKD 4.968186378479004, LDE 0.0, LReg 0.0, POD 9.02799129486084 EntMin 0.0
Epoch 3, Batch 40/102, Loss=15.695489457249641
Loss made of: CE 0.35984036326408386, LKD 6.219779968261719, LDE 0.0, LReg 0.0, POD 8.219436645507812 EntMin 0.0
Epoch 3, Batch 50/102, Loss=15.354457449913024
Loss made of: CE 0.3840816020965576, LKD 5.487196922302246, LDE 0.0, LReg 0.0, POD 9.604683876037598 EntMin 0.0
Epoch 3, Batch 60/102, Loss=15.008917021751405
Loss made of: CE 0.44329726696014404, LKD 6.170888423919678, LDE 0.0, LReg 0.0, POD 8.082097053527832 EntMin 0.0
Epoch 3, Batch 70/102, Loss=15.542047560214996
Loss made of: CE 0.3670703172683716, LKD 5.784003257751465, LDE 0.0, LReg 0.0, POD 9.229293823242188 EntMin 0.0
Epoch 3, Batch 80/102, Loss=14.86540374159813
Loss made of: CE 0.39749622344970703, LKD 4.923081398010254, LDE 0.0, LReg 0.0, POD 8.606756210327148 EntMin 0.0
Epoch 3, Batch 90/102, Loss=15.150093433260917
Loss made of: CE 0.29698875546455383, LKD 5.658775329589844, LDE 0.0, LReg 0.0, POD 8.498882293701172 EntMin 0.0
Epoch 3, Batch 100/102, Loss=15.451981356739998
Loss made of: CE 0.2957395315170288, LKD 5.450438499450684, LDE 0.0, LReg 0.0, POD 8.22956657409668 EntMin 0.0
Epoch 3, Class Loss=0.39860281348228455, Reg Loss=5.893776893615723
Clinet index 18, End of Epoch 3/6, Average Loss=6.292379856109619, Class Loss=0.39860281348228455, Reg Loss=5.893776893615723
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/102, Loss=15.573312845826148
Loss made of: CE 0.3759325444698334, LKD 5.767366409301758, LDE 0.0, LReg 0.0, POD 8.789346694946289 EntMin 0.0
Epoch 4, Batch 20/102, Loss=15.230456998944282
Loss made of: CE 0.38064369559288025, LKD 6.3973541259765625, LDE 0.0, LReg 0.0, POD 8.663444519042969 EntMin 0.0
Epoch 4, Batch 30/102, Loss=15.504446524381638
Loss made of: CE 0.48500579595565796, LKD 6.376490592956543, LDE 0.0, LReg 0.0, POD 9.181373596191406 EntMin 0.0
Epoch 4, Batch 40/102, Loss=15.092860808968544
Loss made of: CE 0.4151080548763275, LKD 7.387386322021484, LDE 0.0, LReg 0.0, POD 9.41252326965332 EntMin 0.0
Epoch 4, Batch 50/102, Loss=15.435878190398217
Loss made of: CE 0.3134310841560364, LKD 5.240706920623779, LDE 0.0, LReg 0.0, POD 9.717769622802734 EntMin 0.0
Epoch 4, Batch 60/102, Loss=15.404457652568817
Loss made of: CE 0.38870295882225037, LKD 6.217790603637695, LDE 0.0, LReg 0.0, POD 9.01888656616211 EntMin 0.0
Epoch 4, Batch 70/102, Loss=14.677673998475075
Loss made of: CE 0.26657867431640625, LKD 4.601162910461426, LDE 0.0, LReg 0.0, POD 9.590168952941895 EntMin 0.0
Epoch 4, Batch 80/102, Loss=15.009296169877052
Loss made of: CE 0.3800371289253235, LKD 5.504235744476318, LDE 0.0, LReg 0.0, POD 9.099729537963867 EntMin 0.0
Epoch 4, Batch 90/102, Loss=15.53690841794014
Loss made of: CE 0.4543125629425049, LKD 5.342836380004883, LDE 0.0, LReg 0.0, POD 9.702893257141113 EntMin 0.0
Epoch 4, Batch 100/102, Loss=15.32201020717621
Loss made of: CE 0.3303186893463135, LKD 6.074013710021973, LDE 0.0, LReg 0.0, POD 8.027441024780273 EntMin 0.0
Epoch 4, Class Loss=0.3826865553855896, Reg Loss=5.855226039886475
Clinet index 18, End of Epoch 4/6, Average Loss=6.237912654876709, Class Loss=0.3826865553855896, Reg Loss=5.855226039886475
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/102, Loss=14.614738908410072
Loss made of: CE 0.38399264216423035, LKD 5.606210708618164, LDE 0.0, LReg 0.0, POD 8.517131805419922 EntMin 0.0
Epoch 5, Batch 20/102, Loss=15.405056771636009
Loss made of: CE 0.3897723853588104, LKD 5.827770233154297, LDE 0.0, LReg 0.0, POD 9.839351654052734 EntMin 0.0
Epoch 5, Batch 30/102, Loss=14.780463036894798
Loss made of: CE 0.2677778899669647, LKD 5.954087734222412, LDE 0.0, LReg 0.0, POD 8.748421669006348 EntMin 0.0
Epoch 5, Batch 40/102, Loss=14.79799917936325
Loss made of: CE 0.31369322538375854, LKD 5.936611175537109, LDE 0.0, LReg 0.0, POD 8.55569076538086 EntMin 0.0
Epoch 5, Batch 50/102, Loss=15.229210603237153
Loss made of: CE 0.38590025901794434, LKD 5.448842525482178, LDE 0.0, LReg 0.0, POD 8.220474243164062 EntMin 0.0
Epoch 5, Batch 60/102, Loss=15.101656880974769
Loss made of: CE 0.38753077387809753, LKD 5.614432334899902, LDE 0.0, LReg 0.0, POD 8.78820514678955 EntMin 0.0
Epoch 5, Batch 70/102, Loss=14.933078607916832
Loss made of: CE 0.44281062483787537, LKD 7.060568809509277, LDE 0.0, LReg 0.0, POD 8.327262878417969 EntMin 0.0
Epoch 5, Batch 80/102, Loss=14.982142668962478
Loss made of: CE 0.28577882051467896, LKD 5.811186790466309, LDE 0.0, LReg 0.0, POD 9.721729278564453 EntMin 0.0
Epoch 5, Batch 90/102, Loss=15.14138996899128
Loss made of: CE 0.3396725058555603, LKD 5.510153770446777, LDE 0.0, LReg 0.0, POD 9.286417961120605 EntMin 0.0
Epoch 5, Batch 100/102, Loss=14.690411451458932
Loss made of: CE 0.2886560559272766, LKD 5.664722442626953, LDE 0.0, LReg 0.0, POD 7.841381072998047 EntMin 0.0
Epoch 5, Class Loss=0.37406009435653687, Reg Loss=5.8491315841674805
Clinet index 18, End of Epoch 5/6, Average Loss=6.223191738128662, Class Loss=0.37406009435653687, Reg Loss=5.8491315841674805
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/102, Loss=15.04509003162384
Loss made of: CE 0.3489012122154236, LKD 5.913756370544434, LDE 0.0, LReg 0.0, POD 9.235008239746094 EntMin 0.0
Epoch 6, Batch 20/102, Loss=14.959679287672042
Loss made of: CE 0.3537832200527191, LKD 5.563687801361084, LDE 0.0, LReg 0.0, POD 8.604246139526367 EntMin 0.0
Epoch 6, Batch 30/102, Loss=15.168105155229568
Loss made of: CE 0.4026167392730713, LKD 6.434749603271484, LDE 0.0, LReg 0.0, POD 9.001519203186035 EntMin 0.0
Epoch 6, Batch 40/102, Loss=15.134170252084733
Loss made of: CE 0.49700888991355896, LKD 6.419215202331543, LDE 0.0, LReg 0.0, POD 8.967348098754883 EntMin 0.0
Epoch 6, Batch 50/102, Loss=15.475928783416748
Loss made of: CE 0.2995200753211975, LKD 5.073460102081299, LDE 0.0, LReg 0.0, POD 8.357431411743164 EntMin 0.0
Epoch 6, Batch 60/102, Loss=14.965671673417091
Loss made of: CE 0.38706380128860474, LKD 5.36664342880249, LDE 0.0, LReg 0.0, POD 11.330944061279297 EntMin 0.0
Epoch 6, Batch 70/102, Loss=15.092585611343384
Loss made of: CE 0.3570520281791687, LKD 5.488799095153809, LDE 0.0, LReg 0.0, POD 9.02163028717041 EntMin 0.0
Epoch 6, Batch 80/102, Loss=15.461967661976814
Loss made of: CE 0.26307329535484314, LKD 5.913018703460693, LDE 0.0, LReg 0.0, POD 8.678526878356934 EntMin 0.0
Epoch 6, Batch 90/102, Loss=15.064708185195922
Loss made of: CE 0.2979995906352997, LKD 6.699817657470703, LDE 0.0, LReg 0.0, POD 10.046276092529297 EntMin 0.0
Epoch 6, Batch 100/102, Loss=14.892406618595123
Loss made of: CE 0.43414437770843506, LKD 5.989274501800537, LDE 0.0, LReg 0.0, POD 9.327089309692383 EntMin 0.0
Epoch 6, Class Loss=0.36835962533950806, Reg Loss=5.885558128356934
Clinet index 18, End of Epoch 6/6, Average Loss=6.253917694091797, Class Loss=0.36835962533950806, Reg Loss=5.885558128356934
federated aggregation...
Validation, Class Loss=0.5742873549461365, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.831487
Mean Acc: 0.502225
FreqW Acc: 0.734520
Mean IoU: 0.350153
Class IoU:
	class 0: 0.8765949
	class 1: 0.68591607
	class 2: 0.2542749
	class 3: 0.4213702
	class 4: 0.5199875
	class 5: 0.05666752
	class 6: 0.6701353
	class 7: 0.4918645
	class 8: 0.36783668
	class 9: 0.0011689824
	class 10: 0.34249106
	class 11: 0.26276988
	class 12: 0.033391364
	class 13: 0.0
	class 14: 0.27250385
	class 15: 0.6956232
	class 16: 0.0
Class Acc:
	class 0: 0.94816935
	class 1: 0.6969286
	class 2: 0.635708
	class 3: 0.9202937
	class 4: 0.8840581
	class 5: 0.05674548
	class 6: 0.7016057
	class 7: 0.7093369
	class 8: 0.8187599
	class 9: 0.0011697655
	class 10: 0.484419
	class 11: 0.49370167
	class 12: 0.03351559
	class 13: 0.0
	class 14: 0.30244845
	class 15: 0.85095805
	class 16: 0.0

federated global round: 17, step: 3
select part of clients to conduct local training
[5, 3, 17, 11]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=13.990047991275787
Loss made of: CE 0.34733590483665466, LKD 5.118746280670166, LDE 0.0, LReg 0.0, POD 7.148730278015137 EntMin 0.0
Epoch 1, Batch 20/102, Loss=14.4383943349123
Loss made of: CE 0.3308509886264801, LKD 5.651534080505371, LDE 0.0, LReg 0.0, POD 8.182138442993164 EntMin 0.0
Epoch 1, Batch 30/102, Loss=14.692497649788857
Loss made of: CE 0.3139461278915405, LKD 5.898528099060059, LDE 0.0, LReg 0.0, POD 7.749617576599121 EntMin 0.0
Epoch 1, Batch 40/102, Loss=14.45785523056984
Loss made of: CE 0.3812001645565033, LKD 5.947160243988037, LDE 0.0, LReg 0.0, POD 8.477103233337402 EntMin 0.0
Epoch 1, Batch 50/102, Loss=14.53542891740799
Loss made of: CE 0.37566208839416504, LKD 5.814556121826172, LDE 0.0, LReg 0.0, POD 8.756216049194336 EntMin 0.0
Epoch 1, Batch 60/102, Loss=15.02707149386406
Loss made of: CE 0.55418461561203, LKD 6.827573299407959, LDE 0.0, LReg 0.0, POD 9.23088550567627 EntMin 0.0
Epoch 1, Batch 70/102, Loss=15.318299171328544
Loss made of: CE 0.40649014711380005, LKD 5.490950584411621, LDE 0.0, LReg 0.0, POD 8.902885437011719 EntMin 0.0
Epoch 1, Batch 80/102, Loss=15.307187351584435
Loss made of: CE 0.36372241377830505, LKD 4.867972373962402, LDE 0.0, LReg 0.0, POD 8.915915489196777 EntMin 0.0
Epoch 1, Batch 90/102, Loss=14.557360675930976
Loss made of: CE 0.34268030524253845, LKD 5.018483638763428, LDE 0.0, LReg 0.0, POD 8.539995193481445 EntMin 0.0
Epoch 1, Batch 100/102, Loss=14.860367912054063
Loss made of: CE 0.33509790897369385, LKD 6.406551361083984, LDE 0.0, LReg 0.0, POD 7.555789470672607 EntMin 0.0
Epoch 1, Class Loss=0.36414337158203125, Reg Loss=5.844164848327637
Clinet index 5, End of Epoch 1/6, Average Loss=6.208308219909668, Class Loss=0.36414337158203125, Reg Loss=5.844164848327637
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=15.266376875340939
Loss made of: CE 0.35064324736595154, LKD 5.5695414543151855, LDE 0.0, LReg 0.0, POD 8.94915771484375 EntMin 0.0
Epoch 2, Batch 20/102, Loss=15.121919783949853
Loss made of: CE 0.32272961735725403, LKD 5.2688188552856445, LDE 0.0, LReg 0.0, POD 7.5908379554748535 EntMin 0.0
Epoch 2, Batch 30/102, Loss=14.710349729657173
Loss made of: CE 0.31741422414779663, LKD 6.215668678283691, LDE 0.0, LReg 0.0, POD 8.5619535446167 EntMin 0.0
Epoch 2, Batch 40/102, Loss=15.086539560556412
Loss made of: CE 0.4191412627696991, LKD 6.127650260925293, LDE 0.0, LReg 0.0, POD 8.702455520629883 EntMin 0.0
Epoch 2, Batch 50/102, Loss=15.192769461870194
Loss made of: CE 0.2891092896461487, LKD 5.5194854736328125, LDE 0.0, LReg 0.0, POD 8.27814769744873 EntMin 0.0
Epoch 2, Batch 60/102, Loss=15.443868812918662
Loss made of: CE 0.3361075520515442, LKD 5.344066619873047, LDE 0.0, LReg 0.0, POD 9.294599533081055 EntMin 0.0
Epoch 2, Batch 70/102, Loss=14.534987276792526
Loss made of: CE 0.34293660521507263, LKD 5.716465950012207, LDE 0.0, LReg 0.0, POD 8.168530464172363 EntMin 0.0
Epoch 2, Batch 80/102, Loss=15.409869077801705
Loss made of: CE 0.42970284819602966, LKD 6.956618309020996, LDE 0.0, LReg 0.0, POD 9.283308029174805 EntMin 0.0
Epoch 2, Batch 90/102, Loss=15.10697387456894
Loss made of: CE 0.35339462757110596, LKD 5.586102485656738, LDE 0.0, LReg 0.0, POD 7.521515369415283 EntMin 0.0
Epoch 2, Batch 100/102, Loss=15.42175524532795
Loss made of: CE 0.3058377504348755, LKD 6.274954795837402, LDE 0.0, LReg 0.0, POD 9.351825714111328 EntMin 0.0
Epoch 2, Class Loss=0.35937264561653137, Reg Loss=5.85027551651001
Clinet index 5, End of Epoch 2/6, Average Loss=6.209648132324219, Class Loss=0.35937264561653137, Reg Loss=5.85027551651001
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=14.955543795228005
Loss made of: CE 0.3283771872520447, LKD 5.75529670715332, LDE 0.0, LReg 0.0, POD 8.940046310424805 EntMin 0.0
Epoch 3, Batch 20/102, Loss=15.117858266830444
Loss made of: CE 0.2864436209201813, LKD 5.365616798400879, LDE 0.0, LReg 0.0, POD 8.899017333984375 EntMin 0.0
Epoch 3, Batch 30/102, Loss=14.751175174117089
Loss made of: CE 0.4269639849662781, LKD 6.344899654388428, LDE 0.0, LReg 0.0, POD 8.045146942138672 EntMin 0.0
Epoch 3, Batch 40/102, Loss=15.217748048901559
Loss made of: CE 0.3500637710094452, LKD 5.159968376159668, LDE 0.0, LReg 0.0, POD 8.648062705993652 EntMin 0.0
Epoch 3, Batch 50/102, Loss=15.17624626159668
Loss made of: CE 0.4595760703086853, LKD 6.3994293212890625, LDE 0.0, LReg 0.0, POD 8.384709358215332 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 60/102, Loss=14.708155027031898
Loss made of: CE 0.4561668336391449, LKD 7.094744682312012, LDE 0.0, LReg 0.0, POD 9.239974975585938 EntMin 0.0
Epoch 3, Batch 70/102, Loss=14.819289514422417
Loss made of: CE 0.4392097592353821, LKD 6.826152801513672, LDE 0.0, LReg 0.0, POD 8.899101257324219 EntMin 0.0
Epoch 3, Batch 80/102, Loss=15.831936591863633
Loss made of: CE 0.29262274503707886, LKD 5.49216365814209, LDE 0.0, LReg 0.0, POD 8.4370756149292 EntMin 0.0
Epoch 3, Batch 90/102, Loss=14.952103781700135
Loss made of: CE 0.31935712695121765, LKD 5.2908430099487305, LDE 0.0, LReg 0.0, POD 8.00685977935791 EntMin 0.0
Epoch 3, Batch 100/102, Loss=14.809812304377555
Loss made of: CE 0.302670419216156, LKD 5.4205451011657715, LDE 0.0, LReg 0.0, POD 8.684903144836426 EntMin 0.0
Epoch 3, Class Loss=0.3519204258918762, Reg Loss=5.851422309875488
Clinet index 5, End of Epoch 3/6, Average Loss=6.203342914581299, Class Loss=0.3519204258918762, Reg Loss=5.851422309875488
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=15.262140524387359
Loss made of: CE 0.353610634803772, LKD 6.331336498260498, LDE 0.0, LReg 0.0, POD 8.570413589477539 EntMin 0.0
Epoch 4, Batch 20/102, Loss=14.818639072775841
Loss made of: CE 0.27533474564552307, LKD 5.561470031738281, LDE 0.0, LReg 0.0, POD 8.219696044921875 EntMin 0.0
Epoch 4, Batch 30/102, Loss=15.200831672549247
Loss made of: CE 0.36284539103507996, LKD 4.708590507507324, LDE 0.0, LReg 0.0, POD 8.4478120803833 EntMin 0.0
Epoch 4, Batch 40/102, Loss=15.259232980012893
Loss made of: CE 0.3265340328216553, LKD 5.822772026062012, LDE 0.0, LReg 0.0, POD 8.762937545776367 EntMin 0.0
Epoch 4, Batch 50/102, Loss=14.586692821979522
Loss made of: CE 0.3563607335090637, LKD 6.3225531578063965, LDE 0.0, LReg 0.0, POD 8.553474426269531 EntMin 0.0
Epoch 4, Batch 60/102, Loss=14.943813464045524
Loss made of: CE 0.4783566892147064, LKD 6.07742977142334, LDE 0.0, LReg 0.0, POD 8.687774658203125 EntMin 0.0
Epoch 4, Batch 70/102, Loss=14.698399385809898
Loss made of: CE 0.30144578218460083, LKD 4.516940116882324, LDE 0.0, LReg 0.0, POD 9.774286270141602 EntMin 0.0
Epoch 4, Batch 80/102, Loss=15.011753350496292
Loss made of: CE 0.3425063490867615, LKD 5.028728485107422, LDE 0.0, LReg 0.0, POD 8.764786720275879 EntMin 0.0
Epoch 4, Batch 90/102, Loss=14.942535543441773
Loss made of: CE 0.3888317942619324, LKD 6.186763286590576, LDE 0.0, LReg 0.0, POD 8.608616828918457 EntMin 0.0
Epoch 4, Batch 100/102, Loss=14.991048803925514
Loss made of: CE 0.30131345987319946, LKD 7.080564498901367, LDE 0.0, LReg 0.0, POD 9.20189094543457 EntMin 0.0
Epoch 4, Class Loss=0.34641894698143005, Reg Loss=5.8815202713012695
Clinet index 5, End of Epoch 4/6, Average Loss=6.227939128875732, Class Loss=0.34641894698143005, Reg Loss=5.8815202713012695
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=14.686176645755769
Loss made of: CE 0.31597790122032166, LKD 5.7904887199401855, LDE 0.0, LReg 0.0, POD 8.302665710449219 EntMin 0.0
Epoch 5, Batch 20/102, Loss=15.043533715605736
Loss made of: CE 0.36601147055625916, LKD 5.829524040222168, LDE 0.0, LReg 0.0, POD 8.26964282989502 EntMin 0.0
Epoch 5, Batch 30/102, Loss=14.487576082348824
Loss made of: CE 0.3066689968109131, LKD 5.229223251342773, LDE 0.0, LReg 0.0, POD 8.341492652893066 EntMin 0.0
Epoch 5, Batch 40/102, Loss=14.835477143526077
Loss made of: CE 0.35911592841148376, LKD 6.060059547424316, LDE 0.0, LReg 0.0, POD 8.090238571166992 EntMin 0.0
Epoch 5, Batch 50/102, Loss=14.705128794908523
Loss made of: CE 0.2619757652282715, LKD 5.693907737731934, LDE 0.0, LReg 0.0, POD 8.788841247558594 EntMin 0.0
Epoch 5, Batch 60/102, Loss=14.939165717363357
Loss made of: CE 0.36432069540023804, LKD 6.323856353759766, LDE 0.0, LReg 0.0, POD 7.704751014709473 EntMin 0.0
Epoch 5, Batch 70/102, Loss=14.795200580358506
Loss made of: CE 0.340219110250473, LKD 5.199497699737549, LDE 0.0, LReg 0.0, POD 9.179803848266602 EntMin 0.0
Epoch 5, Batch 80/102, Loss=14.429932776093484
Loss made of: CE 0.41266387701034546, LKD 5.679676055908203, LDE 0.0, LReg 0.0, POD 7.999083518981934 EntMin 0.0
Epoch 5, Batch 90/102, Loss=14.540323171019555
Loss made of: CE 0.3667376935482025, LKD 5.984242916107178, LDE 0.0, LReg 0.0, POD 8.689460754394531 EntMin 0.0
Epoch 5, Batch 100/102, Loss=14.697260457277299
Loss made of: CE 0.300241082906723, LKD 5.247865676879883, LDE 0.0, LReg 0.0, POD 8.079141616821289 EntMin 0.0
Epoch 5, Class Loss=0.3417726457118988, Reg Loss=5.838003158569336
Clinet index 5, End of Epoch 5/6, Average Loss=6.179775714874268, Class Loss=0.3417726457118988, Reg Loss=5.838003158569336
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=14.66452464312315
Loss made of: CE 0.2910257875919342, LKD 5.572475433349609, LDE 0.0, LReg 0.0, POD 8.423225402832031 EntMin 0.0
Epoch 6, Batch 20/102, Loss=14.499074831604958
Loss made of: CE 0.29560035467147827, LKD 5.640627861022949, LDE 0.0, LReg 0.0, POD 9.300291061401367 EntMin 0.0
Epoch 6, Batch 30/102, Loss=14.82888600230217
Loss made of: CE 0.3808010518550873, LKD 5.9780426025390625, LDE 0.0, LReg 0.0, POD 7.821213245391846 EntMin 0.0
Epoch 6, Batch 40/102, Loss=14.516135239601136
Loss made of: CE 0.3667169213294983, LKD 5.9033613204956055, LDE 0.0, LReg 0.0, POD 8.271204948425293 EntMin 0.0
Epoch 6, Batch 50/102, Loss=14.43532906472683
Loss made of: CE 0.39869993925094604, LKD 5.350171089172363, LDE 0.0, LReg 0.0, POD 9.792433738708496 EntMin 0.0
Epoch 6, Batch 60/102, Loss=14.387992453575134
Loss made of: CE 0.24625787138938904, LKD 5.899407863616943, LDE 0.0, LReg 0.0, POD 8.099396705627441 EntMin 0.0
Epoch 6, Batch 70/102, Loss=14.61737469136715
Loss made of: CE 0.3926710784435272, LKD 6.062625408172607, LDE 0.0, LReg 0.0, POD 8.399733543395996 EntMin 0.0
Epoch 6, Batch 80/102, Loss=14.038659170269966
Loss made of: CE 0.2969200611114502, LKD 5.393554210662842, LDE 0.0, LReg 0.0, POD 8.397151947021484 EntMin 0.0
Epoch 6, Batch 90/102, Loss=14.661307440698147
Loss made of: CE 0.3428846001625061, LKD 6.505879878997803, LDE 0.0, LReg 0.0, POD 8.421735763549805 EntMin 0.0
Epoch 6, Batch 100/102, Loss=15.173848104476928
Loss made of: CE 0.31676697731018066, LKD 5.627161026000977, LDE 0.0, LReg 0.0, POD 8.406627655029297 EntMin 0.0
Epoch 6, Class Loss=0.3381081223487854, Reg Loss=5.837429046630859
Clinet index 5, End of Epoch 6/6, Average Loss=6.175537109375, Class Loss=0.3381081223487854, Reg Loss=5.837429046630859
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=14.287605631351472
Loss made of: CE 0.3575098216533661, LKD 6.317911148071289, LDE 0.0, LReg 0.0, POD 9.079023361206055 EntMin 0.0
Epoch 1, Batch 20/102, Loss=14.38603289425373
Loss made of: CE 0.3334234654903412, LKD 6.0617804527282715, LDE 0.0, LReg 0.0, POD 7.413573741912842 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 30/102, Loss=14.843939357995987
Loss made of: CE 0.41992971301078796, LKD 6.369440078735352, LDE 0.0, LReg 0.0, POD 8.037588119506836 EntMin 0.0
Epoch 1, Batch 40/102, Loss=14.675768750905991
Loss made of: CE 0.46280428767204285, LKD 5.543723106384277, LDE 0.0, LReg 0.0, POD 11.977161407470703 EntMin 0.0
Epoch 1, Batch 50/102, Loss=15.149926288425922
Loss made of: CE 0.3627791404724121, LKD 6.645671367645264, LDE 0.0, LReg 0.0, POD 10.35348892211914 EntMin 0.0
Epoch 1, Batch 60/102, Loss=14.952060261368752
Loss made of: CE 0.431959867477417, LKD 5.822649002075195, LDE 0.0, LReg 0.0, POD 8.352317810058594 EntMin 0.0
Epoch 1, Batch 70/102, Loss=14.975593921542167
Loss made of: CE 0.39061301946640015, LKD 5.325925350189209, LDE 0.0, LReg 0.0, POD 8.984981536865234 EntMin 0.0
Epoch 1, Batch 80/102, Loss=14.842829385399819
Loss made of: CE 0.3565319776535034, LKD 5.9635443687438965, LDE 0.0, LReg 0.0, POD 8.113184928894043 EntMin 0.0
Epoch 1, Batch 90/102, Loss=15.437462410330772
Loss made of: CE 0.38789889216423035, LKD 6.796097755432129, LDE 0.0, LReg 0.0, POD 8.475706100463867 EntMin 0.0
Epoch 1, Batch 100/102, Loss=15.257152885198593
Loss made of: CE 0.42167171835899353, LKD 5.72388219833374, LDE 0.0, LReg 0.0, POD 7.849031448364258 EntMin 0.0
Epoch 1, Class Loss=0.36710914969444275, Reg Loss=5.846357345581055
Clinet index 3, End of Epoch 1/6, Average Loss=6.213466644287109, Class Loss=0.36710914969444275, Reg Loss=5.846357345581055
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=15.062668108940125
Loss made of: CE 0.4779502749443054, LKD 6.143458843231201, LDE 0.0, LReg 0.0, POD 9.82481575012207 EntMin 0.0
Epoch 2, Batch 20/102, Loss=15.025413119792939
Loss made of: CE 0.5502896904945374, LKD 6.843753814697266, LDE 0.0, LReg 0.0, POD 8.620903015136719 EntMin 0.0
Epoch 2, Batch 30/102, Loss=15.465838021039962
Loss made of: CE 0.4017343521118164, LKD 6.379514217376709, LDE 0.0, LReg 0.0, POD 9.217877388000488 EntMin 0.0
Epoch 2, Batch 40/102, Loss=15.02753663957119
Loss made of: CE 0.2894931435585022, LKD 5.699682235717773, LDE 0.0, LReg 0.0, POD 7.9534149169921875 EntMin 0.0
Epoch 2, Batch 50/102, Loss=15.060560950636864
Loss made of: CE 0.32845354080200195, LKD 5.478371620178223, LDE 0.0, LReg 0.0, POD 7.876904010772705 EntMin 0.0
Epoch 2, Batch 60/102, Loss=14.75985171198845
Loss made of: CE 0.2922501266002655, LKD 5.3913702964782715, LDE 0.0, LReg 0.0, POD 9.361703872680664 EntMin 0.0
Epoch 2, Batch 70/102, Loss=15.088402450084686
Loss made of: CE 0.2816949486732483, LKD 5.527998924255371, LDE 0.0, LReg 0.0, POD 9.076277732849121 EntMin 0.0
Epoch 2, Batch 80/102, Loss=14.82483329474926
Loss made of: CE 0.3437487483024597, LKD 6.182786464691162, LDE 0.0, LReg 0.0, POD 9.646936416625977 EntMin 0.0
Epoch 2, Batch 90/102, Loss=15.029441890120506
Loss made of: CE 0.29079723358154297, LKD 6.142268657684326, LDE 0.0, LReg 0.0, POD 8.247858047485352 EntMin 0.0
Epoch 2, Batch 100/102, Loss=15.273456305265427
Loss made of: CE 0.3703567385673523, LKD 5.2421488761901855, LDE 0.0, LReg 0.0, POD 8.903802871704102 EntMin 0.0
Epoch 2, Class Loss=0.3543456196784973, Reg Loss=5.835752010345459
Clinet index 3, End of Epoch 2/6, Average Loss=6.190097808837891, Class Loss=0.3543456196784973, Reg Loss=5.835752010345459
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=15.55060347020626
Loss made of: CE 0.3251301050186157, LKD 5.463192939758301, LDE 0.0, LReg 0.0, POD 8.197571754455566 EntMin 0.0
Epoch 3, Batch 20/102, Loss=15.152858352661132
Loss made of: CE 0.30220842361450195, LKD 5.050541877746582, LDE 0.0, LReg 0.0, POD 9.658683776855469 EntMin 0.0
Epoch 3, Batch 30/102, Loss=14.84918559640646
Loss made of: CE 0.29437240958213806, LKD 4.979931831359863, LDE 0.0, LReg 0.0, POD 9.420251846313477 EntMin 0.0
Epoch 3, Batch 40/102, Loss=14.844237434864045
Loss made of: CE 0.39954304695129395, LKD 6.022407531738281, LDE 0.0, LReg 0.0, POD 8.436175346374512 EntMin 0.0
Epoch 3, Batch 50/102, Loss=15.639286777377128
Loss made of: CE 0.3228895962238312, LKD 5.0622124671936035, LDE 0.0, LReg 0.0, POD 10.104585647583008 EntMin 0.0
Epoch 3, Batch 60/102, Loss=15.015997979044915
Loss made of: CE 0.44410595297813416, LKD 5.878269195556641, LDE 0.0, LReg 0.0, POD 8.018457412719727 EntMin 0.0
Epoch 3, Batch 70/102, Loss=15.28686834871769
Loss made of: CE 0.26371049880981445, LKD 5.279839515686035, LDE 0.0, LReg 0.0, POD 8.50955867767334 EntMin 0.0
Epoch 3, Batch 80/102, Loss=14.733884981274604
Loss made of: CE 0.32709383964538574, LKD 4.917072296142578, LDE 0.0, LReg 0.0, POD 8.6068115234375 EntMin 0.0
Epoch 3, Batch 90/102, Loss=14.73003684580326
Loss made of: CE 0.3379436433315277, LKD 6.2131781578063965, LDE 0.0, LReg 0.0, POD 7.660141944885254 EntMin 0.0
Epoch 3, Batch 100/102, Loss=14.901234319806099
Loss made of: CE 0.43305695056915283, LKD 6.30981969833374, LDE 0.0, LReg 0.0, POD 10.2681303024292 EntMin 0.0
Epoch 3, Class Loss=0.3520384430885315, Reg Loss=5.865523815155029
Clinet index 3, End of Epoch 3/6, Average Loss=6.217562198638916, Class Loss=0.3520384430885315, Reg Loss=5.865523815155029
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=14.62632308602333
Loss made of: CE 0.40706485509872437, LKD 6.54058837890625, LDE 0.0, LReg 0.0, POD 8.615072250366211 EntMin 0.0
Epoch 4, Batch 20/102, Loss=15.231917944550514
Loss made of: CE 0.3893527388572693, LKD 5.31577205657959, LDE 0.0, LReg 0.0, POD 8.552458763122559 EntMin 0.0
Epoch 4, Batch 30/102, Loss=15.194062447547912
Loss made of: CE 0.3017461895942688, LKD 6.30908727645874, LDE 0.0, LReg 0.0, POD 8.616437911987305 EntMin 0.0
Epoch 4, Batch 40/102, Loss=15.254229843616486
Loss made of: CE 0.3972029685974121, LKD 6.497668743133545, LDE 0.0, LReg 0.0, POD 7.991809844970703 EntMin 0.0
Epoch 4, Batch 50/102, Loss=15.124393764138222
Loss made of: CE 0.35291236639022827, LKD 5.72330379486084, LDE 0.0, LReg 0.0, POD 8.444656372070312 EntMin 0.0
Epoch 4, Batch 60/102, Loss=15.213286912441253
Loss made of: CE 0.3605058193206787, LKD 6.549811363220215, LDE 0.0, LReg 0.0, POD 8.848367691040039 EntMin 0.0
Epoch 4, Batch 70/102, Loss=14.86873185634613
Loss made of: CE 0.4232228994369507, LKD 5.8836212158203125, LDE 0.0, LReg 0.0, POD 8.35391902923584 EntMin 0.0
Epoch 4, Batch 80/102, Loss=14.888069105148315
Loss made of: CE 0.2769266366958618, LKD 5.869596004486084, LDE 0.0, LReg 0.0, POD 8.80347728729248 EntMin 0.0
Epoch 4, Batch 90/102, Loss=14.88012206107378
Loss made of: CE 0.34144195914268494, LKD 5.699904441833496, LDE 0.0, LReg 0.0, POD 7.81070613861084 EntMin 0.0
Epoch 4, Batch 100/102, Loss=15.131408396363259
Loss made of: CE 0.3878828287124634, LKD 6.5686798095703125, LDE 0.0, LReg 0.0, POD 8.775867462158203 EntMin 0.0
Epoch 4, Class Loss=0.3460470139980316, Reg Loss=5.827854633331299
Clinet index 3, End of Epoch 4/6, Average Loss=6.173901557922363, Class Loss=0.3460470139980316, Reg Loss=5.827854633331299
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=14.540029910206794
Loss made of: CE 0.3610430955886841, LKD 5.6786909103393555, LDE 0.0, LReg 0.0, POD 8.834012031555176 EntMin 0.0
Epoch 5, Batch 20/102, Loss=14.908030304312707
Loss made of: CE 0.4190908670425415, LKD 6.732845306396484, LDE 0.0, LReg 0.0, POD 8.809813499450684 EntMin 0.0
Epoch 5, Batch 30/102, Loss=14.732730224728584
Loss made of: CE 0.26347124576568604, LKD 5.178376197814941, LDE 0.0, LReg 0.0, POD 8.129070281982422 EntMin 0.0
Epoch 5, Batch 40/102, Loss=15.02927487194538
Loss made of: CE 0.34539175033569336, LKD 6.005658149719238, LDE 0.0, LReg 0.0, POD 7.963750839233398 EntMin 0.0
Epoch 5, Batch 50/102, Loss=15.420167362689972
Loss made of: CE 0.324211061000824, LKD 6.353578567504883, LDE 0.0, LReg 0.0, POD 8.451507568359375 EntMin 0.0
Epoch 5, Batch 60/102, Loss=14.553477731347083
Loss made of: CE 0.2708492577075958, LKD 5.214363098144531, LDE 0.0, LReg 0.0, POD 7.80401611328125 EntMin 0.0
Epoch 5, Batch 70/102, Loss=14.868031853437424
Loss made of: CE 0.269528865814209, LKD 5.071035385131836, LDE 0.0, LReg 0.0, POD 8.234073638916016 EntMin 0.0
Epoch 5, Batch 80/102, Loss=14.883153018355369
Loss made of: CE 0.27133673429489136, LKD 4.824742317199707, LDE 0.0, LReg 0.0, POD 9.970972061157227 EntMin 0.0
Epoch 5, Batch 90/102, Loss=15.040259033441544
Loss made of: CE 0.3200054168701172, LKD 6.252877712249756, LDE 0.0, LReg 0.0, POD 9.890295028686523 EntMin 0.0
Epoch 5, Batch 100/102, Loss=14.793870595097541
Loss made of: CE 0.370136022567749, LKD 5.246910095214844, LDE 0.0, LReg 0.0, POD 8.272775650024414 EntMin 0.0
Epoch 5, Class Loss=0.3445710241794586, Reg Loss=5.821547031402588
Clinet index 3, End of Epoch 5/6, Average Loss=6.166118144989014, Class Loss=0.3445710241794586, Reg Loss=5.821547031402588
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=14.495969223976136
Loss made of: CE 0.39964473247528076, LKD 6.147943019866943, LDE 0.0, LReg 0.0, POD 8.435483932495117 EntMin 0.0
Epoch 6, Batch 20/102, Loss=14.59000272154808
Loss made of: CE 0.3009437322616577, LKD 6.317803382873535, LDE 0.0, LReg 0.0, POD 8.26160717010498 EntMin 0.0
Epoch 6, Batch 30/102, Loss=14.89467677474022
Loss made of: CE 0.3350105881690979, LKD 5.139337062835693, LDE 0.0, LReg 0.0, POD 9.873566627502441 EntMin 0.0
Epoch 6, Batch 40/102, Loss=14.019890439510345
Loss made of: CE 0.2856915593147278, LKD 5.448436260223389, LDE 0.0, LReg 0.0, POD 7.392605304718018 EntMin 0.0
Epoch 6, Batch 50/102, Loss=14.460236483812333
Loss made of: CE 0.266154021024704, LKD 5.762774467468262, LDE 0.0, LReg 0.0, POD 9.073198318481445 EntMin 0.0
Epoch 6, Batch 60/102, Loss=15.120160464942455
Loss made of: CE 0.3910325765609741, LKD 6.252708435058594, LDE 0.0, LReg 0.0, POD 7.931728363037109 EntMin 0.0
Epoch 6, Batch 70/102, Loss=14.188759666681289
Loss made of: CE 0.2797553241252899, LKD 5.664130210876465, LDE 0.0, LReg 0.0, POD 7.911216735839844 EntMin 0.0
Epoch 6, Batch 80/102, Loss=14.215574163198472
Loss made of: CE 0.27848300337791443, LKD 5.15540885925293, LDE 0.0, LReg 0.0, POD 8.180839538574219 EntMin 0.0
Epoch 6, Batch 90/102, Loss=14.68835096359253
Loss made of: CE 0.40600913763046265, LKD 6.655190467834473, LDE 0.0, LReg 0.0, POD 8.096577644348145 EntMin 0.0
Epoch 6, Batch 100/102, Loss=14.746040374040604
Loss made of: CE 0.2561212182044983, LKD 4.9387006759643555, LDE 0.0, LReg 0.0, POD 8.453300476074219 EntMin 0.0
Epoch 6, Class Loss=0.3351753056049347, Reg Loss=5.813140392303467
Clinet index 3, End of Epoch 6/6, Average Loss=6.148315906524658, Class Loss=0.3351753056049347, Reg Loss=5.813140392303467
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/23, Loss=18.898786598443984
Loss made of: CE 0.7839106917381287, LKD 6.304505825042725, LDE 0.0, LReg 0.0, POD 10.304279327392578 EntMin 0.0
Epoch 1, Batch 20/23, Loss=16.92662689089775
Loss made of: CE 0.8854734301567078, LKD 6.3785810470581055, LDE 0.0, LReg 0.0, POD 9.676586151123047 EntMin 0.0
Epoch 1, Class Loss=0.8956446647644043, Reg Loss=6.370387554168701
Clinet index 17, End of Epoch 1/6, Average Loss=7.2660322189331055, Class Loss=0.8956446647644043, Reg Loss=6.370387554168701
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/23, Loss=16.81304461359978
Loss made of: CE 0.8136820793151855, LKD 5.906603813171387, LDE 0.0, LReg 0.0, POD 8.915143966674805 EntMin 0.0
Epoch 2, Batch 20/23, Loss=16.03962884545326
Loss made of: CE 0.5011300444602966, LKD 6.294694423675537, LDE 0.0, LReg 0.0, POD 9.516794204711914 EntMin 0.0
Epoch 2, Class Loss=0.7356168627738953, Reg Loss=6.252355098724365
Clinet index 17, End of Epoch 2/6, Average Loss=6.987971782684326, Class Loss=0.7356168627738953, Reg Loss=6.252355098724365
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/23, Loss=15.999237191677093
Loss made of: CE 0.5231561064720154, LKD 5.740121364593506, LDE 0.0, LReg 0.0, POD 8.965644836425781 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 20/23, Loss=16.22239135503769
Loss made of: CE 0.7739777565002441, LKD 7.106229305267334, LDE 0.0, LReg 0.0, POD 9.590545654296875 EntMin 0.0
Epoch 3, Class Loss=0.6453298926353455, Reg Loss=6.230439186096191
Clinet index 17, End of Epoch 3/6, Average Loss=6.875769138336182, Class Loss=0.6453298926353455, Reg Loss=6.230439186096191
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/23, Loss=16.450753077864647
Loss made of: CE 0.7491855621337891, LKD 7.119708061218262, LDE 0.0, LReg 0.0, POD 9.284095764160156 EntMin 0.0
Epoch 4, Batch 20/23, Loss=15.89828694164753
Loss made of: CE 0.5256508588790894, LKD 6.956082820892334, LDE 0.0, LReg 0.0, POD 9.573761940002441 EntMin 0.0
Epoch 4, Class Loss=0.5912712812423706, Reg Loss=6.2532854080200195
Clinet index 17, End of Epoch 4/6, Average Loss=6.84455680847168, Class Loss=0.5912712812423706, Reg Loss=6.2532854080200195
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/23, Loss=16.010635736584664
Loss made of: CE 0.449240505695343, LKD 4.918283462524414, LDE 0.0, LReg 0.0, POD 8.732057571411133 EntMin 0.0
Epoch 5, Batch 20/23, Loss=15.649014124274254
Loss made of: CE 0.5081008672714233, LKD 6.397186279296875, LDE 0.0, LReg 0.0, POD 9.584800720214844 EntMin 0.0
Epoch 5, Class Loss=0.565073549747467, Reg Loss=6.278444290161133
Clinet index 17, End of Epoch 5/6, Average Loss=6.843517780303955, Class Loss=0.565073549747467, Reg Loss=6.278444290161133
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/23, Loss=16.021644753217696
Loss made of: CE 0.5214498043060303, LKD 5.367753982543945, LDE 0.0, LReg 0.0, POD 8.22191047668457 EntMin 0.0
Epoch 6, Batch 20/23, Loss=15.33857786655426
Loss made of: CE 0.6591976284980774, LKD 6.073753356933594, LDE 0.0, LReg 0.0, POD 10.213815689086914 EntMin 0.0
Epoch 6, Class Loss=0.5493757128715515, Reg Loss=6.227781772613525
Clinet index 17, End of Epoch 6/6, Average Loss=6.777157306671143, Class Loss=0.5493757128715515, Reg Loss=6.227781772613525
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=13.754484939575196
Loss made of: CE 0.31402406096458435, LKD 4.9432573318481445, LDE 0.0, LReg 0.0, POD 8.445145606994629 EntMin 0.0
Epoch 1, Batch 20/102, Loss=14.961799857020377
Loss made of: CE 0.3561253547668457, LKD 5.356112480163574, LDE 0.0, LReg 0.0, POD 9.012304306030273 EntMin 0.0
Epoch 1, Batch 30/102, Loss=14.522765624523164
Loss made of: CE 0.42900580167770386, LKD 6.633143424987793, LDE 0.0, LReg 0.0, POD 8.811260223388672 EntMin 0.0
Epoch 1, Batch 40/102, Loss=14.901500225067139
Loss made of: CE 0.2747540771961212, LKD 5.697574615478516, LDE 0.0, LReg 0.0, POD 8.052804946899414 EntMin 0.0
Epoch 1, Batch 50/102, Loss=14.454586547613143
Loss made of: CE 0.2847627103328705, LKD 4.91548490524292, LDE 0.0, LReg 0.0, POD 8.157414436340332 EntMin 0.0
Epoch 1, Batch 60/102, Loss=14.862893480062485
Loss made of: CE 0.3323626220226288, LKD 5.341334342956543, LDE 0.0, LReg 0.0, POD 7.781163215637207 EntMin 0.0
Epoch 1, Batch 70/102, Loss=14.31156691610813
Loss made of: CE 0.34097546339035034, LKD 5.670619487762451, LDE 0.0, LReg 0.0, POD 8.101070404052734 EntMin 0.0
Epoch 1, Batch 80/102, Loss=14.594756272435188
Loss made of: CE 0.37418878078460693, LKD 5.428709983825684, LDE 0.0, LReg 0.0, POD 7.757420063018799 EntMin 0.0
Epoch 1, Batch 90/102, Loss=13.984465923905372
Loss made of: CE 0.32418984174728394, LKD 6.971063137054443, LDE 0.0, LReg 0.0, POD 7.9431986808776855 EntMin 0.0
Epoch 1, Batch 100/102, Loss=14.384683272242546
Loss made of: CE 0.4290103614330292, LKD 7.220220565795898, LDE 0.0, LReg 0.0, POD 8.941629409790039 EntMin 0.0
Epoch 1, Class Loss=0.36763277649879456, Reg Loss=5.872409343719482
Clinet index 11, End of Epoch 1/6, Average Loss=6.240042209625244, Class Loss=0.36763277649879456, Reg Loss=5.872409343719482
Pseudo labeling is: None
Epoch 2, lr = 0.000600
Epoch 2, Batch 10/102, Loss=14.530134025216103
Loss made of: CE 0.3411884903907776, LKD 5.514180660247803, LDE 0.0, LReg 0.0, POD 7.463631629943848 EntMin 0.0
Epoch 2, Batch 20/102, Loss=14.03606019616127
Loss made of: CE 0.31222814321517944, LKD 5.583705902099609, LDE 0.0, LReg 0.0, POD 8.480093955993652 EntMin 0.0
Epoch 2, Batch 30/102, Loss=14.427577415108681
Loss made of: CE 0.4407798647880554, LKD 6.540988922119141, LDE 0.0, LReg 0.0, POD 8.101054191589355 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 40/102, Loss=14.541596508026123
Loss made of: CE 0.43405580520629883, LKD 6.506182670593262, LDE 0.0, LReg 0.0, POD 7.615530967712402 EntMin 0.0
Epoch 2, Batch 50/102, Loss=14.427040505409241
Loss made of: CE 0.3892176151275635, LKD 6.585057735443115, LDE 0.0, LReg 0.0, POD 8.431400299072266 EntMin 0.0
Epoch 2, Batch 60/102, Loss=14.710826101899148
Loss made of: CE 0.323457807302475, LKD 6.580176830291748, LDE 0.0, LReg 0.0, POD 8.490795135498047 EntMin 0.0
Epoch 2, Batch 70/102, Loss=13.817727360129357
Loss made of: CE 0.3119550347328186, LKD 6.636684417724609, LDE 0.0, LReg 0.0, POD 7.532095909118652 EntMin 0.0
Epoch 2, Batch 80/102, Loss=14.744017481803894
Loss made of: CE 0.3042004108428955, LKD 7.067156791687012, LDE 0.0, LReg 0.0, POD 9.791343688964844 EntMin 0.0
Epoch 2, Batch 90/102, Loss=14.182672396302223
Loss made of: CE 0.4955953359603882, LKD 6.337770462036133, LDE 0.0, LReg 0.0, POD 8.676250457763672 EntMin 0.0
Epoch 2, Batch 100/102, Loss=14.464075702428818
Loss made of: CE 0.3189084529876709, LKD 5.778552055358887, LDE 0.0, LReg 0.0, POD 8.92444896697998 EntMin 0.0
Epoch 2, Class Loss=0.35952988266944885, Reg Loss=5.860551834106445
Clinet index 11, End of Epoch 2/6, Average Loss=6.220081806182861, Class Loss=0.35952988266944885, Reg Loss=5.860551834106445
Pseudo labeling is: None
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/102, Loss=14.572782132029534
Loss made of: CE 0.4214061498641968, LKD 6.346037864685059, LDE 0.0, LReg 0.0, POD 8.311420440673828 EntMin 0.0
Epoch 3, Batch 20/102, Loss=14.1881064504385
Loss made of: CE 0.42330336570739746, LKD 6.054839134216309, LDE 0.0, LReg 0.0, POD 7.472516059875488 EntMin 0.0
Epoch 3, Batch 30/102, Loss=14.054615944623947
Loss made of: CE 0.30984124541282654, LKD 6.114621639251709, LDE 0.0, LReg 0.0, POD 8.444828033447266 EntMin 0.0
Epoch 3, Batch 40/102, Loss=14.356093695759773
Loss made of: CE 0.29682838916778564, LKD 6.100738525390625, LDE 0.0, LReg 0.0, POD 8.173070907592773 EntMin 0.0
Epoch 3, Batch 50/102, Loss=14.624817833304405
Loss made of: CE 0.3493567705154419, LKD 6.332206726074219, LDE 0.0, LReg 0.0, POD 7.545038223266602 EntMin 0.0
Epoch 3, Batch 60/102, Loss=14.367787046730518
Loss made of: CE 0.27386701107025146, LKD 5.244567394256592, LDE 0.0, LReg 0.0, POD 7.971559047698975 EntMin 0.0
Epoch 3, Batch 70/102, Loss=14.484655609726905
Loss made of: CE 0.30611392855644226, LKD 5.489007472991943, LDE 0.0, LReg 0.0, POD 8.543318748474121 EntMin 0.0
Epoch 3, Batch 80/102, Loss=14.817455843091011
Loss made of: CE 0.30312198400497437, LKD 5.761363983154297, LDE 0.0, LReg 0.0, POD 9.4581298828125 EntMin 0.0
Epoch 3, Batch 90/102, Loss=14.601216220855713
Loss made of: CE 0.36929845809936523, LKD 5.248741149902344, LDE 0.0, LReg 0.0, POD 8.161354064941406 EntMin 0.0
Epoch 3, Batch 100/102, Loss=14.66271114051342
Loss made of: CE 0.3117028474807739, LKD 5.914414882659912, LDE 0.0, LReg 0.0, POD 7.576901435852051 EntMin 0.0
Epoch 3, Class Loss=0.3515762686729431, Reg Loss=5.868198871612549
Clinet index 11, End of Epoch 3/6, Average Loss=6.219775199890137, Class Loss=0.3515762686729431, Reg Loss=5.868198871612549
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/102, Loss=14.25300303697586
Loss made of: CE 0.3799540400505066, LKD 5.53163480758667, LDE 0.0, LReg 0.0, POD 7.6015729904174805 EntMin 0.0
Epoch 4, Batch 20/102, Loss=14.561466866731644
Loss made of: CE 0.31020960211753845, LKD 4.922027587890625, LDE 0.0, LReg 0.0, POD 8.775714874267578 EntMin 0.0
Epoch 4, Batch 30/102, Loss=14.633323067426682
Loss made of: CE 0.5025100708007812, LKD 6.1224517822265625, LDE 0.0, LReg 0.0, POD 8.445146560668945 EntMin 0.0
Epoch 4, Batch 40/102, Loss=14.342270177602767
Loss made of: CE 0.3184346556663513, LKD 5.577040672302246, LDE 0.0, LReg 0.0, POD 7.231988906860352 EntMin 0.0
Epoch 4, Batch 50/102, Loss=14.49347738623619
Loss made of: CE 0.45456212759017944, LKD 6.4107818603515625, LDE 0.0, LReg 0.0, POD 8.172279357910156 EntMin 0.0
Epoch 4, Batch 60/102, Loss=14.697436147928238
Loss made of: CE 0.3448648452758789, LKD 5.311834335327148, LDE 0.0, LReg 0.0, POD 8.102471351623535 EntMin 0.0
Epoch 4, Batch 70/102, Loss=14.538549074530602
Loss made of: CE 0.4201314151287079, LKD 6.540280818939209, LDE 0.0, LReg 0.0, POD 8.050126075744629 EntMin 0.0
Epoch 4, Batch 80/102, Loss=14.773635520040989
Loss made of: CE 0.39429640769958496, LKD 6.159533500671387, LDE 0.0, LReg 0.0, POD 10.417905807495117 EntMin 0.0
Epoch 4, Batch 90/102, Loss=14.415411081910133
Loss made of: CE 0.314203679561615, LKD 6.556562423706055, LDE 0.0, LReg 0.0, POD 7.511274337768555 EntMin 0.0
Epoch 4, Batch 100/102, Loss=14.274447917938232
Loss made of: CE 0.45664072036743164, LKD 6.197696685791016, LDE 0.0, LReg 0.0, POD 9.548008918762207 EntMin 0.0
Epoch 4, Class Loss=0.3537295162677765, Reg Loss=5.86580228805542
Clinet index 11, End of Epoch 4/6, Average Loss=6.219532012939453, Class Loss=0.3537295162677765, Reg Loss=5.86580228805542
Pseudo labeling is: None
Epoch 5, lr = 0.000504
Epoch 5, Batch 10/102, Loss=14.873490872979165
Loss made of: CE 0.34436464309692383, LKD 6.656929016113281, LDE 0.0, LReg 0.0, POD 7.614141941070557 EntMin 0.0
Epoch 5, Batch 20/102, Loss=14.704177451133727
Loss made of: CE 0.2517162561416626, LKD 5.874981880187988, LDE 0.0, LReg 0.0, POD 8.590293884277344 EntMin 0.0
Epoch 5, Batch 30/102, Loss=14.56297358572483
Loss made of: CE 0.3887619376182556, LKD 5.233129978179932, LDE 0.0, LReg 0.0, POD 7.685622692108154 EntMin 0.0
Epoch 5, Batch 40/102, Loss=14.136662605404855
Loss made of: CE 0.39987701177597046, LKD 6.048944473266602, LDE 0.0, LReg 0.0, POD 8.002341270446777 EntMin 0.0
Epoch 5, Batch 50/102, Loss=14.637335032224655
Loss made of: CE 0.3002837002277374, LKD 5.545010566711426, LDE 0.0, LReg 0.0, POD 9.355631828308105 EntMin 0.0
Epoch 5, Batch 60/102, Loss=14.497400784492493
Loss made of: CE 0.3718719482421875, LKD 7.14790678024292, LDE 0.0, LReg 0.0, POD 8.417792320251465 EntMin 0.0
Epoch 5, Batch 70/102, Loss=13.57124257683754
Loss made of: CE 0.36303481459617615, LKD 6.366900444030762, LDE 0.0, LReg 0.0, POD 7.617654800415039 EntMin 0.0
Epoch 5, Batch 80/102, Loss=14.075012850761414
Loss made of: CE 0.2926810383796692, LKD 5.552779197692871, LDE 0.0, LReg 0.0, POD 8.39921760559082 EntMin 0.0
Epoch 5, Batch 90/102, Loss=14.258370652794838
Loss made of: CE 0.28613990545272827, LKD 6.257399559020996, LDE 0.0, LReg 0.0, POD 7.720552444458008 EntMin 0.0
Epoch 5, Batch 100/102, Loss=14.202672570943832
Loss made of: CE 0.33577126264572144, LKD 6.048029899597168, LDE 0.0, LReg 0.0, POD 8.024552345275879 EntMin 0.0
Epoch 5, Class Loss=0.3468068242073059, Reg Loss=5.8643975257873535
Clinet index 11, End of Epoch 5/6, Average Loss=6.211204528808594, Class Loss=0.3468068242073059, Reg Loss=5.8643975257873535
Pseudo labeling is: None
Epoch 6, lr = 0.000471
Epoch 6, Batch 10/102, Loss=14.4621128231287
Loss made of: CE 0.3080403506755829, LKD 5.348445415496826, LDE 0.0, LReg 0.0, POD 7.395475387573242 EntMin 0.0
Epoch 6, Batch 20/102, Loss=13.987973353266716
Loss made of: CE 0.36754941940307617, LKD 6.106984615325928, LDE 0.0, LReg 0.0, POD 7.727445125579834 EntMin 0.0
Epoch 6, Batch 30/102, Loss=13.817199939489365
Loss made of: CE 0.2656284272670746, LKD 5.133897304534912, LDE 0.0, LReg 0.0, POD 8.016532897949219 EntMin 0.0
Epoch 6, Batch 40/102, Loss=14.5826335221529
Loss made of: CE 0.4755040109157562, LKD 6.0744171142578125, LDE 0.0, LReg 0.0, POD 8.537557601928711 EntMin 0.0
Epoch 6, Batch 50/102, Loss=14.221242573857307
Loss made of: CE 0.35031670331954956, LKD 5.844216346740723, LDE 0.0, LReg 0.0, POD 7.381636619567871 EntMin 0.0
Epoch 6, Batch 60/102, Loss=14.430801905691624
Loss made of: CE 0.39625978469848633, LKD 7.329526424407959, LDE 0.0, LReg 0.0, POD 8.525944709777832 EntMin 0.0
Epoch 6, Batch 70/102, Loss=14.069049718976022
Loss made of: CE 0.3239089250564575, LKD 5.473959922790527, LDE 0.0, LReg 0.0, POD 7.844465255737305 EntMin 0.0
Epoch 6, Batch 80/102, Loss=13.941290551424027
Loss made of: CE 0.3431738018989563, LKD 6.639403343200684, LDE 0.0, LReg 0.0, POD 8.02900505065918 EntMin 0.0
Epoch 6, Batch 90/102, Loss=14.331091916561126
Loss made of: CE 0.3339049518108368, LKD 5.1104583740234375, LDE 0.0, LReg 0.0, POD 7.42201042175293 EntMin 0.0
Epoch 6, Batch 100/102, Loss=14.171794253587723
Loss made of: CE 0.4065035283565521, LKD 6.604207515716553, LDE 0.0, LReg 0.0, POD 8.297040939331055 EntMin 0.0
Epoch 6, Class Loss=0.34675928950309753, Reg Loss=5.847833633422852
Clinet index 11, End of Epoch 6/6, Average Loss=6.1945929527282715, Class Loss=0.34675928950309753, Reg Loss=5.847833633422852
federated aggregation...
Validation, Class Loss=0.558948814868927, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.833828
Mean Acc: 0.511846
FreqW Acc: 0.740236
Mean IoU: 0.357090
Class IoU:
	class 0: 0.8802882
	class 1: 0.70011413
	class 2: 0.25610405
	class 3: 0.42594135
	class 4: 0.4979863
	class 5: 0.055367235
	class 6: 0.6930469
	class 7: 0.50675726
	class 8: 0.35731915
	class 9: 0.0008789924
	class 10: 0.346298
	class 11: 0.25949225
	class 12: 0.041542374
	class 13: 0.04857926
	class 14: 0.28570974
	class 15: 0.71511227
	class 16: 0.0
Class Acc:
	class 0: 0.94827634
	class 1: 0.7129459
	class 2: 0.6593183
	class 3: 0.9231616
	class 4: 0.8944931
	class 5: 0.055451337
	class 6: 0.72626054
	class 7: 0.7329645
	class 8: 0.8353459
	class 9: 0.0008794939
	class 10: 0.47897598
	class 11: 0.4866374
	class 12: 0.041866254
	class 13: 0.04895622
	class 14: 0.31503347
	class 15: 0.84082085
	class 16: 0.0

federated global round: 18, step: 3
select part of clients to conduct local training
[9, 11, 13, 10]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=15.093830239772796
Loss made of: CE 0.4288291931152344, LKD 5.947460174560547, LDE 0.0, LReg 0.0, POD 7.1789398193359375 EntMin 0.0
Epoch 1, Batch 20/102, Loss=14.539636869728565
Loss made of: CE 0.3360752761363983, LKD 6.095641136169434, LDE 0.0, LReg 0.0, POD 7.496253967285156 EntMin 0.0
Epoch 1, Batch 30/102, Loss=14.939400818943977
Loss made of: CE 0.42602190375328064, LKD 5.562969207763672, LDE 0.0, LReg 0.0, POD 7.939666748046875 EntMin 0.0
Epoch 1, Batch 40/102, Loss=14.701636654138564
Loss made of: CE 0.36147159337997437, LKD 6.182855129241943, LDE 0.0, LReg 0.0, POD 7.775529384613037 EntMin 0.0
Epoch 1, Batch 50/102, Loss=15.30558513402939
Loss made of: CE 0.40461465716362, LKD 6.026736736297607, LDE 0.0, LReg 0.0, POD 8.493839263916016 EntMin 0.0
Epoch 1, Batch 60/102, Loss=14.363004478812218
Loss made of: CE 0.4136298596858978, LKD 5.860800743103027, LDE 0.0, LReg 0.0, POD 9.274273872375488 EntMin 0.0
Epoch 1, Batch 70/102, Loss=14.72160409092903
Loss made of: CE 0.3154810667037964, LKD 5.804879665374756, LDE 0.0, LReg 0.0, POD 8.424274444580078 EntMin 0.0
Epoch 1, Batch 80/102, Loss=14.71476777791977
Loss made of: CE 0.343588650226593, LKD 6.372359275817871, LDE 0.0, LReg 0.0, POD 8.492288589477539 EntMin 0.0
Epoch 1, Batch 90/102, Loss=14.827178859710694
Loss made of: CE 0.34342631697654724, LKD 6.039098739624023, LDE 0.0, LReg 0.0, POD 7.95271110534668 EntMin 0.0
Epoch 1, Batch 100/102, Loss=14.15956891477108
Loss made of: CE 0.4359814524650574, LKD 5.469093322753906, LDE 0.0, LReg 0.0, POD 8.059804916381836 EntMin 0.0
Epoch 1, Class Loss=0.35020560026168823, Reg Loss=5.858378887176514
Clinet index 9, End of Epoch 1/6, Average Loss=6.208584308624268, Class Loss=0.35020560026168823, Reg Loss=5.858378887176514
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/102, Loss=14.77220705151558
Loss made of: CE 0.3271859884262085, LKD 5.667021751403809, LDE 0.0, LReg 0.0, POD 8.129654884338379 EntMin 0.0
Epoch 2, Batch 20/102, Loss=14.90696160197258
Loss made of: CE 0.3834506869316101, LKD 6.564406871795654, LDE 0.0, LReg 0.0, POD 8.64781379699707 EntMin 0.0
Epoch 2, Batch 30/102, Loss=15.254400263726712
Loss made of: CE 0.5527848601341248, LKD 6.812223434448242, LDE 0.0, LReg 0.0, POD 9.48649787902832 EntMin 0.0
Epoch 2, Batch 40/102, Loss=14.271045255661011
Loss made of: CE 0.27333107590675354, LKD 5.04981803894043, LDE 0.0, LReg 0.0, POD 7.739704132080078 EntMin 0.0
Epoch 2, Batch 50/102, Loss=15.01672251522541
Loss made of: CE 0.27737319469451904, LKD 5.459441184997559, LDE 0.0, LReg 0.0, POD 8.169363975524902 EntMin 0.0
Epoch 2, Batch 60/102, Loss=15.09437848329544
Loss made of: CE 0.2802163362503052, LKD 5.968556880950928, LDE 0.0, LReg 0.0, POD 8.14932632446289 EntMin 0.0
Epoch 2, Batch 70/102, Loss=15.064862626791001
Loss made of: CE 0.2919808328151703, LKD 5.23558235168457, LDE 0.0, LReg 0.0, POD 8.556600570678711 EntMin 0.0
Epoch 2, Batch 80/102, Loss=14.586978974938393
Loss made of: CE 0.31205466389656067, LKD 5.589898586273193, LDE 0.0, LReg 0.0, POD 8.363374710083008 EntMin 0.0
Epoch 2, Batch 90/102, Loss=14.769242906570435
Loss made of: CE 0.2893620431423187, LKD 6.500302791595459, LDE 0.0, LReg 0.0, POD 8.98223876953125 EntMin 0.0
Epoch 2, Batch 100/102, Loss=14.342228963971138
Loss made of: CE 0.321617066860199, LKD 5.592180252075195, LDE 0.0, LReg 0.0, POD 7.738926887512207 EntMin 0.0
Epoch 2, Class Loss=0.3387638032436371, Reg Loss=5.8450026512146
Clinet index 9, End of Epoch 2/6, Average Loss=6.1837663650512695, Class Loss=0.3387638032436371, Reg Loss=5.8450026512146
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/102, Loss=14.744760827720166
Loss made of: CE 0.3741837441921234, LKD 6.117972373962402, LDE 0.0, LReg 0.0, POD 8.604244232177734 EntMin 0.0
Epoch 3, Batch 20/102, Loss=15.303674320876599
Loss made of: CE 0.41031861305236816, LKD 5.079324722290039, LDE 0.0, LReg 0.0, POD 9.61571216583252 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 30/102, Loss=14.676202282309532
Loss made of: CE 0.282045841217041, LKD 7.0745954513549805, LDE 0.0, LReg 0.0, POD 8.146240234375 EntMin 0.0
Epoch 3, Batch 40/102, Loss=15.145388489961624
Loss made of: CE 0.33411163091659546, LKD 6.0006842613220215, LDE 0.0, LReg 0.0, POD 8.363719940185547 EntMin 0.0
Epoch 3, Batch 50/102, Loss=14.20457050204277
Loss made of: CE 0.3733251988887787, LKD 6.453591823577881, LDE 0.0, LReg 0.0, POD 7.842917442321777 EntMin 0.0
Epoch 3, Batch 60/102, Loss=15.004080906510353
Loss made of: CE 0.3100392818450928, LKD 6.019313812255859, LDE 0.0, LReg 0.0, POD 8.767016410827637 EntMin 0.0
Epoch 3, Batch 70/102, Loss=15.036768329143523
Loss made of: CE 0.36884424090385437, LKD 5.624162197113037, LDE 0.0, LReg 0.0, POD 9.250020980834961 EntMin 0.0
Epoch 3, Batch 80/102, Loss=14.497328911721706
Loss made of: CE 0.3322962522506714, LKD 5.291668891906738, LDE 0.0, LReg 0.0, POD 7.627127647399902 EntMin 0.0
Epoch 3, Batch 90/102, Loss=14.344760617613792
Loss made of: CE 0.2756943702697754, LKD 5.417476654052734, LDE 0.0, LReg 0.0, POD 8.560087203979492 EntMin 0.0
Epoch 3, Batch 100/102, Loss=14.853876748681069
Loss made of: CE 0.2931329011917114, LKD 5.7740607261657715, LDE 0.0, LReg 0.0, POD 8.317939758300781 EntMin 0.0
Epoch 3, Class Loss=0.33979561924934387, Reg Loss=5.858807563781738
Clinet index 9, End of Epoch 3/6, Average Loss=6.19860315322876, Class Loss=0.33979561924934387, Reg Loss=5.858807563781738
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/102, Loss=14.722691757977008
Loss made of: CE 0.40302786231040955, LKD 5.355895042419434, LDE 0.0, LReg 0.0, POD 8.414302825927734 EntMin 0.0
Epoch 4, Batch 20/102, Loss=15.04520254433155
Loss made of: CE 0.4172651469707489, LKD 6.612908363342285, LDE 0.0, LReg 0.0, POD 8.561506271362305 EntMin 0.0
Epoch 4, Batch 30/102, Loss=14.165767750144004
Loss made of: CE 0.32105186581611633, LKD 5.427340507507324, LDE 0.0, LReg 0.0, POD 8.848077774047852 EntMin 0.0
Epoch 4, Batch 40/102, Loss=14.8872882604599
Loss made of: CE 0.3100990056991577, LKD 5.371415138244629, LDE 0.0, LReg 0.0, POD 8.538785934448242 EntMin 0.0
Epoch 4, Batch 50/102, Loss=14.913212484121322
Loss made of: CE 0.3256661891937256, LKD 6.222552299499512, LDE 0.0, LReg 0.0, POD 8.427545547485352 EntMin 0.0
Epoch 4, Batch 60/102, Loss=14.817768590152264
Loss made of: CE 0.23966006934642792, LKD 5.391075134277344, LDE 0.0, LReg 0.0, POD 7.427670955657959 EntMin 0.0
Epoch 4, Batch 70/102, Loss=14.856216397881507
Loss made of: CE 0.3053951859474182, LKD 5.487943172454834, LDE 0.0, LReg 0.0, POD 9.0787353515625 EntMin 0.0
Epoch 4, Batch 80/102, Loss=14.885912424325943
Loss made of: CE 0.41808292269706726, LKD 6.419631481170654, LDE 0.0, LReg 0.0, POD 9.821460723876953 EntMin 0.0
Epoch 4, Batch 90/102, Loss=14.970070806145667
Loss made of: CE 0.32393690943717957, LKD 6.051749229431152, LDE 0.0, LReg 0.0, POD 7.340618133544922 EntMin 0.0
Epoch 4, Batch 100/102, Loss=14.587613905966283
Loss made of: CE 0.30467623472213745, LKD 5.881096839904785, LDE 0.0, LReg 0.0, POD 9.653553009033203 EntMin 0.0
Epoch 4, Class Loss=0.3367980718612671, Reg Loss=5.838965892791748
Clinet index 9, End of Epoch 4/6, Average Loss=6.175764083862305, Class Loss=0.3367980718612671, Reg Loss=5.838965892791748
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=14.448701533675194
Loss made of: CE 0.32173559069633484, LKD 4.932945251464844, LDE 0.0, LReg 0.0, POD 8.643178939819336 EntMin 0.0
Epoch 5, Batch 20/102, Loss=14.522011640667916
Loss made of: CE 0.30762767791748047, LKD 5.673211574554443, LDE 0.0, LReg 0.0, POD 8.078930854797363 EntMin 0.0
Epoch 5, Batch 30/102, Loss=14.335382652282714
Loss made of: CE 0.30998995900154114, LKD 5.319802761077881, LDE 0.0, LReg 0.0, POD 8.586112976074219 EntMin 0.0
Epoch 5, Batch 40/102, Loss=14.616848877072334
Loss made of: CE 0.3598710894584656, LKD 5.814733028411865, LDE 0.0, LReg 0.0, POD 7.960991382598877 EntMin 0.0
Epoch 5, Batch 50/102, Loss=14.693986478447915
Loss made of: CE 0.29830819368362427, LKD 5.572116851806641, LDE 0.0, LReg 0.0, POD 9.20198917388916 EntMin 0.0
Epoch 5, Batch 60/102, Loss=14.41391575783491
Loss made of: CE 0.4399937093257904, LKD 6.466464042663574, LDE 0.0, LReg 0.0, POD 8.829574584960938 EntMin 0.0
Epoch 5, Batch 70/102, Loss=14.38079338669777
Loss made of: CE 0.29022639989852905, LKD 5.3778228759765625, LDE 0.0, LReg 0.0, POD 9.024871826171875 EntMin 0.0
Epoch 5, Batch 80/102, Loss=14.26834433376789
Loss made of: CE 0.29834067821502686, LKD 5.25714111328125, LDE 0.0, LReg 0.0, POD 7.806807994842529 EntMin 0.0
Epoch 5, Batch 90/102, Loss=14.564617949724198
Loss made of: CE 0.2830943763256073, LKD 6.401111602783203, LDE 0.0, LReg 0.0, POD 7.845659255981445 EntMin 0.0
Epoch 5, Batch 100/102, Loss=14.997066819667817
Loss made of: CE 0.25956273078918457, LKD 5.4760541915893555, LDE 0.0, LReg 0.0, POD 7.378656387329102 EntMin 0.0
Epoch 5, Class Loss=0.33054980635643005, Reg Loss=5.854299545288086
Clinet index 9, End of Epoch 5/6, Average Loss=6.184849262237549, Class Loss=0.33054980635643005, Reg Loss=5.854299545288086
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/102, Loss=14.692601200938224
Loss made of: CE 0.2910694479942322, LKD 6.255101680755615, LDE 0.0, LReg 0.0, POD 9.429567337036133 EntMin 0.0
Epoch 6, Batch 20/102, Loss=14.237141561508178
Loss made of: CE 0.37377867102622986, LKD 5.763913154602051, LDE 0.0, LReg 0.0, POD 7.352816104888916 EntMin 0.0
Epoch 6, Batch 30/102, Loss=14.646397274732589
Loss made of: CE 0.40823158621788025, LKD 6.307677268981934, LDE 0.0, LReg 0.0, POD 7.160813331604004 EntMin 0.0
Epoch 6, Batch 40/102, Loss=14.369186070561408
Loss made of: CE 0.3038635551929474, LKD 7.159865379333496, LDE 0.0, LReg 0.0, POD 7.872158527374268 EntMin 0.0
Epoch 6, Batch 50/102, Loss=14.445301330089569
Loss made of: CE 0.27454861998558044, LKD 6.193498611450195, LDE 0.0, LReg 0.0, POD 8.121350288391113 EntMin 0.0
Epoch 6, Batch 60/102, Loss=14.601011131703853
Loss made of: CE 0.3030194044113159, LKD 6.244663238525391, LDE 0.0, LReg 0.0, POD 7.891898155212402 EntMin 0.0
Epoch 6, Batch 70/102, Loss=14.564282146096229
Loss made of: CE 0.3130805492401123, LKD 5.879159450531006, LDE 0.0, LReg 0.0, POD 11.238133430480957 EntMin 0.0
Epoch 6, Batch 80/102, Loss=14.601074278354645
Loss made of: CE 0.3202129602432251, LKD 5.860373497009277, LDE 0.0, LReg 0.0, POD 8.46866226196289 EntMin 0.0
Epoch 6, Batch 90/102, Loss=14.332103604078293
Loss made of: CE 0.35604220628738403, LKD 5.487850189208984, LDE 0.0, LReg 0.0, POD 7.712625980377197 EntMin 0.0
Epoch 6, Batch 100/102, Loss=14.190277716517448
Loss made of: CE 0.3026998043060303, LKD 6.073537349700928, LDE 0.0, LReg 0.0, POD 7.6079864501953125 EntMin 0.0
Epoch 6, Class Loss=0.3332725465297699, Reg Loss=5.859144687652588
Clinet index 9, End of Epoch 6/6, Average Loss=6.192417144775391, Class Loss=0.3332725465297699, Reg Loss=5.859144687652588
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000438
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=13.851271814107895
Loss made of: CE 0.30294644832611084, LKD 5.028631210327148, LDE 0.0, LReg 0.0, POD 8.95599365234375 EntMin 0.0
Epoch 1, Batch 20/102, Loss=14.718986135721206
Loss made of: CE 0.29466867446899414, LKD 5.547336578369141, LDE 0.0, LReg 0.0, POD 8.246308326721191 EntMin 0.0
Epoch 1, Batch 30/102, Loss=14.079064816236496
Loss made of: CE 0.4256555736064911, LKD 6.615582466125488, LDE 0.0, LReg 0.0, POD 8.474082946777344 EntMin 0.0
Epoch 1, Batch 40/102, Loss=14.675886112451554
Loss made of: CE 0.27227383852005005, LKD 5.627328395843506, LDE 0.0, LReg 0.0, POD 7.913009166717529 EntMin 0.0
Epoch 1, Batch 50/102, Loss=14.0571504175663
Loss made of: CE 0.27371716499328613, LKD 5.215737819671631, LDE 0.0, LReg 0.0, POD 7.493147850036621 EntMin 0.0
Epoch 1, Batch 60/102, Loss=14.506381371617318
Loss made of: CE 0.3181772828102112, LKD 5.610433101654053, LDE 0.0, LReg 0.0, POD 7.529333114624023 EntMin 0.0
Epoch 1, Batch 70/102, Loss=14.01048671901226
Loss made of: CE 0.32321736216545105, LKD 5.482274532318115, LDE 0.0, LReg 0.0, POD 7.157932281494141 EntMin 0.0
Epoch 1, Batch 80/102, Loss=14.203875169157982
Loss made of: CE 0.3789604902267456, LKD 5.467202663421631, LDE 0.0, LReg 0.0, POD 7.640103340148926 EntMin 0.0
Epoch 1, Batch 90/102, Loss=13.548650646209717
Loss made of: CE 0.2991197109222412, LKD 6.680531978607178, LDE 0.0, LReg 0.0, POD 7.443755149841309 EntMin 0.0
Epoch 1, Batch 100/102, Loss=13.909262084960938
Loss made of: CE 0.41127508878707886, LKD 7.0776286125183105, LDE 0.0, LReg 0.0, POD 8.559589385986328 EntMin 0.0
Epoch 1, Class Loss=0.35038894414901733, Reg Loss=5.871767044067383
Clinet index 11, End of Epoch 1/6, Average Loss=6.222156047821045, Class Loss=0.35038894414901733, Reg Loss=5.871767044067383
Pseudo labeling is: None
Epoch 2, lr = 0.000405
Epoch 2, Batch 10/102, Loss=14.232747700810432
Loss made of: CE 0.35479867458343506, LKD 5.407416343688965, LDE 0.0, LReg 0.0, POD 7.73081111907959 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 20/102, Loss=13.674861189723014
Loss made of: CE 0.3154553174972534, LKD 5.634368419647217, LDE 0.0, LReg 0.0, POD 7.96310567855835 EntMin 0.0
Epoch 2, Batch 30/102, Loss=13.92963374555111
Loss made of: CE 0.40782493352890015, LKD 5.988551139831543, LDE 0.0, LReg 0.0, POD 7.314818859100342 EntMin 0.0
Epoch 2, Batch 40/102, Loss=13.975946193933487
Loss made of: CE 0.3953824043273926, LKD 6.280980587005615, LDE 0.0, LReg 0.0, POD 7.344194412231445 EntMin 0.0
Epoch 2, Batch 50/102, Loss=14.235575476288796
Loss made of: CE 0.37713393568992615, LKD 6.736729621887207, LDE 0.0, LReg 0.0, POD 8.236205101013184 EntMin 0.0
Epoch 2, Batch 60/102, Loss=14.307386890053749
Loss made of: CE 0.311881422996521, LKD 6.546945095062256, LDE 0.0, LReg 0.0, POD 8.04974365234375 EntMin 0.0
Epoch 2, Batch 70/102, Loss=13.584067189693451
Loss made of: CE 0.3228414058685303, LKD 6.935672283172607, LDE 0.0, LReg 0.0, POD 7.422113418579102 EntMin 0.0
Epoch 2, Batch 80/102, Loss=14.308146017789841
Loss made of: CE 0.29441988468170166, LKD 6.632412910461426, LDE 0.0, LReg 0.0, POD 9.293927192687988 EntMin 0.0
Epoch 2, Batch 90/102, Loss=13.815938645601273
Loss made of: CE 0.5375722646713257, LKD 7.111043453216553, LDE 0.0, LReg 0.0, POD 8.343341827392578 EntMin 0.0
Epoch 2, Batch 100/102, Loss=13.990457811951638
Loss made of: CE 0.28101491928100586, LKD 5.684596061706543, LDE 0.0, LReg 0.0, POD 8.106815338134766 EntMin 0.0
Epoch 2, Class Loss=0.34529274702072144, Reg Loss=5.868160724639893
Clinet index 11, End of Epoch 2/6, Average Loss=6.21345329284668, Class Loss=0.34529274702072144, Reg Loss=5.868160724639893
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/102, Loss=14.05793206691742
Loss made of: CE 0.38105496764183044, LKD 6.238930702209473, LDE 0.0, LReg 0.0, POD 7.356331825256348 EntMin 0.0
Epoch 3, Batch 20/102, Loss=13.745496305823327
Loss made of: CE 0.40347275137901306, LKD 6.005461692810059, LDE 0.0, LReg 0.0, POD 6.962285041809082 EntMin 0.0
Epoch 3, Batch 30/102, Loss=13.563643214106559
Loss made of: CE 0.2843352258205414, LKD 5.8889288902282715, LDE 0.0, LReg 0.0, POD 8.110910415649414 EntMin 0.0
Epoch 3, Batch 40/102, Loss=14.10534485578537
Loss made of: CE 0.30363497138023376, LKD 6.621846675872803, LDE 0.0, LReg 0.0, POD 8.393123626708984 EntMin 0.0
Epoch 3, Batch 50/102, Loss=14.35336691737175
Loss made of: CE 0.3566979765892029, LKD 6.478485107421875, LDE 0.0, LReg 0.0, POD 7.238583564758301 EntMin 0.0
Epoch 3, Batch 60/102, Loss=13.834926848113536
Loss made of: CE 0.27347955107688904, LKD 5.36853551864624, LDE 0.0, LReg 0.0, POD 7.682430267333984 EntMin 0.0
Epoch 3, Batch 70/102, Loss=14.000192081928253
Loss made of: CE 0.29443082213401794, LKD 5.050321102142334, LDE 0.0, LReg 0.0, POD 8.160867691040039 EntMin 0.0
Epoch 3, Batch 80/102, Loss=14.200842314958573
Loss made of: CE 0.2818204164505005, LKD 6.031522274017334, LDE 0.0, LReg 0.0, POD 9.056821823120117 EntMin 0.0
Epoch 3, Batch 90/102, Loss=14.218152856826782
Loss made of: CE 0.3713865876197815, LKD 5.477516174316406, LDE 0.0, LReg 0.0, POD 7.5359063148498535 EntMin 0.0
Epoch 3, Batch 100/102, Loss=14.049071043729782
Loss made of: CE 0.29075539112091064, LKD 5.675466060638428, LDE 0.0, LReg 0.0, POD 7.364394187927246 EntMin 0.0
Epoch 3, Class Loss=0.33855563402175903, Reg Loss=5.856186866760254
Clinet index 11, End of Epoch 3/6, Average Loss=6.194742679595947, Class Loss=0.33855563402175903, Reg Loss=5.856186866760254
Pseudo labeling is: None
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=13.98094513118267
Loss made of: CE 0.3448212742805481, LKD 4.956620216369629, LDE 0.0, LReg 0.0, POD 7.08272647857666 EntMin 0.0
Epoch 4, Batch 20/102, Loss=14.02033734023571
Loss made of: CE 0.25254368782043457, LKD 5.124688625335693, LDE 0.0, LReg 0.0, POD 7.774963855743408 EntMin 0.0
Epoch 4, Batch 30/102, Loss=14.159410330653191
Loss made of: CE 0.46391409635543823, LKD 6.00458288192749, LDE 0.0, LReg 0.0, POD 7.717156887054443 EntMin 0.0
Epoch 4, Batch 40/102, Loss=14.06518996655941
Loss made of: CE 0.32275182008743286, LKD 5.699056148529053, LDE 0.0, LReg 0.0, POD 7.1320719718933105 EntMin 0.0
Epoch 4, Batch 50/102, Loss=13.98249592781067
Loss made of: CE 0.42855748534202576, LKD 6.134106636047363, LDE 0.0, LReg 0.0, POD 8.263893127441406 EntMin 0.0
Epoch 4, Batch 60/102, Loss=14.149943035840987
Loss made of: CE 0.3039132058620453, LKD 4.958858489990234, LDE 0.0, LReg 0.0, POD 8.095342636108398 EntMin 0.0
Epoch 4, Batch 70/102, Loss=14.0670300334692
Loss made of: CE 0.40375399589538574, LKD 6.665708065032959, LDE 0.0, LReg 0.0, POD 7.452822685241699 EntMin 0.0
Epoch 4, Batch 80/102, Loss=14.217647337913514
Loss made of: CE 0.3494497537612915, LKD 6.157697677612305, LDE 0.0, LReg 0.0, POD 9.769804000854492 EntMin 0.0
Epoch 4, Batch 90/102, Loss=14.201857322454453
Loss made of: CE 0.30612218379974365, LKD 6.132571220397949, LDE 0.0, LReg 0.0, POD 6.90056037902832 EntMin 0.0
Epoch 4, Batch 100/102, Loss=13.896674627065659
Loss made of: CE 0.4642449617385864, LKD 5.847190856933594, LDE 0.0, LReg 0.0, POD 9.229413986206055 EntMin 0.0
Epoch 4, Class Loss=0.34332311153411865, Reg Loss=5.839626789093018
Clinet index 11, End of Epoch 4/6, Average Loss=6.182950019836426, Class Loss=0.34332311153411865, Reg Loss=5.839626789093018
Pseudo labeling is: None
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/102, Loss=14.165824085474014
Loss made of: CE 0.32195279002189636, LKD 6.4777021408081055, LDE 0.0, LReg 0.0, POD 7.288747787475586 EntMin 0.0
Epoch 5, Batch 20/102, Loss=14.135717442631721
Loss made of: CE 0.25090664625167847, LKD 6.246288299560547, LDE 0.0, LReg 0.0, POD 7.828221321105957 EntMin 0.0
Epoch 5, Batch 30/102, Loss=14.18072327375412
Loss made of: CE 0.3784114122390747, LKD 5.358029842376709, LDE 0.0, LReg 0.0, POD 7.315011501312256 EntMin 0.0
Epoch 5, Batch 40/102, Loss=13.689435136318206
Loss made of: CE 0.3977656066417694, LKD 5.932787895202637, LDE 0.0, LReg 0.0, POD 7.64963436126709 EntMin 0.0
Epoch 5, Batch 50/102, Loss=14.082676786184312
Loss made of: CE 0.2979040741920471, LKD 5.469914436340332, LDE 0.0, LReg 0.0, POD 8.960396766662598 EntMin 0.0
Epoch 5, Batch 60/102, Loss=13.914323228597642
Loss made of: CE 0.3623121976852417, LKD 6.7643232345581055, LDE 0.0, LReg 0.0, POD 7.684112548828125 EntMin 0.0
Epoch 5, Batch 70/102, Loss=13.201319906115533
Loss made of: CE 0.34012120962142944, LKD 6.22705602645874, LDE 0.0, LReg 0.0, POD 7.352741241455078 EntMin 0.0
Epoch 5, Batch 80/102, Loss=13.716877076029778
Loss made of: CE 0.30705735087394714, LKD 5.686809539794922, LDE 0.0, LReg 0.0, POD 8.469613075256348 EntMin 0.0
Epoch 5, Batch 90/102, Loss=13.844787330925465
Loss made of: CE 0.28411829471588135, LKD 6.284061431884766, LDE 0.0, LReg 0.0, POD 7.430883884429932 EntMin 0.0
Epoch 5, Batch 100/102, Loss=13.7948442786932
Loss made of: CE 0.31299933791160583, LKD 5.893095970153809, LDE 0.0, LReg 0.0, POD 7.860296249389648 EntMin 0.0
Epoch 5, Class Loss=0.3359757363796234, Reg Loss=5.852527141571045
Clinet index 11, End of Epoch 5/6, Average Loss=6.188502788543701, Class Loss=0.3359757363796234, Reg Loss=5.852527141571045
Pseudo labeling is: None
Epoch 6, lr = 0.000270
Epoch 6, Batch 10/102, Loss=14.098054757714271
Loss made of: CE 0.28673452138900757, LKD 5.5977582931518555, LDE 0.0, LReg 0.0, POD 6.959695816040039 EntMin 0.0
Epoch 6, Batch 20/102, Loss=13.71784166097641
Loss made of: CE 0.3645312190055847, LKD 6.186763763427734, LDE 0.0, LReg 0.0, POD 7.930794715881348 EntMin 0.0
Epoch 6, Batch 30/102, Loss=13.429093450307846
Loss made of: CE 0.2678713798522949, LKD 5.00822639465332, LDE 0.0, LReg 0.0, POD 7.840553283691406 EntMin 0.0
Epoch 6, Batch 40/102, Loss=14.381357434391976
Loss made of: CE 0.45748960971832275, LKD 5.937869548797607, LDE 0.0, LReg 0.0, POD 8.872472763061523 EntMin 0.0
Epoch 6, Batch 50/102, Loss=13.841718447208404
Loss made of: CE 0.3634411692619324, LKD 5.964568138122559, LDE 0.0, LReg 0.0, POD 7.190988540649414 EntMin 0.0
Epoch 6, Batch 60/102, Loss=14.01627624630928
Loss made of: CE 0.4108104407787323, LKD 7.380029201507568, LDE 0.0, LReg 0.0, POD 8.338483810424805 EntMin 0.0
Epoch 6, Batch 70/102, Loss=13.618008449673653
Loss made of: CE 0.3513352572917938, LKD 5.326511383056641, LDE 0.0, LReg 0.0, POD 7.3182477951049805 EntMin 0.0
Epoch 6, Batch 80/102, Loss=13.634230688214302
Loss made of: CE 0.3586793839931488, LKD 6.917177200317383, LDE 0.0, LReg 0.0, POD 7.6388139724731445 EntMin 0.0
Epoch 6, Batch 90/102, Loss=13.922212475538254
Loss made of: CE 0.2973122000694275, LKD 5.1194634437561035, LDE 0.0, LReg 0.0, POD 6.787190914154053 EntMin 0.0
Epoch 6, Batch 100/102, Loss=13.786783614754677
Loss made of: CE 0.37366944551467896, LKD 5.949139595031738, LDE 0.0, LReg 0.0, POD 7.570468425750732 EntMin 0.0
Epoch 6, Class Loss=0.33750906586647034, Reg Loss=5.880398273468018
Clinet index 11, End of Epoch 6/6, Average Loss=6.217907428741455, Class Loss=0.33750906586647034, Reg Loss=5.880398273468018
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=17.411565482616425
Loss made of: CE 0.7703622579574585, LKD 5.763602256774902, LDE 0.0, LReg 0.0, POD 8.690593719482422 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/23, Loss=16.323140293359756
Loss made of: CE 0.771550178527832, LKD 6.433307647705078, LDE 0.0, LReg 0.0, POD 9.506065368652344 EntMin 0.0
Epoch 1, Class Loss=0.7847158312797546, Reg Loss=6.304268836975098
Clinet index 13, End of Epoch 1/6, Average Loss=7.088984489440918, Class Loss=0.7847158312797546, Reg Loss=6.304268836975098
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/23, Loss=16.75987091064453
Loss made of: CE 0.7269631624221802, LKD 6.86178731918335, LDE 0.0, LReg 0.0, POD 10.131965637207031 EntMin 0.0
Epoch 2, Batch 20/23, Loss=16.188989835977555
Loss made of: CE 0.677105188369751, LKD 6.5549821853637695, LDE 0.0, LReg 0.0, POD 8.585537910461426 EntMin 0.0
Epoch 2, Class Loss=0.6772645711898804, Reg Loss=6.348191261291504
Clinet index 13, End of Epoch 2/6, Average Loss=7.025455951690674, Class Loss=0.6772645711898804, Reg Loss=6.348191261291504
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/23, Loss=15.833502644300461
Loss made of: CE 0.6750829219818115, LKD 6.6614227294921875, LDE 0.0, LReg 0.0, POD 10.22050952911377 EntMin 0.0
Epoch 3, Batch 20/23, Loss=16.428925088047983
Loss made of: CE 0.5952121019363403, LKD 6.744681358337402, LDE 0.0, LReg 0.0, POD 10.992506980895996 EntMin 0.0
Epoch 3, Class Loss=0.6007503867149353, Reg Loss=6.3759355545043945
Clinet index 13, End of Epoch 3/6, Average Loss=6.976686000823975, Class Loss=0.6007503867149353, Reg Loss=6.3759355545043945
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/23, Loss=15.471685779094695
Loss made of: CE 0.6617407202720642, LKD 6.528712272644043, LDE 0.0, LReg 0.0, POD 8.281316757202148 EntMin 0.0
Epoch 4, Batch 20/23, Loss=16.00350923538208
Loss made of: CE 0.5917598605155945, LKD 6.506057262420654, LDE 0.0, LReg 0.0, POD 8.576935768127441 EntMin 0.0
Epoch 4, Class Loss=0.5765658020973206, Reg Loss=6.283006191253662
Clinet index 13, End of Epoch 4/6, Average Loss=6.859571933746338, Class Loss=0.5765658020973206, Reg Loss=6.283006191253662
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/23, Loss=15.149507468938827
Loss made of: CE 0.4906688332557678, LKD 5.702239036560059, LDE 0.0, LReg 0.0, POD 8.793245315551758 EntMin 0.0
Epoch 5, Batch 20/23, Loss=15.693784064054489
Loss made of: CE 0.5350181460380554, LKD 6.776798248291016, LDE 0.0, LReg 0.0, POD 8.181225776672363 EntMin 0.0
Epoch 5, Class Loss=0.5523620843887329, Reg Loss=6.3713765144348145
Clinet index 13, End of Epoch 5/6, Average Loss=6.923738479614258, Class Loss=0.5523620843887329, Reg Loss=6.3713765144348145
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/23, Loss=15.432681745290756
Loss made of: CE 0.7529335021972656, LKD 7.130621910095215, LDE 0.0, LReg 0.0, POD 9.636452674865723 EntMin 0.0
Epoch 6, Batch 20/23, Loss=15.519995245337487
Loss made of: CE 0.49664244055747986, LKD 6.850490570068359, LDE 0.0, LReg 0.0, POD 9.195682525634766 EntMin 0.0
Epoch 6, Class Loss=0.548347532749176, Reg Loss=6.344592571258545
Clinet index 13, End of Epoch 6/6, Average Loss=6.892940044403076, Class Loss=0.548347532749176, Reg Loss=6.344592571258545
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/23, Loss=17.725806564092636
Loss made of: CE 0.8040056228637695, LKD 6.983151912689209, LDE 0.0, LReg 0.0, POD 9.356245040893555 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/23, Loss=16.19289646744728
Loss made of: CE 0.7357840538024902, LKD 6.296828269958496, LDE 0.0, LReg 0.0, POD 8.790608406066895 EntMin 0.0
Epoch 1, Class Loss=0.7676575183868408, Reg Loss=6.736932754516602
Clinet index 10, End of Epoch 1/6, Average Loss=7.504590034484863, Class Loss=0.7676575183868408, Reg Loss=6.736932754516602
Pseudo labeling is: None
Epoch 2, lr = 0.000756
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/23, Loss=16.11060026884079
Loss made of: CE 0.7100863456726074, LKD 7.802907466888428, LDE 0.0, LReg 0.0, POD 9.907938003540039 EntMin 0.0
Epoch 2, Batch 20/23, Loss=15.497566410899163
Loss made of: CE 0.5608543157577515, LKD 7.226897239685059, LDE 0.0, LReg 0.0, POD 8.813981056213379 EntMin 0.0
Epoch 2, Class Loss=0.6518841981887817, Reg Loss=6.727018356323242
Clinet index 10, End of Epoch 2/6, Average Loss=7.378902435302734, Class Loss=0.6518841981887817, Reg Loss=6.727018356323242
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/23, Loss=15.15567564368248
Loss made of: CE 0.6115759611129761, LKD 6.8261871337890625, LDE 0.0, LReg 0.0, POD 8.907499313354492 EntMin 0.0
Epoch 3, Batch 20/23, Loss=15.686884039640427
Loss made of: CE 0.5591124296188354, LKD 6.18354606628418, LDE 0.0, LReg 0.0, POD 8.206453323364258 EntMin 0.0
Epoch 3, Class Loss=0.6178941130638123, Reg Loss=6.640665054321289
Clinet index 10, End of Epoch 3/6, Average Loss=7.258559226989746, Class Loss=0.6178941130638123, Reg Loss=6.640665054321289
Pseudo labeling is: None
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/23, Loss=15.132514628767968
Loss made of: CE 0.5493986010551453, LKD 6.813277244567871, LDE 0.0, LReg 0.0, POD 8.012687683105469 EntMin 0.0
Epoch 4, Batch 20/23, Loss=15.54631448984146
Loss made of: CE 0.5993074178695679, LKD 6.537095546722412, LDE 0.0, LReg 0.0, POD 8.272869110107422 EntMin 0.0
Epoch 4, Class Loss=0.5782756209373474, Reg Loss=6.709921360015869
Clinet index 10, End of Epoch 4/6, Average Loss=7.288197040557861, Class Loss=0.5782756209373474, Reg Loss=6.709921360015869
Pseudo labeling is: None
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/23, Loss=15.823520457744598
Loss made of: CE 0.5583989024162292, LKD 7.1345038414001465, LDE 0.0, LReg 0.0, POD 7.389866828918457 EntMin 0.0
Epoch 5, Batch 20/23, Loss=15.217854765057563
Loss made of: CE 0.7917555570602417, LKD 8.417060852050781, LDE 0.0, LReg 0.0, POD 9.179396629333496 EntMin 0.0
Epoch 5, Class Loss=0.5615667700767517, Reg Loss=6.773128509521484
Clinet index 10, End of Epoch 5/6, Average Loss=7.334695339202881, Class Loss=0.5615667700767517, Reg Loss=6.773128509521484
Pseudo labeling is: None
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/23, Loss=15.183694565296173
Loss made of: CE 0.3813715875148773, LKD 5.988245964050293, LDE 0.0, LReg 0.0, POD 7.769960403442383 EntMin 0.0
Epoch 6, Batch 20/23, Loss=15.157835793495178
Loss made of: CE 0.5355933904647827, LKD 6.299800395965576, LDE 0.0, LReg 0.0, POD 7.985539436340332 EntMin 0.0
Epoch 6, Class Loss=0.551075279712677, Reg Loss=6.746296405792236
Clinet index 10, End of Epoch 6/6, Average Loss=7.297371864318848, Class Loss=0.551075279712677, Reg Loss=6.746296405792236
federated aggregation...
Validation, Class Loss=0.554497480392456, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.834912
Mean Acc: 0.514254
FreqW Acc: 0.741796
Mean IoU: 0.361698
Class IoU:
	class 0: 0.8805996
	class 1: 0.6951925
	class 2: 0.26047564
	class 3: 0.4239632
	class 4: 0.51397127
	class 5: 0.05486528
	class 6: 0.6734817
	class 7: 0.5018878
	class 8: 0.35874483
	class 9: 0.0018643067
	class 10: 0.35755247
	class 11: 0.25758326
	class 12: 0.03982993
	class 13: 0.1117991
	class 14: 0.29305503
	class 15: 0.7239924
	class 16: 0.0
Class Acc:
	class 0: 0.9478977
	class 1: 0.70743823
	class 2: 0.65051335
	class 3: 0.92472595
	class 4: 0.88168997
	class 5: 0.054944612
	class 6: 0.704225
	class 7: 0.7252704
	class 8: 0.8408781
	class 9: 0.0018660314
	class 10: 0.48206627
	class 11: 0.4909176
	class 12: 0.040130753
	class 13: 0.11373578
	class 14: 0.3226276
	class 15: 0.85339385
	class 16: 0.0

federated global round: 19, step: 3
select part of clients to conduct local training
[4, 7, 17, 15]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/105, Loss=14.868765011429787
Loss made of: CE 0.49755820631980896, LKD 5.244379043579102, LDE 0.0, LReg 0.0, POD 9.472854614257812 EntMin 0.0
Epoch 1, Batch 20/105, Loss=14.353764802217484
Loss made of: CE 0.48517417907714844, LKD 5.150822639465332, LDE 0.0, LReg 0.0, POD 8.405416488647461 EntMin 0.0
Epoch 1, Batch 30/105, Loss=14.087204290926456
Loss made of: CE 0.39345455169677734, LKD 5.5377607345581055, LDE 0.0, LReg 0.0, POD 9.201608657836914 EntMin 0.0
Epoch 1, Batch 40/105, Loss=14.290227609872819
Loss made of: CE 0.3191276788711548, LKD 5.432826519012451, LDE 0.0, LReg 0.0, POD 7.597728729248047 EntMin 0.0
Epoch 1, Batch 50/105, Loss=14.105002444982528
Loss made of: CE 0.27623045444488525, LKD 5.112536430358887, LDE 0.0, LReg 0.0, POD 7.5292181968688965 EntMin 0.0
Epoch 1, Batch 60/105, Loss=14.786956161260605
Loss made of: CE 0.3376593589782715, LKD 4.967998027801514, LDE 0.0, LReg 0.0, POD 7.944379806518555 EntMin 0.0
Epoch 1, Batch 70/105, Loss=15.047616663575173
Loss made of: CE 0.2962420880794525, LKD 6.136115074157715, LDE 0.0, LReg 0.0, POD 8.034648895263672 EntMin 0.0
Epoch 1, Batch 80/105, Loss=14.279287412762642
Loss made of: CE 0.25075915455818176, LKD 4.891103744506836, LDE 0.0, LReg 0.0, POD 7.860379219055176 EntMin 0.0
Epoch 1, Batch 90/105, Loss=14.594871214032173
Loss made of: CE 0.2324577271938324, LKD 5.441065788269043, LDE 0.0, LReg 0.0, POD 8.1378173828125 EntMin 0.0
Epoch 1, Batch 100/105, Loss=14.668431335687638
Loss made of: CE 0.35977333784103394, LKD 5.409873962402344, LDE 0.0, LReg 0.0, POD 9.40526008605957 EntMin 0.0
Epoch 1, Class Loss=0.3512051999568939, Reg Loss=5.730132579803467
Clinet index 4, End of Epoch 1/6, Average Loss=6.081337928771973, Class Loss=0.3512051999568939, Reg Loss=5.730132579803467
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/105, Loss=14.570147593319415
Loss made of: CE 0.35892924666404724, LKD 6.062058925628662, LDE 0.0, LReg 0.0, POD 8.062071800231934 EntMin 0.0
Epoch 2, Batch 20/105, Loss=15.008281993865968
Loss made of: CE 0.2848370671272278, LKD 5.841350078582764, LDE 0.0, LReg 0.0, POD 8.967354774475098 EntMin 0.0
Epoch 2, Batch 30/105, Loss=14.41242818236351
Loss made of: CE 0.2707487940788269, LKD 5.559329986572266, LDE 0.0, LReg 0.0, POD 9.228130340576172 EntMin 0.0
Epoch 2, Batch 40/105, Loss=14.674551209807396
Loss made of: CE 0.3754662871360779, LKD 5.976879119873047, LDE 0.0, LReg 0.0, POD 8.545015335083008 EntMin 0.0
Epoch 2, Batch 50/105, Loss=14.763204422593116
Loss made of: CE 0.38086840510368347, LKD 5.962271690368652, LDE 0.0, LReg 0.0, POD 7.881704330444336 EntMin 0.0
Epoch 2, Batch 60/105, Loss=15.070385271310807
Loss made of: CE 0.2925381362438202, LKD 5.504891872406006, LDE 0.0, LReg 0.0, POD 8.505663871765137 EntMin 0.0
Epoch 2, Batch 70/105, Loss=14.704778179526329
Loss made of: CE 0.3781796395778656, LKD 5.743430137634277, LDE 0.0, LReg 0.0, POD 8.520009994506836 EntMin 0.0
Epoch 2, Batch 80/105, Loss=14.88716824054718
Loss made of: CE 0.35216712951660156, LKD 6.0418925285339355, LDE 0.0, LReg 0.0, POD 8.408529281616211 EntMin 0.0
Epoch 2, Batch 90/105, Loss=14.35234518200159
Loss made of: CE 0.2559484839439392, LKD 5.303131103515625, LDE 0.0, LReg 0.0, POD 7.955013751983643 EntMin 0.0
Epoch 2, Batch 100/105, Loss=14.55791293233633
Loss made of: CE 0.3332725167274475, LKD 5.5767621994018555, LDE 0.0, LReg 0.0, POD 8.7398681640625 EntMin 0.0
Epoch 2, Class Loss=0.34051835536956787, Reg Loss=5.731703758239746
Clinet index 4, End of Epoch 2/6, Average Loss=6.0722222328186035, Class Loss=0.34051835536956787, Reg Loss=5.731703758239746
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/105, Loss=14.972931149601937
Loss made of: CE 0.3840245008468628, LKD 5.518367767333984, LDE 0.0, LReg 0.0, POD 8.236833572387695 EntMin 0.0
Epoch 3, Batch 20/105, Loss=14.365035039186477
Loss made of: CE 0.40244776010513306, LKD 5.667891502380371, LDE 0.0, LReg 0.0, POD 7.867819786071777 EntMin 0.0
Epoch 3, Batch 30/105, Loss=14.158908820152282
Loss made of: CE 0.3707352578639984, LKD 5.933128356933594, LDE 0.0, LReg 0.0, POD 8.557670593261719 EntMin 0.0
Epoch 3, Batch 40/105, Loss=14.490926659107208
Loss made of: CE 0.3376379907131195, LKD 5.18889045715332, LDE 0.0, LReg 0.0, POD 8.48632526397705 EntMin 0.0
Epoch 3, Batch 50/105, Loss=14.170275139808655
Loss made of: CE 0.41462790966033936, LKD 5.813854217529297, LDE 0.0, LReg 0.0, POD 7.870087623596191 EntMin 0.0
Epoch 3, Batch 60/105, Loss=14.320895281434058
Loss made of: CE 0.3582291305065155, LKD 5.4125261306762695, LDE 0.0, LReg 0.0, POD 8.864130020141602 EntMin 0.0
Epoch 3, Batch 70/105, Loss=14.402751231193543
Loss made of: CE 0.3721548914909363, LKD 6.6171979904174805, LDE 0.0, LReg 0.0, POD 8.534908294677734 EntMin 0.0
Epoch 3, Batch 80/105, Loss=14.391149839758873
Loss made of: CE 0.3508676290512085, LKD 6.123422622680664, LDE 0.0, LReg 0.0, POD 7.083405494689941 EntMin 0.0
Epoch 3, Batch 90/105, Loss=14.349004024267197
Loss made of: CE 0.3875157833099365, LKD 5.7124176025390625, LDE 0.0, LReg 0.0, POD 8.271284103393555 EntMin 0.0
Epoch 3, Batch 100/105, Loss=14.317379692196846
Loss made of: CE 0.2755553722381592, LKD 6.255710601806641, LDE 0.0, LReg 0.0, POD 7.579929351806641 EntMin 0.0
Epoch 3, Class Loss=0.33659568428993225, Reg Loss=5.684194087982178
Clinet index 4, End of Epoch 3/6, Average Loss=6.020789623260498, Class Loss=0.33659568428993225, Reg Loss=5.684194087982178
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/105, Loss=13.566961032152175
Loss made of: CE 0.35117650032043457, LKD 4.741391658782959, LDE 0.0, LReg 0.0, POD 9.105752944946289 EntMin 0.0
Epoch 4, Batch 20/105, Loss=14.396856141090392
Loss made of: CE 0.29068881273269653, LKD 4.861948013305664, LDE 0.0, LReg 0.0, POD 8.089780807495117 EntMin 0.0
Epoch 4, Batch 30/105, Loss=14.422753283381462
Loss made of: CE 0.3699745535850525, LKD 6.20465087890625, LDE 0.0, LReg 0.0, POD 7.634938716888428 EntMin 0.0
Epoch 4, Batch 40/105, Loss=13.963191211223602
Loss made of: CE 0.3282744884490967, LKD 5.8534088134765625, LDE 0.0, LReg 0.0, POD 8.448936462402344 EntMin 0.0
Epoch 4, Batch 50/105, Loss=13.91796345114708
Loss made of: CE 0.376613974571228, LKD 6.0239386558532715, LDE 0.0, LReg 0.0, POD 7.783788681030273 EntMin 0.0
Epoch 4, Batch 60/105, Loss=13.959694746136666
Loss made of: CE 0.4309774339199066, LKD 5.371089935302734, LDE 0.0, LReg 0.0, POD 9.097044944763184 EntMin 0.0
Epoch 4, Batch 70/105, Loss=13.983435475826264
Loss made of: CE 0.35920053720474243, LKD 5.970100402832031, LDE 0.0, LReg 0.0, POD 7.2591938972473145 EntMin 0.0
Epoch 4, Batch 80/105, Loss=14.177751137316227
Loss made of: CE 0.3066639304161072, LKD 6.208995819091797, LDE 0.0, LReg 0.0, POD 7.790927886962891 EntMin 0.0
Epoch 4, Batch 90/105, Loss=14.585161605477333
Loss made of: CE 0.31473273038864136, LKD 5.837150573730469, LDE 0.0, LReg 0.0, POD 7.329115390777588 EntMin 0.0
Epoch 4, Batch 100/105, Loss=14.140749970078469
Loss made of: CE 0.30979812145233154, LKD 5.441103935241699, LDE 0.0, LReg 0.0, POD 6.910388946533203 EntMin 0.0
Epoch 4, Class Loss=0.3309744596481323, Reg Loss=5.695385932922363
Clinet index 4, End of Epoch 4/6, Average Loss=6.026360511779785, Class Loss=0.3309744596481323, Reg Loss=5.695385932922363
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/105, Loss=13.906884795427322
Loss made of: CE 0.28395527601242065, LKD 5.613288879394531, LDE 0.0, LReg 0.0, POD 7.773078918457031 EntMin 0.0
Epoch 5, Batch 20/105, Loss=14.13404925763607
Loss made of: CE 0.36643362045288086, LKD 6.341134071350098, LDE 0.0, LReg 0.0, POD 8.428571701049805 EntMin 0.0
Epoch 5, Batch 30/105, Loss=13.757818230986596
Loss made of: CE 0.32669731974601746, LKD 6.051421165466309, LDE 0.0, LReg 0.0, POD 7.7283124923706055 EntMin 0.0
Epoch 5, Batch 40/105, Loss=14.248272320628166
Loss made of: CE 0.24893364310264587, LKD 5.244301795959473, LDE 0.0, LReg 0.0, POD 7.896801948547363 EntMin 0.0
Epoch 5, Batch 50/105, Loss=13.663681027293205
Loss made of: CE 0.266536682844162, LKD 5.026705265045166, LDE 0.0, LReg 0.0, POD 7.121127605438232 EntMin 0.0
Epoch 5, Batch 60/105, Loss=14.414243619143964
Loss made of: CE 0.20129863917827606, LKD 4.859931468963623, LDE 0.0, LReg 0.0, POD 8.807662010192871 EntMin 0.0
Epoch 5, Batch 70/105, Loss=13.91554352939129
Loss made of: CE 0.3186158239841461, LKD 4.676610469818115, LDE 0.0, LReg 0.0, POD 10.165251731872559 EntMin 0.0
Epoch 5, Batch 80/105, Loss=14.007795226573943
Loss made of: CE 0.26115167140960693, LKD 4.939618110656738, LDE 0.0, LReg 0.0, POD 7.989950656890869 EntMin 0.0
Epoch 5, Batch 90/105, Loss=13.876086166501045
Loss made of: CE 0.28734028339385986, LKD 6.001288890838623, LDE 0.0, LReg 0.0, POD 6.927087783813477 EntMin 0.0
Epoch 5, Batch 100/105, Loss=13.683450403809548
Loss made of: CE 0.38000166416168213, LKD 5.8441314697265625, LDE 0.0, LReg 0.0, POD 7.647150993347168 EntMin 0.0
Epoch 5, Class Loss=0.3306860625743866, Reg Loss=5.702033042907715
Clinet index 4, End of Epoch 5/6, Average Loss=6.032719135284424, Class Loss=0.3306860625743866, Reg Loss=5.702033042907715
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/105, Loss=14.078751075267792
Loss made of: CE 0.28894078731536865, LKD 4.758281230926514, LDE 0.0, LReg 0.0, POD 6.9950103759765625 EntMin 0.0
Epoch 6, Batch 20/105, Loss=13.670267024636269
Loss made of: CE 0.3291900157928467, LKD 6.050632476806641, LDE 0.0, LReg 0.0, POD 7.1935272216796875 EntMin 0.0
Epoch 6, Batch 30/105, Loss=13.418011343479156
Loss made of: CE 0.2293190360069275, LKD 5.310900688171387, LDE 0.0, LReg 0.0, POD 8.364180564880371 EntMin 0.0
Epoch 6, Batch 40/105, Loss=13.995928999781608
Loss made of: CE 0.2867618203163147, LKD 5.604135036468506, LDE 0.0, LReg 0.0, POD 7.611756324768066 EntMin 0.0
Epoch 6, Batch 50/105, Loss=14.023561955988407
Loss made of: CE 0.2880873680114746, LKD 5.366117000579834, LDE 0.0, LReg 0.0, POD 10.00283145904541 EntMin 0.0
Epoch 6, Batch 60/105, Loss=13.760348154604435
Loss made of: CE 0.3621673583984375, LKD 6.074970245361328, LDE 0.0, LReg 0.0, POD 7.749163627624512 EntMin 0.0
Epoch 6, Batch 70/105, Loss=13.540034312009812
Loss made of: CE 0.2735225558280945, LKD 5.006507873535156, LDE 0.0, LReg 0.0, POD 7.1261091232299805 EntMin 0.0
Epoch 6, Batch 80/105, Loss=13.723961567878723
Loss made of: CE 0.2540231943130493, LKD 5.19254207611084, LDE 0.0, LReg 0.0, POD 7.316515922546387 EntMin 0.0
Epoch 6, Batch 90/105, Loss=13.6137890458107
Loss made of: CE 0.3117859661579132, LKD 5.510542869567871, LDE 0.0, LReg 0.0, POD 7.947432518005371 EntMin 0.0
Epoch 6, Batch 100/105, Loss=13.462815433740616
Loss made of: CE 0.2351660430431366, LKD 5.176482200622559, LDE 0.0, LReg 0.0, POD 7.0144805908203125 EntMin 0.0
Epoch 6, Class Loss=0.3248707354068756, Reg Loss=5.719115257263184
Clinet index 4, End of Epoch 6/6, Average Loss=6.043985843658447, Class Loss=0.3248707354068756, Reg Loss=5.719115257263184
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=14.475619742274285
Loss made of: CE 0.3847450911998749, LKD 6.181318283081055, LDE 0.0, LReg 0.0, POD 7.550177574157715 EntMin 0.0
Epoch 1, Batch 20/102, Loss=13.987067389488221
Loss made of: CE 0.3369600772857666, LKD 5.990212917327881, LDE 0.0, LReg 0.0, POD 8.85782241821289 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 30/102, Loss=14.08757121860981
Loss made of: CE 0.2773330509662628, LKD 5.576469421386719, LDE 0.0, LReg 0.0, POD 7.706381320953369 EntMin 0.0
Epoch 1, Batch 40/102, Loss=14.95587533712387
Loss made of: CE 0.2817738652229309, LKD 7.012882232666016, LDE 0.0, LReg 0.0, POD 8.82465648651123 EntMin 0.0
Epoch 1, Batch 50/102, Loss=14.716034483909606
Loss made of: CE 0.36199983954429626, LKD 5.925092697143555, LDE 0.0, LReg 0.0, POD 8.751935958862305 EntMin 0.0
Epoch 1, Batch 60/102, Loss=14.445268321037293
Loss made of: CE 0.3136354684829712, LKD 5.624907493591309, LDE 0.0, LReg 0.0, POD 7.2236809730529785 EntMin 0.0
Epoch 1, Batch 70/102, Loss=14.275497603416444
Loss made of: CE 0.3717753291130066, LKD 5.959840297698975, LDE 0.0, LReg 0.0, POD 7.642124176025391 EntMin 0.0
Epoch 1, Batch 80/102, Loss=14.625619539618492
Loss made of: CE 0.3891620337963104, LKD 6.862586975097656, LDE 0.0, LReg 0.0, POD 7.683218002319336 EntMin 0.0
Epoch 1, Batch 90/102, Loss=13.792025259137153
Loss made of: CE 0.312450647354126, LKD 5.569074630737305, LDE 0.0, LReg 0.0, POD 6.954983234405518 EntMin 0.0
Epoch 1, Batch 100/102, Loss=14.349971070885658
Loss made of: CE 0.36758995056152344, LKD 5.778799057006836, LDE 0.0, LReg 0.0, POD 7.479672431945801 EntMin 0.0
Epoch 1, Class Loss=0.34787169098854065, Reg Loss=5.933599472045898
Clinet index 7, End of Epoch 1/6, Average Loss=6.281471252441406, Class Loss=0.34787169098854065, Reg Loss=5.933599472045898
Pseudo labeling is: None
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/102, Loss=14.61594998240471
Loss made of: CE 0.3689543604850769, LKD 5.8756842613220215, LDE 0.0, LReg 0.0, POD 7.417360305786133 EntMin 0.0
Epoch 2, Batch 20/102, Loss=14.033460110425949
Loss made of: CE 0.31132227182388306, LKD 5.6703290939331055, LDE 0.0, LReg 0.0, POD 8.220784187316895 EntMin 0.0
Epoch 2, Batch 30/102, Loss=14.379026016592979
Loss made of: CE 0.339945912361145, LKD 6.360481262207031, LDE 0.0, LReg 0.0, POD 8.820314407348633 EntMin 0.0
Epoch 2, Batch 40/102, Loss=14.241207066178323
Loss made of: CE 0.3141758441925049, LKD 5.846953392028809, LDE 0.0, LReg 0.0, POD 9.206740379333496 EntMin 0.0
Epoch 2, Batch 50/102, Loss=14.413405534625053
Loss made of: CE 0.23550140857696533, LKD 4.970211029052734, LDE 0.0, LReg 0.0, POD 7.709366798400879 EntMin 0.0
Epoch 2, Batch 60/102, Loss=14.472361236810684
Loss made of: CE 0.3124074339866638, LKD 5.1534743309021, LDE 0.0, LReg 0.0, POD 7.0637526512146 EntMin 0.0
Epoch 2, Batch 70/102, Loss=14.550031900405884
Loss made of: CE 0.33123040199279785, LKD 5.697387218475342, LDE 0.0, LReg 0.0, POD 8.386543273925781 EntMin 0.0
Epoch 2, Batch 80/102, Loss=13.93923515379429
Loss made of: CE 0.2820734977722168, LKD 5.454250335693359, LDE 0.0, LReg 0.0, POD 7.528437614440918 EntMin 0.0
Epoch 2, Batch 90/102, Loss=14.396669952571392
Loss made of: CE 0.2810712456703186, LKD 5.55130672454834, LDE 0.0, LReg 0.0, POD 7.659201145172119 EntMin 0.0
Epoch 2, Batch 100/102, Loss=14.13434221148491
Loss made of: CE 0.2638959288597107, LKD 5.284605979919434, LDE 0.0, LReg 0.0, POD 8.490382194519043 EntMin 0.0
Epoch 2, Class Loss=0.3354695439338684, Reg Loss=5.895175457000732
Clinet index 7, End of Epoch 2/6, Average Loss=6.230645179748535, Class Loss=0.3354695439338684, Reg Loss=5.895175457000732
Pseudo labeling is: None
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/102, Loss=14.726804399490357
Loss made of: CE 0.36280936002731323, LKD 6.344750881195068, LDE 0.0, LReg 0.0, POD 7.651890754699707 EntMin 0.0
Epoch 3, Batch 20/102, Loss=13.805539253354073
Loss made of: CE 0.37759289145469666, LKD 6.368037223815918, LDE 0.0, LReg 0.0, POD 6.849878311157227 EntMin 0.0
Epoch 3, Batch 30/102, Loss=14.082957857847214
Loss made of: CE 0.32991474866867065, LKD 6.115993499755859, LDE 0.0, LReg 0.0, POD 7.6994948387146 EntMin 0.0
Epoch 3, Batch 40/102, Loss=14.103758826851845
Loss made of: CE 0.3034762144088745, LKD 4.888806343078613, LDE 0.0, LReg 0.0, POD 8.157364845275879 EntMin 0.0
Epoch 3, Batch 50/102, Loss=14.421266436576843
Loss made of: CE 0.31507402658462524, LKD 5.558950424194336, LDE 0.0, LReg 0.0, POD 9.32466983795166 EntMin 0.0
Epoch 3, Batch 60/102, Loss=13.902765068411828
Loss made of: CE 0.2555250823497772, LKD 5.920356273651123, LDE 0.0, LReg 0.0, POD 7.456657409667969 EntMin 0.0
Epoch 3, Batch 70/102, Loss=14.165212681889534
Loss made of: CE 0.2811354100704193, LKD 6.872353553771973, LDE 0.0, LReg 0.0, POD 7.547093391418457 EntMin 0.0
Epoch 3, Batch 80/102, Loss=13.829383766651153
Loss made of: CE 0.32492387294769287, LKD 6.8040771484375, LDE 0.0, LReg 0.0, POD 7.380118370056152 EntMin 0.0
Epoch 3, Batch 90/102, Loss=14.557110542058945
Loss made of: CE 0.33845242857933044, LKD 6.446704864501953, LDE 0.0, LReg 0.0, POD 7.592597961425781 EntMin 0.0
Epoch 3, Batch 100/102, Loss=13.774608345329762
Loss made of: CE 0.3123020529747009, LKD 5.678112030029297, LDE 0.0, LReg 0.0, POD 7.631255149841309 EntMin 0.0
Epoch 3, Class Loss=0.3381410241127014, Reg Loss=5.899395942687988
Clinet index 7, End of Epoch 3/6, Average Loss=6.237536907196045, Class Loss=0.3381410241127014, Reg Loss=5.899395942687988
Pseudo labeling is: None
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=13.8317049741745
Loss made of: CE 0.3223728835582733, LKD 5.7659454345703125, LDE 0.0, LReg 0.0, POD 7.6315460205078125 EntMin 0.0
Epoch 4, Batch 20/102, Loss=14.081830358505249
Loss made of: CE 0.4621325433254242, LKD 7.0809173583984375, LDE 0.0, LReg 0.0, POD 8.489425659179688 EntMin 0.0
Epoch 4, Batch 30/102, Loss=14.217976775765418
Loss made of: CE 0.2772854268550873, LKD 5.744563102722168, LDE 0.0, LReg 0.0, POD 6.993109703063965 EntMin 0.0
Epoch 4, Batch 40/102, Loss=13.927164828777313
Loss made of: CE 0.3343660235404968, LKD 5.49925422668457, LDE 0.0, LReg 0.0, POD 7.951480865478516 EntMin 0.0
Epoch 4, Batch 50/102, Loss=13.889203828573226
Loss made of: CE 0.3634083867073059, LKD 5.860951900482178, LDE 0.0, LReg 0.0, POD 7.265634536743164 EntMin 0.0
Epoch 4, Batch 60/102, Loss=14.12536887228489
Loss made of: CE 0.379067599773407, LKD 5.658695220947266, LDE 0.0, LReg 0.0, POD 7.7547125816345215 EntMin 0.0
Epoch 4, Batch 70/102, Loss=13.83085176050663
Loss made of: CE 0.33076921105384827, LKD 5.901696681976318, LDE 0.0, LReg 0.0, POD 7.070723533630371 EntMin 0.0
Epoch 4, Batch 80/102, Loss=14.02842104434967
Loss made of: CE 0.316951721906662, LKD 6.246011257171631, LDE 0.0, LReg 0.0, POD 6.993879318237305 EntMin 0.0
Epoch 4, Batch 90/102, Loss=13.952070325613022
Loss made of: CE 0.3207688629627228, LKD 5.172267913818359, LDE 0.0, LReg 0.0, POD 8.123140335083008 EntMin 0.0
Epoch 4, Batch 100/102, Loss=14.247966253757477
Loss made of: CE 0.26937106251716614, LKD 5.320044994354248, LDE 0.0, LReg 0.0, POD 7.242941379547119 EntMin 0.0
Epoch 4, Class Loss=0.33627697825431824, Reg Loss=5.885948181152344
Clinet index 7, End of Epoch 4/6, Average Loss=6.222225189208984, Class Loss=0.33627697825431824, Reg Loss=5.885948181152344
Pseudo labeling is: None
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/102, Loss=14.161328300833702
Loss made of: CE 0.3457048833370209, LKD 6.712120056152344, LDE 0.0, LReg 0.0, POD 8.623211860656738 EntMin 0.0
Epoch 5, Batch 20/102, Loss=13.923640844225883
Loss made of: CE 0.29940927028656006, LKD 5.526912689208984, LDE 0.0, LReg 0.0, POD 8.225711822509766 EntMin 0.0
Epoch 5, Batch 30/102, Loss=13.959634774923325
Loss made of: CE 0.41490209102630615, LKD 5.89882755279541, LDE 0.0, LReg 0.0, POD 7.778183460235596 EntMin 0.0
Epoch 5, Batch 40/102, Loss=13.842286908626557
Loss made of: CE 0.3093520998954773, LKD 5.937376022338867, LDE 0.0, LReg 0.0, POD 6.863357067108154 EntMin 0.0
Epoch 5, Batch 50/102, Loss=13.595204520225526
Loss made of: CE 0.4421221911907196, LKD 5.877788543701172, LDE 0.0, LReg 0.0, POD 7.792996883392334 EntMin 0.0
Epoch 5, Batch 60/102, Loss=13.790770834684372
Loss made of: CE 0.29189997911453247, LKD 6.390254020690918, LDE 0.0, LReg 0.0, POD 6.526242256164551 EntMin 0.0
Epoch 5, Batch 70/102, Loss=14.359862877428531
Loss made of: CE 0.32421761751174927, LKD 5.433322906494141, LDE 0.0, LReg 0.0, POD 8.295458793640137 EntMin 0.0
Epoch 5, Batch 80/102, Loss=14.149777695536613
Loss made of: CE 0.29289114475250244, LKD 5.8947601318359375, LDE 0.0, LReg 0.0, POD 8.52558708190918 EntMin 0.0
Epoch 5, Batch 90/102, Loss=13.972667470574379
Loss made of: CE 0.48638805747032166, LKD 6.7886762619018555, LDE 0.0, LReg 0.0, POD 7.279623031616211 EntMin 0.0
Epoch 5, Batch 100/102, Loss=13.538716161251068
Loss made of: CE 0.27221232652664185, LKD 5.361701011657715, LDE 0.0, LReg 0.0, POD 7.976066589355469 EntMin 0.0
Epoch 5, Class Loss=0.33436891436576843, Reg Loss=5.905383586883545
Clinet index 7, End of Epoch 5/6, Average Loss=6.239752292633057, Class Loss=0.33436891436576843, Reg Loss=5.905383586883545
Pseudo labeling is: None
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/102, Loss=13.984113864600658
Loss made of: CE 0.3870888352394104, LKD 6.639788627624512, LDE 0.0, LReg 0.0, POD 6.642055988311768 EntMin 0.0
Epoch 6, Batch 20/102, Loss=14.190474718809128
Loss made of: CE 0.33062314987182617, LKD 6.809477806091309, LDE 0.0, LReg 0.0, POD 6.575664520263672 EntMin 0.0
Epoch 6, Batch 30/102, Loss=13.707270427048206
Loss made of: CE 0.3367815613746643, LKD 5.926117897033691, LDE 0.0, LReg 0.0, POD 7.022729396820068 EntMin 0.0
Epoch 6, Batch 40/102, Loss=14.383908200263978
Loss made of: CE 0.3388541638851166, LKD 6.260465621948242, LDE 0.0, LReg 0.0, POD 6.67245626449585 EntMin 0.0
Epoch 6, Batch 50/102, Loss=13.776945704221726
Loss made of: CE 0.45279181003570557, LKD 5.57272481918335, LDE 0.0, LReg 0.0, POD 7.4770307540893555 EntMin 0.0
Epoch 6, Batch 60/102, Loss=13.4217849701643
Loss made of: CE 0.2669638991355896, LKD 5.572345733642578, LDE 0.0, LReg 0.0, POD 7.24781608581543 EntMin 0.0
Epoch 6, Batch 70/102, Loss=13.436815741658211
Loss made of: CE 0.3473319411277771, LKD 6.193292140960693, LDE 0.0, LReg 0.0, POD 7.471683025360107 EntMin 0.0
Epoch 6, Batch 80/102, Loss=13.70021532177925
Loss made of: CE 0.28881654143333435, LKD 6.339377403259277, LDE 0.0, LReg 0.0, POD 8.090539932250977 EntMin 0.0
Epoch 6, Batch 90/102, Loss=13.503525018692017
Loss made of: CE 0.3061079978942871, LKD 6.056393623352051, LDE 0.0, LReg 0.0, POD 7.633073806762695 EntMin 0.0
Epoch 6, Batch 100/102, Loss=13.43191446363926
Loss made of: CE 0.4156394898891449, LKD 5.908759593963623, LDE 0.0, LReg 0.0, POD 6.71766471862793 EntMin 0.0
Epoch 6, Class Loss=0.3325396180152893, Reg Loss=5.900933742523193
Clinet index 7, End of Epoch 6/6, Average Loss=6.233473300933838, Class Loss=0.3325396180152893, Reg Loss=5.900933742523193
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/23, Loss=16.65782368183136
Loss made of: CE 0.5889167785644531, LKD 6.190115928649902, LDE 0.0, LReg 0.0, POD 8.751411437988281 EntMin 0.0
Epoch 1, Batch 20/23, Loss=15.778842270374298
Loss made of: CE 0.6890729665756226, LKD 6.466456413269043, LDE 0.0, LReg 0.0, POD 9.296379089355469 EntMin 0.0
Epoch 1, Class Loss=0.6745546460151672, Reg Loss=6.247073650360107
Clinet index 17, End of Epoch 1/6, Average Loss=6.921628475189209, Class Loss=0.6745546460151672, Reg Loss=6.247073650360107
Pseudo labeling is: None
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/23, Loss=15.998442816734315
Loss made of: CE 0.659066379070282, LKD 5.742798805236816, LDE 0.0, LReg 0.0, POD 8.628374099731445 EntMin 0.0
Epoch 2, Batch 20/23, Loss=15.354726520180701
Loss made of: CE 0.45667800307273865, LKD 6.6703619956970215, LDE 0.0, LReg 0.0, POD 9.201088905334473 EntMin 0.0
Epoch 2, Class Loss=0.6087288856506348, Reg Loss=6.241325378417969
Clinet index 17, End of Epoch 2/6, Average Loss=6.8500542640686035, Class Loss=0.6087288856506348, Reg Loss=6.241325378417969
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/23, Loss=15.306287956237792
Loss made of: CE 0.47926172614097595, LKD 5.801447868347168, LDE 0.0, LReg 0.0, POD 8.363748550415039 EntMin 0.0
Epoch 3, Batch 20/23, Loss=15.399797013401985
Loss made of: CE 0.6557384729385376, LKD 6.916350364685059, LDE 0.0, LReg 0.0, POD 8.951582908630371 EntMin 0.0
Epoch 3, Class Loss=0.5794881582260132, Reg Loss=6.184675216674805
Clinet index 17, End of Epoch 3/6, Average Loss=6.764163494110107, Class Loss=0.5794881582260132, Reg Loss=6.184675216674805
Pseudo labeling is: None
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/23, Loss=15.692566552758217
Loss made of: CE 0.8006452322006226, LKD 7.1046881675720215, LDE 0.0, LReg 0.0, POD 8.510811805725098 EntMin 0.0
Epoch 4, Batch 20/23, Loss=15.214878642559052
Loss made of: CE 0.5067321062088013, LKD 6.7313151359558105, LDE 0.0, LReg 0.0, POD 8.516929626464844 EntMin 0.0
Epoch 4, Class Loss=0.5659512877464294, Reg Loss=6.219916343688965
Clinet index 17, End of Epoch 4/6, Average Loss=6.785867691040039, Class Loss=0.5659512877464294, Reg Loss=6.219916343688965
Pseudo labeling is: None
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/23, Loss=15.22949723303318
Loss made of: CE 0.45273834466934204, LKD 4.993910312652588, LDE 0.0, LReg 0.0, POD 8.486303329467773 EntMin 0.0
Epoch 5, Batch 20/23, Loss=14.859025618433952
Loss made of: CE 0.48784953355789185, LKD 6.321352005004883, LDE 0.0, LReg 0.0, POD 8.834604263305664 EntMin 0.0
Epoch 5, Class Loss=0.5564428567886353, Reg Loss=6.224809646606445
Clinet index 17, End of Epoch 5/6, Average Loss=6.781252384185791, Class Loss=0.5564428567886353, Reg Loss=6.224809646606445
Pseudo labeling is: None
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/23, Loss=15.265931421518326
Loss made of: CE 0.502776563167572, LKD 5.649434566497803, LDE 0.0, LReg 0.0, POD 7.922856330871582 EntMin 0.0
Epoch 6, Batch 20/23, Loss=14.654780265688895
Loss made of: CE 0.701318621635437, LKD 5.92862606048584, LDE 0.0, LReg 0.0, POD 8.822930335998535 EntMin 0.0
Epoch 6, Class Loss=0.5440398454666138, Reg Loss=6.193299293518066
Clinet index 17, End of Epoch 6/6, Average Loss=6.737339019775391, Class Loss=0.5440398454666138, Reg Loss=6.193299293518066
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/103, Loss=14.198800686001778
Loss made of: CE 0.3567042946815491, LKD 5.935721397399902, LDE 0.0, LReg 0.0, POD 7.682427406311035 EntMin 0.0
Epoch 1, Batch 20/103, Loss=14.099554841220378
Loss made of: CE 0.3448143005371094, LKD 5.810722351074219, LDE 0.0, LReg 0.0, POD 7.87855339050293 EntMin 0.0
Epoch 1, Batch 30/103, Loss=14.058250349760055
Loss made of: CE 0.3140469193458557, LKD 5.765873908996582, LDE 0.0, LReg 0.0, POD 8.252176284790039 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 40/103, Loss=14.490557840466499
Loss made of: CE 0.30685240030288696, LKD 5.220497131347656, LDE 0.0, LReg 0.0, POD 7.6071271896362305 EntMin 0.0
Epoch 1, Batch 50/103, Loss=14.63837473988533
Loss made of: CE 0.3474195897579193, LKD 5.906615257263184, LDE 0.0, LReg 0.0, POD 9.941776275634766 EntMin 0.0
Epoch 1, Batch 60/103, Loss=14.71625845581293
Loss made of: CE 0.3486050069332123, LKD 5.685105800628662, LDE 0.0, LReg 0.0, POD 9.069005966186523 EntMin 0.0
Epoch 1, Batch 70/103, Loss=14.789737170934677
Loss made of: CE 0.32848989963531494, LKD 5.574269771575928, LDE 0.0, LReg 0.0, POD 8.411773681640625 EntMin 0.0
Epoch 1, Batch 80/103, Loss=15.089582945406438
Loss made of: CE 0.2960796058177948, LKD 5.263873100280762, LDE 0.0, LReg 0.0, POD 9.359661102294922 EntMin 0.0
Epoch 1, Batch 90/103, Loss=13.972798857092858
Loss made of: CE 0.3044840097427368, LKD 5.603056907653809, LDE 0.0, LReg 0.0, POD 7.80699348449707 EntMin 0.0
Epoch 1, Batch 100/103, Loss=14.969081681966781
Loss made of: CE 0.3185872435569763, LKD 6.356807231903076, LDE 0.0, LReg 0.0, POD 7.990911960601807 EntMin 0.0
Epoch 1, Class Loss=0.34370309114456177, Reg Loss=5.7594499588012695
Clinet index 15, End of Epoch 1/6, Average Loss=6.103153228759766, Class Loss=0.34370309114456177, Reg Loss=5.7594499588012695
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/103, Loss=15.223015286028385
Loss made of: CE 0.4436570405960083, LKD 6.200359344482422, LDE 0.0, LReg 0.0, POD 10.313542366027832 EntMin 0.0
Epoch 2, Batch 20/103, Loss=14.845021432638168
Loss made of: CE 0.28051888942718506, LKD 5.641330718994141, LDE 0.0, LReg 0.0, POD 8.536312103271484 EntMin 0.0
Epoch 2, Batch 30/103, Loss=14.746673999726772
Loss made of: CE 0.29596543312072754, LKD 5.664516448974609, LDE 0.0, LReg 0.0, POD 9.326250076293945 EntMin 0.0
Epoch 2, Batch 40/103, Loss=14.432061564922332
Loss made of: CE 0.39791053533554077, LKD 5.744750022888184, LDE 0.0, LReg 0.0, POD 8.93998908996582 EntMin 0.0
Epoch 2, Batch 50/103, Loss=14.393872950971126
Loss made of: CE 0.26633012294769287, LKD 5.878654479980469, LDE 0.0, LReg 0.0, POD 7.885076999664307 EntMin 0.0
Epoch 2, Batch 60/103, Loss=15.272767096757889
Loss made of: CE 0.25153547525405884, LKD 5.394634246826172, LDE 0.0, LReg 0.0, POD 8.920233726501465 EntMin 0.0
Epoch 2, Batch 70/103, Loss=14.500427421927451
Loss made of: CE 0.3665750026702881, LKD 5.759403228759766, LDE 0.0, LReg 0.0, POD 8.198786735534668 EntMin 0.0
Epoch 2, Batch 80/103, Loss=14.10554396212101
Loss made of: CE 0.3656464219093323, LKD 5.536996364593506, LDE 0.0, LReg 0.0, POD 8.916006088256836 EntMin 0.0
Epoch 2, Batch 90/103, Loss=14.167061273753642
Loss made of: CE 0.24411199986934662, LKD 4.42292594909668, LDE 0.0, LReg 0.0, POD 9.637014389038086 EntMin 0.0
Epoch 2, Batch 100/103, Loss=15.075140058994293
Loss made of: CE 0.3130154609680176, LKD 5.214787483215332, LDE 0.0, LReg 0.0, POD 8.96401596069336 EntMin 0.0
Epoch 2, Class Loss=0.332920104265213, Reg Loss=5.739283561706543
Clinet index 15, End of Epoch 2/6, Average Loss=6.072203636169434, Class Loss=0.332920104265213, Reg Loss=5.739283561706543
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/103, Loss=14.93140380680561
Loss made of: CE 0.2721620500087738, LKD 5.671462535858154, LDE 0.0, LReg 0.0, POD 8.487604141235352 EntMin 0.0
Epoch 3, Batch 20/103, Loss=14.144909635186195
Loss made of: CE 0.3433903157711029, LKD 5.486536026000977, LDE 0.0, LReg 0.0, POD 8.210296630859375 EntMin 0.0
Epoch 3, Batch 30/103, Loss=14.127730140089989
Loss made of: CE 0.32285380363464355, LKD 6.490529537200928, LDE 0.0, LReg 0.0, POD 7.7833662033081055 EntMin 0.0
Epoch 3, Batch 40/103, Loss=14.783072051405906
Loss made of: CE 0.2544209361076355, LKD 5.588071823120117, LDE 0.0, LReg 0.0, POD 8.580527305603027 EntMin 0.0
Epoch 3, Batch 50/103, Loss=14.886719289422036
Loss made of: CE 0.41410085558891296, LKD 6.116971015930176, LDE 0.0, LReg 0.0, POD 10.239299774169922 EntMin 0.0
Epoch 3, Batch 60/103, Loss=14.033905902504921
Loss made of: CE 0.2753961384296417, LKD 5.6936445236206055, LDE 0.0, LReg 0.0, POD 7.3133544921875 EntMin 0.0
Epoch 3, Batch 70/103, Loss=14.461353388428687
Loss made of: CE 0.2753481864929199, LKD 5.850179195404053, LDE 0.0, LReg 0.0, POD 10.078214645385742 EntMin 0.0
Epoch 3, Batch 80/103, Loss=14.555326753854752
Loss made of: CE 0.34464165568351746, LKD 5.764611721038818, LDE 0.0, LReg 0.0, POD 7.683849334716797 EntMin 0.0
Epoch 3, Batch 90/103, Loss=14.508909916877746
Loss made of: CE 0.3099188804626465, LKD 5.846182823181152, LDE 0.0, LReg 0.0, POD 8.091986656188965 EntMin 0.0
Epoch 3, Batch 100/103, Loss=14.567069637775422
Loss made of: CE 0.3945632874965668, LKD 6.047286033630371, LDE 0.0, LReg 0.0, POD 9.091886520385742 EntMin 0.0
Epoch 3, Class Loss=0.3276783525943756, Reg Loss=5.745004653930664
Clinet index 15, End of Epoch 3/6, Average Loss=6.072682857513428, Class Loss=0.3276783525943756, Reg Loss=5.745004653930664
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/103, Loss=14.417804378271104
Loss made of: CE 0.3601723313331604, LKD 6.442255973815918, LDE 0.0, LReg 0.0, POD 8.047943115234375 EntMin 0.0
Epoch 4, Batch 20/103, Loss=14.61798537671566
Loss made of: CE 0.443103551864624, LKD 6.07816219329834, LDE 0.0, LReg 0.0, POD 9.559857368469238 EntMin 0.0
Epoch 4, Batch 30/103, Loss=14.377759405970574
Loss made of: CE 0.34069961309432983, LKD 5.671567440032959, LDE 0.0, LReg 0.0, POD 8.692818641662598 EntMin 0.0
Epoch 4, Batch 40/103, Loss=13.748516955971718
Loss made of: CE 0.31220611929893494, LKD 5.332605838775635, LDE 0.0, LReg 0.0, POD 7.885900020599365 EntMin 0.0
Epoch 4, Batch 50/103, Loss=14.061795109510422
Loss made of: CE 0.23417994379997253, LKD 5.627439022064209, LDE 0.0, LReg 0.0, POD 7.984893798828125 EntMin 0.0
Epoch 4, Batch 60/103, Loss=14.491699741780758
Loss made of: CE 0.32209691405296326, LKD 6.357512474060059, LDE 0.0, LReg 0.0, POD 7.974493503570557 EntMin 0.0
Epoch 4, Batch 70/103, Loss=14.28574321269989
Loss made of: CE 0.31315866112709045, LKD 5.17979097366333, LDE 0.0, LReg 0.0, POD 8.620543479919434 EntMin 0.0
Epoch 4, Batch 80/103, Loss=14.109599114954472
Loss made of: CE 0.32685402035713196, LKD 5.176146984100342, LDE 0.0, LReg 0.0, POD 7.511647701263428 EntMin 0.0
Epoch 4, Batch 90/103, Loss=13.937574391067027
Loss made of: CE 0.26587921380996704, LKD 5.391408443450928, LDE 0.0, LReg 0.0, POD 7.965823650360107 EntMin 0.0
Epoch 4, Batch 100/103, Loss=14.161972665786744
Loss made of: CE 0.28619086742401123, LKD 4.929645538330078, LDE 0.0, LReg 0.0, POD 7.748385429382324 EntMin 0.0
Epoch 4, Class Loss=0.32369524240493774, Reg Loss=5.744124889373779
Clinet index 15, End of Epoch 4/6, Average Loss=6.067820072174072, Class Loss=0.32369524240493774, Reg Loss=5.744124889373779
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/103, Loss=14.178997030854225
Loss made of: CE 0.2915329933166504, LKD 5.531375408172607, LDE 0.0, LReg 0.0, POD 7.85462760925293 EntMin 0.0
Epoch 5, Batch 20/103, Loss=14.282856833934783
Loss made of: CE 0.3827529549598694, LKD 5.241391181945801, LDE 0.0, LReg 0.0, POD 10.056570053100586 EntMin 0.0
Epoch 5, Batch 30/103, Loss=13.74146331846714
Loss made of: CE 0.2823329567909241, LKD 4.966564178466797, LDE 0.0, LReg 0.0, POD 8.88134765625 EntMin 0.0
Epoch 5, Batch 40/103, Loss=14.352953465282917
Loss made of: CE 0.24066989123821259, LKD 4.945291519165039, LDE 0.0, LReg 0.0, POD 8.846658706665039 EntMin 0.0
Epoch 5, Batch 50/103, Loss=13.805876687169075
Loss made of: CE 0.2257450819015503, LKD 5.259387969970703, LDE 0.0, LReg 0.0, POD 7.6257829666137695 EntMin 0.0
Epoch 5, Batch 60/103, Loss=14.042000675201416
Loss made of: CE 0.3211180567741394, LKD 5.538898468017578, LDE 0.0, LReg 0.0, POD 7.30448055267334 EntMin 0.0
Epoch 5, Batch 70/103, Loss=14.294127136468887
Loss made of: CE 0.2916463315486908, LKD 6.16884708404541, LDE 0.0, LReg 0.0, POD 8.1348876953125 EntMin 0.0
Epoch 5, Batch 80/103, Loss=14.236613497138023
Loss made of: CE 0.37595582008361816, LKD 5.058727741241455, LDE 0.0, LReg 0.0, POD 7.545302867889404 EntMin 0.0
Epoch 5, Batch 90/103, Loss=14.360691903531551
Loss made of: CE 0.3332909345626831, LKD 6.3384528160095215, LDE 0.0, LReg 0.0, POD 8.639742851257324 EntMin 0.0
Epoch 5, Batch 100/103, Loss=14.231207066774369
Loss made of: CE 0.356813907623291, LKD 5.88370418548584, LDE 0.0, LReg 0.0, POD 7.89825963973999 EntMin 0.0
Epoch 5, Class Loss=0.3253580629825592, Reg Loss=5.754340648651123
Clinet index 15, End of Epoch 5/6, Average Loss=6.07969856262207, Class Loss=0.3253580629825592, Reg Loss=5.754340648651123
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/103, Loss=13.778407754004002
Loss made of: CE 0.3282897472381592, LKD 5.864171028137207, LDE 0.0, LReg 0.0, POD 6.8681230545043945 EntMin 0.0
Epoch 6, Batch 20/103, Loss=14.168916860222817
Loss made of: CE 0.3150199055671692, LKD 6.01883602142334, LDE 0.0, LReg 0.0, POD 6.978603839874268 EntMin 0.0
Epoch 6, Batch 30/103, Loss=13.743259471654891
Loss made of: CE 0.28260666131973267, LKD 4.978025436401367, LDE 0.0, LReg 0.0, POD 8.154695510864258 EntMin 0.0
Epoch 6, Batch 40/103, Loss=13.45009858906269
Loss made of: CE 0.3255375027656555, LKD 6.457450866699219, LDE 0.0, LReg 0.0, POD 7.124087333679199 EntMin 0.0
Epoch 6, Batch 50/103, Loss=13.723026725649834
Loss made of: CE 0.25578659772872925, LKD 4.967167854309082, LDE 0.0, LReg 0.0, POD 7.33432674407959 EntMin 0.0
Epoch 6, Batch 60/103, Loss=13.677538532018662
Loss made of: CE 0.2635480761528015, LKD 5.682713985443115, LDE 0.0, LReg 0.0, POD 8.683013916015625 EntMin 0.0
Epoch 6, Batch 70/103, Loss=14.226151664555072
Loss made of: CE 0.36420148611068726, LKD 6.281394004821777, LDE 0.0, LReg 0.0, POD 8.47239875793457 EntMin 0.0
Epoch 6, Batch 80/103, Loss=14.026119276881218
Loss made of: CE 0.4211674630641937, LKD 5.9530744552612305, LDE 0.0, LReg 0.0, POD 7.392938613891602 EntMin 0.0
Epoch 6, Batch 90/103, Loss=13.906838549673557
Loss made of: CE 0.29324445128440857, LKD 6.079604148864746, LDE 0.0, LReg 0.0, POD 7.644674301147461 EntMin 0.0
Epoch 6, Batch 100/103, Loss=14.17076804637909
Loss made of: CE 0.3211808502674103, LKD 5.204988479614258, LDE 0.0, LReg 0.0, POD 7.712292671203613 EntMin 0.0
Epoch 6, Class Loss=0.3235403597354889, Reg Loss=5.739957332611084
Clinet index 15, End of Epoch 6/6, Average Loss=6.063497543334961, Class Loss=0.3235403597354889, Reg Loss=5.739957332611084
federated aggregation...
Validation, Class Loss=0.5535328388214111, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.836075
Mean Acc: 0.521286
FreqW Acc: 0.744105
Mean IoU: 0.368102
Class IoU:
	class 0: 0.88145703
	class 1: 0.7077294
	class 2: 0.25792906
	class 3: 0.41651195
	class 4: 0.51072556
	class 5: 0.051964812
	class 6: 0.69120854
	class 7: 0.51268077
	class 8: 0.35529628
	class 9: 0.0009220638
	class 10: 0.35858592
	class 11: 0.25589654
	class 12: 0.043623123
	class 13: 0.16804738
	class 14: 0.32449436
	class 15: 0.7205762
	class 16: 9.312056e-05
Class Acc:
	class 0: 0.946069
	class 1: 0.7207333
	class 2: 0.655367
	class 3: 0.9263769
	class 4: 0.8919798
	class 5: 0.05203575
	class 6: 0.7266562
	class 7: 0.7214107
	class 8: 0.8273604
	class 9: 0.00092256855
	class 10: 0.45581388
	class 11: 0.5003862
	class 12: 0.043970965
	class 13: 0.17277229
	class 14: 0.35197982
	class 15: 0.867942
	class 16: 9.312139e-05

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[19, 23, 1, 8]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=23.381463623046876
Loss made of: CE 1.4377472400665283, LKD 6.223077774047852, LDE 0.0, LReg 0.0, POD 12.660961151123047 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/24, Loss=20.13021640777588
Loss made of: CE 1.068959355354309, LKD 5.518553733825684, LDE 0.0, LReg 0.0, POD 11.825080871582031 EntMin 0.0
Epoch 1, Class Loss=1.2675950527191162, Reg Loss=5.834076881408691
Clinet index 19, End of Epoch 1/6, Average Loss=7.101672172546387, Class Loss=1.2675950527191162, Reg Loss=5.834076881408691
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=19.65777603983879
Loss made of: CE 0.9206501841545105, LKD 5.480332851409912, LDE 0.0, LReg 0.0, POD 13.842512130737305 EntMin 0.0
Epoch 2, Batch 20/24, Loss=18.550158900022506
Loss made of: CE 0.8681563138961792, LKD 5.395877361297607, LDE 0.0, LReg 0.0, POD 13.339138984680176 EntMin 0.0
Epoch 2, Class Loss=0.9401911497116089, Reg Loss=5.658272743225098
Clinet index 19, End of Epoch 2/6, Average Loss=6.598464012145996, Class Loss=0.9401911497116089, Reg Loss=5.658272743225098
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=16.96106054186821
Loss made of: CE 0.7276622653007507, LKD 5.638884544372559, LDE 0.0, LReg 0.0, POD 10.418427467346191 EntMin 0.0
Epoch 3, Batch 20/24, Loss=16.842850464582444
Loss made of: CE 0.7376574873924255, LKD 5.7954816818237305, LDE 0.0, LReg 0.0, POD 9.589776992797852 EntMin 0.0
Epoch 3, Class Loss=0.7775073051452637, Reg Loss=5.532564163208008
Clinet index 19, End of Epoch 3/6, Average Loss=6.3100714683532715, Class Loss=0.7775073051452637, Reg Loss=5.532564163208008
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=16.500125932693482
Loss made of: CE 0.72132408618927, LKD 5.745226860046387, LDE 0.0, LReg 0.0, POD 9.81460952758789 EntMin 0.0
Epoch 4, Batch 20/24, Loss=16.868011260032652
Loss made of: CE 0.6401106119155884, LKD 4.881481647491455, LDE 0.0, LReg 0.0, POD 12.575281143188477 EntMin 0.0
Epoch 4, Class Loss=0.6942624449729919, Reg Loss=5.528054237365723
Clinet index 19, End of Epoch 4/6, Average Loss=6.222316741943359, Class Loss=0.6942624449729919, Reg Loss=5.528054237365723
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=15.91303853392601
Loss made of: CE 0.6177182793617249, LKD 5.739684104919434, LDE 0.0, LReg 0.0, POD 10.064981460571289 EntMin 0.0
Epoch 5, Batch 20/24, Loss=16.342358988523483
Loss made of: CE 0.5386132001876831, LKD 5.727445125579834, LDE 0.0, LReg 0.0, POD 10.137672424316406 EntMin 0.0
Epoch 5, Class Loss=0.6348717212677002, Reg Loss=5.567620754241943
Clinet index 19, End of Epoch 5/6, Average Loss=6.202492713928223, Class Loss=0.6348717212677002, Reg Loss=5.567620754241943
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=16.31416113972664
Loss made of: CE 0.5400584936141968, LKD 4.916746139526367, LDE 0.0, LReg 0.0, POD 9.854402542114258 EntMin 0.0
Epoch 6, Batch 20/24, Loss=16.487902599573136
Loss made of: CE 0.6547583341598511, LKD 5.088991165161133, LDE 0.0, LReg 0.0, POD 8.958442687988281 EntMin 0.0
Epoch 6, Class Loss=0.5958914160728455, Reg Loss=5.56281852722168
Clinet index 19, End of Epoch 6/6, Average Loss=6.15871000289917, Class Loss=0.5958914160728455, Reg Loss=5.56281852722168
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=23.19606145620346
Loss made of: CE 1.1432737112045288, LKD 6.812355041503906, LDE 0.0, LReg 0.0, POD 16.512109756469727 EntMin 0.0
Epoch 1, Batch 20/21, Loss=20.64073691368103
Loss made of: CE 0.9867948293685913, LKD 5.948056697845459, LDE 0.0, LReg 0.0, POD 13.884500503540039 EntMin 0.0
Epoch 1, Class Loss=1.226547122001648, Reg Loss=6.26678991317749
Clinet index 23, End of Epoch 1/6, Average Loss=7.493337154388428, Class Loss=1.226547122001648, Reg Loss=6.26678991317749
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/21, Loss=19.680614429712296
Loss made of: CE 0.8196460008621216, LKD 6.489964962005615, LDE 0.0, LReg 0.0, POD 13.080876350402832 EntMin 0.0
Epoch 2, Batch 20/21, Loss=18.426922351121902
Loss made of: CE 0.749224841594696, LKD 5.513566970825195, LDE 0.0, LReg 0.0, POD 12.333578109741211 EntMin 0.0
Epoch 2, Class Loss=0.8683291077613831, Reg Loss=6.139716625213623
Clinet index 23, End of Epoch 2/6, Average Loss=7.008045673370361, Class Loss=0.8683291077613831, Reg Loss=6.139716625213623
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/21, Loss=19.235027676820756
Loss made of: CE 0.7052778005599976, LKD 6.76988410949707, LDE 0.0, LReg 0.0, POD 11.24213695526123 EntMin 0.0
Epoch 3, Batch 20/21, Loss=17.995391952991486
Loss made of: CE 0.7797343134880066, LKD 6.228442192077637, LDE 0.0, LReg 0.0, POD 12.370887756347656 EntMin 0.0
Epoch 3, Class Loss=0.7437136769294739, Reg Loss=6.098282814025879
Clinet index 23, End of Epoch 3/6, Average Loss=6.841996669769287, Class Loss=0.7437136769294739, Reg Loss=6.098282814025879
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/21, Loss=19.096343606710434
Loss made of: CE 0.8025463819503784, LKD 5.940769195556641, LDE 0.0, LReg 0.0, POD 12.948936462402344 EntMin 0.0
Epoch 4, Batch 20/21, Loss=16.897646802663804
Loss made of: CE 0.48429059982299805, LKD 5.755596160888672, LDE 0.0, LReg 0.0, POD 10.032485961914062 EntMin 0.0
Epoch 4, Class Loss=0.6797249913215637, Reg Loss=6.1071624755859375
Clinet index 23, End of Epoch 4/6, Average Loss=6.7868876457214355, Class Loss=0.6797249913215637, Reg Loss=6.1071624755859375
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/21, Loss=17.583073750138283
Loss made of: CE 0.6619147062301636, LKD 6.3152666091918945, LDE 0.0, LReg 0.0, POD 10.704360961914062 EntMin 0.0
Epoch 5, Batch 20/21, Loss=17.630144110322
Loss made of: CE 0.6051104068756104, LKD 5.431298732757568, LDE 0.0, LReg 0.0, POD 10.07004165649414 EntMin 0.0
Epoch 5, Class Loss=0.6244752407073975, Reg Loss=6.107642650604248
Clinet index 23, End of Epoch 5/6, Average Loss=6.732117652893066, Class Loss=0.6244752407073975, Reg Loss=6.107642650604248
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/21, Loss=17.68719862997532
Loss made of: CE 0.5831491947174072, LKD 5.799949645996094, LDE 0.0, LReg 0.0, POD 11.195327758789062 EntMin 0.0
Epoch 6, Batch 20/21, Loss=17.27035762667656
Loss made of: CE 0.5491095781326294, LKD 6.447073936462402, LDE 0.0, LReg 0.0, POD 10.03276252746582 EntMin 0.0
Epoch 6, Class Loss=0.601651668548584, Reg Loss=6.128734111785889
Clinet index 23, End of Epoch 6/6, Average Loss=6.730385780334473, Class Loss=0.601651668548584, Reg Loss=6.128734111785889
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=26.62726719379425
Loss made of: CE 1.6033294200897217, LKD 7.311973571777344, LDE 0.0, LReg 0.0, POD 20.226329803466797 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Class Loss=1.4979995489120483, Reg Loss=5.978167533874512
Clinet index 1, End of Epoch 1/6, Average Loss=7.47616720199585, Class Loss=1.4979995489120483, Reg Loss=5.978167533874512
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=21.35574542284012
Loss made of: CE 0.9192709922790527, LKD 4.872215270996094, LDE 0.0, LReg 0.0, POD 16.583499908447266 EntMin 0.0
Epoch 2, Class Loss=1.0563760995864868, Reg Loss=5.661059856414795
Clinet index 1, End of Epoch 2/6, Average Loss=6.717435836791992, Class Loss=1.0563760995864868, Reg Loss=5.661059856414795
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=19.414042508602144
Loss made of: CE 0.9328016042709351, LKD 6.048650741577148, LDE 0.0, LReg 0.0, POD 13.348965644836426 EntMin 0.0
Epoch 3, Class Loss=0.8465204834938049, Reg Loss=5.513045787811279
Clinet index 1, End of Epoch 3/6, Average Loss=6.3595662117004395, Class Loss=0.8465204834938049, Reg Loss=5.513045787811279
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=18.63736324906349
Loss made of: CE 0.6683648824691772, LKD 5.575671195983887, LDE 0.0, LReg 0.0, POD 10.781750679016113 EntMin 0.0
Epoch 4, Class Loss=0.7364756464958191, Reg Loss=5.579077243804932
Clinet index 1, End of Epoch 4/6, Average Loss=6.315552711486816, Class Loss=0.7364756464958191, Reg Loss=5.579077243804932
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=17.770087718963623
Loss made of: CE 0.6264070868492126, LKD 5.310400009155273, LDE 0.0, LReg 0.0, POD 12.242572784423828 EntMin 0.0
Epoch 5, Class Loss=0.6519288420677185, Reg Loss=5.539761066436768
Clinet index 1, End of Epoch 5/6, Average Loss=6.191689968109131, Class Loss=0.6519288420677185, Reg Loss=5.539761066436768
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=16.967599618434907
Loss made of: CE 0.5241124629974365, LKD 4.73689079284668, LDE 0.0, LReg 0.0, POD 10.435020446777344 EntMin 0.0
Epoch 6, Class Loss=0.6066527366638184, Reg Loss=5.503110408782959
Clinet index 1, End of Epoch 6/6, Average Loss=6.109763145446777, Class Loss=0.6066527366638184, Reg Loss=5.503110408782959
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=24.740143060684204
Loss made of: CE 1.054238200187683, LKD 6.923689842224121, LDE 0.0, LReg 0.0, POD 13.951416015625 EntMin 0.0
Epoch 1, Batch 20/24, Loss=21.192304027080535
Loss made of: CE 0.8259820938110352, LKD 7.1432204246521, LDE 0.0, LReg 0.0, POD 12.232921600341797 EntMin 0.0
Epoch 1, Class Loss=1.0464885234832764, Reg Loss=6.426461696624756
Clinet index 8, End of Epoch 1/6, Average Loss=7.472949981689453, Class Loss=1.0464885234832764, Reg Loss=6.426461696624756
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=19.628868770599365
Loss made of: CE 0.8047951459884644, LKD 6.912911415100098, LDE 0.0, LReg 0.0, POD 12.34611701965332 EntMin 0.0
Epoch 2, Batch 20/24, Loss=19.219308018684387
Loss made of: CE 0.6945335268974304, LKD 6.244731426239014, LDE 0.0, LReg 0.0, POD 13.190468788146973 EntMin 0.0
Epoch 2, Class Loss=0.798350989818573, Reg Loss=6.258233070373535
Clinet index 8, End of Epoch 2/6, Average Loss=7.056583881378174, Class Loss=0.798350989818573, Reg Loss=6.258233070373535
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=19.120495104789732
Loss made of: CE 0.7223665118217468, LKD 6.142218589782715, LDE 0.0, LReg 0.0, POD 11.637147903442383 EntMin 0.0
Epoch 3, Batch 20/24, Loss=17.97737510204315
Loss made of: CE 0.6013364791870117, LKD 6.447373390197754, LDE 0.0, LReg 0.0, POD 10.359919548034668 EntMin 0.0
Epoch 3, Class Loss=0.7033863067626953, Reg Loss=6.199975490570068
Clinet index 8, End of Epoch 3/6, Average Loss=6.903361797332764, Class Loss=0.7033863067626953, Reg Loss=6.199975490570068
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=17.96237094402313
Loss made of: CE 0.6987762451171875, LKD 5.9333815574646, LDE 0.0, LReg 0.0, POD 10.439603805541992 EntMin 0.0
Epoch 4, Batch 20/24, Loss=17.948827177286148
Loss made of: CE 0.5585962533950806, LKD 6.398311614990234, LDE 0.0, LReg 0.0, POD 10.603567123413086 EntMin 0.0
Epoch 4, Class Loss=0.6533474922180176, Reg Loss=6.235358715057373
Clinet index 8, End of Epoch 4/6, Average Loss=6.888706207275391, Class Loss=0.6533474922180176, Reg Loss=6.235358715057373
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 10/24, Loss=17.58947147130966
Loss made of: CE 0.7053513526916504, LKD 6.381244659423828, LDE 0.0, LReg 0.0, POD 11.863109588623047 EntMin 0.0
Epoch 5, Batch 20/24, Loss=17.146877011656763
Loss made of: CE 0.5970324277877808, LKD 6.525683879852295, LDE 0.0, LReg 0.0, POD 9.722524642944336 EntMin 0.0
Epoch 5, Class Loss=0.6124584078788757, Reg Loss=6.250908851623535
Clinet index 8, End of Epoch 5/6, Average Loss=6.863367080688477, Class Loss=0.6124584078788757, Reg Loss=6.250908851623535
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=17.093238651752472
Loss made of: CE 0.5917243957519531, LKD 6.643202781677246, LDE 0.0, LReg 0.0, POD 10.071517944335938 EntMin 0.0
Epoch 6, Batch 20/24, Loss=16.889921468496322
Loss made of: CE 0.4571536183357239, LKD 5.919723033905029, LDE 0.0, LReg 0.0, POD 10.033712387084961 EntMin 0.0
Epoch 6, Class Loss=0.5818187594413757, Reg Loss=6.237129211425781
Clinet index 8, End of Epoch 6/6, Average Loss=6.818947792053223, Class Loss=0.5818187594413757, Reg Loss=6.237129211425781
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.7675139307975769, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.806797
Mean Acc: 0.434532
FreqW Acc: 0.695470
Mean IoU: 0.292691
Class IoU:
	class 0: 0.8550204
	class 1: 0.59670585
	class 2: 0.28484198
	class 3: 0.3939949
	class 4: 0.4161052
	class 5: 0.03181428
	class 6: 0.6282855
	class 7: 0.5360699
	class 8: 0.33139
	class 9: 0.004886455
	class 10: 0.35085922
	class 11: 0.23166999
	class 12: 0.027092997
	class 13: 0.2437393
	class 14: 0.4888086
	class 15: 0.7166479
	class 16: 0.008579521
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0
Class Acc:
	class 0: 0.94670224
	class 1: 0.62171173
	class 2: 0.57387465
	class 3: 0.9111312
	class 4: 0.90383273
	class 5: 0.031821717
	class 6: 0.78714323
	class 7: 0.7333362
	class 8: 0.8552979
	class 9: 0.004905686
	class 10: 0.45602846
	class 11: 0.5303497
	class 12: 0.027225314
	class 13: 0.2595973
	class 14: 0.58903944
	class 15: 0.88452065
	class 16: 0.008648061
	class 17: 0.0
	class 18: 0.0
	class 19: 0.0
	class 20: 0.0

federated global round: 21, step: 4
select part of clients to conduct local training
[23, 14, 4, 11]
Current Client Index:  23
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=18.658933180570603
Loss made of: CE 0.6601299047470093, LKD 6.307209014892578, LDE 0.0, LReg 0.0, POD 12.434340476989746 EntMin 0.0
Epoch 1, Batch 20/21, Loss=17.945521473884583
Loss made of: CE 0.7261835932731628, LKD 5.764222145080566, LDE 0.0, LReg 0.0, POD 10.865928649902344 EntMin 0.0
Epoch 1, Class Loss=0.8254743218421936, Reg Loss=6.073314666748047
Clinet index 23, End of Epoch 1/6, Average Loss=6.898788928985596, Class Loss=0.8254743218421936, Reg Loss=6.073314666748047
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/21, Loss=17.582351684570312
Loss made of: CE 0.6564065217971802, LKD 6.268641471862793, LDE 0.0, LReg 0.0, POD 11.152397155761719 EntMin 0.0
Epoch 2, Batch 20/21, Loss=16.953136140108107
Loss made of: CE 0.5886921882629395, LKD 5.677724361419678, LDE 0.0, LReg 0.0, POD 12.631893157958984 EntMin 0.0
Epoch 2, Class Loss=0.7182733416557312, Reg Loss=6.065661430358887
Clinet index 23, End of Epoch 2/6, Average Loss=6.783934593200684, Class Loss=0.7182733416557312, Reg Loss=6.065661430358887
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/21, Loss=17.894971203804015
Loss made of: CE 0.6013185977935791, LKD 6.977847099304199, LDE 0.0, LReg 0.0, POD 9.775008201599121 EntMin 0.0
Epoch 3, Batch 20/21, Loss=16.877820590138434
Loss made of: CE 0.6643495559692383, LKD 6.310565948486328, LDE 0.0, LReg 0.0, POD 11.08813190460205 EntMin 0.0
Epoch 3, Class Loss=0.6459879875183105, Reg Loss=6.1119065284729
Clinet index 23, End of Epoch 3/6, Average Loss=6.757894515991211, Class Loss=0.6459879875183105, Reg Loss=6.1119065284729
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/21, Loss=17.679320549964906
Loss made of: CE 0.6901787519454956, LKD 5.941801071166992, LDE 0.0, LReg 0.0, POD 11.117618560791016 EntMin 0.0
Epoch 4, Batch 20/21, Loss=16.107474860548972
Loss made of: CE 0.42548951506614685, LKD 5.810497283935547, LDE 0.0, LReg 0.0, POD 9.121063232421875 EntMin 0.0
Epoch 4, Class Loss=0.5923377871513367, Reg Loss=6.126901626586914
Clinet index 23, End of Epoch 4/6, Average Loss=6.719239234924316, Class Loss=0.5923377871513367, Reg Loss=6.126901626586914
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=16.742407980561257
Loss made of: CE 0.598842442035675, LKD 6.582149505615234, LDE 0.0, LReg 0.0, POD 10.236159324645996 EntMin 0.0
Epoch 5, Batch 20/21, Loss=16.909091305732726
Loss made of: CE 0.5384446382522583, LKD 5.341693878173828, LDE 0.0, LReg 0.0, POD 9.723882675170898 EntMin 0.0
Epoch 5, Class Loss=0.5592042207717896, Reg Loss=6.1108317375183105
Clinet index 23, End of Epoch 5/6, Average Loss=6.6700358390808105, Class Loss=0.5592042207717896, Reg Loss=6.1108317375183105
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/21, Loss=17.020690190792084
Loss made of: CE 0.5304845571517944, LKD 5.817617416381836, LDE 0.0, LReg 0.0, POD 11.601240158081055 EntMin 0.0
Epoch 6, Batch 20/21, Loss=16.76437638401985
Loss made of: CE 0.5612608194351196, LKD 6.821366786956787, LDE 0.0, LReg 0.0, POD 9.503351211547852 EntMin 0.0
Epoch 6, Class Loss=0.5474286675453186, Reg Loss=6.115106582641602
Clinet index 23, End of Epoch 6/6, Average Loss=6.662535190582275, Class Loss=0.5474286675453186, Reg Loss=6.115106582641602
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=20.265438115596773
Loss made of: CE 0.9354836940765381, LKD 6.221498489379883, LDE 0.0, LReg 0.0, POD 10.98459243774414 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/26, Loss=17.975404393672942
Loss made of: CE 0.76532381772995, LKD 5.720332145690918, LDE 0.0, LReg 0.0, POD 9.661842346191406 EntMin 0.0
Epoch 1, Class Loss=0.8419814705848694, Reg Loss=5.934169769287109
Clinet index 14, End of Epoch 1/6, Average Loss=6.776151180267334, Class Loss=0.8419814705848694, Reg Loss=5.934169769287109
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=18.389368414878845
Loss made of: CE 0.761660099029541, LKD 6.031017303466797, LDE 0.0, LReg 0.0, POD 11.158836364746094 EntMin 0.0
Epoch 2, Batch 20/26, Loss=17.56208943724632
Loss made of: CE 0.757811427116394, LKD 5.599491119384766, LDE 0.0, LReg 0.0, POD 10.202072143554688 EntMin 0.0
Epoch 2, Class Loss=0.6782952547073364, Reg Loss=5.87834358215332
Clinet index 14, End of Epoch 2/6, Average Loss=6.556638717651367, Class Loss=0.6782952547073364, Reg Loss=5.87834358215332
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 3, Batch 10/26, Loss=17.407861599326132
Loss made of: CE 0.69246506690979, LKD 6.180545806884766, LDE 0.0, LReg 0.0, POD 11.490111351013184 EntMin 0.0
Epoch 3, Batch 20/26, Loss=17.381869491934776
Loss made of: CE 0.48051390051841736, LKD 5.851360321044922, LDE 0.0, LReg 0.0, POD 9.94723892211914 EntMin 0.0
Epoch 3, Class Loss=0.6120460629463196, Reg Loss=5.841660499572754
Clinet index 14, End of Epoch 3/6, Average Loss=6.453706741333008, Class Loss=0.6120460629463196, Reg Loss=5.841660499572754
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=16.64606073498726
Loss made of: CE 0.5921846032142639, LKD 5.466920852661133, LDE 0.0, LReg 0.0, POD 12.911895751953125 EntMin 0.0
Epoch 4, Batch 20/26, Loss=17.24509209394455
Loss made of: CE 0.645404577255249, LKD 6.1337432861328125, LDE 0.0, LReg 0.0, POD 9.417773246765137 EntMin 0.0
Epoch 4, Class Loss=0.5664352178573608, Reg Loss=5.82620096206665
Clinet index 14, End of Epoch 4/6, Average Loss=6.392636299133301, Class Loss=0.5664352178573608, Reg Loss=5.82620096206665
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=16.14604397416115
Loss made of: CE 0.7601399421691895, LKD 5.591164588928223, LDE 0.0, LReg 0.0, POD 10.973663330078125 EntMin 0.0
Epoch 5, Batch 20/26, Loss=16.916332572698593
Loss made of: CE 0.6331701278686523, LKD 5.4643683433532715, LDE 0.0, LReg 0.0, POD 10.104375839233398 EntMin 0.0
Epoch 5, Class Loss=0.5442842841148376, Reg Loss=5.856136322021484
Clinet index 14, End of Epoch 5/6, Average Loss=6.400420665740967, Class Loss=0.5442842841148376, Reg Loss=5.856136322021484
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=16.111916264891626
Loss made of: CE 0.4647367000579834, LKD 6.133091926574707, LDE 0.0, LReg 0.0, POD 13.200268745422363 EntMin 0.0
Epoch 6, Batch 20/26, Loss=16.45991257727146
Loss made of: CE 0.46181315183639526, LKD 5.511447906494141, LDE 0.0, LReg 0.0, POD 10.961005210876465 EntMin 0.0
Epoch 6, Class Loss=0.5266038775444031, Reg Loss=5.866899013519287
Clinet index 14, End of Epoch 6/6, Average Loss=6.393502712249756, Class Loss=0.5266038775444031, Reg Loss=5.866899013519287
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/26, Loss=19.491390138864517
Loss made of: CE 1.0594452619552612, LKD 6.6422271728515625, LDE 0.0, LReg 0.0, POD 10.47966480255127 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/26, Loss=18.68656731247902
Loss made of: CE 0.6928984522819519, LKD 5.289647102355957, LDE 0.0, LReg 0.0, POD 12.952887535095215 EntMin 0.0
Epoch 1, Class Loss=0.8285350203514099, Reg Loss=5.892350673675537
Clinet index 4, End of Epoch 1/6, Average Loss=6.720885753631592, Class Loss=0.8285350203514099, Reg Loss=5.892350673675537
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=17.2855230987072
Loss made of: CE 0.8741480708122253, LKD 7.2710371017456055, LDE 0.0, LReg 0.0, POD 10.146753311157227 EntMin 0.0
Epoch 2, Batch 20/26, Loss=17.394350638985635
Loss made of: CE 0.7467020750045776, LKD 5.980517864227295, LDE 0.0, LReg 0.0, POD 10.690231323242188 EntMin 0.0
Epoch 2, Class Loss=0.68611079454422, Reg Loss=5.860292434692383
Clinet index 4, End of Epoch 2/6, Average Loss=6.546403408050537, Class Loss=0.68611079454422, Reg Loss=5.860292434692383
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=17.098933660984038
Loss made of: CE 0.586707592010498, LKD 5.932555198669434, LDE 0.0, LReg 0.0, POD 11.466140747070312 EntMin 0.0
Epoch 3, Batch 20/26, Loss=16.9479188144207
Loss made of: CE 0.6285603046417236, LKD 6.158729553222656, LDE 0.0, LReg 0.0, POD 11.763431549072266 EntMin 0.0
Epoch 3, Class Loss=0.5936661958694458, Reg Loss=5.8188934326171875
Clinet index 4, End of Epoch 3/6, Average Loss=6.412559509277344, Class Loss=0.5936661958694458, Reg Loss=5.8188934326171875
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=16.883325469493865
Loss made of: CE 0.4978025555610657, LKD 6.433985710144043, LDE 0.0, LReg 0.0, POD 10.549322128295898 EntMin 0.0
Epoch 4, Batch 20/26, Loss=16.493488588929175
Loss made of: CE 0.5012809634208679, LKD 6.024512767791748, LDE 0.0, LReg 0.0, POD 13.096881866455078 EntMin 0.0
Epoch 4, Class Loss=0.5639215707778931, Reg Loss=5.794454574584961
Clinet index 4, End of Epoch 4/6, Average Loss=6.3583760261535645, Class Loss=0.5639215707778931, Reg Loss=5.794454574584961
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=16.950563737750052
Loss made of: CE 0.4844765067100525, LKD 5.131995677947998, LDE 0.0, LReg 0.0, POD 13.300663948059082 EntMin 0.0
Epoch 5, Batch 20/26, Loss=16.783999487757683
Loss made of: CE 0.5177066326141357, LKD 5.904690742492676, LDE 0.0, LReg 0.0, POD 10.486178398132324 EntMin 0.0
Epoch 5, Class Loss=0.5368571281433105, Reg Loss=5.847396373748779
Clinet index 4, End of Epoch 5/6, Average Loss=6.38425350189209, Class Loss=0.5368571281433105, Reg Loss=5.847396373748779
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=16.976384633779524
Loss made of: CE 0.4724556803703308, LKD 6.276619911193848, LDE 0.0, LReg 0.0, POD 9.122303009033203 EntMin 0.0
Epoch 6, Batch 20/26, Loss=16.340189465880393
Loss made of: CE 0.5403149724006653, LKD 6.121273994445801, LDE 0.0, LReg 0.0, POD 9.139013290405273 EntMin 0.0
Epoch 6, Class Loss=0.5212482810020447, Reg Loss=5.84187126159668
Clinet index 4, End of Epoch 6/6, Average Loss=6.363119602203369, Class Loss=0.5212482810020447, Reg Loss=5.84187126159668
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=19.661466860771178
Loss made of: CE 0.8334572315216064, LKD 6.084819316864014, LDE 0.0, LReg 0.0, POD 11.47130298614502 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/24, Loss=18.74087210893631
Loss made of: CE 0.8707787394523621, LKD 6.205462455749512, LDE 0.0, LReg 0.0, POD 12.285811424255371 EntMin 0.0
Epoch 1, Class Loss=0.768366277217865, Reg Loss=6.223459243774414
Clinet index 11, End of Epoch 1/6, Average Loss=6.991825580596924, Class Loss=0.768366277217865, Reg Loss=6.223459243774414
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/24, Loss=17.92745614051819
Loss made of: CE 0.7909846305847168, LKD 5.8853068351745605, LDE 0.0, LReg 0.0, POD 10.906018257141113 EntMin 0.0
Epoch 2, Batch 20/24, Loss=18.080541881918908
Loss made of: CE 0.6839685440063477, LKD 6.578731536865234, LDE 0.0, LReg 0.0, POD 10.73914909362793 EntMin 0.0
Epoch 2, Class Loss=0.6658200621604919, Reg Loss=6.185655117034912
Clinet index 11, End of Epoch 2/6, Average Loss=6.851475238800049, Class Loss=0.6658200621604919, Reg Loss=6.185655117034912
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/24, Loss=17.61788540482521
Loss made of: CE 0.6216587424278259, LKD 6.311074733734131, LDE 0.0, LReg 0.0, POD 11.004453659057617 EntMin 0.0
Epoch 3, Batch 20/24, Loss=17.796331349015237
Loss made of: CE 0.4716038703918457, LKD 6.2342329025268555, LDE 0.0, LReg 0.0, POD 10.933727264404297 EntMin 0.0
Epoch 3, Class Loss=0.5985370874404907, Reg Loss=6.158690929412842
Clinet index 11, End of Epoch 3/6, Average Loss=6.757227897644043, Class Loss=0.5985370874404907, Reg Loss=6.158690929412842
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/24, Loss=16.874693694710732
Loss made of: CE 0.5943634510040283, LKD 6.7383646965026855, LDE 0.0, LReg 0.0, POD 9.704631805419922 EntMin 0.0
Epoch 4, Batch 20/24, Loss=17.639647644758224
Loss made of: CE 0.5678607225418091, LKD 5.910998344421387, LDE 0.0, LReg 0.0, POD 10.661827087402344 EntMin 0.0
Epoch 4, Class Loss=0.5548077821731567, Reg Loss=6.14819860458374
Clinet index 11, End of Epoch 4/6, Average Loss=6.703006267547607, Class Loss=0.5548077821731567, Reg Loss=6.14819860458374
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/24, Loss=17.675171783566476
Loss made of: CE 0.5360329151153564, LKD 6.29931640625, LDE 0.0, LReg 0.0, POD 10.903032302856445 EntMin 0.0
Epoch 5, Batch 20/24, Loss=16.812492752075194
Loss made of: CE 0.4801790714263916, LKD 5.864500045776367, LDE 0.0, LReg 0.0, POD 9.080223083496094 EntMin 0.0
Epoch 5, Class Loss=0.5294808149337769, Reg Loss=6.116197109222412
Clinet index 11, End of Epoch 5/6, Average Loss=6.6456780433654785, Class Loss=0.5294808149337769, Reg Loss=6.116197109222412
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/24, Loss=16.90107947587967
Loss made of: CE 0.5836291313171387, LKD 6.76334285736084, LDE 0.0, LReg 0.0, POD 11.356674194335938 EntMin 0.0
Epoch 6, Batch 20/24, Loss=16.865522468090056
Loss made of: CE 0.6203598976135254, LKD 6.937891960144043, LDE 0.0, LReg 0.0, POD 11.438539505004883 EntMin 0.0
Epoch 6, Class Loss=0.5034111142158508, Reg Loss=6.121734142303467
Clinet index 11, End of Epoch 6/6, Average Loss=6.625145435333252, Class Loss=0.5034111142158508, Reg Loss=6.121734142303467
federated aggregation...
Validation, Class Loss=0.7483561635017395, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.805452
Mean Acc: 0.458816
FreqW Acc: 0.699215
Mean IoU: 0.305452
Class IoU:
	class 0: 0.85763973
	class 1: 0.6345989
	class 2: 0.28796828
	class 3: 0.38967392
	class 4: 0.40516517
	class 5: 0.017352052
	class 6: 0.62928325
	class 7: 0.5287342
	class 8: 0.33330402
	class 9: 0.002514519
	class 10: 0.34154192
	class 11: 0.2126669
	class 12: 0.028964465
	class 13: 0.2437156
	class 14: 0.45813358
	class 15: 0.70645785
	class 16: 0.0097902445
	class 17: 0.0
	class 18: 0.0
	class 19: 0.049877312
	class 20: 0.2771136
Class Acc:
	class 0: 0.93614435
	class 1: 0.670538
	class 2: 0.5997305
	class 3: 0.90989006
	class 4: 0.910209
	class 5: 0.017352143
	class 6: 0.80499786
	class 7: 0.7512562
	class 8: 0.8516543
	class 9: 0.0025301522
	class 10: 0.4422561
	class 11: 0.58605605
	class 12: 0.029109152
	class 13: 0.25769457
	class 14: 0.5533175
	class 15: 0.90072
	class 16: 0.009881685
	class 17: 0.0
	class 18: 0.0
	class 19: 0.05005403
	class 20: 0.35174114

federated global round: 22, step: 4
select part of clients to conduct local training
[0, 8, 16, 14]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=18.437957841157914
Loss made of: CE 0.7584710121154785, LKD 6.196568965911865, LDE 0.0, LReg 0.0, POD 11.27885627746582 EntMin 0.0
Epoch 1, Batch 20/24, Loss=17.797132658958436
Loss made of: CE 0.747646689414978, LKD 5.780515193939209, LDE 0.0, LReg 0.0, POD 9.608269691467285 EntMin 0.0
Epoch 1, Class Loss=0.6545321941375732, Reg Loss=6.1648478507995605
Clinet index 0, End of Epoch 1/6, Average Loss=6.819379806518555, Class Loss=0.6545321941375732, Reg Loss=6.1648478507995605
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/24, Loss=17.516185823082925
Loss made of: CE 0.49942493438720703, LKD 5.945899963378906, LDE 0.0, LReg 0.0, POD 9.53348159790039 EntMin 0.0
Epoch 2, Batch 20/24, Loss=16.70447709560394
Loss made of: CE 0.5036258697509766, LKD 5.609152793884277, LDE 0.0, LReg 0.0, POD 11.044536590576172 EntMin 0.0
Epoch 2, Class Loss=0.5649975538253784, Reg Loss=6.134836196899414
Clinet index 0, End of Epoch 2/6, Average Loss=6.699833869934082, Class Loss=0.5649975538253784, Reg Loss=6.134836196899414
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/24, Loss=17.045894223451615
Loss made of: CE 0.6353952288627625, LKD 6.474294662475586, LDE 0.0, LReg 0.0, POD 11.371474266052246 EntMin 0.0
Epoch 3, Batch 20/24, Loss=17.41208389699459
Loss made of: CE 0.546954870223999, LKD 6.169039726257324, LDE 0.0, LReg 0.0, POD 11.212312698364258 EntMin 0.0
Epoch 3, Class Loss=0.5245787501335144, Reg Loss=6.12846565246582
Clinet index 0, End of Epoch 3/6, Average Loss=6.6530442237854, Class Loss=0.5245787501335144, Reg Loss=6.12846565246582
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/24, Loss=16.56738385260105
Loss made of: CE 0.49830707907676697, LKD 5.891397476196289, LDE 0.0, LReg 0.0, POD 10.719633102416992 EntMin 0.0
Epoch 4, Batch 20/24, Loss=16.944814321398734
Loss made of: CE 0.5118510723114014, LKD 6.002881050109863, LDE 0.0, LReg 0.0, POD 10.793614387512207 EntMin 0.0
Epoch 4, Class Loss=0.49940669536590576, Reg Loss=6.127763748168945
Clinet index 0, End of Epoch 4/6, Average Loss=6.627170562744141, Class Loss=0.49940669536590576, Reg Loss=6.127763748168945
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/24, Loss=16.80513952076435
Loss made of: CE 0.5152766704559326, LKD 6.23019552230835, LDE 0.0, LReg 0.0, POD 8.694132804870605 EntMin 0.0
Epoch 5, Batch 20/24, Loss=16.588330191373824
Loss made of: CE 0.3994360864162445, LKD 5.994664192199707, LDE 0.0, LReg 0.0, POD 9.985457420349121 EntMin 0.0
Epoch 5, Class Loss=0.4865841269493103, Reg Loss=6.137995719909668
Clinet index 0, End of Epoch 5/6, Average Loss=6.624579906463623, Class Loss=0.4865841269493103, Reg Loss=6.137995719909668
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/24, Loss=15.937932008504868
Loss made of: CE 0.49964284896850586, LKD 6.508546352386475, LDE 0.0, LReg 0.0, POD 10.133087158203125 EntMin 0.0
Epoch 6, Batch 20/24, Loss=16.551646873354912
Loss made of: CE 0.3752656877040863, LKD 6.375338554382324, LDE 0.0, LReg 0.0, POD 8.330069541931152 EntMin 0.0
Epoch 6, Class Loss=0.46731460094451904, Reg Loss=6.133995056152344
Clinet index 0, End of Epoch 6/6, Average Loss=6.601309776306152, Class Loss=0.46731460094451904, Reg Loss=6.133995056152344
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=18.31459254026413
Loss made of: CE 0.6583418846130371, LKD 6.980306148529053, LDE 0.0, LReg 0.0, POD 9.712234497070312 EntMin 0.0
Epoch 1, Batch 20/24, Loss=17.15718634724617
Loss made of: CE 0.509227991104126, LKD 7.339328289031982, LDE 0.0, LReg 0.0, POD 9.167783737182617 EntMin 0.0
Epoch 1, Class Loss=0.6798591017723083, Reg Loss=6.2660675048828125
Clinet index 8, End of Epoch 1/6, Average Loss=6.945926666259766, Class Loss=0.6798591017723083, Reg Loss=6.2660675048828125
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/24, Loss=16.99318642616272
Loss made of: CE 0.5944844484329224, LKD 6.777736663818359, LDE 0.0, LReg 0.0, POD 9.673450469970703 EntMin 0.0
Epoch 2, Batch 20/24, Loss=16.75882497727871
Loss made of: CE 0.5070804953575134, LKD 6.15442419052124, LDE 0.0, LReg 0.0, POD 10.729150772094727 EntMin 0.0
Epoch 2, Class Loss=0.6022771000862122, Reg Loss=6.179421424865723
Clinet index 8, End of Epoch 2/6, Average Loss=6.781698703765869, Class Loss=0.6022771000862122, Reg Loss=6.179421424865723
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/24, Loss=17.031601563096046
Loss made of: CE 0.5232794284820557, LKD 6.36624813079834, LDE 0.0, LReg 0.0, POD 10.31153678894043 EntMin 0.0
Epoch 3, Batch 20/24, Loss=16.464217540621757
Loss made of: CE 0.5517748594284058, LKD 6.469367980957031, LDE 0.0, LReg 0.0, POD 9.339178085327148 EntMin 0.0
Epoch 3, Class Loss=0.5486776828765869, Reg Loss=6.207665920257568
Clinet index 8, End of Epoch 3/6, Average Loss=6.756343841552734, Class Loss=0.5486776828765869, Reg Loss=6.207665920257568
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/24, Loss=16.045834565162657
Loss made of: CE 0.5353726148605347, LKD 5.755897521972656, LDE 0.0, LReg 0.0, POD 9.677545547485352 EntMin 0.0
Epoch 4, Batch 20/24, Loss=16.529706189036368
Loss made of: CE 0.43780261278152466, LKD 6.561256408691406, LDE 0.0, LReg 0.0, POD 8.994945526123047 EntMin 0.0
Epoch 4, Class Loss=0.5203356742858887, Reg Loss=6.146420478820801
Clinet index 8, End of Epoch 4/6, Average Loss=6.6667561531066895, Class Loss=0.5203356742858887, Reg Loss=6.146420478820801
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/24, Loss=16.372902858257294
Loss made of: CE 0.6070330142974854, LKD 6.785203456878662, LDE 0.0, LReg 0.0, POD 11.075767517089844 EntMin 0.0
Epoch 5, Batch 20/24, Loss=15.904981133341789
Loss made of: CE 0.5265054702758789, LKD 6.448093414306641, LDE 0.0, LReg 0.0, POD 8.315229415893555 EntMin 0.0
Epoch 5, Class Loss=0.5086435079574585, Reg Loss=6.196803569793701
Clinet index 8, End of Epoch 5/6, Average Loss=6.705447196960449, Class Loss=0.5086435079574585, Reg Loss=6.196803569793701
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/24, Loss=16.11167251765728
Loss made of: CE 0.5214654803276062, LKD 6.636005401611328, LDE 0.0, LReg 0.0, POD 9.590892791748047 EntMin 0.0
Epoch 6, Batch 20/24, Loss=15.929896634817123
Loss made of: CE 0.3929271697998047, LKD 5.997354507446289, LDE 0.0, LReg 0.0, POD 9.07558822631836 EntMin 0.0
Epoch 6, Class Loss=0.5018268823623657, Reg Loss=6.216123104095459
Clinet index 8, End of Epoch 6/6, Average Loss=6.717949867248535, Class Loss=0.5018268823623657, Reg Loss=6.216123104095459
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=22.856043672561647
Loss made of: CE 0.9412838816642761, LKD 5.394362449645996, LDE 0.0, LReg 0.0, POD 13.094625473022461 EntMin 0.0
Epoch 1, Class Loss=0.9473938345909119, Reg Loss=5.604121208190918
Clinet index 16, End of Epoch 1/6, Average Loss=6.551515102386475, Class Loss=0.9473938345909119, Reg Loss=5.604121208190918
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/19, Loss=18.947295904159546
Loss made of: CE 0.6561940908432007, LKD 5.374728679656982, LDE 0.0, LReg 0.0, POD 13.134449005126953 EntMin 0.0
Epoch 2, Class Loss=0.7428699135780334, Reg Loss=5.557384014129639
Clinet index 16, End of Epoch 2/6, Average Loss=6.300253868103027, Class Loss=0.7428699135780334, Reg Loss=5.557384014129639
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/19, Loss=17.811590695381163
Loss made of: CE 0.6739928126335144, LKD 5.331789970397949, LDE 0.0, LReg 0.0, POD 10.892478942871094 EntMin 0.0
Epoch 3, Class Loss=0.6119982004165649, Reg Loss=5.456255912780762
Clinet index 16, End of Epoch 3/6, Average Loss=6.068253993988037, Class Loss=0.6119982004165649, Reg Loss=5.456255912780762
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/19, Loss=16.940232375264166
Loss made of: CE 0.711698055267334, LKD 5.718108177185059, LDE 0.0, LReg 0.0, POD 12.246367454528809 EntMin 0.0
Epoch 4, Class Loss=0.5603922009468079, Reg Loss=5.414460182189941
Clinet index 16, End of Epoch 4/6, Average Loss=5.974852561950684, Class Loss=0.5603922009468079, Reg Loss=5.414460182189941
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/19, Loss=16.800461873412132
Loss made of: CE 0.4739290475845337, LKD 5.060604095458984, LDE 0.0, LReg 0.0, POD 11.881623268127441 EntMin 0.0
Epoch 5, Class Loss=0.5453450679779053, Reg Loss=5.539554595947266
Clinet index 16, End of Epoch 5/6, Average Loss=6.08489990234375, Class Loss=0.5453450679779053, Reg Loss=5.539554595947266
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/19, Loss=16.170191606879236
Loss made of: CE 0.48766177892684937, LKD 4.687668800354004, LDE 0.0, LReg 0.0, POD 10.223543167114258 EntMin 0.0
Epoch 6, Class Loss=0.5223288536071777, Reg Loss=5.467239856719971
Clinet index 16, End of Epoch 6/6, Average Loss=5.989568710327148, Class Loss=0.5223288536071777, Reg Loss=5.467239856719971
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/26, Loss=17.872727698087694
Loss made of: CE 0.6862890124320984, LKD 5.950690746307373, LDE 0.0, LReg 0.0, POD 9.190237045288086 EntMin 0.0
Epoch 1, Batch 20/26, Loss=16.048131132125853
Loss made of: CE 0.5953494310379028, LKD 5.481193542480469, LDE 0.0, LReg 0.0, POD 8.023087501525879 EntMin 0.0
Epoch 1, Class Loss=0.657453179359436, Reg Loss=5.89309024810791
Clinet index 14, End of Epoch 1/6, Average Loss=6.550543308258057, Class Loss=0.657453179359436, Reg Loss=5.89309024810791
Pseudo labeling is: None
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/26, Loss=17.099006628990175
Loss made of: CE 0.6583766341209412, LKD 5.836711883544922, LDE 0.0, LReg 0.0, POD 10.408138275146484 EntMin 0.0
Epoch 2, Batch 20/26, Loss=16.857720920443533
Loss made of: CE 0.623980700969696, LKD 5.7992048263549805, LDE 0.0, LReg 0.0, POD 9.511872291564941 EntMin 0.0
Epoch 2, Class Loss=0.5671634078025818, Reg Loss=5.823012351989746
Clinet index 14, End of Epoch 2/6, Average Loss=6.390175819396973, Class Loss=0.5671634078025818, Reg Loss=5.823012351989746
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/26, Loss=16.464471608400345
Loss made of: CE 0.5690429210662842, LKD 6.253540992736816, LDE 0.0, LReg 0.0, POD 10.265463829040527 EntMin 0.0
Epoch 3, Batch 20/26, Loss=16.363848406076432
Loss made of: CE 0.4348495900630951, LKD 5.831308364868164, LDE 0.0, LReg 0.0, POD 9.171795845031738 EntMin 0.0
Epoch 3, Class Loss=0.5377505421638489, Reg Loss=5.863394260406494
Clinet index 14, End of Epoch 3/6, Average Loss=6.401144981384277, Class Loss=0.5377505421638489, Reg Loss=5.863394260406494
Pseudo labeling is: None
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/26, Loss=15.623977279663086
Loss made of: CE 0.5193628668785095, LKD 5.702235221862793, LDE 0.0, LReg 0.0, POD 11.680643081665039 EntMin 0.0
Epoch 4, Batch 20/26, Loss=16.456643652915954
Loss made of: CE 0.5516781806945801, LKD 6.009063720703125, LDE 0.0, LReg 0.0, POD 9.089850425720215 EntMin 0.0
Epoch 4, Class Loss=0.5126382112503052, Reg Loss=5.879795074462891
Clinet index 14, End of Epoch 4/6, Average Loss=6.392433166503906, Class Loss=0.5126382112503052, Reg Loss=5.879795074462891
Pseudo labeling is: None
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/26, Loss=15.386683401465415
Loss made of: CE 0.6780533790588379, LKD 5.486202239990234, LDE 0.0, LReg 0.0, POD 9.760101318359375 EntMin 0.0
Epoch 5, Batch 20/26, Loss=16.08575669527054
Loss made of: CE 0.5741013884544373, LKD 5.588349342346191, LDE 0.0, LReg 0.0, POD 8.989567756652832 EntMin 0.0
Epoch 5, Class Loss=0.502292811870575, Reg Loss=5.86376953125
Clinet index 14, End of Epoch 5/6, Average Loss=6.366062164306641, Class Loss=0.502292811870575, Reg Loss=5.86376953125
Pseudo labeling is: None
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/26, Loss=15.536833691596986
Loss made of: CE 0.43525180220603943, LKD 6.187803268432617, LDE 0.0, LReg 0.0, POD 12.818137168884277 EntMin 0.0
Epoch 6, Batch 20/26, Loss=15.497883054614068
Loss made of: CE 0.4395381808280945, LKD 5.518589019775391, LDE 0.0, LReg 0.0, POD 10.208597183227539 EntMin 0.0
Epoch 6, Class Loss=0.4996751844882965, Reg Loss=5.859592437744141
Clinet index 14, End of Epoch 6/6, Average Loss=6.359267711639404, Class Loss=0.4996751844882965, Reg Loss=5.859592437744141
federated aggregation...
Validation, Class Loss=0.7480959296226501, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.806194
Mean Acc: 0.469341
FreqW Acc: 0.703683
Mean IoU: 0.315123
Class IoU:
	class 0: 0.8602277
	class 1: 0.65485114
	class 2: 0.2864652
	class 3: 0.40401778
	class 4: 0.41876674
	class 5: 0.022064012
	class 6: 0.64338595
	class 7: 0.5170312
	class 8: 0.3164677
	class 9: 0.0044290368
	class 10: 0.31909397
	class 11: 0.21139704
	class 12: 0.036318406
	class 13: 0.21449952
	class 14: 0.4482702
	class 15: 0.69411236
	class 16: 0.013207235
	class 17: 0.0
	class 18: 0.0145836305
	class 19: 0.22655977
	class 20: 0.31183955
Class Acc:
	class 0: 0.9301764
	class 1: 0.6939288
	class 2: 0.5854412
	class 3: 0.910597
	class 4: 0.91754556
	class 5: 0.022065125
	class 6: 0.8176079
	class 7: 0.7728666
	class 8: 0.8693307
	class 9: 0.0044797612
	class 10: 0.41169935
	class 11: 0.5570675
	class 12: 0.036653325
	class 13: 0.22636907
	class 14: 0.5472388
	class 15: 0.9064124
	class 16: 0.013326235
	class 17: 0.0
	class 18: 0.014653691
	class 19: 0.23761617
	class 20: 0.3810903

federated global round: 23, step: 4
select part of clients to conduct local training
[12, 10, 9, 6]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/26, Loss=16.563007068634032
Loss made of: CE 0.4892691969871521, LKD 5.10076904296875, LDE 0.0, LReg 0.0, POD 8.047053337097168 EntMin 0.0
Epoch 1, Batch 20/26, Loss=16.86028055846691
Loss made of: CE 0.43118271231651306, LKD 5.52701473236084, LDE 0.0, LReg 0.0, POD 7.932929992675781 EntMin 0.0
Epoch 1, Class Loss=0.6226080060005188, Reg Loss=5.929554462432861
Clinet index 12, End of Epoch 1/6, Average Loss=6.5521626472473145, Class Loss=0.6226080060005188, Reg Loss=5.929554462432861
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/26, Loss=16.018375670909883
Loss made of: CE 0.5762379169464111, LKD 5.768211364746094, LDE 0.0, LReg 0.0, POD 8.561253547668457 EntMin 0.0
Epoch 2, Batch 20/26, Loss=16.38662999868393
Loss made of: CE 0.42370057106018066, LKD 5.6963019371032715, LDE 0.0, LReg 0.0, POD 10.312288284301758 EntMin 0.0
Epoch 2, Class Loss=0.5289302468299866, Reg Loss=5.878196716308594
Clinet index 12, End of Epoch 2/6, Average Loss=6.4071269035339355, Class Loss=0.5289302468299866, Reg Loss=5.878196716308594
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/26, Loss=16.723896807432176
Loss made of: CE 0.4170534312725067, LKD 6.293027400970459, LDE 0.0, LReg 0.0, POD 11.201842308044434 EntMin 0.0
Epoch 3, Batch 20/26, Loss=15.851557013392448
Loss made of: CE 0.49611610174179077, LKD 5.787383079528809, LDE 0.0, LReg 0.0, POD 9.09885025024414 EntMin 0.0
Epoch 3, Class Loss=0.5038794279098511, Reg Loss=5.8618621826171875
Clinet index 12, End of Epoch 3/6, Average Loss=6.365741729736328, Class Loss=0.5038794279098511, Reg Loss=5.8618621826171875
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/26, Loss=15.780122929811478
Loss made of: CE 0.5732981562614441, LKD 6.461496353149414, LDE 0.0, LReg 0.0, POD 10.4351806640625 EntMin 0.0
Epoch 4, Batch 20/26, Loss=15.82722993195057
Loss made of: CE 0.4719841480255127, LKD 5.831897735595703, LDE 0.0, LReg 0.0, POD 8.737768173217773 EntMin 0.0
Epoch 4, Class Loss=0.48307159543037415, Reg Loss=5.895354270935059
Clinet index 12, End of Epoch 4/6, Average Loss=6.3784260749816895, Class Loss=0.48307159543037415, Reg Loss=5.895354270935059
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/26, Loss=15.501237288117409
Loss made of: CE 0.598274827003479, LKD 6.578845977783203, LDE 0.0, LReg 0.0, POD 8.893312454223633 EntMin 0.0
Epoch 5, Batch 20/26, Loss=15.820580044388771
Loss made of: CE 0.5278140306472778, LKD 5.403048515319824, LDE 0.0, LReg 0.0, POD 9.389486312866211 EntMin 0.0
Epoch 5, Class Loss=0.4841582477092743, Reg Loss=5.858339309692383
Clinet index 12, End of Epoch 5/6, Average Loss=6.3424973487854, Class Loss=0.4841582477092743, Reg Loss=5.858339309692383
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/26, Loss=15.024368324875832
Loss made of: CE 0.42507404088974, LKD 4.6660051345825195, LDE 0.0, LReg 0.0, POD 9.815217971801758 EntMin 0.0
Epoch 6, Batch 20/26, Loss=15.343263575434685
Loss made of: CE 0.4005836248397827, LKD 6.094533443450928, LDE 0.0, LReg 0.0, POD 8.634267807006836 EntMin 0.0
Epoch 6, Class Loss=0.4708109200000763, Reg Loss=5.890380382537842
Clinet index 12, End of Epoch 6/6, Average Loss=6.361191272735596, Class Loss=0.4708109200000763, Reg Loss=5.890380382537842
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=19.445837807655334
Loss made of: CE 0.8162441253662109, LKD 6.002130031585693, LDE 0.0, LReg 0.0, POD 11.596983909606934 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.8773354887962341, Reg Loss=5.860284328460693
Clinet index 10, End of Epoch 1/6, Average Loss=6.737619876861572, Class Loss=0.8773354887962341, Reg Loss=5.860284328460693
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=17.38850468993187
Loss made of: CE 0.5248264074325562, LKD 5.205552577972412, LDE 0.0, LReg 0.0, POD 10.253311157226562 EntMin 0.0
Epoch 2, Class Loss=0.7482306957244873, Reg Loss=5.852978706359863
Clinet index 10, End of Epoch 2/6, Average Loss=6.60120964050293, Class Loss=0.7482306957244873, Reg Loss=5.852978706359863
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=16.367399203777314
Loss made of: CE 0.6210881471633911, LKD 6.474191188812256, LDE 0.0, LReg 0.0, POD 9.239606857299805 EntMin 0.0
Epoch 3, Class Loss=0.6431499123573303, Reg Loss=5.814022541046143
Clinet index 10, End of Epoch 3/6, Average Loss=6.457172393798828, Class Loss=0.6431499123573303, Reg Loss=5.814022541046143
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=17.119910338521002
Loss made of: CE 0.6609783172607422, LKD 6.232678413391113, LDE 0.0, LReg 0.0, POD 9.398576736450195 EntMin 0.0
Epoch 4, Class Loss=0.5898650288581848, Reg Loss=5.835459232330322
Clinet index 10, End of Epoch 4/6, Average Loss=6.425324440002441, Class Loss=0.5898650288581848, Reg Loss=5.835459232330322
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=16.451266649365426
Loss made of: CE 0.6166357398033142, LKD 5.153386116027832, LDE 0.0, LReg 0.0, POD 11.445085525512695 EntMin 0.0
Epoch 5, Class Loss=0.5793939828872681, Reg Loss=5.787302017211914
Clinet index 10, End of Epoch 5/6, Average Loss=6.366695880889893, Class Loss=0.5793939828872681, Reg Loss=5.787302017211914
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=15.35488910973072
Loss made of: CE 0.5392318964004517, LKD 5.537852764129639, LDE 0.0, LReg 0.0, POD 10.086524963378906 EntMin 0.0
Epoch 6, Class Loss=0.5615909099578857, Reg Loss=5.7400126457214355
Clinet index 10, End of Epoch 6/6, Average Loss=6.301603317260742, Class Loss=0.5615909099578857, Reg Loss=5.7400126457214355
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=17.66084417104721
Loss made of: CE 0.7398936748504639, LKD 6.706714630126953, LDE 0.0, LReg 0.0, POD 10.716557502746582 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 20/21, Loss=17.074379858374595
Loss made of: CE 0.5380326509475708, LKD 5.889904022216797, LDE 0.0, LReg 0.0, POD 9.632655143737793 EntMin 0.0
Epoch 1, Class Loss=0.6454910039901733, Reg Loss=6.189534664154053
Clinet index 9, End of Epoch 1/6, Average Loss=6.835025787353516, Class Loss=0.6454910039901733, Reg Loss=6.189534664154053
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/21, Loss=16.869630959630012
Loss made of: CE 0.5285927057266235, LKD 6.285848140716553, LDE 0.0, LReg 0.0, POD 10.387226104736328 EntMin 0.0
Epoch 2, Batch 20/21, Loss=17.116419398784636
Loss made of: CE 0.44993776082992554, LKD 6.060932159423828, LDE 0.0, LReg 0.0, POD 8.670608520507812 EntMin 0.0
Epoch 2, Class Loss=0.572455108165741, Reg Loss=6.179093360900879
Clinet index 9, End of Epoch 2/6, Average Loss=6.7515482902526855, Class Loss=0.572455108165741, Reg Loss=6.179093360900879
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/21, Loss=17.342527458071707
Loss made of: CE 0.5784668922424316, LKD 5.674039840698242, LDE 0.0, LReg 0.0, POD 9.22384262084961 EntMin 0.0
Epoch 3, Batch 20/21, Loss=16.907228550314905
Loss made of: CE 0.5709477663040161, LKD 6.226218223571777, LDE 0.0, LReg 0.0, POD 10.58316707611084 EntMin 0.0
Epoch 3, Class Loss=0.5206111073493958, Reg Loss=6.157401084899902
Clinet index 9, End of Epoch 3/6, Average Loss=6.678012371063232, Class Loss=0.5206111073493958, Reg Loss=6.157401084899902
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/21, Loss=16.32591800391674
Loss made of: CE 0.5644816756248474, LKD 6.210917949676514, LDE 0.0, LReg 0.0, POD 9.872705459594727 EntMin 0.0
Epoch 4, Batch 20/21, Loss=16.936066615581513
Loss made of: CE 0.4652588963508606, LKD 6.776363372802734, LDE 0.0, LReg 0.0, POD 8.820564270019531 EntMin 0.0
Epoch 4, Class Loss=0.4960436224937439, Reg Loss=6.20081901550293
Clinet index 9, End of Epoch 4/6, Average Loss=6.696862697601318, Class Loss=0.4960436224937439, Reg Loss=6.20081901550293
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=16.23697994351387
Loss made of: CE 0.33476635813713074, LKD 6.270041465759277, LDE 0.0, LReg 0.0, POD 9.893312454223633 EntMin 0.0
Epoch 5, Batch 20/21, Loss=16.395090824365617
Loss made of: CE 0.43059229850769043, LKD 5.660806179046631, LDE 0.0, LReg 0.0, POD 8.635272026062012 EntMin 0.0
Epoch 5, Class Loss=0.48350411653518677, Reg Loss=6.1624226570129395
Clinet index 9, End of Epoch 5/6, Average Loss=6.6459269523620605, Class Loss=0.48350411653518677, Reg Loss=6.1624226570129395
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/21, Loss=16.979116335511208
Loss made of: CE 0.38694581389427185, LKD 6.395870208740234, LDE 0.0, LReg 0.0, POD 11.718118667602539 EntMin 0.0
Epoch 6, Batch 20/21, Loss=16.540558260679244
Loss made of: CE 0.40479040145874023, LKD 5.514564514160156, LDE 0.0, LReg 0.0, POD 8.627883911132812 EntMin 0.0
Epoch 6, Class Loss=0.47213736176490784, Reg Loss=6.224299430847168
Clinet index 9, End of Epoch 6/6, Average Loss=6.696436882019043, Class Loss=0.47213736176490784, Reg Loss=6.224299430847168
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=17.600526654720305
Loss made of: CE 0.5259067416191101, LKD 6.1603193283081055, LDE 0.0, LReg 0.0, POD 9.97597599029541 EntMin 0.0
Epoch 1, Batch 20/24, Loss=16.84247878789902
Loss made of: CE 0.6167289018630981, LKD 5.80457067489624, LDE 0.0, LReg 0.0, POD 9.516805648803711 EntMin 0.0
Epoch 1, Class Loss=0.5852677822113037, Reg Loss=6.099288463592529
Clinet index 6, End of Epoch 1/6, Average Loss=6.684556007385254, Class Loss=0.5852677822113037, Reg Loss=6.099288463592529
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 10/24, Loss=16.156413415074347
Loss made of: CE 0.4863879084587097, LKD 5.96286678314209, LDE 0.0, LReg 0.0, POD 10.174259185791016 EntMin 0.0
Epoch 2, Batch 20/24, Loss=16.651467052102088
Loss made of: CE 0.5577138662338257, LKD 6.1972246170043945, LDE 0.0, LReg 0.0, POD 9.867364883422852 EntMin 0.0
Epoch 2, Class Loss=0.517359733581543, Reg Loss=6.037579536437988
Clinet index 6, End of Epoch 2/6, Average Loss=6.554939270019531, Class Loss=0.517359733581543, Reg Loss=6.037579536437988
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/24, Loss=16.365942230820657
Loss made of: CE 0.3868842124938965, LKD 5.830667495727539, LDE 0.0, LReg 0.0, POD 9.130032539367676 EntMin 0.0
Epoch 3, Batch 20/24, Loss=16.34121860563755
Loss made of: CE 0.4854947030544281, LKD 6.584990501403809, LDE 0.0, LReg 0.0, POD 10.018112182617188 EntMin 0.0
Epoch 3, Class Loss=0.4826013445854187, Reg Loss=6.092062950134277
Clinet index 6, End of Epoch 3/6, Average Loss=6.574664115905762, Class Loss=0.4826013445854187, Reg Loss=6.092062950134277
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/24, Loss=15.995774269104004
Loss made of: CE 0.39649102091789246, LKD 5.904941558837891, LDE 0.0, LReg 0.0, POD 9.53980827331543 EntMin 0.0
Epoch 4, Batch 20/24, Loss=15.944789952039718
Loss made of: CE 0.595184326171875, LKD 6.196681976318359, LDE 0.0, LReg 0.0, POD 8.807273864746094 EntMin 0.0
Epoch 4, Class Loss=0.47908148169517517, Reg Loss=6.060004711151123
Clinet index 6, End of Epoch 4/6, Average Loss=6.53908634185791, Class Loss=0.47908148169517517, Reg Loss=6.060004711151123
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/24, Loss=16.028985208272935
Loss made of: CE 0.7347589731216431, LKD 5.688175201416016, LDE 0.0, LReg 0.0, POD 10.800809860229492 EntMin 0.0
Epoch 5, Batch 20/24, Loss=16.016223073005676
Loss made of: CE 0.4028458595275879, LKD 6.157329559326172, LDE 0.0, LReg 0.0, POD 8.409534454345703 EntMin 0.0
Epoch 5, Class Loss=0.45541760325431824, Reg Loss=6.041139125823975
Clinet index 6, End of Epoch 5/6, Average Loss=6.496556758880615, Class Loss=0.45541760325431824, Reg Loss=6.041139125823975
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/24, Loss=16.116744023561477
Loss made of: CE 0.5143787264823914, LKD 6.091680526733398, LDE 0.0, LReg 0.0, POD 8.543636322021484 EntMin 0.0
Epoch 6, Batch 20/24, Loss=15.718280366063118
Loss made of: CE 0.4714508056640625, LKD 6.143117427825928, LDE 0.0, LReg 0.0, POD 10.552481651306152 EntMin 0.0
Epoch 6, Class Loss=0.45177873969078064, Reg Loss=6.041934490203857
Clinet index 6, End of Epoch 6/6, Average Loss=6.49371337890625, Class Loss=0.45177873969078064, Reg Loss=6.041934490203857
federated aggregation...
Validation, Class Loss=0.75517737865448, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.804307
Mean Acc: 0.484289
FreqW Acc: 0.704283
Mean IoU: 0.323786
Class IoU:
	class 0: 0.8586431
	class 1: 0.65200907
	class 2: 0.2930626
	class 3: 0.40224916
	class 4: 0.42461798
	class 5: 0.012131549
	class 6: 0.64098334
	class 7: 0.51621026
	class 8: 0.314393
	class 9: 0.004605228
	class 10: 0.32989895
	class 11: 0.210307
	class 12: 0.03872837
	class 13: 0.19835839
	class 14: 0.41860425
	class 15: 0.6831781
	class 16: 0.012017364
	class 17: 0.0056024524
	class 18: 0.16906622
	class 19: 0.22613911
	class 20: 0.38869157
Class Acc:
	class 0: 0.92204946
	class 1: 0.68966603
	class 2: 0.6239422
	class 3: 0.9157902
	class 4: 0.912598
	class 5: 0.012131549
	class 6: 0.78259945
	class 7: 0.7654622
	class 8: 0.87414044
	class 9: 0.004648845
	class 10: 0.44597265
	class 11: 0.5948961
	class 12: 0.0392147
	class 13: 0.20788267
	class 14: 0.501061
	class 15: 0.9157379
	class 16: 0.012057808
	class 17: 0.0056219483
	class 18: 0.18674582
	class 19: 0.23243606
	class 20: 0.5254049

federated global round: 24, step: 4
select part of clients to conduct local training
[18, 24, 25, 10]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=17.874953639507293
Loss made of: CE 0.6766989231109619, LKD 5.549771308898926, LDE 0.0, LReg 0.0, POD 10.6707181930542 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.7229183316230774, Reg Loss=5.685693740844727
Clinet index 18, End of Epoch 1/6, Average Loss=6.408612251281738, Class Loss=0.7229183316230774, Reg Loss=5.685693740844727
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/19, Loss=16.61152344942093
Loss made of: CE 0.6623298525810242, LKD 5.5872344970703125, LDE 0.0, LReg 0.0, POD 9.861116409301758 EntMin 0.0
Epoch 2, Class Loss=0.6217591166496277, Reg Loss=5.661721706390381
Clinet index 18, End of Epoch 2/6, Average Loss=6.283480644226074, Class Loss=0.6217591166496277, Reg Loss=5.661721706390381
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/19, Loss=16.540910172462464
Loss made of: CE 0.6541579961776733, LKD 6.362663269042969, LDE 0.0, LReg 0.0, POD 10.050350189208984 EntMin 0.0
Epoch 3, Class Loss=0.5777704119682312, Reg Loss=5.683979511260986
Clinet index 18, End of Epoch 3/6, Average Loss=6.261749744415283, Class Loss=0.5777704119682312, Reg Loss=5.683979511260986
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/19, Loss=16.014231011271477
Loss made of: CE 0.5311760306358337, LKD 5.996396541595459, LDE 0.0, LReg 0.0, POD 9.23785400390625 EntMin 0.0
Epoch 4, Class Loss=0.5551276803016663, Reg Loss=5.636860370635986
Clinet index 18, End of Epoch 4/6, Average Loss=6.191987991333008, Class Loss=0.5551276803016663, Reg Loss=5.636860370635986
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/19, Loss=16.056571394205093
Loss made of: CE 0.44422489404678345, LKD 4.920135974884033, LDE 0.0, LReg 0.0, POD 9.479240417480469 EntMin 0.0
Epoch 5, Class Loss=0.5488229393959045, Reg Loss=5.632806301116943
Clinet index 18, End of Epoch 5/6, Average Loss=6.181629180908203, Class Loss=0.5488229393959045, Reg Loss=5.632806301116943
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/19, Loss=15.43648910522461
Loss made of: CE 0.6513199210166931, LKD 6.069066524505615, LDE 0.0, LReg 0.0, POD 9.804869651794434 EntMin 0.0
Epoch 6, Class Loss=0.5336456298828125, Reg Loss=5.647734642028809
Clinet index 18, End of Epoch 6/6, Average Loss=6.181380271911621, Class Loss=0.5336456298828125, Reg Loss=5.647734642028809
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/21, Loss=16.27454207241535
Loss made of: CE 0.40801891684532166, LKD 5.694051742553711, LDE 0.0, LReg 0.0, POD 8.21298599243164 EntMin 0.0
Epoch 1, Batch 20/21, Loss=17.597668504714967
Loss made of: CE 0.6578235030174255, LKD 6.208713531494141, LDE 0.0, LReg 0.0, POD 10.431900024414062 EntMin 0.0
Epoch 1, Class Loss=0.6091170907020569, Reg Loss=6.2203168869018555
Clinet index 24, End of Epoch 1/6, Average Loss=6.829433917999268, Class Loss=0.6091170907020569, Reg Loss=6.2203168869018555
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/21, Loss=16.38050438463688
Loss made of: CE 0.5385875701904297, LKD 6.218100547790527, LDE 0.0, LReg 0.0, POD 9.318469047546387 EntMin 0.0
Epoch 2, Batch 20/21, Loss=17.408831486105917
Loss made of: CE 0.7255458235740662, LKD 6.588757514953613, LDE 0.0, LReg 0.0, POD 11.655620574951172 EntMin 0.0
Epoch 2, Class Loss=0.540581464767456, Reg Loss=6.204552173614502
Clinet index 24, End of Epoch 2/6, Average Loss=6.745133399963379, Class Loss=0.540581464767456, Reg Loss=6.204552173614502
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/21, Loss=16.07211589217186
Loss made of: CE 0.5423205494880676, LKD 6.386664390563965, LDE 0.0, LReg 0.0, POD 9.496368408203125 EntMin 0.0
Epoch 3, Batch 20/21, Loss=16.603102213144304
Loss made of: CE 0.43766945600509644, LKD 5.665399074554443, LDE 0.0, LReg 0.0, POD 9.962894439697266 EntMin 0.0
Epoch 3, Class Loss=0.5011081099510193, Reg Loss=6.1515984535217285
Clinet index 24, End of Epoch 3/6, Average Loss=6.652706623077393, Class Loss=0.5011081099510193, Reg Loss=6.1515984535217285
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/21, Loss=16.167664554715156
Loss made of: CE 0.5903116464614868, LKD 7.148877143859863, LDE 0.0, LReg 0.0, POD 10.174955368041992 EntMin 0.0
Epoch 4, Batch 20/21, Loss=15.37699943780899
Loss made of: CE 0.49871826171875, LKD 6.134164810180664, LDE 0.0, LReg 0.0, POD 9.33409309387207 EntMin 0.0
Epoch 4, Class Loss=0.4999910295009613, Reg Loss=6.21475887298584
Clinet index 24, End of Epoch 4/6, Average Loss=6.714749813079834, Class Loss=0.4999910295009613, Reg Loss=6.21475887298584
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/21, Loss=15.763003459572792
Loss made of: CE 0.4254480302333832, LKD 5.42941951751709, LDE 0.0, LReg 0.0, POD 8.918519973754883 EntMin 0.0
Epoch 5, Batch 20/21, Loss=15.74180510342121
Loss made of: CE 0.514164388179779, LKD 6.694270133972168, LDE 0.0, LReg 0.0, POD 9.240185737609863 EntMin 0.0
Epoch 5, Class Loss=0.4824688732624054, Reg Loss=6.156209945678711
Clinet index 24, End of Epoch 5/6, Average Loss=6.638679027557373, Class Loss=0.4824688732624054, Reg Loss=6.156209945678711
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/21, Loss=16.34847288131714
Loss made of: CE 0.6727479696273804, LKD 7.423652648925781, LDE 0.0, LReg 0.0, POD 10.332523345947266 EntMin 0.0
Epoch 6, Batch 20/21, Loss=14.991795989871026
Loss made of: CE 0.4565487802028656, LKD 5.936501502990723, LDE 0.0, LReg 0.0, POD 8.18340015411377 EntMin 0.0
Epoch 6, Class Loss=0.4892120361328125, Reg Loss=6.165689468383789
Clinet index 24, End of Epoch 6/6, Average Loss=6.654901504516602, Class Loss=0.4892120361328125, Reg Loss=6.165689468383789
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/24, Loss=16.532015463709833
Loss made of: CE 0.6369274854660034, LKD 6.486512660980225, LDE 0.0, LReg 0.0, POD 10.341510772705078 EntMin 0.0
Epoch 1, Batch 20/24, Loss=16.36014541089535
Loss made of: CE 0.5253438353538513, LKD 6.376578330993652, LDE 0.0, LReg 0.0, POD 10.032115936279297 EntMin 0.0
Epoch 1, Class Loss=0.5294567346572876, Reg Loss=6.261887550354004
Clinet index 25, End of Epoch 1/6, Average Loss=6.791344165802002, Class Loss=0.5294567346572876, Reg Loss=6.261887550354004
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/24, Loss=16.603731644153594
Loss made of: CE 0.5948711633682251, LKD 6.756717681884766, LDE 0.0, LReg 0.0, POD 9.635920524597168 EntMin 0.0
Epoch 2, Batch 20/24, Loss=16.23303704857826
Loss made of: CE 0.48571547865867615, LKD 6.660101890563965, LDE 0.0, LReg 0.0, POD 9.794061660766602 EntMin 0.0
Epoch 2, Class Loss=0.4954289197921753, Reg Loss=6.251828670501709
Clinet index 25, End of Epoch 2/6, Average Loss=6.747257709503174, Class Loss=0.4954289197921753, Reg Loss=6.251828670501709
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/24, Loss=16.216768565773965
Loss made of: CE 0.4722289443016052, LKD 6.195981025695801, LDE 0.0, LReg 0.0, POD 9.599329948425293 EntMin 0.0
Epoch 3, Batch 20/24, Loss=16.246783098578454
Loss made of: CE 0.34769290685653687, LKD 5.952533721923828, LDE 0.0, LReg 0.0, POD 8.520391464233398 EntMin 0.0
Epoch 3, Class Loss=0.47472020983695984, Reg Loss=6.234588623046875
Clinet index 25, End of Epoch 3/6, Average Loss=6.709308624267578, Class Loss=0.47472020983695984, Reg Loss=6.234588623046875
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/24, Loss=16.16825689673424
Loss made of: CE 0.48873090744018555, LKD 7.080245494842529, LDE 0.0, LReg 0.0, POD 8.924229621887207 EntMin 0.0
Epoch 4, Batch 20/24, Loss=16.128546166419984
Loss made of: CE 0.5055931806564331, LKD 7.090305328369141, LDE 0.0, LReg 0.0, POD 10.330008506774902 EntMin 0.0
Epoch 4, Class Loss=0.46932846307754517, Reg Loss=6.290555953979492
Clinet index 25, End of Epoch 4/6, Average Loss=6.759884357452393, Class Loss=0.46932846307754517, Reg Loss=6.290555953979492
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/24, Loss=15.737494027614593
Loss made of: CE 0.4499318599700928, LKD 6.9296674728393555, LDE 0.0, LReg 0.0, POD 8.739424705505371 EntMin 0.0
Epoch 5, Batch 20/24, Loss=15.736165237426757
Loss made of: CE 0.5340373516082764, LKD 6.230824947357178, LDE 0.0, LReg 0.0, POD 10.156272888183594 EntMin 0.0
Epoch 5, Class Loss=0.46256107091903687, Reg Loss=6.241551876068115
Clinet index 25, End of Epoch 5/6, Average Loss=6.704113006591797, Class Loss=0.46256107091903687, Reg Loss=6.241551876068115
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/24, Loss=15.147266894578934
Loss made of: CE 0.5486305952072144, LKD 6.462333679199219, LDE 0.0, LReg 0.0, POD 9.30137825012207 EntMin 0.0
Epoch 6, Batch 20/24, Loss=15.414057612419128
Loss made of: CE 0.46396714448928833, LKD 6.144136428833008, LDE 0.0, LReg 0.0, POD 9.278791427612305 EntMin 0.0
Epoch 6, Class Loss=0.457802414894104, Reg Loss=6.206949234008789
Clinet index 25, End of Epoch 6/6, Average Loss=6.6647515296936035, Class Loss=0.457802414894104, Reg Loss=6.206949234008789
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/19, Loss=17.815746861696244
Loss made of: CE 0.7127606272697449, LKD 6.15397834777832, LDE 0.0, LReg 0.0, POD 10.704436302185059 EntMin 0.0
Epoch 1, Class Loss=0.77447509765625, Reg Loss=5.768143177032471
Clinet index 10, End of Epoch 1/6, Average Loss=6.542618274688721, Class Loss=0.77447509765625, Reg Loss=5.768143177032471
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/19, Loss=16.40555684566498
Loss made of: CE 0.4980188012123108, LKD 5.299030303955078, LDE 0.0, LReg 0.0, POD 10.172889709472656 EntMin 0.0
Epoch 2, Class Loss=0.6897538900375366, Reg Loss=5.794126033782959
Clinet index 10, End of Epoch 2/6, Average Loss=6.483880043029785, Class Loss=0.6897538900375366, Reg Loss=5.794126033782959
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/19, Loss=15.58657253086567
Loss made of: CE 0.6312925815582275, LKD 6.593455791473389, LDE 0.0, LReg 0.0, POD 8.631376266479492 EntMin 0.0
Epoch 3, Class Loss=0.6529563665390015, Reg Loss=5.849496841430664
Clinet index 10, End of Epoch 3/6, Average Loss=6.502453327178955, Class Loss=0.6529563665390015, Reg Loss=5.849496841430664
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/19, Loss=16.407252067327498
Loss made of: CE 0.7382901906967163, LKD 6.244071006774902, LDE 0.0, LReg 0.0, POD 8.731161117553711 EntMin 0.0
Epoch 4, Class Loss=0.6244668960571289, Reg Loss=5.774329662322998
Clinet index 10, End of Epoch 4/6, Average Loss=6.398796558380127, Class Loss=0.6244668960571289, Reg Loss=5.774329662322998
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/19, Loss=15.78146321773529
Loss made of: CE 0.6245263814926147, LKD 5.054027557373047, LDE 0.0, LReg 0.0, POD 10.627840042114258 EntMin 0.0
Epoch 5, Class Loss=0.6154282689094543, Reg Loss=5.787425518035889
Clinet index 10, End of Epoch 5/6, Average Loss=6.402853965759277, Class Loss=0.6154282689094543, Reg Loss=5.787425518035889
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/19, Loss=14.93301617205143
Loss made of: CE 0.5886968970298767, LKD 5.5431437492370605, LDE 0.0, LReg 0.0, POD 9.709283828735352 EntMin 0.0
Epoch 6, Class Loss=0.6063250303268433, Reg Loss=5.756272315979004
Clinet index 10, End of Epoch 6/6, Average Loss=6.362597465515137, Class Loss=0.6063250303268433, Reg Loss=5.756272315979004
federated aggregation...
Validation, Class Loss=0.7690243124961853, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.801451
Mean Acc: 0.493575
FreqW Acc: 0.702055
Mean IoU: 0.323593
Class IoU:
	class 0: 0.8555418
	class 1: 0.6282267
	class 2: 0.29568413
	class 3: 0.38851258
	class 4: 0.39316452
	class 5: 0.009197274
	class 6: 0.6228693
	class 7: 0.5121399
	class 8: 0.32642826
	class 9: 0.003394558
	class 10: 0.34081632
	class 11: 0.21105297
	class 12: 0.038662042
	class 13: 0.18056135
	class 14: 0.40796295
	class 15: 0.68614
	class 16: 0.011941972
	class 17: 0.065251574
	class 18: 0.27667618
	class 19: 0.1503875
	class 20: 0.39084157
Class Acc:
	class 0: 0.9162934
	class 1: 0.6595834
	class 2: 0.6594538
	class 3: 0.91793424
	class 4: 0.9121401
	class 5: 0.009197274
	class 6: 0.7594507
	class 7: 0.7459757
	class 8: 0.8637389
	class 9: 0.0034128607
	class 10: 0.46593097
	class 11: 0.59488785
	class 12: 0.03907998
	class 13: 0.18898973
	class 14: 0.47732204
	class 15: 0.91060567
	class 16: 0.011972683
	class 17: 0.07087159
	class 18: 0.42118776
	class 19: 0.15150726
	class 20: 0.5855344

voc_4-4_RCIL On GPUs 2
Run in 148529s
