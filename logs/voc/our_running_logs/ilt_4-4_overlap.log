nohup: ignoring input
25
kvoc_4-4_ILT On GPUs 0\Writing in results/seed_2023-ov/2023-03-09_voc_4-4_ILT.csv
Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 0, step: 0
select part of clients to conduct local training
[6, 7, 9, 2]
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
federated aggregation...
federated global round: 1, step: 0
select part of clients to conduct local training
[6, 0, 7, 2]
Current Client Index:  6
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
Current Client Index:  2
federated aggregation...
federated global round: 2, step: 0
select part of clients to conduct local training
[8, 5, 3, 7]
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
federated global round: 3, step: 0
select part of clients to conduct local training
[5, 2, 0, 3]
Current Client Index:  5
Current Client Index:  2
Current Client Index:  0
Current Client Index:  3
federated aggregation...
federated global round: 4, step: 0
select part of clients to conduct local training
[3, 6, 1, 7]
Current Client Index:  3
Current Client Index:  6
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
Current Client Index:  7
federated aggregation...
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
Validation, Class Loss=0.09879439324140549, Reg Loss=0.0 (without scaling)

Total samples: 341.000000
Overall Acc: 0.957759
Mean Acc: 0.761733
FreqW Acc: 0.923270
Mean IoU: 0.703965
Class IoU:
	class 0: 0.9539515191877496
	class 1: 0.7615770129403676
	class 2: 0.2054173196242969
	class 3: 0.8920301311931925
	class 4: 0.7068509839220561
Class Acc:
	class 0: 0.9848764620456163
	class 1: 0.7738026584549321
	class 2: 0.27733822617675347
	class 3: 0.9519182023560161
	class 4: 0.8207287142557349

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 5, step: 1
select part of clients to conduct local training
[0, 12, 6, 10]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError("/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so)",)
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
/home/amax/anaconda3/envs/FCIL/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/52, Loss=33.154309606552125
Loss made of: CE 1.6151683330535889, LKD 4.225778579711914, LDE 22.133609771728516, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/52, Loss=24.677953678369523
Loss made of: CE 0.9618743658065796, LKD 4.433593273162842, LDE 19.12764549255371, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=23.31958681344986
Loss made of: CE 0.7971638441085815, LKD 3.264472007751465, LDE 19.00075340270996, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=22.304148492217063
Loss made of: CE 0.38501691818237305, LKD 2.7598063945770264, LDE 18.580446243286133, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/52, Loss=22.16401988863945
Loss made of: CE 0.393404483795166, LKD 2.6479952335357666, LDE 17.005794525146484, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.8987863063812256, Reg Loss=24.04784393310547
Clinet index 0, End of Epoch 1/6, Average Loss=24.946630477905273, Class Loss=0.8987863063812256, Reg Loss=24.04784393310547
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/52, Loss=21.257471165061
Loss made of: CE 0.4848235547542572, LKD 3.3162765502929688, LDE 15.371835708618164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=20.144879055023193
Loss made of: CE 0.35422617197036743, LKD 3.0730319023132324, LDE 15.54870891571045, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=20.839558199048042
Loss made of: CE 0.5141599178314209, LKD 3.409062385559082, LDE 17.35090446472168, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/52, Loss=20.15490864813328
Loss made of: CE 0.39115971326828003, LKD 4.583912372589111, LDE 15.146927833557129, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=19.33405583500862
Loss made of: CE 0.4274079203605652, LKD 4.572790145874023, LDE 19.6928768157959, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4121030569076538, Reg Loss=19.85574722290039
Clinet index 0, End of Epoch 2/6, Average Loss=20.267850875854492, Class Loss=0.4121030569076538, Reg Loss=19.85574722290039
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/52, Loss=18.615241342782973
Loss made of: CE 0.32994112372398376, LKD 3.7501449584960938, LDE 15.200215339660645, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=18.94822888672352
Loss made of: CE 0.4110869765281677, LKD 4.22045373916626, LDE 14.238182067871094, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=18.76429903805256
Loss made of: CE 0.4177539050579071, LKD 5.320624828338623, LDE 12.368146896362305, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=17.98761257827282
Loss made of: CE 0.35428744554519653, LKD 2.5813660621643066, LDE 14.616399765014648, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/52, Loss=18.179797354340554
Loss made of: CE 0.29940322041511536, LKD 3.37453556060791, LDE 13.363724708557129, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.35982027649879456, Reg Loss=18.08079719543457
Clinet index 0, End of Epoch 3/6, Average Loss=18.440616607666016, Class Loss=0.35982027649879456, Reg Loss=18.08079719543457
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/52, Loss=18.493560045957565
Loss made of: CE 0.43360161781311035, LKD 4.03123664855957, LDE 12.970684051513672, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=16.461939366161822
Loss made of: CE 0.257002592086792, LKD 3.6603827476501465, LDE 11.491032600402832, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=17.057682606577874
Loss made of: CE 0.28495854139328003, LKD 3.284284830093384, LDE 12.489261627197266, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=16.79913007169962
Loss made of: CE 0.2450140416622162, LKD 2.8418891429901123, LDE 11.553772926330566, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=15.53652476966381
Loss made of: CE 0.30929872393608093, LKD 2.849480390548706, LDE 11.316259384155273, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3212738037109375, Reg Loss=16.471569061279297
Clinet index 0, End of Epoch 4/6, Average Loss=16.792842864990234, Class Loss=0.3212738037109375, Reg Loss=16.471569061279297
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/52, Loss=16.07391445338726
Loss made of: CE 0.3678753077983856, LKD 3.1137213706970215, LDE 12.069988250732422, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/52, Loss=15.529461821913719
Loss made of: CE 0.21656560897827148, LKD 3.2871716022491455, LDE 12.770936012268066, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=15.548615035414695
Loss made of: CE 0.30826428532600403, LKD 2.933255195617676, LDE 10.992159843444824, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=15.2428078815341
Loss made of: CE 0.29937922954559326, LKD 3.3724207878112793, LDE 12.484357833862305, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=15.04637249559164
Loss made of: CE 0.23184262216091156, LKD 3.3122777938842773, LDE 10.492752075195312, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2981731593608856, Reg Loss=15.352901458740234
Clinet index 0, End of Epoch 5/6, Average Loss=15.651074409484863, Class Loss=0.2981731593608856, Reg Loss=15.352901458740234
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/52, Loss=14.26080616414547
Loss made of: CE 0.25225576758384705, LKD 3.262277603149414, LDE 9.520894050598145, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=15.538181361556052
Loss made of: CE 0.21756665408611298, LKD 2.967089891433716, LDE 10.553964614868164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/52, Loss=15.902902701497078
Loss made of: CE 0.2953159213066101, LKD 2.6145691871643066, LDE 11.898460388183594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=14.650626347959042
Loss made of: CE 0.3589809834957123, LKD 2.2281675338745117, LDE 12.021856307983398, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=15.018720923364162
Loss made of: CE 0.2741779088973999, LKD 2.318455219268799, LDE 11.934876441955566, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.28868260979652405, Reg Loss=14.73873519897461
Clinet index 0, End of Epoch 6/6, Average Loss=15.02741813659668, Class Loss=0.28868260979652405, Reg Loss=14.73873519897461
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/40, Loss=41.896721839904785
Loss made of: CE 1.2167789936065674, LKD 4.5468220710754395, LDE 29.836851119995117, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/40, Loss=32.10355494618416
Loss made of: CE 0.6918167471885681, LKD 4.057610988616943, LDE 24.10418701171875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/40, Loss=28.047480136156082
Loss made of: CE 0.5277512073516846, LKD 3.9445955753326416, LDE 23.786901473999023, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/40, Loss=25.121766617894174
Loss made of: CE 0.48740801215171814, LKD 4.720437049865723, LDE 20.76795768737793, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.8263476490974426, Reg Loss=30.966033935546875
Clinet index 12, End of Epoch 1/6, Average Loss=31.792381286621094, Class Loss=0.8263476490974426, Reg Loss=30.966033935546875
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/40, Loss=24.79758741259575
Loss made of: CE 0.2707007825374603, LKD 3.8844220638275146, LDE 19.53129005432129, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/40, Loss=23.475723358988763
Loss made of: CE 0.2956957221031189, LKD 3.8680949211120605, LDE 16.62074851989746, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/40, Loss=22.60922605097294
Loss made of: CE 0.39286720752716064, LKD 2.786369562149048, LDE 18.542449951171875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/40, Loss=22.31575537323952
Loss made of: CE 0.31507670879364014, LKD 4.43269681930542, LDE 19.111494064331055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3578142821788788, Reg Loss=22.94175910949707
Clinet index 12, End of Epoch 2/6, Average Loss=23.29957389831543, Class Loss=0.3578142821788788, Reg Loss=22.94175910949707
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/40, Loss=21.747622513771056
Loss made of: CE 0.22275976836681366, LKD 3.80787992477417, LDE 16.127002716064453, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/40, Loss=20.343709793686866
Loss made of: CE 0.36494287848472595, LKD 3.7748050689697266, LDE 16.290699005126953, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/40, Loss=20.41889587342739
Loss made of: CE 0.2395446002483368, LKD 3.267183542251587, LDE 16.09869956970215, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/40, Loss=19.986528858542442
Loss made of: CE 0.2884754538536072, LKD 3.8448646068573, LDE 15.98542308807373, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.30504241585731506, Reg Loss=20.31914710998535
Clinet index 12, End of Epoch 3/6, Average Loss=20.624189376831055, Class Loss=0.30504241585731506, Reg Loss=20.31914710998535
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/40, Loss=18.36352137029171
Loss made of: CE 0.32181620597839355, LKD 2.8808538913726807, LDE 13.617790222167969, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/40, Loss=20.236832131445407
Loss made of: CE 0.2207680195569992, LKD 3.6344997882843018, LDE 14.78106689453125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/40, Loss=18.49283841252327
Loss made of: CE 0.2634086608886719, LKD 2.726282835006714, LDE 15.17501163482666, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/40, Loss=18.99133893698454
Loss made of: CE 0.2725134491920471, LKD 4.211094856262207, LDE 16.178836822509766, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.28563642501831055, Reg Loss=18.735496520996094
Clinet index 12, End of Epoch 4/6, Average Loss=19.021133422851562, Class Loss=0.28563642501831055, Reg Loss=18.735496520996094
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/40, Loss=18.865142542123795
Loss made of: CE 0.2946661114692688, LKD 3.448274612426758, LDE 15.921494483947754, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/40, Loss=17.88751118183136
Loss made of: CE 0.33056676387786865, LKD 4.185089588165283, LDE 18.929218292236328, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/40, Loss=18.238823720812796
Loss made of: CE 0.19152991473674774, LKD 3.2900850772857666, LDE 12.575697898864746, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/40, Loss=17.870335634052754
Loss made of: CE 0.259381502866745, LKD 3.2561254501342773, LDE 12.693995475769043, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.25464507937431335, Reg Loss=17.96080780029297
Clinet index 12, End of Epoch 5/6, Average Loss=18.215452194213867, Class Loss=0.25464507937431335, Reg Loss=17.96080780029297
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/40, Loss=17.758953312039374
Loss made of: CE 0.1862359493970871, LKD 4.401595592498779, LDE 12.772181510925293, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/40, Loss=17.46426467001438
Loss made of: CE 0.2552183270454407, LKD 3.6503169536590576, LDE 14.823534965515137, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/40, Loss=17.79190934151411
Loss made of: CE 0.2198648899793625, LKD 3.2793502807617188, LDE 15.265122413635254, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/40, Loss=17.794136880338193
Loss made of: CE 0.17889909446239471, LKD 2.9165024757385254, LDE 12.795790672302246, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.24743954837322235, Reg Loss=17.454877853393555
Clinet index 12, End of Epoch 6/6, Average Loss=17.70231819152832, Class Loss=0.24743954837322235, Reg Loss=17.454877853393555
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/34, Loss=42.945084917545316
Loss made of: CE 2.30112361907959, LKD 4.597792148590088, LDE 27.859050750732422, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=28.211010110378265
Loss made of: CE 0.6931108236312866, LKD 3.647930145263672, LDE 19.934635162353516, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=27.52663958668709
Loss made of: CE 0.6426438093185425, LKD 3.963400363922119, LDE 18.72547149658203, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.2308900356292725, Reg Loss=30.611316680908203
Clinet index 6, End of Epoch 1/6, Average Loss=31.842206954956055, Class Loss=1.2308900356292725, Reg Loss=30.611316680908203
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/34, Loss=22.955008909106255
Loss made of: CE 0.3443060517311096, LKD 3.9838855266571045, LDE 17.943132400512695, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=21.862245854735374
Loss made of: CE 0.40068596601486206, LKD 4.102949142456055, LDE 16.292957305908203, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=22.174659153819086
Loss made of: CE 0.32938796281814575, LKD 5.093031883239746, LDE 19.480262756347656, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3596818745136261, Reg Loss=21.701414108276367
Clinet index 6, End of Epoch 2/6, Average Loss=22.06109619140625, Class Loss=0.3596818745136261, Reg Loss=21.701414108276367
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/34, Loss=19.684298819303514
Loss made of: CE 0.3736690878868103, LKD 4.3432087898254395, LDE 15.265117645263672, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=19.7929625838995
Loss made of: CE 0.3132108449935913, LKD 3.3345773220062256, LDE 14.220322608947754, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=20.336498560011385
Loss made of: CE 0.35831183195114136, LKD 3.9375059604644775, LDE 14.822763442993164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3072677254676819, Reg Loss=19.67629623413086
Clinet index 6, End of Epoch 3/6, Average Loss=19.983564376831055, Class Loss=0.3072677254676819, Reg Loss=19.67629623413086
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/34, Loss=19.31294089704752
Loss made of: CE 0.25912415981292725, LKD 2.5820772647857666, LDE 15.762630462646484, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=18.146488946676254
Loss made of: CE 0.28952205181121826, LKD 2.9113168716430664, LDE 13.155603408813477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=18.942754560708998
Loss made of: CE 0.41657865047454834, LKD 3.740518808364868, LDE 17.10111427307129, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2912299633026123, Reg Loss=18.314373016357422
Clinet index 6, End of Epoch 4/6, Average Loss=18.605602264404297, Class Loss=0.2912299633026123, Reg Loss=18.314373016357422
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/34, Loss=17.08757403790951
Loss made of: CE 0.194431334733963, LKD 3.0860884189605713, LDE 11.627199172973633, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=17.764134702086448
Loss made of: CE 0.26816844940185547, LKD 4.331209659576416, LDE 17.938011169433594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=16.180417639017104
Loss made of: CE 0.22213861346244812, LKD 4.003562927246094, LDE 11.127257347106934, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2754102349281311, Reg Loss=17.010848999023438
Clinet index 6, End of Epoch 5/6, Average Loss=17.286258697509766, Class Loss=0.2754102349281311, Reg Loss=17.010848999023438
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/34, Loss=17.574471689760685
Loss made of: CE 0.3662285804748535, LKD 3.045361280441284, LDE 13.291439056396484, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=16.78306273072958
Loss made of: CE 0.261885404586792, LKD 4.230439186096191, LDE 18.862951278686523, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=16.53615070730448
Loss made of: CE 0.18398796021938324, LKD 3.4083974361419678, LDE 11.28182601928711, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2651020884513855, Reg Loss=16.639286041259766
Clinet index 6, End of Epoch 6/6, Average Loss=16.904388427734375, Class Loss=0.2651020884513855, Reg Loss=16.639286041259766
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/33, Loss=37.69526906013489
Loss made of: CE 1.4415297508239746, LKD 2.0803472995758057, LDE 23.942874908447266, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/33, Loss=25.31173231601715
Loss made of: CE 1.1862508058547974, LKD 2.813032865524292, LDE 20.61075210571289, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/33, Loss=21.599357855319976
Loss made of: CE 0.6883087158203125, LKD 2.1569247245788574, LDE 16.639421463012695, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.0802642107009888, Reg Loss=26.379730224609375
Clinet index 10, End of Epoch 1/6, Average Loss=27.45999526977539, Class Loss=1.0802642107009888, Reg Loss=26.379730224609375
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/33, Loss=19.203539848327637
Loss made of: CE 0.5924831628799438, LKD 3.9501748085021973, LDE 15.323959350585938, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/33, Loss=18.824108511209488
Loss made of: CE 0.7853748798370361, LKD 3.668386459350586, LDE 18.238292694091797, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/33, Loss=17.31773459613323
Loss made of: CE 0.3271048069000244, LKD 1.997118592262268, LDE 14.217339515686035, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5114962458610535, Reg Loss=17.635265350341797
Clinet index 10, End of Epoch 2/6, Average Loss=18.146760940551758, Class Loss=0.5114962458610535, Reg Loss=17.635265350341797
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/33, Loss=16.422714799642563
Loss made of: CE 0.40936094522476196, LKD 2.8323750495910645, LDE 12.93930435180664, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/33, Loss=15.925016847252845
Loss made of: CE 0.3636781573295593, LKD 3.6761722564697266, LDE 13.231725692749023, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/33, Loss=15.410291346907616
Loss made of: CE 0.256814569234848, LKD 1.5242213010787964, LDE 11.777791023254395, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3593786954879761, Reg Loss=15.505976676940918
Clinet index 10, End of Epoch 3/6, Average Loss=15.865355491638184, Class Loss=0.3593786954879761, Reg Loss=15.505976676940918
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/33, Loss=14.435007202625275
Loss made of: CE 0.2783347964286804, LKD 2.217972755432129, LDE 10.655665397644043, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/33, Loss=13.991524866223335
Loss made of: CE 0.3077244758605957, LKD 2.1814732551574707, LDE 10.47482681274414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/33, Loss=14.204182553291322
Loss made of: CE 0.2762471139431, LKD 2.513439893722534, LDE 9.515961647033691, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.31466588377952576, Reg Loss=13.792876243591309
Clinet index 10, End of Epoch 4/6, Average Loss=14.107542037963867, Class Loss=0.31466588377952576, Reg Loss=13.792876243591309
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/33, Loss=13.732648184895515
Loss made of: CE 0.4045659899711609, LKD 4.386183261871338, LDE 10.68458366394043, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/33, Loss=13.912771162390708
Loss made of: CE 0.3760979175567627, LKD 2.831089496612549, LDE 9.785534858703613, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/33, Loss=13.37845940887928
Loss made of: CE 0.28062960505485535, LKD 2.557720184326172, LDE 11.411993026733398, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.30150115489959717, Reg Loss=13.45846939086914
Clinet index 10, End of Epoch 5/6, Average Loss=13.759970664978027, Class Loss=0.30150115489959717, Reg Loss=13.45846939086914
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/33, Loss=13.880646312236786
Loss made of: CE 0.4164615869522095, LKD 3.991746187210083, LDE 11.68828010559082, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/33, Loss=13.093558885157108
Loss made of: CE 0.26930108666419983, LKD 2.616806983947754, LDE 10.084341049194336, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/33, Loss=13.000089554488659
Loss made of: CE 0.26773208379745483, LKD 2.054906129837036, LDE 10.792951583862305, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2878698408603668, Reg Loss=13.111152648925781
Clinet index 10, End of Epoch 6/6, Average Loss=13.399022102355957, Class Loss=0.2878698408603668, Reg Loss=13.111152648925781
federated aggregation...
Validation, Class Loss=0.35823968052864075, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.860090
Mean Acc: 0.408103
FreqW Acc: 0.752667
Mean IoU: 0.354735
Class IoU:
	class 0: 0.8742756
	class 1: 0.53035194
	class 2: 0.12968978
	class 3: 0.019967353
	class 4: 0.5930363
	class 5: 0.0
	class 6: 0.44633564
	class 7: 0.0791448
	class 8: 0.5198125
Class Acc:
	class 0: 0.9899644
	class 1: 0.5320765
	class 2: 0.15644749
	class 3: 0.019967906
	class 4: 0.6229037
	class 5: 0.0
	class 6: 0.48804674
	class 7: 0.083375186
	class 8: 0.78014594

federated global round: 6, step: 1
select part of clients to conduct local training
[11, 5, 8, 12]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/43, Loss=19.544162568449973
Loss made of: CE 0.32210880517959595, LKD 2.065610408782959, LDE 16.930076599121094, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/43, Loss=16.512032768130304
Loss made of: CE 0.5023947954177856, LKD 3.0179803371429443, LDE 11.738670349121094, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 30/43, Loss=15.650267857313157
Loss made of: CE 0.33998221158981323, LKD 1.78360915184021, LDE 13.619271278381348, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/43, Loss=14.949703711271287
Loss made of: CE 0.4223245084285736, LKD 3.404995918273926, LDE 11.654273986816406, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.47405868768692017, Reg Loss=16.04716682434082
Clinet index 11, End of Epoch 1/6, Average Loss=16.521224975585938, Class Loss=0.47405868768692017, Reg Loss=16.04716682434082
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/43, Loss=14.415552115440368
Loss made of: CE 0.26958853006362915, LKD 1.4889581203460693, LDE 11.283561706542969, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/43, Loss=14.384705051779747
Loss made of: CE 0.28361308574676514, LKD 1.8274317979812622, LDE 10.281005859375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/43, Loss=14.366273793578149
Loss made of: CE 0.39109623432159424, LKD 2.6557443141937256, LDE 13.636345863342285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/43, Loss=14.452678608894349
Loss made of: CE 0.3336464762687683, LKD 2.0403072834014893, LDE 11.545478820800781, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3470902740955353, Reg Loss=14.075122833251953
Clinet index 11, End of Epoch 2/6, Average Loss=14.422213554382324, Class Loss=0.3470902740955353, Reg Loss=14.075122833251953
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/43, Loss=13.661896029114724
Loss made of: CE 0.367463082075119, LKD 2.675149917602539, LDE 10.152191162109375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/43, Loss=13.527898946404457
Loss made of: CE 0.27701330184936523, LKD 1.6896318197250366, LDE 11.297216415405273, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/43, Loss=14.096021588146687
Loss made of: CE 0.3310636878013611, LKD 2.197042942047119, LDE 10.368671417236328, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/43, Loss=13.611905565857887
Loss made of: CE 0.24114888906478882, LKD 1.8615309000015259, LDE 12.146655082702637, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.31336709856987, Reg Loss=13.363845825195312
Clinet index 11, End of Epoch 3/6, Average Loss=13.677212715148926, Class Loss=0.31336709856987, Reg Loss=13.363845825195312
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/43, Loss=13.538712333142758
Loss made of: CE 0.2831827402114868, LKD 2.803605556488037, LDE 11.325931549072266, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/43, Loss=12.583355355262757
Loss made of: CE 0.32192713022232056, LKD 2.9364888668060303, LDE 9.886580467224121, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/43, Loss=13.105182917416096
Loss made of: CE 0.3101709485054016, LKD 2.4524717330932617, LDE 9.049930572509766, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/43, Loss=13.757835295796394
Loss made of: CE 0.2648763060569763, LKD 2.0566930770874023, LDE 9.306528091430664, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.28956660628318787, Reg Loss=12.904840469360352
Clinet index 11, End of Epoch 4/6, Average Loss=13.19440746307373, Class Loss=0.28956660628318787, Reg Loss=12.904840469360352
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/43, Loss=13.532107612490654
Loss made of: CE 0.2613998353481293, LKD 2.749279499053955, LDE 11.440728187561035, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/43, Loss=13.088617685437203
Loss made of: CE 0.37630924582481384, LKD 1.9443259239196777, LDE 10.614051818847656, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/43, Loss=13.233162969350815
Loss made of: CE 0.20730285346508026, LKD 1.7309259176254272, LDE 10.035242080688477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/43, Loss=13.10291879326105
Loss made of: CE 0.2594608664512634, LKD 2.123035192489624, LDE 9.502907752990723, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2778722643852234, Reg Loss=13.006037712097168
Clinet index 11, End of Epoch 5/6, Average Loss=13.283909797668457, Class Loss=0.2778722643852234, Reg Loss=13.006037712097168
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/43, Loss=12.612808538973331
Loss made of: CE 0.2471713125705719, LKD 1.8585113286972046, LDE 9.682548522949219, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/43, Loss=12.805562843382358
Loss made of: CE 0.2430580109357834, LKD 2.4532382488250732, LDE 9.642046928405762, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/43, Loss=12.647306396067142
Loss made of: CE 0.23684567213058472, LKD 3.2869019508361816, LDE 8.58954906463623, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/43, Loss=12.19550957530737
Loss made of: CE 0.28860047459602356, LKD 2.320108413696289, LDE 8.902101516723633, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2567782402038574, Reg Loss=12.313528060913086
Clinet index 11, End of Epoch 6/6, Average Loss=12.570306777954102, Class Loss=0.2567782402038574, Reg Loss=12.313528060913086
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/34, Loss=18.591211143136025
Loss made of: CE 0.4213648736476898, LKD 2.8906431198120117, LDE 14.109238624572754, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=17.646722429990767
Loss made of: CE 0.3439285159111023, LKD 3.9849088191986084, LDE 12.797747611999512, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=17.735530024766923
Loss made of: CE 0.3391292989253998, LKD 6.5350422859191895, LDE 18.758222579956055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.36376383900642395, Reg Loss=17.582719802856445
Clinet index 5, End of Epoch 1/6, Average Loss=17.946483612060547, Class Loss=0.36376383900642395, Reg Loss=17.582719802856445
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/34, Loss=18.847161981463433
Loss made of: CE 0.2558130919933319, LKD 3.7838172912597656, LDE 12.911194801330566, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=16.475875014066695
Loss made of: CE 0.25385695695877075, LKD 3.904513359069824, LDE 13.392667770385742, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=18.165200938284396
Loss made of: CE 0.3318590223789215, LKD 4.607211589813232, LDE 14.983468055725098, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.28145915269851685, Reg Loss=17.365388870239258
Clinet index 5, End of Epoch 2/6, Average Loss=17.646848678588867, Class Loss=0.28145915269851685, Reg Loss=17.365388870239258
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/34, Loss=15.982831829786301
Loss made of: CE 0.33450233936309814, LKD 4.392989635467529, LDE 11.886215209960938, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=15.664055080711842
Loss made of: CE 0.22820763289928436, LKD 3.401766300201416, LDE 15.75296401977539, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=16.578103582561017
Loss made of: CE 0.3078303337097168, LKD 3.9279963970184326, LDE 14.021357536315918, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.24917364120483398, Reg Loss=15.876367568969727
Clinet index 5, End of Epoch 3/6, Average Loss=16.12554168701172, Class Loss=0.24917364120483398, Reg Loss=15.876367568969727
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/34, Loss=15.596215605735779
Loss made of: CE 0.2035546749830246, LKD 3.378354072570801, LDE 11.362550735473633, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=15.897974772751331
Loss made of: CE 0.23862001299858093, LKD 4.027898788452148, LDE 12.237479209899902, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=16.33169697970152
Loss made of: CE 0.23116378486156464, LKD 3.800849199295044, LDE 10.399313926696777, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.24713681638240814, Reg Loss=15.672548294067383
Clinet index 5, End of Epoch 4/6, Average Loss=15.919685363769531, Class Loss=0.24713681638240814, Reg Loss=15.672548294067383
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/34, Loss=15.752959287166595
Loss made of: CE 0.21538394689559937, LKD 4.4016828536987305, LDE 11.784907341003418, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=15.14309067428112
Loss made of: CE 0.2802945375442505, LKD 3.4975197315216064, LDE 11.782475471496582, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=15.761017520725726
Loss made of: CE 0.26205700635910034, LKD 4.197915554046631, LDE 12.352890014648438, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2376062124967575, Reg Loss=15.249871253967285
Clinet index 5, End of Epoch 5/6, Average Loss=15.48747730255127, Class Loss=0.2376062124967575, Reg Loss=15.249871253967285
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/34, Loss=15.95061203688383
Loss made of: CE 0.3261820077896118, LKD 3.7888009548187256, LDE 11.895992279052734, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=15.362746059894562
Loss made of: CE 0.20974402129650116, LKD 3.595029592514038, LDE 13.538472175598145, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=15.622558729350567
Loss made of: CE 0.28417131304740906, LKD 4.933960914611816, LDE 15.270975112915039, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.24100884795188904, Reg Loss=15.390921592712402
Clinet index 5, End of Epoch 6/6, Average Loss=15.631930351257324, Class Loss=0.24100884795188904, Reg Loss=15.390921592712402
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/40, Loss=20.924272346496583
Loss made of: CE 0.4281976819038391, LKD 3.375739574432373, LDE 16.72116470336914, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/40, Loss=20.033502754569053
Loss made of: CE 0.28553318977355957, LKD 3.8136329650878906, LDE 14.699177742004395, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 30/40, Loss=19.905271008610725
Loss made of: CE 0.36347126960754395, LKD 2.783949375152588, LDE 15.639719009399414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/40, Loss=19.336392575502394
Loss made of: CE 0.3379668593406677, LKD 3.427720069885254, LDE 13.28549861907959, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.37305939197540283, Reg Loss=19.676801681518555
Clinet index 8, End of Epoch 1/6, Average Loss=20.049861907958984, Class Loss=0.37305939197540283, Reg Loss=19.676801681518555
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/40, Loss=18.30611258596182
Loss made of: CE 0.276819109916687, LKD 3.4187004566192627, LDE 13.045625686645508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/40, Loss=17.597175553441048
Loss made of: CE 0.24812190234661102, LKD 3.628629446029663, LDE 12.24257755279541, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/40, Loss=18.60784756541252
Loss made of: CE 0.402204692363739, LKD 3.6694672107696533, LDE 13.202373504638672, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/40, Loss=18.84367699623108
Loss made of: CE 0.22161734104156494, LKD 2.724248170852661, LDE 13.3530855178833, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3036626875400543, Reg Loss=18.0350399017334
Clinet index 8, End of Epoch 2/6, Average Loss=18.338703155517578, Class Loss=0.3036626875400543, Reg Loss=18.0350399017334
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/40, Loss=17.183736762404443
Loss made of: CE 0.2004070281982422, LKD 2.303882598876953, LDE 12.541006088256836, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/40, Loss=17.175749754905702
Loss made of: CE 0.20446054637432098, LKD 3.2634994983673096, LDE 12.493600845336914, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/40, Loss=17.653439286351205
Loss made of: CE 0.27384909987449646, LKD 5.163749694824219, LDE 13.063285827636719, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/40, Loss=17.408749021589756
Loss made of: CE 0.23377667367458344, LKD 3.0749809741973877, LDE 14.28212833404541, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.26124057173728943, Reg Loss=17.094179153442383
Clinet index 8, End of Epoch 3/6, Average Loss=17.355419158935547, Class Loss=0.26124057173728943, Reg Loss=17.094179153442383
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/40, Loss=16.867963993549346
Loss made of: CE 0.33497652411460876, LKD 3.5051941871643066, LDE 16.492284774780273, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/40, Loss=16.24467367976904
Loss made of: CE 0.2586565613746643, LKD 3.4515976905822754, LDE 13.195225715637207, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/40, Loss=16.785721687972547
Loss made of: CE 0.2279512584209442, LKD 2.145720958709717, LDE 15.455709457397461, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/40, Loss=17.32915560156107
Loss made of: CE 0.2036615014076233, LKD 3.4869492053985596, LDE 11.616899490356445, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.24957989156246185, Reg Loss=16.55729866027832
Clinet index 8, End of Epoch 4/6, Average Loss=16.8068790435791, Class Loss=0.24957989156246185, Reg Loss=16.55729866027832
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/40, Loss=15.562961579859257
Loss made of: CE 0.16096988320350647, LKD 2.572253704071045, LDE 11.558125495910645, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/40, Loss=17.114827558398247
Loss made of: CE 0.33058878779411316, LKD 2.454349994659424, LDE 13.402933120727539, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/40, Loss=16.581519941985608
Loss made of: CE 0.2314205765724182, LKD 3.642930746078491, LDE 12.496551513671875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/40, Loss=15.995906339585781
Loss made of: CE 0.19462701678276062, LKD 3.1951518058776855, LDE 13.336624145507812, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.23673272132873535, Reg Loss=16.077070236206055
Clinet index 8, End of Epoch 5/6, Average Loss=16.31380271911621, Class Loss=0.23673272132873535, Reg Loss=16.077070236206055
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/40, Loss=15.105232624709606
Loss made of: CE 0.22726736962795258, LKD 2.8976306915283203, LDE 11.202522277832031, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/40, Loss=15.286824035644532
Loss made of: CE 0.1920185089111328, LKD 3.4225404262542725, LDE 10.922829627990723, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/40, Loss=16.072666281461714
Loss made of: CE 0.23537659645080566, LKD 3.218451976776123, LDE 11.861666679382324, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/40, Loss=15.662488308548927
Loss made of: CE 0.21036523580551147, LKD 4.1837358474731445, LDE 10.519649505615234, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.22787891328334808, Reg Loss=15.303924560546875
Clinet index 8, End of Epoch 6/6, Average Loss=15.531803131103516, Class Loss=0.22787891328334808, Reg Loss=15.303924560546875
Current Client Index:  12
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/40, Loss=20.956684413552285
Loss made of: CE 0.33393973112106323, LKD 3.268651247024536, LDE 16.56191062927246, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/40, Loss=18.921510443091393
Loss made of: CE 0.2944366931915283, LKD 2.9006874561309814, LDE 14.367998123168945, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/40, Loss=18.6917540833354
Loss made of: CE 0.44004201889038086, LKD 3.475912570953369, LDE 15.764719009399414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/40, Loss=17.38146216869354
Loss made of: CE 0.3708055019378662, LKD 4.710355281829834, LDE 13.735361099243164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.37953993678092957, Reg Loss=18.608312606811523
Clinet index 12, End of Epoch 1/6, Average Loss=18.987852096557617, Class Loss=0.37953993678092957, Reg Loss=18.608312606811523
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/40, Loss=18.66517301350832
Loss made of: CE 0.23219387233257294, LKD 4.141724109649658, LDE 14.223784446716309, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/40, Loss=17.78822667300701
Loss made of: CE 0.22647690773010254, LKD 3.2743077278137207, LDE 12.037761688232422, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/40, Loss=18.177975779771806
Loss made of: CE 0.36292576789855957, LKD 2.66241192817688, LDE 14.305838584899902, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/40, Loss=17.404558701813222
Loss made of: CE 0.2703314423561096, LKD 3.8908190727233887, LDE 15.050225257873535, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3048109710216522, Reg Loss=17.704174041748047
Clinet index 12, End of Epoch 2/6, Average Loss=18.00898551940918, Class Loss=0.3048109710216522, Reg Loss=17.704174041748047
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/40, Loss=17.8011377453804
Loss made of: CE 0.19810643792152405, LKD 3.774294376373291, LDE 12.548301696777344, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/40, Loss=17.027815717458726
Loss made of: CE 0.3234536647796631, LKD 3.467998743057251, LDE 13.049880027770996, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/40, Loss=16.780191980302334
Loss made of: CE 0.24997949600219727, LKD 3.337851047515869, LDE 12.580961227416992, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/40, Loss=16.800555126368998
Loss made of: CE 0.2837677597999573, LKD 3.391918659210205, LDE 12.669149398803711, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.27975818514823914, Reg Loss=16.822668075561523
Clinet index 12, End of Epoch 3/6, Average Loss=17.102426528930664, Class Loss=0.27975818514823914, Reg Loss=16.822668075561523
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/40, Loss=15.987308830022812
Loss made of: CE 0.2544616162776947, LKD 3.195777416229248, LDE 11.679025650024414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/40, Loss=16.536411315202713
Loss made of: CE 0.20225587487220764, LKD 3.2105963230133057, LDE 12.709715843200684, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/40, Loss=15.783766900002956
Loss made of: CE 0.2373807728290558, LKD 2.5642449855804443, LDE 12.311869621276855, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/40, Loss=16.726208421587945
Loss made of: CE 0.28965479135513306, LKD 3.888688564300537, LDE 13.467065811157227, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2582566440105438, Reg Loss=16.000167846679688
Clinet index 12, End of Epoch 4/6, Average Loss=16.258424758911133, Class Loss=0.2582566440105438, Reg Loss=16.000167846679688
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/40, Loss=16.725810860097408
Loss made of: CE 0.2786219120025635, LKD 3.397881269454956, LDE 14.217599868774414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/40, Loss=15.462406322360039
Loss made of: CE 0.2552904486656189, LKD 3.9618520736694336, LDE 16.165864944458008, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/40, Loss=16.10363215357065
Loss made of: CE 0.17698000371456146, LKD 3.3895232677459717, LDE 11.171656608581543, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/40, Loss=15.492374075949192
Loss made of: CE 0.2554365396499634, LKD 3.3698863983154297, LDE 11.034131050109863, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.23537681996822357, Reg Loss=15.71068000793457
Clinet index 12, End of Epoch 5/6, Average Loss=15.946056365966797, Class Loss=0.23537681996822357, Reg Loss=15.71068000793457
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/40, Loss=15.544834688305855
Loss made of: CE 0.1872851848602295, LKD 4.067362308502197, LDE 10.305331230163574, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/40, Loss=15.419174084067345
Loss made of: CE 0.23177127540111542, LKD 3.63887095451355, LDE 12.852118492126465, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/40, Loss=15.874203789234162
Loss made of: CE 0.23427948355674744, LKD 3.132981538772583, LDE 12.892884254455566, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/40, Loss=15.343821996450425
Loss made of: CE 0.1708802431821823, LKD 2.627981185913086, LDE 11.305944442749023, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2399723082780838, Reg Loss=15.305536270141602
Clinet index 12, End of Epoch 6/6, Average Loss=15.54550838470459, Class Loss=0.2399723082780838, Reg Loss=15.305536270141602
federated aggregation...
Validation, Class Loss=0.2828895151615143, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.886033
Mean Acc: 0.508635
FreqW Acc: 0.797709
Mean IoU: 0.446086
Class IoU:
	class 0: 0.8993125
	class 1: 0.5422375
	class 2: 0.13883425
	class 3: 0.03511251
	class 4: 0.6412347
	class 5: 0.28548086
	class 6: 0.63632834
	class 7: 0.2366305
	class 8: 0.599606
Class Acc:
	class 0: 0.9862248
	class 1: 0.54428095
	class 2: 0.16954061
	class 3: 0.035127196
	class 4: 0.6883365
	class 5: 0.3431829
	class 6: 0.66730225
	class 7: 0.24669695
	class 8: 0.8970229

federated global round: 7, step: 1
select part of clients to conduct local training
[0, 6, 13, 5]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/52, Loss=14.904887619614602
Loss made of: CE 0.5556467771530151, LKD 3.54995059967041, LDE 10.26073932647705, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/52, Loss=14.160531917214394
Loss made of: CE 0.3453773856163025, LKD 3.9513514041900635, LDE 9.937129974365234, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=13.785946527123452
Loss made of: CE 0.31679514050483704, LKD 2.965423345565796, LDE 9.923124313354492, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=14.288640339672565
Loss made of: CE 0.30331581830978394, LKD 2.541841983795166, LDE 13.195830345153809, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/52, Loss=14.391649794578552
Loss made of: CE 0.20135921239852905, LKD 2.2600722312927246, LDE 10.64231014251709, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3506321907043457, Reg Loss=13.896539688110352
Clinet index 0, End of Epoch 1/6, Average Loss=14.247171401977539, Class Loss=0.3506321907043457, Reg Loss=13.896539688110352
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/52, Loss=14.535213640332222
Loss made of: CE 0.2730279862880707, LKD 3.1902544498443604, LDE 10.142441749572754, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=14.099359214305878
Loss made of: CE 0.26408252120018005, LKD 3.274916172027588, LDE 9.679841041564941, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=14.987472587823868
Loss made of: CE 0.3984094560146332, LKD 2.7672088146209717, LDE 12.660053253173828, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/52, Loss=15.011166703701019
Loss made of: CE 0.33545026183128357, LKD 4.134307384490967, LDE 11.142313003540039, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=14.753984978795051
Loss made of: CE 0.2778472304344177, LKD 3.768042802810669, LDE 13.886502265930176, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2806764543056488, Reg Loss=14.362678527832031
Clinet index 0, End of Epoch 2/6, Average Loss=14.643355369567871, Class Loss=0.2806764543056488, Reg Loss=14.362678527832031
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/52, Loss=14.626164972782135
Loss made of: CE 0.21776454150676727, LKD 3.3277299404144287, LDE 11.14018440246582, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=15.308643801510334
Loss made of: CE 0.32291513681411743, LKD 3.6544535160064697, LDE 11.77702522277832, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=14.840826562047004
Loss made of: CE 0.39694279432296753, LKD 4.6920905113220215, LDE 10.379905700683594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=14.39370705485344
Loss made of: CE 0.28712281584739685, LKD 2.330371856689453, LDE 12.07833480834961, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/52, Loss=14.37671255916357
Loss made of: CE 0.26575836539268494, LKD 3.510530948638916, LDE 10.15829086303711, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2749030590057373, Reg Loss=14.389971733093262
Clinet index 0, End of Epoch 3/6, Average Loss=14.664875030517578, Class Loss=0.2749030590057373, Reg Loss=14.389971733093262
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/52, Loss=15.051725107431412
Loss made of: CE 0.35539430379867554, LKD 3.5947999954223633, LDE 10.402016639709473, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=13.235595862567425
Loss made of: CE 0.18448296189308167, LKD 3.522916078567505, LDE 9.363128662109375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=13.618851006031036
Loss made of: CE 0.2349238097667694, LKD 3.121551036834717, LDE 9.647772789001465, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=13.974343289434909
Loss made of: CE 0.23882520198822021, LKD 2.3544719219207764, LDE 9.144821166992188, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=12.892740136384964
Loss made of: CE 0.2427537441253662, LKD 3.2864599227905273, LDE 8.946191787719727, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2537206709384918, Reg Loss=13.432384490966797
Clinet index 0, End of Epoch 4/6, Average Loss=13.686104774475098, Class Loss=0.2537206709384918, Reg Loss=13.432384490966797
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/52, Loss=13.236823593080043
Loss made of: CE 0.3031874895095825, LKD 3.2718114852905273, LDE 9.866866111755371, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/52, Loss=13.062276467680931
Loss made of: CE 0.17389753460884094, LKD 3.211355209350586, LDE 10.085762977600098, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=13.315309454500674
Loss made of: CE 0.24784588813781738, LKD 2.8947434425354004, LDE 8.621277809143066, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=13.263916684687137
Loss made of: CE 0.2789055109024048, LKD 3.3316731452941895, LDE 10.431305885314941, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=13.280099005997181
Loss made of: CE 0.2303857058286667, LKD 3.2592556476593018, LDE 8.527543067932129, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.24776692688465118, Reg Loss=13.161195755004883
Clinet index 0, End of Epoch 5/6, Average Loss=13.40896224975586, Class Loss=0.24776692688465118, Reg Loss=13.161195755004883
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/52, Loss=12.288823208212852
Loss made of: CE 0.21808558702468872, LKD 3.292893886566162, LDE 8.197792053222656, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=13.848002673685551
Loss made of: CE 0.18871456384658813, LKD 3.09521746635437, LDE 8.758039474487305, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 30/52, Loss=14.19670920073986
Loss made of: CE 0.2519409954547882, LKD 2.6162538528442383, LDE 11.022224426269531, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=12.948155218362809
Loss made of: CE 0.27619999647140503, LKD 2.109722852706909, LDE 11.029829978942871, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=13.409610986709595
Loss made of: CE 0.22719436883926392, LKD 2.2749743461608887, LDE 11.340352058410645, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2491498440504074, Reg Loss=13.068926811218262
Clinet index 0, End of Epoch 6/6, Average Loss=13.318077087402344, Class Loss=0.2491498440504074, Reg Loss=13.068926811218262
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/34, Loss=17.370035588741302
Loss made of: CE 0.4281129837036133, LKD 3.0258944034576416, LDE 11.796285629272461, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=14.807933737337589
Loss made of: CE 0.2128707617521286, LKD 3.2426021099090576, LDE 10.303194999694824, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=16.760903564095496
Loss made of: CE 0.2981190085411072, LKD 3.3869175910949707, LDE 10.970674514770508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3392299711704254, Reg Loss=15.873188972473145
Clinet index 6, End of Epoch 1/6, Average Loss=16.212419509887695, Class Loss=0.3392299711704254, Reg Loss=15.873188972473145
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/34, Loss=15.164722789824008
Loss made of: CE 0.24813401699066162, LKD 3.7521579265594482, LDE 11.232224464416504, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=14.623335389792919
Loss made of: CE 0.2669639587402344, LKD 3.7466652393341064, LDE 10.989009857177734, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=15.818618139624595
Loss made of: CE 0.2100881040096283, LKD 4.613424301147461, LDE 15.130892753601074, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.24615825712680817, Reg Loss=14.835549354553223
Clinet index 6, End of Epoch 2/6, Average Loss=15.081707954406738, Class Loss=0.24615825712680817, Reg Loss=14.835549354553223
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/34, Loss=14.468491110205651
Loss made of: CE 0.24877409636974335, LKD 3.8707163333892822, LDE 10.555397033691406, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=14.909702925384044
Loss made of: CE 0.2035040557384491, LKD 3.0558888912200928, LDE 10.378257751464844, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=15.39269887804985
Loss made of: CE 0.2908100485801697, LKD 3.463282346725464, LDE 10.676070213317871, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2413245439529419, Reg Loss=14.780506134033203
Clinet index 6, End of Epoch 3/6, Average Loss=15.021830558776855, Class Loss=0.2413245439529419, Reg Loss=14.780506134033203
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/34, Loss=14.842688289284705
Loss made of: CE 0.19024460017681122, LKD 3.278095245361328, LDE 11.331474304199219, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=14.032991491258144
Loss made of: CE 0.2371005266904831, LKD 2.5645759105682373, LDE 10.865567207336426, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=14.921440415084362
Loss made of: CE 0.35590624809265137, LKD 3.708148717880249, LDE 12.72400951385498, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2353363335132599, Reg Loss=14.266908645629883
Clinet index 6, End of Epoch 4/6, Average Loss=14.50224494934082, Class Loss=0.2353363335132599, Reg Loss=14.266908645629883
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/34, Loss=14.01850448846817
Loss made of: CE 0.16747228801250458, LKD 2.944282293319702, LDE 8.705070495605469, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=14.636536096036433
Loss made of: CE 0.25032657384872437, LKD 4.650548934936523, LDE 15.657499313354492, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=13.120284317433834
Loss made of: CE 0.20889727771282196, LKD 4.064518928527832, LDE 8.947502136230469, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.23351407051086426, Reg Loss=13.849820137023926
Clinet index 6, End of Epoch 5/6, Average Loss=14.083333969116211, Class Loss=0.23351407051086426, Reg Loss=13.849820137023926
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/34, Loss=14.120983362197876
Loss made of: CE 0.27908825874328613, LKD 2.5641229152679443, LDE 9.754986763000488, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=13.730713512003422
Loss made of: CE 0.2556554973125458, LKD 4.090619087219238, LDE 14.72325611114502, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=13.916316774487495
Loss made of: CE 0.17508769035339355, LKD 3.6116085052490234, LDE 9.021231651306152, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.229452982544899, Reg Loss=13.707261085510254
Clinet index 6, End of Epoch 6/6, Average Loss=13.936714172363281, Class Loss=0.229452982544899, Reg Loss=13.707261085510254
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=26.43016527593136
Loss made of: CE 0.5194425582885742, LKD 3.6368448734283447, LDE 15.716436386108398, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/33, Loss=18.82053516805172
Loss made of: CE 0.5050449967384338, LKD 2.050276756286621, LDE 14.259708404541016, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/33, Loss=16.05026952624321
Loss made of: CE 0.3177349865436554, LKD 2.9162440299987793, LDE 11.735912322998047, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5351231694221497, Reg Loss=19.492759704589844
Clinet index 13, End of Epoch 1/6, Average Loss=20.027883529663086, Class Loss=0.5351231694221497, Reg Loss=19.492759704589844
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/33, Loss=14.394985231757165
Loss made of: CE 0.36437326669692993, LKD 3.265815258026123, LDE 10.830272674560547, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/33, Loss=13.9837592035532
Loss made of: CE 0.3374558389186859, LKD 2.534940719604492, LDE 10.724361419677734, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 2, Batch 30/33, Loss=13.60827687382698
Loss made of: CE 0.42242884635925293, LKD 3.2563648223876953, LDE 11.607675552368164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.32075098156929016, Reg Loss=13.617475509643555
Clinet index 13, End of Epoch 2/6, Average Loss=13.938226699829102, Class Loss=0.32075098156929016, Reg Loss=13.617475509643555
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/33, Loss=13.001632498204708
Loss made of: CE 0.269815057516098, LKD 2.38688588142395, LDE 12.217952728271484, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/33, Loss=13.602383798360824
Loss made of: CE 0.23276689648628235, LKD 3.1254687309265137, LDE 12.551823616027832, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/33, Loss=13.05211537629366
Loss made of: CE 0.23057615756988525, LKD 2.100456953048706, LDE 9.2901611328125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.28330352902412415, Reg Loss=12.99797248840332
Clinet index 13, End of Epoch 3/6, Average Loss=13.281275749206543, Class Loss=0.28330352902412415, Reg Loss=12.99797248840332
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/33, Loss=13.015502679347993
Loss made of: CE 0.22647011280059814, LKD 1.7479549646377563, LDE 8.448265075683594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/33, Loss=12.40447686612606
Loss made of: CE 0.253553569316864, LKD 2.450143337249756, LDE 10.346306800842285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/33, Loss=12.349153588712216
Loss made of: CE 0.26738059520721436, LKD 2.288938045501709, LDE 8.8501558303833, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.26086318492889404, Reg Loss=12.234132766723633
Clinet index 13, End of Epoch 4/6, Average Loss=12.494996070861816, Class Loss=0.26086318492889404, Reg Loss=12.234132766723633
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 5, Batch 10/33, Loss=12.567531624436379
Loss made of: CE 0.2908509373664856, LKD 3.2611560821533203, LDE 8.3372802734375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/33, Loss=13.00879888832569
Loss made of: CE 0.23527351021766663, LKD 2.440971612930298, LDE 9.315767288208008, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/33, Loss=12.501793168485165
Loss made of: CE 0.19624736905097961, LKD 1.7627995014190674, LDE 9.184627532958984, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.26030465960502625, Reg Loss=12.332853317260742
Clinet index 13, End of Epoch 5/6, Average Loss=12.593157768249512, Class Loss=0.26030465960502625, Reg Loss=12.332853317260742
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/33, Loss=12.530176554620265
Loss made of: CE 0.2547823190689087, LKD 3.0100207328796387, LDE 9.18592357635498, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/33, Loss=12.390365190804005
Loss made of: CE 0.2103467434644699, LKD 2.771866798400879, LDE 9.136396408081055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/33, Loss=12.001396790146828
Loss made of: CE 0.22664472460746765, LKD 1.892141580581665, LDE 8.736960411071777, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.25839245319366455, Reg Loss=11.925447463989258
Clinet index 13, End of Epoch 6/6, Average Loss=12.183839797973633, Class Loss=0.25839245319366455, Reg Loss=11.925447463989258
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/34, Loss=17.839124128222466
Loss made of: CE 0.4174439609050751, LKD 3.123563051223755, LDE 13.456583023071289, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=15.825393936038017
Loss made of: CE 0.3246464729309082, LKD 3.9448599815368652, LDE 10.733121871948242, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=15.771069170534611
Loss made of: CE 0.2880713939666748, LKD 5.824334144592285, LDE 16.243345260620117, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.335286021232605, Reg Loss=16.051820755004883
Clinet index 5, End of Epoch 1/6, Average Loss=16.38710594177246, Class Loss=0.335286021232605, Reg Loss=16.051820755004883
Pseudo labeling is: None
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/34, Loss=16.38621980845928
Loss made of: CE 0.22445857524871826, LKD 3.457991361618042, LDE 11.079718589782715, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=14.31954234391451
Loss made of: CE 0.24487082660198212, LKD 3.434345245361328, LDE 10.481188774108887, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=15.967435099184513
Loss made of: CE 0.28364133834838867, LKD 4.598163604736328, LDE 13.542762756347656, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.26256054639816284, Reg Loss=15.125357627868652
Clinet index 5, End of Epoch 2/6, Average Loss=15.387918472290039, Class Loss=0.26256054639816284, Reg Loss=15.125357627868652
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/34, Loss=14.238324519991874
Loss made of: CE 0.293549120426178, LKD 3.8827545642852783, LDE 10.702258110046387, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=14.141278150677682
Loss made of: CE 0.21049313247203827, LKD 3.6407039165496826, LDE 15.472992897033691, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=15.067614097893237
Loss made of: CE 0.2636238932609558, LKD 3.8000576496124268, LDE 13.272744178771973, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.23543930053710938, Reg Loss=14.249247550964355
Clinet index 5, End of Epoch 3/6, Average Loss=14.484686851501465, Class Loss=0.23543930053710938, Reg Loss=14.249247550964355
Pseudo labeling is: None
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/34, Loss=13.801813291013241
Loss made of: CE 0.19548456370830536, LKD 3.328056573867798, LDE 9.902658462524414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=14.082684856653213
Loss made of: CE 0.2408413290977478, LKD 3.6355984210968018, LDE 10.202352523803711, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=14.578530287742614
Loss made of: CE 0.28029441833496094, LKD 3.6788508892059326, LDE 9.277172088623047, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2342025339603424, Reg Loss=13.913073539733887
Clinet index 5, End of Epoch 4/6, Average Loss=14.147275924682617, Class Loss=0.2342025339603424, Reg Loss=13.913073539733887
Pseudo labeling is: None
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/34, Loss=14.045562273263931
Loss made of: CE 0.24101126194000244, LKD 3.8361716270446777, LDE 10.305486679077148, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=13.635258136689663
Loss made of: CE 0.26174458861351013, LKD 3.283968448638916, LDE 9.971064567565918, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=14.333460928499699
Loss made of: CE 0.21772250533103943, LKD 4.084001541137695, LDE 10.597480773925781, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.22821591794490814, Reg Loss=13.750715255737305
Clinet index 5, End of Epoch 5/6, Average Loss=13.978931427001953, Class Loss=0.22821591794490814, Reg Loss=13.750715255737305
Pseudo labeling is: None
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/34, Loss=14.763810905814172
Loss made of: CE 0.28186914324760437, LKD 3.9862916469573975, LDE 10.596232414245605, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=14.081580448150635
Loss made of: CE 0.1926339566707611, LKD 3.210167169570923, LDE 11.292001724243164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=14.217361485958099
Loss made of: CE 0.24133865535259247, LKD 4.534822463989258, LDE 13.436216354370117, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2294241338968277, Reg Loss=14.086769104003906
Clinet index 5, End of Epoch 6/6, Average Loss=14.316193580627441, Class Loss=0.2294241338968277, Reg Loss=14.086769104003906
federated aggregation...
Validation, Class Loss=0.25757479667663574, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.904658
Mean Acc: 0.559457
FreqW Acc: 0.831687
Mean IoU: 0.479675
Class IoU:
	class 0: 0.923305
	class 1: 0.5460686
	class 2: 0.15640925
	class 3: 0.014077043
	class 4: 0.5945676
	class 5: 0.08573003
	class 6: 0.773939
	class 7: 0.6194102
	class 8: 0.60356706
Class Acc:
	class 0: 0.9788681
	class 1: 0.5482839
	class 2: 0.20375134
	class 3: 0.014080853
	class 4: 0.63296896
	class 5: 0.08577061
	class 6: 0.9288868
	class 7: 0.71273327
	class 8: 0.92976683

federated global round: 8, step: 1
select part of clients to conduct local training
[4, 11, 9, 6]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=19.067879709601403
Loss made of: CE 0.44305095076560974, LKD 2.596925735473633, LDE 11.803899765014648, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/25, Loss=15.467002591490745
Loss made of: CE 0.3304592967033386, LKD 2.3231141567230225, LDE 11.077901840209961, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4055027961730957, Reg Loss=16.323745727539062
Clinet index 4, End of Epoch 1/6, Average Loss=16.729248046875, Class Loss=0.4055027961730957, Reg Loss=16.323745727539062
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/25, Loss=14.29737056493759
Loss made of: CE 0.3222961127758026, LKD 1.8227897882461548, LDE 11.293246269226074, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/25, Loss=13.83513733446598
Loss made of: CE 0.3306357264518738, LKD 3.1597740650177, LDE 11.983392715454102, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.29348620772361755, Reg Loss=13.668802261352539
Clinet index 4, End of Epoch 2/6, Average Loss=13.962288856506348, Class Loss=0.29348620772361755, Reg Loss=13.668802261352539
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/25, Loss=13.607020400464535
Loss made of: CE 0.19991877675056458, LKD 1.7924729585647583, LDE 12.721207618713379, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/25, Loss=13.00525219887495
Loss made of: CE 0.2705964744091034, LKD 2.2987728118896484, LDE 9.085691452026367, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.26860615611076355, Reg Loss=13.019350051879883
Clinet index 4, End of Epoch 3/6, Average Loss=13.287956237792969, Class Loss=0.26860615611076355, Reg Loss=13.019350051879883
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/25, Loss=13.18636662811041
Loss made of: CE 0.27498120069503784, LKD 2.166637897491455, LDE 11.878225326538086, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/25, Loss=12.148102314770222
Loss made of: CE 0.24091695249080658, LKD 3.0589890480041504, LDE 12.303509712219238, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2305910587310791, Reg Loss=12.24399471282959
Clinet index 4, End of Epoch 4/6, Average Loss=12.47458553314209, Class Loss=0.2305910587310791, Reg Loss=12.24399471282959
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/25, Loss=12.400271655619145
Loss made of: CE 0.2197655886411667, LKD 2.340869188308716, LDE 9.529298782348633, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/25, Loss=12.83392117023468
Loss made of: CE 0.4075431227684021, LKD 2.6121981143951416, LDE 12.961511611938477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.23778170347213745, Reg Loss=12.366997718811035
Clinet index 4, End of Epoch 5/6, Average Loss=12.604779243469238, Class Loss=0.23778170347213745, Reg Loss=12.366997718811035
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/25, Loss=12.151590351760387
Loss made of: CE 0.2817293107509613, LKD 2.6823079586029053, LDE 9.796756744384766, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/25, Loss=12.404560095071792
Loss made of: CE 0.23695753514766693, LKD 2.0421314239501953, LDE 11.031039237976074, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.21177642047405243, Reg Loss=11.907662391662598
Clinet index 4, End of Epoch 6/6, Average Loss=12.119439125061035, Class Loss=0.21177642047405243, Reg Loss=11.907662391662598
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/43, Loss=17.922884631156922
Loss made of: CE 0.274356484413147, LKD 1.6691797971725464, LDE 15.155450820922852, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/43, Loss=15.04833897948265
Loss made of: CE 0.4152395725250244, LKD 3.2520816326141357, LDE 10.897168159484863, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 30/43, Loss=13.895906582474709
Loss made of: CE 0.2565598487854004, LKD 1.8462138175964355, LDE 11.97484016418457, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/43, Loss=13.133064898848534
Loss made of: CE 0.4388434588909149, LKD 3.1195051670074463, LDE 9.780081748962402, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3937898278236389, Reg Loss=14.462629318237305
Clinet index 11, End of Epoch 1/6, Average Loss=14.856419563293457, Class Loss=0.3937898278236389, Reg Loss=14.462629318237305
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/43, Loss=12.699902057647705
Loss made of: CE 0.2224658727645874, LKD 1.5343643426895142, LDE 9.783156394958496, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/43, Loss=12.763592694699764
Loss made of: CE 0.24470216035842896, LKD 1.9261246919631958, LDE 9.09467601776123, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/43, Loss=12.8662238702178
Loss made of: CE 0.3133053779602051, LKD 2.6848654747009277, LDE 12.288870811462402, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/43, Loss=12.944011434912682
Loss made of: CE 0.2421683669090271, LKD 1.6883926391601562, LDE 10.108076095581055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.2958826422691345, Reg Loss=12.543333053588867
Clinet index 11, End of Epoch 2/6, Average Loss=12.839215278625488, Class Loss=0.2958826422691345, Reg Loss=12.543333053588867
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/43, Loss=12.070344376564027
Loss made of: CE 0.30297088623046875, LKD 2.4741952419281006, LDE 9.022289276123047, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/43, Loss=12.326460322737693
Loss made of: CE 0.2545052170753479, LKD 1.649553894996643, LDE 9.699212074279785, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/43, Loss=12.56261043548584
Loss made of: CE 0.2743913531303406, LKD 2.0126535892486572, LDE 9.282344818115234, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/43, Loss=12.00705969631672
Loss made of: CE 0.24425137042999268, LKD 2.0495057106018066, LDE 10.451006889343262, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2774481177330017, Reg Loss=11.932180404663086
Clinet index 11, End of Epoch 3/6, Average Loss=12.209628105163574, Class Loss=0.2774481177330017, Reg Loss=11.932180404663086
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/43, Loss=12.172369836270809
Loss made of: CE 0.26515743136405945, LKD 3.5534627437591553, LDE 10.172661781311035, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/43, Loss=11.027321879565715
Loss made of: CE 0.2969711422920227, LKD 3.0648019313812256, LDE 8.582012176513672, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/43, Loss=11.706916880607604
Loss made of: CE 0.26372289657592773, LKD 2.060722589492798, LDE 7.654230117797852, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/43, Loss=12.486225073039531
Loss made of: CE 0.2413373440504074, LKD 1.832045555114746, LDE 8.117722511291504, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.25948289036750793, Reg Loss=11.549753189086914
Clinet index 11, End of Epoch 4/6, Average Loss=11.809236526489258, Class Loss=0.25948289036750793, Reg Loss=11.549753189086914
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/43, Loss=12.394565604627132
Loss made of: CE 0.23798057436943054, LKD 3.0981714725494385, LDE 10.770617485046387, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/43, Loss=11.702653855085373
Loss made of: CE 0.2996596693992615, LKD 2.0587716102600098, LDE 10.252317428588867, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/43, Loss=12.241657999157905
Loss made of: CE 0.20398354530334473, LKD 1.7585253715515137, LDE 8.634923934936523, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/43, Loss=11.548954498767852
Loss made of: CE 0.2260732501745224, LKD 2.1436359882354736, LDE 8.398690223693848, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2614298164844513, Reg Loss=11.738953590393066
Clinet index 11, End of Epoch 5/6, Average Loss=12.000383377075195, Class Loss=0.2614298164844513, Reg Loss=11.738953590393066
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/43, Loss=11.405064356327056
Loss made of: CE 0.22772154211997986, LKD 2.0335512161254883, LDE 8.128902435302734, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/43, Loss=11.243936245143413
Loss made of: CE 0.2074519842863083, LKD 1.6445353031158447, LDE 8.974628448486328, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/43, Loss=11.064834763109683
Loss made of: CE 0.22011952102184296, LKD 2.9811902046203613, LDE 7.2360334396362305, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/43, Loss=10.808683378994465
Loss made of: CE 0.29842907190322876, LKD 2.318756341934204, LDE 7.935832500457764, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.24271820485591888, Reg Loss=10.909833908081055
Clinet index 11, End of Epoch 6/6, Average Loss=11.152551651000977, Class Loss=0.24271820485591888, Reg Loss=10.909833908081055
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/52, Loss=12.714993333816528
Loss made of: CE 0.31505998969078064, LKD 4.0130438804626465, LDE 9.070425033569336, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/52, Loss=14.608272226154805
Loss made of: CE 0.22185789048671722, LKD 3.768677234649658, LDE 9.998303413391113, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=14.463834835588932
Loss made of: CE 0.24521967768669128, LKD 3.189751148223877, LDE 11.187712669372559, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=13.577248293161393
Loss made of: CE 0.36898553371429443, LKD 3.4951906204223633, LDE 9.024614334106445, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/52, Loss=14.517217175662518
Loss made of: CE 0.2819095551967621, LKD 2.689408302307129, LDE 11.210819244384766, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2820296287536621, Reg Loss=13.711525917053223
Clinet index 9, End of Epoch 1/6, Average Loss=13.993555068969727, Class Loss=0.2820296287536621, Reg Loss=13.711525917053223
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/52, Loss=14.924081408977509
Loss made of: CE 0.27952879667282104, LKD 3.3703386783599854, LDE 10.612863540649414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=15.082948498427868
Loss made of: CE 0.2466331124305725, LKD 3.46757435798645, LDE 9.864073753356934, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=16.015048377215862
Loss made of: CE 0.2604067325592041, LKD 4.152971267700195, LDE 10.996489524841309, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/52, Loss=14.968194252252578
Loss made of: CE 0.19455496966838837, LKD 2.9271249771118164, LDE 9.310152053833008, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=14.98132790774107
Loss made of: CE 0.2038252055644989, LKD 3.565112829208374, LDE 12.386300086975098, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.26737651228904724, Reg Loss=15.028149604797363
Clinet index 9, End of Epoch 2/6, Average Loss=15.295526504516602, Class Loss=0.26737651228904724, Reg Loss=15.028149604797363
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/52, Loss=15.30673237144947
Loss made of: CE 0.30469194054603577, LKD 4.082529067993164, LDE 11.445707321166992, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=14.620867103338242
Loss made of: CE 0.2595582902431488, LKD 2.756601572036743, LDE 15.241424560546875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=14.862218545377255
Loss made of: CE 0.2561149001121521, LKD 2.8400776386260986, LDE 11.217111587524414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=15.448702028393745
Loss made of: CE 0.24854187667369843, LKD 4.094076156616211, LDE 10.666549682617188, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/52, Loss=14.614669036865234
Loss made of: CE 0.24555173516273499, LKD 3.6371920108795166, LDE 10.438702583312988, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.25089019536972046, Reg Loss=14.715792655944824
Clinet index 9, End of Epoch 3/6, Average Loss=14.966682434082031, Class Loss=0.25089019536972046, Reg Loss=14.715792655944824
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/52, Loss=14.872535994648933
Loss made of: CE 0.2614096701145172, LKD 3.73945689201355, LDE 9.606085777282715, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=14.255910903215408
Loss made of: CE 0.21456114947795868, LKD 3.1719179153442383, LDE 9.616669654846191, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=13.796011754870415
Loss made of: CE 0.23802843689918518, LKD 3.235318422317505, LDE 9.879252433776855, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=14.132826212048531
Loss made of: CE 0.19883671402931213, LKD 3.9911582469940186, LDE 10.875096321105957, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=13.776047955453397
Loss made of: CE 0.2861407697200775, LKD 4.2449259757995605, LDE 10.536724090576172, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.24125079810619354, Reg Loss=13.986764907836914
Clinet index 9, End of Epoch 4/6, Average Loss=14.228015899658203, Class Loss=0.24125079810619354, Reg Loss=13.986764907836914
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/52, Loss=13.036930912733078
Loss made of: CE 0.2565205693244934, LKD 2.5030977725982666, LDE 8.010089874267578, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 20/52, Loss=14.014075043797494
Loss made of: CE 0.24024084210395813, LKD 3.1481857299804688, LDE 10.161920547485352, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=12.85503680408001
Loss made of: CE 0.2931147813796997, LKD 4.514720916748047, LDE 10.546577453613281, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=14.707542707026004
Loss made of: CE 0.21164467930793762, LKD 3.6645426750183105, LDE 10.541961669921875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=14.242132361233235
Loss made of: CE 0.21211344003677368, LKD 3.9035332202911377, LDE 10.881324768066406, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.23785147070884705, Reg Loss=13.561919212341309
Clinet index 9, End of Epoch 5/6, Average Loss=13.79977035522461, Class Loss=0.23785147070884705, Reg Loss=13.561919212341309
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/52, Loss=13.357932820916176
Loss made of: CE 0.1589515209197998, LKD 3.339606523513794, LDE 9.359493255615234, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=14.076809518039227
Loss made of: CE 0.1788216531276703, LKD 3.0995874404907227, LDE 12.452849388122559, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/52, Loss=13.395069743692876
Loss made of: CE 0.18533435463905334, LKD 3.3627331256866455, LDE 10.082584381103516, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=13.926529876887798
Loss made of: CE 0.23204314708709717, LKD 3.7089638710021973, LDE 12.88328742980957, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=12.929346694052219
Loss made of: CE 0.1955636441707611, LKD 3.2532074451446533, LDE 9.498571395874023, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2333773672580719, Reg Loss=13.228171348571777
Clinet index 9, End of Epoch 6/6, Average Loss=13.461548805236816, Class Loss=0.2333773672580719, Reg Loss=13.228171348571777
Current Client Index:  6
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/34, Loss=16.13854591548443
Loss made of: CE 0.3029341399669647, LKD 2.922910213470459, LDE 9.77720832824707, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=13.318961587548255
Loss made of: CE 0.21965543925762177, LKD 3.345613956451416, LDE 8.745613098144531, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=15.452249452471733
Loss made of: CE 0.23306572437286377, LKD 3.458967685699463, LDE 8.949800491333008, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.2755012512207031, Reg Loss=14.535933494567871
Clinet index 6, End of Epoch 1/6, Average Loss=14.811434745788574, Class Loss=0.2755012512207031, Reg Loss=14.535933494567871
Pseudo labeling is: None
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/34, Loss=13.49435862749815
Loss made of: CE 0.26305073499679565, LKD 3.7595272064208984, LDE 9.988920211791992, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=13.141324320435524
Loss made of: CE 0.22068914771080017, LKD 3.355757236480713, LDE 9.508977890014648, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=13.945633290708065
Loss made of: CE 0.19285544753074646, LKD 4.259718418121338, LDE 13.50732421875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.23229660093784332, Reg Loss=13.134017944335938
Clinet index 6, End of Epoch 2/6, Average Loss=13.366314888000488, Class Loss=0.23229660093784332, Reg Loss=13.134017944335938
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/34, Loss=12.781446202099323
Loss made of: CE 0.21416258811950684, LKD 4.545864582061768, LDE 9.772645950317383, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=13.45899444371462
Loss made of: CE 0.17367979884147644, LKD 3.1476781368255615, LDE 9.004592895507812, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=13.941632550954818
Loss made of: CE 0.23070427775382996, LKD 3.463951826095581, LDE 8.955394744873047, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.229776993393898, Reg Loss=13.297040939331055
Clinet index 6, End of Epoch 3/6, Average Loss=13.52681827545166, Class Loss=0.229776993393898, Reg Loss=13.297040939331055
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/34, Loss=13.664861649274826
Loss made of: CE 0.17946460843086243, LKD 2.726942300796509, LDE 9.9358491897583, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=12.938134360313416
Loss made of: CE 0.22391635179519653, LKD 3.2473559379577637, LDE 10.503374099731445, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 30/34, Loss=13.932486143708228
Loss made of: CE 0.3208693265914917, LKD 3.757659673690796, LDE 10.997390747070312, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.22241728007793427, Reg Loss=13.161742210388184
Clinet index 6, End of Epoch 4/6, Average Loss=13.384159088134766, Class Loss=0.22241728007793427, Reg Loss=13.161742210388184
Pseudo labeling is: None
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/34, Loss=12.70268947929144
Loss made of: CE 0.14397341012954712, LKD 3.12581467628479, LDE 7.907261848449707, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=13.36616349965334
Loss made of: CE 0.2741008996963501, LKD 4.656001567840576, LDE 14.546248435974121, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=11.98783460110426
Loss made of: CE 0.197102352976799, LKD 3.9612860679626465, LDE 7.97671365737915, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.21872951090335846, Reg Loss=12.670893669128418
Clinet index 6, End of Epoch 5/6, Average Loss=12.889623641967773, Class Loss=0.21872951090335846, Reg Loss=12.670893669128418
Pseudo labeling is: None
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/34, Loss=13.171654435992242
Loss made of: CE 0.3186205327510834, LKD 2.608384847640991, LDE 9.511983871459961, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=12.514585733413696
Loss made of: CE 0.2713257968425751, LKD 4.049376964569092, LDE 13.350764274597168, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=12.594417056441307
Loss made of: CE 0.14595942199230194, LKD 3.345137119293213, LDE 7.101895809173584, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.22263723611831665, Reg Loss=12.496898651123047
Clinet index 6, End of Epoch 6/6, Average Loss=12.719535827636719, Class Loss=0.22263723611831665, Reg Loss=12.496898651123047
federated aggregation...
Validation, Class Loss=0.21831171214580536, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.919069
Mean Acc: 0.622224
FreqW Acc: 0.854971
Mean IoU: 0.554028
Class IoU:
	class 0: 0.9304744
	class 1: 0.60413134
	class 2: 0.14582267
	class 3: 0.1652831
	class 4: 0.64515805
	class 5: 0.3195814
	class 6: 0.8419815
	class 7: 0.6900939
	class 8: 0.64372945
Class Acc:
	class 0: 0.9820068
	class 1: 0.6069688
	class 2: 0.18516235
	class 3: 0.16602209
	class 4: 0.6964874
	class 5: 0.32930514
	class 6: 0.9090558
	class 7: 0.8113145
	class 8: 0.91368926

federated global round: 9, step: 1
select part of clients to conduct local training
[5, 0, 9, 2]
Current Client Index:  5
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/34, Loss=18.073948121070863
Loss made of: CE 0.29772451519966125, LKD 2.964898109436035, LDE 11.759779930114746, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/34, Loss=15.628820194303989
Loss made of: CE 0.3541547656059265, LKD 3.8328983783721924, LDE 10.714244842529297, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/34, Loss=15.08472294807434
Loss made of: CE 0.34924477338790894, LKD 5.477563858032227, LDE 15.768146514892578, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.31703391671180725, Reg Loss=15.84539794921875
Clinet index 5, End of Epoch 1/6, Average Loss=16.162431716918945, Class Loss=0.31703391671180725, Reg Loss=15.84539794921875
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/34, Loss=15.166002839803696
Loss made of: CE 0.21126151084899902, LKD 3.500150203704834, LDE 9.86168098449707, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/34, Loss=12.842487144470216
Loss made of: CE 0.22042468190193176, LKD 3.6494038105010986, LDE 9.028520584106445, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/34, Loss=14.60576871484518
Loss made of: CE 0.2931552529335022, LKD 4.541515827178955, LDE 11.696769714355469, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.25095728039741516, Reg Loss=13.780634880065918
Clinet index 5, End of Epoch 2/6, Average Loss=14.03159236907959, Class Loss=0.25095728039741516, Reg Loss=13.780634880065918
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/34, Loss=12.58621094673872
Loss made of: CE 0.2866472005844116, LKD 3.817829132080078, LDE 9.028464317321777, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/34, Loss=12.409533566236496
Loss made of: CE 0.2101510465145111, LKD 3.035614252090454, LDE 12.461755752563477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/34, Loss=13.770667807757855
Loss made of: CE 0.3281080722808838, LKD 4.061737537384033, LDE 13.141533851623535, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.23255297541618347, Reg Loss=12.850333213806152
Clinet index 5, End of Epoch 3/6, Average Loss=13.0828857421875, Class Loss=0.23255297541618347, Reg Loss=12.850333213806152
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/34, Loss=12.41265105009079
Loss made of: CE 0.19668611884117126, LKD 3.164891242980957, LDE 8.690736770629883, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/34, Loss=12.869542133808135
Loss made of: CE 0.2881985604763031, LKD 3.4361236095428467, LDE 8.744009971618652, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/34, Loss=13.20763020515442
Loss made of: CE 0.23588335514068604, LKD 3.6000595092773438, LDE 7.699975490570068, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.22855153679847717, Reg Loss=12.552660942077637
Clinet index 5, End of Epoch 4/6, Average Loss=12.78121280670166, Class Loss=0.22855153679847717, Reg Loss=12.552660942077637
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/34, Loss=12.521073998510838
Loss made of: CE 0.19803445041179657, LKD 3.777700662612915, LDE 8.25985050201416, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/34, Loss=12.234205923974514
Loss made of: CE 0.24967369437217712, LKD 3.22617506980896, LDE 9.001566886901855, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/34, Loss=12.868985740840435
Loss made of: CE 0.256664514541626, LKD 3.371091842651367, LDE 9.43639087677002, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.2220189869403839, Reg Loss=12.31187629699707
Clinet index 5, End of Epoch 5/6, Average Loss=12.533895492553711, Class Loss=0.2220189869403839, Reg Loss=12.31187629699707
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/34, Loss=13.567413225769997
Loss made of: CE 0.2829267382621765, LKD 3.4993879795074463, LDE 8.421902656555176, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/34, Loss=12.581076815724373
Loss made of: CE 0.19135499000549316, LKD 3.3770368099212646, LDE 9.462942123413086, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/34, Loss=12.678075581789017
Loss made of: CE 0.2685013711452484, LKD 4.638797760009766, LDE 12.201391220092773, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2272629737854004, Reg Loss=12.72362995147705
Clinet index 5, End of Epoch 6/6, Average Loss=12.95089340209961, Class Loss=0.2272629737854004, Reg Loss=12.72362995147705
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/52, Loss=13.427243903279305
Loss made of: CE 0.42374008893966675, LKD 3.309492826461792, LDE 7.91796875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/52, Loss=12.11675010919571
Loss made of: CE 0.2639352083206177, LKD 4.273765563964844, LDE 9.01700496673584, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=12.214226865768433
Loss made of: CE 0.26591578125953674, LKD 2.5501668453216553, LDE 7.88357400894165, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=12.456432312726974
Loss made of: CE 0.22068747878074646, LKD 2.5069150924682617, LDE 11.445140838623047, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/52, Loss=12.797401367127895
Loss made of: CE 0.18600940704345703, LKD 2.3483729362487793, LDE 8.40261173248291, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.280499666929245, Reg Loss=12.28807258605957
Clinet index 0, End of Epoch 1/6, Average Loss=12.568572044372559, Class Loss=0.280499666929245, Reg Loss=12.28807258605957
Pseudo labeling is: None
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/52, Loss=12.800375787913799
Loss made of: CE 0.29009050130844116, LKD 3.0205776691436768, LDE 8.254237174987793, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=12.093186050653458
Loss made of: CE 0.23816590011119843, LKD 2.9940340518951416, LDE 7.7735514640808105, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=13.442413212358952
Loss made of: CE 0.34302470088005066, LKD 2.4932100772857666, LDE 10.620888710021973, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/52, Loss=13.182008101046085
Loss made of: CE 0.29398447275161743, LKD 4.064684867858887, LDE 8.78511905670166, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=12.861619834601878
Loss made of: CE 0.2509274482727051, LKD 4.097914695739746, LDE 12.257099151611328, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.25077444314956665, Reg Loss=12.60721492767334
Clinet index 0, End of Epoch 2/6, Average Loss=12.857989311218262, Class Loss=0.25077444314956665, Reg Loss=12.60721492767334
Pseudo labeling is: None
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/52, Loss=12.764460431039334
Loss made of: CE 0.21951961517333984, LKD 3.4560348987579346, LDE 9.626209259033203, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=13.092980018258094
Loss made of: CE 0.29913532733917236, LKD 3.898524522781372, LDE 9.732683181762695, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=12.881569278240203
Loss made of: CE 0.3469768464565277, LKD 4.017921447753906, LDE 8.095916748046875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=12.66529390513897
Loss made of: CE 0.24448376893997192, LKD 2.4240403175354004, LDE 9.38646411895752, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 50/52, Loss=12.568192259967327
Loss made of: CE 0.24908721446990967, LKD 3.7323131561279297, LDE 8.153596878051758, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2519747316837311, Reg Loss=12.483294486999512
Clinet index 0, End of Epoch 3/6, Average Loss=12.735269546508789, Class Loss=0.2519747316837311, Reg Loss=12.483294486999512
Pseudo labeling is: None
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/52, Loss=13.4204708263278
Loss made of: CE 0.3064576983451843, LKD 3.458162307739258, LDE 8.294649124145508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=11.365864506363868
Loss made of: CE 0.18280422687530518, LKD 3.6538071632385254, LDE 7.745963096618652, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=11.960624332726002
Loss made of: CE 0.24410466849803925, LKD 2.6847026348114014, LDE 7.662321090698242, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=12.153273271024227
Loss made of: CE 0.22143730521202087, LKD 2.5202622413635254, LDE 7.343733310699463, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=11.287982204556466
Loss made of: CE 0.20724427700042725, LKD 3.1180036067962646, LDE 7.195460796356201, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.2382795214653015, Reg Loss=11.742210388183594
Clinet index 0, End of Epoch 4/6, Average Loss=11.980489730834961, Class Loss=0.2382795214653015, Reg Loss=11.742210388183594
Pseudo labeling is: None
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/52, Loss=11.695704527199268
Loss made of: CE 0.25896498560905457, LKD 3.0694191455841064, LDE 7.807371139526367, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/52, Loss=11.243786826729774
Loss made of: CE 0.19281473755836487, LKD 2.9825239181518555, LDE 8.365440368652344, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=11.674328491091728
Loss made of: CE 0.25994399189949036, LKD 2.990966558456421, LDE 7.651491641998291, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=11.630938284099102
Loss made of: CE 0.28461846709251404, LKD 3.0834908485412598, LDE 8.428682327270508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=11.326633206009864
Loss made of: CE 0.23060815036296844, LKD 3.191725969314575, LDE 7.040590286254883, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.23987890779972076, Reg Loss=11.434033393859863
Clinet index 0, End of Epoch 5/6, Average Loss=11.673912048339844, Class Loss=0.23987890779972076, Reg Loss=11.434033393859863
Pseudo labeling is: None
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/52, Loss=10.526597544550896
Loss made of: CE 0.21436606347560883, LKD 3.093351125717163, LDE 6.688407897949219, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=11.72151987105608
Loss made of: CE 0.18137085437774658, LKD 2.9616708755493164, LDE 7.08838415145874, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/52, Loss=11.897419522702695
Loss made of: CE 0.22470645606517792, LKD 2.223750114440918, LDE 8.209264755249023, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=11.112059800326824
Loss made of: CE 0.2467058002948761, LKD 2.2243552207946777, LDE 8.738353729248047, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=11.368751539289951
Loss made of: CE 0.2134242057800293, LKD 2.0843186378479004, LDE 8.539542198181152, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.23749473690986633, Reg Loss=11.067391395568848
Clinet index 0, End of Epoch 6/6, Average Loss=11.304885864257812, Class Loss=0.23749473690986633, Reg Loss=11.067391395568848
Current Client Index:  9
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/52, Loss=13.141342838108539
Loss made of: CE 0.3258439898490906, LKD 3.9665586948394775, LDE 8.604400634765625, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/52, Loss=14.30708712786436
Loss made of: CE 0.2590726613998413, LKD 3.3345673084259033, LDE 9.871824264526367, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/52, Loss=13.518560265004634
Loss made of: CE 0.2514083683490753, LKD 3.377746820449829, LDE 9.518232345581055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/52, Loss=12.375659711658955
Loss made of: CE 0.3486536145210266, LKD 3.7609751224517822, LDE 7.921829700469971, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 50/52, Loss=12.926613104343414
Loss made of: CE 0.27279165387153625, LKD 2.541179895401001, LDE 9.959737777709961, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.29071763157844543, Reg Loss=12.936613082885742
Clinet index 9, End of Epoch 1/6, Average Loss=13.227331161499023, Class Loss=0.29071763157844543, Reg Loss=12.936613082885742
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/52, Loss=13.345381774008274
Loss made of: CE 0.3202165961265564, LKD 3.712435722351074, LDE 9.519749641418457, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/52, Loss=13.248950642347335
Loss made of: CE 0.2648197412490845, LKD 3.6504836082458496, LDE 8.3792085647583, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/52, Loss=14.210521844029426
Loss made of: CE 0.2533421814441681, LKD 3.9894185066223145, LDE 9.603084564208984, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/52, Loss=12.677973139286042
Loss made of: CE 0.17486035823822021, LKD 2.618954658508301, LDE 7.159421443939209, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/52, Loss=12.766541284322738
Loss made of: CE 0.2049466371536255, LKD 3.3393778800964355, LDE 10.783072471618652, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.26562824845314026, Reg Loss=13.062625885009766
Clinet index 9, End of Epoch 2/6, Average Loss=13.328253746032715, Class Loss=0.26562824845314026, Reg Loss=13.062625885009766
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/52, Loss=12.953095179796218
Loss made of: CE 0.26938396692276, LKD 4.248453140258789, LDE 8.691763877868652, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/52, Loss=12.423211745917797
Loss made of: CE 0.26536181569099426, LKD 2.6299870014190674, LDE 13.624074935913086, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/52, Loss=13.0286601126194
Loss made of: CE 0.23234222829341888, LKD 2.6185622215270996, LDE 9.280591011047363, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/52, Loss=13.111209698021412
Loss made of: CE 0.23615512251853943, LKD 3.5174527168273926, LDE 8.690645217895508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/52, Loss=12.773378509283067
Loss made of: CE 0.244583398103714, LKD 3.4812233448028564, LDE 8.720539093017578, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2547014653682709, Reg Loss=12.605948448181152
Clinet index 9, End of Epoch 3/6, Average Loss=12.860650062561035, Class Loss=0.2547014653682709, Reg Loss=12.605948448181152
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/52, Loss=12.647117556631565
Loss made of: CE 0.2678198516368866, LKD 3.859557867050171, LDE 8.486344337463379, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/52, Loss=12.151407603919505
Loss made of: CE 0.22713300585746765, LKD 2.752795696258545, LDE 8.381706237792969, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/52, Loss=11.959544256329536
Loss made of: CE 0.24893443286418915, LKD 3.320866107940674, LDE 8.267189979553223, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/52, Loss=12.60563779771328
Loss made of: CE 0.19761702418327332, LKD 3.9869818687438965, LDE 8.571557998657227, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/52, Loss=12.179022978246213
Loss made of: CE 0.23796439170837402, LKD 3.8582584857940674, LDE 8.541732788085938, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.24638262391090393, Reg Loss=12.106485366821289
Clinet index 9, End of Epoch 4/6, Average Loss=12.35286808013916, Class Loss=0.24638262391090393, Reg Loss=12.106485366821289
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/52, Loss=11.675790618360043
Loss made of: CE 0.21854063868522644, LKD 2.369701623916626, LDE 6.226895809173584, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/52, Loss=12.328244610130787
Loss made of: CE 0.2254035472869873, LKD 3.6683435440063477, LDE 8.799748420715332, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/52, Loss=11.074371147155762
Loss made of: CE 0.3029126524925232, LKD 4.294655799865723, LDE 8.787734985351562, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/52, Loss=12.97706601768732
Loss made of: CE 0.21239718794822693, LKD 3.5432770252227783, LDE 8.86255168914795, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/52, Loss=12.19156236052513
Loss made of: CE 0.2260625660419464, LKD 3.5720467567443848, LDE 10.302167892456055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.24279510974884033, Reg Loss=11.826322555541992
Clinet index 9, End of Epoch 5/6, Average Loss=12.069117546081543, Class Loss=0.24279510974884033, Reg Loss=11.826322555541992
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/52, Loss=11.63776171207428
Loss made of: CE 0.18916378915309906, LKD 3.011446952819824, LDE 7.695108413696289, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/52, Loss=12.032852284610271
Loss made of: CE 0.18097934126853943, LKD 2.8536736965179443, LDE 10.037933349609375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/52, Loss=11.91859444975853
Loss made of: CE 0.19383011758327484, LKD 3.325554847717285, LDE 7.918240070343018, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/52, Loss=12.344670754671096
Loss made of: CE 0.27812817692756653, LKD 4.094895362854004, LDE 12.236647605895996, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/52, Loss=10.983199071884155
Loss made of: CE 0.22141942381858826, LKD 2.9697413444519043, LDE 7.412444114685059, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2449437528848648, Reg Loss=11.465059280395508
Clinet index 9, End of Epoch 6/6, Average Loss=11.710002899169922, Class Loss=0.2449437528848648, Reg Loss=11.465059280395508
Current Client Index:  2
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/25, Loss=14.781376755237579
Loss made of: CE 0.3356020152568817, LKD 2.7522122859954834, LDE 10.825743675231934, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 20/25, Loss=13.763188602030278
Loss made of: CE 0.36072713136672974, LKD 2.513888359069824, LDE 10.450361251831055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.3133152425289154, Reg Loss=13.459216117858887
Clinet index 2, End of Epoch 1/6, Average Loss=13.772531509399414, Class Loss=0.3133152425289154, Reg Loss=13.459216117858887
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/25, Loss=12.755771212279797
Loss made of: CE 0.24784299731254578, LKD 3.0041542053222656, LDE 9.94020938873291, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/25, Loss=13.154248243570327
Loss made of: CE 0.18534144759178162, LKD 2.1634697914123535, LDE 10.192261695861816, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.24825745820999146, Reg Loss=12.493705749511719
Clinet index 2, End of Epoch 2/6, Average Loss=12.741963386535645, Class Loss=0.24825745820999146, Reg Loss=12.493705749511719
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/25, Loss=12.110445016622544
Loss made of: CE 0.15199466049671173, LKD 1.728110432624817, LDE 9.844482421875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/25, Loss=12.711245818436145
Loss made of: CE 0.23137697577476501, LKD 2.2574994564056396, LDE 9.987430572509766, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.2223242074251175, Reg Loss=11.902928352355957
Clinet index 2, End of Epoch 3/6, Average Loss=12.125252723693848, Class Loss=0.2223242074251175, Reg Loss=11.902928352355957
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/25, Loss=11.770342902839184
Loss made of: CE 0.21657153964042664, LKD 2.4196853637695312, LDE 8.526926040649414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/25, Loss=12.22244004458189
Loss made of: CE 0.17482495307922363, LKD 1.812180519104004, LDE 8.040234565734863, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.21747711300849915, Reg Loss=11.76346206665039
Clinet index 2, End of Epoch 4/6, Average Loss=11.980938911437988, Class Loss=0.21747711300849915, Reg Loss=11.76346206665039
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/25, Loss=11.15413669347763
Loss made of: CE 0.14487501978874207, LKD 1.8390406370162964, LDE 8.419987678527832, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/25, Loss=11.402816818654538
Loss made of: CE 0.19948574900627136, LKD 2.494372844696045, LDE 7.8077287673950195, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.20201703906059265, Reg Loss=11.148152351379395
Clinet index 2, End of Epoch 5/6, Average Loss=11.35016918182373, Class Loss=0.20201703906059265, Reg Loss=11.148152351379395
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/25, Loss=10.512390027940274
Loss made of: CE 0.19204096496105194, LKD 2.218524694442749, LDE 8.776050567626953, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/25, Loss=10.883278581500054
Loss made of: CE 0.2222062349319458, LKD 1.8548074960708618, LDE 8.326688766479492, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2094096541404724, Reg Loss=10.465299606323242
Clinet index 2, End of Epoch 6/6, Average Loss=10.67470932006836, Class Loss=0.2094096541404724, Reg Loss=10.465299606323242
federated aggregation...
Validation, Class Loss=0.21329282224178314, Reg Loss=0.0 (without scaling)

Total samples: 699.000000
Overall Acc: 0.920716
Mean Acc: 0.639525
FreqW Acc: 0.859237
Mean IoU: 0.563700
Class IoU:
	class 0: 0.93379307
	class 1: 0.61123395
	class 2: 0.15729913
	class 3: 0.124308236
	class 4: 0.6512413
	class 5: 0.40003422
	class 6: 0.84895027
	class 7: 0.7056474
	class 8: 0.6407885
Class Acc:
	class 0: 0.97818404
	class 1: 0.6145483
	class 2: 0.20326468
	class 3: 0.12493072
	class 4: 0.7053803
	class 5: 0.41931364
	class 6: 0.9218111
	class 7: 0.85102034
	class 8: 0.9372679

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 10, step: 2
select part of clients to conduct local training
[0, 15, 1, 4]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=19.21853619813919
Loss made of: CE 1.3355677127838135, LKD 3.0171146392822266, LDE 12.615731239318848, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.3534953594207764, Reg Loss=15.23734188079834
Clinet index 0, End of Epoch 1/6, Average Loss=16.590837478637695, Class Loss=1.3534953594207764, Reg Loss=15.23734188079834
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=12.760900181531905
Loss made of: CE 1.0229558944702148, LKD 3.210153102874756, LDE 8.32858943939209, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.9723318219184875, Reg Loss=11.500860214233398
Clinet index 0, End of Epoch 2/6, Average Loss=12.47319221496582, Class Loss=0.9723318219184875, Reg Loss=11.500860214233398
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=12.520968133211136
Loss made of: CE 0.7254618406295776, LKD 3.789480686187744, LDE 7.60962438583374, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7215975522994995, Reg Loss=11.146991729736328
Clinet index 0, End of Epoch 3/6, Average Loss=11.868589401245117, Class Loss=0.7215975522994995, Reg Loss=11.146991729736328
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=10.697565692663193
Loss made of: CE 0.8172752857208252, LKD 2.7296667098999023, LDE 7.939505577087402, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5987334251403809, Reg Loss=10.578977584838867
Clinet index 0, End of Epoch 4/6, Average Loss=11.177711486816406, Class Loss=0.5987334251403809, Reg Loss=10.578977584838867
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=11.127132138609886
Loss made of: CE 0.5278427004814148, LKD 3.1917498111724854, LDE 7.045782566070557, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5071728229522705, Reg Loss=10.309616088867188
Clinet index 0, End of Epoch 5/6, Average Loss=10.816788673400879, Class Loss=0.5071728229522705, Reg Loss=10.309616088867188
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=10.740020185709
Loss made of: CE 0.5014724135398865, LKD 2.4891927242279053, LDE 6.367211818695068, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4840025305747986, Reg Loss=10.044039726257324
Clinet index 0, End of Epoch 6/6, Average Loss=10.52804183959961, Class Loss=0.4840025305747986, Reg Loss=10.044039726257324
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/19, Loss=20.86365188360214
Loss made of: CE 1.1406962871551514, LKD 2.959214687347412, LDE 13.563982963562012, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.4648466110229492, Reg Loss=16.111474990844727
Clinet index 15, End of Epoch 1/6, Average Loss=17.57632064819336, Class Loss=1.4648466110229492, Reg Loss=16.111474990844727
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=13.826797193288803
Loss made of: CE 0.6669453978538513, LKD 2.7721643447875977, LDE 10.16040325164795, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=1.0526572465896606, Reg Loss=12.270509719848633
Clinet index 15, End of Epoch 2/6, Average Loss=13.323166847229004, Class Loss=1.0526572465896606, Reg Loss=12.270509719848633
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=11.702469694614411
Loss made of: CE 0.738294243812561, LKD 2.8721697330474854, LDE 6.548362731933594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7448848485946655, Reg Loss=10.625927925109863
Clinet index 15, End of Epoch 3/6, Average Loss=11.37081241607666, Class Loss=0.7448848485946655, Reg Loss=10.625927925109863
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=12.041540950536728
Loss made of: CE 0.5471609830856323, LKD 2.9927079677581787, LDE 7.993622779846191, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6023770570755005, Reg Loss=10.86748218536377
Clinet index 15, End of Epoch 4/6, Average Loss=11.46985912322998, Class Loss=0.6023770570755005, Reg Loss=10.86748218536377
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=10.468526244163513
Loss made of: CE 0.47853952646255493, LKD 3.0260634422302246, LDE 7.045277118682861, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5313807725906372, Reg Loss=9.753974914550781
Clinet index 15, End of Epoch 5/6, Average Loss=10.285355567932129, Class Loss=0.5313807725906372, Reg Loss=9.753974914550781
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=9.990298646688462
Loss made of: CE 0.5403796434402466, LKD 2.945864677429199, LDE 5.182587146759033, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.502386212348938, Reg Loss=9.829102516174316
Clinet index 15, End of Epoch 6/6, Average Loss=10.331488609313965, Class Loss=0.502386212348938, Reg Loss=9.829102516174316
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 1, Batch 10/35, Loss=23.966960966587067
Loss made of: CE 1.2567787170410156, LKD 5.432044506072998, LDE 15.662752151489258, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/35, Loss=17.14907600879669
Loss made of: CE 0.8057316541671753, LKD 3.8787167072296143, LDE 10.68454360961914, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/35, Loss=14.976198470592498
Loss made of: CE 0.782632052898407, LKD 4.958593845367432, LDE 9.317835807800293, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.0526671409606934, Reg Loss=16.952058792114258
Clinet index 1, End of Epoch 1/6, Average Loss=18.00472640991211, Class Loss=1.0526671409606934, Reg Loss=16.952058792114258
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/35, Loss=14.49899226129055
Loss made of: CE 0.4476954936981201, LKD 4.488532066345215, LDE 9.498431205749512, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/35, Loss=13.957513061165809
Loss made of: CE 0.6081650257110596, LKD 4.127230644226074, LDE 10.442519187927246, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/35, Loss=13.7504419028759
Loss made of: CE 0.3646021783351898, LKD 4.562239170074463, LDE 8.964091300964355, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4711257517337799, Reg Loss=13.439160346984863
Clinet index 1, End of Epoch 2/6, Average Loss=13.910285949707031, Class Loss=0.4711257517337799, Reg Loss=13.439160346984863
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/35, Loss=12.447863551974297
Loss made of: CE 0.3960622251033783, LKD 4.909329891204834, LDE 6.963570594787598, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/35, Loss=13.016253933310509
Loss made of: CE 0.33536621928215027, LKD 4.711911678314209, LDE 7.807998180389404, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/35, Loss=12.7619399279356
Loss made of: CE 0.3523934781551361, LKD 4.632007598876953, LDE 8.089415550231934, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3805190920829773, Reg Loss=12.249442100524902
Clinet index 1, End of Epoch 3/6, Average Loss=12.629961013793945, Class Loss=0.3805190920829773, Reg Loss=12.249442100524902
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/35, Loss=12.348285213112831
Loss made of: CE 0.29613181948661804, LKD 4.190016746520996, LDE 7.165524959564209, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/35, Loss=11.756522205471992
Loss made of: CE 0.25290000438690186, LKD 3.5804104804992676, LDE 8.77719783782959, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/35, Loss=11.891830876469612
Loss made of: CE 0.32703787088394165, LKD 3.9346981048583984, LDE 6.69478178024292, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3504008948802948, Reg Loss=11.649171829223633
Clinet index 1, End of Epoch 4/6, Average Loss=11.99957275390625, Class Loss=0.3504008948802948, Reg Loss=11.649171829223633
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/35, Loss=12.121600258350373
Loss made of: CE 0.3182511627674103, LKD 4.356337547302246, LDE 6.019937038421631, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/35, Loss=12.193424379825592
Loss made of: CE 0.31460338830947876, LKD 4.257343769073486, LDE 6.906276226043701, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/35, Loss=11.497502759099007
Loss made of: CE 0.2635457217693329, LKD 3.66660737991333, LDE 6.306947231292725, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3282952904701233, Reg Loss=11.58533000946045
Clinet index 1, End of Epoch 5/6, Average Loss=11.913625717163086, Class Loss=0.3282952904701233, Reg Loss=11.58533000946045
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/35, Loss=11.308003017306328
Loss made of: CE 0.2669695019721985, LKD 3.7068498134613037, LDE 6.47809362411499, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/35, Loss=12.365845903754234
Loss made of: CE 0.31011420488357544, LKD 4.829638957977295, LDE 7.139377117156982, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/35, Loss=12.450278198719024
Loss made of: CE 0.3780273199081421, LKD 4.27875280380249, LDE 6.6782965660095215, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.31803032755851746, Reg Loss=11.714656829833984
Clinet index 1, End of Epoch 6/6, Average Loss=12.032687187194824, Class Loss=0.31803032755851746, Reg Loss=11.714656829833984
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/19, Loss=19.190736949443817
Loss made of: CE 1.387458324432373, LKD 3.2497658729553223, LDE 11.204176902770996, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.4038666486740112, Reg Loss=15.575857162475586
Clinet index 4, End of Epoch 1/6, Average Loss=16.97972297668457, Class Loss=1.4038666486740112, Reg Loss=15.575857162475586
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=13.371331959962845
Loss made of: CE 0.8754708170890808, LKD 3.063962936401367, LDE 9.475008010864258, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.9943022727966309, Reg Loss=11.573821067810059
Clinet index 4, End of Epoch 2/6, Average Loss=12.568122863769531, Class Loss=0.9943022727966309, Reg Loss=11.573821067810059
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/19, Loss=11.900147902965546
Loss made of: CE 0.6087586879730225, LKD 3.28226375579834, LDE 6.663873672485352, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7472444772720337, Reg Loss=10.894948959350586
Clinet index 4, End of Epoch 3/6, Average Loss=11.642193794250488, Class Loss=0.7472444772720337, Reg Loss=10.894948959350586
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=10.77790662944317
Loss made of: CE 0.5302897691726685, LKD 3.0166540145874023, LDE 6.71455717086792, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5794066190719604, Reg Loss=10.276166915893555
Clinet index 4, End of Epoch 4/6, Average Loss=10.855573654174805, Class Loss=0.5794066190719604, Reg Loss=10.276166915893555
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=10.956005051732063
Loss made of: CE 0.5555131435394287, LKD 2.8494021892547607, LDE 7.177811145782471, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5204150676727295, Reg Loss=9.976219177246094
Clinet index 4, End of Epoch 5/6, Average Loss=10.496634483337402, Class Loss=0.5204150676727295, Reg Loss=9.976219177246094
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=10.462558460235595
Loss made of: CE 0.42591267824172974, LKD 2.660738468170166, LDE 6.292727470397949, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4781515896320343, Reg Loss=10.271965980529785
Clinet index 4, End of Epoch 6/6, Average Loss=10.750117301940918, Class Loss=0.4781515896320343, Reg Loss=10.271965980529785
federated aggregation...
Validation, Class Loss=0.5005648732185364, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.837311
Mean Acc: 0.374254
FreqW Acc: 0.757699
Mean IoU: 0.299883
Class IoU:
	class 0: 0.8998885
	class 1: 0.076372676
	class 2: 0.054260604
	class 3: 5.861329e-05
	class 4: 0.5231836
	class 5: 0.08446744
	class 6: 0.72685426
	class 7: 0.6336867
	class 8: 0.62917125
	class 9: 0.0
	class 10: 0.21352005
	class 11: 0.05701949
	class 12: 0.0
Class Acc:
	class 0: 0.97957027
	class 1: 0.07641891
	class 2: 0.060022146
	class 3: 5.8613452e-05
	class 4: 0.5519379
	class 5: 0.084477946
	class 6: 0.7363979
	class 7: 0.6406328
	class 8: 0.70919317
	class 9: 0.0
	class 10: 0.8927828
	class 11: 0.1338082
	class 12: 0.0

federated global round: 11, step: 2
select part of clients to conduct local training
[1, 13, 11, 12]
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=18.251941901445388
Loss made of: CE 0.8600931167602539, LKD 5.3031840324401855, LDE 12.674983024597168, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/35, Loss=14.21127963066101
Loss made of: CE 0.6035139560699463, LKD 3.841050386428833, LDE 8.175420761108398, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/35, Loss=12.96340434551239
Loss made of: CE 0.5585519075393677, LKD 4.7078728675842285, LDE 8.5909423828125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7070487141609192, Reg Loss=13.977814674377441
Clinet index 1, End of Epoch 1/6, Average Loss=14.684863090515137, Class Loss=0.7070487141609192, Reg Loss=13.977814674377441
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/35, Loss=13.23894931077957
Loss made of: CE 0.40109413862228394, LKD 4.408272743225098, LDE 9.15664005279541, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/35, Loss=12.27883855998516
Loss made of: CE 0.45210346579551697, LKD 4.091663360595703, LDE 8.529440879821777, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/35, Loss=12.436377948522567
Loss made of: CE 0.3472428321838379, LKD 4.378066539764404, LDE 7.6523661613464355, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.39119938015937805, Reg Loss=12.176745414733887
Clinet index 1, End of Epoch 2/6, Average Loss=12.567944526672363, Class Loss=0.39119938015937805, Reg Loss=12.176745414733887
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/35, Loss=11.625930139422417
Loss made of: CE 0.32511234283447266, LKD 5.120837688446045, LDE 6.111130237579346, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/35, Loss=11.941112986207008
Loss made of: CE 0.31245607137680054, LKD 4.594745635986328, LDE 6.879186630249023, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/35, Loss=11.750799342989922
Loss made of: CE 0.33516213297843933, LKD 4.514861106872559, LDE 7.303371429443359, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.34230130910873413, Reg Loss=11.33669376373291
Clinet index 1, End of Epoch 3/6, Average Loss=11.678995132446289, Class Loss=0.34230130910873413, Reg Loss=11.33669376373291
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/35, Loss=11.68361238837242
Loss made of: CE 0.2922104597091675, LKD 4.0915021896362305, LDE 6.729538917541504, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/35, Loss=10.998803731799125
Loss made of: CE 0.24505579471588135, LKD 3.375936985015869, LDE 8.006244659423828, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/35, Loss=11.370663829147816
Loss made of: CE 0.27853864431381226, LKD 3.8218588829040527, LDE 6.023758888244629, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.32017284631729126, Reg Loss=11.021587371826172
Clinet index 1, End of Epoch 4/6, Average Loss=11.341760635375977, Class Loss=0.32017284631729126, Reg Loss=11.021587371826172
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/35, Loss=11.62888593673706
Loss made of: CE 0.3308454751968384, LKD 4.129176616668701, LDE 5.995319366455078, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/35, Loss=11.58891561627388
Loss made of: CE 0.327161580324173, LKD 4.414090633392334, LDE 6.3124494552612305, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/35, Loss=11.138850183784962
Loss made of: CE 0.2650313377380371, LKD 3.9340484142303467, LDE 5.687385559082031, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3124811053276062, Reg Loss=11.175751686096191
Clinet index 1, End of Epoch 5/6, Average Loss=11.488232612609863, Class Loss=0.3124811053276062, Reg Loss=11.175751686096191
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/35, Loss=11.06875137835741
Loss made of: CE 0.24099501967430115, LKD 3.4689855575561523, LDE 6.363293647766113, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/35, Loss=11.454526703059674
Loss made of: CE 0.31320464611053467, LKD 4.887901782989502, LDE 6.741155624389648, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/35, Loss=11.642664709687233
Loss made of: CE 0.32731035351753235, LKD 4.185948848724365, LDE 6.394989013671875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.30928197503089905, Reg Loss=11.123234748840332
Clinet index 1, End of Epoch 6/6, Average Loss=11.432517051696777, Class Loss=0.30928197503089905, Reg Loss=11.123234748840332
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=10.31427598297596
Loss made of: CE 0.5685459971427917, LKD 2.6189258098602295, LDE 6.464572906494141, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=0.5587093234062195, Reg Loss=9.723695755004883
Clinet index 13, End of Epoch 1/6, Average Loss=10.282404899597168, Class Loss=0.5587093234062195, Reg Loss=9.723695755004883
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/19, Loss=10.645987609028817
Loss made of: CE 0.6998876333236694, LKD 3.099071502685547, LDE 8.981915473937988, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4870879054069519, Reg Loss=9.789581298828125
Clinet index 13, End of Epoch 2/6, Average Loss=10.2766695022583, Class Loss=0.4870879054069519, Reg Loss=9.789581298828125
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/19, Loss=10.751905611157417
Loss made of: CE 0.39594048261642456, LKD 2.7793819904327393, LDE 6.583212375640869, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4631420075893402, Reg Loss=10.117901802062988
Clinet index 13, End of Epoch 3/6, Average Loss=10.58104419708252, Class Loss=0.4631420075893402, Reg Loss=10.117901802062988
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/19, Loss=10.060357102751732
Loss made of: CE 0.4269754886627197, LKD 2.788811445236206, LDE 6.42476224899292, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.43486765027046204, Reg Loss=9.707162857055664
Clinet index 13, End of Epoch 4/6, Average Loss=10.142030715942383, Class Loss=0.43486765027046204, Reg Loss=9.707162857055664
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/19, Loss=10.65293075144291
Loss made of: CE 0.3642756938934326, LKD 3.321800708770752, LDE 5.9270100593566895, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4200771152973175, Reg Loss=9.924633979797363
Clinet index 13, End of Epoch 5/6, Average Loss=10.344711303710938, Class Loss=0.4200771152973175, Reg Loss=9.924633979797363
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/19, Loss=9.982875421643257
Loss made of: CE 0.48546671867370605, LKD 2.8068666458129883, LDE 6.567060947418213, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.40413179993629456, Reg Loss=9.549202919006348
Clinet index 13, End of Epoch 6/6, Average Loss=9.95333480834961, Class Loss=0.40413179993629456, Reg Loss=9.549202919006348
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/33, Loss=11.504493820667268
Loss made of: CE 0.6254292130470276, LKD 2.441068649291992, LDE 6.081897735595703, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/33, Loss=10.829227072000503
Loss made of: CE 0.6709436178207397, LKD 3.764732599258423, LDE 6.1340460777282715, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/33, Loss=10.984667664766311
Loss made of: CE 0.48085254430770874, LKD 3.3217127323150635, LDE 6.125119209289551, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6465433239936829, Reg Loss=10.484071731567383
Clinet index 11, End of Epoch 1/6, Average Loss=11.130615234375, Class Loss=0.6465433239936829, Reg Loss=10.484071731567383
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=10.851483565568923
Loss made of: CE 0.622707724571228, LKD 2.436779499053955, LDE 7.9979939460754395, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/33, Loss=10.310472029447556
Loss made of: CE 0.5711261630058289, LKD 3.126016139984131, LDE 5.794504165649414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/33, Loss=10.276724350452422
Loss made of: CE 0.645211398601532, LKD 2.7757911682128906, LDE 6.096757411956787, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5371629595756531, Reg Loss=9.917174339294434
Clinet index 11, End of Epoch 2/6, Average Loss=10.454337120056152, Class Loss=0.5371629595756531, Reg Loss=9.917174339294434
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=10.538601535558701
Loss made of: CE 0.41002970933914185, LKD 3.1374433040618896, LDE 5.742822170257568, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/33, Loss=10.378051963448524
Loss made of: CE 0.5590351819992065, LKD 2.1282782554626465, LDE 6.591202259063721, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/33, Loss=10.06446530520916
Loss made of: CE 0.5437801480293274, LKD 2.888094425201416, LDE 6.407786846160889, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4983603358268738, Reg Loss=9.704822540283203
Clinet index 11, End of Epoch 3/6, Average Loss=10.2031831741333, Class Loss=0.4983603358268738, Reg Loss=9.704822540283203
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=9.821802750229836
Loss made of: CE 0.47565340995788574, LKD 2.984027862548828, LDE 5.698360443115234, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/33, Loss=10.451228296756744
Loss made of: CE 0.4936942458152771, LKD 3.178098678588867, LDE 7.1242451667785645, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/33, Loss=10.102370384335519
Loss made of: CE 0.5504132509231567, LKD 3.5731310844421387, LDE 5.371903896331787, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.47589483857154846, Reg Loss=9.613073348999023
Clinet index 11, End of Epoch 4/6, Average Loss=10.088968276977539, Class Loss=0.47589483857154846, Reg Loss=9.613073348999023
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=9.790537637472152
Loss made of: CE 0.36982282996177673, LKD 2.9001777172088623, LDE 5.968830108642578, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/33, Loss=10.270765039324761
Loss made of: CE 0.4411355257034302, LKD 2.6231839656829834, LDE 6.270226001739502, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/33, Loss=9.810096797347068
Loss made of: CE 0.5185455083847046, LKD 2.8839211463928223, LDE 6.494197368621826, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.47090330719947815, Reg Loss=9.454904556274414
Clinet index 11, End of Epoch 5/6, Average Loss=9.92580795288086, Class Loss=0.47090330719947815, Reg Loss=9.454904556274414
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=9.339691752195359
Loss made of: CE 0.46714043617248535, LKD 2.9175868034362793, LDE 6.094476222991943, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/33, Loss=9.15278713107109
Loss made of: CE 0.3613494336605072, LKD 3.0268256664276123, LDE 4.797468662261963, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/33, Loss=9.32861053943634
Loss made of: CE 0.3952590823173523, LKD 2.5796902179718018, LDE 7.604303359985352, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.44692835211753845, Reg Loss=8.925909042358398
Clinet index 11, End of Epoch 6/6, Average Loss=9.37283706665039, Class Loss=0.44692835211753845, Reg Loss=8.925909042358398
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/33, Loss=11.799044704437256
Loss made of: CE 0.7858654260635376, LKD 2.753203868865967, LDE 8.533792495727539, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/33, Loss=10.662157678604126
Loss made of: CE 0.7902883291244507, LKD 3.5097718238830566, LDE 8.466471672058105, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/33, Loss=10.364831840991974
Loss made of: CE 0.46300002932548523, LKD 2.7175755500793457, LDE 7.537195205688477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.624992847442627, Reg Loss=10.233403205871582
Clinet index 12, End of Epoch 1/6, Average Loss=10.858396530151367, Class Loss=0.624992847442627, Reg Loss=10.233403205871582
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/33, Loss=10.298195478320121
Loss made of: CE 0.35787591338157654, LKD 2.9189884662628174, LDE 7.702870845794678, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/33, Loss=10.8835763245821
Loss made of: CE 0.6958831548690796, LKD 3.658649444580078, LDE 7.763957977294922, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/33, Loss=10.083917263150216
Loss made of: CE 0.5718380212783813, LKD 3.3935420513153076, LDE 5.24159049987793, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5328912138938904, Reg Loss=9.903627395629883
Clinet index 12, End of Epoch 2/6, Average Loss=10.436518669128418, Class Loss=0.5328912138938904, Reg Loss=9.903627395629883
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/33, Loss=10.166399994492531
Loss made of: CE 0.4437928795814514, LKD 2.7030324935913086, LDE 5.473655700683594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/33, Loss=10.438976469635964
Loss made of: CE 0.476627916097641, LKD 3.425006151199341, LDE 6.810173511505127, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/33, Loss=9.834977501630783
Loss made of: CE 0.5455811023712158, LKD 3.368117570877075, LDE 5.588903903961182, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4905926287174225, Reg Loss=9.715597152709961
Clinet index 12, End of Epoch 3/6, Average Loss=10.20619010925293, Class Loss=0.4905926287174225, Reg Loss=9.715597152709961
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/33, Loss=10.702678135037422
Loss made of: CE 0.4103916585445404, LKD 2.683302402496338, LDE 7.13415002822876, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/33, Loss=9.822826179862023
Loss made of: CE 0.4995107352733612, LKD 3.696500778198242, LDE 5.517972469329834, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/33, Loss=9.203916445374489
Loss made of: CE 0.4052782952785492, LKD 2.351569175720215, LDE 6.695950031280518, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4655870795249939, Reg Loss=9.450281143188477
Clinet index 12, End of Epoch 4/6, Average Loss=9.915867805480957, Class Loss=0.4655870795249939, Reg Loss=9.450281143188477
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/33, Loss=9.888073599338531
Loss made of: CE 0.3422629237174988, LKD 3.4086246490478516, LDE 5.014573097229004, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/33, Loss=9.413873589038849
Loss made of: CE 0.5882403254508972, LKD 3.7946250438690186, LDE 5.162169933319092, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/33, Loss=9.495673471689225
Loss made of: CE 0.5581543445587158, LKD 3.7747950553894043, LDE 6.319215297698975, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4556818902492523, Reg Loss=9.14400577545166
Clinet index 12, End of Epoch 5/6, Average Loss=9.599687576293945, Class Loss=0.4556818902492523, Reg Loss=9.14400577545166
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/33, Loss=9.913383936882019
Loss made of: CE 0.6001163721084595, LKD 3.7059526443481445, LDE 6.004494667053223, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/33, Loss=9.61432056427002
Loss made of: CE 0.43761321902275085, LKD 3.7729907035827637, LDE 5.394468784332275, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/33, Loss=10.15839958190918
Loss made of: CE 0.49975356459617615, LKD 3.9191181659698486, LDE 5.9226155281066895, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.45583146810531616, Reg Loss=9.449097633361816
Clinet index 12, End of Epoch 6/6, Average Loss=9.904929161071777, Class Loss=0.45583146810531616, Reg Loss=9.449097633361816
federated aggregation...
Validation, Class Loss=0.4333636164665222, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.856030
Mean Acc: 0.432331
FreqW Acc: 0.779301
Mean IoU: 0.352227
Class IoU:
	class 0: 0.9057375
	class 1: 0.09506255
	class 2: 0.017038161
	class 3: 0.00032648587
	class 4: 0.5696024
	class 5: 0.17481068
	class 6: 0.8117039
	class 7: 0.73574346
	class 8: 0.6703928
	class 9: 0.040649265
	class 10: 0.2544599
	class 11: 0.1532811
	class 12: 0.15013777
Class Acc:
	class 0: 0.9788852
	class 1: 0.09513578
	class 2: 0.017398503
	class 3: 0.0003264872
	class 4: 0.59843296
	class 5: 0.17492756
	class 6: 0.83715403
	class 7: 0.7585123
	class 8: 0.7621654
	class 9: 0.06552455
	class 10: 0.91775006
	class 11: 0.2555447
	class 12: 0.15855008

federated global round: 12, step: 2
select part of clients to conduct local training
[0, 16, 7, 4]
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=9.678733167052268
Loss made of: CE 0.5406187176704407, LKD 2.531493902206421, LDE 7.372185707092285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4883118271827698, Reg Loss=8.845069885253906
Clinet index 0, End of Epoch 1/6, Average Loss=9.333381652832031, Class Loss=0.4883118271827698, Reg Loss=8.845069885253906
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=9.306269985437392
Loss made of: CE 0.44386810064315796, LKD 3.0081536769866943, LDE 6.187741279602051, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.43007659912109375, Reg Loss=8.799637794494629
Clinet index 0, End of Epoch 2/6, Average Loss=9.229714393615723, Class Loss=0.43007659912109375, Reg Loss=8.799637794494629
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=10.430645555257797
Loss made of: CE 0.4312621057033539, LKD 3.8292698860168457, LDE 5.784014701843262, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.42479461431503296, Reg Loss=9.322966575622559
Clinet index 0, End of Epoch 3/6, Average Loss=9.747760772705078, Class Loss=0.42479461431503296, Reg Loss=9.322966575622559
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=9.15787273645401
Loss made of: CE 0.49823957681655884, LKD 2.90875244140625, LDE 6.079681873321533, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3963497579097748, Reg Loss=9.396622657775879
Clinet index 0, End of Epoch 4/6, Average Loss=9.792972564697266, Class Loss=0.3963497579097748, Reg Loss=9.396622657775879
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=9.628035032749176
Loss made of: CE 0.39568185806274414, LKD 3.277263879776001, LDE 5.552412033081055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3874029517173767, Reg Loss=8.93889045715332
Clinet index 0, End of Epoch 5/6, Average Loss=9.326292991638184, Class Loss=0.3874029517173767, Reg Loss=8.93889045715332
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=9.274825462698937
Loss made of: CE 0.4610553979873657, LKD 2.561816930770874, LDE 5.071363925933838, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.38774755597114563, Reg Loss=8.861560821533203
Clinet index 0, End of Epoch 6/6, Average Loss=9.249308586120605, Class Loss=0.38774755597114563, Reg Loss=8.861560821533203
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/42, Loss=12.374743044376373
Loss made of: CE 0.5983169078826904, LKD 4.16404390335083, LDE 7.537348747253418, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/42, Loss=11.463952994346618
Loss made of: CE 0.46074652671813965, LKD 3.5656609535217285, LDE 6.46575927734375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/42, Loss=11.911089134216308
Loss made of: CE 0.5388598442077637, LKD 4.384102821350098, LDE 6.886788368225098, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/42, Loss=10.795903906226158
Loss made of: CE 0.2964155375957489, LKD 4.119870662689209, LDE 6.022710800170898, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5317538380622864, Reg Loss=11.027718544006348
Clinet index 16, End of Epoch 1/6, Average Loss=11.55947208404541, Class Loss=0.5317538380622864, Reg Loss=11.027718544006348
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/42, Loss=11.691123023629189
Loss made of: CE 0.38011717796325684, LKD 4.180301189422607, LDE 6.443377494812012, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/42, Loss=10.991563525795936
Loss made of: CE 0.3185182809829712, LKD 4.377270698547363, LDE 6.8781280517578125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/42, Loss=10.723349225521087
Loss made of: CE 0.2792593240737915, LKD 3.7835352420806885, LDE 6.9176926612854, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/42, Loss=10.96516794860363
Loss made of: CE 0.3574239909648895, LKD 2.9521961212158203, LDE 6.343527793884277, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.38628843426704407, Reg Loss=10.724732398986816
Clinet index 16, End of Epoch 2/6, Average Loss=11.111021041870117, Class Loss=0.38628843426704407, Reg Loss=10.724732398986816
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/42, Loss=10.402253514528274
Loss made of: CE 0.3907589316368103, LKD 3.5856099128723145, LDE 6.032493591308594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/42, Loss=11.635324570536614
Loss made of: CE 0.29889413714408875, LKD 4.813879013061523, LDE 7.794699192047119, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/42, Loss=10.56014364361763
Loss made of: CE 0.2965407967567444, LKD 4.355605602264404, LDE 6.622332572937012, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/42, Loss=10.544902300834655
Loss made of: CE 0.3619752824306488, LKD 4.516132831573486, LDE 6.503653526306152, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.37255609035491943, Reg Loss=10.443268775939941
Clinet index 16, End of Epoch 3/6, Average Loss=10.815824508666992, Class Loss=0.37255609035491943, Reg Loss=10.443268775939941
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/42, Loss=11.064487683773041
Loss made of: CE 0.3516457974910736, LKD 3.3152859210968018, LDE 7.0831379890441895, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/42, Loss=10.758296117186546
Loss made of: CE 0.4486308693885803, LKD 4.182582378387451, LDE 5.472672939300537, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/42, Loss=10.123465633392334
Loss made of: CE 0.2745097279548645, LKD 3.5221986770629883, LDE 5.455358982086182, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/42, Loss=10.294817581772804
Loss made of: CE 0.33881133794784546, LKD 3.3527731895446777, LDE 5.87639045715332, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3611091077327728, Reg Loss=10.236211776733398
Clinet index 16, End of Epoch 4/6, Average Loss=10.597320556640625, Class Loss=0.3611091077327728, Reg Loss=10.236211776733398
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/42, Loss=9.81933480501175
Loss made of: CE 0.4165716767311096, LKD 4.07919454574585, LDE 5.547664165496826, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/42, Loss=10.312636551260947
Loss made of: CE 0.4422776699066162, LKD 3.202096462249756, LDE 5.950242042541504, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/42, Loss=10.525954431295395
Loss made of: CE 0.35860562324523926, LKD 3.2181389331817627, LDE 5.727394104003906, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/42, Loss=10.100301548838615
Loss made of: CE 0.32175299525260925, LKD 4.4757080078125, LDE 6.243961334228516, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.34823569655418396, Reg Loss=9.860106468200684
Clinet index 16, End of Epoch 5/6, Average Loss=10.208342552185059, Class Loss=0.34823569655418396, Reg Loss=9.860106468200684
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/42, Loss=10.635488620400428
Loss made of: CE 0.30398833751678467, LKD 3.742774486541748, LDE 5.176513671875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/42, Loss=10.181882753968239
Loss made of: CE 0.3675476014614105, LKD 3.5385522842407227, LDE 5.628733158111572, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/42, Loss=10.128690052032471
Loss made of: CE 0.3732932209968567, LKD 3.728199005126953, LDE 5.976889133453369, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 6, Batch 40/42, Loss=10.73483963906765
Loss made of: CE 0.4506723880767822, LKD 3.7590713500976562, LDE 6.7677693367004395, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3501446545124054, Reg Loss=10.026963233947754
Clinet index 16, End of Epoch 6/6, Average Loss=10.377107620239258, Class Loss=0.3501446545124054, Reg Loss=10.026963233947754
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/31, Loss=12.507929763197899
Loss made of: CE 0.641144335269928, LKD 2.3192992210388184, LDE 7.868161678314209, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/31, Loss=10.8919065117836
Loss made of: CE 0.5910590887069702, LKD 3.45229434967041, LDE 7.609663963317871, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/31, Loss=10.433252385258674
Loss made of: CE 0.5249399542808533, LKD 2.5887680053710938, LDE 6.87951135635376, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.581509530544281, Reg Loss=10.73302936553955
Clinet index 7, End of Epoch 1/6, Average Loss=11.314538955688477, Class Loss=0.581509530544281, Reg Loss=10.73302936553955
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/31, Loss=10.242894527316093
Loss made of: CE 0.456885427236557, LKD 2.4121170043945312, LDE 5.674304962158203, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/31, Loss=9.253779622912408
Loss made of: CE 0.520111083984375, LKD 2.7092204093933105, LDE 5.73002815246582, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/31, Loss=10.014045578241348
Loss made of: CE 0.4527549743652344, LKD 3.01750111579895, LDE 5.871877193450928, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.51703941822052, Reg Loss=9.334192276000977
Clinet index 7, End of Epoch 2/6, Average Loss=9.851231575012207, Class Loss=0.51703941822052, Reg Loss=9.334192276000977
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/31, Loss=9.580970552563667
Loss made of: CE 0.5654336214065552, LKD 3.6399576663970947, LDE 6.231775760650635, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/31, Loss=9.218065270781517
Loss made of: CE 0.4786275029182434, LKD 3.4288997650146484, LDE 5.78389835357666, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/31, Loss=9.339060628414154
Loss made of: CE 0.441255658864975, LKD 3.095045328140259, LDE 5.686868190765381, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.48117589950561523, Reg Loss=8.875126838684082
Clinet index 7, End of Epoch 3/6, Average Loss=9.356302261352539, Class Loss=0.48117589950561523, Reg Loss=8.875126838684082
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/31, Loss=9.87845250070095
Loss made of: CE 0.555570125579834, LKD 2.918635368347168, LDE 7.488667011260986, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/31, Loss=9.235490760207176
Loss made of: CE 0.4953407049179077, LKD 3.1876933574676514, LDE 5.957769870758057, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/31, Loss=9.39710537791252
Loss made of: CE 0.6852468848228455, LKD 2.3036458492279053, LDE 9.523028373718262, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4800145924091339, Reg Loss=9.044755935668945
Clinet index 7, End of Epoch 4/6, Average Loss=9.524770736694336, Class Loss=0.4800145924091339, Reg Loss=9.044755935668945
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/31, Loss=9.039570134878158
Loss made of: CE 0.4043728709220886, LKD 2.627981185913086, LDE 5.333797454833984, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/31, Loss=9.033628445863723
Loss made of: CE 0.4328359067440033, LKD 2.770334243774414, LDE 5.491518020629883, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/31, Loss=9.40245595574379
Loss made of: CE 0.42788445949554443, LKD 3.106865406036377, LDE 5.9251556396484375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.45258915424346924, Reg Loss=8.677180290222168
Clinet index 7, End of Epoch 5/6, Average Loss=9.129769325256348, Class Loss=0.45258915424346924, Reg Loss=8.677180290222168
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/31, Loss=8.921619659662246
Loss made of: CE 0.4654383063316345, LKD 2.990527391433716, LDE 6.061768531799316, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/31, Loss=9.262194859981538
Loss made of: CE 0.45435065031051636, LKD 2.5426502227783203, LDE 6.462374210357666, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/31, Loss=8.519286176562309
Loss made of: CE 0.4940266013145447, LKD 3.076798915863037, LDE 4.873135089874268, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.44734513759613037, Reg Loss=8.43805980682373
Clinet index 7, End of Epoch 6/6, Average Loss=8.885404586791992, Class Loss=0.44734513759613037, Reg Loss=8.43805980682373
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/19, Loss=9.676615098118782
Loss made of: CE 0.40963658690452576, LKD 3.130154609680176, LDE 6.483099460601807, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5040914416313171, Reg Loss=9.130256652832031
Clinet index 4, End of Epoch 1/6, Average Loss=9.634347915649414, Class Loss=0.5040914416313171, Reg Loss=9.130256652832031
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/19, Loss=9.948284870386123
Loss made of: CE 0.4280986785888672, LKD 3.293243885040283, LDE 6.014510154724121, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.43943384289741516, Reg Loss=9.019082069396973
Clinet index 4, End of Epoch 2/6, Average Loss=9.458516120910645, Class Loss=0.43943384289741516, Reg Loss=9.019082069396973
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/19, Loss=9.50431503355503
Loss made of: CE 0.40228304266929626, LKD 3.1212503910064697, LDE 4.989030838012695, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.41510942578315735, Reg Loss=8.981467247009277
Clinet index 4, End of Epoch 3/6, Average Loss=9.396576881408691, Class Loss=0.41510942578315735, Reg Loss=8.981467247009277
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/19, Loss=9.068728375434876
Loss made of: CE 0.42062172293663025, LKD 3.3052544593811035, LDE 5.160170078277588, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4080258905887604, Reg Loss=8.860444068908691
Clinet index 4, End of Epoch 4/6, Average Loss=9.26846981048584, Class Loss=0.4080258905887604, Reg Loss=8.860444068908691
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/19, Loss=9.449500143527985
Loss made of: CE 0.4186245799064636, LKD 2.9135632514953613, LDE 5.579915523529053, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3942417502403259, Reg Loss=8.637969970703125
Clinet index 4, End of Epoch 5/6, Average Loss=9.032211303710938, Class Loss=0.3942417502403259, Reg Loss=8.637969970703125
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/19, Loss=8.896183770895004
Loss made of: CE 0.351320743560791, LKD 2.3990530967712402, LDE 4.9485673904418945, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3833147883415222, Reg Loss=8.829550743103027
Clinet index 4, End of Epoch 6/6, Average Loss=9.212865829467773, Class Loss=0.3833147883415222, Reg Loss=8.829550743103027
federated aggregation...
Validation, Class Loss=0.4203139841556549, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.871395
Mean Acc: 0.484200
FreqW Acc: 0.798601
Mean IoU: 0.392824
Class IoU:
	class 0: 0.90975094
	class 1: 0.10056691
	class 2: 0.005823179
	class 3: 0.00022571321
	class 4: 0.5021636
	class 5: 0.10453796
	class 6: 0.79504716
	class 7: 0.7323459
	class 8: 0.65824544
	class 9: 0.08118742
	class 10: 0.5535303
	class 11: 0.20876314
	class 12: 0.45451972
Class Acc:
	class 0: 0.9702923
	class 1: 0.100605726
	class 2: 0.0058441767
	class 3: 0.00022571321
	class 4: 0.5278423
	class 5: 0.10459152
	class 6: 0.81491256
	class 7: 0.7598654
	class 8: 0.7010505
	class 9: 0.13723545
	class 10: 0.86072874
	class 11: 0.58092195
	class 12: 0.7304779

federated global round: 13, step: 2
select part of clients to conduct local training
[14, 13, 0, 1]
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=9.093654957413673
Loss made of: CE 0.4737134873867035, LKD 3.4119784832000732, LDE 5.876177787780762, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4282751977443695, Reg Loss=9.24489974975586
Clinet index 14, End of Epoch 1/6, Average Loss=9.673174858093262, Class Loss=0.4282751977443695, Reg Loss=9.24489974975586
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=9.341452699899673
Loss made of: CE 0.49788904190063477, LKD 3.9693124294281006, LDE 5.991870880126953, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.38605496287345886, Reg Loss=8.93149185180664
Clinet index 14, End of Epoch 2/6, Average Loss=9.317546844482422, Class Loss=0.38605496287345886, Reg Loss=8.93149185180664
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=9.625796294212341
Loss made of: CE 0.3333955705165863, LKD 2.505826950073242, LDE 6.505852699279785, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.37182632088661194, Reg Loss=8.94198989868164
Clinet index 14, End of Epoch 3/6, Average Loss=9.31381607055664, Class Loss=0.37182632088661194, Reg Loss=8.94198989868164
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=9.063715621829033
Loss made of: CE 0.3271894156932831, LKD 3.297064781188965, LDE 5.171771049499512, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.363228976726532, Reg Loss=8.840633392333984
Clinet index 14, End of Epoch 4/6, Average Loss=9.203862190246582, Class Loss=0.363228976726532, Reg Loss=8.840633392333984
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=9.463364064693451
Loss made of: CE 0.3571179509162903, LKD 3.134469747543335, LDE 4.978221893310547, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3671097457408905, Reg Loss=8.939746856689453
Clinet index 14, End of Epoch 5/6, Average Loss=9.306856155395508, Class Loss=0.3671097457408905, Reg Loss=8.939746856689453
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=9.303976979851722
Loss made of: CE 0.3026377558708191, LKD 3.020141124725342, LDE 5.297334671020508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.36290809512138367, Reg Loss=8.516596794128418
Clinet index 14, End of Epoch 6/6, Average Loss=8.879505157470703, Class Loss=0.36290809512138367, Reg Loss=8.516596794128418
Current Client Index:  13
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/19, Loss=9.396474239230155
Loss made of: CE 0.4268810749053955, LKD 2.509769916534424, LDE 5.650722503662109, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.42377251386642456, Reg Loss=8.70796012878418
Clinet index 13, End of Epoch 1/6, Average Loss=9.131732940673828, Class Loss=0.42377251386642456, Reg Loss=8.70796012878418
Pseudo labeling is: None
Epoch 2, lr = 0.000714
Epoch 2, Batch 10/19, Loss=9.276990458369255
Loss made of: CE 0.5429850220680237, LKD 3.0978641510009766, LDE 7.11539363861084, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3890155255794525, Reg Loss=8.603806495666504
Clinet index 13, End of Epoch 2/6, Average Loss=8.99282169342041, Class Loss=0.3890155255794525, Reg Loss=8.603806495666504
Pseudo labeling is: None
Epoch 3, lr = 0.000655
Epoch 3, Batch 10/19, Loss=9.508646839857102
Loss made of: CE 0.33432480692863464, LKD 2.947798013687134, LDE 5.848689079284668, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3831917345523834, Reg Loss=8.962685585021973
Clinet index 13, End of Epoch 3/6, Average Loss=9.345877647399902, Class Loss=0.3831917345523834, Reg Loss=8.962685585021973
Pseudo labeling is: None
Epoch 4, lr = 0.000596
Epoch 4, Batch 10/19, Loss=8.8923859000206
Loss made of: CE 0.37391194701194763, LKD 2.3830881118774414, LDE 5.6589436531066895, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3672466278076172, Reg Loss=8.685198783874512
Clinet index 13, End of Epoch 4/6, Average Loss=9.052445411682129, Class Loss=0.3672466278076172, Reg Loss=8.685198783874512
Pseudo labeling is: None
Epoch 5, lr = 0.000536
Epoch 5, Batch 10/19, Loss=9.69434551000595
Loss made of: CE 0.3451624810695648, LKD 3.2284035682678223, LDE 4.558259010314941, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.36800315976142883, Reg Loss=9.070915222167969
Clinet index 13, End of Epoch 5/6, Average Loss=9.438918113708496, Class Loss=0.36800315976142883, Reg Loss=9.070915222167969
Pseudo labeling is: None
Epoch 6, lr = 0.000475
Epoch 6, Batch 10/19, Loss=8.850920310616493
Loss made of: CE 0.40119022130966187, LKD 2.9485557079315186, LDE 6.306671142578125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.35755547881126404, Reg Loss=8.689435958862305
Clinet index 13, End of Epoch 6/6, Average Loss=9.046991348266602, Class Loss=0.35755547881126404, Reg Loss=8.689435958862305
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=8.900117421150208
Loss made of: CE 0.4179670810699463, LKD 2.863161325454712, LDE 5.837075233459473, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.42332229018211365, Reg Loss=8.253117561340332
Clinet index 0, End of Epoch 1/6, Average Loss=8.676440238952637, Class Loss=0.42332229018211365, Reg Loss=8.253117561340332
Pseudo labeling is: None
Epoch 2, lr = 0.000525
Epoch 2, Batch 10/19, Loss=8.82424038350582
Loss made of: CE 0.4117550253868103, LKD 3.0340991020202637, LDE 5.840970039367676, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4002790153026581, Reg Loss=8.358263969421387
Clinet index 0, End of Epoch 2/6, Average Loss=8.758543014526367, Class Loss=0.4002790153026581, Reg Loss=8.358263969421387
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/19, Loss=9.567744892835616
Loss made of: CE 0.39885908365249634, LKD 3.531140089035034, LDE 4.9310150146484375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.39067816734313965, Reg Loss=8.599876403808594
Clinet index 0, End of Epoch 3/6, Average Loss=8.990554809570312, Class Loss=0.39067816734313965, Reg Loss=8.599876403808594
Pseudo labeling is: None
Epoch 4, lr = 0.000438
Epoch 4, Batch 10/19, Loss=8.599871987104416
Loss made of: CE 0.486575186252594, LKD 2.6538054943084717, LDE 6.036108016967773, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.38108617067337036, Reg Loss=8.728837013244629
Clinet index 0, End of Epoch 4/6, Average Loss=9.109923362731934, Class Loss=0.38108617067337036, Reg Loss=8.728837013244629
Pseudo labeling is: None
Epoch 5, lr = 0.000394
Epoch 5, Batch 10/19, Loss=9.108804488182068
Loss made of: CE 0.38448333740234375, LKD 3.192295789718628, LDE 5.334558486938477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.37185215950012207, Reg Loss=8.454458236694336
Clinet index 0, End of Epoch 5/6, Average Loss=8.826310157775879, Class Loss=0.37185215950012207, Reg Loss=8.454458236694336
Pseudo labeling is: None
Epoch 6, lr = 0.000350
Epoch 6, Batch 10/19, Loss=8.689314597845078
Loss made of: CE 0.4353710114955902, LKD 2.6938347816467285, LDE 4.65637731552124, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.37153270840644836, Reg Loss=8.353194236755371
Clinet index 0, End of Epoch 6/6, Average Loss=8.724726676940918, Class Loss=0.37153270840644836, Reg Loss=8.353194236755371
Current Client Index:  1
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=20.949435347318648
Loss made of: CE 0.7060006856918335, LKD 5.329463958740234, LDE 14.943511962890625, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/35, Loss=15.262195122241973
Loss made of: CE 0.5142788290977478, LKD 4.1408538818359375, LDE 8.600186347961426, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/35, Loss=13.449024030566216
Loss made of: CE 0.5262700915336609, LKD 5.166992664337158, LDE 9.35793685913086, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5668677687644958, Reg Loss=15.324700355529785
Clinet index 1, End of Epoch 1/6, Average Loss=15.891568183898926, Class Loss=0.5668677687644958, Reg Loss=15.324700355529785
Pseudo labeling is: None
Epoch 2, lr = 0.000584
Epoch 2, Batch 10/35, Loss=13.08263872563839
Loss made of: CE 0.39976367354393005, LKD 4.427141189575195, LDE 8.23146915435791, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/35, Loss=12.204473569989204
Loss made of: CE 0.4244229793548584, LKD 4.109421253204346, LDE 9.227993965148926, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/35, Loss=12.09939187169075
Loss made of: CE 0.3534866273403168, LKD 4.606775283813477, LDE 6.928682327270508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.38375353813171387, Reg Loss=11.983134269714355
Clinet index 1, End of Epoch 2/6, Average Loss=12.366888046264648, Class Loss=0.38375353813171387, Reg Loss=11.983134269714355
Pseudo labeling is: None
Epoch 3, lr = 0.000536
Epoch 3, Batch 10/35, Loss=11.349159885942935
Loss made of: CE 0.34931132197380066, LKD 5.234034538269043, LDE 6.767383098602295, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/35, Loss=11.669725304841995
Loss made of: CE 0.3817718029022217, LKD 4.912729740142822, LDE 7.021781921386719, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/35, Loss=11.44598346054554
Loss made of: CE 0.32307833433151245, LKD 4.896223545074463, LDE 7.65203857421875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3365321457386017, Reg Loss=11.054855346679688
Clinet index 1, End of Epoch 3/6, Average Loss=11.391387939453125, Class Loss=0.3365321457386017, Reg Loss=11.054855346679688
Pseudo labeling is: None
Epoch 4, lr = 0.000487
Epoch 4, Batch 10/35, Loss=11.234097239375114
Loss made of: CE 0.26479339599609375, LKD 3.928445339202881, LDE 6.612124919891357, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 4, Batch 20/35, Loss=11.016800053417683
Loss made of: CE 0.24797110259532928, LKD 3.310046672821045, LDE 8.305221557617188, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/35, Loss=10.928907322883607
Loss made of: CE 0.2716168165206909, LKD 3.7234420776367188, LDE 5.780481338500977, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3116947114467621, Reg Loss=10.760394096374512
Clinet index 1, End of Epoch 4/6, Average Loss=11.072089195251465, Class Loss=0.3116947114467621, Reg Loss=10.760394096374512
Pseudo labeling is: None
Epoch 5, lr = 0.000438
Epoch 5, Batch 10/35, Loss=11.190867885947227
Loss made of: CE 0.30576932430267334, LKD 4.4119696617126465, LDE 5.742588043212891, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/35, Loss=11.313578420877457
Loss made of: CE 0.3258870244026184, LKD 4.275178909301758, LDE 6.055159091949463, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/35, Loss=10.694589625298978
Loss made of: CE 0.29240232706069946, LKD 3.8422539234161377, LDE 5.972772598266602, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.30717065930366516, Reg Loss=10.757661819458008
Clinet index 1, End of Epoch 5/6, Average Loss=11.06483268737793, Class Loss=0.30717065930366516, Reg Loss=10.757661819458008
Pseudo labeling is: None
Epoch 6, lr = 0.000389
Epoch 6, Batch 10/35, Loss=10.303559567034245
Loss made of: CE 0.2312283217906952, LKD 3.589244842529297, LDE 5.660667896270752, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/35, Loss=11.232602816820144
Loss made of: CE 0.33766573667526245, LKD 4.817595958709717, LDE 6.191473960876465, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/35, Loss=11.213534024357795
Loss made of: CE 0.28515416383743286, LKD 4.13378381729126, LDE 5.4932098388671875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3040805757045746, Reg Loss=10.627321243286133
Clinet index 1, End of Epoch 6/6, Average Loss=10.931402206420898, Class Loss=0.3040805757045746, Reg Loss=10.627321243286133
federated aggregation...
Validation, Class Loss=0.4165803790092468, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.867046
Mean Acc: 0.474928
FreqW Acc: 0.792934
Mean IoU: 0.375848
Class IoU:
	class 0: 0.91114193
	class 1: 0.06213654
	class 2: 0.011447797
	class 3: 0.00010103108
	class 4: 0.52788436
	class 5: 0.13825776
	class 6: 0.789714
	class 7: 0.735375
	class 8: 0.5795994
	class 9: 0.07729457
	class 10: 0.37230924
	class 11: 0.23585892
	class 12: 0.44489858
Class Acc:
	class 0: 0.97226506
	class 1: 0.062146723
	class 2: 0.0115815615
	class 3: 0.00010103108
	class 4: 0.5519864
	class 5: 0.13832667
	class 6: 0.80867434
	class 7: 0.76336455
	class 8: 0.599652
	class 9: 0.11106118
	class 10: 0.9176207
	class 11: 0.5784908
	class 12: 0.6587876

federated global round: 14, step: 2
select part of clients to conduct local training
[16, 9, 4, 0]
Current Client Index:  16
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/42, Loss=11.772986036539077
Loss made of: CE 0.5374656915664673, LKD 4.241011142730713, LDE 8.0553560256958, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/42, Loss=10.884773877263068
Loss made of: CE 0.47495996952056885, LKD 3.5521857738494873, LDE 5.90961217880249, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/42, Loss=11.124660503864288
Loss made of: CE 0.4595406949520111, LKD 3.8573338985443115, LDE 6.009594917297363, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/42, Loss=10.088085648417472
Loss made of: CE 0.2850993871688843, LKD 4.035764217376709, LDE 4.867124557495117, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.467386394739151, Reg Loss=10.406695365905762
Clinet index 16, End of Epoch 1/6, Average Loss=10.8740816116333, Class Loss=0.467386394739151, Reg Loss=10.406695365905762
Pseudo labeling is: None
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/42, Loss=10.52230807542801
Loss made of: CE 0.3529077172279358, LKD 4.020551681518555, LDE 5.958081245422363, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/42, Loss=9.98820493221283
Loss made of: CE 0.31704556941986084, LKD 4.36411714553833, LDE 6.206409454345703, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/42, Loss=10.007096520066261
Loss made of: CE 0.3109126091003418, LKD 3.969564914703369, LDE 6.20722770690918, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/42, Loss=10.277582281827927
Loss made of: CE 0.3425614535808563, LKD 3.1830339431762695, LDE 5.346592426300049, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3593027889728546, Reg Loss=9.852102279663086
Clinet index 16, End of Epoch 2/6, Average Loss=10.211404800415039, Class Loss=0.3593027889728546, Reg Loss=9.852102279663086
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/42, Loss=9.60865993797779
Loss made of: CE 0.38504478335380554, LKD 3.5064473152160645, LDE 5.541853427886963, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/42, Loss=10.911490923166275
Loss made of: CE 0.3007739186286926, LKD 4.936886787414551, LDE 7.119529724121094, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/42, Loss=9.716205355525016
Loss made of: CE 0.30069953203201294, LKD 4.232397079467773, LDE 5.988958358764648, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/42, Loss=9.723595821857453
Loss made of: CE 0.35318130254745483, LKD 4.401820182800293, LDE 5.852654457092285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.35010966658592224, Reg Loss=9.63404655456543
Clinet index 16, End of Epoch 3/6, Average Loss=9.984156608581543, Class Loss=0.35010966658592224, Reg Loss=9.63404655456543
Pseudo labeling is: None
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/42, Loss=10.121283388137817
Loss made of: CE 0.3434007167816162, LKD 3.404630661010742, LDE 5.811800956726074, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/42, Loss=9.517863127589226
Loss made of: CE 0.3819522261619568, LKD 4.114445209503174, LDE 4.306983470916748, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/42, Loss=9.21436266452074
Loss made of: CE 0.24255602061748505, LKD 3.484992265701294, LDE 4.4246368408203125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/42, Loss=9.520093435049057
Loss made of: CE 0.3117224872112274, LKD 3.3348560333251953, LDE 5.498081207275391, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.34513232111930847, Reg Loss=9.335847854614258
Clinet index 16, End of Epoch 4/6, Average Loss=9.68097972869873, Class Loss=0.34513232111930847, Reg Loss=9.335847854614258
Pseudo labeling is: None
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/42, Loss=8.945145255327224
Loss made of: CE 0.3773352801799774, LKD 3.901005983352661, LDE 4.903355121612549, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/42, Loss=9.824735602736473
Loss made of: CE 0.42399176955223083, LKD 3.209660291671753, LDE 4.95280122756958, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/42, Loss=9.929672917723655
Loss made of: CE 0.3490370213985443, LKD 3.228076934814453, LDE 5.029417514801025, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/42, Loss=9.437495389580727
Loss made of: CE 0.3663495182991028, LKD 4.334776878356934, LDE 5.95428466796875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.3424239158630371, Reg Loss=9.201860427856445
Clinet index 16, End of Epoch 5/6, Average Loss=9.54428482055664, Class Loss=0.3424239158630371, Reg Loss=9.201860427856445
Pseudo labeling is: None
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/42, Loss=9.581149604916572
Loss made of: CE 0.3083980977535248, LKD 3.5279557704925537, LDE 4.452617645263672, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/42, Loss=9.327537164092064
Loss made of: CE 0.35136669874191284, LKD 3.4807567596435547, LDE 4.557634353637695, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/42, Loss=9.333590239286423
Loss made of: CE 0.3772619664669037, LKD 3.691662073135376, LDE 5.02687931060791, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/42, Loss=9.886802497506142
Loss made of: CE 0.40767616033554077, LKD 3.6067187786102295, LDE 5.372550964355469, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3427383303642273, Reg Loss=9.15195369720459
Clinet index 16, End of Epoch 6/6, Average Loss=9.494691848754883, Class Loss=0.3427383303642273, Reg Loss=9.15195369720459
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/35, Loss=17.34049246311188
Loss made of: CE 0.6054648756980896, LKD 4.821240425109863, LDE 11.021759033203125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/35, Loss=14.14455286860466
Loss made of: CE 0.35536426305770874, LKD 4.403247833251953, LDE 7.657273292541504, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/35, Loss=12.325112280249595
Loss made of: CE 0.36260080337524414, LKD 4.539982795715332, LDE 7.2440080642700195, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.48358047008514404, Reg Loss=13.702713966369629
Clinet index 9, End of Epoch 1/6, Average Loss=14.186294555664062, Class Loss=0.48358047008514404, Reg Loss=13.702713966369629
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/35, Loss=12.299968427419662
Loss made of: CE 0.404247522354126, LKD 3.8055100440979004, LDE 8.114693641662598, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/35, Loss=13.210284775495529
Loss made of: CE 0.41450774669647217, LKD 4.385434627532959, LDE 7.76229190826416, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/35, Loss=12.113620296120644
Loss made of: CE 0.3166274428367615, LKD 3.929443359375, LDE 6.481812953948975, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3468768298625946, Reg Loss=12.037694931030273
Clinet index 9, End of Epoch 2/6, Average Loss=12.38457202911377, Class Loss=0.3468768298625946, Reg Loss=12.037694931030273
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/35, Loss=11.819193196296691
Loss made of: CE 0.28321704268455505, LKD 4.557458400726318, LDE 6.421342372894287, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 20/35, Loss=12.596655538678169
Loss made of: CE 0.27947157621383667, LKD 3.743121862411499, LDE 7.233734607696533, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/35, Loss=11.787133327126503
Loss made of: CE 0.24658218026161194, LKD 3.9775967597961426, LDE 6.083118915557861, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.32585394382476807, Reg Loss=11.637699127197266
Clinet index 9, End of Epoch 3/6, Average Loss=11.963553428649902, Class Loss=0.32585394382476807, Reg Loss=11.637699127197266
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/35, Loss=12.003816762566567
Loss made of: CE 0.21153852343559265, LKD 3.664588212966919, LDE 7.716427803039551, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/35, Loss=11.156963795423508
Loss made of: CE 0.34724968671798706, LKD 4.055477619171143, LDE 5.218888282775879, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/35, Loss=11.682794252038002
Loss made of: CE 0.2762758135795593, LKD 3.9812655448913574, LDE 8.588628768920898, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.304872065782547, Reg Loss=11.1981782913208
Clinet index 9, End of Epoch 4/6, Average Loss=11.503050804138184, Class Loss=0.304872065782547, Reg Loss=11.1981782913208
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/35, Loss=10.368593345582486
Loss made of: CE 0.2890993058681488, LKD 3.591128349304199, LDE 7.022750377655029, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/35, Loss=10.891550789773465
Loss made of: CE 0.21948114037513733, LKD 3.9637439250946045, LDE 5.197975158691406, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/35, Loss=10.89023316204548
Loss made of: CE 0.29847514629364014, LKD 5.578125476837158, LDE 7.5842437744140625, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.30085113644599915, Reg Loss=10.427556991577148
Clinet index 9, End of Epoch 5/6, Average Loss=10.728407859802246, Class Loss=0.30085113644599915, Reg Loss=10.427556991577148
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/35, Loss=11.000002652406693
Loss made of: CE 0.2684251070022583, LKD 4.475841045379639, LDE 7.054107666015625, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/35, Loss=10.250198218226434
Loss made of: CE 0.3821749687194824, LKD 4.530843257904053, LDE 6.227654457092285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/35, Loss=10.514723128080368
Loss made of: CE 0.25259214639663696, LKD 4.1699042320251465, LDE 6.25374174118042, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.2943797707557678, Reg Loss=10.257673263549805
Clinet index 9, End of Epoch 6/6, Average Loss=10.552053451538086, Class Loss=0.2943797707557678, Reg Loss=10.257673263549805
Current Client Index:  4
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000568
Epoch 1, Batch 10/19, Loss=8.804521575570107
Loss made of: CE 0.3047165274620056, LKD 3.142629623413086, LDE 5.222555160522461, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.39686349034309387, Reg Loss=8.566665649414062
Clinet index 4, End of Epoch 1/6, Average Loss=8.963529586791992, Class Loss=0.39686349034309387, Reg Loss=8.566665649414062
Pseudo labeling is: None
Epoch 2, lr = 0.000482
Epoch 2, Batch 10/19, Loss=9.163178583979606
Loss made of: CE 0.39587366580963135, LKD 3.1142618656158447, LDE 5.726356506347656, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3878389000892639, Reg Loss=8.358811378479004
Clinet index 4, End of Epoch 2/6, Average Loss=8.746650695800781, Class Loss=0.3878389000892639, Reg Loss=8.358811378479004
Pseudo labeling is: None
Epoch 3, lr = 0.000394
Epoch 3, Batch 10/19, Loss=8.583004820346833
Loss made of: CE 0.36189353466033936, LKD 3.1470696926116943, LDE 4.297170162200928, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.3723413646221161, Reg Loss=8.182733535766602
Clinet index 4, End of Epoch 3/6, Average Loss=8.555074691772461, Class Loss=0.3723413646221161, Reg Loss=8.182733535766602
Pseudo labeling is: None
Epoch 4, lr = 0.000304
Epoch 4, Batch 10/19, Loss=8.401419129967689
Loss made of: CE 0.3740498423576355, LKD 3.1546859741210938, LDE 4.589846611022949, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3779214024543762, Reg Loss=8.055848121643066
Clinet index 4, End of Epoch 4/6, Average Loss=8.433769226074219, Class Loss=0.3779214024543762, Reg Loss=8.055848121643066
Pseudo labeling is: None
Epoch 5, lr = 0.000211
Epoch 5, Batch 10/19, Loss=8.671330738067628
Loss made of: CE 0.4051792025566101, LKD 3.1021487712860107, LDE 5.4888596534729, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.36805006861686707, Reg Loss=7.872896671295166
Clinet index 4, End of Epoch 5/6, Average Loss=8.240946769714355, Class Loss=0.36805006861686707, Reg Loss=7.872896671295166
Pseudo labeling is: None
Epoch 6, lr = 0.000113
Epoch 6, Batch 10/19, Loss=8.41185154914856
Loss made of: CE 0.3189949691295624, LKD 2.3395490646362305, LDE 3.987215518951416, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3655485510826111, Reg Loss=8.175213813781738
Clinet index 4, End of Epoch 6/6, Average Loss=8.540761947631836, Class Loss=0.3655485510826111, Reg Loss=8.175213813781738
Current Client Index:  0
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000304
Epoch 1, Batch 10/19, Loss=9.074046316742898
Loss made of: CE 0.40899038314819336, LKD 2.7612850666046143, LDE 7.202258110046387, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4014122486114502, Reg Loss=8.290597915649414
Clinet index 0, End of Epoch 1/6, Average Loss=8.692009925842285, Class Loss=0.4014122486114502, Reg Loss=8.290597915649414
Pseudo labeling is: None
Epoch 2, lr = 0.000258
Epoch 2, Batch 10/19, Loss=8.551172724366188
Loss made of: CE 0.38388702273368835, LKD 2.8987443447113037, LDE 5.238210678100586, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.3764004111289978, Reg Loss=8.109062194824219
Clinet index 0, End of Epoch 2/6, Average Loss=8.485462188720703, Class Loss=0.3764004111289978, Reg Loss=8.109062194824219
Pseudo labeling is: None
Epoch 3, lr = 0.000211
Epoch 3, Batch 10/19, Loss=9.424951919913292
Loss made of: CE 0.39701128005981445, LKD 3.799835205078125, LDE 4.604944229125977, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.38604408502578735, Reg Loss=8.354452133178711
Clinet index 0, End of Epoch 3/6, Average Loss=8.740496635437012, Class Loss=0.38604408502578735, Reg Loss=8.354452133178711
Pseudo labeling is: None
Epoch 4, lr = 0.000163
Epoch 4, Batch 10/19, Loss=8.419819474220276
Loss made of: CE 0.4622628092765808, LKD 2.8819596767425537, LDE 5.771636009216309, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3738352954387665, Reg Loss=8.490659713745117
Clinet index 0, End of Epoch 4/6, Average Loss=8.864495277404785, Class Loss=0.3738352954387665, Reg Loss=8.490659713745117
Pseudo labeling is: None
Epoch 5, lr = 0.000113
Epoch 5, Batch 10/19, Loss=8.602891522645951
Loss made of: CE 0.3877720832824707, LKD 3.0315334796905518, LDE 4.901622772216797, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.37071219086647034, Reg Loss=8.10066032409668
Clinet index 0, End of Epoch 5/6, Average Loss=8.471372604370117, Class Loss=0.37071219086647034, Reg Loss=8.10066032409668
Pseudo labeling is: None
Epoch 6, lr = 0.000061
Epoch 6, Batch 10/19, Loss=8.65912297964096
Loss made of: CE 0.4418588876724243, LKD 2.6259384155273438, LDE 4.452816963195801, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.37771520018577576, Reg Loss=8.260932922363281
Clinet index 0, End of Epoch 6/6, Average Loss=8.63864803314209, Class Loss=0.37771520018577576, Reg Loss=8.260932922363281
federated aggregation...
Validation, Class Loss=0.41522547602653503, Reg Loss=0.0 (without scaling)

Total samples: 1005.000000
Overall Acc: 0.861299
Mean Acc: 0.459173
FreqW Acc: 0.784528
Mean IoU: 0.369041
Class IoU:
	class 0: 0.9133857
	class 1: 0.06644475
	class 2: 0.009703564
	class 3: 0.0
	class 4: 0.51801884
	class 5: 0.14406769
	class 6: 0.79256964
	class 7: 0.73936087
	class 8: 0.22297871
	class 9: 0.06786815
	class 10: 0.6971857
	class 11: 0.2704062
	class 12: 0.35554665
Class Acc:
	class 0: 0.97317195
	class 1: 0.06646056
	class 2: 0.009867872
	class 3: 0.0
	class 4: 0.5429789
	class 5: 0.14416006
	class 6: 0.81136763
	class 7: 0.7807033
	class 8: 0.22538663
	class 9: 0.08417361
	class 10: 0.8372386
	class 11: 0.5608482
	class 12: 0.9328919

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 15, step: 3
select part of clients to conduct local training
[11, 6, 10, 7]
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Batch 10/102, Loss=15.71020200252533
Loss made of: CE 1.5857032537460327, LKD 4.182968616485596, LDE 7.6462626457214355, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=12.911926651000977
Loss made of: CE 1.136927843093872, LKD 4.591588497161865, LDE 6.344120979309082, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=11.324404036998748
Loss made of: CE 1.1071367263793945, LKD 4.82961368560791, LDE 5.3827314376831055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=11.524307692050934
Loss made of: CE 0.7853958606719971, LKD 4.718106746673584, LDE 5.371153354644775, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=11.588690340518951
Loss made of: CE 0.5801219344139099, LKD 4.434358596801758, LDE 6.215150356292725, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=11.353598105907441
Loss made of: CE 0.6979605555534363, LKD 4.07452917098999, LDE 4.446435928344727, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=10.325950294733047
Loss made of: CE 0.6188735365867615, LKD 4.516995906829834, LDE 5.362002372741699, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=10.595357012748718
Loss made of: CE 0.5815550088882446, LKD 4.5861968994140625, LDE 4.382892608642578, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=9.948912066221236
Loss made of: CE 0.5816226005554199, LKD 5.582113265991211, LDE 4.5365495681762695, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=10.169793879985809
Loss made of: CE 0.6083680391311646, LKD 5.605035305023193, LDE 5.044198989868164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.9584062099456787, Reg Loss=10.552165031433105
Clinet index 11, End of Epoch 1/6, Average Loss=11.510571479797363, Class Loss=0.9584062099456787, Reg Loss=10.552165031433105
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=10.104552471637726
Loss made of: CE 0.5167818069458008, LKD 4.254286766052246, LDE 4.744833946228027, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=9.761848047375679
Loss made of: CE 0.4566981792449951, LKD 3.7975127696990967, LDE 5.647960186004639, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=9.940695703029633
Loss made of: CE 0.5398423671722412, LKD 5.085993766784668, LDE 4.6708879470825195, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=9.830219089984894
Loss made of: CE 0.650557816028595, LKD 4.9925408363342285, LDE 4.539834976196289, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=9.93893835246563
Loss made of: CE 0.5697234272956848, LKD 5.287850856781006, LDE 4.903578758239746, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=10.029077410697937
Loss made of: CE 0.5682854652404785, LKD 5.554620265960693, LDE 4.112894058227539, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=9.366408556699753
Loss made of: CE 0.5557568073272705, LKD 5.703492164611816, LDE 4.030545711517334, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=10.352156311273575
Loss made of: CE 0.5847036838531494, LKD 5.962664604187012, LDE 5.139921188354492, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=9.375450178980827
Loss made of: CE 0.5631952285766602, LKD 5.087882041931152, LDE 5.490056991577148, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=9.717920035123825
Loss made of: CE 0.41473257541656494, LKD 4.416903018951416, LDE 5.521099090576172, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5521828532218933, Reg Loss=9.291288375854492
Clinet index 11, End of Epoch 2/6, Average Loss=9.84347152709961, Class Loss=0.5521828532218933, Reg Loss=9.291288375854492
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=9.613531273603439
Loss made of: CE 0.5059974193572998, LKD 5.090818881988525, LDE 4.028166770935059, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=9.427196416258813
Loss made of: CE 0.5375388860702515, LKD 4.690433979034424, LDE 3.7766239643096924, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=9.154285609722137
Loss made of: CE 0.5193407535552979, LKD 4.619316101074219, LDE 3.923879623413086, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=9.708547860383987
Loss made of: CE 0.5010544061660767, LKD 5.157683849334717, LDE 4.722945213317871, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=9.76371247768402
Loss made of: CE 0.5073720216751099, LKD 5.119077682495117, LDE 3.8787736892700195, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=9.697263729572295
Loss made of: CE 0.5497223138809204, LKD 4.473065376281738, LDE 4.721118450164795, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=9.401540911197662
Loss made of: CE 0.381058931350708, LKD 4.567509174346924, LDE 4.785020351409912, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=9.770833253860474
Loss made of: CE 0.48379620909690857, LKD 4.651226997375488, LDE 5.211519718170166, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=10.048344522714615
Loss made of: CE 0.4133285880088806, LKD 4.336899757385254, LDE 4.385482311248779, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=9.803761002421378
Loss made of: CE 0.4507574439048767, LKD 4.501369476318359, LDE 3.5865790843963623, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5021777749061584, Reg Loss=9.120662689208984
Clinet index 11, End of Epoch 3/6, Average Loss=9.622840881347656, Class Loss=0.5021777749061584, Reg Loss=9.120662689208984
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=9.8112146794796
Loss made of: CE 0.47474703192710876, LKD 4.188100814819336, LDE 4.237981796264648, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=9.643099102377892
Loss made of: CE 0.4663183093070984, LKD 3.8771345615386963, LDE 4.3959150314331055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=9.93921102285385
Loss made of: CE 0.5285629034042358, LKD 4.620092391967773, LDE 4.952140808105469, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=9.355860710144043
Loss made of: CE 0.48476240038871765, LKD 4.130073070526123, LDE 3.4594337940216064, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=9.368055918812752
Loss made of: CE 0.5219231843948364, LKD 4.669998645782471, LDE 4.181952476501465, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=9.57549332678318
Loss made of: CE 0.40685316920280457, LKD 4.020252227783203, LDE 4.259932994842529, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=9.429775062203408
Loss made of: CE 0.5628628134727478, LKD 5.188931465148926, LDE 3.6434710025787354, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=9.876953506469727
Loss made of: CE 0.573222279548645, LKD 4.7665934562683105, LDE 6.7010416984558105, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=9.486767148971557
Loss made of: CE 0.42531582713127136, LKD 4.2404913902282715, LDE 4.116207122802734, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=9.309746634960174
Loss made of: CE 0.502344012260437, LKD 4.2104387283325195, LDE 5.712486267089844, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4823930561542511, Reg Loss=9.109265327453613
Clinet index 11, End of Epoch 4/6, Average Loss=9.591658592224121, Class Loss=0.4823930561542511, Reg Loss=9.109265327453613
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=9.318144926428795
Loss made of: CE 0.44894593954086304, LKD 5.477231979370117, LDE 4.228773593902588, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=9.489019775390625
Loss made of: CE 0.4209376573562622, LKD 4.75089168548584, LDE 4.660336494445801, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=9.133565387129783
Loss made of: CE 0.38338249921798706, LKD 4.082674026489258, LDE 3.5842843055725098, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=9.05151874423027
Loss made of: CE 0.4779510498046875, LKD 4.661640167236328, LDE 3.4538023471832275, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=9.539774638414382
Loss made of: CE 0.5145758986473083, LKD 4.7396416664123535, LDE 5.102547645568848, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=9.01125451028347
Loss made of: CE 0.4553748071193695, LKD 6.120532989501953, LDE 4.76547384262085, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=8.854998838901519
Loss made of: CE 0.4871881604194641, LKD 4.93818473815918, LDE 4.283555507659912, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=9.094287899136543
Loss made of: CE 0.327956885099411, LKD 4.176797866821289, LDE 4.887808799743652, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=9.141344165802002
Loss made of: CE 0.5334514379501343, LKD 5.0065388679504395, LDE 3.2034544944763184, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=9.196159857511521
Loss made of: CE 0.48723292350769043, LKD 4.807459354400635, LDE 3.5812251567840576, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4611901342868805, Reg Loss=8.730264663696289
Clinet index 11, End of Epoch 5/6, Average Loss=9.191454887390137, Class Loss=0.4611901342868805, Reg Loss=8.730264663696289
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=9.359490823745727
Loss made of: CE 0.48752307891845703, LKD 3.979330539703369, LDE 3.85746431350708, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=8.869619178771973
Loss made of: CE 0.4835544526576996, LKD 5.7270331382751465, LDE 3.8735437393188477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.908301961421966
Loss made of: CE 0.5031445026397705, LKD 4.28327751159668, LDE 4.1253252029418945, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=9.619933873414993
Loss made of: CE 0.4611667990684509, LKD 4.270205020904541, LDE 4.52524995803833, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=9.219093623757363
Loss made of: CE 0.38937753438949585, LKD 4.668177127838135, LDE 3.4719479084014893, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=9.452654168009758
Loss made of: CE 0.45851588249206543, LKD 5.861636161804199, LDE 4.585564136505127, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=9.116544020175933
Loss made of: CE 0.48836931586265564, LKD 4.114288806915283, LDE 4.0369415283203125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=8.561964648962022
Loss made of: CE 0.4303998351097107, LKD 5.827420234680176, LDE 4.273425102233887, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=9.259883359074593
Loss made of: CE 0.4746759831905365, LKD 4.308170795440674, LDE 3.305115222930908, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=9.119709771871566
Loss made of: CE 0.44683894515037537, LKD 5.086562156677246, LDE 3.8455705642700195, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.45682796835899353, Reg Loss=8.707667350769043
Clinet index 11, End of Epoch 6/6, Average Loss=9.164495468139648, Class Loss=0.45682796835899353, Reg Loss=8.707667350769043
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=16.962158489227296
Loss made of: CE 1.3261336088180542, LKD 4.877408027648926, LDE 8.395902633666992, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=13.35780302286148
Loss made of: CE 1.2474054098129272, LKD 5.310939788818359, LDE 6.119317531585693, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.393430471420288, Reg Loss=13.474693298339844
Clinet index 6, End of Epoch 1/6, Average Loss=14.868124008178711, Class Loss=1.393430471420288, Reg Loss=13.474693298339844
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=12.025102108716965
Loss made of: CE 1.1893328428268433, LKD 5.452357292175293, LDE 5.679261684417725, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=11.606961238384248
Loss made of: CE 0.6841740608215332, LKD 4.782010555267334, LDE 5.261979579925537, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.9431321024894714, Reg Loss=10.821846961975098
Clinet index 6, End of Epoch 2/6, Average Loss=11.764979362487793, Class Loss=0.9431321024894714, Reg Loss=10.821846961975098
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=11.126967650651931
Loss made of: CE 0.7606838941574097, LKD 5.64359712600708, LDE 5.959799289703369, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=10.954472744464875
Loss made of: CE 0.6786251068115234, LKD 4.916510105133057, LDE 4.6808085441589355, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.717708170413971, Reg Loss=10.210107803344727
Clinet index 6, End of Epoch 3/6, Average Loss=10.927816390991211, Class Loss=0.717708170413971, Reg Loss=10.210107803344727
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Batch 10/23, Loss=10.843583750724793
Loss made of: CE 0.6993505358695984, LKD 6.379383087158203, LDE 5.229076385498047, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=10.468657577037812
Loss made of: CE 0.5197931528091431, LKD 4.944741249084473, LDE 5.79263973236084, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6237085461616516, Reg Loss=9.962597846984863
Clinet index 6, End of Epoch 4/6, Average Loss=10.58630657196045, Class Loss=0.6237085461616516, Reg Loss=9.962597846984863
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=10.199295046925545
Loss made of: CE 0.743925929069519, LKD 4.647281646728516, LDE 4.829041481018066, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=10.430554357171058
Loss made of: CE 0.4768216609954834, LKD 4.930825233459473, LDE 5.9629597663879395, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.549235999584198, Reg Loss=9.946698188781738
Clinet index 6, End of Epoch 5/6, Average Loss=10.49593448638916, Class Loss=0.549235999584198, Reg Loss=9.946698188781738
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=10.14443443417549
Loss made of: CE 0.5052626729011536, LKD 6.273059368133545, LDE 4.853734493255615, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=10.170768502354623
Loss made of: CE 0.4667843282222748, LKD 5.014842510223389, LDE 3.201202630996704, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5378857851028442, Reg Loss=9.603670120239258
Clinet index 6, End of Epoch 6/6, Average Loss=10.141555786132812, Class Loss=0.5378857851028442, Reg Loss=9.603670120239258
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=17.490373504161834
Loss made of: CE 1.5086225271224976, LKD 5.584778785705566, LDE 7.664402484893799, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=13.074841606616975
Loss made of: CE 1.1046106815338135, LKD 4.7865095138549805, LDE 6.515640735626221, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.3916088342666626, Reg Loss=13.500983238220215
Clinet index 10, End of Epoch 1/6, Average Loss=14.892592430114746, Class Loss=1.3916088342666626, Reg Loss=13.500983238220215
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/23, Loss=11.976582610607148
Loss made of: CE 1.0326452255249023, LKD 6.8628950119018555, LDE 7.399539470672607, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=10.984223318099975
Loss made of: CE 0.8284416198730469, LKD 6.392796993255615, LDE 5.400872230529785, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.9387527108192444, Reg Loss=10.369755744934082
Clinet index 10, End of Epoch 2/6, Average Loss=11.30850887298584, Class Loss=0.9387527108192444, Reg Loss=10.369755744934082
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/23, Loss=10.454719716310501
Loss made of: CE 0.6045247316360474, LKD 5.579197406768799, LDE 5.157960414886475, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=10.959100389480591
Loss made of: CE 0.5370420217514038, LKD 5.231448173522949, LDE 6.018034934997559, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7278270125389099, Reg Loss=9.964446067810059
Clinet index 10, End of Epoch 3/6, Average Loss=10.692273139953613, Class Loss=0.7278270125389099, Reg Loss=9.964446067810059
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/23, Loss=10.2678523093462
Loss made of: CE 0.49307164549827576, LKD 5.060791969299316, LDE 3.9786431789398193, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=10.855233347415924
Loss made of: CE 0.5598891973495483, LKD 5.179270267486572, LDE 4.190067768096924, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6198804378509521, Reg Loss=9.888129234313965
Clinet index 10, End of Epoch 4/6, Average Loss=10.508009910583496, Class Loss=0.6198804378509521, Reg Loss=9.888129234313965
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/23, Loss=10.970294764637947
Loss made of: CE 0.5009361505508423, LKD 5.714897632598877, LDE 3.5877935886383057, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=10.339185440540314
Loss made of: CE 0.6099953651428223, LKD 6.659467697143555, LDE 5.8394455909729, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5658096075057983, Reg Loss=10.080955505371094
Clinet index 10, End of Epoch 5/6, Average Loss=10.646764755249023, Class Loss=0.5658096075057983, Reg Loss=10.080955505371094
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/23, Loss=10.123742780089378
Loss made of: CE 0.5754479169845581, LKD 4.39927864074707, LDE 4.450078010559082, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=10.12647786140442
Loss made of: CE 0.5702301263809204, LKD 4.598931789398193, LDE 4.33633279800415, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5350522994995117, Reg Loss=9.560171127319336
Clinet index 10, End of Epoch 6/6, Average Loss=10.095223426818848, Class Loss=0.5350522994995117, Reg Loss=9.560171127319336
Current Client Index:  7
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=15.141626751422882
Loss made of: CE 1.7287976741790771, LKD 5.2663445472717285, LDE 6.704191207885742, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=12.62979723215103
Loss made of: CE 1.5366839170455933, LKD 4.8690361976623535, LDE 6.081676483154297, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=11.830749142169953
Loss made of: CE 1.1291625499725342, LKD 4.565271854400635, LDE 5.453425407409668, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=11.555725675821304
Loss made of: CE 0.8108329772949219, LKD 5.674973011016846, LDE 5.979902744293213, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=11.164195030927658
Loss made of: CE 0.812624454498291, LKD 4.938076019287109, LDE 5.221075534820557, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=10.700402194261551
Loss made of: CE 0.70624178647995, LKD 4.263210296630859, LDE 4.724782943725586, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=10.448395717144013
Loss made of: CE 0.6958487033843994, LKD 4.147889137268066, LDE 5.960313320159912, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=10.612609070539474
Loss made of: CE 0.656072199344635, LKD 5.579929828643799, LDE 4.705184459686279, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=9.850284296274186
Loss made of: CE 0.5957156419754028, LKD 4.643216133117676, LDE 3.877232313156128, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=10.043830454349518
Loss made of: CE 0.6269531846046448, LKD 4.572628974914551, LDE 4.609621524810791, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.9482787251472473, Reg Loss=10.423598289489746
Clinet index 7, End of Epoch 1/6, Average Loss=11.37187671661377, Class Loss=0.9482787251472473, Reg Loss=10.423598289489746
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/102, Loss=10.135563480854035
Loss made of: CE 0.5742383003234863, LKD 4.500472545623779, LDE 4.175217151641846, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=9.58743517100811
Loss made of: CE 0.5893256664276123, LKD 4.434202194213867, LDE 4.434915065765381, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=9.792089360952378
Loss made of: CE 0.5059913992881775, LKD 5.038419246673584, LDE 4.807350158691406, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=9.743001011013984
Loss made of: CE 0.5567535758018494, LKD 3.9861176013946533, LDE 4.476486682891846, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=9.924649432301521
Loss made of: CE 0.4967127740383148, LKD 4.122066974639893, LDE 4.069994926452637, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=9.932300087809562
Loss made of: CE 0.480803906917572, LKD 4.091705322265625, LDE 3.769624948501587, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=10.200315031409264
Loss made of: CE 0.5580644607543945, LKD 4.10692024230957, LDE 5.1572585105896, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=9.460903823375702
Loss made of: CE 0.486710786819458, LKD 4.377493381500244, LDE 4.236151218414307, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=9.9389133810997
Loss made of: CE 0.5192236304283142, LKD 4.374946594238281, LDE 4.180154323577881, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=9.49466356933117
Loss made of: CE 0.46384701132774353, LKD 4.481531620025635, LDE 5.518885612487793, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5512601733207703, Reg Loss=9.280770301818848
Clinet index 7, End of Epoch 2/6, Average Loss=9.832030296325684, Class Loss=0.5512601733207703, Reg Loss=9.280770301818848
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/102, Loss=9.846457460522652
Loss made of: CE 0.6148905754089355, LKD 5.005195140838623, LDE 3.7336275577545166, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=9.555479741096496
Loss made of: CE 0.4711622893810272, LKD 5.4253082275390625, LDE 3.8900182247161865, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=9.378125092387199
Loss made of: CE 0.47215914726257324, LKD 4.586508274078369, LDE 4.225135803222656, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=9.120894914865493
Loss made of: CE 0.4654483199119568, LKD 4.099668025970459, LDE 4.449146270751953, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=9.565050560235978
Loss made of: CE 0.5445479154586792, LKD 4.179599285125732, LDE 5.493669033050537, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=9.232331112027168
Loss made of: CE 0.5319790840148926, LKD 4.2122931480407715, LDE 3.7981576919555664, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=9.650314518809319
Loss made of: CE 0.5416571497917175, LKD 5.140823841094971, LDE 3.9317052364349365, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=9.751252871751785
Loss made of: CE 0.45834919810295105, LKD 6.0824079513549805, LDE 4.345365047454834, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=10.002852833271026
Loss made of: CE 0.4989873766899109, LKD 5.156004428863525, LDE 4.040371417999268, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=9.023438137769698
Loss made of: CE 0.4839889705181122, LKD 3.802889108657837, LDE 3.9428977966308594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5038046836853027, Reg Loss=8.98868465423584
Clinet index 7, End of Epoch 3/6, Average Loss=9.492488861083984, Class Loss=0.5038046836853027, Reg Loss=8.98868465423584
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/102, Loss=9.138824832439422
Loss made of: CE 0.4872008264064789, LKD 4.474617004394531, LDE 4.26505184173584, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=9.766009557247163
Loss made of: CE 0.38739413022994995, LKD 5.277068614959717, LDE 4.19760274887085, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=10.031466540694236
Loss made of: CE 0.5620795488357544, LKD 4.550818920135498, LDE 3.7253551483154297, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=9.706091785430909
Loss made of: CE 0.42724066972732544, LKD 4.142139911651611, LDE 3.894162654876709, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=9.36268804371357
Loss made of: CE 0.5011187195777893, LKD 4.636762619018555, LDE 3.8223206996917725, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=9.449973207712173
Loss made of: CE 0.5125681757926941, LKD 4.730309009552002, LDE 4.7113142013549805, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=9.222992545366287
Loss made of: CE 0.5038654804229736, LKD 4.5718207359313965, LDE 3.5856258869171143, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=9.48892051577568
Loss made of: CE 0.5074817538261414, LKD 5.10032320022583, LDE 4.316007137298584, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=9.624629709124566
Loss made of: CE 0.47114428877830505, LKD 3.5530917644500732, LDE 5.736050605773926, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=9.545483276247978
Loss made of: CE 0.495556116104126, LKD 4.511110305786133, LDE 4.047524929046631, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.48525506258010864, Reg Loss=9.041449546813965
Clinet index 7, End of Epoch 4/6, Average Loss=9.526704788208008, Class Loss=0.48525506258010864, Reg Loss=9.041449546813965
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/102, Loss=9.454737809300422
Loss made of: CE 0.5606879591941833, LKD 4.892034530639648, LDE 3.911870241165161, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=8.972889301180839
Loss made of: CE 0.585873544216156, LKD 4.553299903869629, LDE 3.6982059478759766, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=9.408184397220612
Loss made of: CE 0.4783225953578949, LKD 4.706809043884277, LDE 4.584394931793213, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=9.35397266149521
Loss made of: CE 0.4238440990447998, LKD 5.073642730712891, LDE 3.6939620971679688, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=8.855719676613807
Loss made of: CE 0.48068469762802124, LKD 4.741257667541504, LDE 3.8182411193847656, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=9.10741548538208
Loss made of: CE 0.5053771734237671, LKD 5.189879417419434, LDE 3.670940637588501, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=9.343762844800949
Loss made of: CE 0.4423622786998749, LKD 4.307609558105469, LDE 4.537417888641357, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=9.688443627953529
Loss made of: CE 0.4651850461959839, LKD 3.7994682788848877, LDE 4.661720275878906, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=9.320106607675552
Loss made of: CE 0.4853445887565613, LKD 4.988563060760498, LDE 4.31691312789917, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=9.293143665790558
Loss made of: CE 0.5388824939727783, LKD 4.578808784484863, LDE 3.939197540283203, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.46814870834350586, Reg Loss=8.8033447265625
Clinet index 7, End of Epoch 5/6, Average Loss=9.271493911743164, Class Loss=0.46814870834350586, Reg Loss=8.8033447265625
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/102, Loss=9.293595948815346
Loss made of: CE 0.4098939299583435, LKD 4.960433483123779, LDE 2.970289468765259, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=9.628685730695725
Loss made of: CE 0.4573175311088562, LKD 5.169780731201172, LDE 3.2062883377075195, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.964131695032119
Loss made of: CE 0.4131283760070801, LKD 5.143277168273926, LDE 3.354691743850708, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=9.526902759075165
Loss made of: CE 0.4631863534450531, LKD 4.276168346405029, LDE 3.3236260414123535, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=9.121164944767951
Loss made of: CE 0.4918642044067383, LKD 4.617588043212891, LDE 4.2062530517578125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=8.888306573033333
Loss made of: CE 0.4986170530319214, LKD 4.306867599487305, LDE 3.8085193634033203, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=8.83277731835842
Loss made of: CE 0.40233343839645386, LKD 4.816144943237305, LDE 4.144646167755127, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=9.273061671853066
Loss made of: CE 0.4573366045951843, LKD 4.895780563354492, LDE 4.864960670471191, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=9.180489897727966
Loss made of: CE 0.46807271242141724, LKD 5.124290943145752, LDE 3.470440149307251, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=8.801803362369537
Loss made of: CE 0.5298237800598145, LKD 4.599395275115967, LDE 3.6931748390197754, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.45196250081062317, Reg Loss=8.7184476852417
Clinet index 7, End of Epoch 6/6, Average Loss=9.17041015625, Class Loss=0.45196250081062317, Reg Loss=8.7184476852417
federated aggregation...
Validation, Class Loss=0.5415447950363159, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.827483
Mean Acc: 0.401157
FreqW Acc: 0.722885
Mean IoU: 0.298039
Class IoU:
	class 0: 0.8773684
	class 1: 0.00086790306
	class 2: 0.0
	class 3: 0.0
	class 4: 0.48427746
	class 5: 0.087054536
	class 6: 0.72717094
	class 7: 0.5671752
	class 8: 0.22619161
	class 9: 0.043242585
	class 10: 0.4928506
	class 11: 0.3266699
	class 12: 0.35964945
	class 13: 0.0
	class 14: 0.32603619
	class 15: 0.54657125
	class 16: 0.0015452886
Class Acc:
	class 0: 0.9720606
	class 1: 0.00086790306
	class 2: 0.0
	class 3: 0.0
	class 4: 0.50014347
	class 5: 0.08707864
	class 6: 0.75998205
	class 7: 0.57567674
	class 8: 0.22706969
	class 9: 0.04468189
	class 10: 0.7743945
	class 11: 0.4647672
	class 12: 0.81806546
	class 13: 0.0
	class 14: 0.9283819
	class 15: 0.664946
	class 16: 0.0015468497

federated global round: 16, step: 3
select part of clients to conduct local training
[7, 11, 16, 18]
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=9.707927197217941
Loss made of: CE 0.6512853503227234, LKD 4.940674304962158, LDE 3.780762195587158, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=9.680417269468307
Loss made of: CE 0.6295616626739502, LKD 4.454823970794678, LDE 4.40214729309082, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=9.453058731555938
Loss made of: CE 0.5065135359764099, LKD 4.170445442199707, LDE 4.405982494354248, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=9.773999357223511
Loss made of: CE 0.5486354827880859, LKD 5.548914432525635, LDE 4.463543891906738, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=9.785033193230628
Loss made of: CE 0.5879194140434265, LKD 4.726230621337891, LDE 4.362671852111816, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=9.517312064766884
Loss made of: CE 0.5122508406639099, LKD 4.228011608123779, LDE 3.5795540809631348, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=9.207981303334236
Loss made of: CE 0.4971073865890503, LKD 4.189589023590088, LDE 4.892144680023193, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=9.706127461791038
Loss made of: CE 0.5342068672180176, LKD 6.112244606018066, LDE 4.434401512145996, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=8.87671601176262
Loss made of: CE 0.5234515070915222, LKD 4.390440464019775, LDE 3.4232258796691895, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=9.194793671369553
Loss made of: CE 0.5089738368988037, LKD 4.582619667053223, LDE 3.783923387527466, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5236464738845825, Reg Loss=8.959784507751465
Clinet index 7, End of Epoch 1/6, Average Loss=9.483430862426758, Class Loss=0.5236464738845825, Reg Loss=8.959784507751465
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=9.256275129318237
Loss made of: CE 0.43994849920272827, LKD 4.2750678062438965, LDE 3.5048089027404785, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=8.986456900835037
Loss made of: CE 0.5040673017501831, LKD 4.587087631225586, LDE 3.8258676528930664, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=9.170915931463242
Loss made of: CE 0.4014891982078552, LKD 4.9476318359375, LDE 4.302974700927734, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=9.128181490302087
Loss made of: CE 0.5130552053451538, LKD 3.9531350135803223, LDE 4.289036273956299, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=9.28821339905262
Loss made of: CE 0.4586929678916931, LKD 4.106756687164307, LDE 3.764568328857422, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=9.163204777240754
Loss made of: CE 0.41493934392929077, LKD 4.074717998504639, LDE 3.288663148880005, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=9.642417296767235
Loss made of: CE 0.49202287197113037, LKD 4.414095878601074, LDE 4.392273902893066, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=8.780681133270264
Loss made of: CE 0.43222594261169434, LKD 4.3661651611328125, LDE 3.6951093673706055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=9.538212463259697
Loss made of: CE 0.470986545085907, LKD 4.2103400230407715, LDE 3.446559190750122, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=8.975048819184304
Loss made of: CE 0.4320337176322937, LKD 4.105461597442627, LDE 5.184641361236572, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4778500199317932, Reg Loss=8.733528137207031
Clinet index 7, End of Epoch 2/6, Average Loss=9.21137809753418, Class Loss=0.4778500199317932, Reg Loss=8.733528137207031
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=9.402785325050354
Loss made of: CE 0.5107021331787109, LKD 4.805436134338379, LDE 3.091639757156372, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=9.002269041538238
Loss made of: CE 0.4140655994415283, LKD 5.262941360473633, LDE 3.4100725650787354, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=9.039328664541245
Loss made of: CE 0.4566647410392761, LKD 4.778087139129639, LDE 3.792065382003784, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=8.745288681983947
Loss made of: CE 0.45113903284072876, LKD 3.8621201515197754, LDE 4.155065059661865, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=9.311877751350403
Loss made of: CE 0.499856173992157, LKD 4.3148884773254395, LDE 5.839396953582764, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=8.768413081765175
Loss made of: CE 0.5212900638580322, LKD 4.524211406707764, LDE 3.615453004837036, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=9.214896243810653
Loss made of: CE 0.5119981169700623, LKD 5.236511707305908, LDE 3.6935575008392334, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=9.186999240517617
Loss made of: CE 0.41871580481529236, LKD 5.957704544067383, LDE 3.812609910964966, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=9.458248847723008
Loss made of: CE 0.4852260947227478, LKD 5.352504730224609, LDE 3.5703084468841553, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=8.641912192106247
Loss made of: CE 0.46787571907043457, LKD 3.9578585624694824, LDE 3.453312873840332, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.46026021242141724, Reg Loss=8.601962089538574
Clinet index 7, End of Epoch 3/6, Average Loss=9.062222480773926, Class Loss=0.46026021242141724, Reg Loss=8.601962089538574
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=8.781606739759445
Loss made of: CE 0.4476354718208313, LKD 4.472630500793457, LDE 4.084629058837891, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=9.076199620962143
Loss made of: CE 0.35720905661582947, LKD 5.237660884857178, LDE 3.748311758041382, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=9.614281663298607
Loss made of: CE 0.541113018989563, LKD 4.722187042236328, LDE 3.1521027088165283, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=9.395868721604348
Loss made of: CE 0.3985712230205536, LKD 4.3664093017578125, LDE 3.8249597549438477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=8.892040649056435
Loss made of: CE 0.45074662566185, LKD 4.802124977111816, LDE 3.6829590797424316, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=8.97817597091198
Loss made of: CE 0.5026462078094482, LKD 4.796186923980713, LDE 4.390048980712891, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=8.951132750511169
Loss made of: CE 0.46393904089927673, LKD 4.471508979797363, LDE 3.4830875396728516, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=9.01774708032608
Loss made of: CE 0.4812602996826172, LKD 4.907804489135742, LDE 4.641817569732666, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=9.13732405602932
Loss made of: CE 0.4507730007171631, LKD 3.849931001663208, LDE 4.819058418273926, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=9.052452409267426
Loss made of: CE 0.49258512258529663, LKD 4.749652862548828, LDE 3.958876371383667, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.45602136850357056, Reg Loss=8.627030372619629
Clinet index 7, End of Epoch 4/6, Average Loss=9.083051681518555, Class Loss=0.45602136850357056, Reg Loss=8.627030372619629
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=9.108238032460212
Loss made of: CE 0.5305246114730835, LKD 4.909980297088623, LDE 3.530298948287964, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=8.643247658014298
Loss made of: CE 0.5377384424209595, LKD 4.4303388595581055, LDE 2.9818906784057617, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=9.150898405909539
Loss made of: CE 0.41385024785995483, LKD 4.730603218078613, LDE 4.174975872039795, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=9.033054828643799
Loss made of: CE 0.3836173415184021, LKD 4.8836894035339355, LDE 3.686372995376587, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=8.45796050131321
Loss made of: CE 0.43131470680236816, LKD 4.6579999923706055, LDE 3.7298240661621094, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=8.849722889065742
Loss made of: CE 0.45893311500549316, LKD 5.140703201293945, LDE 3.2841973304748535, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=9.030592685937881
Loss made of: CE 0.42888540029525757, LKD 4.298885822296143, LDE 4.520068645477295, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=9.264191496372224
Loss made of: CE 0.4538390338420868, LKD 3.784546375274658, LDE 3.9896674156188965, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=9.123017078638076
Loss made of: CE 0.4686734080314636, LKD 5.343999862670898, LDE 3.7894833087921143, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=8.849865582585334
Loss made of: CE 0.4980590343475342, LKD 4.381158828735352, LDE 3.9738707542419434, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.44503000378608704, Reg Loss=8.500516891479492
Clinet index 7, End of Epoch 5/6, Average Loss=8.945547103881836, Class Loss=0.44503000378608704, Reg Loss=8.500516891479492
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=9.044222509860992
Loss made of: CE 0.3966773450374603, LKD 5.22987174987793, LDE 3.187448501586914, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=9.122089099884032
Loss made of: CE 0.43257397413253784, LKD 5.103147983551025, LDE 2.988956928253174, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.856810948252678
Loss made of: CE 0.3976483941078186, LKD 5.076415061950684, LDE 3.366572380065918, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=9.200205463171006
Loss made of: CE 0.44734400510787964, LKD 4.242642879486084, LDE 3.2154977321624756, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=8.840019419789314
Loss made of: CE 0.473018616437912, LKD 4.497810363769531, LDE 4.08762264251709, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=8.47622825205326
Loss made of: CE 0.4379776418209076, LKD 4.135332107543945, LDE 3.8751044273376465, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=8.578959974646569
Loss made of: CE 0.38802099227905273, LKD 4.858935832977295, LDE 3.8980557918548584, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=9.076214203238488
Loss made of: CE 0.44692015647888184, LKD 5.196784019470215, LDE 4.11614990234375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=8.822647076845168
Loss made of: CE 0.43520301580429077, LKD 5.274081230163574, LDE 3.288994073867798, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=8.53271050453186
Loss made of: CE 0.5069595575332642, LKD 4.444850921630859, LDE 3.4215614795684814, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4356624186038971, Reg Loss=8.442973136901855
Clinet index 7, End of Epoch 6/6, Average Loss=8.87863540649414, Class Loss=0.4356624186038971, Reg Loss=8.442973136901855
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/102, Loss=9.567786133289337
Loss made of: CE 0.5711367726325989, LKD 4.308617115020752, LDE 5.2634711265563965, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=10.056643784046173
Loss made of: CE 0.6039313077926636, LKD 4.316514492034912, LDE 4.792929649353027, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=9.283258414268493
Loss made of: CE 0.4765692949295044, LKD 4.8019609451293945, LDE 3.900832414627075, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=9.71150599718094
Loss made of: CE 0.48660415410995483, LKD 4.627418518066406, LDE 4.13983154296875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=10.125275951623916
Loss made of: CE 0.43163785338401794, LKD 3.999343156814575, LDE 5.655343532562256, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=10.236378875374793
Loss made of: CE 0.4779056906700134, LKD 3.991769552230835, LDE 3.910937786102295, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=9.336347085237502
Loss made of: CE 0.4837825298309326, LKD 4.470607280731201, LDE 4.5541300773620605, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=9.571690008044243
Loss made of: CE 0.4449567198753357, LKD 4.553549766540527, LDE 3.592883825302124, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=8.915415820479392
Loss made of: CE 0.4613582491874695, LKD 5.119353294372559, LDE 3.9953935146331787, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=9.379195812344552
Loss made of: CE 0.4599297046661377, LKD 5.569689750671387, LDE 4.459801197052002, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5229359865188599, Reg Loss=9.087103843688965
Clinet index 11, End of Epoch 1/6, Average Loss=9.610039710998535, Class Loss=0.5229359865188599, Reg Loss=9.087103843688965
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/102, Loss=9.2020194709301
Loss made of: CE 0.41270825266838074, LKD 4.0300750732421875, LDE 3.8556272983551025, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=8.879461792111396
Loss made of: CE 0.43591877818107605, LKD 3.875600576400757, LDE 4.951226234436035, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=9.125695019960403
Loss made of: CE 0.4432123303413391, LKD 5.118616580963135, LDE 3.711160659790039, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=9.249029269814491
Loss made of: CE 0.5071281790733337, LKD 5.113276958465576, LDE 4.35451078414917, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=9.395152646303178
Loss made of: CE 0.49785682559013367, LKD 5.180666923522949, LDE 4.637354850769043, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=9.309956964850425
Loss made of: CE 0.5166404247283936, LKD 4.947202682495117, LDE 3.4590911865234375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=8.60345564186573
Loss made of: CE 0.49537205696105957, LKD 6.0497941970825195, LDE 3.4416959285736084, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=9.580619639158249
Loss made of: CE 0.5428380966186523, LKD 6.261621952056885, LDE 5.17848539352417, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=8.736228716373443
Loss made of: CE 0.4772113263607025, LKD 4.747612953186035, LDE 4.7993340492248535, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=9.031791293621064
Loss made of: CE 0.38371437788009644, LKD 4.511055946350098, LDE 5.00731086730957, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.47589945793151855, Reg Loss=8.645009994506836
Clinet index 11, End of Epoch 2/6, Average Loss=9.120909690856934, Class Loss=0.47589945793151855, Reg Loss=8.645009994506836
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/102, Loss=9.148162046074868
Loss made of: CE 0.44062668085098267, LKD 5.260840892791748, LDE 3.767470598220825, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=8.984721517562866
Loss made of: CE 0.4590633809566498, LKD 4.611950397491455, LDE 3.275949478149414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=8.694705027341843
Loss made of: CE 0.49661874771118164, LKD 4.56329870223999, LDE 3.9141197204589844, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=9.287966012954712
Loss made of: CE 0.4830799102783203, LKD 5.138586044311523, LDE 4.379561424255371, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=9.202165615558624
Loss made of: CE 0.49976369738578796, LKD 5.177281856536865, LDE 3.4692752361297607, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=9.10409454703331
Loss made of: CE 0.48179158568382263, LKD 4.2219414710998535, LDE 4.110925674438477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=8.995953565835952
Loss made of: CE 0.3527901768684387, LKD 4.235705852508545, LDE 4.342394828796387, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=9.30265606045723
Loss made of: CE 0.46262404322624207, LKD 4.285656452178955, LDE 4.778085231781006, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=9.50380049943924
Loss made of: CE 0.35285061597824097, LKD 4.424337863922119, LDE 3.9005701541900635, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=9.393786099553108
Loss made of: CE 0.4360995292663574, LKD 4.427590370178223, LDE 3.6888561248779297, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.46052634716033936, Reg Loss=8.691115379333496
Clinet index 11, End of Epoch 3/6, Average Loss=9.151641845703125, Class Loss=0.46052634716033936, Reg Loss=8.691115379333496
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/102, Loss=9.37326928973198
Loss made of: CE 0.42044568061828613, LKD 4.083745002746582, LDE 3.753162145614624, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=9.130027246475219
Loss made of: CE 0.45326346158981323, LKD 3.702836513519287, LDE 3.9043262004852295, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=9.568880718946456
Loss made of: CE 0.5002080202102661, LKD 4.995141506195068, LDE 4.856915473937988, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=8.873899886012078
Loss made of: CE 0.4200116991996765, LKD 4.131374835968018, LDE 3.23638916015625, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=9.017863672971725
Loss made of: CE 0.4965108335018158, LKD 4.844675064086914, LDE 3.772307872772217, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=9.120571997761726
Loss made of: CE 0.36462339758872986, LKD 3.8366594314575195, LDE 4.015776634216309, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=8.946459424495696
Loss made of: CE 0.5042320489883423, LKD 5.218336582183838, LDE 3.010518789291382, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=9.491417655348778
Loss made of: CE 0.5380850434303284, LKD 4.859232425689697, LDE 6.326460838317871, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=9.152964276075362
Loss made of: CE 0.4031461179256439, LKD 4.42862606048584, LDE 3.661691665649414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=8.94774844944477
Loss made of: CE 0.4758947789669037, LKD 4.163883686065674, LDE 5.251307964324951, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4536355137825012, Reg Loss=8.712933540344238
Clinet index 11, End of Epoch 4/6, Average Loss=9.166568756103516, Class Loss=0.4536355137825012, Reg Loss=8.712933540344238
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=9.00755036175251
Loss made of: CE 0.41884875297546387, LKD 5.587157726287842, LDE 3.666926145553589, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=9.318080806732178
Loss made of: CE 0.44946691393852234, LKD 5.174683094024658, LDE 4.070398807525635, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=8.990777316689492
Loss made of: CE 0.3778584897518158, LKD 4.090695858001709, LDE 3.5797173976898193, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=8.80488293170929
Loss made of: CE 0.4400894045829773, LKD 4.5947394371032715, LDE 3.2988553047180176, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=9.213575759530068
Loss made of: CE 0.502779483795166, LKD 4.685507297515869, LDE 4.5636515617370605, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=8.806257992982864
Loss made of: CE 0.41707658767700195, LKD 5.8912882804870605, LDE 4.3493523597717285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=8.324029216170311
Loss made of: CE 0.4431566596031189, LKD 4.8297905921936035, LDE 3.471832752227783, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=8.727050322294236
Loss made of: CE 0.30132967233657837, LKD 4.243020057678223, LDE 4.340696334838867, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=8.85979314148426
Loss made of: CE 0.5474306344985962, LKD 5.418255805969238, LDE 3.466977119445801, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=8.815420362353326
Loss made of: CE 0.47897666692733765, LKD 4.5890092849731445, LDE 3.473043441772461, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.43964409828186035, Reg Loss=8.452582359313965
Clinet index 11, End of Epoch 5/6, Average Loss=8.892226219177246, Class Loss=0.43964409828186035, Reg Loss=8.452582359313965
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Epoch 6, Batch 10/102, Loss=9.173348784446716
Loss made of: CE 0.45032423734664917, LKD 4.031954288482666, LDE 3.515822410583496, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=8.626945632696152
Loss made of: CE 0.4110385775566101, LKD 5.402712345123291, LDE 3.470289707183838, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.594351315498352
Loss made of: CE 0.4874536991119385, LKD 4.496103286743164, LDE 4.3978376388549805, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=9.139466381072998
Loss made of: CE 0.43346041440963745, LKD 4.575697898864746, LDE 3.664860486984253, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=8.91676968038082
Loss made of: CE 0.36393094062805176, LKD 4.354013442993164, LDE 3.563997983932495, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=8.90214576125145
Loss made of: CE 0.40320885181427, LKD 5.541549205780029, LDE 4.102630138397217, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=8.724584862589836
Loss made of: CE 0.43855470418930054, LKD 4.192073345184326, LDE 3.3600659370422363, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=8.256578096747399
Loss made of: CE 0.4164925515651703, LKD 5.720695972442627, LDE 3.791311025619507, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=8.91813929080963
Loss made of: CE 0.4415183663368225, LKD 4.26308536529541, LDE 3.1011900901794434, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=8.77831706404686
Loss made of: CE 0.44242382049560547, LKD 5.423917770385742, LDE 3.591033935546875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.43466857075691223, Reg Loss=8.389130592346191
Clinet index 11, End of Epoch 6/6, Average Loss=8.823799133300781, Class Loss=0.43466857075691223, Reg Loss=8.389130592346191
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/105, Loss=10.405195462703706
Loss made of: CE 0.782119631767273, LKD 3.8956143856048584, LDE 5.804681301116943, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/105, Loss=9.869233053922652
Loss made of: CE 0.59743332862854, LKD 4.496292591094971, LDE 4.761282920837402, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/105, Loss=9.16761080622673
Loss made of: CE 0.5227400064468384, LKD 4.4758195877075195, LDE 4.406764984130859, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/105, Loss=9.702553898096085
Loss made of: CE 0.5816223621368408, LKD 4.448732852935791, LDE 3.8305270671844482, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/105, Loss=9.177502673864364
Loss made of: CE 0.49471282958984375, LKD 3.7439398765563965, LDE 3.536890745162964, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/105, Loss=9.035055604577064
Loss made of: CE 0.5955163240432739, LKD 6.037621974945068, LDE 5.188877105712891, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/105, Loss=9.65970907509327
Loss made of: CE 0.4242064356803894, LKD 4.098901271820068, LDE 4.358720302581787, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/105, Loss=10.066833212971687
Loss made of: CE 0.5631997585296631, LKD 4.277421474456787, LDE 6.575196266174316, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/105, Loss=9.1325518399477
Loss made of: CE 0.5374985933303833, LKD 4.432236194610596, LDE 3.3827483654022217, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/105, Loss=9.322360983490944
Loss made of: CE 0.44937723875045776, LKD 4.489147186279297, LDE 4.038876533508301, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5491538643836975, Reg Loss=9.028000831604004
Clinet index 16, End of Epoch 1/6, Average Loss=9.577155113220215, Class Loss=0.5491538643836975, Reg Loss=9.028000831604004
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/105, Loss=9.185626477003098
Loss made of: CE 0.43468570709228516, LKD 3.781801462173462, LDE 3.925673723220825, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/105, Loss=8.963776516914368
Loss made of: CE 0.507575273513794, LKD 4.705458641052246, LDE 3.9401485919952393, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/105, Loss=8.94273155927658
Loss made of: CE 0.5160424709320068, LKD 4.73007869720459, LDE 5.1543354988098145, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/105, Loss=9.463282316923141
Loss made of: CE 0.4623754918575287, LKD 4.110378742218018, LDE 5.048744201660156, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/105, Loss=8.947350996732713
Loss made of: CE 0.5280749201774597, LKD 4.990999221801758, LDE 3.6286633014678955, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/105, Loss=9.125364044308663
Loss made of: CE 0.4608723819255829, LKD 3.6514806747436523, LDE 4.119058132171631, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/105, Loss=9.196639314293861
Loss made of: CE 0.5363477468490601, LKD 4.911391258239746, LDE 4.346738338470459, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/105, Loss=9.217344266176223
Loss made of: CE 0.5089837312698364, LKD 3.931363582611084, LDE 4.1113104820251465, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/105, Loss=9.56824300289154
Loss made of: CE 0.4858929514884949, LKD 4.994786739349365, LDE 3.249516010284424, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/105, Loss=9.776114571094514
Loss made of: CE 0.5377976894378662, LKD 4.768588542938232, LDE 4.07278299331665, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.49525463581085205, Reg Loss=8.75674057006836
Clinet index 16, End of Epoch 2/6, Average Loss=9.251995086669922, Class Loss=0.49525463581085205, Reg Loss=8.75674057006836
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/105, Loss=9.128458860516549
Loss made of: CE 0.4690573215484619, LKD 5.205900192260742, LDE 3.8002188205718994, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/105, Loss=9.06866337954998
Loss made of: CE 0.4616553783416748, LKD 4.4797234535217285, LDE 4.137029647827148, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/105, Loss=8.708600196242333
Loss made of: CE 0.43316400051116943, LKD 3.973036527633667, LDE 4.421136856079102, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/105, Loss=9.535432109236718
Loss made of: CE 0.5443295836448669, LKD 4.302465915679932, LDE 4.706186294555664, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/105, Loss=9.008544489741325
Loss made of: CE 0.5003490447998047, LKD 5.024013996124268, LDE 4.133471965789795, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/105, Loss=9.232756912708282
Loss made of: CE 0.5675846338272095, LKD 5.590861797332764, LDE 3.8401670455932617, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/105, Loss=8.76427997648716
Loss made of: CE 0.4637565612792969, LKD 3.7129859924316406, LDE 4.271191596984863, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/105, Loss=9.401159951090813
Loss made of: CE 0.5405830144882202, LKD 4.290018081665039, LDE 4.150215148925781, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/105, Loss=9.310081842541695
Loss made of: CE 0.4888172149658203, LKD 4.758231163024902, LDE 3.870579719543457, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/105, Loss=9.31101461648941
Loss made of: CE 0.4584676921367645, LKD 4.371200084686279, LDE 4.204805374145508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4757673144340515, Reg Loss=8.674571990966797
Clinet index 16, End of Epoch 3/6, Average Loss=9.150339126586914, Class Loss=0.4757673144340515, Reg Loss=8.674571990966797
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/105, Loss=9.007678052783012
Loss made of: CE 0.39600324630737305, LKD 3.6617562770843506, LDE 3.5104103088378906, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/105, Loss=8.681500652432442
Loss made of: CE 0.5090070366859436, LKD 4.4174485206604, LDE 2.9820268154144287, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/105, Loss=8.791433283686638
Loss made of: CE 0.4306744933128357, LKD 4.548354625701904, LDE 3.6200881004333496, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/105, Loss=9.106964653730392
Loss made of: CE 0.37648507952690125, LKD 4.24177885055542, LDE 3.6447677612304688, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/105, Loss=8.988651981949806
Loss made of: CE 0.5123900771141052, LKD 4.836385726928711, LDE 3.585635185241699, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/105, Loss=9.238608068227768
Loss made of: CE 0.5073344111442566, LKD 4.957009315490723, LDE 4.029240608215332, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/105, Loss=9.202846065163612
Loss made of: CE 0.40930575132369995, LKD 3.8992443084716797, LDE 3.6901419162750244, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/105, Loss=8.688882809877395
Loss made of: CE 0.3803372383117676, LKD 3.503828763961792, LDE 4.03828239440918, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/105, Loss=8.82061764895916
Loss made of: CE 0.47613662481307983, LKD 3.7004940509796143, LDE 4.40990686416626, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/105, Loss=8.97727561891079
Loss made of: CE 0.4552692174911499, LKD 4.781941890716553, LDE 3.6981277465820312, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4616234004497528, Reg Loss=8.494346618652344
Clinet index 16, End of Epoch 4/6, Average Loss=8.95596981048584, Class Loss=0.4616234004497528, Reg Loss=8.494346618652344
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/105, Loss=9.5434479534626
Loss made of: CE 0.5521166920661926, LKD 4.707246780395508, LDE 4.901556491851807, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/105, Loss=9.311730387806893
Loss made of: CE 0.41141483187675476, LKD 4.203915596008301, LDE 3.7724626064300537, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/105, Loss=8.735213688015937
Loss made of: CE 0.41201895475387573, LKD 3.717334508895874, LDE 3.382629632949829, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/105, Loss=8.911227160692215
Loss made of: CE 0.4604763984680176, LKD 4.742371082305908, LDE 5.330269813537598, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/105, Loss=9.104006898403167
Loss made of: CE 0.5157635807991028, LKD 4.510883331298828, LDE 6.1047563552856445, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/105, Loss=9.284648954868317
Loss made of: CE 0.4268719553947449, LKD 4.370244979858398, LDE 4.527093410491943, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/105, Loss=8.838224256038666
Loss made of: CE 0.41961437463760376, LKD 4.882626056671143, LDE 3.797088146209717, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/105, Loss=8.719721952080727
Loss made of: CE 0.4073938727378845, LKD 4.5174946784973145, LDE 3.1167354583740234, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/105, Loss=9.540016528964042
Loss made of: CE 0.5067926645278931, LKD 4.976068019866943, LDE 3.7861428260803223, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/105, Loss=8.783415138721466
Loss made of: CE 0.4490377902984619, LKD 3.73417067527771, LDE 3.63956618309021, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.456670880317688, Reg Loss=8.62236499786377
Clinet index 16, End of Epoch 5/6, Average Loss=9.079035758972168, Class Loss=0.456670880317688, Reg Loss=8.62236499786377
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/105, Loss=9.040993025898933
Loss made of: CE 0.4408457279205322, LKD 4.863300323486328, LDE 3.9296648502349854, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/105, Loss=9.73199584186077
Loss made of: CE 0.5300348401069641, LKD 5.425044059753418, LDE 6.73533821105957, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/105, Loss=9.031186282634735
Loss made of: CE 0.4508056044578552, LKD 4.698349475860596, LDE 3.3964622020721436, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/105, Loss=8.754430317878723
Loss made of: CE 0.41569018363952637, LKD 4.534592628479004, LDE 3.0184779167175293, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/105, Loss=9.090517842769623
Loss made of: CE 0.42059680819511414, LKD 4.375442981719971, LDE 3.589768409729004, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/105, Loss=9.386141729354858
Loss made of: CE 0.43484658002853394, LKD 5.310742378234863, LDE 4.2384843826293945, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/105, Loss=8.691925683617592
Loss made of: CE 0.4434056282043457, LKD 5.396720886230469, LDE 4.310667514801025, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/105, Loss=8.788405922055244
Loss made of: CE 0.4659595489501953, LKD 3.6685984134674072, LDE 5.255522727966309, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/105, Loss=8.369615852832794
Loss made of: CE 0.3508976399898529, LKD 4.235812664031982, LDE 3.0764009952545166, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/105, Loss=8.531672883033753
Loss made of: CE 0.38988250494003296, LKD 4.983439922332764, LDE 3.6516382694244385, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.44550442695617676, Reg Loss=8.501237869262695
Clinet index 16, End of Epoch 6/6, Average Loss=8.946742057800293, Class Loss=0.44550442695617676, Reg Loss=8.501237869262695
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=9.938288360834122
Loss made of: CE 0.5689982771873474, LKD 4.58162260055542, LDE 3.580029010772705, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=10.185528936982156
Loss made of: CE 0.5210980176925659, LKD 4.875882625579834, LDE 5.011923313140869, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=9.932520222663879
Loss made of: CE 0.4796116352081299, LKD 5.127593994140625, LDE 4.097817420959473, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=9.863451752066613
Loss made of: CE 0.4990174472332001, LKD 4.690704822540283, LDE 4.778343200683594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=9.532363930344582
Loss made of: CE 0.45113325119018555, LKD 4.604255199432373, LDE 3.300046920776367, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=9.77915462255478
Loss made of: CE 0.5810753107070923, LKD 4.8103766441345215, LDE 3.8877804279327393, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=9.45217373073101
Loss made of: CE 0.5385385751724243, LKD 4.162078857421875, LDE 4.570106506347656, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=9.437737172842025
Loss made of: CE 0.45482224225997925, LKD 4.218259334564209, LDE 4.330286026000977, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=9.444142204523086
Loss made of: CE 0.49275287985801697, LKD 5.546144962310791, LDE 3.7629575729370117, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=9.72549803853035
Loss made of: CE 0.47623953223228455, LKD 5.281410217285156, LDE 3.9941868782043457, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5155095458030701, Reg Loss=9.217672348022461
Clinet index 18, End of Epoch 1/6, Average Loss=9.733181953430176, Class Loss=0.5155095458030701, Reg Loss=9.217672348022461
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/102, Loss=8.964243918657303
Loss made of: CE 0.4515467584133148, LKD 4.978835105895996, LDE 3.7645394802093506, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=9.7610443264246
Loss made of: CE 0.5437082052230835, LKD 5.948139190673828, LDE 4.238378047943115, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=9.634164568781852
Loss made of: CE 0.5457079410552979, LKD 4.430323600769043, LDE 4.442763328552246, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=9.038568314909934
Loss made of: CE 0.4461376667022705, LKD 3.9780220985412598, LDE 4.6767096519470215, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=9.285699698328973
Loss made of: CE 0.5827192068099976, LKD 5.023561477661133, LDE 3.5381243228912354, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=9.66208193898201
Loss made of: CE 0.5219794511795044, LKD 6.149407386779785, LDE 4.763735294342041, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=8.88818608224392
Loss made of: CE 0.3958587646484375, LKD 4.199640274047852, LDE 3.329787492752075, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=9.600081619620322
Loss made of: CE 0.5466983914375305, LKD 4.97641658782959, LDE 6.080297946929932, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=8.649891337752342
Loss made of: CE 0.5374106764793396, LKD 4.413930892944336, LDE 3.90279221534729, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=9.26466911137104
Loss made of: CE 0.5021439790725708, LKD 4.7799458503723145, LDE 3.82889461517334, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4708872139453888, Reg Loss=8.821432113647461
Clinet index 18, End of Epoch 2/6, Average Loss=9.292319297790527, Class Loss=0.4708872139453888, Reg Loss=8.821432113647461
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/102, Loss=8.693162712454797
Loss made of: CE 0.4312981963157654, LKD 5.356834411621094, LDE 3.4413111209869385, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=9.243536055088043
Loss made of: CE 0.4693567752838135, LKD 4.030328750610352, LDE 4.20880126953125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=9.698931017518044
Loss made of: CE 0.4607747793197632, LKD 3.756774663925171, LDE 4.745038032531738, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=9.773864564299583
Loss made of: CE 0.4677170515060425, LKD 4.932254791259766, LDE 3.480271577835083, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=9.043820026516915
Loss made of: CE 0.44764575362205505, LKD 4.217194080352783, LDE 4.038496971130371, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=8.902121296525001
Loss made of: CE 0.4250276982784271, LKD 4.204030513763428, LDE 3.711803674697876, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=9.196028211712838
Loss made of: CE 0.423433780670166, LKD 5.302996635437012, LDE 4.430248737335205, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=8.954292219877242
Loss made of: CE 0.39112645387649536, LKD 3.7109382152557373, LDE 3.7479872703552246, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=9.419923609495163
Loss made of: CE 0.40824100375175476, LKD 4.412525177001953, LDE 3.9891741275787354, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=9.440638357400895
Loss made of: CE 0.4654774069786072, LKD 4.142127513885498, LDE 3.4911434650421143, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4536231458187103, Reg Loss=8.794637680053711
Clinet index 18, End of Epoch 3/6, Average Loss=9.248260498046875, Class Loss=0.4536231458187103, Reg Loss=8.794637680053711
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/102, Loss=9.459954085946084
Loss made of: CE 0.4852902591228485, LKD 4.160373687744141, LDE 4.0100908279418945, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=9.106151106953622
Loss made of: CE 0.45040711760520935, LKD 4.684308052062988, LDE 4.130351543426514, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=9.227528649568558
Loss made of: CE 0.34687861800193787, LKD 5.353017807006836, LDE 4.555141925811768, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=9.075976526737213
Loss made of: CE 0.5011858940124512, LKD 6.157587051391602, LDE 4.7137250900268555, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=9.360660284757614
Loss made of: CE 0.4671742618083954, LKD 4.521813869476318, LDE 4.335395812988281, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=9.215597921609879
Loss made of: CE 0.47700366377830505, LKD 4.8070454597473145, LDE 3.7745308876037598, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=9.006754010915756
Loss made of: CE 0.3532315492630005, LKD 3.650402545928955, LDE 5.097469806671143, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=8.777623942494392
Loss made of: CE 0.393496036529541, LKD 4.918376922607422, LDE 3.507554531097412, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=9.370834320783615
Loss made of: CE 0.4025808274745941, LKD 4.149449348449707, LDE 4.072957515716553, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=9.44831357896328
Loss made of: CE 0.46818193793296814, LKD 4.744356155395508, LDE 3.1883962154388428, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.44243258237838745, Reg Loss=8.750906944274902
Clinet index 18, End of Epoch 4/6, Average Loss=9.193339347839355, Class Loss=0.44243258237838745, Reg Loss=8.750906944274902
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/102, Loss=8.768972599506379
Loss made of: CE 0.41493964195251465, LKD 4.454981803894043, LDE 3.6922171115875244, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=9.31064893603325
Loss made of: CE 0.42730969190597534, LKD 4.234687805175781, LDE 3.988403797149658, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=8.821451130509377
Loss made of: CE 0.4410008192062378, LKD 4.767171382904053, LDE 3.522308111190796, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=8.90753418803215
Loss made of: CE 0.41887855529785156, LKD 4.657671928405762, LDE 3.2569382190704346, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=9.331172931194306
Loss made of: CE 0.426230788230896, LKD 4.4478559494018555, LDE 3.3682570457458496, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=8.954357540607452
Loss made of: CE 0.4533081650733948, LKD 3.9302923679351807, LDE 3.9402987957000732, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=8.882643184065818
Loss made of: CE 0.4433813989162445, LKD 5.653031826019287, LDE 4.0539727210998535, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=8.859618890285493
Loss made of: CE 0.4901045262813568, LKD 4.6640095710754395, LDE 3.9698119163513184, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=8.883058747649192
Loss made of: CE 0.5044259428977966, LKD 4.6269707679748535, LDE 4.2154974937438965, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=8.672631362080574
Loss made of: CE 0.400511234998703, LKD 4.568338394165039, LDE 3.238285779953003, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4387407600879669, Reg Loss=8.50550365447998
Clinet index 18, End of Epoch 5/6, Average Loss=8.944244384765625, Class Loss=0.4387407600879669, Reg Loss=8.50550365447998
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/102, Loss=9.021028682589531
Loss made of: CE 0.42283686995506287, LKD 5.082595348358154, LDE 4.532124042510986, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=8.975995618104935
Loss made of: CE 0.44352778792381287, LKD 4.23554801940918, LDE 4.040632247924805, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=9.08006634414196
Loss made of: CE 0.33904406428337097, LKD 5.682196617126465, LDE 4.267728328704834, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=8.953498205542564
Loss made of: CE 0.4431190490722656, LKD 5.022161483764648, LDE 3.712151288986206, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=9.37324417233467
Loss made of: CE 0.36877474188804626, LKD 4.270222187042236, LDE 3.9408798217773438, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=9.231385871767998
Loss made of: CE 0.4225273132324219, LKD 4.762643337249756, LDE 5.5539116859436035, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=9.053800114989281
Loss made of: CE 0.402639240026474, LKD 4.2455525398254395, LDE 4.5553741455078125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=9.779704752564431
Loss made of: CE 0.5377495884895325, LKD 4.964059829711914, LDE 3.15282940864563, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=8.983578917384147
Loss made of: CE 0.49938544631004333, LKD 4.78488302230835, LDE 4.571261882781982, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=9.183239647746086
Loss made of: CE 0.42724475264549255, LKD 4.782474994659424, LDE 4.231770992279053, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4337075352668762, Reg Loss=8.72213077545166
Clinet index 18, End of Epoch 6/6, Average Loss=9.155838012695312, Class Loss=0.4337075352668762, Reg Loss=8.72213077545166
federated aggregation...
Validation, Class Loss=0.4826802909374237, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.847383
Mean Acc: 0.446062
FreqW Acc: 0.750246
Mean IoU: 0.348972
Class IoU:
	class 0: 0.8885685
	class 1: 0.08239978
	class 2: 0.0
	class 3: 0.0
	class 4: 0.5026103
	class 5: 0.060567092
	class 6: 0.7311354
	class 7: 0.67199427
	class 8: 0.22209989
	class 9: 0.040500995
	class 10: 0.44004747
	class 11: 0.31513235
	class 12: 0.36672884
	class 13: 0.44219604
	class 14: 0.5066574
	class 15: 0.6617971
	class 16: 8.982921e-05
Class Acc:
	class 0: 0.96752244
	class 1: 0.0824079
	class 2: 0.0
	class 3: 0.0
	class 4: 0.52262247
	class 5: 0.06057738
	class 6: 0.7592283
	class 7: 0.6897733
	class 8: 0.2230371
	class 9: 0.042402793
	class 10: 0.5430893
	class 11: 0.49110207
	class 12: 0.8465085
	class 13: 0.5978576
	class 14: 0.88945353
	class 15: 0.86737853
	class 16: 8.982921e-05

federated global round: 17, step: 3
select part of clients to conduct local training
[5, 3, 17, 11]
Current Client Index:  5
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=8.575226938724517
Loss made of: CE 0.46177172660827637, LKD 3.783949375152588, LDE 3.091956615447998, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=8.730392354726792
Loss made of: CE 0.3878122568130493, LKD 4.629413604736328, LDE 3.7531700134277344, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=8.975447815656661
Loss made of: CE 0.4618716239929199, LKD 4.150919437408447, LDE 3.3548243045806885, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=8.574928161501884
Loss made of: CE 0.4506121575832367, LKD 4.394712448120117, LDE 3.1620845794677734, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=9.014176803827286
Loss made of: CE 0.4467872977256775, LKD 4.535026550292969, LDE 3.5347812175750732, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=8.917678168416023
Loss made of: CE 0.5906180143356323, LKD 5.014587879180908, LDE 3.306190013885498, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=8.929580435156822
Loss made of: CE 0.4717141389846802, LKD 4.594315052032471, LDE 4.6515326499938965, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=9.322236502170563
Loss made of: CE 0.41878849267959595, LKD 3.8313567638397217, LDE 4.473469257354736, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=8.670665618777274
Loss made of: CE 0.4062530994415283, LKD 3.975217342376709, LDE 3.812932252883911, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=8.735352206230164
Loss made of: CE 0.40685033798217773, LKD 5.071865081787109, LDE 2.6033167839050293, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.43523478507995605, Reg Loss=8.407504081726074
Clinet index 5, End of Epoch 1/6, Average Loss=8.84273910522461, Class Loss=0.43523478507995605, Reg Loss=8.407504081726074
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=9.490109199285508
Loss made of: CE 0.4112482964992523, LKD 4.370525360107422, LDE 3.9270236492156982, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=9.14068229496479
Loss made of: CE 0.4252344071865082, LKD 4.440991401672363, LDE 3.345890998840332, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=8.804617524147034
Loss made of: CE 0.40041959285736084, LKD 5.364611625671387, LDE 3.373929023742676, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=8.86201025545597
Loss made of: CE 0.3740541934967041, LKD 4.6227498054504395, LDE 3.2848246097564697, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=9.076128956675529
Loss made of: CE 0.424208402633667, LKD 4.420717716217041, LDE 3.309210777282715, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=9.394997045397758
Loss made of: CE 0.4030400514602661, LKD 4.189867973327637, LDE 4.104822158813477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=8.395174404978752
Loss made of: CE 0.4436313509941101, LKD 4.231381416320801, LDE 3.5728089809417725, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=9.021163967251777
Loss made of: CE 0.48700767755508423, LKD 5.369266033172607, LDE 4.291615009307861, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=8.78045438528061
Loss made of: CE 0.44556891918182373, LKD 4.576503276824951, LDE 2.9550015926361084, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=9.32133848965168
Loss made of: CE 0.3828589618206024, LKD 5.328627586364746, LDE 4.970459461212158, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4288952648639679, Reg Loss=8.596527099609375
Clinet index 5, End of Epoch 2/6, Average Loss=9.025422096252441, Class Loss=0.4288952648639679, Reg Loss=8.596527099609375
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=8.930525270104408
Loss made of: CE 0.34186601638793945, LKD 4.268791675567627, LDE 4.09703254699707, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=8.827845379710197
Loss made of: CE 0.3618090748786926, LKD 4.158165454864502, LDE 3.0214576721191406, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=8.51340019106865
Loss made of: CE 0.41439875960350037, LKD 5.188724994659424, LDE 3.4120054244995117, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=8.861588129401207
Loss made of: CE 0.35336610674858093, LKD 4.112702369689941, LDE 4.111084461212158, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=9.107503283023835
Loss made of: CE 0.4358639717102051, LKD 4.819709777832031, LDE 3.6367740631103516, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=8.81222131550312
Loss made of: CE 0.4734277129173279, LKD 5.76500129699707, LDE 4.19672966003418, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=8.817490884661675
Loss made of: CE 0.4150794446468353, LKD 5.5277533531188965, LDE 4.336183547973633, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=9.451002898812295
Loss made of: CE 0.4623100161552429, LKD 4.34363317489624, LDE 3.402482748031616, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=9.069687700271606
Loss made of: CE 0.34423673152923584, LKD 4.078157901763916, LDE 3.6385960578918457, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=8.681417143344879
Loss made of: CE 0.3804405629634857, LKD 4.152307510375977, LDE 4.420851707458496, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4250662624835968, Reg Loss=8.460409164428711
Clinet index 5, End of Epoch 3/6, Average Loss=8.885475158691406, Class Loss=0.4250662624835968, Reg Loss=8.460409164428711
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=9.576794704794883
Loss made of: CE 0.40114688873291016, LKD 4.521078109741211, LDE 3.366288185119629, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=8.59362089931965
Loss made of: CE 0.43565744161605835, LKD 4.447251319885254, LDE 3.665748119354248, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=9.165137019753455
Loss made of: CE 0.35474929213523865, LKD 3.6338181495666504, LDE 3.9851672649383545, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=9.166413807868958
Loss made of: CE 0.4213433265686035, LKD 4.3812642097473145, LDE 3.4653148651123047, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=8.098018062114715
Loss made of: CE 0.3530561923980713, LKD 5.006972789764404, LDE 3.918566942214966, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=8.707650437951088
Loss made of: CE 0.4170519709587097, LKD 4.560147762298584, LDE 4.029826641082764, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=8.708059653639793
Loss made of: CE 0.3386210799217224, LKD 3.5283167362213135, LDE 4.899148464202881, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=8.956051501631737
Loss made of: CE 0.39178216457366943, LKD 3.8066723346710205, LDE 3.5304884910583496, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=8.782440784573556
Loss made of: CE 0.4883897304534912, LKD 4.3927202224731445, LDE 3.660825729370117, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=9.048915526270866
Loss made of: CE 0.457235187292099, LKD 5.118566036224365, LDE 4.302749156951904, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4183393120765686, Reg Loss=8.455327033996582
Clinet index 5, End of Epoch 4/6, Average Loss=8.873666763305664, Class Loss=0.4183393120765686, Reg Loss=8.455327033996582
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=8.611311784386634
Loss made of: CE 0.4405837059020996, LKD 5.293260097503662, LDE 3.6296515464782715, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=9.061014890670776
Loss made of: CE 0.40112823247909546, LKD 4.567749500274658, LDE 3.5222535133361816, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=8.549616605043411
Loss made of: CE 0.4455980658531189, LKD 3.96738600730896, LDE 3.7795722484588623, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=8.956645160913467
Loss made of: CE 0.4656895101070404, LKD 5.14512300491333, LDE 3.3251073360443115, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=8.581251722574233
Loss made of: CE 0.41489362716674805, LKD 4.603308200836182, LDE 3.929701328277588, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=8.754768493771554
Loss made of: CE 0.412432998418808, LKD 4.703148365020752, LDE 3.2898898124694824, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=8.693132197856903
Loss made of: CE 0.32869952917099, LKD 4.063811302185059, LDE 3.6869139671325684, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=8.475242757797242
Loss made of: CE 0.4034268856048584, LKD 4.103396415710449, LDE 3.7089858055114746, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=8.98536487519741
Loss made of: CE 0.30996474623680115, LKD 4.913426399230957, LDE 4.345882892608643, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=8.621997404098511
Loss made of: CE 0.37358972430229187, LKD 4.309426784515381, LDE 3.5974106788635254, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.41627785563468933, Reg Loss=8.297513008117676
Clinet index 5, End of Epoch 5/6, Average Loss=8.713790893554688, Class Loss=0.41627785563468933, Reg Loss=8.297513008117676
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=8.559081116318703
Loss made of: CE 0.4006273150444031, LKD 4.28851842880249, LDE 3.9276528358459473, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=8.456170824170112
Loss made of: CE 0.40505450963974, LKD 4.252587795257568, LDE 4.066615104675293, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.937962377071381
Loss made of: CE 0.4529739320278168, LKD 4.421473979949951, LDE 3.333332061767578, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=8.61056716144085
Loss made of: CE 0.4167900085449219, LKD 4.397641658782959, LDE 3.089357852935791, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=8.508948668837547
Loss made of: CE 0.5402444005012512, LKD 4.155622482299805, LDE 5.1311821937561035, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=8.417925581336021
Loss made of: CE 0.3958314061164856, LKD 4.148005962371826, LDE 3.266265869140625, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=8.692521446943283
Loss made of: CE 0.48503798246383667, LKD 4.157858848571777, LDE 3.655701160430908, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=8.18845455944538
Loss made of: CE 0.4147803485393524, LKD 3.9763689041137695, LDE 3.9660465717315674, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=8.806300893425941
Loss made of: CE 0.34135347604751587, LKD 5.187387466430664, LDE 3.6434576511383057, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=8.877339774370194
Loss made of: CE 0.48065489530563354, LKD 4.6968512535095215, LDE 3.3327839374542236, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4089319407939911, Reg Loss=8.193673133850098
Clinet index 5, End of Epoch 6/6, Average Loss=8.602604866027832, Class Loss=0.4089319407939911, Reg Loss=8.193673133850098
Current Client Index:  3
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=8.682890433073045
Loss made of: CE 0.5034220814704895, LKD 4.650192737579346, LDE 5.218833923339844, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=8.72676739692688
Loss made of: CE 0.4428255259990692, LKD 4.899075031280518, LDE 2.9904370307922363, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=9.087127354741096
Loss made of: CE 0.4671494960784912, LKD 4.981675624847412, LDE 3.3328123092651367, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=8.884094086289405
Loss made of: CE 0.47591376304626465, LKD 4.443689823150635, LDE 7.738367557525635, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=9.133468887209892
Loss made of: CE 0.4441797733306885, LKD 5.1375732421875, LDE 3.9532690048217773, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=8.991574797034264
Loss made of: CE 0.4017990529537201, LKD 4.884350299835205, LDE 4.117441177368164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=8.65989280641079
Loss made of: CE 0.34298598766326904, LKD 4.141343593597412, LDE 3.6687002182006836, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=8.561070680618286
Loss made of: CE 0.5373665690422058, LKD 4.493032932281494, LDE 3.288792848587036, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=9.383360886573792
Loss made of: CE 0.49464279413223267, LKD 5.016989707946777, LDE 3.6754980087280273, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=9.269592955708504
Loss made of: CE 0.4068520665168762, LKD 4.267218589782715, LDE 3.211967706680298, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.43130168318748474, Reg Loss=8.540645599365234
Clinet index 3, End of Epoch 1/6, Average Loss=8.97194766998291, Class Loss=0.43130168318748474, Reg Loss=8.540645599365234
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/102, Loss=9.037517777085304
Loss made of: CE 0.4401647448539734, LKD 4.663822174072266, LDE 5.068869113922119, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=9.210774603486062
Loss made of: CE 0.431476354598999, LKD 4.538762092590332, LDE 4.137531757354736, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=9.18201465010643
Loss made of: CE 0.4097149968147278, LKD 4.798852443695068, LDE 3.8257572650909424, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=8.883034572005272
Loss made of: CE 0.43320250511169434, LKD 4.455484390258789, LDE 3.274894952774048, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=8.922855082154275
Loss made of: CE 0.4500938653945923, LKD 4.385640621185303, LDE 3.026845693588257, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=8.73763057589531
Loss made of: CE 0.40448158979415894, LKD 4.191727638244629, LDE 4.059457778930664, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=8.818219342827797
Loss made of: CE 0.3811269998550415, LKD 4.275222301483154, LDE 3.929583787918091, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=8.758808121085167
Loss made of: CE 0.39628303050994873, LKD 5.147522926330566, LDE 5.136495113372803, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=9.036757203936578
Loss made of: CE 0.41185280680656433, LKD 5.165863037109375, LDE 3.8239405155181885, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=8.847100123763084
Loss made of: CE 0.3506016135215759, LKD 4.280453205108643, LDE 3.5160071849823, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4271342158317566, Reg Loss=8.507034301757812
Clinet index 3, End of Epoch 2/6, Average Loss=8.934168815612793, Class Loss=0.4271342158317566, Reg Loss=8.507034301757812
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/102, Loss=9.482687029242516
Loss made of: CE 0.39909884333610535, LKD 4.2070536613464355, LDE 3.4637608528137207, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=8.83044147491455
Loss made of: CE 0.3703610897064209, LKD 4.212463855743408, LDE 4.414073944091797, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=8.961882731318473
Loss made of: CE 0.39450573921203613, LKD 3.654486656188965, LDE 5.115654945373535, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=8.64437073469162
Loss made of: CE 0.4094344973564148, LKD 4.2763190269470215, LDE 3.468637228012085, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=9.404310548305512
Loss made of: CE 0.40317654609680176, LKD 3.9605274200439453, LDE 5.331479549407959, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=8.842965415120124
Loss made of: CE 0.4176136255264282, LKD 4.781851291656494, LDE 3.1721413135528564, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=8.858076718449592
Loss made of: CE 0.2891674339771271, LKD 3.8987717628479004, LDE 3.778531312942505, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=8.464972841739655
Loss made of: CE 0.401674747467041, LKD 4.058029651641846, LDE 3.35849666595459, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=8.56225390434265
Loss made of: CE 0.39240893721580505, LKD 5.690644264221191, LDE 2.942065954208374, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=8.854642322659492
Loss made of: CE 0.5074450969696045, LKD 4.373072147369385, LDE 5.248203277587891, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4173144996166229, Reg Loss=8.468517303466797
Clinet index 3, End of Epoch 3/6, Average Loss=8.885831832885742, Class Loss=0.4173144996166229, Reg Loss=8.468517303466797
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/102, Loss=8.885746815800667
Loss made of: CE 0.42414093017578125, LKD 5.075353622436523, LDE 3.306550979614258, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=9.276583018898965
Loss made of: CE 0.3856753706932068, LKD 4.066490173339844, LDE 3.986011505126953, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=9.355481874942779
Loss made of: CE 0.44789013266563416, LKD 6.1930460929870605, LDE 3.4485552310943604, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=9.138757088780403
Loss made of: CE 0.4102081060409546, LKD 5.151424884796143, LDE 3.0643489360809326, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=8.855427464842796
Loss made of: CE 0.3752705752849579, LKD 4.7179460525512695, LDE 3.786646842956543, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=8.917789530754089
Loss made of: CE 0.4292164742946625, LKD 5.342888832092285, LDE 3.591714859008789, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=8.85827415883541
Loss made of: CE 0.509842038154602, LKD 4.987465858459473, LDE 4.143039703369141, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=8.91542702615261
Loss made of: CE 0.2849251627922058, LKD 4.692679405212402, LDE 4.171231269836426, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=8.86680665910244
Loss made of: CE 0.36279281973838806, LKD 4.82715368270874, LDE 3.186671018600464, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=8.837536054849625
Loss made of: CE 0.4328934848308563, LKD 5.199715614318848, LDE 3.30930757522583, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.41532015800476074, Reg Loss=8.570099830627441
Clinet index 3, End of Epoch 4/6, Average Loss=8.985420227050781, Class Loss=0.41532015800476074, Reg Loss=8.570099830627441
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/102, Loss=8.42835958302021
Loss made of: CE 0.4622846841812134, LKD 4.245182037353516, LDE 4.23258638381958, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=8.926722374558448
Loss made of: CE 0.4148493707180023, LKD 5.6928300857543945, LDE 4.392630577087402, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=8.890961703658103
Loss made of: CE 0.47556787729263306, LKD 4.00189733505249, LDE 3.470348834991455, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=9.087641295790672
Loss made of: CE 0.4520878791809082, LKD 4.956714630126953, LDE 3.020407199859619, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=8.830667501688003
Loss made of: CE 0.3903077244758606, LKD 5.636153221130371, LDE 3.9894065856933594, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=9.03824778497219
Loss made of: CE 0.3666480481624603, LKD 4.60680627822876, LDE 3.7551238536834717, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=9.115516844391824
Loss made of: CE 0.3728989064693451, LKD 4.044009208679199, LDE 3.629094362258911, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=8.796305105090141
Loss made of: CE 0.42834147810935974, LKD 3.6502816677093506, LDE 5.0900678634643555, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=8.887229904532433
Loss made of: CE 0.444316565990448, LKD 4.905014991760254, LDE 4.012938022613525, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=8.883091339468956
Loss made of: CE 0.3809342384338379, LKD 3.7388947010040283, LDE 3.7513203620910645, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4147294759750366, Reg Loss=8.477021217346191
Clinet index 3, End of Epoch 5/6, Average Loss=8.89175033569336, Class Loss=0.4147294759750366, Reg Loss=8.477021217346191
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/102, Loss=8.438991543650626
Loss made of: CE 0.3963542878627777, LKD 4.740516662597656, LDE 3.393582582473755, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=8.512191715836526
Loss made of: CE 0.3965565860271454, LKD 4.649273872375488, LDE 3.245741605758667, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.908787560462951
Loss made of: CE 0.3430614173412323, LKD 3.914911985397339, LDE 4.335060119628906, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=8.064563450217246
Loss made of: CE 0.36555296182632446, LKD 4.272928714752197, LDE 3.1613197326660156, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=8.713575834035874
Loss made of: CE 0.4246482849121094, LKD 4.837723731994629, LDE 4.881728649139404, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=9.08992275595665
Loss made of: CE 0.3459782600402832, LKD 4.925222396850586, LDE 3.1912288665771484, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=8.605611398816109
Loss made of: CE 0.4449678063392639, LKD 5.045770645141602, LDE 3.2838735580444336, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=8.428709205985069
Loss made of: CE 0.4783618450164795, LKD 4.416540145874023, LDE 3.1790831089019775, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=8.626571309566497
Loss made of: CE 0.4596117436885834, LKD 5.394406318664551, LDE 2.9784531593322754, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=8.796947664022445
Loss made of: CE 0.35377663373947144, LKD 4.000482559204102, LDE 4.040571689605713, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4035325050354004, Reg Loss=8.208736419677734
Clinet index 3, End of Epoch 6/6, Average Loss=8.612268447875977, Class Loss=0.4035325050354004, Reg Loss=8.208736419677734
Current Client Index:  17
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=13.408185482025146
Loss made of: CE 0.7777235507965088, LKD 4.371432781219482, LDE 6.020670413970947, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=10.85621457695961
Loss made of: CE 0.7609257698059082, LKD 4.665003299713135, LDE 5.081265926361084, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7710339426994324, Reg Loss=11.260955810546875
Clinet index 17, End of Epoch 1/6, Average Loss=12.031990051269531, Class Loss=0.7710339426994324, Reg Loss=11.260955810546875
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/23, Loss=10.511116623878479
Loss made of: CE 0.5189869999885559, LKD 4.428379058837891, LDE 4.437897205352783, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=9.824097391963004
Loss made of: CE 0.47237664461135864, LKD 4.860713481903076, LDE 5.356326103210449, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5700815320014954, Reg Loss=9.649218559265137
Clinet index 17, End of Epoch 2/6, Average Loss=10.219300270080566, Class Loss=0.5700815320014954, Reg Loss=9.649218559265137
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/23, Loss=9.664139845967293
Loss made of: CE 0.34826764464378357, LKD 3.941545248031616, LDE 4.293852806091309, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=9.876782846450805
Loss made of: CE 0.4557170569896698, LKD 5.031231880187988, LDE 4.799217224121094, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.48033374547958374, Reg Loss=9.194293975830078
Clinet index 17, End of Epoch 3/6, Average Loss=9.674627304077148, Class Loss=0.48033374547958374, Reg Loss=9.194293975830078
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/23, Loss=9.839186191558838
Loss made of: CE 0.48595207929611206, LKD 5.6401543617248535, LDE 4.349635124206543, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=9.438091331720353
Loss made of: CE 0.39796754717826843, LKD 5.2148895263671875, LDE 4.8187785148620605, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.45390504598617554, Reg Loss=9.165857315063477
Clinet index 17, End of Epoch 4/6, Average Loss=9.619762420654297, Class Loss=0.45390504598617554, Reg Loss=9.165857315063477
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/23, Loss=9.419630014896393
Loss made of: CE 0.43631017208099365, LKD 3.9380125999450684, LDE 4.3216423988342285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=9.181462851166724
Loss made of: CE 0.3979429602622986, LKD 4.341774940490723, LDE 3.8715436458587646, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4335862398147583, Reg Loss=8.869272232055664
Clinet index 17, End of Epoch 5/6, Average Loss=9.302858352661133, Class Loss=0.4335862398147583, Reg Loss=8.869272232055664
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/23, Loss=9.504583460092544
Loss made of: CE 0.4772472381591797, LKD 4.061378002166748, LDE 3.945814847946167, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=9.181390762329102
Loss made of: CE 0.4273429811000824, LKD 4.045095920562744, LDE 4.880218029022217, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4255231022834778, Reg Loss=8.94654655456543
Clinet index 17, End of Epoch 6/6, Average Loss=9.372069358825684, Class Loss=0.4255231022834778, Reg Loss=8.94654655456543
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/102, Loss=7.9621811479330065
Loss made of: CE 0.4088665843009949, LKD 3.9253599643707275, LDE 4.167108058929443, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=9.194319856166839
Loss made of: CE 0.4670071005821228, LKD 4.094236373901367, LDE 4.360304832458496, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=8.293625465035438
Loss made of: CE 0.42832010984420776, LKD 4.648415565490723, LDE 3.2841153144836426, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=9.073207372426987
Loss made of: CE 0.402224063873291, LKD 4.621901988983154, LDE 3.471177101135254, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=9.138645008206367
Loss made of: CE 0.3590123653411865, LKD 4.0846781730651855, LDE 4.632137775421143, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=9.29609281718731
Loss made of: CE 0.36932605504989624, LKD 4.0555925369262695, LDE 3.02268123626709, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=8.725486308336258
Loss made of: CE 0.3936156630516052, LKD 4.480703353881836, LDE 4.0040602684021, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=9.02255975306034
Loss made of: CE 0.4115223288536072, LKD 4.522852420806885, LDE 3.447970390319824, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=8.325415462255478
Loss made of: CE 0.3836856782436371, LKD 5.389338493347168, LDE 3.489797592163086, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=8.703518527746201
Loss made of: CE 0.42715197801589966, LKD 5.216255187988281, LDE 4.150359153747559, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4325791001319885, Reg Loss=8.338987350463867
Clinet index 11, End of Epoch 1/6, Average Loss=8.771566390991211, Class Loss=0.4325791001319885, Reg Loss=8.338987350463867
Pseudo labeling is: None
Epoch 2, lr = 0.000600
Epoch 2, Batch 10/102, Loss=8.79522538483143
Loss made of: CE 0.33430537581443787, LKD 4.244575023651123, LDE 3.5269947052001953, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=8.303073665499687
Loss made of: CE 0.37884026765823364, LKD 3.893885850906372, LDE 3.977195978164673, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=8.625192019343377
Loss made of: CE 0.3635334372520447, LKD 4.874003887176514, LDE 3.01782488822937, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=8.532732132077218
Loss made of: CE 0.42103904485702515, LKD 5.075484275817871, LDE 3.7097995281219482, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=8.829430082440377
Loss made of: CE 0.4572060704231262, LKD 5.405945777893066, LDE 4.279453754425049, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=8.84699923992157
Loss made of: CE 0.4772226810455322, LKD 5.580373764038086, LDE 3.6310532093048096, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=8.14366382062435
Loss made of: CE 0.42855948209762573, LKD 5.314672946929932, LDE 3.0989739894866943, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=9.127888238430023
Loss made of: CE 0.4983181953430176, LKD 5.888653755187988, LDE 4.45307731628418, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=8.325301769375802
Loss made of: CE 0.47488903999328613, LKD 4.831089973449707, LDE 4.354203701019287, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=8.50883041024208
Loss made of: CE 0.3566586375236511, LKD 4.441802024841309, LDE 4.267282485961914, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.42555683851242065, Reg Loss=8.191428184509277
Clinet index 11, End of Epoch 2/6, Average Loss=8.616985321044922, Class Loss=0.42555683851242065, Reg Loss=8.191428184509277
Pseudo labeling is: None
Epoch 3, lr = 0.000568
Epoch 3, Batch 10/102, Loss=8.534125119447708
Loss made of: CE 0.38890236616134644, LKD 5.2256951332092285, LDE 3.353898286819458, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=8.501584213972091
Loss made of: CE 0.3884173333644867, LKD 4.725236892700195, LDE 3.1049723625183105, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=8.19114793241024
Loss made of: CE 0.4633285403251648, LKD 4.894117832183838, LDE 3.2415993213653564, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=8.871672984957694
Loss made of: CE 0.4183642864227295, LKD 5.072153568267822, LDE 4.089242935180664, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=8.793929076194763
Loss made of: CE 0.4434266686439514, LKD 5.232789516448975, LDE 2.950241804122925, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=8.696958565711975
Loss made of: CE 0.44551894068717957, LKD 4.451726913452148, LDE 3.387732744216919, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=8.650228759646415
Loss made of: CE 0.34789347648620605, LKD 4.345458984375, LDE 4.026088714599609, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=8.881397673487664
Loss made of: CE 0.4195350110530853, LKD 4.490713596343994, LDE 4.290807247161865, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=8.960837250947952
Loss made of: CE 0.33451348543167114, LKD 4.401346206665039, LDE 3.56634521484375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=8.976960161328316
Loss made of: CE 0.3810761868953705, LKD 4.543975830078125, LDE 3.4036941528320312, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4209747314453125, Reg Loss=8.27444839477539
Clinet index 11, End of Epoch 3/6, Average Loss=8.695423126220703, Class Loss=0.4209747314453125, Reg Loss=8.27444839477539
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/102, Loss=9.00668199956417
Loss made of: CE 0.40489432215690613, LKD 4.241564750671387, LDE 3.133653163909912, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=8.62686075270176
Loss made of: CE 0.42285093665122986, LKD 3.5070552825927734, LDE 3.697033166885376, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=8.905766132473946
Loss made of: CE 0.4324983060359955, LKD 4.400778770446777, LDE 4.42529821395874, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=8.413882148265838
Loss made of: CE 0.3992035388946533, LKD 4.20798921585083, LDE 2.780064821243286, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=8.653289106488227
Loss made of: CE 0.42012059688568115, LKD 4.799553394317627, LDE 3.715928554534912, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=8.754667097330094
Loss made of: CE 0.3366210460662842, LKD 3.7488605976104736, LDE 3.396793842315674, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=8.59991711974144
Loss made of: CE 0.4616161286830902, LKD 5.209506511688232, LDE 3.0971760749816895, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=8.89544951915741
Loss made of: CE 0.4863855242729187, LKD 4.546390056610107, LDE 5.2543487548828125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=8.751212158799172
Loss made of: CE 0.35990867018699646, LKD 4.442805290222168, LDE 3.4802675247192383, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=8.565455928444862
Loss made of: CE 0.44524678587913513, LKD 4.445285797119141, LDE 4.998906135559082, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.42105650901794434, Reg Loss=8.308499336242676
Clinet index 11, End of Epoch 4/6, Average Loss=8.7295560836792, Class Loss=0.42105650901794434, Reg Loss=8.308499336242676
Pseudo labeling is: None
Epoch 5, lr = 0.000504
Epoch 5, Batch 10/102, Loss=8.686917027831077
Loss made of: CE 0.40108340978622437, LKD 5.386291027069092, LDE 3.366858959197998, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=8.891523951292038
Loss made of: CE 0.4277169704437256, LKD 4.782071113586426, LDE 3.8531224727630615, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=8.52134028673172
Loss made of: CE 0.3380136489868164, LKD 4.045515060424805, LDE 2.954981565475464, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=8.387984734773635
Loss made of: CE 0.3912145495414734, LKD 4.578892707824707, LDE 2.931096315383911, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=8.843163192272186
Loss made of: CE 0.4760162830352783, LKD 4.659373760223389, LDE 4.371830940246582, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=8.55708874464035
Loss made of: CE 0.4052240252494812, LKD 5.9126057624816895, LDE 4.053083896636963, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=7.984187078475952
Loss made of: CE 0.42624133825302124, LKD 4.865049362182617, LDE 2.890383243560791, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=8.206382468342781
Loss made of: CE 0.2813195288181305, LKD 4.38351583480835, LDE 3.841564178466797, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=8.597567808628082
Loss made of: CE 0.5090912580490112, LKD 5.386564254760742, LDE 2.925746202468872, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=8.692389661073685
Loss made of: CE 0.4615764021873474, LKD 4.663313865661621, LDE 3.0795435905456543, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4144083857536316, Reg Loss=8.133755683898926
Clinet index 11, End of Epoch 5/6, Average Loss=8.548164367675781, Class Loss=0.4144083857536316, Reg Loss=8.133755683898926
Pseudo labeling is: None
Epoch 6, lr = 0.000471
Epoch 6, Batch 10/102, Loss=8.698509737849236
Loss made of: CE 0.4760245680809021, LKD 3.9115428924560547, LDE 3.160975456237793, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=8.22095609307289
Loss made of: CE 0.3856101930141449, LKD 5.280583381652832, LDE 3.4108028411865234, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.352373698353768
Loss made of: CE 0.4442637860774994, LKD 4.34390926361084, LDE 3.779977560043335, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=8.835027587413787
Loss made of: CE 0.4172205626964569, LKD 4.613253116607666, LDE 3.8603310585021973, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=8.479784059524537
Loss made of: CE 0.35032302141189575, LKD 4.579562664031982, LDE 2.935436248779297, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=8.80351314842701
Loss made of: CE 0.41776809096336365, LKD 5.946083068847656, LDE 3.7905306816101074, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=8.377611345052719
Loss made of: CE 0.4444323480129242, LKD 4.147761821746826, LDE 3.2807118892669678, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=8.061545342206955
Loss made of: CE 0.38321375846862793, LKD 5.619940280914307, LDE 3.229351282119751, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=8.623623448610306
Loss made of: CE 0.40846094489097595, LKD 4.486049175262451, LDE 2.9611496925354004, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=8.25983286201954
Loss made of: CE 0.4109654426574707, LKD 4.8585615158081055, LDE 2.9356822967529297, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4156827926635742, Reg Loss=8.07772159576416
Clinet index 11, End of Epoch 6/6, Average Loss=8.493404388427734, Class Loss=0.4156827926635742, Reg Loss=8.07772159576416
federated aggregation...
Validation, Class Loss=0.46866896748542786, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.846689
Mean Acc: 0.440190
FreqW Acc: 0.750686
Mean IoU: 0.336902
Class IoU:
	class 0: 0.89053476
	class 1: 0.10779604
	class 2: 0.00015193169
	class 3: 0.0
	class 4: 0.52424556
	class 5: 0.06918234
	class 6: 0.7342441
	class 7: 0.67806065
	class 8: 0.23786594
	class 9: 0.04625036
	class 10: 0.09400023
	class 11: 0.3090889
	class 12: 0.3789429
	class 13: 0.3944084
	class 14: 0.5499901
	class 15: 0.70708823
	class 16: 0.005486468
Class Acc:
	class 0: 0.9692523
	class 1: 0.10781913
	class 2: 0.00015194573
	class 3: 0.0
	class 4: 0.546496
	class 5: 0.06919229
	class 6: 0.76216453
	class 7: 0.6958087
	class 8: 0.23896307
	class 9: 0.049130794
	class 10: 0.10528615
	class 11: 0.49534702
	class 12: 0.86430854
	class 13: 0.8078545
	class 14: 0.90042096
	class 15: 0.86554563
	class 16: 0.0054875775

federated global round: 18, step: 3
select part of clients to conduct local training
[9, 11, 13, 10]
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/102, Loss=9.328526291251183
Loss made of: CE 0.40875861048698425, LKD 4.729828834533691, LDE 2.5349185466766357, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=8.804244810342789
Loss made of: CE 0.4511055648326874, LKD 4.914274215698242, LDE 3.3596932888031006, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=9.016919887065887
Loss made of: CE 0.348891943693161, LKD 3.8922905921936035, LDE 3.6780483722686768, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=8.89490367770195
Loss made of: CE 0.43431609869003296, LKD 4.361965656280518, LDE 3.485078811645508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=9.496077200770378
Loss made of: CE 0.4413762092590332, LKD 4.85765266418457, LDE 4.683614730834961, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=9.070952159166335
Loss made of: CE 0.41144245862960815, LKD 4.638731002807617, LDE 4.6529107093811035, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=8.875820955634117
Loss made of: CE 0.3808642327785492, LKD 4.220975399017334, LDE 3.104462146759033, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=8.816681706905365
Loss made of: CE 0.4248300790786743, LKD 5.434797286987305, LDE 3.824232578277588, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=8.990102648735046
Loss made of: CE 0.409163236618042, LKD 5.048469543457031, LDE 3.649426221847534, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=8.396498814225197
Loss made of: CE 0.37097829580307007, LKD 4.2894463539123535, LDE 3.4963152408599854, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4185818135738373, Reg Loss=8.552382469177246
Clinet index 9, End of Epoch 1/6, Average Loss=8.970964431762695, Class Loss=0.4185818135738373, Reg Loss=8.552382469177246
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/102, Loss=8.481363600492477
Loss made of: CE 0.3276112675666809, LKD 4.77835750579834, LDE 2.9190938472747803, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=8.69420271217823
Loss made of: CE 0.46399515867233276, LKD 4.692095756530762, LDE 3.6669962406158447, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=9.438807794451714
Loss made of: CE 0.4628185033798218, LKD 5.474605560302734, LDE 4.300741672515869, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=8.567027512192727
Loss made of: CE 0.3796672224998474, LKD 3.855639696121216, LDE 3.3074326515197754, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=9.117525765299797
Loss made of: CE 0.3984457850456238, LKD 4.447271823883057, LDE 3.6687088012695312, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=9.057063496112823
Loss made of: CE 0.47039955854415894, LKD 4.5159912109375, LDE 3.7152044773101807, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=9.060675671696663
Loss made of: CE 0.3960663676261902, LKD 4.484357833862305, LDE 4.364480972290039, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=8.632362416386604
Loss made of: CE 0.3777630925178528, LKD 4.4471611976623535, LDE 3.4473209381103516, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=8.63298137485981
Loss made of: CE 0.5012325048446655, LKD 4.823526859283447, LDE 4.038486480712891, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=8.255746495723724
Loss made of: CE 0.3656153678894043, LKD 4.108945369720459, LDE 3.0064315795898438, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4124862551689148, Reg Loss=8.381288528442383
Clinet index 9, End of Epoch 2/6, Average Loss=8.793774604797363, Class Loss=0.4124862551689148, Reg Loss=8.381288528442383
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/102, Loss=8.615684780478478
Loss made of: CE 0.4288826882839203, LKD 4.481727600097656, LDE 3.2777295112609863, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=8.821135690808296
Loss made of: CE 0.41693219542503357, LKD 4.026068210601807, LDE 4.0247883796691895, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=8.897125455737115
Loss made of: CE 0.47808170318603516, LKD 5.861356735229492, LDE 3.4706010818481445, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=8.72834978401661
Loss made of: CE 0.3685159683227539, LKD 4.525274276733398, LDE 3.843381643295288, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=8.214150455594062
Loss made of: CE 0.4437410235404968, LKD 5.070279598236084, LDE 3.1207199096679688, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=8.944795149564744
Loss made of: CE 0.5536795258522034, LKD 4.525837421417236, LDE 3.1432266235351562, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=8.86083849966526
Loss made of: CE 0.4896697402000427, LKD 4.3481974601745605, LDE 3.9863083362579346, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=8.703968292474746
Loss made of: CE 0.3880648612976074, LKD 4.484203338623047, LDE 3.018799304962158, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=8.592686608433723
Loss made of: CE 0.43188661336898804, LKD 4.343067646026611, LDE 3.192206382751465, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=8.738513961434364
Loss made of: CE 0.3885057866573334, LKD 4.316693305969238, LDE 4.4384074211120605, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.41113221645355225, Reg Loss=8.308269500732422
Clinet index 9, End of Epoch 3/6, Average Loss=8.719401359558105, Class Loss=0.41113221645355225, Reg Loss=8.308269500732422
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/102, Loss=8.967139601707458
Loss made of: CE 0.3663272261619568, LKD 4.0661091804504395, LDE 3.4585156440734863, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=9.057584553956985
Loss made of: CE 0.40533944964408875, LKD 5.448529243469238, LDE 3.991223096847534, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=8.463272696733474
Loss made of: CE 0.38161811232566833, LKD 4.104950904846191, LDE 3.62241530418396, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=9.062275996804237
Loss made of: CE 0.4109176993370056, LKD 4.170469760894775, LDE 3.7804007530212402, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=9.14228797852993
Loss made of: CE 0.38981160521507263, LKD 4.698854446411133, LDE 3.9885177612304688, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=8.693619754910468
Loss made of: CE 0.3961254060268402, LKD 4.684279441833496, LDE 2.692004680633545, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=9.088044100999833
Loss made of: CE 0.38602107763290405, LKD 4.592593669891357, LDE 3.7105846405029297, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=9.079420509934426
Loss made of: CE 0.44944486021995544, LKD 4.968240737915039, LDE 5.843604564666748, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=9.122013813257217
Loss made of: CE 0.42046207189559937, LKD 4.917209625244141, LDE 2.836740016937256, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=8.658315414190293
Loss made of: CE 0.39999693632125854, LKD 4.70139217376709, LDE 5.043321132659912, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4069121181964874, Reg Loss=8.523569107055664
Clinet index 9, End of Epoch 4/6, Average Loss=8.93048095703125, Class Loss=0.4069121181964874, Reg Loss=8.523569107055664
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/102, Loss=8.429924556612969
Loss made of: CE 0.32833367586135864, LKD 3.6549642086029053, LDE 3.979248523712158, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=8.580786472558975
Loss made of: CE 0.46931934356689453, LKD 4.184267997741699, LDE 3.139089345932007, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=8.375589030981065
Loss made of: CE 0.32613933086395264, LKD 4.323389530181885, LDE 4.120172500610352, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=8.871128073334694
Loss made of: CE 0.36163458228111267, LKD 4.725861549377441, LDE 3.463178873062134, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=8.452663949131965
Loss made of: CE 0.3422466516494751, LKD 4.497074127197266, LDE 3.1829826831817627, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=8.563300949335098
Loss made of: CE 0.47140228748321533, LKD 5.133624076843262, LDE 4.549654483795166, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=8.642884412407875
Loss made of: CE 0.4468774199485779, LKD 4.286005973815918, LDE 3.680224657058716, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=8.51445319056511
Loss made of: CE 0.280825138092041, LKD 3.8404316902160645, LDE 3.6312737464904785, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=8.870063117146492
Loss made of: CE 0.4298652410507202, LKD 5.19875955581665, LDE 2.7926993370056152, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=9.086062940955163
Loss made of: CE 0.45557451248168945, LKD 4.740297794342041, LDE 3.2523865699768066, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.40413084626197815, Reg Loss=8.2340726852417
Clinet index 9, End of Epoch 5/6, Average Loss=8.638203620910645, Class Loss=0.40413084626197815, Reg Loss=8.2340726852417
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/102, Loss=8.401984664797784
Loss made of: CE 0.3817608952522278, LKD 4.786123275756836, LDE 4.339929103851318, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=8.364003804326057
Loss made of: CE 0.35574668645858765, LKD 4.453055381774902, LDE 2.6051268577575684, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.758961728215217
Loss made of: CE 0.4378621280193329, LKD 4.609160900115967, LDE 3.4497153759002686, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=8.538453483581543
Loss made of: CE 0.34968432784080505, LKD 5.278050899505615, LDE 2.914721965789795, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=8.692192855477334
Loss made of: CE 0.4154800772666931, LKD 4.934048175811768, LDE 3.6169209480285645, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=8.561798387765885
Loss made of: CE 0.400789737701416, LKD 4.647521018981934, LDE 2.8513054847717285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=8.508290427923203
Loss made of: CE 0.4063842296600342, LKD 4.886778354644775, LDE 4.545072555541992, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=8.526040551066398
Loss made of: CE 0.5089821219444275, LKD 4.576496601104736, LDE 3.3635802268981934, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=8.625040709972382
Loss made of: CE 0.34101399779319763, LKD 3.911184787750244, LDE 3.5891618728637695, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=8.476070508360863
Loss made of: CE 0.4106319844722748, LKD 4.881268501281738, LDE 2.9453322887420654, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.39721018075942993, Reg Loss=8.136748313903809
Clinet index 9, End of Epoch 6/6, Average Loss=8.533958435058594, Class Loss=0.39721018075942993, Reg Loss=8.136748313903809
Current Client Index:  11
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000438
Epoch 1, Batch 10/102, Loss=8.14875894188881
Loss made of: CE 0.39537787437438965, LKD 4.217053413391113, LDE 4.007936477661133, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=9.034680539369583
Loss made of: CE 0.42093905806541443, LKD 4.005346775054932, LDE 4.174285411834717, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=8.292059209942817
Loss made of: CE 0.43914493918418884, LKD 4.468727111816406, LDE 3.5511157512664795, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=8.795137539505959
Loss made of: CE 0.393115371465683, LKD 4.404659271240234, LDE 3.7473857402801514, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=8.926537942886352
Loss made of: CE 0.3446381688117981, LKD 4.019598484039307, LDE 4.621634006500244, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=9.111137804389
Loss made of: CE 0.3643057346343994, LKD 3.979464292526245, LDE 3.0543923377990723, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=8.546442034840585
Loss made of: CE 0.3788783550262451, LKD 4.573254108428955, LDE 3.56719970703125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=8.757168984413147
Loss made of: CE 0.4116113781929016, LKD 4.536323070526123, LDE 3.1274445056915283, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=8.087446025013923
Loss made of: CE 0.3761022090911865, LKD 5.2157135009765625, LDE 3.0442190170288086, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=8.617987307906152
Loss made of: CE 0.39980050921440125, LKD 5.54638147354126, LDE 4.083142280578613, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.4234806299209595, Reg Loss=8.202073097229004
Clinet index 11, End of Epoch 1/6, Average Loss=8.625554084777832, Class Loss=0.4234806299209595, Reg Loss=8.202073097229004
Pseudo labeling is: None
Epoch 2, lr = 0.000405
Epoch 2, Batch 10/102, Loss=8.395448446273804
Loss made of: CE 0.37739452719688416, LKD 4.459612846374512, LDE 2.9848546981811523, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=8.199907213449478
Loss made of: CE 0.424633264541626, LKD 3.7871084213256836, LDE 4.254273891448975, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=8.461921721696854
Loss made of: CE 0.36474546790122986, LKD 5.2593674659729, LDE 3.118565320968628, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=8.479091477394103
Loss made of: CE 0.41232413053512573, LKD 4.78801155090332, LDE 3.214733362197876, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=8.557056084275246
Loss made of: CE 0.4109683930873871, LKD 5.326239585876465, LDE 3.8935556411743164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=8.580329340696334
Loss made of: CE 0.4494262933731079, LKD 5.42358922958374, LDE 3.3015496730804443, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=7.912356308102607
Loss made of: CE 0.42968499660491943, LKD 5.998776435852051, LDE 2.7605533599853516, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=8.88909034729004
Loss made of: CE 0.48645591735839844, LKD 6.10679292678833, LDE 4.54062032699585, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=8.196491599082947
Loss made of: CE 0.47160816192626953, LKD 5.082067489624023, LDE 4.291471481323242, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=8.460459676384925
Loss made of: CE 0.34024009108543396, LKD 4.395233631134033, LDE 4.109770774841309, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4143020212650299, Reg Loss=8.008861541748047
Clinet index 11, End of Epoch 2/6, Average Loss=8.423163414001465, Class Loss=0.4143020212650299, Reg Loss=8.008861541748047
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/102, Loss=8.477887299656867
Loss made of: CE 0.3767816424369812, LKD 5.218654632568359, LDE 3.3661694526672363, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=8.111405017971993
Loss made of: CE 0.37956327199935913, LKD 4.406703472137451, LDE 2.6169679164886475, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=7.982584252953529
Loss made of: CE 0.4665939509868622, LKD 4.683932781219482, LDE 3.181931734085083, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=8.586810603737831
Loss made of: CE 0.42501509189605713, LKD 5.236671447753906, LDE 3.9019994735717773, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=8.43387829065323
Loss made of: CE 0.44726037979125977, LKD 5.067939281463623, LDE 3.1561439037323, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=8.581244918704034
Loss made of: CE 0.458006352186203, LKD 4.479142189025879, LDE 3.3172574043273926, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=8.383762940764427
Loss made of: CE 0.35374385118484497, LKD 4.385959148406982, LDE 4.004926681518555, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=8.696685302257539
Loss made of: CE 0.39891690015792847, LKD 4.391770839691162, LDE 4.144701957702637, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=8.935104292631149
Loss made of: CE 0.3133482336997986, LKD 4.112756252288818, LDE 3.261807441711426, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=8.647823870182037
Loss made of: CE 0.3970577120780945, LKD 4.589613437652588, LDE 3.004833698272705, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4086489975452423, Reg Loss=8.072543144226074
Clinet index 11, End of Epoch 3/6, Average Loss=8.481192588806152, Class Loss=0.4086489975452423, Reg Loss=8.072543144226074
Pseudo labeling is: None
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=8.657185518741608
Loss made of: CE 0.37871241569519043, LKD 4.066446304321289, LDE 2.9911344051361084, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=8.671202141046525
Loss made of: CE 0.4396081864833832, LKD 3.7547760009765625, LDE 3.59804105758667, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=8.810551100969315
Loss made of: CE 0.3947451114654541, LKD 4.3111395835876465, LDE 4.123016834259033, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=8.378328257799149
Loss made of: CE 0.4019588232040405, LKD 4.23225736618042, LDE 2.939934015274048, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=8.486554098129272
Loss made of: CE 0.45050248503685, LKD 4.86284065246582, LDE 3.464834690093994, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=8.682019606232643
Loss made of: CE 0.35095319151878357, LKD 3.7059335708618164, LDE 3.8968942165374756, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=8.36098524928093
Loss made of: CE 0.45887407660484314, LKD 5.591798782348633, LDE 3.021451950073242, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=8.641641137003898
Loss made of: CE 0.4464688301086426, LKD 4.6857171058654785, LDE 5.331980228424072, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=8.431059730052947
Loss made of: CE 0.36434024572372437, LKD 4.39448356628418, LDE 2.886056900024414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=8.324260747432708
Loss made of: CE 0.4322202801704407, LKD 4.480788707733154, LDE 5.0457892417907715, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.41218382120132446, Reg Loss=8.136885643005371
Clinet index 11, End of Epoch 4/6, Average Loss=8.54906940460205, Class Loss=0.41218382120132446, Reg Loss=8.136885643005371
Pseudo labeling is: None
Epoch 5, lr = 0.000304
Epoch 5, Batch 10/102, Loss=8.587761172652245
Loss made of: CE 0.3840131163597107, LKD 5.6327900886535645, LDE 3.0615131855010986, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=8.67027984559536
Loss made of: CE 0.4041469693183899, LKD 4.9048542976379395, LDE 4.011776447296143, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=8.383860170841217
Loss made of: CE 0.34739479422569275, LKD 3.9323792457580566, LDE 2.8926565647125244, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=8.201193475723267
Loss made of: CE 0.40162843465805054, LKD 4.695260524749756, LDE 2.9602413177490234, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=8.677670893073081
Loss made of: CE 0.4723449647426605, LKD 4.710514068603516, LDE 4.132281303405762, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=8.158896559476853
Loss made of: CE 0.3962007761001587, LKD 5.985140800476074, LDE 3.491008996963501, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=7.796349292993545
Loss made of: CE 0.40472567081451416, LKD 4.543451309204102, LDE 2.8318545818328857, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=8.30830221772194
Loss made of: CE 0.25684475898742676, LKD 4.368512153625488, LDE 4.176399230957031, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=8.351510342955589
Loss made of: CE 0.5078965425491333, LKD 5.164535045623779, LDE 2.631662368774414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=8.410644587874412
Loss made of: CE 0.44993695616722107, LKD 4.984519004821777, LDE 2.9699172973632812, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.40582767128944397, Reg Loss=7.956536293029785
Clinet index 11, End of Epoch 5/6, Average Loss=8.362363815307617, Class Loss=0.40582767128944397, Reg Loss=7.956536293029785
Pseudo labeling is: None
Epoch 6, lr = 0.000270
Epoch 6, Batch 10/102, Loss=8.455408349633217
Loss made of: CE 0.4416963458061218, LKD 3.781479597091675, LDE 3.021249532699585, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=8.004478597640992
Loss made of: CE 0.40412187576293945, LKD 5.115720748901367, LDE 3.0921010971069336, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.054670691490173
Loss made of: CE 0.4654528796672821, LKD 4.401670455932617, LDE 3.182466983795166, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=8.57953951060772
Loss made of: CE 0.42925891280174255, LKD 4.477102756500244, LDE 3.656374931335449, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=8.322401016950607
Loss made of: CE 0.3344789445400238, LKD 4.481447696685791, LDE 2.9803876876831055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=8.632103249430656
Loss made of: CE 0.42359408736228943, LKD 5.870787620544434, LDE 3.839953899383545, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=8.2231615960598
Loss made of: CE 0.4128395915031433, LKD 4.056893825531006, LDE 3.117659568786621, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=7.7989523857831955
Loss made of: CE 0.38091468811035156, LKD 5.553094863891602, LDE 3.202277421951294, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=8.425698947906493
Loss made of: CE 0.4108439087867737, LKD 4.20100212097168, LDE 2.937525987625122, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=8.213489970564842
Loss made of: CE 0.3979419469833374, LKD 4.8242950439453125, LDE 2.899010419845581, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4078757166862488, Reg Loss=7.883832931518555
Clinet index 11, End of Epoch 6/6, Average Loss=8.291708946228027, Class Loss=0.4078757166862488, Reg Loss=7.883832931518555
Current Client Index:  13
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/23, Loss=11.754936343431472
Loss made of: CE 0.7221681475639343, LKD 4.4229326248168945, LDE 4.9465413093566895, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=10.324293780326844
Loss made of: CE 0.634366512298584, LKD 4.783361434936523, LDE 5.381690502166748, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6800610423088074, Reg Loss=10.154838562011719
Clinet index 13, End of Epoch 1/6, Average Loss=10.83489990234375, Class Loss=0.6800610423088074, Reg Loss=10.154838562011719
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/23, Loss=10.190922227501868
Loss made of: CE 0.5271707773208618, LKD 4.8915605545043945, LDE 4.406364917755127, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=10.086163857579232
Loss made of: CE 0.5137984752655029, LKD 5.154542446136475, LDE 4.280066013336182, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5257525444030762, Reg Loss=9.463316917419434
Clinet index 13, End of Epoch 2/6, Average Loss=9.989068984985352, Class Loss=0.5257525444030762, Reg Loss=9.463316917419434
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/23, Loss=9.539960584044456
Loss made of: CE 0.45908114314079285, LKD 5.0697455406188965, LDE 4.856115341186523, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=9.689856985211373
Loss made of: CE 0.39747363328933716, LKD 4.6148176193237305, LDE 5.2832255363464355, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.45764806866645813, Reg Loss=9.09554386138916
Clinet index 13, End of Epoch 3/6, Average Loss=9.553192138671875, Class Loss=0.45764806866645813, Reg Loss=9.09554386138916
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/23, Loss=9.317779386043549
Loss made of: CE 0.48106855154037476, LKD 4.932130336761475, LDE 3.8878495693206787, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=9.716296091675758
Loss made of: CE 0.3661814332008362, LKD 4.953993320465088, LDE 3.8923912048339844, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.43540942668914795, Reg Loss=9.031179428100586
Clinet index 13, End of Epoch 4/6, Average Loss=9.466588973999023, Class Loss=0.43540942668914795, Reg Loss=9.031179428100586
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/23, Loss=9.120681154727937
Loss made of: CE 0.41781115531921387, LKD 3.8730103969573975, LDE 4.43966007232666, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=9.190608218312263
Loss made of: CE 0.44175195693969727, LKD 5.022215843200684, LDE 3.8134970664978027, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.42601945996284485, Reg Loss=8.812667846679688
Clinet index 13, End of Epoch 5/6, Average Loss=9.238687515258789, Class Loss=0.42601945996284485, Reg Loss=8.812667846679688
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/23, Loss=9.238894215226173
Loss made of: CE 0.47006580233573914, LKD 5.589820384979248, LDE 5.15767765045166, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=9.2633625715971
Loss made of: CE 0.4109157919883728, LKD 5.332590103149414, LDE 4.63194465637207, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.42586687207221985, Reg Loss=8.87503719329834
Clinet index 13, End of Epoch 6/6, Average Loss=9.300904273986816, Class Loss=0.42586687207221985, Reg Loss=8.87503719329834
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/23, Loss=12.716061651706696
Loss made of: CE 0.6386091709136963, LKD 5.360128879547119, LDE 5.7455244064331055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=10.62252406179905
Loss made of: CE 0.5575554966926575, LKD 4.740362644195557, LDE 5.063004016876221, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.645268976688385, Reg Loss=10.856842041015625
Clinet index 10, End of Epoch 1/6, Average Loss=11.502111434936523, Class Loss=0.645268976688385, Reg Loss=10.856842041015625
Pseudo labeling is: None
Epoch 2, lr = 0.000756
Epoch 2, Batch 10/23, Loss=10.529933425784112
Loss made of: CE 0.5039724111557007, LKD 6.65916109085083, LDE 5.755981922149658, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=9.732120954990387
Loss made of: CE 0.5002379417419434, LKD 6.394663333892822, LDE 4.320900917053223, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4923011064529419, Reg Loss=9.468148231506348
Clinet index 10, End of Epoch 2/6, Average Loss=9.96044921875, Class Loss=0.4923011064529419, Reg Loss=9.468148231506348
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/23, Loss=9.43994361460209
Loss made of: CE 0.3737770915031433, LKD 5.3834075927734375, LDE 4.656109809875488, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=10.034684318304063
Loss made of: CE 0.3213711082935333, LKD 5.107306957244873, LDE 4.841784954071045, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4403294324874878, Reg Loss=9.284671783447266
Clinet index 10, End of Epoch 3/6, Average Loss=9.725001335144043, Class Loss=0.4403294324874878, Reg Loss=9.284671783447266
Pseudo labeling is: None
Epoch 4, lr = 0.000631
Epoch 4, Batch 10/23, Loss=9.483260229229927
Loss made of: CE 0.42699289321899414, LKD 4.9393839836120605, LDE 3.796229600906372, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=9.827459320425987
Loss made of: CE 0.43084704875946045, LKD 4.990208148956299, LDE 3.605546712875366, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.42441946268081665, Reg Loss=9.202733993530273
Clinet index 10, End of Epoch 4/6, Average Loss=9.627153396606445, Class Loss=0.42441946268081665, Reg Loss=9.202733993530273
Pseudo labeling is: None
Epoch 5, lr = 0.000568
Epoch 5, Batch 10/23, Loss=10.250996422767638
Loss made of: CE 0.386760413646698, LKD 6.079965591430664, LDE 3.4051015377044678, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=9.475779223442078
Loss made of: CE 0.4633485674858093, LKD 6.314949035644531, LDE 4.7985663414001465, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.41441968083381653, Reg Loss=9.433022499084473
Clinet index 10, End of Epoch 5/6, Average Loss=9.847442626953125, Class Loss=0.41441968083381653, Reg Loss=9.433022499084473
Pseudo labeling is: None
Epoch 6, lr = 0.000504
Epoch 6, Batch 10/23, Loss=9.491150802373886
Loss made of: CE 0.4312916696071625, LKD 4.483100891113281, LDE 4.228352069854736, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=9.42712339758873
Loss made of: CE 0.4700692296028137, LKD 4.451078414916992, LDE 4.047550201416016, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4069277346134186, Reg Loss=9.017921447753906
Clinet index 10, End of Epoch 6/6, Average Loss=9.424849510192871, Class Loss=0.4069277346134186, Reg Loss=9.017921447753906
federated aggregation...
Validation, Class Loss=0.46757957339286804, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.846235
Mean Acc: 0.444127
FreqW Acc: 0.750313
Mean IoU: 0.338703
Class IoU:
	class 0: 0.8907196
	class 1: 0.10188477
	class 2: 6.0916835e-05
	class 3: 0.0
	class 4: 0.5015112
	class 5: 0.081364796
	class 6: 0.7027848
	class 7: 0.6508314
	class 8: 0.24368621
	class 9: 0.043700587
	class 10: 0.03468009
	class 11: 0.31242722
	class 12: 0.38323513
	class 13: 0.3763876
	class 14: 0.5344426
	class 15: 0.71947366
	class 16: 0.18076311
Class Acc:
	class 0: 0.9694634
	class 1: 0.101904444
	class 2: 6.0921637e-05
	class 3: 0.0
	class 4: 0.51918614
	class 5: 0.08138234
	class 6: 0.7248557
	class 7: 0.6647169
	class 8: 0.24479727
	class 9: 0.045979593
	class 10: 0.03776255
	class 11: 0.4772292
	class 12: 0.85469484
	class 13: 0.84934
	class 14: 0.9135792
	class 15: 0.871751
	class 16: 0.19345921

federated global round: 19, step: 3
select part of clients to conduct local training
[4, 7, 17, 15]
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/105, Loss=9.607301613688469
Loss made of: CE 0.5021037459373474, LKD 3.7935726642608643, LDE 5.419912815093994, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/105, Loss=9.085771983861923
Loss made of: CE 0.5085327625274658, LKD 4.115241050720215, LDE 4.597695827484131, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/105, Loss=8.790044221282006
Loss made of: CE 0.44893354177474976, LKD 4.189718723297119, LDE 4.659639358520508, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/105, Loss=8.665735670924187
Loss made of: CE 0.3952450752258301, LKD 4.7006707191467285, LDE 3.522679328918457, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/105, Loss=8.619674727320671
Loss made of: CE 0.45909959077835083, LKD 4.106502056121826, LDE 3.1657662391662598, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/105, Loss=9.207685005664825
Loss made of: CE 0.42172956466674805, LKD 3.7669312953948975, LDE 3.643986225128174, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/105, Loss=8.954052197933198
Loss made of: CE 0.46962589025497437, LKD 4.698615550994873, LDE 3.131676435470581, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/105, Loss=8.35941286087036
Loss made of: CE 0.3591620624065399, LKD 4.08635950088501, LDE 3.318222999572754, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/105, Loss=8.644188952445983
Loss made of: CE 0.39663442969322205, LKD 3.822899341583252, LDE 3.379703998565674, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/105, Loss=8.673937767744064
Loss made of: CE 0.41165947914123535, LKD 3.8868823051452637, LDE 5.016195774078369, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.444408655166626, Reg Loss=8.408149719238281
Clinet index 4, End of Epoch 1/6, Average Loss=8.852558135986328, Class Loss=0.444408655166626, Reg Loss=8.408149719238281
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/105, Loss=8.757721948623658
Loss made of: CE 0.42850756645202637, LKD 4.338988780975342, LDE 3.509138584136963, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/105, Loss=8.854208573698997
Loss made of: CE 0.40791457891464233, LKD 5.133932113647461, LDE 4.4109063148498535, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/105, Loss=8.649122911691666
Loss made of: CE 0.4069969654083252, LKD 4.521302700042725, LDE 4.738285541534424, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/105, Loss=8.82866493165493
Loss made of: CE 0.4630657434463501, LKD 4.75621223449707, LDE 3.556657552719116, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/105, Loss=9.049919366836548
Loss made of: CE 0.3913406729698181, LKD 4.485560894012451, LDE 3.6977946758270264, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/105, Loss=9.05188962817192
Loss made of: CE 0.4006403088569641, LKD 4.218357563018799, LDE 3.3923897743225098, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/105, Loss=8.943136432766915
Loss made of: CE 0.39588722586631775, LKD 4.724937438964844, LDE 3.891177177429199, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/105, Loss=8.981330794095992
Loss made of: CE 0.392859548330307, LKD 5.029743194580078, LDE 3.738435745239258, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/105, Loss=8.396984580159188
Loss made of: CE 0.3873576521873474, LKD 3.8633835315704346, LDE 3.237541675567627, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/105, Loss=8.89206492304802
Loss made of: CE 0.3900100886821747, LKD 4.000731468200684, LDE 4.914512634277344, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.42574557662010193, Reg Loss=8.362106323242188
Clinet index 4, End of Epoch 2/6, Average Loss=8.78785228729248, Class Loss=0.42574557662010193, Reg Loss=8.362106323242188
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/105, Loss=9.087721201777459
Loss made of: CE 0.495311975479126, LKD 4.283103942871094, LDE 4.199138164520264, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/105, Loss=8.652309522032738
Loss made of: CE 0.34891700744628906, LKD 4.734425067901611, LDE 3.316659927368164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/105, Loss=8.293107917904853
Loss made of: CE 0.3829271197319031, LKD 3.8834593296051025, LDE 3.251202344894409, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/105, Loss=8.878990796208381
Loss made of: CE 0.4027479887008667, LKD 3.8940205574035645, LDE 4.209325313568115, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/105, Loss=8.383928471803666
Loss made of: CE 0.43739795684814453, LKD 4.637686252593994, LDE 3.9119958877563477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/105, Loss=8.690221664309501
Loss made of: CE 0.40229496359825134, LKD 4.163482189178467, LDE 3.5656557083129883, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/105, Loss=8.609059876203537
Loss made of: CE 0.45759257674217224, LKD 6.0111870765686035, LDE 3.6497859954833984, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/105, Loss=8.40314824283123
Loss made of: CE 0.396467387676239, LKD 4.870436668395996, LDE 2.561396837234497, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/105, Loss=8.642455425858497
Loss made of: CE 0.4684000015258789, LKD 4.906836986541748, LDE 3.535156011581421, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/105, Loss=8.725508788228035
Loss made of: CE 0.4319877624511719, LKD 5.217566967010498, LDE 3.373110294342041, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4224125146865845, Reg Loss=8.184601783752441
Clinet index 4, End of Epoch 3/6, Average Loss=8.607014656066895, Class Loss=0.4224125146865845, Reg Loss=8.184601783752441
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/105, Loss=7.9694246590137485
Loss made of: CE 0.3454238772392273, LKD 3.7909369468688965, LDE 4.553675651550293, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/105, Loss=8.589383167028426
Loss made of: CE 0.4272676408290863, LKD 3.907832622528076, LDE 3.4227700233459473, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/105, Loss=8.741731414198876
Loss made of: CE 0.35308271646499634, LKD 5.055377006530762, LDE 3.9363107681274414, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/105, Loss=8.279579284787179
Loss made of: CE 0.4313264787197113, LKD 4.775270938873291, LDE 3.4439568519592285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/105, Loss=8.004645350575448
Loss made of: CE 0.37600192427635193, LKD 4.371923446655273, LDE 2.870770215988159, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/105, Loss=8.27173283994198
Loss made of: CE 0.4995504319667816, LKD 3.8212087154388428, LDE 4.261120796203613, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/105, Loss=8.113089856505393
Loss made of: CE 0.3766096234321594, LKD 4.90548038482666, LDE 2.6902859210968018, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/105, Loss=8.739782512187958
Loss made of: CE 0.4263293743133545, LKD 5.096737861633301, LDE 3.6096951961517334, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/105, Loss=8.79818053841591
Loss made of: CE 0.4409918189048767, LKD 5.097972393035889, LDE 2.8939907550811768, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/105, Loss=8.546537604928016
Loss made of: CE 0.47405552864074707, LKD 4.231647968292236, LDE 3.111747980117798, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4162483811378479, Reg Loss=7.975230693817139
Clinet index 4, End of Epoch 4/6, Average Loss=8.3914794921875, Class Loss=0.4162483811378479, Reg Loss=7.975230693817139
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/105, Loss=8.199137324094773
Loss made of: CE 0.4689721465110779, LKD 4.254227638244629, LDE 3.5668559074401855, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/105, Loss=8.498210859298705
Loss made of: CE 0.4708620011806488, LKD 4.859986782073975, LDE 4.303966045379639, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/105, Loss=8.254912412166595
Loss made of: CE 0.4417714476585388, LKD 4.79965353012085, LDE 3.268498182296753, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/105, Loss=8.770397028326988
Loss made of: CE 0.45553308725357056, LKD 4.110628128051758, LDE 3.3443405628204346, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/105, Loss=7.966421365737915
Loss made of: CE 0.39283156394958496, LKD 3.843311309814453, LDE 3.002789258956909, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/105, Loss=8.75278457403183
Loss made of: CE 0.4934252202510834, LKD 3.532386541366577, LDE 5.22849702835083, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/105, Loss=8.234975802898408
Loss made of: CE 0.35544008016586304, LKD 3.787013053894043, LDE 3.815260171890259, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/105, Loss=8.385687294602395
Loss made of: CE 0.38349565863609314, LKD 3.725606918334961, LDE 4.009832382202148, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/105, Loss=8.358308705687524
Loss made of: CE 0.497689425945282, LKD 4.5403828620910645, LDE 2.3141777515411377, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/105, Loss=8.065595763921738
Loss made of: CE 0.43027162551879883, LKD 4.761005401611328, LDE 2.7685463428497314, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.41893646121025085, Reg Loss=7.925984859466553
Clinet index 4, End of Epoch 5/6, Average Loss=8.344921112060547, Class Loss=0.41893646121025085, Reg Loss=7.925984859466553
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/105, Loss=8.55903980433941
Loss made of: CE 0.43025779724121094, LKD 3.5509915351867676, LDE 3.062303066253662, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/105, Loss=8.466643759608269
Loss made of: CE 0.4025202691555023, LKD 4.630324363708496, LDE 3.213742733001709, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/105, Loss=7.7790730625391005
Loss made of: CE 0.38364899158477783, LKD 3.8445160388946533, LDE 4.702228546142578, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/105, Loss=8.366451358795166
Loss made of: CE 0.37607330083847046, LKD 4.2130818367004395, LDE 3.87349271774292, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/105, Loss=8.055819112062455
Loss made of: CE 0.41801533102989197, LKD 4.036858081817627, LDE 4.441830635070801, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/105, Loss=8.281619840860367
Loss made of: CE 0.3949059844017029, LKD 4.718449115753174, LDE 3.4227230548858643, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/105, Loss=8.21650864481926
Loss made of: CE 0.38858234882354736, LKD 3.7474334239959717, LDE 3.1951916217803955, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/105, Loss=8.103526943922043
Loss made of: CE 0.41354936361312866, LKD 4.001767158508301, LDE 2.8133533000946045, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/105, Loss=8.227809309959412
Loss made of: CE 0.4716452956199646, LKD 4.416481971740723, LDE 3.4257233142852783, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/105, Loss=7.912429705262184
Loss made of: CE 0.38536667823791504, LKD 4.2131266593933105, LDE 2.5624146461486816, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.41221103072166443, Reg Loss=7.755152225494385
Clinet index 4, End of Epoch 6/6, Average Loss=8.167363166809082, Class Loss=0.41221103072166443, Reg Loss=7.755152225494385
Current Client Index:  7
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000631
Epoch 1, Batch 10/102, Loss=8.745271629095077
Loss made of: CE 0.4568294882774353, LKD 5.183689117431641, LDE 3.404745578765869, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/102, Loss=8.827682667970658
Loss made of: CE 0.5501248240470886, LKD 4.648935794830322, LDE 4.283236503601074, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/102, Loss=8.751870861649513
Loss made of: CE 0.42935436964035034, LKD 4.423437118530273, LDE 3.3724586963653564, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/102, Loss=9.22546543776989
Loss made of: CE 0.47434544563293457, LKD 5.868447780609131, LDE 4.708960056304932, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/102, Loss=8.991490918397904
Loss made of: CE 0.45248913764953613, LKD 4.717507839202881, LDE 3.3547110557556152, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/102, Loss=8.846066412329673
Loss made of: CE 0.38952189683914185, LKD 4.431675434112549, LDE 3.0615310668945312, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/102, Loss=8.408050018548966
Loss made of: CE 0.37771615386009216, LKD 4.056957721710205, LDE 4.302236557006836, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/102, Loss=8.790344673395158
Loss made of: CE 0.39555031061172485, LKD 5.90647554397583, LDE 3.7835030555725098, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/102, Loss=8.26884044110775
Loss made of: CE 0.4473121166229248, LKD 4.574124336242676, LDE 2.808612585067749, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/102, Loss=8.66828393638134
Loss made of: CE 0.45186346769332886, LKD 4.508857250213623, LDE 3.4104785919189453, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.42543739080429077, Reg Loss=8.31932544708252
Clinet index 7, End of Epoch 1/6, Average Loss=8.744762420654297, Class Loss=0.42543739080429077, Reg Loss=8.31932544708252
Pseudo labeling is: None
Epoch 2, lr = 0.000536
Epoch 2, Batch 10/102, Loss=8.550565227866173
Loss made of: CE 0.3743712306022644, LKD 4.401350021362305, LDE 2.6964333057403564, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/102, Loss=8.132658141851426
Loss made of: CE 0.4444878101348877, LKD 4.60294771194458, LDE 3.499053478240967, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/102, Loss=8.527083429694176
Loss made of: CE 0.33292466402053833, LKD 5.113382816314697, LDE 3.8185458183288574, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/102, Loss=8.422771495580673
Loss made of: CE 0.4042339324951172, LKD 3.8743481636047363, LDE 3.74949049949646, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/102, Loss=8.580788105726242
Loss made of: CE 0.40915200114250183, LKD 3.869605541229248, LDE 2.962827682495117, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/102, Loss=8.523405939340591
Loss made of: CE 0.34418758749961853, LKD 3.8566782474517822, LDE 2.7514612674713135, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/102, Loss=8.87653752863407
Loss made of: CE 0.4021739363670349, LKD 4.164960861206055, LDE 3.5626516342163086, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/102, Loss=8.331147888302803
Loss made of: CE 0.37832146883010864, LKD 4.580351829528809, LDE 3.3793530464172363, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/102, Loss=8.656985992193222
Loss made of: CE 0.39820289611816406, LKD 4.371671199798584, LDE 3.1090199947357178, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/102, Loss=8.123576670885086
Loss made of: CE 0.40109074115753174, LKD 4.273128509521484, LDE 4.1254563331604, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.41192638874053955, Reg Loss=8.076427459716797
Clinet index 7, End of Epoch 2/6, Average Loss=8.488353729248047, Class Loss=0.41192638874053955, Reg Loss=8.076427459716797
Pseudo labeling is: None
Epoch 3, lr = 0.000438
Epoch 3, Batch 10/102, Loss=8.83937602043152
Loss made of: CE 0.4217303693294525, LKD 4.7252678871154785, LDE 3.002211093902588, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/102, Loss=8.200198954343795
Loss made of: CE 0.36878499388694763, LKD 5.2183685302734375, LDE 2.4707841873168945, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/102, Loss=8.39956210553646
Loss made of: CE 0.3831617534160614, LKD 4.663447856903076, LDE 3.3164873123168945, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/102, Loss=8.1315229088068
Loss made of: CE 0.38921302556991577, LKD 4.085832595825195, LDE 3.292729616165161, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/102, Loss=8.815403997898102
Loss made of: CE 0.46492061018943787, LKD 4.14103889465332, LDE 5.664353847503662, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/102, Loss=8.267699894309043
Loss made of: CE 0.4836292266845703, LKD 4.607667922973633, LDE 2.9294517040252686, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/102, Loss=8.640170964598656
Loss made of: CE 0.470644474029541, LKD 5.4511003494262695, LDE 3.219334840774536, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/102, Loss=8.475873485207558
Loss made of: CE 0.3700927197933197, LKD 5.740389347076416, LDE 3.3839516639709473, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/102, Loss=8.663801610469818
Loss made of: CE 0.4134117364883423, LKD 5.115479946136475, LDE 3.0308377742767334, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/102, Loss=8.070355388522149
Loss made of: CE 0.42746782302856445, LKD 3.8044447898864746, LDE 3.138120174407959, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.40878599882125854, Reg Loss=8.026656150817871
Clinet index 7, End of Epoch 3/6, Average Loss=8.435441970825195, Class Loss=0.40878599882125854, Reg Loss=8.026656150817871
Pseudo labeling is: None
Epoch 4, lr = 0.000338
Epoch 4, Batch 10/102, Loss=8.101205524802207
Loss made of: CE 0.40244409441947937, LKD 4.308881759643555, LDE 3.8130977153778076, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/102, Loss=8.632665279507638
Loss made of: CE 0.34074217081069946, LKD 5.346317291259766, LDE 3.6667048931121826, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/102, Loss=8.891287589073182
Loss made of: CE 0.4838493764400482, LKD 5.0658488273620605, LDE 2.5280444622039795, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/102, Loss=8.72512358725071
Loss made of: CE 0.36370354890823364, LKD 4.150775909423828, LDE 3.320939064025879, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/102, Loss=8.442823258042335
Loss made of: CE 0.4104464054107666, LKD 4.8201751708984375, LDE 3.044332265853882, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/102, Loss=8.296980941295624
Loss made of: CE 0.4245152473449707, LKD 4.4790358543396, LDE 3.530208110809326, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/102, Loss=8.284419021010399
Loss made of: CE 0.46001386642456055, LKD 4.492254257202148, LDE 2.8440937995910645, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/102, Loss=8.467648601531982
Loss made of: CE 0.4262107014656067, LKD 4.8545026779174805, LDE 3.1783103942871094, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/102, Loss=8.495781233906746
Loss made of: CE 0.4373922348022461, LKD 4.016844749450684, LDE 4.01679801940918, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/102, Loss=8.562670370936393
Loss made of: CE 0.4422447681427002, LKD 4.780831813812256, LDE 3.3601298332214355, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4115467071533203, Reg Loss=8.076266288757324
Clinet index 7, End of Epoch 4/6, Average Loss=8.487812995910645, Class Loss=0.4115467071533203, Reg Loss=8.076266288757324
Pseudo labeling is: None
Epoch 5, lr = 0.000235
Epoch 5, Batch 10/102, Loss=8.473959252238274
Loss made of: CE 0.5482430458068848, LKD 4.93140172958374, LDE 3.4096262454986572, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/102, Loss=8.000903016328811
Loss made of: CE 0.5229760408401489, LKD 4.390841007232666, LDE 2.6610116958618164, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/102, Loss=8.552182418107986
Loss made of: CE 0.38709113001823425, LKD 4.726291656494141, LDE 3.283893346786499, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/102, Loss=8.348051911592483
Loss made of: CE 0.36512884497642517, LKD 4.8587422370910645, LDE 2.728257656097412, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/102, Loss=7.90459258556366
Loss made of: CE 0.4326675832271576, LKD 4.9337873458862305, LDE 3.1598434448242188, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/102, Loss=8.234341722726821
Loss made of: CE 0.4214647710323334, LKD 4.963726997375488, LDE 2.773778200149536, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/102, Loss=8.60312253832817
Loss made of: CE 0.3548926115036011, LKD 4.340047836303711, LDE 4.320074081420898, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/102, Loss=8.621973013877868
Loss made of: CE 0.4142119586467743, LKD 4.008336544036865, LDE 3.4568421840667725, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/102, Loss=8.457486766576768
Loss made of: CE 0.4167463779449463, LKD 5.066784858703613, LDE 3.4506797790527344, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/102, Loss=8.184935656189918
Loss made of: CE 0.4499194025993347, LKD 4.158882141113281, LDE 3.239720344543457, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.40968057513237, Reg Loss=7.9263410568237305
Clinet index 7, End of Epoch 5/6, Average Loss=8.336021423339844, Class Loss=0.40968057513237, Reg Loss=7.9263410568237305
Pseudo labeling is: None
Epoch 6, lr = 0.000126
Epoch 6, Batch 10/102, Loss=8.326721435785293
Loss made of: CE 0.36173582077026367, LKD 4.729287147521973, LDE 2.1256794929504395, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/102, Loss=8.41940442621708
Loss made of: CE 0.3888199031352997, LKD 5.025205135345459, LDE 2.4590649604797363, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/102, Loss=8.240798896551132
Loss made of: CE 0.37289318442344666, LKD 4.979930400848389, LDE 2.528827428817749, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/102, Loss=8.827524101734161
Loss made of: CE 0.4154796004295349, LKD 4.311241149902344, LDE 2.721294403076172, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/102, Loss=8.214548408985138
Loss made of: CE 0.44115322828292847, LKD 4.5264153480529785, LDE 3.65144681930542, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/102, Loss=7.9017964720726015
Loss made of: CE 0.45494765043258667, LKD 4.401369571685791, LDE 2.887881278991699, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/102, Loss=8.058671829104423
Loss made of: CE 0.3552292287349701, LKD 5.0940656661987305, LDE 3.447164535522461, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/102, Loss=8.50411496758461
Loss made of: CE 0.42477697134017944, LKD 5.38185453414917, LDE 4.0882697105407715, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/102, Loss=8.237382778525353
Loss made of: CE 0.4321977496147156, LKD 5.441224098205566, LDE 2.628247022628784, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/102, Loss=7.944516092538834
Loss made of: CE 0.440004825592041, LKD 4.368921756744385, LDE 2.5096933841705322, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.40673989057540894, Reg Loss=7.870891094207764
Clinet index 7, End of Epoch 6/6, Average Loss=8.277630805969238, Class Loss=0.40673989057540894, Reg Loss=7.870891094207764
Current Client Index:  17
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000694
Epoch 1, Batch 10/23, Loss=10.872428545355797
Loss made of: CE 0.591023325920105, LKD 4.14292049407959, LDE 5.014920234680176, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/23, Loss=9.753781393170357
Loss made of: CE 0.5997985601425171, LKD 4.981510162353516, LDE 4.6485185623168945, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5640631914138794, Reg Loss=9.76990795135498
Clinet index 17, End of Epoch 1/6, Average Loss=10.33397102355957, Class Loss=0.5640631914138794, Reg Loss=9.76990795135498
Pseudo labeling is: None
Epoch 2, lr = 0.000589
Epoch 2, Batch 10/23, Loss=9.722546640038491
Loss made of: CE 0.4672977924346924, LKD 4.161639213562012, LDE 3.9544517993927, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/23, Loss=9.2734945833683
Loss made of: CE 0.4542834162712097, LKD 5.080051422119141, LDE 4.721627712249756, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.4926985502243042, Reg Loss=9.039381980895996
Clinet index 17, End of Epoch 2/6, Average Loss=9.53208065032959, Class Loss=0.4926985502243042, Reg Loss=9.039381980895996
Pseudo labeling is: None
Epoch 3, lr = 0.000482
Epoch 3, Batch 10/23, Loss=9.18245259821415
Loss made of: CE 0.3197475075721741, LKD 4.091551303863525, LDE 3.880918025970459, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/23, Loss=9.3624896556139
Loss made of: CE 0.4624652862548828, LKD 5.1546430587768555, LDE 4.946563720703125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4533267319202423, Reg Loss=8.704070091247559
Clinet index 17, End of Epoch 3/6, Average Loss=9.157397270202637, Class Loss=0.4533267319202423, Reg Loss=8.704070091247559
Pseudo labeling is: None
Epoch 4, lr = 0.000372
Epoch 4, Batch 10/23, Loss=9.241503086686134
Loss made of: CE 0.5015255212783813, LKD 5.328948497772217, LDE 3.882932662963867, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/23, Loss=9.090766602754593
Loss made of: CE 0.41450369358062744, LKD 4.9602460861206055, LDE 4.463179588317871, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4516841173171997, Reg Loss=8.712318420410156
Clinet index 17, End of Epoch 4/6, Average Loss=9.164002418518066, Class Loss=0.4516841173171997, Reg Loss=8.712318420410156
Pseudo labeling is: None
Epoch 5, lr = 0.000258
Epoch 5, Batch 10/23, Loss=9.140138521790504
Loss made of: CE 0.4130896031856537, LKD 3.8305130004882812, LDE 3.723388910293579, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/23, Loss=8.929896929860115
Loss made of: CE 0.39298877120018005, LKD 4.382052421569824, LDE 3.7545926570892334, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.43104109168052673, Reg Loss=8.609128952026367
Clinet index 17, End of Epoch 5/6, Average Loss=9.040169715881348, Class Loss=0.43104109168052673, Reg Loss=8.609128952026367
Pseudo labeling is: None
Epoch 6, lr = 0.000138
Epoch 6, Batch 10/23, Loss=9.446748122572899
Loss made of: CE 0.4572579860687256, LKD 3.9666202068328857, LDE 3.9518520832061768, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/23, Loss=8.849164429306985
Loss made of: CE 0.439006507396698, LKD 4.14021110534668, LDE 4.323615074157715, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4317781329154968, Reg Loss=8.736533164978027
Clinet index 17, End of Epoch 6/6, Average Loss=9.16831111907959, Class Loss=0.4317781329154968, Reg Loss=8.736533164978027
Current Client Index:  15
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/103, Loss=8.736793079972268
Loss made of: CE 0.3970210552215576, LKD 4.871375560760498, LDE 3.1425018310546875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/103, Loss=8.668151727318763
Loss made of: CE 0.44371867179870605, LKD 4.393877983093262, LDE 3.86741304397583, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 30/103, Loss=8.520958581566811
Loss made of: CE 0.41015028953552246, LKD 4.433529853820801, LDE 3.184434413909912, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 40/103, Loss=8.795534452795982
Loss made of: CE 0.40109843015670776, LKD 4.239734172821045, LDE 3.29325270652771, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 50/103, Loss=8.795390263199806
Loss made of: CE 0.524941623210907, LKD 4.497216701507568, LDE 3.694566011428833, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 60/103, Loss=8.913558179140091
Loss made of: CE 0.3827323913574219, LKD 4.257002353668213, LDE 4.7117838859558105, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 70/103, Loss=8.968361875414848
Loss made of: CE 0.3901247978210449, LKD 4.612842082977295, LDE 3.6825478076934814, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 80/103, Loss=9.007427206635475
Loss made of: CE 0.4192960858345032, LKD 3.852644205093384, LDE 4.3655571937561035, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 90/103, Loss=8.519226294755935
Loss made of: CE 0.3905608057975769, LKD 4.760336875915527, LDE 3.312863349914551, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 100/103, Loss=8.837816324830055
Loss made of: CE 0.43620213866233826, LKD 4.885536193847656, LDE 3.240527629852295, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.42891934514045715, Reg Loss=8.378265380859375
Clinet index 15, End of Epoch 1/6, Average Loss=8.807185173034668, Class Loss=0.42891934514045715, Reg Loss=8.378265380859375
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/103, Loss=9.054155924916268
Loss made of: CE 0.34865090250968933, LKD 4.450544834136963, LDE 3.52182936668396, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/103, Loss=8.631778883934022
Loss made of: CE 0.40888357162475586, LKD 4.473814487457275, LDE 3.7745447158813477, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 30/103, Loss=8.659961858391762
Loss made of: CE 0.4439921975135803, LKD 4.089142799377441, LDE 4.315848350524902, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 40/103, Loss=8.49316572546959
Loss made of: CE 0.39796197414398193, LKD 4.387507438659668, LDE 4.256402492523193, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 50/103, Loss=8.521833631396294
Loss made of: CE 0.4365886449813843, LKD 4.509924411773682, LDE 3.1061654090881348, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 60/103, Loss=9.190178862214088
Loss made of: CE 0.3971819281578064, LKD 4.674153804779053, LDE 3.3978283405303955, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 70/103, Loss=8.359278190135957
Loss made of: CE 0.3467545211315155, LKD 4.309518337249756, LDE 3.4906671047210693, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 80/103, Loss=8.19241106212139
Loss made of: CE 0.37757036089897156, LKD 4.294416427612305, LDE 4.065896034240723, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 90/103, Loss=8.334742054343224
Loss made of: CE 0.30026352405548096, LKD 3.293344020843506, LDE 4.283936500549316, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 100/103, Loss=9.490150997042656
Loss made of: CE 0.45139551162719727, LKD 4.692821025848389, LDE 4.190088272094727, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.415100634098053, Reg Loss=8.276360511779785
Clinet index 15, End of Epoch 2/6, Average Loss=8.691461563110352, Class Loss=0.415100634098053, Reg Loss=8.276360511779785
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/103, Loss=8.858771106600761
Loss made of: CE 0.37372177839279175, LKD 4.4558329582214355, LDE 3.198730230331421, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/103, Loss=8.323812916874886
Loss made of: CE 0.4450148940086365, LKD 4.46893835067749, LDE 3.993873357772827, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 30/103, Loss=8.272941610217094
Loss made of: CE 0.5594121813774109, LKD 4.789370536804199, LDE 3.0267045497894287, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 40/103, Loss=8.793112203478813
Loss made of: CE 0.4642091989517212, LKD 4.322298049926758, LDE 3.390277147293091, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 50/103, Loss=8.713068184256553
Loss made of: CE 0.40605491399765015, LKD 4.455123424530029, LDE 5.762308120727539, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 60/103, Loss=8.25386688709259
Loss made of: CE 0.41119152307510376, LKD 4.936389446258545, LDE 3.3083863258361816, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 70/103, Loss=8.623350217938423
Loss made of: CE 0.46044421195983887, LKD 4.159235000610352, LDE 4.854107856750488, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 80/103, Loss=8.081431302428246
Loss made of: CE 0.32603004574775696, LKD 4.440436840057373, LDE 2.9040732383728027, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 90/103, Loss=8.780768087506294
Loss made of: CE 0.4004850387573242, LKD 4.559834957122803, LDE 3.339263916015625, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 100/103, Loss=8.589339676499367
Loss made of: CE 0.4133847653865814, LKD 5.297015190124512, LDE 4.647834300994873, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4093579351902008, Reg Loss=8.103389739990234
Clinet index 15, End of Epoch 3/6, Average Loss=8.512747764587402, Class Loss=0.4093579351902008, Reg Loss=8.103389739990234
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/103, Loss=8.828271809220315
Loss made of: CE 0.44554492831230164, LKD 5.4904890060424805, LDE 3.2079834938049316, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/103, Loss=8.606002175807953
Loss made of: CE 0.44390004873275757, LKD 4.680686950683594, LDE 4.814438343048096, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 30/103, Loss=8.474994811415673
Loss made of: CE 0.415939062833786, LKD 4.473496437072754, LDE 3.5320003032684326, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 40/103, Loss=7.926866912841797
Loss made of: CE 0.3683300018310547, LKD 3.885636806488037, LDE 3.431260585784912, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 50/103, Loss=8.208809405565262
Loss made of: CE 0.3781129717826843, LKD 4.507122993469238, LDE 3.225924015045166, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 60/103, Loss=8.704770255088807
Loss made of: CE 0.3890575170516968, LKD 4.329105854034424, LDE 3.4844887256622314, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 70/103, Loss=8.440008595585823
Loss made of: CE 0.4210103452205658, LKD 4.226186752319336, LDE 4.278500556945801, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 80/103, Loss=8.30515802204609
Loss made of: CE 0.3166985511779785, LKD 3.7026565074920654, LDE 3.4130330085754395, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 90/103, Loss=8.416502523422242
Loss made of: CE 0.4618980586528778, LKD 4.3299407958984375, LDE 3.6083908081054688, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 100/103, Loss=8.400303161144256
Loss made of: CE 0.4081920087337494, LKD 3.882688283920288, LDE 3.7709813117980957, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4067997634410858, Reg Loss=8.016231536865234
Clinet index 15, End of Epoch 4/6, Average Loss=8.423030853271484, Class Loss=0.4067997634410858, Reg Loss=8.016231536865234
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/103, Loss=8.322163689136506
Loss made of: CE 0.35984882712364197, LKD 3.7541885375976562, LDE 3.3834304809570312, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/103, Loss=8.24618464410305
Loss made of: CE 0.39629459381103516, LKD 4.27608585357666, LDE 4.982867240905762, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 30/103, Loss=8.29534021615982
Loss made of: CE 0.36507388949394226, LKD 4.315123558044434, LDE 5.285923004150391, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 40/103, Loss=8.695252093672753
Loss made of: CE 0.3628903031349182, LKD 3.7360332012176514, LDE 5.418859958648682, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 50/103, Loss=8.103649446368218
Loss made of: CE 0.34403109550476074, LKD 4.1185383796691895, LDE 3.4622859954833984, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 60/103, Loss=8.184646275639533
Loss made of: CE 0.39072471857070923, LKD 4.56873893737793, LDE 3.2386817932128906, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 70/103, Loss=8.488130700588226
Loss made of: CE 0.442546010017395, LKD 4.732275009155273, LDE 3.2077279090881348, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 80/103, Loss=8.43573273718357
Loss made of: CE 0.3781782388687134, LKD 4.22678279876709, LDE 3.1379685401916504, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 90/103, Loss=8.788366681337356
Loss made of: CE 0.45422643423080444, LKD 5.313180446624756, LDE 3.4401702880859375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 100/103, Loss=8.592163565754891
Loss made of: CE 0.39877256751060486, LKD 4.287180423736572, LDE 3.2562241554260254, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4086407721042633, Reg Loss=7.98997688293457
Clinet index 15, End of Epoch 5/6, Average Loss=8.3986177444458, Class Loss=0.4086407721042633, Reg Loss=7.98997688293457
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/103, Loss=7.98500169813633
Loss made of: CE 0.3445400893688202, LKD 4.304673671722412, LDE 2.5341403484344482, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/103, Loss=8.593174332380295
Loss made of: CE 0.37008556723594666, LKD 4.755199909210205, LDE 2.4901692867279053, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 30/103, Loss=8.228972321748733
Loss made of: CE 0.4116388261318207, LKD 4.337728023529053, LDE 3.729039430618286, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 40/103, Loss=7.833873242139816
Loss made of: CE 0.4590774476528168, LKD 4.283773899078369, LDE 2.907564401626587, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 50/103, Loss=8.114626115560531
Loss made of: CE 0.3515063226222992, LKD 3.9464869499206543, LDE 2.885511875152588, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 60/103, Loss=8.125426357984542
Loss made of: CE 0.3416082262992859, LKD 4.5269455909729, LDE 3.948791265487671, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 70/103, Loss=8.996422386169433
Loss made of: CE 0.42056339979171753, LKD 4.781908988952637, LDE 4.0005011558532715, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 80/103, Loss=8.483093801140786
Loss made of: CE 0.46791085600852966, LKD 4.181917190551758, LDE 3.254298210144043, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 90/103, Loss=8.326236516237259
Loss made of: CE 0.4890929162502289, LKD 4.562161922454834, LDE 3.439326524734497, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 100/103, Loss=8.342187482118607
Loss made of: CE 0.41589343547821045, LKD 3.582756280899048, LDE 3.4407074451446533, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4024352431297302, Reg Loss=7.895857334136963
Clinet index 15, End of Epoch 6/6, Average Loss=8.29829216003418, Class Loss=0.4024352431297302, Reg Loss=7.895857334136963
federated aggregation...
Validation, Class Loss=0.465628981590271, Reg Loss=0.0 (without scaling)

Total samples: 1277.000000
Overall Acc: 0.848183
Mean Acc: 0.457053
FreqW Acc: 0.753858
Mean IoU: 0.352816
Class IoU:
	class 0: 0.89281434
	class 1: 0.11325221
	class 2: 0.0009451261
	class 3: 0.0
	class 4: 0.52230316
	class 5: 0.073295504
	class 6: 0.7333735
	class 7: 0.66968375
	class 8: 0.23461977
	class 9: 0.04568031
	class 10: 0.038155083
	class 11: 0.3064151
	class 12: 0.37888148
	class 13: 0.38376012
	class 14: 0.60868156
	class 15: 0.70854914
	class 16: 0.28745905
Class Acc:
	class 0: 0.9681888
	class 1: 0.11327269
	class 2: 0.00094607717
	class 3: 0.0
	class 4: 0.5448309
	class 5: 0.073307544
	class 6: 0.76193136
	class 7: 0.68709296
	class 8: 0.23569883
	class 9: 0.048493996
	class 10: 0.04236257
	class 11: 0.49538296
	class 12: 0.85337776
	class 13: 0.8406508
	class 14: 0.9013497
	class 15: 0.8814636
	class 16: 0.32154438

Filtering images...
	0/1449 ...
	1000/1449 ...
federated global round: 20, step: 4
select part of clients to conduct local training
[19, 23, 1, 8]
Current Client Index:  19
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=13.81700142621994
Loss made of: CE 1.7750213146209717, LKD 5.338850498199463, LDE 5.11561918258667, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=11.230433678627014
Loss made of: CE 1.3915036916732788, LKD 4.366250038146973, LDE 4.157630920410156, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.5218873023986816, Reg Loss=10.729114532470703
Clinet index 19, End of Epoch 1/6, Average Loss=12.251001358032227, Class Loss=1.5218873023986816, Reg Loss=10.729114532470703
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=10.15330775976181
Loss made of: CE 0.749265193939209, LKD 4.940235614776611, LDE 4.5302042961120605, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=9.926641410589218
Loss made of: CE 0.7677496671676636, LKD 4.384782314300537, LDE 4.4235100746154785, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.8882277011871338, Reg Loss=9.030776977539062
Clinet index 19, End of Epoch 2/6, Average Loss=9.919004440307617, Class Loss=0.8882277011871338, Reg Loss=9.030776977539062
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=8.997206366062164
Loss made of: CE 0.537346601486206, LKD 4.496365547180176, LDE 3.384039878845215, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=8.695850101113319
Loss made of: CE 0.5852937698364258, LKD 4.880192279815674, LDE 3.176764488220215, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6006045341491699, Reg Loss=8.27871036529541
Clinet index 19, End of Epoch 3/6, Average Loss=8.879314422607422, Class Loss=0.6006045341491699, Reg Loss=8.27871036529541
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=8.649695932865143
Loss made of: CE 0.5646398663520813, LKD 4.865716934204102, LDE 2.979379653930664, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=8.96650839149952
Loss made of: CE 0.3948659300804138, LKD 4.697697639465332, LDE 4.552956581115723, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5429750680923462, Reg Loss=8.424331665039062
Clinet index 19, End of Epoch 4/6, Average Loss=8.967307090759277, Class Loss=0.5429750680923462, Reg Loss=8.424331665039062
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=8.41759660243988
Loss made of: CE 0.6137565970420837, LKD 4.269222736358643, LDE 3.3173460960388184, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=8.63103296160698
Loss made of: CE 0.564083993434906, LKD 4.8228068351745605, LDE 3.488406181335449, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5304992198944092, Reg Loss=8.161438941955566
Clinet index 19, End of Epoch 5/6, Average Loss=8.691938400268555, Class Loss=0.5304992198944092, Reg Loss=8.161438941955566
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=8.626100462675094
Loss made of: CE 0.5149246454238892, LKD 3.93841552734375, LDE 3.133803129196167, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=8.56861433684826
Loss made of: CE 0.5235825777053833, LKD 4.158519744873047, LDE 3.0970466136932373, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5242118239402771, Reg Loss=8.057367324829102
Clinet index 19, End of Epoch 6/6, Average Loss=8.581579208374023, Class Loss=0.5242118239402771, Reg Loss=8.057367324829102
Current Client Index:  23
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/21, Loss=13.67626075744629
Loss made of: CE 1.1762789487838745, LKD 4.618905067443848, LDE 8.55589485168457, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/21, Loss=11.115203535556793
Loss made of: CE 1.1448323726654053, LKD 3.471085548400879, LDE 5.9485907554626465, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.2809202671051025, Reg Loss=10.982627868652344
Clinet index 23, End of Epoch 1/6, Average Loss=12.263547897338867, Class Loss=1.2809202671051025, Reg Loss=10.982627868652344
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/21, Loss=10.073314785957336
Loss made of: CE 0.9662708044052124, LKD 4.32561731338501, LDE 4.853996753692627, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/21, Loss=9.209357059001922
Loss made of: CE 0.6873430609703064, LKD 3.913619041442871, LDE 3.972968816757202, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.9291749596595764, Reg Loss=8.69875431060791
Clinet index 23, End of Epoch 2/6, Average Loss=9.6279296875, Class Loss=0.9291749596595764, Reg Loss=8.69875431060791
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/21, Loss=10.126680511236192
Loss made of: CE 0.6663405299186707, LKD 4.809115409851074, LDE 3.5819509029388428, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/21, Loss=9.52502475976944
Loss made of: CE 0.8090062141418457, LKD 4.612768650054932, LDE 4.296852111816406, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7100948691368103, Reg Loss=9.0354585647583
Clinet index 23, End of Epoch 3/6, Average Loss=9.745553016662598, Class Loss=0.7100948691368103, Reg Loss=9.0354585647583
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/21, Loss=9.520547696948052
Loss made of: CE 0.6398522853851318, LKD 4.565512180328369, LDE 4.543981075286865, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/21, Loss=8.701034367084503
Loss made of: CE 0.6133617758750916, LKD 3.821071147918701, LDE 3.2612099647521973, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6052863001823425, Reg Loss=8.582599639892578
Clinet index 23, End of Epoch 4/6, Average Loss=9.187886238098145, Class Loss=0.6052863001823425, Reg Loss=8.582599639892578
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/21, Loss=9.032795628905296
Loss made of: CE 0.49414122104644775, LKD 4.7244157791137695, LDE 5.227592468261719, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/21, Loss=8.816009318828582
Loss made of: CE 0.7298784255981445, LKD 3.5112640857696533, LDE 4.4351348876953125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5629467964172363, Reg Loss=8.358118057250977
Clinet index 23, End of Epoch 5/6, Average Loss=8.921064376831055, Class Loss=0.5629467964172363, Reg Loss=8.358118057250977
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/21, Loss=8.711463788151741
Loss made of: CE 0.5285956859588623, LKD 3.909342050552368, LDE 2.643636465072632, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/21, Loss=8.591022059321404
Loss made of: CE 0.5307826399803162, LKD 4.716947555541992, LDE 3.3748764991760254, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5447164177894592, Reg Loss=8.074565887451172
Clinet index 23, End of Epoch 6/6, Average Loss=8.619282722473145, Class Loss=0.5447164177894592, Reg Loss=8.074565887451172
Current Client Index:  1
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=17.208159494400025
Loss made of: CE 1.5423519611358643, LKD 6.43636417388916, LDE 8.426194190979004, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 1, Class Loss=1.2766026258468628, Reg Loss=14.565580368041992
Clinet index 1, End of Epoch 1/6, Average Loss=15.842183113098145, Class Loss=1.2766026258468628, Reg Loss=14.565580368041992
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/19, Loss=12.121276074647904
Loss made of: CE 0.5849547386169434, LKD 4.52807092666626, LDE 6.570033550262451, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.790649950504303, Reg Loss=10.959319114685059
Clinet index 1, End of Epoch 2/6, Average Loss=11.749969482421875, Class Loss=0.790649950504303, Reg Loss=10.959319114685059
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Batch 10/19, Loss=11.057118487358093
Loss made of: CE 0.592039942741394, LKD 5.716091156005859, LDE 6.530272960662842, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch 3, Class Loss=0.47773391008377075, Reg Loss=10.629373550415039
Clinet index 1, End of Epoch 3/6, Average Loss=11.107107162475586, Class Loss=0.47773391008377075, Reg Loss=10.629373550415039
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/19, Loss=10.361054188013076
Loss made of: CE 0.4088262915611267, LKD 4.995355129241943, LDE 3.833770751953125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.3718721866607666, Reg Loss=9.755812644958496
Clinet index 1, End of Epoch 4/6, Average Loss=10.127684593200684, Class Loss=0.3718721866607666, Reg Loss=9.755812644958496
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/19, Loss=9.718617895245552
Loss made of: CE 0.31162363290786743, LKD 4.949192523956299, LDE 3.7432785034179688, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.32810112833976746, Reg Loss=9.529919624328613
Clinet index 1, End of Epoch 5/6, Average Loss=9.858020782470703, Class Loss=0.32810112833976746, Reg Loss=9.529919624328613
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/19, Loss=9.829041400551796
Loss made of: CE 0.30776381492614746, LKD 4.417174816131592, LDE 4.421386241912842, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3014906048774719, Reg Loss=9.601815223693848
Clinet index 1, End of Epoch 6/6, Average Loss=9.903306007385254, Class Loss=0.3014906048774719, Reg Loss=9.601815223693848
Current Client Index:  8
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=15.575338304042816
Loss made of: CE 1.7204245328903198, LKD 4.931673049926758, LDE 5.98511266708374, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=11.896927440166474
Loss made of: CE 1.307483196258545, LKD 4.879974842071533, LDE 5.80320930480957, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=1.5449800491333008, Reg Loss=11.765607833862305
Clinet index 8, End of Epoch 1/6, Average Loss=13.310587882995605, Class Loss=1.5449800491333008, Reg Loss=11.765607833862305
Pseudo labeling is: None
Epoch 2, lr = 0.000970
Epoch 2, Batch 10/24, Loss=10.594774758815765
Loss made of: CE 1.15255606174469, LKD 4.851375579833984, LDE 5.469274520874023, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=10.22754939198494
Loss made of: CE 0.8896942138671875, LKD 4.102868556976318, LDE 4.956818580627441, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=1.132917881011963, Reg Loss=9.113182067871094
Clinet index 8, End of Epoch 2/6, Average Loss=10.246099472045898, Class Loss=1.132917881011963, Reg Loss=9.113182067871094
Pseudo labeling is: None
Epoch 3, lr = 0.000940
Epoch 3, Batch 10/24, Loss=9.50567826628685
Loss made of: CE 0.8447017073631287, LKD 4.033074378967285, LDE 4.212419509887695, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=9.09386694431305
Loss made of: CE 0.8969756960868835, LKD 4.260378837585449, LDE 4.4330291748046875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.8550056219100952, Reg Loss=8.386276245117188
Clinet index 8, End of Epoch 3/6, Average Loss=9.241281509399414, Class Loss=0.8550056219100952, Reg Loss=8.386276245117188
Pseudo labeling is: None
Epoch 4, lr = 0.000910
Epoch 4, Batch 10/24, Loss=8.81675102710724
Loss made of: CE 0.6885794401168823, LKD 3.6038029193878174, LDE 4.580204963684082, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=9.316734504699706
Loss made of: CE 0.8406867980957031, LKD 4.267631530761719, LDE 3.3498222827911377, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.735316276550293, Reg Loss=8.318869590759277
Clinet index 8, End of Epoch 4/6, Average Loss=9.05418586730957, Class Loss=0.735316276550293, Reg Loss=8.318869590759277
Pseudo labeling is: None
Epoch 5, lr = 0.000879
Epoch 5, Batch 10/24, Loss=8.676911228895188
Loss made of: CE 0.7883214950561523, LKD 4.3472580909729, LDE 4.933571815490723, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=9.056892728805542
Loss made of: CE 0.7201547622680664, LKD 4.230923175811768, LDE 3.913435697555542, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.7179896831512451, Reg Loss=8.090502738952637
Clinet index 8, End of Epoch 5/6, Average Loss=8.808492660522461, Class Loss=0.7179896831512451, Reg Loss=8.090502738952637
Pseudo labeling is: None
Epoch 6, lr = 0.000849
Epoch 6, Batch 10/24, Loss=8.857490491867065
Loss made of: CE 0.721645712852478, LKD 4.608752727508545, LDE 4.030757904052734, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=8.410849779844284
Loss made of: CE 0.7052803039550781, LKD 3.801243305206299, LDE 3.351304769515991, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.6967839598655701, Reg Loss=8.017533302307129
Clinet index 8, End of Epoch 6/6, Average Loss=8.714317321777344, Class Loss=0.6967839598655701, Reg Loss=8.017533302307129
federated aggregation...
Validation, Class Loss=0.5959988832473755, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.817626
Mean Acc: 0.351212
FreqW Acc: 0.700448
Mean IoU: 0.272835
Class IoU:
	class 0: 0.8580318
	class 1: 0.00015571443
	class 2: 0.00057528703
	class 3: 0.0
	class 4: 0.2816973
	class 5: 0.014190654
	class 6: 0.6094163
	class 7: 0.51942444
	class 8: 0.23156591
	class 9: 0.02700652
	class 10: 0.043088645
	class 11: 0.317938
	class 12: 0.38345468
	class 13: 0.38104337
	class 14: 0.6743705
	class 15: 0.7259889
	class 16: 0.20513326
	class 17: 0.18241599
	class 18: 0.012404714
	class 19: 0.25702754
	class 20: 0.0046010218
Class Acc:
	class 0: 0.98385817
	class 1: 0.00015571443
	class 2: 0.0005755303
	class 3: 0.0
	class 4: 0.28773856
	class 5: 0.014190654
	class 6: 0.6284302
	class 7: 0.5224192
	class 8: 0.23216748
	class 9: 0.027470361
	class 10: 0.04751838
	class 11: 0.39585787
	class 12: 0.8392324
	class 13: 0.8057328
	class 14: 0.8345256
	class 15: 0.8042169
	class 16: 0.21337967
	class 17: 0.23785393
	class 18: 0.012959603
	class 19: 0.48254013
	class 20: 0.0046188124

federated global round: 21, step: 4
select part of clients to conduct local training
[23, 14, 4, 11]
Current Client Index:  23
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/21, Loss=9.844778388738632
Loss made of: CE 0.8494467735290527, LKD 4.405501365661621, LDE 5.654507160186768, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/21, Loss=9.204605007171631
Loss made of: CE 0.7389835119247437, LKD 3.5461201667785645, LDE 4.122459888458252, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.784650444984436, Reg Loss=8.673958778381348
Clinet index 23, End of Epoch 1/6, Average Loss=9.458609580993652, Class Loss=0.784650444984436, Reg Loss=8.673958778381348
Pseudo labeling is: None
Epoch 2, lr = 0.000787
Epoch 2, Batch 10/21, Loss=8.897696751356126
Loss made of: CE 0.6951167583465576, LKD 4.202313423156738, LDE 4.102914810180664, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/21, Loss=8.24978872537613
Loss made of: CE 0.5207913517951965, LKD 3.7972898483276367, LDE 3.562917947769165, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6245101690292358, Reg Loss=7.94827938079834
Clinet index 23, End of Epoch 2/6, Average Loss=8.572789192199707, Class Loss=0.6245101690292358, Reg Loss=7.94827938079834
Pseudo labeling is: None
Epoch 3, lr = 0.000756
Epoch 3, Batch 10/21, Loss=9.427048516273498
Loss made of: CE 0.5503334403038025, LKD 4.924531936645508, LDE 3.210689067840576, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/21, Loss=8.873801609873771
Loss made of: CE 0.6394213438034058, LKD 4.750438690185547, LDE 3.717576026916504, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5657196640968323, Reg Loss=8.52069091796875
Clinet index 23, End of Epoch 3/6, Average Loss=9.086410522460938, Class Loss=0.5657196640968323, Reg Loss=8.52069091796875
Pseudo labeling is: None
Epoch 4, lr = 0.000725
Epoch 4, Batch 10/21, Loss=8.977664425969124
Loss made of: CE 0.5421532392501831, LKD 4.559751987457275, LDE 4.401280879974365, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/21, Loss=8.251061940193177
Loss made of: CE 0.5324493646621704, LKD 3.7790651321411133, LDE 3.0849359035491943, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5327904224395752, Reg Loss=8.168889999389648
Clinet index 23, End of Epoch 4/6, Average Loss=8.701680183410645, Class Loss=0.5327904224395752, Reg Loss=8.168889999389648
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=8.600880649685859
Loss made of: CE 0.4797441363334656, LKD 4.716160297393799, LDE 4.60231876373291, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/21, Loss=8.620208260416984
Loss made of: CE 0.611579418182373, LKD 3.548734664916992, LDE 3.9830856323242188, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5329922437667847, Reg Loss=8.063106536865234
Clinet index 23, End of Epoch 5/6, Average Loss=8.596098899841309, Class Loss=0.5329922437667847, Reg Loss=8.063106536865234
Pseudo labeling is: None
Epoch 6, lr = 0.000663
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 6, Batch 10/21, Loss=8.426461866497993
Loss made of: CE 0.46161386370658875, LKD 4.103937149047852, LDE 2.49721360206604, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/21, Loss=8.199451705813408
Loss made of: CE 0.48636263608932495, LKD 4.736478328704834, LDE 3.1473166942596436, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5058622360229492, Reg Loss=7.76251220703125
Clinet index 23, End of Epoch 6/6, Average Loss=8.2683744430542, Class Loss=0.5058622360229492, Reg Loss=7.76251220703125
Current Client Index:  14
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=10.797119230031967
Loss made of: CE 0.6943860054016113, LKD 4.7678327560424805, LDE 3.896921396255493, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=9.423365354537964
Loss made of: CE 0.6136139631271362, LKD 4.305392265319824, LDE 3.3036365509033203, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7671419382095337, Reg Loss=9.201155662536621
Clinet index 14, End of Epoch 1/6, Average Loss=9.968297958374023, Class Loss=0.7671419382095337, Reg Loss=9.201155662536621
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=9.446146684885026
Loss made of: CE 0.6398613452911377, LKD 5.342356204986572, LDE 3.964792251586914, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=9.1722712546587
Loss made of: CE 0.4438650608062744, LKD 4.474435329437256, LDE 4.943565845489502, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5953847169876099, Reg Loss=8.563970565795898
Clinet index 14, End of Epoch 2/6, Average Loss=9.159355163574219, Class Loss=0.5953847169876099, Reg Loss=8.563970565795898
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=8.775471639633178
Loss made of: CE 0.7847384214401245, LKD 4.6335906982421875, LDE 3.2106659412384033, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=9.009123298525811
Loss made of: CE 0.4963538646697998, LKD 5.014443397521973, LDE 3.054558277130127, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5440071821212769, Reg Loss=8.277260780334473
Clinet index 14, End of Epoch 3/6, Average Loss=8.821268081665039, Class Loss=0.5440071821212769, Reg Loss=8.277260780334473
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=8.438084849715233
Loss made of: CE 0.5043317079544067, LKD 4.851634502410889, LDE 4.858420372009277, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=8.89205954670906
Loss made of: CE 0.4984091520309448, LKD 5.017648220062256, LDE 3.596733808517456, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5101907849311829, Reg Loss=8.158063888549805
Clinet index 14, End of Epoch 4/6, Average Loss=8.668254852294922, Class Loss=0.5101907849311829, Reg Loss=8.158063888549805
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=8.138679522275925
Loss made of: CE 0.5003378987312317, LKD 4.9163079261779785, LDE 3.1923844814300537, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=8.784163251519203
Loss made of: CE 0.48055100440979004, LKD 4.218242645263672, LDE 2.794419765472412, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.49101394414901733, Reg Loss=7.961757659912109
Clinet index 14, End of Epoch 5/6, Average Loss=8.452771186828613, Class Loss=0.49101394414901733, Reg Loss=7.961757659912109
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=8.37949466407299
Loss made of: CE 0.5150468349456787, LKD 3.9644858837127686, LDE 3.3967857360839844, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=8.74245368540287
Loss made of: CE 0.38598769903182983, LKD 4.628558158874512, LDE 4.213469505310059, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.476462721824646, Reg Loss=8.114628791809082
Clinet index 14, End of Epoch 6/6, Average Loss=8.59109115600586, Class Loss=0.476462721824646, Reg Loss=8.114628791809082
Current Client Index:  4
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=10.790372383594512
Loss made of: CE 0.8932082653045654, LKD 5.3792619705200195, LDE 4.837752342224121, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=9.936874723434448
Loss made of: CE 0.6250466108322144, LKD 4.332442283630371, LDE 4.9662275314331055, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7609609961509705, Reg Loss=9.388434410095215
Clinet index 4, End of Epoch 1/6, Average Loss=10.149394989013672, Class Loss=0.7609609961509705, Reg Loss=9.388434410095215
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/26, Loss=9.169606548547744
Loss made of: CE 0.7516922950744629, LKD 6.3812971115112305, LDE 4.112265110015869, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=9.037925377488136
Loss made of: CE 0.553011417388916, LKD 4.836938858032227, LDE 3.3690075874328613, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5871709585189819, Reg Loss=8.626727104187012
Clinet index 4, End of Epoch 2/6, Average Loss=9.213897705078125, Class Loss=0.5871709585189819, Reg Loss=8.626727104187012
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/26, Loss=8.626297271251678
Loss made of: CE 0.5106059908866882, LKD 4.7203803062438965, LDE 3.34279203414917, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=8.91927444934845
Loss made of: CE 0.5858798623085022, LKD 4.9260478019714355, LDE 4.175452709197998, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5224074721336365, Reg Loss=8.237682342529297
Clinet index 4, End of Epoch 3/6, Average Loss=8.760089874267578, Class Loss=0.5224074721336365, Reg Loss=8.237682342529297
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/26, Loss=8.905285948514939
Loss made of: CE 0.5268199443817139, LKD 4.908763885498047, LDE 3.8206074237823486, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=8.175620254874229
Loss made of: CE 0.5585880875587463, LKD 4.336367607116699, LDE 3.3704280853271484, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 4, Class Loss=0.5010442733764648, Reg Loss=8.232386589050293
Clinet index 4, End of Epoch 4/6, Average Loss=8.733430862426758, Class Loss=0.5010442733764648, Reg Loss=8.232386589050293
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/26, Loss=8.643010675907135
Loss made of: CE 0.4857722520828247, LKD 4.116915702819824, LDE 3.598965644836426, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 20/26, Loss=8.94505078792572
Loss made of: CE 0.5498315095901489, LKD 4.887632846832275, LDE 3.328822612762451, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.49648386240005493, Reg Loss=8.209829330444336
Clinet index 4, End of Epoch 5/6, Average Loss=8.706313133239746, Class Loss=0.49648386240005493, Reg Loss=8.209829330444336
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/26, Loss=8.60220454633236
Loss made of: CE 0.4723471999168396, LKD 4.73762845993042, LDE 3.406209945678711, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=8.319437783956527
Loss made of: CE 0.44945308566093445, LKD 5.011191368103027, LDE 2.644404649734497, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4771727919578552, Reg Loss=8.031801223754883
Clinet index 4, End of Epoch 6/6, Average Loss=8.508974075317383, Class Loss=0.4771727919578552, Reg Loss=8.031801223754883
Current Client Index:  11
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=11.328998827934265
Loss made of: CE 1.0035898685455322, LKD 3.835576057434082, LDE 5.0853424072265625, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=9.821692484617234
Loss made of: CE 0.985560417175293, LKD 4.078258991241455, LDE 3.7802398204803467, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.970852255821228, Reg Loss=9.48215389251709
Clinet index 11, End of Epoch 1/6, Average Loss=10.45300579071045, Class Loss=0.970852255821228, Reg Loss=9.48215389251709
Pseudo labeling is: None
Epoch 2, lr = 0.000962
Epoch 2, Batch 10/24, Loss=9.255638384819031
Loss made of: CE 0.8677350878715515, LKD 4.418645858764648, LDE 4.920629978179932, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=9.249413812160492
Loss made of: CE 0.7505911588668823, LKD 4.554174900054932, LDE 4.535756587982178, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.7641358375549316, Reg Loss=8.455160140991211
Clinet index 11, End of Epoch 2/6, Average Loss=9.219295501708984, Class Loss=0.7641358375549316, Reg Loss=8.455160140991211
Pseudo labeling is: None
Epoch 3, lr = 0.000925
Epoch 3, Batch 10/24, Loss=8.658835047483445
Loss made of: CE 0.8028069734573364, LKD 4.340639591217041, LDE 4.61806583404541, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=8.978402251005173
Loss made of: CE 0.6829036474227905, LKD 3.848074436187744, LDE 4.777744293212891, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.7137338519096375, Reg Loss=8.262706756591797
Clinet index 11, End of Epoch 3/6, Average Loss=8.9764404296875, Class Loss=0.7137338519096375, Reg Loss=8.262706756591797
Pseudo labeling is: None
Epoch 4, lr = 0.000887
Epoch 4, Batch 10/24, Loss=8.379438996315002
Loss made of: CE 0.6722585558891296, LKD 4.367820739746094, LDE 3.67141056060791, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=8.689630490541457
Loss made of: CE 0.715936541557312, LKD 3.958953857421875, LDE 4.463742256164551, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6909257173538208, Reg Loss=7.9199371337890625
Clinet index 11, End of Epoch 4/6, Average Loss=8.610862731933594, Class Loss=0.6909257173538208, Reg Loss=7.9199371337890625
Pseudo labeling is: None
Epoch 5, lr = 0.000849
Epoch 5, Batch 10/24, Loss=8.595399576425553
Loss made of: CE 0.7009754180908203, LKD 4.024900436401367, LDE 3.8248159885406494, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=8.575490453839302
Loss made of: CE 0.6175702810287476, LKD 4.019806385040283, LDE 3.1926169395446777, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.6808116436004639, Reg Loss=7.896445274353027
Clinet index 11, End of Epoch 5/6, Average Loss=8.57725715637207, Class Loss=0.6808116436004639, Reg Loss=7.896445274353027
Pseudo labeling is: None
Epoch 6, lr = 0.000810
Epoch 6, Batch 10/24, Loss=8.163974785804749
Loss made of: CE 0.7539123296737671, LKD 4.756837844848633, LDE 4.265235900878906, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=8.364166444540023
Loss made of: CE 0.7463587522506714, LKD 4.780923843383789, LDE 3.5681746006011963, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.6544294953346252, Reg Loss=7.641576766967773
Clinet index 11, End of Epoch 6/6, Average Loss=8.296006202697754, Class Loss=0.6544294953346252, Reg Loss=7.641576766967773
federated aggregation...
Validation, Class Loss=0.6411550641059875, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.806746
Mean Acc: 0.372246
FreqW Acc: 0.705258
Mean IoU: 0.255531
Class IoU:
	class 0: 0.8781104
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.06402162
	class 5: 0.0
	class 6: 0.17031598
	class 7: 0.31452882
	class 8: 0.18028083
	class 9: 0.027056323
	class 10: 0.021771926
	class 11: 0.3537222
	class 12: 0.4172992
	class 13: 0.4156776
	class 14: 0.69027257
	class 15: 0.7326015
	class 16: 0.18598561
	class 17: 0.2999414
	class 18: 0.19353102
	class 19: 0.25086015
	class 20: 0.17016329
Class Acc:
	class 0: 0.9700433
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.06497899
	class 5: 0.0
	class 6: 0.17110708
	class 7: 0.31488925
	class 8: 0.18056948
	class 9: 0.027369104
	class 10: 0.02206876
	class 11: 0.43759415
	class 12: 0.74575424
	class 13: 0.77855444
	class 14: 0.76476634
	class 15: 0.8163166
	class 16: 0.19039844
	class 17: 0.67820215
	class 18: 0.24446031
	class 19: 0.7411336
	class 20: 0.6689578

federated global round: 22, step: 4
select part of clients to conduct local training
[0, 8, 16, 14]
Current Client Index:  0
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=9.671864461898803
Loss made of: CE 0.8102331161499023, LKD 4.449005126953125, LDE 3.9287357330322266, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=9.378893405199051
Loss made of: CE 0.6987020373344421, LKD 4.022977828979492, LDE 4.243459701538086, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7597526907920837, Reg Loss=8.694857597351074
Clinet index 0, End of Epoch 1/6, Average Loss=9.454609870910645, Class Loss=0.7597526907920837, Reg Loss=8.694857597351074
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Epoch 2, Batch 10/24, Loss=8.537718099355697
Loss made of: CE 0.6984893679618835, LKD 3.8502283096313477, LDE 3.267826795578003, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=8.45432568192482
Loss made of: CE 0.7403934001922607, LKD 3.518019914627075, LDE 3.687190294265747, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6837925314903259, Reg Loss=7.788270950317383
Clinet index 0, End of Epoch 2/6, Average Loss=8.472063064575195, Class Loss=0.6837925314903259, Reg Loss=7.788270950317383
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/24, Loss=8.628193825483322
Loss made of: CE 0.6851224303245544, LKD 4.753026485443115, LDE 5.550106525421143, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=8.388222408294677
Loss made of: CE 0.5924460887908936, LKD 4.174552917480469, LDE 3.319427490234375, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6504665613174438, Reg Loss=7.793724060058594
Clinet index 0, End of Epoch 3/6, Average Loss=8.444190979003906, Class Loss=0.6504665613174438, Reg Loss=7.793724060058594
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/24, Loss=8.106568843126297
Loss made of: CE 0.7017917633056641, LKD 3.914416551589966, LDE 3.7106707096099854, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=8.168218779563905
Loss made of: CE 0.571123480796814, LKD 3.8625195026397705, LDE 3.782456636428833, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6435215473175049, Reg Loss=7.502885341644287
Clinet index 0, End of Epoch 4/6, Average Loss=8.146407127380371, Class Loss=0.6435215473175049, Reg Loss=7.502885341644287
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/24, Loss=8.36579978466034
Loss made of: CE 0.5885025262832642, LKD 4.210371017456055, LDE 3.593640089035034, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=8.380845123529435
Loss made of: CE 0.6901462078094482, LKD 4.095767974853516, LDE 4.3260297775268555, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.6259424090385437, Reg Loss=7.662984371185303
Clinet index 0, End of Epoch 5/6, Average Loss=8.28892707824707, Class Loss=0.6259424090385437, Reg Loss=7.662984371185303
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/24, Loss=7.944226717948913
Loss made of: CE 0.6769177317619324, LKD 3.998260498046875, LDE 4.377993106842041, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=7.896244323253631
Loss made of: CE 0.5947377681732178, LKD 3.8018622398376465, LDE 2.8413286209106445, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.6315410137176514, Reg Loss=7.2959723472595215
Clinet index 0, End of Epoch 6/6, Average Loss=7.927513122558594, Class Loss=0.6315410137176514, Reg Loss=7.2959723472595215
Current Client Index:  8
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000818
Epoch 1, Batch 10/24, Loss=10.3678402364254
Loss made of: CE 0.9254599809646606, LKD 5.012776851654053, LDE 3.896897554397583, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=8.92776687145233
Loss made of: CE 0.7274069786071777, LKD 4.6092400550842285, LDE 3.9543027877807617, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.8017058372497559, Reg Loss=8.791328430175781
Clinet index 8, End of Epoch 1/6, Average Loss=9.593034744262695, Class Loss=0.8017058372497559, Reg Loss=8.791328430175781
Pseudo labeling is: None
Epoch 2, lr = 0.000777
Epoch 2, Batch 10/24, Loss=8.891322779655457
Loss made of: CE 0.7006976008415222, LKD 4.962765216827393, LDE 3.720327377319336, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=8.661681747436523
Loss made of: CE 0.6816073656082153, LKD 4.042823791503906, LDE 3.8088173866271973, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.7296528220176697, Reg Loss=7.915406227111816
Clinet index 8, End of Epoch 2/6, Average Loss=8.645058631896973, Class Loss=0.7296528220176697, Reg Loss=7.915406227111816
Pseudo labeling is: None
Epoch 3, lr = 0.000736
Epoch 3, Batch 10/24, Loss=8.358964920043945
Loss made of: CE 0.6052482724189758, LKD 4.09176778793335, LDE 3.552532911300659, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=8.118673646450043
Loss made of: CE 0.7974907159805298, LKD 4.357555389404297, LDE 4.009858131408691, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6813946962356567, Reg Loss=7.541286468505859
Clinet index 8, End of Epoch 3/6, Average Loss=8.222681045532227, Class Loss=0.6813946962356567, Reg Loss=7.541286468505859
Pseudo labeling is: None
Epoch 4, lr = 0.000694
Epoch 4, Batch 10/24, Loss=7.9416561901569365
Loss made of: CE 0.6558001637458801, LKD 3.577089786529541, LDE 3.64532470703125, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=8.458605647087097
Loss made of: CE 0.7618173360824585, LKD 4.660668849945068, LDE 2.929205894470215, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6575119495391846, Reg Loss=7.585616111755371
Clinet index 8, End of Epoch 4/6, Average Loss=8.243127822875977, Class Loss=0.6575119495391846, Reg Loss=7.585616111755371
Pseudo labeling is: None
Epoch 5, lr = 0.000652
Epoch 5, Batch 10/24, Loss=8.212665462493897
Loss made of: CE 0.7548993825912476, LKD 4.2782301902771, LDE 4.2884745597839355, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=8.306779301166534
Loss made of: CE 0.6988315582275391, LKD 4.286952972412109, LDE 3.069679021835327, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.677788496017456, Reg Loss=7.537485122680664
Clinet index 8, End of Epoch 5/6, Average Loss=8.2152738571167, Class Loss=0.677788496017456, Reg Loss=7.537485122680664
Pseudo labeling is: None
Epoch 6, lr = 0.000610
Epoch 6, Batch 10/24, Loss=8.242297875881196
Loss made of: CE 0.6465174555778503, LKD 4.214822292327881, LDE 3.752124309539795, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=7.882866978645325
Loss made of: CE 0.675210177898407, LKD 3.8800337314605713, LDE 2.949467182159424, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.6581863760948181, Reg Loss=7.518510818481445
Clinet index 8, End of Epoch 6/6, Average Loss=8.17669677734375, Class Loss=0.6581863760948181, Reg Loss=7.518510818481445
Current Client Index:  16
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=16.19375773668289
Loss made of: CE 0.5827826261520386, LKD 5.13765811920166, LDE 7.0606560707092285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5954691171646118, Reg Loss=13.9823637008667
Clinet index 16, End of Epoch 1/6, Average Loss=14.57783317565918, Class Loss=0.5954691171646118, Reg Loss=13.9823637008667
Pseudo labeling is: None
Epoch 2, lr = 0.000950
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 10/19, Loss=11.598330828547478
Loss made of: CE 0.4258890748023987, LKD 5.092106342315674, LDE 5.889451503753662, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.41031986474990845, Reg Loss=11.213397026062012
Clinet index 16, End of Epoch 2/6, Average Loss=11.623717308044434, Class Loss=0.41031986474990845, Reg Loss=11.213397026062012
Pseudo labeling is: None
Epoch 3, lr = 0.000899
Epoch 3, Batch 10/19, Loss=10.332500952482224
Loss made of: CE 0.30530285835266113, LKD 4.831592559814453, LDE 4.353500843048096, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 3, Class Loss=0.3469688296318054, Reg Loss=9.98293399810791
Clinet index 16, End of Epoch 3/6, Average Loss=10.329902648925781, Class Loss=0.3469688296318054, Reg Loss=9.98293399810791
Pseudo labeling is: None
Epoch 4, lr = 0.000849
Epoch 4, Batch 10/19, Loss=10.185998913645744
Loss made of: CE 0.3394060730934143, LKD 4.950921058654785, LDE 5.3502092361450195, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.31470128893852234, Reg Loss=9.910709381103516
Clinet index 16, End of Epoch 4/6, Average Loss=10.225410461425781, Class Loss=0.31470128893852234, Reg Loss=9.910709381103516
Pseudo labeling is: None
Epoch 5, lr = 0.000798
Epoch 5, Batch 10/19, Loss=9.800399854779243
Loss made of: CE 0.27300477027893066, LKD 4.803015232086182, LDE 4.383893966674805, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.33120790123939514, Reg Loss=9.647773742675781
Clinet index 16, End of Epoch 5/6, Average Loss=9.978981971740723, Class Loss=0.33120790123939514, Reg Loss=9.647773742675781
Pseudo labeling is: None
Epoch 6, lr = 0.000746
Epoch 6, Batch 10/19, Loss=9.528781419992447
Loss made of: CE 0.27139151096343994, LKD 4.535614490509033, LDE 4.247128009796143, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.3166970908641815, Reg Loss=9.250943183898926
Clinet index 16, End of Epoch 6/6, Average Loss=9.56764030456543, Class Loss=0.3166970908641815, Reg Loss=9.250943183898926
Current Client Index:  14
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000772
Epoch 1, Batch 10/26, Loss=9.635323619842529
Loss made of: CE 0.5068241357803345, LKD 4.764044284820557, LDE 2.884138822555542, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=8.588111913204193
Loss made of: CE 0.5823236703872681, LKD 4.398397922515869, LDE 3.1360373497009277, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5637022852897644, Reg Loss=8.474963188171387
Clinet index 14, End of Epoch 1/6, Average Loss=9.038665771484375, Class Loss=0.5637022852897644, Reg Loss=8.474963188171387
Pseudo labeling is: None
Epoch 2, lr = 0.000733
Epoch 2, Batch 10/26, Loss=8.727275732159615
Loss made of: CE 0.586334228515625, LKD 5.159684181213379, LDE 3.4524877071380615, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=8.699249464273453
Loss made of: CE 0.3790896534919739, LKD 4.763434410095215, LDE 4.591947555541992, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5130227208137512, Reg Loss=8.095712661743164
Clinet index 14, End of Epoch 2/6, Average Loss=8.608735084533691, Class Loss=0.5130227208137512, Reg Loss=8.095712661743164
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/26, Loss=8.379327538609505
Loss made of: CE 0.6980597376823425, LKD 4.435302257537842, LDE 2.472318410873413, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=8.464289861917496
Loss made of: CE 0.43299388885498047, LKD 5.047897815704346, LDE 2.7775440216064453, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.49476200342178345, Reg Loss=7.846526622772217
Clinet index 14, End of Epoch 3/6, Average Loss=8.341288566589355, Class Loss=0.49476200342178345, Reg Loss=7.846526622772217
Pseudo labeling is: None
Epoch 4, lr = 0.000655
Epoch 4, Batch 10/26, Loss=8.1177126288414
Loss made of: CE 0.44972383975982666, LKD 4.8497514724731445, LDE 4.916574954986572, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=8.43532453775406
Loss made of: CE 0.4596451222896576, LKD 5.043534755706787, LDE 2.79168701171875, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4722370505332947, Reg Loss=7.812363147735596
Clinet index 14, End of Epoch 4/6, Average Loss=8.284600257873535, Class Loss=0.4722370505332947, Reg Loss=7.812363147735596
Pseudo labeling is: None
Epoch 5, lr = 0.000616
Epoch 5, Batch 10/26, Loss=7.842375081777573
Loss made of: CE 0.4588600993156433, LKD 4.791721820831299, LDE 2.9163992404937744, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=8.512610420584679
Loss made of: CE 0.4639790654182434, LKD 4.383381366729736, LDE 2.6962742805480957, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4736584722995758, Reg Loss=7.708107948303223
Clinet index 14, End of Epoch 5/6, Average Loss=8.181766510009766, Class Loss=0.4736584722995758, Reg Loss=7.708107948303223
Pseudo labeling is: None
Epoch 6, lr = 0.000576
Epoch 6, Batch 10/26, Loss=8.01879941523075
Loss made of: CE 0.513957142829895, LKD 3.899097442626953, LDE 3.2815630435943604, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=8.43684654533863
Loss made of: CE 0.39344751834869385, LKD 4.516026496887207, LDE 3.6222290992736816, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4628428816795349, Reg Loss=7.83065128326416
Clinet index 14, End of Epoch 6/6, Average Loss=8.29349422454834, Class Loss=0.4628428816795349, Reg Loss=7.83065128326416
federated aggregation...
Validation, Class Loss=0.6567409038543701, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.800812
Mean Acc: 0.366708
FreqW Acc: 0.703702
Mean IoU: 0.243752
Class IoU:
	class 0: 0.8822438
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.010407281
	class 5: 0.0
	class 6: 0.023190646
	class 7: 0.24724054
	class 8: 0.2186951
	class 9: 0.022632375
	class 10: 0.0049029146
	class 11: 0.31638926
	class 12: 0.40730253
	class 13: 0.43141443
	class 14: 0.6543837
	class 15: 0.73418796
	class 16: 0.17130373
	class 17: 0.25058687
	class 18: 0.23554784
	class 19: 0.2135459
	class 20: 0.29482266
Class Acc:
	class 0: 0.96576256
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.01044462
	class 5: 0.0
	class 6: 0.023201885
	class 7: 0.24737895
	class 8: 0.21910308
	class 9: 0.022790335
	class 10: 0.004907659
	class 11: 0.36860707
	class 12: 0.6652124
	class 13: 0.73308414
	class 14: 0.7103861
	class 15: 0.8106547
	class 16: 0.17528078
	class 17: 0.8241946
	class 18: 0.4597453
	class 19: 0.7934363
	class 20: 0.6666847

federated global round: 23, step: 4
select part of clients to conduct local training
[12, 10, 9, 6]
Current Client Index:  12
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/26, Loss=9.21451294720173
Loss made of: CE 0.5073464512825012, LKD 3.999504566192627, LDE 2.699219226837158, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/26, Loss=8.759239837527275
Loss made of: CE 0.5329594612121582, LKD 4.3035688400268555, LDE 2.5707080364227295, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5367612838745117, Reg Loss=8.499938011169434
Clinet index 12, End of Epoch 1/6, Average Loss=9.036699295043945, Class Loss=0.5367612838745117, Reg Loss=8.499938011169434
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/26, Loss=8.405379608273506
Loss made of: CE 0.4941645562648773, LKD 4.784244537353516, LDE 2.3638710975646973, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/26, Loss=8.630252641439437
Loss made of: CE 0.5105262398719788, LKD 4.4188642501831055, LDE 3.6759238243103027, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.49182674288749695, Reg Loss=8.028345108032227
Clinet index 12, End of Epoch 2/6, Average Loss=8.520172119140625, Class Loss=0.49182674288749695, Reg Loss=8.028345108032227
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/26, Loss=9.069916939735412
Loss made of: CE 0.48686087131500244, LKD 4.830255508422852, LDE 4.385658264160156, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/26, Loss=8.492815294861794
Loss made of: CE 0.4799692630767822, LKD 4.806109428405762, LDE 2.9108126163482666, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4682246148586273, Reg Loss=8.157267570495605
Clinet index 12, End of Epoch 3/6, Average Loss=8.625492095947266, Class Loss=0.4682246148586273, Reg Loss=8.157267570495605
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/26, Loss=8.12980533838272
Loss made of: CE 0.5238820910453796, LKD 5.326338291168213, LDE 2.6243672370910645, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/26, Loss=8.555417066812515
Loss made of: CE 0.4684598445892334, LKD 5.1420578956604, LDE 3.2015130519866943, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.4490107595920563, Reg Loss=7.877658843994141
Clinet index 12, End of Epoch 4/6, Average Loss=8.326669692993164, Class Loss=0.4490107595920563, Reg Loss=7.877658843994141
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/26, Loss=8.265646061301231
Loss made of: CE 0.45135533809661865, LKD 4.859554767608643, LDE 2.8000388145446777, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/26, Loss=8.208580365777015
Loss made of: CE 0.45269665122032166, LKD 4.488530158996582, LDE 3.1003947257995605, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4533998668193817, Reg Loss=7.8146281242370605
Clinet index 12, End of Epoch 5/6, Average Loss=8.268028259277344, Class Loss=0.4533998668193817, Reg Loss=7.8146281242370605
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/26, Loss=8.04177029132843
Loss made of: CE 0.36574506759643555, LKD 4.456489562988281, LDE 3.100844383239746, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/26, Loss=8.142376509308814
Loss made of: CE 0.5068836212158203, LKD 4.8553853034973145, LDE 2.532848596572876, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.4474917948246002, Reg Loss=7.698756694793701
Clinet index 12, End of Epoch 6/6, Average Loss=8.146248817443848, Class Loss=0.4474917948246002, Reg Loss=7.698756694793701
Current Client Index:  10
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=11.918365621566773
Loss made of: CE 0.8081469535827637, LKD 4.422967910766602, LDE 6.017543315887451, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.8114463686943054, Reg Loss=10.405959129333496
Clinet index 10, End of Epoch 1/6, Average Loss=11.217405319213867, Class Loss=0.8114463686943054, Reg Loss=10.405959129333496
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/19, Loss=9.289378291368484
Loss made of: CE 0.6459378004074097, LKD 3.358135461807251, LDE 4.051694393157959, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6625874042510986, Reg Loss=8.729631423950195
Clinet index 10, End of Epoch 2/6, Average Loss=9.392218589782715, Class Loss=0.6625874042510986, Reg Loss=8.729631423950195
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/19, Loss=8.598389875888824
Loss made of: CE 0.6849460601806641, LKD 4.497700214385986, LDE 3.614556074142456, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.613981306552887, Reg Loss=8.47407054901123
Clinet index 10, End of Epoch 3/6, Average Loss=9.088051795959473, Class Loss=0.613981306552887, Reg Loss=8.47407054901123
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/19, Loss=8.70328841805458
Loss made of: CE 0.6510003805160522, LKD 4.916555881500244, LDE 3.4938805103302, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5889292359352112, Reg Loss=7.849877834320068
Clinet index 10, End of Epoch 4/6, Average Loss=8.438807487487793, Class Loss=0.5889292359352112, Reg Loss=7.849877834320068
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/19, Loss=8.886891397833825
Loss made of: CE 0.46100378036499023, LKD 3.9066665172576904, LDE 5.669196128845215, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5847786664962769, Reg Loss=8.066763877868652
Clinet index 10, End of Epoch 5/6, Average Loss=8.651542663574219, Class Loss=0.5847786664962769, Reg Loss=8.066763877868652
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/19, Loss=7.943089663982391
Loss made of: CE 0.6550347805023193, LKD 3.7522501945495605, LDE 3.391047477722168, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5626349449157715, Reg Loss=7.5466694831848145
Clinet index 10, End of Epoch 6/6, Average Loss=8.109304428100586, Class Loss=0.5626349449157715, Reg Loss=7.5466694831848145
Current Client Index:  9
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/21, Loss=9.314755147695541
Loss made of: CE 0.6777540445327759, LKD 5.085983753204346, LDE 4.140640735626221, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/21, Loss=8.920669570565224
Loss made of: CE 0.5784856081008911, LKD 4.204355716705322, LDE 3.7355222702026367, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6038721799850464, Reg Loss=8.522856712341309
Clinet index 9, End of Epoch 1/6, Average Loss=9.126729011535645, Class Loss=0.6038721799850464, Reg Loss=8.522856712341309
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/21, Loss=8.563770371675492
Loss made of: CE 0.5464987754821777, LKD 4.5321044921875, LDE 3.9831645488739014, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 2, Batch 20/21, Loss=8.491727164387703
Loss made of: CE 0.5359021425247192, LKD 4.658555030822754, LDE 2.8430213928222656, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5364513397216797, Reg Loss=7.936639785766602
Clinet index 9, End of Epoch 2/6, Average Loss=8.473091125488281, Class Loss=0.5364513397216797, Reg Loss=7.936639785766602
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/21, Loss=8.876198562979699
Loss made of: CE 0.45547446608543396, LKD 3.899453639984131, LDE 3.4854936599731445, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/21, Loss=8.605143344402313
Loss made of: CE 0.5374695658683777, LKD 4.642632484436035, LDE 3.729088306427002, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5153937339782715, Reg Loss=8.232845306396484
Clinet index 9, End of Epoch 3/6, Average Loss=8.748239517211914, Class Loss=0.5153937339782715, Reg Loss=8.232845306396484
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/21, Loss=7.9822360008955
Loss made of: CE 0.4270836114883423, LKD 4.409853458404541, LDE 3.263101100921631, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/21, Loss=8.512628281116486
Loss made of: CE 0.5733460187911987, LKD 4.961380481719971, LDE 2.9525933265686035, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.5026270151138306, Reg Loss=7.7484002113342285
Clinet index 9, End of Epoch 4/6, Average Loss=8.25102710723877, Class Loss=0.5026270151138306, Reg Loss=7.7484002113342285
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/21, Loss=8.300101515650748
Loss made of: CE 0.5438514351844788, LKD 4.012925624847412, LDE 3.9557456970214844, LReg 0.0, POD 0.0 EntMin 0.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch 5, Batch 20/21, Loss=8.04876474738121
Loss made of: CE 0.5032718777656555, LKD 4.017341136932373, LDE 2.4738383293151855, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.4941350817680359, Reg Loss=7.658772945404053
Clinet index 9, End of Epoch 5/6, Average Loss=8.152908325195312, Class Loss=0.4941350817680359, Reg Loss=7.658772945404053
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/21, Loss=8.820479118824005
Loss made of: CE 0.4843374192714691, LKD 4.097443580627441, LDE 7.115204811096191, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/21, Loss=8.490983748435974
Loss made of: CE 0.5627467632293701, LKD 3.7691965103149414, LDE 2.9209072589874268, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.47978800535202026, Reg Loss=8.070924758911133
Clinet index 9, End of Epoch 6/6, Average Loss=8.550712585449219, Class Loss=0.47978800535202026, Reg Loss=8.070924758911133
Current Client Index:  6
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=10.18814086318016
Loss made of: CE 0.7857138514518738, LKD 3.9388225078582764, LDE 5.155560493469238, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=8.736805528402328
Loss made of: CE 0.6521749496459961, LKD 3.8148841857910156, LDE 3.576554298400879, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7156200408935547, Reg Loss=8.622056007385254
Clinet index 6, End of Epoch 1/6, Average Loss=9.337676048278809, Class Loss=0.7156200408935547, Reg Loss=8.622056007385254
Pseudo labeling is: None
Epoch 2, lr = 0.000925
Epoch 2, Batch 10/24, Loss=8.154331231117249
Loss made of: CE 0.6691943407058716, LKD 4.1086506843566895, LDE 3.371511459350586, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=8.152694898843766
Loss made of: CE 0.6192576885223389, LKD 4.070276737213135, LDE 3.6442861557006836, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6503984332084656, Reg Loss=7.668864727020264
Clinet index 6, End of Epoch 2/6, Average Loss=8.319263458251953, Class Loss=0.6503984332084656, Reg Loss=7.668864727020264
Pseudo labeling is: None
Epoch 3, lr = 0.000849
Epoch 3, Batch 10/24, Loss=8.231503289937972
Loss made of: CE 0.5784180760383606, LKD 3.866412878036499, LDE 3.194988489151001, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=8.482678228616715
Loss made of: CE 0.6675037145614624, LKD 4.7861785888671875, LDE 3.7017457485198975, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6391519904136658, Reg Loss=7.680676460266113
Clinet index 6, End of Epoch 3/6, Average Loss=8.319828033447266, Class Loss=0.6391519904136658, Reg Loss=7.680676460266113
Pseudo labeling is: None
Epoch 4, lr = 0.000772
Epoch 4, Batch 10/24, Loss=7.865038001537323
Loss made of: CE 0.6017315983772278, LKD 4.149192810058594, LDE 3.6805574893951416, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=8.212517499923706
Loss made of: CE 0.6331007480621338, LKD 4.260004997253418, LDE 3.346015453338623, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6353870630264282, Reg Loss=7.357538223266602
Clinet index 6, End of Epoch 4/6, Average Loss=7.99292516708374, Class Loss=0.6353870630264282, Reg Loss=7.357538223266602
Pseudo labeling is: None
Epoch 5, lr = 0.000694
Epoch 5, Batch 10/24, Loss=7.880181854963302
Loss made of: CE 0.7491011619567871, LKD 3.7267587184906006, LDE 4.101787090301514, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=7.890667951107025
Loss made of: CE 0.5886585712432861, LKD 3.844691038131714, LDE 2.9177825450897217, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.6214228868484497, Reg Loss=7.288688659667969
Clinet index 6, End of Epoch 5/6, Average Loss=7.910111427307129, Class Loss=0.6214228868484497, Reg Loss=7.288688659667969
Pseudo labeling is: None
Epoch 6, lr = 0.000616
Epoch 6, Batch 10/24, Loss=8.023454850912094
Loss made of: CE 0.607689380645752, LKD 3.9421586990356445, LDE 3.0172061920166016, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=7.748907071352005
Loss made of: CE 0.5673621892929077, LKD 4.034121513366699, LDE 3.682170867919922, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.6106765270233154, Reg Loss=7.28520393371582
Clinet index 6, End of Epoch 6/6, Average Loss=7.895880699157715, Class Loss=0.6106765270233154, Reg Loss=7.28520393371582
federated aggregation...
Validation, Class Loss=0.671058177947998, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.795267
Mean Acc: 0.371127
FreqW Acc: 0.703349
Mean IoU: 0.240535
Class IoU:
	class 0: 0.88345534
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.027924368
	class 5: 7.0911905e-05
	class 6: 0.024475528
	class 7: 0.24230374
	class 8: 0.17702526
	class 9: 0.028261267
	class 10: 2.5521493e-05
	class 11: 0.34081602
	class 12: 0.36048314
	class 13: 0.42201126
	class 14: 0.64793646
	class 15: 0.747179
	class 16: 0.1741588
	class 17: 0.17587507
	class 18: 0.28140637
	class 19: 0.22825663
	class 20: 0.28956494
Class Acc:
	class 0: 0.95894516
	class 1: 0.0
	class 2: 0.0
	class 3: 0.0
	class 4: 0.028140496
	class 5: 7.0911905e-05
	class 6: 0.024483794
	class 7: 0.24242163
	class 8: 0.17723279
	class 9: 0.028566193
	class 10: 2.5521867e-05
	class 11: 0.41085112
	class 12: 0.5313155
	class 13: 0.61445063
	class 14: 0.7054792
	class 15: 0.832065
	class 16: 0.1786007
	class 17: 0.9244547
	class 18: 0.60508525
	class 19: 0.7800434
	class 20: 0.7514366

federated global round: 24, step: 4
select part of clients to conduct local training
[18, 24, 25, 10]
Current Client Index:  18
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/19, Loss=10.189351987838744
Loss made of: CE 0.6153942942619324, LKD 3.9550106525421143, LDE 4.49462366104126, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.6611791849136353, Reg Loss=9.286330223083496
Clinet index 18, End of Epoch 1/6, Average Loss=9.947509765625, Class Loss=0.6611791849136353, Reg Loss=9.286330223083496
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/19, Loss=8.786322590708732
Loss made of: CE 0.708388090133667, LKD 4.146019458770752, LDE 4.439005374908447, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.5922985076904297, Reg Loss=7.994576930999756
Clinet index 18, End of Epoch 2/6, Average Loss=8.586875915527344, Class Loss=0.5922985076904297, Reg Loss=7.994576930999756
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/19, Loss=8.739586627483368
Loss made of: CE 0.6180694103240967, LKD 5.125416278839111, LDE 3.8898301124572754, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.5724018216133118, Reg Loss=7.891383171081543
Clinet index 18, End of Epoch 3/6, Average Loss=8.463785171508789, Class Loss=0.5724018216133118, Reg Loss=7.891383171081543
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/19, Loss=8.053712403774261
Loss made of: CE 0.5666362643241882, LKD 4.023401260375977, LDE 2.5124595165252686, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.544669508934021, Reg Loss=7.523038387298584
Clinet index 18, End of Epoch 4/6, Average Loss=8.067708015441895, Class Loss=0.544669508934021, Reg Loss=7.523038387298584
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/19, Loss=8.350317198038102
Loss made of: CE 0.4941016435623169, LKD 3.371232271194458, LDE 2.7595818042755127, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.5430931448936462, Reg Loss=7.478964805603027
Clinet index 18, End of Epoch 5/6, Average Loss=8.02205753326416, Class Loss=0.5430931448936462, Reg Loss=7.478964805603027
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/19, Loss=7.798662003874779
Loss made of: CE 0.5112058520317078, LKD 4.496534824371338, LDE 2.9552927017211914, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5339440703392029, Reg Loss=7.156457901000977
Clinet index 18, End of Epoch 6/6, Average Loss=7.690402030944824, Class Loss=0.5339440703392029, Reg Loss=7.156457901000977
Current Client Index:  24
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/21, Loss=8.553760048747062
Loss made of: CE 0.5102267265319824, LKD 3.8571557998657227, LDE 3.095134735107422, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/21, Loss=8.980062353610993
Loss made of: CE 0.5412119626998901, LKD 4.684144020080566, LDE 4.6442975997924805, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.5483317375183105, Reg Loss=8.15952205657959
Clinet index 24, End of Epoch 1/6, Average Loss=8.707853317260742, Class Loss=0.5483317375183105, Reg Loss=8.15952205657959
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/21, Loss=7.83469649553299
Loss made of: CE 0.4410543441772461, LKD 4.613602161407471, LDE 2.709221601486206, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/21, Loss=9.161571821570396
Loss made of: CE 0.548143744468689, LKD 5.047175884246826, LDE 4.758434772491455, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.49202579259872437, Reg Loss=8.114197731018066
Clinet index 24, End of Epoch 2/6, Average Loss=8.606223106384277, Class Loss=0.49202579259872437, Reg Loss=8.114197731018066
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch 3, Batch 10/21, Loss=8.16942463517189
Loss made of: CE 0.4668346643447876, LKD 4.609756946563721, LDE 3.2538957595825195, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/21, Loss=8.505087938904762
Loss made of: CE 0.4973379969596863, LKD 3.9753661155700684, LDE 3.940861701965332, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.4901816248893738, Reg Loss=7.844449996948242
Clinet index 24, End of Epoch 3/6, Average Loss=8.33463191986084, Class Loss=0.4901816248893738, Reg Loss=7.844449996948242
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/21, Loss=7.909760293364525
Loss made of: CE 0.5221093893051147, LKD 4.942651271820068, LDE 2.915104389190674, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/21, Loss=7.645036596059799
Loss made of: CE 0.41726624965667725, LKD 4.380647659301758, LDE 3.37339448928833, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.47400012612342834, Reg Loss=7.4691548347473145
Clinet index 24, End of Epoch 4/6, Average Loss=7.943154811859131, Class Loss=0.47400012612342834, Reg Loss=7.4691548347473145
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/21, Loss=7.988941890001297
Loss made of: CE 0.4070647954940796, LKD 3.527878522872925, LDE 3.6705262660980225, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/21, Loss=7.923897004127502
Loss made of: CE 0.4336535334587097, LKD 5.386966705322266, LDE 3.450169801712036, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.47151830792427063, Reg Loss=7.464771747589111
Clinet index 24, End of Epoch 5/6, Average Loss=7.936290264129639, Class Loss=0.47151830792427063, Reg Loss=7.464771747589111
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/21, Loss=8.374682819843292
Loss made of: CE 0.5143622159957886, LKD 5.833017826080322, LDE 3.343402624130249, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/21, Loss=7.574806636571884
Loss made of: CE 0.4861133098602295, LKD 4.142680644989014, LDE 2.945647716522217, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.46644243597984314, Reg Loss=7.4473676681518555
Clinet index 24, End of Epoch 6/6, Average Loss=7.9138102531433105, Class Loss=0.46644243597984314, Reg Loss=7.4473676681518555
Current Client Index:  25
Filtering images...
	0/10582 ...
	1000/10582 ...
	2000/10582 ...
	3000/10582 ...
	4000/10582 ...
	5000/10582 ...
	6000/10582 ...
	7000/10582 ...
	8000/10582 ...
	9000/10582 ...
	10000/10582 ...
load old model
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.001000
Epoch 1, Batch 10/24, Loss=9.116304409503936
Loss made of: CE 0.7461376190185547, LKD 4.5953569412231445, LDE 4.371284008026123, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Batch 20/24, Loss=8.757511562108993
Loss made of: CE 0.6528796553611755, LKD 4.001882076263428, LDE 3.7266716957092285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7045614123344421, Reg Loss=8.169760704040527
Clinet index 25, End of Epoch 1/6, Average Loss=8.874321937561035, Class Loss=0.7045614123344421, Reg Loss=8.169760704040527
Pseudo labeling is: None
Epoch 2, lr = 0.000849
Epoch 2, Batch 10/24, Loss=8.620985043048858
Loss made of: CE 0.7323057651519775, LKD 4.872386932373047, LDE 3.5461630821228027, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Batch 20/24, Loss=8.299219578504562
Loss made of: CE 0.757990300655365, LKD 4.387467384338379, LDE 3.5184919834136963, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.669660210609436, Reg Loss=7.730227947235107
Clinet index 25, End of Epoch 2/6, Average Loss=8.399888038635254, Class Loss=0.669660210609436, Reg Loss=7.730227947235107
Pseudo labeling is: None
Epoch 3, lr = 0.000694
Epoch 3, Batch 10/24, Loss=8.200379544496537
Loss made of: CE 0.6595700979232788, LKD 3.7829067707061768, LDE 3.8630247116088867, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Batch 20/24, Loss=8.281976705789566
Loss made of: CE 0.632008969783783, LKD 3.894765615463257, LDE 4.2252583503723145, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6564704179763794, Reg Loss=7.494501113891602
Clinet index 25, End of Epoch 3/6, Average Loss=8.150971412658691, Class Loss=0.6564704179763794, Reg Loss=7.494501113891602
Pseudo labeling is: None
Epoch 4, lr = 0.000536
Epoch 4, Batch 10/24, Loss=8.256645822525025
Loss made of: CE 0.6582915186882019, LKD 4.452244281768799, LDE 3.38250470161438, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Batch 20/24, Loss=8.021770435571671
Loss made of: CE 0.6570147275924683, LKD 4.3695549964904785, LDE 3.6951205730438232, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6349780559539795, Reg Loss=7.565103530883789
Clinet index 25, End of Epoch 4/6, Average Loss=8.200081825256348, Class Loss=0.6349780559539795, Reg Loss=7.565103530883789
Pseudo labeling is: None
Epoch 5, lr = 0.000372
Epoch 5, Batch 10/24, Loss=8.004511034488678
Loss made of: CE 0.7483654022216797, LKD 4.487722396850586, LDE 3.231839895248413, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Batch 20/24, Loss=7.820056068897247
Loss made of: CE 0.6713592410087585, LKD 3.976433038711548, LDE 2.9552724361419678, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.6373258829116821, Reg Loss=7.198892593383789
Clinet index 25, End of Epoch 5/6, Average Loss=7.836218357086182, Class Loss=0.6373258829116821, Reg Loss=7.198892593383789
Pseudo labeling is: None
Epoch 6, lr = 0.000199
Epoch 6, Batch 10/24, Loss=7.557474488019944
Loss made of: CE 0.6456732749938965, LKD 4.588962554931641, LDE 3.437129020690918, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Batch 20/24, Loss=7.764071580767632
Loss made of: CE 0.7077621221542358, LKD 4.17197322845459, LDE 3.0547571182250977, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.6263045072555542, Reg Loss=7.101071357727051
Clinet index 25, End of Epoch 6/6, Average Loss=7.7273759841918945, Class Loss=0.6263045072555542, Reg Loss=7.101071357727051
Current Client Index:  10
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Pseudo labeling is: None
Epoch 1, lr = 0.000536
Epoch 1, Batch 10/19, Loss=10.524189370870591
Loss made of: CE 0.6912583112716675, LKD 4.561617374420166, LDE 5.385343074798584, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 1, Class Loss=0.7298840284347534, Reg Loss=9.424104690551758
Clinet index 10, End of Epoch 1/6, Average Loss=10.1539888381958, Class Loss=0.7298840284347534, Reg Loss=9.424104690551758
Pseudo labeling is: None
Epoch 2, lr = 0.000455
Epoch 2, Batch 10/19, Loss=9.08586077094078
Loss made of: CE 0.6466190814971924, LKD 3.3474690914154053, LDE 4.052730560302734, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 2, Class Loss=0.6561017632484436, Reg Loss=8.526117324829102
Clinet index 10, End of Epoch 2/6, Average Loss=9.182219505310059, Class Loss=0.6561017632484436, Reg Loss=8.526117324829102
Pseudo labeling is: None
Epoch 3, lr = 0.000372
Epoch 3, Batch 10/19, Loss=8.252411979436875
Loss made of: CE 0.7389658689498901, LKD 4.443436622619629, LDE 3.25319766998291, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 3, Class Loss=0.6261461973190308, Reg Loss=8.223989486694336
Clinet index 10, End of Epoch 3/6, Average Loss=8.850135803222656, Class Loss=0.6261461973190308, Reg Loss=8.223989486694336
Pseudo labeling is: None
Epoch 4, lr = 0.000287
Epoch 4, Batch 10/19, Loss=8.668179351091386
Loss made of: CE 0.6787784099578857, LKD 4.910730361938477, LDE 3.2010371685028076, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 4, Class Loss=0.6083382368087769, Reg Loss=7.7809343338012695
Clinet index 10, End of Epoch 4/6, Average Loss=8.389272689819336, Class Loss=0.6083382368087769, Reg Loss=7.7809343338012695
Pseudo labeling is: None
Epoch 5, lr = 0.000199
Epoch 5, Batch 10/19, Loss=8.681110328435897
Loss made of: CE 0.44962942600250244, LKD 3.8835062980651855, LDE 5.4158806800842285, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 5, Class Loss=0.6077055931091309, Reg Loss=7.983808994293213
Clinet index 10, End of Epoch 5/6, Average Loss=8.591514587402344, Class Loss=0.6077055931091309, Reg Loss=7.983808994293213
Pseudo labeling is: None
Epoch 6, lr = 0.000107
Epoch 6, Batch 10/19, Loss=7.875624358654022
Loss made of: CE 0.6563671827316284, LKD 3.6798412799835205, LDE 3.223454236984253, LReg 0.0, POD 0.0 EntMin 0.0
Epoch 6, Class Loss=0.5862389206886292, Reg Loss=7.458617210388184
Clinet index 10, End of Epoch 6/6, Average Loss=8.044856071472168, Class Loss=0.5862389206886292, Reg Loss=7.458617210388184
federated aggregation...
/data/zhangdz/CVPR2023/FISS/metrics/stream_metrics.py:132: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax = plt.subplots()
Validation, Class Loss=0.7029851675033569, Reg Loss=0.0 (without scaling)

Total samples: 1449.000000
Overall Acc: 0.781751
Mean Acc: 0.356416
FreqW Acc: 0.693940
Mean IoU: 0.226924
Class IoU:
	class 0: 0.87769455
	class 1: 0.0
	class 2: 1.4334503e-06
	class 3: 0.0
	class 4: 0.08545428
	class 5: 0.0005203161
	class 6: 0.041176576
	class 7: 0.2267625
	class 8: 0.14517447
	class 9: 0.02393826
	class 10: 0.0
	class 11: 0.34503612
	class 12: 0.25160605
	class 13: 0.36042663
	class 14: 0.63492537
	class 15: 0.7440626
	class 16: 0.15151343
	class 17: 0.13497412
	class 18: 0.23446335
	class 19: 0.24743521
	class 20: 0.26023245
Class Acc:
	class 0: 0.95131284
	class 1: 0.0
	class 2: 1.4334503e-06
	class 3: 0.0
	class 4: 0.08657842
	class 5: 0.0005203161
	class 6: 0.041189678
	class 7: 0.2268693
	class 8: 0.14527479
	class 9: 0.024143009
	class 10: 0.0
	class 11: 0.40202826
	class 12: 0.3221382
	class 13: 0.4620504
	class 14: 0.6850392
	class 15: 0.81731105
	class 16: 0.15464593
	class 17: 0.95060444
	class 18: 0.67560154
	class 19: 0.7500962
	class 20: 0.7893245

voc_4-4_ILT On GPUs 0
Run in 43037s
